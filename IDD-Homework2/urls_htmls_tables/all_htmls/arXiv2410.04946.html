<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness</title>
<!--Generated on Mon Oct  7 11:08:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04946v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Fundamentals of Modern Object Recognition</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S1" title="In Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Supervised Learning in Computer Vision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2" title="In Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep-Learning-Based Object Recognition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS1" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Standard Architecture Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS2" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Attention Mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS3" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Object Classification and Postprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS4" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Training Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS5" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.5 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Relevant State of the Art</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S1" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Real-world Maritime Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S2" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Ship Recognition Using Maritime Monitoring Footage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S3" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Georeferencing of Recognized Ships</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S4" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Deployment on Embedded Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>ShipSG: Ship Segmentation and Georeferencing Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S1" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S2" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Acquisition and Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S3" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Ship Recognition for Improved Maritime Awareness</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Ship Detection for Maritime Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Detection of Abnormal Vessel Behaviour from Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS2" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Ship Detection for Integrity Assessment of Camera Obstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS3" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Ship Detection for 3D Reconstruction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Standard Ship Segmentation Using ShipSG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S3" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Advanced Ship Recognition for Real-time Operation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>The ScatBlock</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>ScatYOLOv8+CBAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S3" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Optimized ScatYOLOv8+CBAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S4" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Enhanced Small Ship Segmentation Using Higher Resolution Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S5" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Ship Georeferencing for Maritime Situational Awareness</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S1" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Homographies for Image Georeferencing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Ship Detection and Georeferencing Using Homographies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S3" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Analysis of Ship Segmentation and Georeferencing Using Homographies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S4" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch8" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Summary and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch9" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Publications by the Author for this Thesis</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-I] </span><span class="ltx_text"> </span>Detection and Geovisualization of Abnormal Vessel Behavior from Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-II] </span><span class="ltx_text"> </span>Ship Segmentation and Georeferencing from Static Oblique View Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-III] </span><span class="ltx_text"> </span>Integrity Assessment of Maritime Object Detection Impacted by Partial Camera Obstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-IV] </span><span class="ltx_text"> </span>Embedded 3D reconstruction of Dynamic Objects in Real Time for Maritime Situational Awareness Pictures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-V] </span><span class="ltx_text"> </span>Improving YOLOv8 with Scattering Transform and Attention for Maritime Awareness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-VI] </span><span class="ltx_text"> </span>Enhanced Small Ship Segmentation with Optimized ScatYOLOv8+CBAM on Embedded Systems</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.2">\ChNameVar</span><span class="ltx_ERROR undefined" id="p1.3">\ChTitleVar</span><span class="ltx_ERROR undefined" id="p1.4" lang="de">\subjectarea</span>
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1" lang="de" style="font-size:80%;">Computer Science<span class="ltx_text" id="p1.1.1.1" style="font-size:125%;">
<span class="ltx_ERROR undefined" id="p1.1.1.1.1">\crest</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="108" id="p1.1.1.1.g1" src="extracted/5906916/logo_english.png" width="299"/>
<span class="ltx_ERROR undefined" id="p1.1.1.1.2">\degreetitle</span></span>Doctor<span class="ltx_ERROR undefined" id="p1.1.1.2">\degreedate</span><span class="ltx_text" id="p1.1.1.3" style="font-size:125%;">2024</span></span></p>
</div>
<h1 class="ltx_title ltx_title_document" lang="de">Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="de">Borja Carrillo Perez
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1"><span class="ltx_text" id="id12.id1.1" lang="en">In an era where maritime infrastructures are paramount supports of societies, the need for advanced maritime situational awareness solutions has become increasingly important.
Existing ship monitoring procedures, such as the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">Automatic Identification System (AIS)</span></a>, have limitations, suffer from delayed updates and are vulnerable to cyberattacks.
Other technologies, such as satellite imagery and radar, face challenges in real-time applications due to delays in acquiring and processing data. The use of optical camera systems and image processing can improve situational awareness, allowing real-time usage of maritime infrastructure footage. However, the number of video streams available poses a challenge for maritime operators, who could be helped by summarized spatial information of recognized ships, irrespective of their size and type, and presented on a map in real-time. This motivates the development of automated ship recognition and georeferencing technologies. Moreover, the deployment of such camera systems, equipped with an embedded device, allows for local data processing on the edge to minimize network demand, energy usage, decrease latency, cut costs, and enhance data protection.</span></p>
<p class="ltx_p" id="id5.5"><span class="ltx_text" id="id5.5.5" lang="en">This thesis, integrating six of my publications, presents a comprehensive investigation into leveraging deep learning and computer vision to advance the research in real-time ship recognition and georeferencing for the improvement of maritime situational awareness. I present a novel dataset for ship recognition and georeferencing, ShipSG, which facilitates the development and validation of recognition and georeferencing methodologies. The dataset contains 3505 images and 11625 ship masks with their corresponding class, geographic position and length. Through a series of studies of state-of-the-art deep-learning-based object recognition algorithms, I introduce a custom real-time segmentation architecture, ScatYOLOv8+CBAM. This architecture was created and optimized for the NVIDIA Jetson AGX Xavier as embedded system. ScatYOLOv8+CBAM incorporates the 2D scattering transform, a novel addition that enhances YOLOv8 in real-world applications such as ship segmentation. Additionally, the performance is further improved with the integration of attention mechanisms.
The proposed architecture exceeds in more than 5% the performance of state-of-the-art methods, achieving a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mean Average Precision (mAP)</span></a> of 75.46%. The inference speed, once the customized architecture is deployed on the embedded system using TensorRT, is of 25.3 ms per frame.
Furthermore, I address the need for precision in recognizing small and distant ships and their real-time processing of full-resolution images on embedded systems, with an enhanced slicing mechanism that performs batch inference and merges predictions, achieving <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> improvements ranging from 8% to 11%.
The recognized ships are georeferenced using my proposed method, which automatically calculates the georeferencing pixel of the recognized ships, and uses homographies to provide the geographic position of ships from single images, without prior camera knowledge.
In the quantitative analysis, the georeferencing method achieved a positioning error of <math alttext="18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mrow id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mrow id="id1.1.1.m1.1.1.2" xref="id1.1.1.m1.1.1.2.cmml"><mn id="id1.1.1.m1.1.1.2.2" xref="id1.1.1.m1.1.1.2.2.cmml">18</mn><mo id="id1.1.1.m1.1.1.2.1" lspace="0.500em" xref="id1.1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="id1.1.1.m1.1.1.2.3" xref="id1.1.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="id1.1.1.m1.1.1.1" lspace="0.722em" xref="id1.1.1.m1.1.1.1.cmml">±</mo><mrow id="id1.1.1.m1.1.1.3" xref="id1.1.1.m1.1.1.3.cmml"><mn id="id1.1.1.m1.1.1.3.2" xref="id1.1.1.m1.1.1.3.2.cmml"> 10</mn><mo id="id1.1.1.m1.1.1.3.1" lspace="0.500em" xref="id1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="id1.1.1.m1.1.1.3.3" xref="id1.1.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.1.m1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="id1.1.1.m1.1.1.2.cmml" xref="id1.1.1.m1.1.1.2"><times id="id1.1.1.m1.1.1.2.1.cmml" xref="id1.1.1.m1.1.1.2.1"></times><cn id="id1.1.1.m1.1.1.2.2.cmml" type="integer" xref="id1.1.1.m1.1.1.2.2">18</cn><ci id="id1.1.1.m1.1.1.2.3.cmml" xref="id1.1.1.m1.1.1.2.3">𝑚</ci></apply><apply id="id1.1.1.m1.1.1.3.cmml" xref="id1.1.1.m1.1.1.3"><times id="id1.1.1.m1.1.1.3.1.cmml" xref="id1.1.1.m1.1.1.3.1"></times><cn id="id1.1.1.m1.1.1.3.2.cmml" type="integer" xref="id1.1.1.m1.1.1.3.2">10</cn><ci id="id1.1.1.m1.1.1.3.3.cmml" xref="id1.1.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">18 italic_m ± 10 italic_m</annotation></semantics></math> for ranges inside the port basin (up to 400 <math alttext="m" class="ltx_Math" display="inline" id="id2.2.2.m2.1"><semantics id="id2.2.2.m2.1a"><mi id="id2.2.2.m2.1.1" xref="id2.2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id2.2.2.m2.1b"><ci id="id2.2.2.m2.1.1.cmml" xref="id2.2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m2.1d">italic_m</annotation></semantics></math>) and <math alttext="44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="id3.3.3.m3.1"><semantics id="id3.3.3.m3.1a"><mrow id="id3.3.3.m3.1.1" xref="id3.3.3.m3.1.1.cmml"><mrow id="id3.3.3.m3.1.1.2" xref="id3.3.3.m3.1.1.2.cmml"><mn id="id3.3.3.m3.1.1.2.2" xref="id3.3.3.m3.1.1.2.2.cmml">44</mn><mo id="id3.3.3.m3.1.1.2.1" lspace="0.500em" xref="id3.3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="id3.3.3.m3.1.1.2.3" xref="id3.3.3.m3.1.1.2.3.cmml">m</mi></mrow><mo id="id3.3.3.m3.1.1.1" lspace="0.722em" xref="id3.3.3.m3.1.1.1.cmml">±</mo><mrow id="id3.3.3.m3.1.1.3" xref="id3.3.3.m3.1.1.3.cmml"><mn id="id3.3.3.m3.1.1.3.2" xref="id3.3.3.m3.1.1.3.2.cmml"> 27</mn><mo id="id3.3.3.m3.1.1.3.1" lspace="0.500em" xref="id3.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="id3.3.3.m3.1.1.3.3" xref="id3.3.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="id3.3.3.m3.1b"><apply id="id3.3.3.m3.1.1.cmml" xref="id3.3.3.m3.1.1"><csymbol cd="latexml" id="id3.3.3.m3.1.1.1.cmml" xref="id3.3.3.m3.1.1.1">plus-or-minus</csymbol><apply id="id3.3.3.m3.1.1.2.cmml" xref="id3.3.3.m3.1.1.2"><times id="id3.3.3.m3.1.1.2.1.cmml" xref="id3.3.3.m3.1.1.2.1"></times><cn id="id3.3.3.m3.1.1.2.2.cmml" type="integer" xref="id3.3.3.m3.1.1.2.2">44</cn><ci id="id3.3.3.m3.1.1.2.3.cmml" xref="id3.3.3.m3.1.1.2.3">𝑚</ci></apply><apply id="id3.3.3.m3.1.1.3.cmml" xref="id3.3.3.m3.1.1.3"><times id="id3.3.3.m3.1.1.3.1.cmml" xref="id3.3.3.m3.1.1.3.1"></times><cn id="id3.3.3.m3.1.1.3.2.cmml" type="integer" xref="id3.3.3.m3.1.1.3.2">27</cn><ci id="id3.3.3.m3.1.1.3.3.cmml" xref="id3.3.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m3.1c">44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="id3.3.3.m3.1d">44 italic_m ± 27 italic_m</annotation></semantics></math> outside (from 400 <math alttext="m" class="ltx_Math" display="inline" id="id4.4.4.m4.1"><semantics id="id4.4.4.m4.1a"><mi id="id4.4.4.m4.1.1" xref="id4.4.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id4.4.4.m4.1b"><ci id="id4.4.4.m4.1.1.cmml" xref="id4.4.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="id4.4.4.m4.1d">italic_m</annotation></semantics></math> to 1200 <math alttext="m" class="ltx_Math" display="inline" id="id5.5.5.m5.1"><semantics id="id5.5.5.m5.1a"><mi id="id5.5.5.m5.1.1" xref="id5.5.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id5.5.5.m5.1b"><ci id="id5.5.5.m5.1.1.cmml" xref="id5.5.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="id5.5.5.m5.1d">italic_m</annotation></semantics></math>).
The main findings reveal significant advancements in maritime situational awareness with the practical demonstration of the applicability of the methodologies in real-world scenarios, such as the detection of abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The approach not only outperforms existing methods in terms of accuracy and processing speed but also provides a framework for seamlessly integrating recognized and georeferenced ships into real-time systems, enhancing operational effectiveness and decision-making for maritime authorities. The integration of these methodologies into embedded systems represents a pivotal advancement in the domain, offering a scalable and efficient solution for improving maritime situational awareness and response capabilities.
This thesis contributes to the maritime computer vision field by establishing a benchmark for ship segmentation and georeferencing research, demonstrating the viability of deep-learning-based recognition and georeferencing methods for real-time maritime monitoring.</span></p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Zusammenfassung</h6>
<p class="ltx_p" id="id13.id1"><span class="ltx_text" id="id13.id1.1" lang="de">In einer Ära, in der maritime Infrastrukturen von größter Bedeutung für Gesellschaften sind, ist der Bedarf an fortschrittlichen Lösungen zur maritimen Lageerkennung zunehmend wichtiger geworden.
Bestehende Verfahrung zur Schiffsbeobachtung wie das <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">Automatic Identification System (AIS)</span></a> haben Einschränkungen, leiden unter verzögerten Aktualisierungen und sind anfällig für Cyberangriffe.
Andere Technologien, wie Satellitenbilder und Radar, haben Schwierigkeiten bei Echtzeitanwendungen aufgrund von Verzögerungen bei der Erfassung und Verarbeitung von Daten. Der Einsatz von optischen Kamerasystemen und Computer Vision kann das Situationsbewusstsein verbessern, indem sie die Echtzeitnutzung von Aufnahmen direkt an marititmen Infrastrukturen ermöglichen. Die große Anzahl verfügbarer Videostreams stellt jedoch eine Herausforderung für maritime Betreiber dar, die durch zusammengefasste, räumliche Informationen über erkannte Schiffe, unabhängig von ihrer Größe und Art, auf einer Karte in Echtzeit unterstützt werden könnten. Dies motiviert die Entwicklung automatisierter Technologien zur Schiffsidentifikation und Georeferenzierung. Darüber hinaus ermöglicht der Einsatz von Kamerasystemen zusammen mit Embedded Systems, die lokale Datenverarbeitung in situ, um den Netzwerkbedarf zu minimieren, den Energieverbrauch zu senken, die Latenz zu verringern, die Kosten zu senken und den Datenschutz zu verbessern.</span></p>
<p class="ltx_p" id="id10.5"><span class="ltx_text" id="id10.5.5" lang="de">Diese Dissertation, die sechs meiner Veröffentlichungen bündelt, präsentiert eine umfassende Untersuchung zur Nutzung von Deep Learning und Computer Vision, um die Forschung zur Echtzeit-Schiffsidentifikation und Georeferenzierung zur Verbesserung des maritimen Situationsbewusstseins voranzutreiben. Ich präsentiere einen neuartigen Datensatz für die Schiffsidentifikation und Georeferenzierung, ShipSG, der die Entwicklung und Validierung von Identifikations- und Georeferenzierungsmethoden erleichtert. Der Datensatz enthält 3505 Bilder und 11625 Schiffsmasken mit entsprechenden Klassen, geografischer Position und Länge. Durch eine Reihe von Studien zu den neuesten Objekterkennungsalgorithmen basierend auf Deep-Learning stelle ich eine neuartige Echtzeit-Segmentierungsarchitektur vor, ScatYOLOv8+CBAM. Diese Architektur wurde speziell für den NVIDIA Jetson AGX Xavier als eingebettetes System entwickelt und optimiert. ScatYOLOv8+CBAM integriert die 2D-Streutransformation, eine neuartige Ergänzung, die YOLOv8 in realen Anwendungen wie der Schiffsegmentierung verbessert. Zudem wird die Leistung durch die Integration von Attention-Mechanismen weiter gesteigert.
Die vorgeschlagene Architektur übertrifft neueste Methoden um mehr als 5% und erreicht eine Mean-Average-Precision (mAP) von 75.46%. Die Inferenzlaufzeit beträgt 25.3 ms pro Frame, sobald die angepasste Architektur auf dem eingebetteten System mit TensorRT bereitgestellt ist.
Darüber hinaus gehe ich auf die Notwendigkeit der Präzision bei der Erkennung kleiner und entfernter Schiffe und ihrer Echtzeitverarbeitung von Bildern in voller Auflösung auf eingebetteten Systemen ein. Hierzu betrachte ich einen verbesserten Slicing-Mechanismus, der Batch-Inferenz durchführt und Vorhersagen zusammenführt, was letztlich in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mean Average Precision (mAP)</span></a>-Verbesserungen von 8% bis 11% resultiert.
Die erkannten Schiffe werden mittels meiner vorgeschlagenen Methode georeferenziert. Dazu wird automatisch das Georeferenzierungspixel der erkannten Schiffe berechnet und Homographien verwendet, um die geografische Position der Schiffe aus Einzelbildern ohne vorherige Kamerakenntnisse zu bestimmen.
In der quantitativen Analyse erreichte die Georeferenzierungsmethode einen Positionierungsfehler von <math alttext="18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="id6.1.1.m1.1"><semantics id="id6.1.1.m1.1a"><mrow id="id6.1.1.m1.1.1" xref="id6.1.1.m1.1.1.cmml"><mrow id="id6.1.1.m1.1.1.2" xref="id6.1.1.m1.1.1.2.cmml"><mn id="id6.1.1.m1.1.1.2.2" xref="id6.1.1.m1.1.1.2.2.cmml">18</mn><mo id="id6.1.1.m1.1.1.2.1" lspace="0.500em" xref="id6.1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="id6.1.1.m1.1.1.2.3" xref="id6.1.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="id6.1.1.m1.1.1.1" lspace="0.722em" xref="id6.1.1.m1.1.1.1.cmml">±</mo><mrow id="id6.1.1.m1.1.1.3" xref="id6.1.1.m1.1.1.3.cmml"><mn id="id6.1.1.m1.1.1.3.2" xref="id6.1.1.m1.1.1.3.2.cmml"> 10</mn><mo id="id6.1.1.m1.1.1.3.1" lspace="0.500em" xref="id6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="id6.1.1.m1.1.1.3.3" xref="id6.1.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="id6.1.1.m1.1b"><apply id="id6.1.1.m1.1.1.cmml" xref="id6.1.1.m1.1.1"><csymbol cd="latexml" id="id6.1.1.m1.1.1.1.cmml" xref="id6.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="id6.1.1.m1.1.1.2.cmml" xref="id6.1.1.m1.1.1.2"><times id="id6.1.1.m1.1.1.2.1.cmml" xref="id6.1.1.m1.1.1.2.1"></times><cn id="id6.1.1.m1.1.1.2.2.cmml" type="integer" xref="id6.1.1.m1.1.1.2.2">18</cn><ci id="id6.1.1.m1.1.1.2.3.cmml" xref="id6.1.1.m1.1.1.2.3">𝑚</ci></apply><apply id="id6.1.1.m1.1.1.3.cmml" xref="id6.1.1.m1.1.1.3"><times id="id6.1.1.m1.1.1.3.1.cmml" xref="id6.1.1.m1.1.1.3.1"></times><cn id="id6.1.1.m1.1.1.3.2.cmml" type="integer" xref="id6.1.1.m1.1.1.3.2">10</cn><ci id="id6.1.1.m1.1.1.3.3.cmml" xref="id6.1.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.1.1.m1.1c">18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="id6.1.1.m1.1d">18 italic_m ± 10 italic_m</annotation></semantics></math> für Entfernungen innerhalb des betrachtetetn Hafenbeckens (bis zu 400 <math alttext="m" class="ltx_Math" display="inline" id="id7.2.2.m2.1"><semantics id="id7.2.2.m2.1a"><mi id="id7.2.2.m2.1.1" xref="id7.2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id7.2.2.m2.1b"><ci id="id7.2.2.m2.1.1.cmml" xref="id7.2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.2.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="id7.2.2.m2.1d">italic_m</annotation></semantics></math>) und <math alttext="44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="id8.3.3.m3.1"><semantics id="id8.3.3.m3.1a"><mrow id="id8.3.3.m3.1.1" xref="id8.3.3.m3.1.1.cmml"><mrow id="id8.3.3.m3.1.1.2" xref="id8.3.3.m3.1.1.2.cmml"><mn id="id8.3.3.m3.1.1.2.2" xref="id8.3.3.m3.1.1.2.2.cmml">44</mn><mo id="id8.3.3.m3.1.1.2.1" lspace="0.500em" xref="id8.3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="id8.3.3.m3.1.1.2.3" xref="id8.3.3.m3.1.1.2.3.cmml">m</mi></mrow><mo id="id8.3.3.m3.1.1.1" lspace="0.722em" xref="id8.3.3.m3.1.1.1.cmml">±</mo><mrow id="id8.3.3.m3.1.1.3" xref="id8.3.3.m3.1.1.3.cmml"><mn id="id8.3.3.m3.1.1.3.2" xref="id8.3.3.m3.1.1.3.2.cmml"> 27</mn><mo id="id8.3.3.m3.1.1.3.1" lspace="0.500em" xref="id8.3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="id8.3.3.m3.1.1.3.3" xref="id8.3.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="id8.3.3.m3.1b"><apply id="id8.3.3.m3.1.1.cmml" xref="id8.3.3.m3.1.1"><csymbol cd="latexml" id="id8.3.3.m3.1.1.1.cmml" xref="id8.3.3.m3.1.1.1">plus-or-minus</csymbol><apply id="id8.3.3.m3.1.1.2.cmml" xref="id8.3.3.m3.1.1.2"><times id="id8.3.3.m3.1.1.2.1.cmml" xref="id8.3.3.m3.1.1.2.1"></times><cn id="id8.3.3.m3.1.1.2.2.cmml" type="integer" xref="id8.3.3.m3.1.1.2.2">44</cn><ci id="id8.3.3.m3.1.1.2.3.cmml" xref="id8.3.3.m3.1.1.2.3">𝑚</ci></apply><apply id="id8.3.3.m3.1.1.3.cmml" xref="id8.3.3.m3.1.1.3"><times id="id8.3.3.m3.1.1.3.1.cmml" xref="id8.3.3.m3.1.1.3.1"></times><cn id="id8.3.3.m3.1.1.3.2.cmml" type="integer" xref="id8.3.3.m3.1.1.3.2">27</cn><ci id="id8.3.3.m3.1.1.3.3.cmml" xref="id8.3.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.3.3.m3.1c">44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="id8.3.3.m3.1d">44 italic_m ± 27 italic_m</annotation></semantics></math> außerhalb (von 400 <math alttext="m" class="ltx_Math" display="inline" id="id9.4.4.m4.1"><semantics id="id9.4.4.m4.1a"><mi id="id9.4.4.m4.1.1" xref="id9.4.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id9.4.4.m4.1b"><ci id="id9.4.4.m4.1.1.cmml" xref="id9.4.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id9.4.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="id9.4.4.m4.1d">italic_m</annotation></semantics></math> bis 1200 <math alttext="m" class="ltx_Math" display="inline" id="id10.5.5.m5.1"><semantics id="id10.5.5.m5.1a"><mi id="id10.5.5.m5.1.1" xref="id10.5.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id10.5.5.m5.1b"><ci id="id10.5.5.m5.1.1.cmml" xref="id10.5.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="id10.5.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="id10.5.5.m5.1d">italic_m</annotation></semantics></math>).
Die Hauptresultate zeigen erhebliche Fortschritte im maritimen Situationsbewusstsein, welche anhand von praktischen Beispielen der Anwendbarkeit der Methoden in realen Szenarien demonstriert werden. Zu diesen Beispielen gehören die Erkennung von abnormalem Schiffsverhalten, die Bewertung der Kameraintegrität als auch die 3D-Rekonstruktion von Schiffen. Der Ansatz übertrifft nicht nur bestehende Methoden in Bezug auf Genauigkeit und Laufzeit, sondern bietet auch ein Framework für die nahtlose Integration erkannter und georeferenzierter Schiffe in Echtzeitsysteme, wodurch die operative Effizienz und Entscheidungsfindung für maritime Behörden verbessert werden kann. Die Integration dieser Methoden in eingebettete Systeme stellt einen entscheidenden Fortschritt in diesem Anwendungsbereich dar und bietet eine skalierbare und effiziente Lösung zur Verbesserung des maritimen Situationsbewusstseins und der Reaktionsfähigkeiten.
Diese Dissertation trägt zum Bereich der maritimen Computer Vision bei, indem sie eine Benchmark für die Forschung zur Schiffssegmentierung und Georeferenzierung etabliert und die Machbarkeit von auf Deep-Learning basierenden Erkennungs- und Georeferenzierungsmethoden für die Echtzeitüberwachung maritimer Umgebungen demonstriert.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" lang="de">This doctoral thesis is a compilation of six articles that I authored and co-authored between 2021 and 2024. The research was conducted at the Institute for the Protection of Maritime Infrastructures of the German Aerospace Center (DLR), in Bremerhaven, Germany.</span></p>
</div>
<div class="ltx_logical-block" id="id11" lang="de">
<div class="ltx_para" id="id11.p1">
<p class="ltx_p ltx_align_center" id="id11.p1.1"><span class="ltx_text ltx_font_bold" id="id11.p1.1.1">Date of the Defense:</span> 09.08.2024</p>
<p class="ltx_p ltx_align_center" id="id11.p1.2"><span class="ltx_text ltx_font_bold" id="id11.p1.2.1">Examiners:</span></p>
<p class="ltx_p ltx_align_center" id="id11.p1.3">Prof. Dr.-Ing. Udo Frese (University of Bremen)</p>
<p class="ltx_p ltx_align_center" id="id11.p1.4">Prof. Dr.-Ing. Tobias Meisen (University of Wuppertal)</p>
<p class="ltx_p ltx_align_center" id="id11.p1.5"><span class="ltx_text ltx_font_bold" id="id11.p1.5.1">Further members of the examination committee:</span></p>
<p class="ltx_p ltx_align_center" id="id11.p1.6">Prof. Dr. Ralf Bachmayer (University of Bremen)</p>
<p class="ltx_p ltx_align_center" id="id11.p1.7">Dr. rer. nat. Enno Peters (German Aerospace Center)</p>
<p class="ltx_p ltx_align_center" id="id11.p1.8">Arne Hasselbring (University of Bremen)</p>
<p class="ltx_p ltx_align_center" id="id11.p1.9">Ayleen Lührsen (University of Bremen)</p>
</div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_bold" id="p3.1.1" lang="de">Declaration<span class="ltx_text ltx_font_medium" id="p3.1.1.1">
<br class="ltx_break"/></span></span></p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1"><span class="ltx_text" id="p4.1.1" lang="de">I, Borja Carrillo Perez, declare that this thesis, titled “Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness”, is my own work and has not been submitted for any other degree or qualification at this or any other institution. All sources are acknowledged and referenced.
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="p5.1.1" lang="de">Borja Carrillo Perez</span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1"><span class="ltx_text" id="p6.1.1" lang="de">June 2024</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_acknowledgements" lang="en">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This academic and personal achievement would not have been possible without the support I have received over these past years. I am profoundly grateful to everyone who has contributed.
Firstly, I extend my deepest thanks to Dr. Sarah Barnes and Dr. Maurice Stephan from the German Aerospace Center for their invaluable guidance, insightful discussions, commitment, and time. I feel very lucky to have had the opportunity to learn from them, and their mentorship has profoundly shaped my professional and personal growth.
I am also grateful to Prof. Dr.-Ing. Udo Frese from the University of Bremen for his guidance and advice, especially during the final stages of this work, which significantly elevated the quality of my research. I appreciate Prof. Dr.-Ing. Tobias Meisen from the University of Wuppertal for his time and willingness to review this thesis, and I thank the committee members for their participation in the defense.
I would like to thank Jens-Michael Schlüter and Michael Busack from the Alfred Wegener Institute for Polar and Marine Research, as well as Marco Gawehns and Tino Flenker from the German Aerospace Center, for their technical support in the data acquisition process. Additionally, I am grateful to Dr. Michael Stadermann and Dr. Frank Sill Torres for their support in the publication of the data.
My sincere thanks to my colleagues at the Institute for the Protection of Maritime Infrastructures of the German Aerospace Center for their incredible support, scientific contributions, and the joy they brought to our work environment. Special thanks go to Dr. Angel Bueno, Dr. Edgardo Solano, Felix Sattler and the Methods and Processing group members. I am also grateful to the Machine Learning group of MI and PI for their valuable input and discussions. And big thanks to those who read the articles and thesis and provided such helpful feedback.
I am grateful to the friends I made in Bremen along these years for their encouragement and support. They have always found kind words that made the journey lighter, even when it felt heavier.
Me gustaría agradecer a mis amigos de España por su constante apoyo.
Muchas gracias a Paloma, por haber sido mi apoyo durante años, por enseñarme tanto y por inspirarme querer alcanzar este objetivo.
Y por último a mi familia, por su amor, por hacerme quien soy y empujarme a luchar por superarme.

</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc" lang="en"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Fundamentals of Modern Object Recognition</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S1" title="In Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Supervised Learning in Computer Vision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2" title="In Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep-Learning-Based Object Recognition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS1" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Standard Architecture Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS2" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Attention Mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS3" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Object Classification and Postprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS4" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Training Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS5" title="In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.5 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Relevant State of the Art</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S1" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Real-world Maritime Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S2" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Ship Recognition Using Maritime Monitoring Footage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S3" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Georeferencing of Recognized Ships</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S4" title="In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Deployment on Embedded Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>ShipSG: Ship Segmentation and Georeferencing Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S1" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S2" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Acquisition and Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.S3" title="In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Ship Recognition for Improved Maritime Awareness</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Ship Detection for Maritime Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Detection of Abnormal Vessel Behaviour from Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS2" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Ship Detection for Integrity Assessment of Camera Obstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS3" title="In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Ship Detection for 3D Reconstruction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Standard Ship Segmentation Using ShipSG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S3" title="In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Advanced Ship Recognition for Real-time Operation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>The ScatBlock</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>ScatYOLOv8+CBAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S3" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Optimized ScatYOLOv8+CBAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S4" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Enhanced Small Ship Segmentation Using Higher Resolution Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S5" title="In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Ship Georeferencing for Maritime Situational Awareness</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S1" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Homographies for Image Georeferencing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Ship Detection and Georeferencing Using Homographies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S3" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Analysis of Ship Segmentation and Georeferencing Using Homographies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S4" title="In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Summary and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch8" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Summary and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch9" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Publications by the Author for this Thesis</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-I] </span><span class="ltx_text"> </span>Detection and Geovisualization of Abnormal Vessel Behavior from Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-II] </span><span class="ltx_text"> </span>Ship Segmentation and Georeferencing from Static Oblique View Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-III] </span><span class="ltx_text"> </span>Integrity Assessment of Maritime Object Detection Impacted by Partial Camera Obstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-IV] </span><span class="ltx_text"> </span>Embedded 3D reconstruction of Dynamic Objects in Real Time for Maritime Situational Awareness Pictures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-V] </span><span class="ltx_text"> </span>Improving YOLOv8 with Scattering Transform and Attention for Maritime Awareness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="In Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">[BCP-VI] </span><span class="ltx_text"> </span>Enhanced Small Ship Segmentation with Optimized ScatYOLOv8+CBAM on Embedded Systems</span></a></li>
</ol>
</li>
</ol></nav>
<nav class="ltx_TOC ltx_list_lof ltx_toc_lof" lang="en"><h6 class="ltx_title ltx_title_contents">List of Figures</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1.F1" title="Figure 1.1In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Conceptual representation of ship detection, segmentation and georeferencing from maritime footage.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F1" title="Figure 2.1In Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Example of object detection and instance segmentation on an image.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F2" title="Figure 2.2In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Standard deep learning object recognition architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F3" title="Figure 2.3In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Illustration of a standard convolution operation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F4" title="Figure 2.4In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Illustration of max pooling and average pooling operations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F5" title="Figure 2.5In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Joint illustration of Intersection over Union calculation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F6" title="Figure 2.6In 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Schematic of the Convolutional Neural Network (CNN) Training Process.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F1" title="Figure 4.1In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Visualisation of ShipSG dataset samples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F2" title="Figure 4.2In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>View of each camera and identification of important elements in the scene.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F3" title="Figure 4.3In Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Examples extracted from the dataset that show the seven ship classes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F1" title="Figure 5.1In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Framework proposed in <span class="ltx_text ltx_ref_tag">[BCP-I]</span> for maritime anomaly detection from video.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F2" title="Figure 5.2In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Inference of the YOLOv4-CSP based vessel detector.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F3" title="Figure 5.3In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Examples of a ship with different synthetic partial obstruction profiles.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F4" title="Figure 5.4In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Framework proposed in <span class="ltx_text ltx_ref_tag">[BCP-IV]</span> for 3D reconstruction of ships using synthetic stereo images.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F5" title="Figure 5.5In 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Object detection example for 3D reconstruction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F6" title="Figure 5.6In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Annotated masks on existing datasets to study the generalization of our models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F1" title="Figure 6.1In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Examples of commonly used 2D wavelets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F2" title="Figure 6.2In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Scattering coefficient decomposition of an image.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F3" title="Figure 6.3In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>ScatBlock as conceived in <span class="ltx_text ltx_ref_tag">[BCP-V]</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F4" title="Figure 6.4In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>CBAM as conceived in <span class="ltx_text ltx_ref_tag">[BCP-V]</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F5" title="Figure 6.5In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Diagram of each attention sub-module in <span class="ltx_glossaryref" title="">CBAM</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F7" title="Figure 6.7In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Visualization of 2D Wavelet Filters used in the ScatBlock</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F8" title="Figure 6.8In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.8 </span>Instance segmentation process of ScatYOLOv8+CBAM on ShipSG.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F9" title="Figure 6.9In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.9 </span>mAP vs TensorRT inference times on NVIDIA Jetson AGX Xavier, showing the improved performance speed.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F10" title="Figure 6.10In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.10 </span>Small ship segmentation on ShipSG.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F11" title="Figure 6.11In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.11 </span>mAP vs TensorRT inference times on NVIDIA Jetson AGX Xavier of ScatYOLOv8+CBAM with SAHI.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F1" title="Figure 7.1In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Homography between two planes.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F2" title="Figure 7.2In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Representation of the two planes used to create the homography of publication <span class="ltx_text ltx_ref_tag">[BCP-I]</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F3" title="Figure 7.3In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Illustrative representation of detected and georeferenced vessel with heading.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F4" title="Figure 7.4In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Visualization of detected and georeferenced vessels with heading.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F5" title="Figure 7.5In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Example of segmented ship mask with calculated pixel to be georeferenced.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F6" title="Figure 7.6In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Georeferencing distance error per ship length.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F7" title="Figure 7.7In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Segmented and georeferenced ships using ScatYOLOv8+CBAM and homographies to improve maritime awareness.</span></a></li>
</ol></nav>
<nav class="ltx_TOC ltx_list_lot ltx_toc_lot" lang="en"><h6 class="ltx_title ltx_title_contents">List of Tables</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.T1" title="Table 3.1In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Ship georeferencing accuracy in existing literature.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.T2" title="Table 3.2In Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Comparison of NVIDIA GPU modules, with focus on the Jetson family and high-end GPU-powered systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T1" title="Table 5.1In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Configurations during training for each instance segmentation method evaluated in <span class="ltx_text ltx_ref_tag">[BCP-II]</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T2" title="Table 5.2In Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Resulting instance segmentation APs and inference speed per initial method evaluated.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T1" title="Table 6.1In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Comparison of state-of-the-art segmentation performances on ShipSG with YOLOv8n and ScatYOLOv8n+CBAM.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T2" title="Table 6.2In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Ablation study of YOLOv8 segmentation models and ScatYOLOv8+CBAM additions.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T3" title="Table 6.3In Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Comparison of <span class="ltx_glossaryref" title="">mAP</span> scores for small objects with all model sizes using standard YOLOv8, our proposed optimized ScatYOLOv8+CBAM, and the addition of <span class="ltx_glossaryref" title="">SAHI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.T1" title="Table 7.1In Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Comparison of the proposed method for ship georeferencing accuracy with existing works.</span></a></li>
</ol></nav>
<section class="ltx_glossary ltx_list_acronym" id="glo.acronym" lang="en">
<h2 class="ltx_title ltx_title_glossary">Acronyms</h2>
<dl class="ltx_glossarylist">
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.ai">AI</dt>
<dd>Artificial Intelligence</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.ais">AIS</dt>
<dd>Automatic Identification System</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.arm">ARM</dt>
<dd>Advanced Reduced instruction set computer Machine</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.cbam">CBAM</dt>
<dd>Convolutional Block Attention Module</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.cnn">CNN</dt>
<dd>Convolutional Neural Network</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.csp">CSP</dt>
<dd>Cross Stage Partial</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.cuda">CUDA</dt>
<dd>Compute Unified Device Architecture</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.dlr">DLR</dt>
<dd>German Aerospace Center</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.dlt">DLT</dt>
<dd>Direct Linear Transformation</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.dtcwt">DTCWT</dt>
<dd>Dual-Tree Complex Wavelet Transform</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.gan">GAN</dt>
<dd>Generative Adversarial Network</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.gde">GDE</dt>
<dd>Georeferencing Distance Error</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.gis">GIS</dt>
<dd>Geographic Information System</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.gpu">GPU</dt>
<dd>Graphics Processing Unit</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.h">H</dt>
<dd>Homography</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.imo">IMO</dt>
<dd>International Maritime Organization</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.iou">IoU</dt>
<dd>Intersection over Union</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.ls">LS</dt>
<dd>Least Squares</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.map">mAP</dt>
<dd>mean Average Precision</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.mlp">MLP</dt>
<dd>Multi-layer Perceptron</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.nlp">NLP</dt>
<dd>Natural Language Processing</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.nms">NMS</dt>
<dd>Non-Maximum Suppression</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.onnx">ONNX</dt>
<dd>Open Neural Network Exchange</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.ransac">RANSAC</dt>
<dd>Random Sample Consensus</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.relu">ReLU</dt>
<dd>Rectified Linear Unit</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.sahi">SAHI</dt>
<dd>Slicing Aided Hyper Inference</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.silu">SiLU</dt>
<dd>Sigmoid-Weighted Linear Unit</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.smd">SMD</dt>
<dd>Singapore Maritime Dataset</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.utm">UTM</dt>
<dd>Universal Transverse Mercator</dd>
<dt class="ltx_glossaryentry ltx_list_acronym" id="glo.acronym.vts">VTS</dt>
<dd>Vessel Traffic Services</dd>
</dl>
</section>
<section class="ltx_chapter" id="Ch1" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 1 </span>Introduction</h2>
<div class="ltx_para" id="Ch1.p1">
<p class="ltx_p" id="Ch1.p1.1">Maritime infrastructures are an essential component towards the support of societal needs, economic activities, mobility, and the advancement of renewable energy sources <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib1" title="">engler2018resiliencen </a></cite>. This highlights the reason why their security, integrity, and operational safety are crucial. In response, maritime research is aiming at developing, testing, and validating systems to thoroughly assess and operate these infrastructures <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib2" title="">torres2020indicator </a></cite>.
Such initiatives aim to cultivate a proactive and informed understanding of maritime contexts, essential for accurately determining the protection status of infrastructures in real-time and enabling prompt action against various threats, including major accidents, natural disasters, and organized crime <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib1" title="">engler2018resiliencen </a></cite>. Maritime situational awareness, facilitated by advanced technologies and data integration, is critical for a proactive and informed understanding of maritime environments <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib3" title="">cetin2013increasing </a></cite>. It encompasses real-time monitoring and drives innovative solutions to enhance the security, safety, structural integrity, and operational reliability of infrastructures against various threats <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib4" title="">ventikos2022risk </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p2">
<p class="ltx_p" id="Ch1.p2.1">In the improvement of maritime situational awareness the introduction of advanced instruments and sensors plays a key role, which should be designed not only to recognize elements of interest but also to suggest practical measures to both users, for operational decisions, and authorities, for regulatory compliance and emergency response <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib5" title="">wang2019maritime </a></cite>. Enhancing maritime situational awareness with technology represents a significant advancement for smart ports, exemplifying the potential for improved maritime operations <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib6" title="">belmoukari2023smart </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p3">
<p class="ltx_p" id="Ch1.p3.1">The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.imo"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.imo" title="International Maritime Organization">International Maritime Organization (IMO)</span></a> mandates that vessels exceeding 300 gross tonnage are equipped with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">Automatic Identification System (AIS)</span></a> transceivers, which broadcast crucial data including identification numbers, type, position, course, and speed through encoded radio messages <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib7" title="">imo2015resolutionA110629 </a></cite>. This system is pivotal for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.vts"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.vts" title="Vessel Traffic Services">Vessel Traffic Services (VTS)</span></a> and nearby vessels, facilitating marine traffic awareness, critical operations such as collision avoidance, and search and rescue missions. <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> transmissions occur at intervals ranging from 2 to 10 seconds when ships are underway, which can be extended by up to 6 minutes when stationary<span class="ltx_note ltx_role_footnote" id="Ch1.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.navcen.uscg.gov/ais-messages" title="">https://www.navcen.uscg.gov/ais-messages</a></span></span></span>. Such intervals may leave gaps in real-time monitoring, highlighting the need for systems capable of analyzing situations with a significantly shorter interval to aid in the prevention and response to potential maritime complications <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib8" title="">kim2019adaptive </a></cite>. Furthermore, the open standards employed by the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> exposes it to various cyber threats, including spoofing, hijacking attacks, and denial of service, underscoring the vulnerability of the system <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib9" title="">jakovlev2020analysis </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib10" title="">struck2021backwards </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib11" title="">wimpenny2018public </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib12" title="">alincourt2016methodology </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib13" title="">balduzzi2014security </a></cite>. Therefore, despite significant efforts, real-time ship monitoring for improved maritime situational awareness only using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> continues to pose a challenge for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.vts"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.vts" title="Vessel Traffic Services">VTS</span></a> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib14" title="">yan2020exploring </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p4">
<p class="ltx_p" id="Ch1.p4.1">Other available sources for the improvement of maritime situational awareness are satellite imagery and radar systems <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib15" title="">reggiannini2024remote </a></cite>. However, their processing for real-time maritime situational awareness faces challenges due to the time-sensitive nature of data acquisition, periodic satellite overpasses, and processing delays (<math alttext="\sim" class="ltx_Math" display="inline" id="Ch1.p4.1.m1.1"><semantics id="Ch1.p4.1.m1.1a"><mo id="Ch1.p4.1.m1.1.1" xref="Ch1.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Ch1.p4.1.m1.1b"><csymbol cd="latexml" id="Ch1.p4.1.m1.1.1.cmml" xref="Ch1.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch1.p4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="Ch1.p4.1.m1.1d">∼</annotation></semantics></math>15 minutes per data cycle), impacting the immediacy and utility of the information <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib16" title="">schwarz2015near </a></cite>. Moreover, revisit times of satellites can range from hours to days.</p>
</div>
<div class="ltx_para" id="Ch1.p5">
<p class="ltx_p" id="Ch1.p5.1">Optical camera systems, on the other hand, due to their accessibility, cost-efficiency, and ease of deployment, are key in rapidly assessing ship traffic, enhancing maritime situational awareness through views of the infrastructure from strategic positions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite>. The vast number of video streams available can present a challenge for operators <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib18" title="">li2020causal </a></cite>.
The efficiency of real-time recognition is significantly boosted by image processing technologies applied to optical monitoring <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite>.
This motivates the use of computer vision and deep learning to automatically recognize and locate geographically (georeference) ships on a map, irrespective of their type or size. This process can support the operational decision-making procedures of maritime authorities by providing them with spatial information in a timely manner <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p6">
<p class="ltx_p" id="Ch1.p6.1">Early detection of potential threats and the prevention of accidents are significantly enhanced by employing optical cameras when used at full-resolution, ensuring detailed and precise imagery for monitoring purposes <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib20" title="">chen2020deep </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib21" title="">rekavandi2022guide </a></cite>. Therefore, a key challenge in maritime monitoring is the recognition of small and distant ships, which is crucial for safety and security at maritime infrastructures, as it helps in early threat detection and accident prevention <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib20" title="">chen2020deep </a></cite>.</p>
</div>
<figure class="ltx_figure" id="Ch1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="399" id="Ch1.F1.g1" src="extracted/5906916/fig/motivation_mask_georef.png" width="550"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch1.F1.2.1.1" style="font-size:90%;">Figure 1.1</span>: </span><span class="ltx_text" id="Ch1.F1.3.2" style="font-size:90%;">Conceptual representation of ship detection, segmentation and georeferencing from maritime footage. (a) Tanker being detected. Point 1 represents the bounding box center, and 2, the bottom-center point. (b) Tanker being segmented. Point 3 represents the intersection of the navigation antenna with the water. (c) Representation of the goereferenced tanker displayed on a map. The georeference from the mask, 3, provides the most accurate ship location of the three points.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch1.p7">
<p class="ltx_p" id="Ch1.p7.1">Object georeferencing involves linking physical objects to specific locations on the Earth’s surface for spatial integration and analysis, a process that can extend to objects captured within an image <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib22" title="">Hastings2009 </a></cite>.
For the improvement of maritime situational awareness, once ships are recognized in real-time from monitoring images, the display of their geographical positions on maps using georeferencing enables a better spatial understanding of the situation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p8">
<p class="ltx_p" id="Ch1.p8.1">Ship detection methods which use monitoring footage to present a bounding box surrounding the detected ship, can be used to improve maritime situational awareness <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib23" title="">wawrzyniak2019vessel </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib24" title="">helgesen2020low </a></cite>. However, ship segmentation provides a more accurate georeference for the ships using the segmented masks, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1.F1" title="Figure 1.1 ‣ Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1.1</span></a>. The georeferenced pixel can be better inferred from the segmented mask of an object than from a surrounding bounding box, which usually contains unnecessary background. The center and bottom-center of the bounding box, given the perspective of the image, provide a more erroneous georeference compared to the point that lies at the intersection point between the ship hull and the water below the bridge or wheelhouse, where the navigation antenna is located. This rationale motivates the exploration of ship segmentation methods beyond bounding box detection, and paves the way for the development of a method to automatically identify the pixel for georeferencing within this thesis.</p>
</div>
<div class="ltx_para" id="Ch1.p9">
<p class="ltx_p" id="Ch1.p9.1">Furthermore, the processing using embedded devices powered with a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">Graphics Processing Unit (GPU)</span></a>, equipped with monitoring cameras and placed at maritime infrastructures, represents a step forward in maritime situational awareness <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib25" title="">mittal2019survey </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib26" title="">zhao2019embedded </a></cite>. These systems can enable on-site deep-learning-based ship recognition, offering significant advantages such as reduced network bandwidth and energy usage, minimized latency, and enhanced security <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib27" title="">ning2020heterogeneous </a></cite>.
The local processing of images using embedded systems directly at the infrastructure facilitates the spatial understanding of the maritime situation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib28" title="">sattler2023maritime </a></cite>.
Recognized and georeferenced ships using an embedded system can then be seamlessly integrated into web services, allowing their display (e.g. on maps) within the situational awareness system <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>. This enhances real-time visualization and enriches the overall situational awareness by providing operators with accurate and timely spatial information <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p10">
<p class="ltx_p" id="Ch1.p10.1">Real-time and accurate ship recognition, classification, and georeferencing are essential, not just for improved spatial visualization. Beyond visualization, its combination with other data sources, such as <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>, satellite imagery and radar systems can further enhance the overall situational understanding <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>. Therefore, the faster the image processing occurs, the better it supports the creation of a comprehensive real-time situational picture by fusing with additional maritime data, thereby elevating the operational effectiveness of maritime situational awareness efforts <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p11">
<p class="ltx_p" id="Ch1.p11.1">We have seen, that in the context of enhancing maritime situational awareness, optimized real-time processing is paramount. The objective, therefore should be to ensure that the developed ship recognition and georeferencing system operates with the highest possible accuracy and the shortest inference times on embedded systems. This dual focus on speed and accuracy is critical for facilitating the fusion of the developed methodologies with other sensor data and services, thereby enabling safer, more secure, and more efficient maritime operations.</p>
</div>
<div class="ltx_para" id="Ch1.p12">
<p class="ltx_p" id="Ch1.p12.1">This thesis presents a compilation of explorations, methods and results proposed in the publications shown in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10" title="Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">10</span></a>, which will be referenced throughout the manuscript from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>. The goals and contributions of this thesis, achieved within these publications, are summarized as follows:</p>
</div>
<div class="ltx_para" id="Ch1.p13">
<ul class="ltx_itemize" id="Ch1.S0.I1">
<li class="ltx_item" id="Ch1.S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i1.p1">
<p class="ltx_p" id="Ch1.S0.I1.i1.p1.1">Production of a real-world maritime dataset for ship recognition and georeferencing, advancing the research field of maritime situational awareness.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i1.p2">
<p class="ltx_p" id="Ch1.S0.I1.i1.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i1.p2.1.1">Contribution:</span> Creation and publication of the ShipSG dataset, which provides a comprehensive set of annotated images for ship recognition and georeferencing <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i2.p1">
<p class="ltx_p" id="Ch1.S0.I1.i2.p1.1">Investigation and development of a ship recognition architecture that seeks for enhanced real-time ship recognition, able to run in real-time embedded systems.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i2.p2">
<p class="ltx_p" id="Ch1.S0.I1.i2.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i2.p2.1.1">Contribution:</span> In-depth study of state-of-the-art methods for ship recognition, using ShipSG and other datasets <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i2.p3">
<p class="ltx_p" id="Ch1.S0.I1.i2.p3.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i2.p3.1.1">Contribution:</span> Introduced ScatYOLOv8+CBAM, an innovative ship recognition architecture optimized for real-time processing on embedded systems <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i3.p1">
<p class="ltx_p" id="Ch1.S0.I1.i3.p1.1">Proposal of an efficient solution for processing full-resolution images on embedded systems, allowing the recognition of small and distant ships.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i3.p2">
<p class="ltx_p" id="Ch1.S0.I1.i3.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i3.p2.1.1">Contribution:</span> Introduced an improved slicing method that enables the processing of full-resolution images for the recognition of small and distant ships on embedded systems <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i4.p1">
<p class="ltx_p" id="Ch1.S0.I1.i4.p1.1">Innovation in the field of ship georeferencing using monocular images by developing a methodology that does not rely on prior camera knowledge.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i4.p2">
<p class="ltx_p" id="Ch1.S0.I1.i4.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i4.p2.1.1">Contribution:</span> Developed a novel ship georeferencing methodology using homographies that operates without requiring prior camera calibration <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i5.p1">
<p class="ltx_p" id="Ch1.S0.I1.i5.p1.1">Optimization of real-time ship recognition and georeferencing methodologies for their deployment on embedded systems, balancing performance with computational efficiency.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i5.p2">
<p class="ltx_p" id="Ch1.S0.I1.i5.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i5.p2.1.1">Contribution:</span> Further improvement of the ScatYOLOv8+CBAM architecture for efficient deployment on embedded systems, balancing computational efficiency with high performance <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S0.I1.i6.p1">
<p class="ltx_p" id="Ch1.S0.I1.i6.p1.1">Demonstration of the practical application of the methodologies by integrating them into systems for improved maritime situational awareness in a variety of applications.</p>
</div>
<div class="ltx_para" id="Ch1.S0.I1.i6.p2">
<p class="ltx_p" id="Ch1.S0.I1.i6.p2.1"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.i6.p2.1.1">Contribution:</span> Successfully integrated the developed methodologies into applications such as ship georeferencing displays including map-based visualization, abnormal ship behavior detection, camera integrity assessment, and 3D ship reconstruction, showcasing their effectiveness in enhancing maritime situational awareness <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch1.p14">
<p class="ltx_p" id="Ch1.p14.1">The remaining chapters of this thesis are organized as follows:</p>
</div>
<div class="ltx_para" id="Ch1.p15">
<dl class="ltx_description" id="Ch1.S0.I2">
<dt class="ltx_item" id="Ch1.S0.I2.ix1"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch2" title="Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix1.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch2" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Fundamentals of Modern Object Recognition</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix1.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix1.p1.1">This chapter dives into the technical background of modern object recognition, introducing key concepts. It explores how deep learning has transformed computer vision for object recognition and how it can be leveraged.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix2"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch3" title="Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix2.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Relevant State of the Art</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix2.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix2.p1.1">This chapter focuses on relevant state-of-the-art, highlighting areas in ship recognition and georeferencing where current research falls short. The chapter presents maritime datasets and object recognition methods essential for this thesis as well as potential improvements, prior ship georeferencing research and deployment on embedded systems.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix3"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix3.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">ShipSG: Ship Segmentation and Georeferencing Dataset</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix3.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix3.p1.1">This chapter presents the creation of ShipSG<span class="ltx_note ltx_role_footnote" id="Ch1.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dlr.de/mi/shipsg" title="">https://dlr.de/mi/shipsg</a></span></span></span>, a novel dataset for ship recognition and georeferencing. ShipSG provides the foundation for this thesis, enabling the development and evaluation of the methods presented in the subsequent chapters.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix4"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix4.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Ship Recognition for Improved Maritime Awareness</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix4.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix4.p1.1">This chapter shows the initial exploration of deep-learning-based methods for ship detection and segmentation, revealing their potential applications, such as ship georeferencing, abnormal ship behavior detection,
camera integrity assessment, and 3D ship reconstruction. The study of state-of-the-art instance segmentation methods sets the stage for the custom developments and analysis proposed in subsequent chapters.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix5"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix5.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Advanced Ship Recognition for Real-time Operation</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix5.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix5.p1.1">This chapter addresses the need for fast and accurate algorithms on embedded systems for real-world use. While ship detection was proven to perform well, deploying instance segmentation (better for ship georeferencing) on embedded systems was shown to be a significant challenge. This chapter addresses this gap by proposing a customized real-time segmentation method (ScatYOLOv8+CBAM), deployed on an embedded system. It also proposes a method to improve the segmentation accuracy for small and distant ships by processing full-resolution images, crucial for better maritime situational awareness.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix6"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix6.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Ship Georeferencing for Maritime Situational Awareness</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix6.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix6.p1.1">This chapter focuses on the georeferencing of the ships recognized using monocular cameras to improve maritime situational awareness. This involves the development of a method to present ships on a global scale using only single images without prior camera knowledge. The chapter first explains homographies and then details the proposed method for georeferencing ship bounding boxes, along with the calculation of ship heading direction from optical flow. Finally, the chapter quantitatively analyses how this monocular ship georeferencing improves maritime situational awareness.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix7"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch8" title="Chapter 8 Summary and Conclusion ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix7.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch8" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Summary and Conclusion</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix7.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix7.p1.1">This chapter summarizes the contributions and key findings of this thesis, and concludes the outcome of the produced results.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix8"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch9" title="Chapter 9 Future Work ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix8.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch9" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Future Work</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix8.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix8.p1.1">This chapter presents the challenges encountered throughout the thesis and proposes new research lines to approach future work.</p>
</div>
</dd>
<dt class="ltx_item" id="Ch1.S0.I2.ix9"><span class="ltx_tag ltx_tag_item"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#Ch10" title="Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text ltx_font_bold" id="Ch1.S0.I2.ix9.1.1.1"> <a class="ltx_ref ltx_refmacro_nameref" href="https://arxiv.org/html/2410.04946v1#Ch10" title="In Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_title">Publications by the Author for this Thesis</span></a></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="Ch1.S0.I2.ix9.p1">
<p class="ltx_p" id="Ch1.S0.I2.ix9.p1.1">This chapter presents the list of publications used in this compilation thesis and includes a short summary of my contributions to each publication.</p>
</div>
</dd>
</dl>
</div>
</section>
<section class="ltx_chapter" id="Ch2" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 2 </span>Fundamentals of Modern Object Recognition</h2>
<div class="ltx_para" id="Ch2.p1">
<p class="ltx_p" id="Ch2.p1.1">As motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, the combination of computer vision and deep learning offers a potent solution for automatic ship recognition using optical monitoring cameras. This chapter provides a technical overview of concepts to understand modern object recognition, starting with the role of supervised learning in computer vision, and followed by the use of deep learning for the two computer vision tasks of interest in this thesis, object detection and instance segmentation.</p>
</div>
<section class="ltx_section" id="Ch2.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">2.1   Supervised Learning in Computer Vision</h3>
<div class="ltx_para" id="Ch2.S1.p1">
<p class="ltx_p" id="Ch2.S1.p1.1">Computer vision is a discipline within <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai" title="Artificial Intelligence">Artificial Intelligence (AI)</span></a> that allows machines to process and interpret visual data.
By harnessing algorithms and data, computer vision systems can identify and classify objects, and make decisions based on visual inputs similar to the way humans do <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
The field of computer vision has significantly advanced with deep learning, a subfield of machine learning, particularly through the use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">Convolutional Neural Networks (CNNs)</span></a> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
Preceding computer vision approaches relied on hand-engineered feature extraction. Deep learning, on the other hand, utilizes vast amounts of visual data to train hierarchical structures of neurons that excel at identifying patterns to therefore perform automatic feature extraction <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib30" title="">goodfellow2016deep </a></cite>. Thanks to the use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPUs</span></a>, deep learning with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a> have significantly surpassed the performance of traditional algorithms in tasks like image classification, object detection, and instance and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib31" title="">talaei2023deep </a></cite>. The computational power of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPUs</span></a>, due to the parallel processing capabilities, enabled the training of deep networks with millions of parameters, allowing for the extraction of complex features from large datasets.</p>
</div>
<div class="ltx_para" id="Ch2.S1.p2">
<p class="ltx_p" id="Ch2.S1.p2.1">Machine learning algorithms are commonly categorized into supervised learning, which requires labeled data for training, unsupervised learning, which operates on unlabeled data to find patterns, and semi-supervised learning, which uses a combination of labeled and unlabeled data to train models <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib32" title="">sammut2011encyclopedia </a></cite>.
In certain real-world applications, supervised learning is preferred for training models with real-world annotated datasets, ensuring accurate identification and categorization of objects represented in the data <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib33" title="">chai2021deep </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S1.p3">
<p class="ltx_p" id="Ch2.S1.p3.1">In supervised learning, models are trained on datasets labeled by human experts <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib34" title="">Cunningham2008 </a></cite>. During training, the supervised model adjusts its parameters by measuring the deviation from the actual labels <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib32" title="">sammut2011encyclopedia </a></cite>. Therefore, the annotation process involves pairing each training sample with its corresponding output labels, serving as a learning guide for the model.
In computer vision tasks, such as image classification, labeled training images are used to predict classes on validation images <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib35" title="">rawat2017deep </a></cite>. In object detection tasks, the annotations and training extends for the classification and localization of objects in the image within bounding boxes. Segmentation tasks demand detailed annotations, labeling each pixel by class <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib36" title="">zaidi2022survey </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S1.p4">
<p class="ltx_p" id="Ch2.S1.p4.1">In summary, supervised learning has greatly advanced computer vision tasks, while also highlighting the continuous need for models that can learn effectively from annotated data.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">2.2   Deep-Learning-Based Object Recognition</h3>
<div class="ltx_para" id="Ch2.S2.p1">
<p class="ltx_p" id="Ch2.S2.p1.1">Object recognition in computer vision involves identifying and classifying objects in images <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>. Two main tasks in the field are object detection and instance segmentation, which are essential for machine interpretation of visual data and widely used in autonomous driving, monitoring, surveillance and medical imaging applications, among others <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib33" title="">chai2021deep </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib36" title="">zaidi2022survey </a></cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F1" title="Figure 2.1 ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.1</span></a> depicts the difference between object detection and instance segmentation.</p>
</div>
<figure class="ltx_figure" id="Ch2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="152" id="Ch2.F1.g1" src="extracted/5906916/fig/od_is.jpg" width="393"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F1.2.1.1" style="font-size:90%;">Figure 2.1</span>: </span><span class="ltx_text" id="Ch2.F1.3.2" style="font-size:90%;">Example of object detection and instance segmentation on an image. Object detection involves bounding box localization and classification, whereas instance segmentation goes beyond that to provide a mask outlining the exact shape of each individual object instance. Adapted from <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib37" title="">shanmugamani2018deep </a></cite>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.p2">
<p class="ltx_p" id="Ch2.S2.p2.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.p2.1.1">Object Detection</span> aims to locate and classify objects within an image, including the determination of their presence and exact location within bounding boxes.</p>
</div>
<div class="ltx_para" id="Ch2.S2.p3">
<p class="ltx_p" id="Ch2.S2.p3.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.p3.1.1">Instance Segmentation</span> advances beyond detection with bounding box by identifying each object instance in an image at the pixel level, delineating its shape with a mask. Unlike semantic segmentation, which classifies each pixel within the image as belonging to a certain class, instance segmentation recognizes each object instance separately and the rest is considered background.</p>
</div>
<section class="ltx_subsection" id="Ch2.S2.SS1">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">2.2.1   Standard Architecture Description</h4>
<div class="ltx_para" id="Ch2.S2.SS1.p1">
<p class="ltx_p" id="Ch2.S2.SS1.p1.1">Modern deep learning architectures for detection and segmentation tasks extensively use <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a>, featuring a combination of a backbone, a neck and head structure <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.</p>
</div>
<figure class="ltx_figure" id="Ch2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="207" id="Ch2.F2.g1" src="extracted/5906916/fig/standard_architecture.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F2.2.1.1" style="font-size:90%;">Figure 2.2</span>: </span><span class="ltx_text" id="Ch2.F2.3.2" style="font-size:90%;">Standard deep learning object recognition architecture. See text for details.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.SS1.p2">
<p class="ltx_p" id="Ch2.S2.SS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F2" title="Figure 2.2 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.2</span></a> illustrates a standard deep learning object recognition architecture. In <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a>-like architectures, a feature map is the output of one filter (also known as kernel) applied across the previous layer to detect specific features <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>. In the case of an object recognition architecture, the backbone focuses on extracting features by learning to recognize task-relevant patterns in visual data, performing changes in feature map resolution (width, height and channel number).
The arrows in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F2" title="Figure 2.2 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.2</span></a> represent the flow of data through the network layers.
As data progresses through the backbone, the resolution typically decreases to reduce the spatial dimensions while increasing the depth (number of channels) to create more abstract and complex feature representations <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
Conversely, in the neck, the resolution can either decrease or increase. Typically, in object detection and instance segmentation tasks, the neck often includes upsampling to increase spatial information, allowing a more accurate location of objects of interest <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
The neck fuses and aggregates features from different resolutions, acting as a bridge between the backbone and the head. The head performs specific tasks based on these features, such as detection, segmentation, and classification. Although they are different tasks, detection and segmentation share similarities in terms their architecture, with each head designed to perform the desired task. However, the design of backbone and neck can be tailored to perform better for the task of interest.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p3">
<p class="ltx_p" id="Ch2.S2.SS1.p3.1">As illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F2" title="Figure 2.2 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.2</span></a>, deep learning architectures for object recognition that use  <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a> typically comprise blocks that represent combinations of structured layers to process the visual data. These blocks include include a combination of convolutional, pooling, upsampling, activation and regularization layers <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib38" title="">lecun2015deep </a></cite>.</p>
</div>
<figure class="ltx_figure" id="Ch2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="178" id="Ch2.F3.g1" src="extracted/5906916/fig/convolutions.jpg" width="510"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F3.6.5.1" style="font-size:90%;">Figure 2.3</span>: </span><span class="ltx_text" id="Ch2.F3.4.4" style="font-size:90%;">Illustration of a standard convolution operation, taken from <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib39" title="">zhang2020lightweight </a></cite>. The input volume has dimensions <math alttext="H\times W\times M" class="ltx_Math" display="inline" id="Ch2.F3.1.1.m1.1"><semantics id="Ch2.F3.1.1.m1.1c"><mrow id="Ch2.F3.1.1.m1.1.1" xref="Ch2.F3.1.1.m1.1.1.cmml"><mi id="Ch2.F3.1.1.m1.1.1.2" xref="Ch2.F3.1.1.m1.1.1.2.cmml">H</mi><mo id="Ch2.F3.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.1.1.m1.1.1.1.cmml">×</mo><mi id="Ch2.F3.1.1.m1.1.1.3" xref="Ch2.F3.1.1.m1.1.1.3.cmml">W</mi><mo id="Ch2.F3.1.1.m1.1.1.1c" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.1.1.m1.1.1.1.cmml">×</mo><mi id="Ch2.F3.1.1.m1.1.1.4" xref="Ch2.F3.1.1.m1.1.1.4.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F3.1.1.m1.1d"><apply id="Ch2.F3.1.1.m1.1.1.cmml" xref="Ch2.F3.1.1.m1.1.1"><times id="Ch2.F3.1.1.m1.1.1.1.cmml" xref="Ch2.F3.1.1.m1.1.1.1"></times><ci id="Ch2.F3.1.1.m1.1.1.2.cmml" xref="Ch2.F3.1.1.m1.1.1.2">𝐻</ci><ci id="Ch2.F3.1.1.m1.1.1.3.cmml" xref="Ch2.F3.1.1.m1.1.1.3">𝑊</ci><ci id="Ch2.F3.1.1.m1.1.1.4.cmml" xref="Ch2.F3.1.1.m1.1.1.4">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F3.1.1.m1.1e">H\times W\times M</annotation><annotation encoding="application/x-llamapun" id="Ch2.F3.1.1.m1.1f">italic_H × italic_W × italic_M</annotation></semantics></math> (height, width, and number of channels). A filter, also named kernel, of size <math alttext="K\times K\times M" class="ltx_Math" display="inline" id="Ch2.F3.2.2.m2.1"><semantics id="Ch2.F3.2.2.m2.1c"><mrow id="Ch2.F3.2.2.m2.1.1" xref="Ch2.F3.2.2.m2.1.1.cmml"><mi id="Ch2.F3.2.2.m2.1.1.2" xref="Ch2.F3.2.2.m2.1.1.2.cmml">K</mi><mo id="Ch2.F3.2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.2.2.m2.1.1.1.cmml">×</mo><mi id="Ch2.F3.2.2.m2.1.1.3" xref="Ch2.F3.2.2.m2.1.1.3.cmml">K</mi><mo id="Ch2.F3.2.2.m2.1.1.1c" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.2.2.m2.1.1.1.cmml">×</mo><mi id="Ch2.F3.2.2.m2.1.1.4" xref="Ch2.F3.2.2.m2.1.1.4.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F3.2.2.m2.1d"><apply id="Ch2.F3.2.2.m2.1.1.cmml" xref="Ch2.F3.2.2.m2.1.1"><times id="Ch2.F3.2.2.m2.1.1.1.cmml" xref="Ch2.F3.2.2.m2.1.1.1"></times><ci id="Ch2.F3.2.2.m2.1.1.2.cmml" xref="Ch2.F3.2.2.m2.1.1.2">𝐾</ci><ci id="Ch2.F3.2.2.m2.1.1.3.cmml" xref="Ch2.F3.2.2.m2.1.1.3">𝐾</ci><ci id="Ch2.F3.2.2.m2.1.1.4.cmml" xref="Ch2.F3.2.2.m2.1.1.4">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F3.2.2.m2.1e">K\times K\times M</annotation><annotation encoding="application/x-llamapun" id="Ch2.F3.2.2.m2.1f">italic_K × italic_K × italic_M</annotation></semantics></math> is convolved with the input, producing an output volume of dimensions <math alttext="H\times W\times N" class="ltx_Math" display="inline" id="Ch2.F3.3.3.m3.1"><semantics id="Ch2.F3.3.3.m3.1c"><mrow id="Ch2.F3.3.3.m3.1.1" xref="Ch2.F3.3.3.m3.1.1.cmml"><mi id="Ch2.F3.3.3.m3.1.1.2" xref="Ch2.F3.3.3.m3.1.1.2.cmml">H</mi><mo id="Ch2.F3.3.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.3.3.m3.1.1.1.cmml">×</mo><mi id="Ch2.F3.3.3.m3.1.1.3" xref="Ch2.F3.3.3.m3.1.1.3.cmml">W</mi><mo id="Ch2.F3.3.3.m3.1.1.1c" lspace="0.222em" rspace="0.222em" xref="Ch2.F3.3.3.m3.1.1.1.cmml">×</mo><mi id="Ch2.F3.3.3.m3.1.1.4" xref="Ch2.F3.3.3.m3.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F3.3.3.m3.1d"><apply id="Ch2.F3.3.3.m3.1.1.cmml" xref="Ch2.F3.3.3.m3.1.1"><times id="Ch2.F3.3.3.m3.1.1.1.cmml" xref="Ch2.F3.3.3.m3.1.1.1"></times><ci id="Ch2.F3.3.3.m3.1.1.2.cmml" xref="Ch2.F3.3.3.m3.1.1.2">𝐻</ci><ci id="Ch2.F3.3.3.m3.1.1.3.cmml" xref="Ch2.F3.3.3.m3.1.1.3">𝑊</ci><ci id="Ch2.F3.3.3.m3.1.1.4.cmml" xref="Ch2.F3.3.3.m3.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F3.3.3.m3.1e">H\times W\times N</annotation><annotation encoding="application/x-llamapun" id="Ch2.F3.3.3.m3.1f">italic_H × italic_W × italic_N</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="Ch2.F3.4.4.m4.1"><semantics id="Ch2.F3.4.4.m4.1c"><mi id="Ch2.F3.4.4.m4.1.1" xref="Ch2.F3.4.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch2.F3.4.4.m4.1d"><ci id="Ch2.F3.4.4.m4.1.1.cmml" xref="Ch2.F3.4.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F3.4.4.m4.1e">N</annotation><annotation encoding="application/x-llamapun" id="Ch2.F3.4.4.m4.1f">italic_N</annotation></semantics></math> is the number of filters. This process involves sliding the filter over the input and computing the dot products between the filter weights and local regions of the input.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.SS1.p4">
<p class="ltx_p" id="Ch2.S2.SS1.p4.1">Convolutional layers apply filters (kernels) to the input to create feature maps. These filters contain learned weights, which are adjusted during training to optimize feature extraction (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F3" title="Figure 2.3 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.3</span></a>). The convolution operation involves sliding the filter over the input to compute dot products between the filter weights and local regions of the input, generating feature maps that capture different aspects of the input data.
Though not depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F3" title="Figure 2.3 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.3</span></a>, to further adjust the output, a learnable bias term is normally also added to each output element.</p>
</div>
<figure class="ltx_figure" id="Ch2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="259" id="Ch2.F4.g1" src="extracted/5906916/fig/pooling.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F4.5.4.1" style="font-size:90%;">Figure 2.4</span>: </span><span class="ltx_text" id="Ch2.F4.3.3" style="font-size:90%;">Illustration of max pooling and average pooling operations with a <math alttext="2\times 2" class="ltx_Math" display="inline" id="Ch2.F4.1.1.m1.1"><semantics id="Ch2.F4.1.1.m1.1c"><mrow id="Ch2.F4.1.1.m1.1.1" xref="Ch2.F4.1.1.m1.1.1.cmml"><mn id="Ch2.F4.1.1.m1.1.1.2" xref="Ch2.F4.1.1.m1.1.1.2.cmml">2</mn><mo id="Ch2.F4.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F4.1.1.m1.1.1.1.cmml">×</mo><mn id="Ch2.F4.1.1.m1.1.1.3" xref="Ch2.F4.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F4.1.1.m1.1d"><apply id="Ch2.F4.1.1.m1.1.1.cmml" xref="Ch2.F4.1.1.m1.1.1"><times id="Ch2.F4.1.1.m1.1.1.1.cmml" xref="Ch2.F4.1.1.m1.1.1.1"></times><cn id="Ch2.F4.1.1.m1.1.1.2.cmml" type="integer" xref="Ch2.F4.1.1.m1.1.1.2">2</cn><cn id="Ch2.F4.1.1.m1.1.1.3.cmml" type="integer" xref="Ch2.F4.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F4.1.1.m1.1e">2\times 2</annotation><annotation encoding="application/x-llamapun" id="Ch2.F4.1.1.m1.1f">2 × 2</annotation></semantics></math> pool size, taken from <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib39" title="">zhang2020lightweight </a></cite>. Max pooling selects the maximum value from each <math alttext="2\times 2" class="ltx_Math" display="inline" id="Ch2.F4.2.2.m2.1"><semantics id="Ch2.F4.2.2.m2.1c"><mrow id="Ch2.F4.2.2.m2.1.1" xref="Ch2.F4.2.2.m2.1.1.cmml"><mn id="Ch2.F4.2.2.m2.1.1.2" xref="Ch2.F4.2.2.m2.1.1.2.cmml">2</mn><mo id="Ch2.F4.2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F4.2.2.m2.1.1.1.cmml">×</mo><mn id="Ch2.F4.2.2.m2.1.1.3" xref="Ch2.F4.2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F4.2.2.m2.1d"><apply id="Ch2.F4.2.2.m2.1.1.cmml" xref="Ch2.F4.2.2.m2.1.1"><times id="Ch2.F4.2.2.m2.1.1.1.cmml" xref="Ch2.F4.2.2.m2.1.1.1"></times><cn id="Ch2.F4.2.2.m2.1.1.2.cmml" type="integer" xref="Ch2.F4.2.2.m2.1.1.2">2</cn><cn id="Ch2.F4.2.2.m2.1.1.3.cmml" type="integer" xref="Ch2.F4.2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F4.2.2.m2.1e">2\times 2</annotation><annotation encoding="application/x-llamapun" id="Ch2.F4.2.2.m2.1f">2 × 2</annotation></semantics></math> block, while average pooling computes the average value from each <math alttext="2\times 2" class="ltx_Math" display="inline" id="Ch2.F4.3.3.m3.1"><semantics id="Ch2.F4.3.3.m3.1c"><mrow id="Ch2.F4.3.3.m3.1.1" xref="Ch2.F4.3.3.m3.1.1.cmml"><mn id="Ch2.F4.3.3.m3.1.1.2" xref="Ch2.F4.3.3.m3.1.1.2.cmml">2</mn><mo id="Ch2.F4.3.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.F4.3.3.m3.1.1.1.cmml">×</mo><mn id="Ch2.F4.3.3.m3.1.1.3" xref="Ch2.F4.3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F4.3.3.m3.1d"><apply id="Ch2.F4.3.3.m3.1.1.cmml" xref="Ch2.F4.3.3.m3.1.1"><times id="Ch2.F4.3.3.m3.1.1.1.cmml" xref="Ch2.F4.3.3.m3.1.1.1"></times><cn id="Ch2.F4.3.3.m3.1.1.2.cmml" type="integer" xref="Ch2.F4.3.3.m3.1.1.2">2</cn><cn id="Ch2.F4.3.3.m3.1.1.3.cmml" type="integer" xref="Ch2.F4.3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F4.3.3.m3.1e">2\times 2</annotation><annotation encoding="application/x-llamapun" id="Ch2.F4.3.3.m3.1f">2 × 2</annotation></semantics></math> block, reducing the spatial dimensions of the input feature maps.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.SS1.p5">
<p class="ltx_p" id="Ch2.S2.SS1.p5.1">Pooling layers perform a fixed operation to reduce the spatial dimensions of the feature maps, down-sampling the input to reduce computational load and enhance invariance to small translations. As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F4" title="Figure 2.4 ‣ 2.2.1 Standard Architecture Description ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.4</span></a>, there are two common types of pooling: max pooling and average pooling. Both types of pooling effectively reduce the spatial dimensions while preserving important spatial features. Moreover, pooling layers reduce the spatial resolution of feature maps to combat overfitting, which happens when a model memorizes training data, failing to generalize to new, unseen data.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p6">
<p class="ltx_p" id="Ch2.S2.SS1.p6.1">In contrast to pooling, upsampling layers perform the opposite operation by increasing the spatial dimensions of the feature maps. Upsampling can be achieved through various methods, such as nearest-neighbor interpolation, bilinear interpolation, or transposed convolutions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
Upsampling layers are used to increase resolution when finer detail is necessary.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p7">
<p class="ltx_p" id="Ch2.S2.SS1.p7.2">Activation layers introduce non-linearity, enabling the network to capture complex patterns <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib38" title="">lecun2015deep </a></cite>. Typical activation functions include <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu" title="Rectified Linear Unit">Rectified Linear Unit (ReLU)</span></a> and <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu" title="Sigmoid-Weighted Linear Unit">Sigmoid-Weighted Linear Unit (SiLU)</span></a>. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu" title="Rectified Linear Unit">ReLU</span></a>, given by <math alttext="f(x)=\max(0,x)" class="ltx_Math" display="inline" id="Ch2.S2.SS1.p7.1.m1.4"><semantics id="Ch2.S2.SS1.p7.1.m1.4a"><mrow id="Ch2.S2.SS1.p7.1.m1.4.5" xref="Ch2.S2.SS1.p7.1.m1.4.5.cmml"><mrow id="Ch2.S2.SS1.p7.1.m1.4.5.2" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.cmml"><mi id="Ch2.S2.SS1.p7.1.m1.4.5.2.2" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.2.cmml">f</mi><mo id="Ch2.S2.SS1.p7.1.m1.4.5.2.1" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.1.cmml">⁢</mo><mrow id="Ch2.S2.SS1.p7.1.m1.4.5.2.3.2" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.cmml"><mo id="Ch2.S2.SS1.p7.1.m1.4.5.2.3.2.1" stretchy="false" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.cmml">(</mo><mi id="Ch2.S2.SS1.p7.1.m1.1.1" xref="Ch2.S2.SS1.p7.1.m1.1.1.cmml">x</mi><mo id="Ch2.S2.SS1.p7.1.m1.4.5.2.3.2.2" stretchy="false" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="Ch2.S2.SS1.p7.1.m1.4.5.1" xref="Ch2.S2.SS1.p7.1.m1.4.5.1.cmml">=</mo><mrow id="Ch2.S2.SS1.p7.1.m1.4.5.3.2" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml"><mi id="Ch2.S2.SS1.p7.1.m1.2.2" xref="Ch2.S2.SS1.p7.1.m1.2.2.cmml">max</mi><mo id="Ch2.S2.SS1.p7.1.m1.4.5.3.2a" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml">⁡</mo><mrow id="Ch2.S2.SS1.p7.1.m1.4.5.3.2.1" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml"><mo id="Ch2.S2.SS1.p7.1.m1.4.5.3.2.1.1" stretchy="false" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml">(</mo><mn id="Ch2.S2.SS1.p7.1.m1.3.3" xref="Ch2.S2.SS1.p7.1.m1.3.3.cmml">0</mn><mo id="Ch2.S2.SS1.p7.1.m1.4.5.3.2.1.2" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml">,</mo><mi id="Ch2.S2.SS1.p7.1.m1.4.4" xref="Ch2.S2.SS1.p7.1.m1.4.4.cmml">x</mi><mo id="Ch2.S2.SS1.p7.1.m1.4.5.3.2.1.3" stretchy="false" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS1.p7.1.m1.4b"><apply id="Ch2.S2.SS1.p7.1.m1.4.5.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5"><eq id="Ch2.S2.SS1.p7.1.m1.4.5.1.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5.1"></eq><apply id="Ch2.S2.SS1.p7.1.m1.4.5.2.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5.2"><times id="Ch2.S2.SS1.p7.1.m1.4.5.2.1.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.1"></times><ci id="Ch2.S2.SS1.p7.1.m1.4.5.2.2.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5.2.2">𝑓</ci><ci id="Ch2.S2.SS1.p7.1.m1.1.1.cmml" xref="Ch2.S2.SS1.p7.1.m1.1.1">𝑥</ci></apply><apply id="Ch2.S2.SS1.p7.1.m1.4.5.3.1.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.5.3.2"><max id="Ch2.S2.SS1.p7.1.m1.2.2.cmml" xref="Ch2.S2.SS1.p7.1.m1.2.2"></max><cn id="Ch2.S2.SS1.p7.1.m1.3.3.cmml" type="integer" xref="Ch2.S2.SS1.p7.1.m1.3.3">0</cn><ci id="Ch2.S2.SS1.p7.1.m1.4.4.cmml" xref="Ch2.S2.SS1.p7.1.m1.4.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS1.p7.1.m1.4c">f(x)=\max(0,x)</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS1.p7.1.m1.4d">italic_f ( italic_x ) = roman_max ( 0 , italic_x )</annotation></semantics></math>, function provides a linear output that is zero for negative inputs and linear with a slope of 1 for positive inputs. <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu" title="Sigmoid-Weighted Linear Unit">SiLU</span></a>, given by <math alttext="f(x)=x\cdot\sigma(x)" class="ltx_Math" display="inline" id="Ch2.S2.SS1.p7.2.m2.2"><semantics id="Ch2.S2.SS1.p7.2.m2.2a"><mrow id="Ch2.S2.SS1.p7.2.m2.2.3" xref="Ch2.S2.SS1.p7.2.m2.2.3.cmml"><mrow id="Ch2.S2.SS1.p7.2.m2.2.3.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.cmml"><mi id="Ch2.S2.SS1.p7.2.m2.2.3.2.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.2.cmml">f</mi><mo id="Ch2.S2.SS1.p7.2.m2.2.3.2.1" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.1.cmml">⁢</mo><mrow id="Ch2.S2.SS1.p7.2.m2.2.3.2.3.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.cmml"><mo id="Ch2.S2.SS1.p7.2.m2.2.3.2.3.2.1" stretchy="false" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.cmml">(</mo><mi id="Ch2.S2.SS1.p7.2.m2.1.1" xref="Ch2.S2.SS1.p7.2.m2.1.1.cmml">x</mi><mo id="Ch2.S2.SS1.p7.2.m2.2.3.2.3.2.2" stretchy="false" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.cmml">)</mo></mrow></mrow><mo id="Ch2.S2.SS1.p7.2.m2.2.3.1" xref="Ch2.S2.SS1.p7.2.m2.2.3.1.cmml">=</mo><mrow id="Ch2.S2.SS1.p7.2.m2.2.3.3" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.cmml"><mrow id="Ch2.S2.SS1.p7.2.m2.2.3.3.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.cmml"><mi id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.2.cmml">x</mi><mo id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.1.cmml">⋅</mo><mi id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.3" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.3.cmml">σ</mi></mrow><mo id="Ch2.S2.SS1.p7.2.m2.2.3.3.1" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.1.cmml">⁢</mo><mrow id="Ch2.S2.SS1.p7.2.m2.2.3.3.3.2" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.cmml"><mo id="Ch2.S2.SS1.p7.2.m2.2.3.3.3.2.1" stretchy="false" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.cmml">(</mo><mi id="Ch2.S2.SS1.p7.2.m2.2.2" xref="Ch2.S2.SS1.p7.2.m2.2.2.cmml">x</mi><mo id="Ch2.S2.SS1.p7.2.m2.2.3.3.3.2.2" stretchy="false" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS1.p7.2.m2.2b"><apply id="Ch2.S2.SS1.p7.2.m2.2.3.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3"><eq id="Ch2.S2.SS1.p7.2.m2.2.3.1.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.1"></eq><apply id="Ch2.S2.SS1.p7.2.m2.2.3.2.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.2"><times id="Ch2.S2.SS1.p7.2.m2.2.3.2.1.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.1"></times><ci id="Ch2.S2.SS1.p7.2.m2.2.3.2.2.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.2.2">𝑓</ci><ci id="Ch2.S2.SS1.p7.2.m2.1.1.cmml" xref="Ch2.S2.SS1.p7.2.m2.1.1">𝑥</ci></apply><apply id="Ch2.S2.SS1.p7.2.m2.2.3.3.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3"><times id="Ch2.S2.SS1.p7.2.m2.2.3.3.1.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.1"></times><apply id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2"><ci id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.1.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.1">⋅</ci><ci id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.2.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.2">𝑥</ci><ci id="Ch2.S2.SS1.p7.2.m2.2.3.3.2.3.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.3.3.2.3">𝜎</ci></apply><ci id="Ch2.S2.SS1.p7.2.m2.2.2.cmml" xref="Ch2.S2.SS1.p7.2.m2.2.2">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS1.p7.2.m2.2c">f(x)=x\cdot\sigma(x)</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS1.p7.2.m2.2d">italic_f ( italic_x ) = italic_x ⋅ italic_σ ( italic_x )</annotation></semantics></math>, incorporates a sigmoid function, described as</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p8">
<table class="ltx_equation ltx_eqn_table" id="Ch2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sigma(x)=\frac{1}{1+e^{-x}}" class="ltx_Math" display="block" id="Ch2.E1.m1.1"><semantics id="Ch2.E1.m1.1a"><mrow id="Ch2.E1.m1.1.2" xref="Ch2.E1.m1.1.2.cmml"><mrow id="Ch2.E1.m1.1.2.2" xref="Ch2.E1.m1.1.2.2.cmml"><mi id="Ch2.E1.m1.1.2.2.2" xref="Ch2.E1.m1.1.2.2.2.cmml">σ</mi><mo id="Ch2.E1.m1.1.2.2.1" xref="Ch2.E1.m1.1.2.2.1.cmml">⁢</mo><mrow id="Ch2.E1.m1.1.2.2.3.2" xref="Ch2.E1.m1.1.2.2.cmml"><mo id="Ch2.E1.m1.1.2.2.3.2.1" stretchy="false" xref="Ch2.E1.m1.1.2.2.cmml">(</mo><mi id="Ch2.E1.m1.1.1" xref="Ch2.E1.m1.1.1.cmml">x</mi><mo id="Ch2.E1.m1.1.2.2.3.2.2" stretchy="false" xref="Ch2.E1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="Ch2.E1.m1.1.2.1" xref="Ch2.E1.m1.1.2.1.cmml">=</mo><mfrac id="Ch2.E1.m1.1.2.3" xref="Ch2.E1.m1.1.2.3.cmml"><mn id="Ch2.E1.m1.1.2.3.2" xref="Ch2.E1.m1.1.2.3.2.cmml">1</mn><mrow id="Ch2.E1.m1.1.2.3.3" xref="Ch2.E1.m1.1.2.3.3.cmml"><mn id="Ch2.E1.m1.1.2.3.3.2" xref="Ch2.E1.m1.1.2.3.3.2.cmml">1</mn><mo id="Ch2.E1.m1.1.2.3.3.1" xref="Ch2.E1.m1.1.2.3.3.1.cmml">+</mo><msup id="Ch2.E1.m1.1.2.3.3.3" xref="Ch2.E1.m1.1.2.3.3.3.cmml"><mi id="Ch2.E1.m1.1.2.3.3.3.2" xref="Ch2.E1.m1.1.2.3.3.3.2.cmml">e</mi><mrow id="Ch2.E1.m1.1.2.3.3.3.3" xref="Ch2.E1.m1.1.2.3.3.3.3.cmml"><mo id="Ch2.E1.m1.1.2.3.3.3.3a" xref="Ch2.E1.m1.1.2.3.3.3.3.cmml">−</mo><mi id="Ch2.E1.m1.1.2.3.3.3.3.2" xref="Ch2.E1.m1.1.2.3.3.3.3.2.cmml">x</mi></mrow></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Ch2.E1.m1.1b"><apply id="Ch2.E1.m1.1.2.cmml" xref="Ch2.E1.m1.1.2"><eq id="Ch2.E1.m1.1.2.1.cmml" xref="Ch2.E1.m1.1.2.1"></eq><apply id="Ch2.E1.m1.1.2.2.cmml" xref="Ch2.E1.m1.1.2.2"><times id="Ch2.E1.m1.1.2.2.1.cmml" xref="Ch2.E1.m1.1.2.2.1"></times><ci id="Ch2.E1.m1.1.2.2.2.cmml" xref="Ch2.E1.m1.1.2.2.2">𝜎</ci><ci id="Ch2.E1.m1.1.1.cmml" xref="Ch2.E1.m1.1.1">𝑥</ci></apply><apply id="Ch2.E1.m1.1.2.3.cmml" xref="Ch2.E1.m1.1.2.3"><divide id="Ch2.E1.m1.1.2.3.1.cmml" xref="Ch2.E1.m1.1.2.3"></divide><cn id="Ch2.E1.m1.1.2.3.2.cmml" type="integer" xref="Ch2.E1.m1.1.2.3.2">1</cn><apply id="Ch2.E1.m1.1.2.3.3.cmml" xref="Ch2.E1.m1.1.2.3.3"><plus id="Ch2.E1.m1.1.2.3.3.1.cmml" xref="Ch2.E1.m1.1.2.3.3.1"></plus><cn id="Ch2.E1.m1.1.2.3.3.2.cmml" type="integer" xref="Ch2.E1.m1.1.2.3.3.2">1</cn><apply id="Ch2.E1.m1.1.2.3.3.3.cmml" xref="Ch2.E1.m1.1.2.3.3.3"><csymbol cd="ambiguous" id="Ch2.E1.m1.1.2.3.3.3.1.cmml" xref="Ch2.E1.m1.1.2.3.3.3">superscript</csymbol><ci id="Ch2.E1.m1.1.2.3.3.3.2.cmml" xref="Ch2.E1.m1.1.2.3.3.3.2">𝑒</ci><apply id="Ch2.E1.m1.1.2.3.3.3.3.cmml" xref="Ch2.E1.m1.1.2.3.3.3.3"><minus id="Ch2.E1.m1.1.2.3.3.3.3.1.cmml" xref="Ch2.E1.m1.1.2.3.3.3.3"></minus><ci id="Ch2.E1.m1.1.2.3.3.3.3.2.cmml" xref="Ch2.E1.m1.1.2.3.3.3.3.2">𝑥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.E1.m1.1c">\sigma(x)=\frac{1}{1+e^{-x}}</annotation><annotation encoding="application/x-llamapun" id="Ch2.E1.m1.1d">italic_σ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p9">
<p class="ltx_p" id="Ch2.S2.SS1.p9.1">allowing it to handle negative inputs more dynamically by scaling the output in a non-linear fashion.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p10">
<p class="ltx_p" id="Ch2.S2.SS1.p10.1">Regularization layers, such as dropout and batch normalization, are also integrated into <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a> architectures. Dropout randomly omits neurons during training to enhance generalization, while batch normalization scales the output of a layer to have a mean of zero and a variance of one, which can expedite training and improve overall performance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib38" title="">lecun2015deep </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS1.p11">
<p class="ltx_p" id="Ch2.S2.SS1.p11.1">State-of-the-art deep learning architectures, such as those that will be described in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3</span></a>, often combine the above-presented layers in innovative ways to enhance model performance and efficiency. For instance, methods like ResNet, normally used as backbone in object recognition architectures, introduce skip connections that allow gradients to flow more easily through very deep networks, facilitating training  <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib40" title="">he2016deep </a></cite>. Other advancements involve combining the layers in specific configurations to achieve desired properties. For example, Feature Pyramid Networks (FPNs) employ a combination of convolutional and upsampling layers to create feature maps at different scales, allowing the model to better handle objects of varying sizes within an image <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib41" title="">lin2017feature </a></cite>. Additionally, various forms of attention mechanisms (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.S2.SS2" title="2.2.2 Attention Mechanisms ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>) can be integrated using these building blocks to selectively focus on relevant parts of the feature maps, leading to improved performance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib42" title="">vaswani2017attention </a></cite>.
In summary, the combination of layers are foundational and their interactions is crucial for the advancements in contemporary deep learning models.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S2.SS2">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">2.2.2   Attention Mechanisms</h4>
<div class="ltx_para" id="Ch2.S2.SS2.p1">
<p class="ltx_p" id="Ch2.S2.SS2.p1.2">Neural networks use attention mechanisms to allow models to dynamically focus on the most relevant parts of the input data, enhancing their ability to process complex information <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib42" title="">vaswani2017attention </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib43" title="">bahdanau2014neural </a></cite>. Initially introduced in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nlp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nlp" title="Natural Language Processing">Natural Language Processing (NLP)</span></a> for tasks like language translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib43" title="">bahdanau2014neural </a></cite>, attention mechanisms have since become integral to various deep learning applications, including computer vision. At the core of these mechanisms are attention weights, learned during training, which determine the importance of different parts of the input <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib44" title="">Luong2015 </a></cite>. For example, in language translation, this results in an <math alttext="n\times n" class="ltx_Math" display="inline" id="Ch2.S2.SS2.p1.1.m1.1"><semantics id="Ch2.S2.SS2.p1.1.m1.1a"><mrow id="Ch2.S2.SS2.p1.1.m1.1.1" xref="Ch2.S2.SS2.p1.1.m1.1.1.cmml"><mi id="Ch2.S2.SS2.p1.1.m1.1.1.2" xref="Ch2.S2.SS2.p1.1.m1.1.1.2.cmml">n</mi><mo id="Ch2.S2.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch2.S2.SS2.p1.1.m1.1.1.1.cmml">×</mo><mi id="Ch2.S2.SS2.p1.1.m1.1.1.3" xref="Ch2.S2.SS2.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS2.p1.1.m1.1b"><apply id="Ch2.S2.SS2.p1.1.m1.1.1.cmml" xref="Ch2.S2.SS2.p1.1.m1.1.1"><times id="Ch2.S2.SS2.p1.1.m1.1.1.1.cmml" xref="Ch2.S2.SS2.p1.1.m1.1.1.1"></times><ci id="Ch2.S2.SS2.p1.1.m1.1.1.2.cmml" xref="Ch2.S2.SS2.p1.1.m1.1.1.2">𝑛</ci><ci id="Ch2.S2.SS2.p1.1.m1.1.1.3.cmml" xref="Ch2.S2.SS2.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS2.p1.1.m1.1c">n\times n</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS2.p1.1.m1.1d">italic_n × italic_n</annotation></semantics></math> attention matrix or map, where <math alttext="n" class="ltx_Math" display="inline" id="Ch2.S2.SS2.p1.2.m2.1"><semantics id="Ch2.S2.SS2.p1.2.m2.1a"><mi id="Ch2.S2.SS2.p1.2.m2.1.1" xref="Ch2.S2.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS2.p1.2.m2.1b"><ci id="Ch2.S2.SS2.p1.2.m2.1.1.cmml" xref="Ch2.S2.SS2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS2.p1.2.m2.1d">italic_n</annotation></semantics></math> is the number of words <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib43" title="">bahdanau2014neural </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS2.p2">
<p class="ltx_p" id="Ch2.S2.SS2.p2.1">The implementation of attention mechanisms involves the calculation of the attention weights using a score function <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib43" title="">bahdanau2014neural </a></cite>. This is achieved by applying the input to a learned weighted matrix that computes relevance scores using functions like dot product <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib42" title="">vaswani2017attention </a></cite>, or pooling operations (e.g., max pooling, average pooling) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>. These scores create an attention map, highlighting the importance of different input parts. The network uses this map to prioritize crucial information, learning to distribute focus effectively across the input data.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS2.p3">
<p class="ltx_p" id="Ch2.S2.SS2.p3.1">In computer vision, the term attention translates to individual pixels attending to other pixels, or patches of pixels attending to other patches, leading to an attention map that captures the relationships across different regions of the image <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib46" title="">guo2022attention </a></cite>. However, this poses significant computational challenges. To address these challenges, various strategies have been proposed in the literature:</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS2.p4">
<ul class="ltx_itemize" id="Ch2.S2.I1">
<li class="ltx_item" id="Ch2.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I1.i1.p1">
<p class="ltx_p" id="Ch2.S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.I1.i1.p1.1.1">Dimensionality reduction</span> with convolutional layers and pooling operations are often used to reduce the dimensions of the input before applying attention, decreasing the computational load by working with smaller feature maps <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib47" title="">Hu2018 </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I1.i2.p1">
<p class="ltx_p" id="Ch2.S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.I1.i2.p1.1.1">Hierarchical attention</span> mechanisms apply attention at different scales or hierarchies, allowing the model to first focus on broad, coarse details and progressively refine its attention to finer details, thus significantly reducing complexity <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib48" title="">Yang2016 </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I1.i3.p1">
<p class="ltx_p" id="Ch2.S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.I1.i3.p1.1.1">Local attention</span> restricts the attention mechanism to a local neighborhood around each pixel, limiting the number of interactions and thereby reducing the computational burden <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib49" title="">Parmar2018 </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I1.i4.p1">
<p class="ltx_p" id="Ch2.S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.I1.i4.p1.1.1">Spatial attention</span> mechanisms identify important spatial locations within an image, thus allowing the model to concentrate on critical regions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib50" title="">Xu2015 </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I1.i5.p1">
<p class="ltx_p" id="Ch2.S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S2.I1.i5.p1.1.1">Channel attention</span> mechanisms focus on identifying important feature channels within a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a>, thereby improving the model’s feature representation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib47" title="">Hu2018 </a></cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch2.S2.SS2.p5">
<p class="ltx_p" id="Ch2.S2.SS2.p5.1">Attention mechanisms are, therefore, used to enhance the ability of models to focus on key parts of an image. Specifically for computer vision tasks, they have been incorporated to <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a> to improve performance in large-scale image classification tasks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib47" title="">Hu2018 </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib51" title="">Wang2017 </a></cite>. Additionally, they have been applied in object detection and instance segmentation tasks by incorporating spatial and channel-wise attention, which improves feature representation and accuracy <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib47" title="">Hu2018 </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib52" title="">Chen2017 </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib53" title="">brauwers2021general </a></cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S2.SS3">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">2.2.3   Object Classification and Postprocessing</h4>
<div class="ltx_para" id="Ch2.S2.SS3.p1">
<p class="ltx_p" id="Ch2.S2.SS3.p1.1">To accurately classify objects, after they are detected or segmented, the final layer of the head, usually a fully connected layer <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib30" title="">goodfellow2016deep </a></cite>, provides a confidence score for each potential prediction, which reflects the likelihood that a recognized object belongs to a specific class. Multiple prediction proposals (bounding boxes or masks) for the same object require further postprocessing beyond the head to enhance the accuracy and reliability of a prediction. Standard object recognition architectures include postprocessing after the head to refine the bounding boxes or segmentation masks recognized by the model and to eliminate redundant predictions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
A key component of this postprocessing phase is <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">Non-Maximum Suppression (NMS)</span></a> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib54" title="">gong2021review </a></cite>, a technique designed to eliminate redundant bounding boxes or segmentation masks that pertain to the same object. Essentially, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">NMS</span></a> ensures that each detected object is represented exclusively by the single, most accurate bounding box or mask, thereby preventing clutter and providing a clearer output.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS3.p2">
<p class="ltx_p" id="Ch2.S2.SS3.p2.1">To decide which bounding boxes or masks to keep and which to discard, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">NMS</span></a> relies on the confidence scores of the predictions, together with a metric known as <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">Intersection over Union (IoU)</span></a>. This metric measures the overlap between two areas—in this case, the area of overlap between a predicted bounding box or mask and the ground truth, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F5" title="Figure 2.5 ‣ 2.2.3 Object Classification and Postprocessing ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.5</span></a>. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> helps in determining the accuracy of the predictions by quantifying how closely the predicted bounding boxes or masks align with the actual objects in the image.</p>
</div>
<figure class="ltx_figure" id="Ch2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="179" id="Ch2.F5.g1" src="extracted/5906916/fig/iou.png" width="510"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F5.2.1.1" style="font-size:90%;">Figure 2.5</span>: </span><span class="ltx_text" id="Ch2.F5.3.2" style="font-size:90%;">Joint illustration of Intersection over Union calculation for boxes and masks. On the left, the ground truth, and predicted bounding boxes and masks are shown. In the middle, the IoU for the bounding box is visualized. On the right, in the case of instance segmentation, the IoU for the mask is calculated.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.SS3.p3">
<p class="ltx_p" id="Ch2.S2.SS3.p3.2">Mathematically, <math alttext="B_{IoU}" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p3.1.m1.1"><semantics id="Ch2.S2.SS3.p3.1.m1.1a"><msub id="Ch2.S2.SS3.p3.1.m1.1.1" xref="Ch2.S2.SS3.p3.1.m1.1.1.cmml"><mi id="Ch2.S2.SS3.p3.1.m1.1.1.2" xref="Ch2.S2.SS3.p3.1.m1.1.1.2.cmml">B</mi><mrow id="Ch2.S2.SS3.p3.1.m1.1.1.3" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.cmml"><mi id="Ch2.S2.SS3.p3.1.m1.1.1.3.2" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.2.cmml">I</mi><mo id="Ch2.S2.SS3.p3.1.m1.1.1.3.1" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p3.1.m1.1.1.3.3" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.3.cmml">o</mi><mo id="Ch2.S2.SS3.p3.1.m1.1.1.3.1a" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p3.1.m1.1.1.3.4" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.4.cmml">U</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p3.1.m1.1b"><apply id="Ch2.S2.SS3.p3.1.m1.1.1.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Ch2.S2.SS3.p3.1.m1.1.1.1.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="Ch2.S2.SS3.p3.1.m1.1.1.2.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.2">𝐵</ci><apply id="Ch2.S2.SS3.p3.1.m1.1.1.3.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.3"><times id="Ch2.S2.SS3.p3.1.m1.1.1.3.1.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.1"></times><ci id="Ch2.S2.SS3.p3.1.m1.1.1.3.2.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.2">𝐼</ci><ci id="Ch2.S2.SS3.p3.1.m1.1.1.3.3.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.3">𝑜</ci><ci id="Ch2.S2.SS3.p3.1.m1.1.1.3.4.cmml" xref="Ch2.S2.SS3.p3.1.m1.1.1.3.4">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p3.1.m1.1c">B_{IoU}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p3.1.m1.1d">italic_B start_POSTSUBSCRIPT italic_I italic_o italic_U end_POSTSUBSCRIPT</annotation></semantics></math> (bounding box) and <math alttext="M_{IoU}" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p3.2.m2.1"><semantics id="Ch2.S2.SS3.p3.2.m2.1a"><msub id="Ch2.S2.SS3.p3.2.m2.1.1" xref="Ch2.S2.SS3.p3.2.m2.1.1.cmml"><mi id="Ch2.S2.SS3.p3.2.m2.1.1.2" xref="Ch2.S2.SS3.p3.2.m2.1.1.2.cmml">M</mi><mrow id="Ch2.S2.SS3.p3.2.m2.1.1.3" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.cmml"><mi id="Ch2.S2.SS3.p3.2.m2.1.1.3.2" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.2.cmml">I</mi><mo id="Ch2.S2.SS3.p3.2.m2.1.1.3.1" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p3.2.m2.1.1.3.3" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.3.cmml">o</mi><mo id="Ch2.S2.SS3.p3.2.m2.1.1.3.1a" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p3.2.m2.1.1.3.4" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.4.cmml">U</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p3.2.m2.1b"><apply id="Ch2.S2.SS3.p3.2.m2.1.1.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="Ch2.S2.SS3.p3.2.m2.1.1.1.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="Ch2.S2.SS3.p3.2.m2.1.1.2.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.2">𝑀</ci><apply id="Ch2.S2.SS3.p3.2.m2.1.1.3.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.3"><times id="Ch2.S2.SS3.p3.2.m2.1.1.3.1.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.1"></times><ci id="Ch2.S2.SS3.p3.2.m2.1.1.3.2.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.2">𝐼</ci><ci id="Ch2.S2.SS3.p3.2.m2.1.1.3.3.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.3">𝑜</ci><ci id="Ch2.S2.SS3.p3.2.m2.1.1.3.4.cmml" xref="Ch2.S2.SS3.p3.2.m2.1.1.3.4">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p3.2.m2.1c">M_{IoU}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p3.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_I italic_o italic_U end_POSTSUBSCRIPT</annotation></semantics></math> (mask) can be denoted as:</p>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<table class="ltx_equation ltx_eqn_table" id="Ch2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="B_{IoU}=\frac{\text{area}(B_{pred}\cap B_{gt})}{\text{area}(B_{pred}\cup B_{gt%
})}" class="ltx_Math" display="block" id="Ch2.E2.m1.2"><semantics id="Ch2.E2.m1.2a"><mrow id="Ch2.E2.m1.2.3" xref="Ch2.E2.m1.2.3.cmml"><msub id="Ch2.E2.m1.2.3.2" xref="Ch2.E2.m1.2.3.2.cmml"><mi id="Ch2.E2.m1.2.3.2.2" xref="Ch2.E2.m1.2.3.2.2.cmml">B</mi><mrow id="Ch2.E2.m1.2.3.2.3" xref="Ch2.E2.m1.2.3.2.3.cmml"><mi id="Ch2.E2.m1.2.3.2.3.2" xref="Ch2.E2.m1.2.3.2.3.2.cmml">I</mi><mo id="Ch2.E2.m1.2.3.2.3.1" xref="Ch2.E2.m1.2.3.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.3.2.3.3" xref="Ch2.E2.m1.2.3.2.3.3.cmml">o</mi><mo id="Ch2.E2.m1.2.3.2.3.1a" xref="Ch2.E2.m1.2.3.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.3.2.3.4" xref="Ch2.E2.m1.2.3.2.3.4.cmml">U</mi></mrow></msub><mo id="Ch2.E2.m1.2.3.1" xref="Ch2.E2.m1.2.3.1.cmml">=</mo><mfrac id="Ch2.E2.m1.2.2" xref="Ch2.E2.m1.2.2.cmml"><mrow id="Ch2.E2.m1.1.1.1" xref="Ch2.E2.m1.1.1.1.cmml"><mtext id="Ch2.E2.m1.1.1.1.3" xref="Ch2.E2.m1.1.1.1.3a.cmml">area</mtext><mo id="Ch2.E2.m1.1.1.1.2" xref="Ch2.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="Ch2.E2.m1.1.1.1.1.1" xref="Ch2.E2.m1.1.1.1.1.1.1.cmml"><mo id="Ch2.E2.m1.1.1.1.1.1.2" stretchy="false" xref="Ch2.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch2.E2.m1.1.1.1.1.1.1" xref="Ch2.E2.m1.1.1.1.1.1.1.cmml"><msub id="Ch2.E2.m1.1.1.1.1.1.1.2" xref="Ch2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="Ch2.E2.m1.1.1.1.1.1.1.2.2" xref="Ch2.E2.m1.1.1.1.1.1.1.2.2.cmml">B</mi><mrow id="Ch2.E2.m1.1.1.1.1.1.1.2.3" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.cmml"><mi id="Ch2.E2.m1.1.1.1.1.1.1.2.3.2" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.2.cmml">p</mi><mo id="Ch2.E2.m1.1.1.1.1.1.1.2.3.1" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.1.1.1.1.1.1.2.3.3" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.3.cmml">r</mi><mo id="Ch2.E2.m1.1.1.1.1.1.1.2.3.1a" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.1.1.1.1.1.1.2.3.4" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.4.cmml">e</mi><mo id="Ch2.E2.m1.1.1.1.1.1.1.2.3.1b" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.1.1.1.1.1.1.2.3.5" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.5.cmml">d</mi></mrow></msub><mo id="Ch2.E2.m1.1.1.1.1.1.1.1" xref="Ch2.E2.m1.1.1.1.1.1.1.1.cmml">∩</mo><msub id="Ch2.E2.m1.1.1.1.1.1.1.3" xref="Ch2.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="Ch2.E2.m1.1.1.1.1.1.1.3.2" xref="Ch2.E2.m1.1.1.1.1.1.1.3.2.cmml">B</mi><mrow id="Ch2.E2.m1.1.1.1.1.1.1.3.3" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.cmml"><mi id="Ch2.E2.m1.1.1.1.1.1.1.3.3.2" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo id="Ch2.E2.m1.1.1.1.1.1.1.3.3.1" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.1.1.1.1.1.1.3.3.3" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="Ch2.E2.m1.1.1.1.1.1.3" stretchy="false" xref="Ch2.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="Ch2.E2.m1.2.2.2" xref="Ch2.E2.m1.2.2.2.cmml"><mtext id="Ch2.E2.m1.2.2.2.3" xref="Ch2.E2.m1.2.2.2.3a.cmml">area</mtext><mo id="Ch2.E2.m1.2.2.2.2" xref="Ch2.E2.m1.2.2.2.2.cmml">⁢</mo><mrow id="Ch2.E2.m1.2.2.2.1.1" xref="Ch2.E2.m1.2.2.2.1.1.1.cmml"><mo id="Ch2.E2.m1.2.2.2.1.1.2" stretchy="false" xref="Ch2.E2.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="Ch2.E2.m1.2.2.2.1.1.1" xref="Ch2.E2.m1.2.2.2.1.1.1.cmml"><msub id="Ch2.E2.m1.2.2.2.1.1.1.2" xref="Ch2.E2.m1.2.2.2.1.1.1.2.cmml"><mi id="Ch2.E2.m1.2.2.2.1.1.1.2.2" xref="Ch2.E2.m1.2.2.2.1.1.1.2.2.cmml">B</mi><mrow id="Ch2.E2.m1.2.2.2.1.1.1.2.3" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.cmml"><mi id="Ch2.E2.m1.2.2.2.1.1.1.2.3.2" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.2.cmml">p</mi><mo id="Ch2.E2.m1.2.2.2.1.1.1.2.3.1" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.2.2.1.1.1.2.3.3" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.3.cmml">r</mi><mo id="Ch2.E2.m1.2.2.2.1.1.1.2.3.1a" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.2.2.1.1.1.2.3.4" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.4.cmml">e</mi><mo id="Ch2.E2.m1.2.2.2.1.1.1.2.3.1b" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.2.2.1.1.1.2.3.5" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.5.cmml">d</mi></mrow></msub><mo id="Ch2.E2.m1.2.2.2.1.1.1.1" xref="Ch2.E2.m1.2.2.2.1.1.1.1.cmml">∪</mo><msub id="Ch2.E2.m1.2.2.2.1.1.1.3" xref="Ch2.E2.m1.2.2.2.1.1.1.3.cmml"><mi id="Ch2.E2.m1.2.2.2.1.1.1.3.2" xref="Ch2.E2.m1.2.2.2.1.1.1.3.2.cmml">B</mi><mrow id="Ch2.E2.m1.2.2.2.1.1.1.3.3" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.cmml"><mi id="Ch2.E2.m1.2.2.2.1.1.1.3.3.2" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.2.cmml">g</mi><mo id="Ch2.E2.m1.2.2.2.1.1.1.3.3.1" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch2.E2.m1.2.2.2.1.1.1.3.3.3" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="Ch2.E2.m1.2.2.2.1.1.3" stretchy="false" xref="Ch2.E2.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Ch2.E2.m1.2b"><apply id="Ch2.E2.m1.2.3.cmml" xref="Ch2.E2.m1.2.3"><eq id="Ch2.E2.m1.2.3.1.cmml" xref="Ch2.E2.m1.2.3.1"></eq><apply id="Ch2.E2.m1.2.3.2.cmml" xref="Ch2.E2.m1.2.3.2"><csymbol cd="ambiguous" id="Ch2.E2.m1.2.3.2.1.cmml" xref="Ch2.E2.m1.2.3.2">subscript</csymbol><ci id="Ch2.E2.m1.2.3.2.2.cmml" xref="Ch2.E2.m1.2.3.2.2">𝐵</ci><apply id="Ch2.E2.m1.2.3.2.3.cmml" xref="Ch2.E2.m1.2.3.2.3"><times id="Ch2.E2.m1.2.3.2.3.1.cmml" xref="Ch2.E2.m1.2.3.2.3.1"></times><ci id="Ch2.E2.m1.2.3.2.3.2.cmml" xref="Ch2.E2.m1.2.3.2.3.2">𝐼</ci><ci id="Ch2.E2.m1.2.3.2.3.3.cmml" xref="Ch2.E2.m1.2.3.2.3.3">𝑜</ci><ci id="Ch2.E2.m1.2.3.2.3.4.cmml" xref="Ch2.E2.m1.2.3.2.3.4">𝑈</ci></apply></apply><apply id="Ch2.E2.m1.2.2.cmml" xref="Ch2.E2.m1.2.2"><divide id="Ch2.E2.m1.2.2.3.cmml" xref="Ch2.E2.m1.2.2"></divide><apply id="Ch2.E2.m1.1.1.1.cmml" xref="Ch2.E2.m1.1.1.1"><times id="Ch2.E2.m1.1.1.1.2.cmml" xref="Ch2.E2.m1.1.1.1.2"></times><ci id="Ch2.E2.m1.1.1.1.3a.cmml" xref="Ch2.E2.m1.1.1.1.3"><mtext id="Ch2.E2.m1.1.1.1.3.cmml" xref="Ch2.E2.m1.1.1.1.3">area</mtext></ci><apply id="Ch2.E2.m1.1.1.1.1.1.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1"><intersect id="Ch2.E2.m1.1.1.1.1.1.1.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.1"></intersect><apply id="Ch2.E2.m1.1.1.1.1.1.1.2.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Ch2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Ch2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.2">𝐵</ci><apply id="Ch2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3"><times id="Ch2.E2.m1.1.1.1.1.1.1.2.3.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.1"></times><ci id="Ch2.E2.m1.1.1.1.1.1.1.2.3.2.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.2">𝑝</ci><ci id="Ch2.E2.m1.1.1.1.1.1.1.2.3.3.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.3">𝑟</ci><ci id="Ch2.E2.m1.1.1.1.1.1.1.2.3.4.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.4">𝑒</ci><ci id="Ch2.E2.m1.1.1.1.1.1.1.2.3.5.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.2.3.5">𝑑</ci></apply></apply><apply id="Ch2.E2.m1.1.1.1.1.1.1.3.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Ch2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Ch2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3.2">𝐵</ci><apply id="Ch2.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3"><times id="Ch2.E2.m1.1.1.1.1.1.1.3.3.1.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.1"></times><ci id="Ch2.E2.m1.1.1.1.1.1.1.3.3.2.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.2">𝑔</ci><ci id="Ch2.E2.m1.1.1.1.1.1.1.3.3.3.cmml" xref="Ch2.E2.m1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="Ch2.E2.m1.2.2.2.cmml" xref="Ch2.E2.m1.2.2.2"><times id="Ch2.E2.m1.2.2.2.2.cmml" xref="Ch2.E2.m1.2.2.2.2"></times><ci id="Ch2.E2.m1.2.2.2.3a.cmml" xref="Ch2.E2.m1.2.2.2.3"><mtext id="Ch2.E2.m1.2.2.2.3.cmml" xref="Ch2.E2.m1.2.2.2.3">area</mtext></ci><apply id="Ch2.E2.m1.2.2.2.1.1.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1"><union id="Ch2.E2.m1.2.2.2.1.1.1.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.1"></union><apply id="Ch2.E2.m1.2.2.2.1.1.1.2.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="Ch2.E2.m1.2.2.2.1.1.1.2.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="Ch2.E2.m1.2.2.2.1.1.1.2.2.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.2">𝐵</ci><apply id="Ch2.E2.m1.2.2.2.1.1.1.2.3.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3"><times id="Ch2.E2.m1.2.2.2.1.1.1.2.3.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.1"></times><ci id="Ch2.E2.m1.2.2.2.1.1.1.2.3.2.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.2">𝑝</ci><ci id="Ch2.E2.m1.2.2.2.1.1.1.2.3.3.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.3">𝑟</ci><ci id="Ch2.E2.m1.2.2.2.1.1.1.2.3.4.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.4">𝑒</ci><ci id="Ch2.E2.m1.2.2.2.1.1.1.2.3.5.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.2.3.5">𝑑</ci></apply></apply><apply id="Ch2.E2.m1.2.2.2.1.1.1.3.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="Ch2.E2.m1.2.2.2.1.1.1.3.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3">subscript</csymbol><ci id="Ch2.E2.m1.2.2.2.1.1.1.3.2.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3.2">𝐵</ci><apply id="Ch2.E2.m1.2.2.2.1.1.1.3.3.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3"><times id="Ch2.E2.m1.2.2.2.1.1.1.3.3.1.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.1"></times><ci id="Ch2.E2.m1.2.2.2.1.1.1.3.3.2.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.2">𝑔</ci><ci id="Ch2.E2.m1.2.2.2.1.1.1.3.3.3.cmml" xref="Ch2.E2.m1.2.2.2.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.E2.m1.2c">B_{IoU}=\frac{\text{area}(B_{pred}\cap B_{gt})}{\text{area}(B_{pred}\cup B_{gt%
})}</annotation><annotation encoding="application/x-llamapun" id="Ch2.E2.m1.2d">italic_B start_POSTSUBSCRIPT italic_I italic_o italic_U end_POSTSUBSCRIPT = divide start_ARG area ( italic_B start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT ∩ italic_B start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG area ( italic_B start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT ∪ italic_B start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="Ch2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M_{IoU}=\frac{\text{area}(M_{pred}\cap M_{gt})}{\text{area}(M_{pred}\cup M_{gt%
})}" class="ltx_Math" display="block" id="Ch2.E3.m1.2"><semantics id="Ch2.E3.m1.2a"><mrow id="Ch2.E3.m1.2.3" xref="Ch2.E3.m1.2.3.cmml"><msub id="Ch2.E3.m1.2.3.2" xref="Ch2.E3.m1.2.3.2.cmml"><mi id="Ch2.E3.m1.2.3.2.2" xref="Ch2.E3.m1.2.3.2.2.cmml">M</mi><mrow id="Ch2.E3.m1.2.3.2.3" xref="Ch2.E3.m1.2.3.2.3.cmml"><mi id="Ch2.E3.m1.2.3.2.3.2" xref="Ch2.E3.m1.2.3.2.3.2.cmml">I</mi><mo id="Ch2.E3.m1.2.3.2.3.1" xref="Ch2.E3.m1.2.3.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.3.2.3.3" xref="Ch2.E3.m1.2.3.2.3.3.cmml">o</mi><mo id="Ch2.E3.m1.2.3.2.3.1a" xref="Ch2.E3.m1.2.3.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.3.2.3.4" xref="Ch2.E3.m1.2.3.2.3.4.cmml">U</mi></mrow></msub><mo id="Ch2.E3.m1.2.3.1" xref="Ch2.E3.m1.2.3.1.cmml">=</mo><mfrac id="Ch2.E3.m1.2.2" xref="Ch2.E3.m1.2.2.cmml"><mrow id="Ch2.E3.m1.1.1.1" xref="Ch2.E3.m1.1.1.1.cmml"><mtext id="Ch2.E3.m1.1.1.1.3" xref="Ch2.E3.m1.1.1.1.3a.cmml">area</mtext><mo id="Ch2.E3.m1.1.1.1.2" xref="Ch2.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="Ch2.E3.m1.1.1.1.1.1" xref="Ch2.E3.m1.1.1.1.1.1.1.cmml"><mo id="Ch2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="Ch2.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch2.E3.m1.1.1.1.1.1.1" xref="Ch2.E3.m1.1.1.1.1.1.1.cmml"><msub id="Ch2.E3.m1.1.1.1.1.1.1.2" xref="Ch2.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="Ch2.E3.m1.1.1.1.1.1.1.2.2" xref="Ch2.E3.m1.1.1.1.1.1.1.2.2.cmml">M</mi><mrow id="Ch2.E3.m1.1.1.1.1.1.1.2.3" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="Ch2.E3.m1.1.1.1.1.1.1.2.3.2" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">p</mi><mo id="Ch2.E3.m1.1.1.1.1.1.1.2.3.1" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.1.1.1.1.1.1.2.3.3" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.3.cmml">r</mi><mo id="Ch2.E3.m1.1.1.1.1.1.1.2.3.1a" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.1.1.1.1.1.1.2.3.4" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.4.cmml">e</mi><mo id="Ch2.E3.m1.1.1.1.1.1.1.2.3.1b" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.1.1.1.1.1.1.2.3.5" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.5.cmml">d</mi></mrow></msub><mo id="Ch2.E3.m1.1.1.1.1.1.1.1" xref="Ch2.E3.m1.1.1.1.1.1.1.1.cmml">∩</mo><msub id="Ch2.E3.m1.1.1.1.1.1.1.3" xref="Ch2.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="Ch2.E3.m1.1.1.1.1.1.1.3.2" xref="Ch2.E3.m1.1.1.1.1.1.1.3.2.cmml">M</mi><mrow id="Ch2.E3.m1.1.1.1.1.1.1.3.3" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.cmml"><mi id="Ch2.E3.m1.1.1.1.1.1.1.3.3.2" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo id="Ch2.E3.m1.1.1.1.1.1.1.3.3.1" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.1.1.1.1.1.1.3.3.3" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="Ch2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="Ch2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="Ch2.E3.m1.2.2.2" xref="Ch2.E3.m1.2.2.2.cmml"><mtext id="Ch2.E3.m1.2.2.2.3" xref="Ch2.E3.m1.2.2.2.3a.cmml">area</mtext><mo id="Ch2.E3.m1.2.2.2.2" xref="Ch2.E3.m1.2.2.2.2.cmml">⁢</mo><mrow id="Ch2.E3.m1.2.2.2.1.1" xref="Ch2.E3.m1.2.2.2.1.1.1.cmml"><mo id="Ch2.E3.m1.2.2.2.1.1.2" stretchy="false" xref="Ch2.E3.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="Ch2.E3.m1.2.2.2.1.1.1" xref="Ch2.E3.m1.2.2.2.1.1.1.cmml"><msub id="Ch2.E3.m1.2.2.2.1.1.1.2" xref="Ch2.E3.m1.2.2.2.1.1.1.2.cmml"><mi id="Ch2.E3.m1.2.2.2.1.1.1.2.2" xref="Ch2.E3.m1.2.2.2.1.1.1.2.2.cmml">M</mi><mrow id="Ch2.E3.m1.2.2.2.1.1.1.2.3" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.cmml"><mi id="Ch2.E3.m1.2.2.2.1.1.1.2.3.2" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.2.cmml">p</mi><mo id="Ch2.E3.m1.2.2.2.1.1.1.2.3.1" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.2.2.1.1.1.2.3.3" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.3.cmml">r</mi><mo id="Ch2.E3.m1.2.2.2.1.1.1.2.3.1a" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.2.2.1.1.1.2.3.4" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.4.cmml">e</mi><mo id="Ch2.E3.m1.2.2.2.1.1.1.2.3.1b" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.2.2.1.1.1.2.3.5" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.5.cmml">d</mi></mrow></msub><mo id="Ch2.E3.m1.2.2.2.1.1.1.1" xref="Ch2.E3.m1.2.2.2.1.1.1.1.cmml">∪</mo><msub id="Ch2.E3.m1.2.2.2.1.1.1.3" xref="Ch2.E3.m1.2.2.2.1.1.1.3.cmml"><mi id="Ch2.E3.m1.2.2.2.1.1.1.3.2" xref="Ch2.E3.m1.2.2.2.1.1.1.3.2.cmml">M</mi><mrow id="Ch2.E3.m1.2.2.2.1.1.1.3.3" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.cmml"><mi id="Ch2.E3.m1.2.2.2.1.1.1.3.3.2" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.2.cmml">g</mi><mo id="Ch2.E3.m1.2.2.2.1.1.1.3.3.1" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch2.E3.m1.2.2.2.1.1.1.3.3.3" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="Ch2.E3.m1.2.2.2.1.1.3" stretchy="false" xref="Ch2.E3.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Ch2.E3.m1.2b"><apply id="Ch2.E3.m1.2.3.cmml" xref="Ch2.E3.m1.2.3"><eq id="Ch2.E3.m1.2.3.1.cmml" xref="Ch2.E3.m1.2.3.1"></eq><apply id="Ch2.E3.m1.2.3.2.cmml" xref="Ch2.E3.m1.2.3.2"><csymbol cd="ambiguous" id="Ch2.E3.m1.2.3.2.1.cmml" xref="Ch2.E3.m1.2.3.2">subscript</csymbol><ci id="Ch2.E3.m1.2.3.2.2.cmml" xref="Ch2.E3.m1.2.3.2.2">𝑀</ci><apply id="Ch2.E3.m1.2.3.2.3.cmml" xref="Ch2.E3.m1.2.3.2.3"><times id="Ch2.E3.m1.2.3.2.3.1.cmml" xref="Ch2.E3.m1.2.3.2.3.1"></times><ci id="Ch2.E3.m1.2.3.2.3.2.cmml" xref="Ch2.E3.m1.2.3.2.3.2">𝐼</ci><ci id="Ch2.E3.m1.2.3.2.3.3.cmml" xref="Ch2.E3.m1.2.3.2.3.3">𝑜</ci><ci id="Ch2.E3.m1.2.3.2.3.4.cmml" xref="Ch2.E3.m1.2.3.2.3.4">𝑈</ci></apply></apply><apply id="Ch2.E3.m1.2.2.cmml" xref="Ch2.E3.m1.2.2"><divide id="Ch2.E3.m1.2.2.3.cmml" xref="Ch2.E3.m1.2.2"></divide><apply id="Ch2.E3.m1.1.1.1.cmml" xref="Ch2.E3.m1.1.1.1"><times id="Ch2.E3.m1.1.1.1.2.cmml" xref="Ch2.E3.m1.1.1.1.2"></times><ci id="Ch2.E3.m1.1.1.1.3a.cmml" xref="Ch2.E3.m1.1.1.1.3"><mtext id="Ch2.E3.m1.1.1.1.3.cmml" xref="Ch2.E3.m1.1.1.1.3">area</mtext></ci><apply id="Ch2.E3.m1.1.1.1.1.1.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1"><intersect id="Ch2.E3.m1.1.1.1.1.1.1.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.1"></intersect><apply id="Ch2.E3.m1.1.1.1.1.1.1.2.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Ch2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Ch2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.2">𝑀</ci><apply id="Ch2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3"><times id="Ch2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="Ch2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.2">𝑝</ci><ci id="Ch2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.3">𝑟</ci><ci id="Ch2.E3.m1.1.1.1.1.1.1.2.3.4.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.4">𝑒</ci><ci id="Ch2.E3.m1.1.1.1.1.1.1.2.3.5.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.2.3.5">𝑑</ci></apply></apply><apply id="Ch2.E3.m1.1.1.1.1.1.1.3.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Ch2.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Ch2.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3.2">𝑀</ci><apply id="Ch2.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3"><times id="Ch2.E3.m1.1.1.1.1.1.1.3.3.1.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.1"></times><ci id="Ch2.E3.m1.1.1.1.1.1.1.3.3.2.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.2">𝑔</ci><ci id="Ch2.E3.m1.1.1.1.1.1.1.3.3.3.cmml" xref="Ch2.E3.m1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="Ch2.E3.m1.2.2.2.cmml" xref="Ch2.E3.m1.2.2.2"><times id="Ch2.E3.m1.2.2.2.2.cmml" xref="Ch2.E3.m1.2.2.2.2"></times><ci id="Ch2.E3.m1.2.2.2.3a.cmml" xref="Ch2.E3.m1.2.2.2.3"><mtext id="Ch2.E3.m1.2.2.2.3.cmml" xref="Ch2.E3.m1.2.2.2.3">area</mtext></ci><apply id="Ch2.E3.m1.2.2.2.1.1.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1"><union id="Ch2.E3.m1.2.2.2.1.1.1.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.1"></union><apply id="Ch2.E3.m1.2.2.2.1.1.1.2.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="Ch2.E3.m1.2.2.2.1.1.1.2.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="Ch2.E3.m1.2.2.2.1.1.1.2.2.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.2">𝑀</ci><apply id="Ch2.E3.m1.2.2.2.1.1.1.2.3.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3"><times id="Ch2.E3.m1.2.2.2.1.1.1.2.3.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.1"></times><ci id="Ch2.E3.m1.2.2.2.1.1.1.2.3.2.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.2">𝑝</ci><ci id="Ch2.E3.m1.2.2.2.1.1.1.2.3.3.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.3">𝑟</ci><ci id="Ch2.E3.m1.2.2.2.1.1.1.2.3.4.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.4">𝑒</ci><ci id="Ch2.E3.m1.2.2.2.1.1.1.2.3.5.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.2.3.5">𝑑</ci></apply></apply><apply id="Ch2.E3.m1.2.2.2.1.1.1.3.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="Ch2.E3.m1.2.2.2.1.1.1.3.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3">subscript</csymbol><ci id="Ch2.E3.m1.2.2.2.1.1.1.3.2.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3.2">𝑀</ci><apply id="Ch2.E3.m1.2.2.2.1.1.1.3.3.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3"><times id="Ch2.E3.m1.2.2.2.1.1.1.3.3.1.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.1"></times><ci id="Ch2.E3.m1.2.2.2.1.1.1.3.3.2.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.2">𝑔</ci><ci id="Ch2.E3.m1.2.2.2.1.1.1.3.3.3.cmml" xref="Ch2.E3.m1.2.2.2.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.E3.m1.2c">M_{IoU}=\frac{\text{area}(M_{pred}\cap M_{gt})}{\text{area}(M_{pred}\cup M_{gt%
})}</annotation><annotation encoding="application/x-llamapun" id="Ch2.E3.m1.2d">italic_M start_POSTSUBSCRIPT italic_I italic_o italic_U end_POSTSUBSCRIPT = divide start_ARG area ( italic_M start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT ∩ italic_M start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG area ( italic_M start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT ∪ italic_M start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3)</span></td>
</tr></tbody>
</table>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div class="ltx_para" id="Ch2.S2.SS3.p4">
<p class="ltx_p" id="Ch2.S2.SS3.p4.4">This ratio ranges from 0 to 1, where 0 indicates no overlap and 1 indicates perfect overlap. In practice, an <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> threshold is set (e.g., 0.5 or 50%) to classify predictions as true positives or false positives. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">NMS</span></a> filters the best final prediction from the possible proposals, represented as <math alttext="P_{final}=NMS(P,S,\tau)" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p4.1.m1.3"><semantics id="Ch2.S2.SS3.p4.1.m1.3a"><mrow id="Ch2.S2.SS3.p4.1.m1.3.4" xref="Ch2.S2.SS3.p4.1.m1.3.4.cmml"><msub id="Ch2.S2.SS3.p4.1.m1.3.4.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.cmml"><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.2.cmml">P</mi><mrow id="Ch2.S2.SS3.p4.1.m1.3.4.2.3" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.cmml"><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.2.cmml">f</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.3" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.3.cmml">i</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1a" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.4" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.4.cmml">n</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1b" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.5" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.5.cmml">a</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1c" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.6" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.6.cmml">l</mi></mrow></msub><mo id="Ch2.S2.SS3.p4.1.m1.3.4.1" xref="Ch2.S2.SS3.p4.1.m1.3.4.1.cmml">=</mo><mrow id="Ch2.S2.SS3.p4.1.m1.3.4.3" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.cmml"><mi id="Ch2.S2.SS3.p4.1.m1.3.4.3.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.2.cmml">N</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.1" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.3.3" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.3.cmml">M</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.1a" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.1.cmml">⁢</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.4.3.4" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.4.cmml">S</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.1b" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.1.cmml">⁢</mo><mrow id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml"><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2.1" stretchy="false" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml">(</mo><mi id="Ch2.S2.SS3.p4.1.m1.1.1" xref="Ch2.S2.SS3.p4.1.m1.1.1.cmml">P</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2.2" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml">,</mo><mi id="Ch2.S2.SS3.p4.1.m1.2.2" xref="Ch2.S2.SS3.p4.1.m1.2.2.cmml">S</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2.3" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml">,</mo><mi id="Ch2.S2.SS3.p4.1.m1.3.3" xref="Ch2.S2.SS3.p4.1.m1.3.3.cmml">τ</mi><mo id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2.4" stretchy="false" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p4.1.m1.3b"><apply id="Ch2.S2.SS3.p4.1.m1.3.4.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4"><eq id="Ch2.S2.SS3.p4.1.m1.3.4.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.1"></eq><apply id="Ch2.S2.SS3.p4.1.m1.3.4.2.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2"><csymbol cd="ambiguous" id="Ch2.S2.SS3.p4.1.m1.3.4.2.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2">subscript</csymbol><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.2.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.2">𝑃</ci><apply id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3"><times id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.1"></times><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.2.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.2">𝑓</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.3.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.3">𝑖</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.4.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.4">𝑛</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.5.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.5">𝑎</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.2.3.6.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.2.3.6">𝑙</ci></apply></apply><apply id="Ch2.S2.SS3.p4.1.m1.3.4.3.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3"><times id="Ch2.S2.SS3.p4.1.m1.3.4.3.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.1"></times><ci id="Ch2.S2.SS3.p4.1.m1.3.4.3.2.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.2">𝑁</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.3.3.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.3">𝑀</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.4.3.4.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.4">𝑆</ci><vector id="Ch2.S2.SS3.p4.1.m1.3.4.3.5.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.4.3.5.2"><ci id="Ch2.S2.SS3.p4.1.m1.1.1.cmml" xref="Ch2.S2.SS3.p4.1.m1.1.1">𝑃</ci><ci id="Ch2.S2.SS3.p4.1.m1.2.2.cmml" xref="Ch2.S2.SS3.p4.1.m1.2.2">𝑆</ci><ci id="Ch2.S2.SS3.p4.1.m1.3.3.cmml" xref="Ch2.S2.SS3.p4.1.m1.3.3">𝜏</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p4.1.m1.3c">P_{final}=NMS(P,S,\tau)</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p4.1.m1.3d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT = italic_N italic_M italic_S ( italic_P , italic_S , italic_τ )</annotation></semantics></math>, for a set of predictions <math alttext="P" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p4.2.m2.1"><semantics id="Ch2.S2.SS3.p4.2.m2.1a"><mi id="Ch2.S2.SS3.p4.2.m2.1.1" xref="Ch2.S2.SS3.p4.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p4.2.m2.1b"><ci id="Ch2.S2.SS3.p4.2.m2.1.1.cmml" xref="Ch2.S2.SS3.p4.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p4.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p4.2.m2.1d">italic_P</annotation></semantics></math> (either masks or bounding boxes) with associated confidence scores <math alttext="S" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p4.3.m3.1"><semantics id="Ch2.S2.SS3.p4.3.m3.1a"><mi id="Ch2.S2.SS3.p4.3.m3.1.1" xref="Ch2.S2.SS3.p4.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p4.3.m3.1b"><ci id="Ch2.S2.SS3.p4.3.m3.1.1.cmml" xref="Ch2.S2.SS3.p4.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p4.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p4.3.m3.1d">italic_S</annotation></semantics></math> and an <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> threshold <math alttext="\tau" class="ltx_Math" display="inline" id="Ch2.S2.SS3.p4.4.m4.1"><semantics id="Ch2.S2.SS3.p4.4.m4.1a"><mi id="Ch2.S2.SS3.p4.4.m4.1.1" xref="Ch2.S2.SS3.p4.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS3.p4.4.m4.1b"><ci id="Ch2.S2.SS3.p4.4.m4.1.1.cmml" xref="Ch2.S2.SS3.p4.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS3.p4.4.m4.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS3.p4.4.m4.1d">italic_τ</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS3.p5">
<p class="ltx_p" id="Ch2.S2.SS3.p5.1">Following the discussion of object recognition architectures and postprocessing, it becomes relevant to address the practical aspects of implementing these frameworks. PyTorch <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib55" title="">NEURIPS2019_9015 </a></cite> is a popular deep learning library for computer vision, valued for its dynamic computation graph and efficient <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> memory management. Its straightforward syntax simplifies the implementation of supervised <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a>, making it ideal for research and development. This thesis leverages PyTorch to develop models for ship recognition.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S2.SS4">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">2.2.4   Training Process</h4>
<figure class="ltx_figure" id="Ch2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="209" id="Ch2.F6.g1" src="extracted/5906916/fig/training_diagram.png" width="393"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch2.F6.5.4.1" style="font-size:90%;">Figure 2.6</span>: </span><span class="ltx_text" id="Ch2.F6.3.3" style="font-size:90%;">Schematic of the Convolutional Neural Network (CNN) Training Process. An input image is passed through the CNN during the forward pass, resulting in a predicted output (<math alttext="y_{pred}" class="ltx_Math" display="inline" id="Ch2.F6.1.1.m1.1"><semantics id="Ch2.F6.1.1.m1.1c"><msub id="Ch2.F6.1.1.m1.1.1" xref="Ch2.F6.1.1.m1.1.1.cmml"><mi id="Ch2.F6.1.1.m1.1.1.2" xref="Ch2.F6.1.1.m1.1.1.2.cmml">y</mi><mrow id="Ch2.F6.1.1.m1.1.1.3" xref="Ch2.F6.1.1.m1.1.1.3.cmml"><mi id="Ch2.F6.1.1.m1.1.1.3.2" xref="Ch2.F6.1.1.m1.1.1.3.2.cmml">p</mi><mo id="Ch2.F6.1.1.m1.1.1.3.1" xref="Ch2.F6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.1.1.m1.1.1.3.3" xref="Ch2.F6.1.1.m1.1.1.3.3.cmml">r</mi><mo id="Ch2.F6.1.1.m1.1.1.3.1c" xref="Ch2.F6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.1.1.m1.1.1.3.4" xref="Ch2.F6.1.1.m1.1.1.3.4.cmml">e</mi><mo id="Ch2.F6.1.1.m1.1.1.3.1d" xref="Ch2.F6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.1.1.m1.1.1.3.5" xref="Ch2.F6.1.1.m1.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch2.F6.1.1.m1.1d"><apply id="Ch2.F6.1.1.m1.1.1.cmml" xref="Ch2.F6.1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch2.F6.1.1.m1.1.1.1.cmml" xref="Ch2.F6.1.1.m1.1.1">subscript</csymbol><ci id="Ch2.F6.1.1.m1.1.1.2.cmml" xref="Ch2.F6.1.1.m1.1.1.2">𝑦</ci><apply id="Ch2.F6.1.1.m1.1.1.3.cmml" xref="Ch2.F6.1.1.m1.1.1.3"><times id="Ch2.F6.1.1.m1.1.1.3.1.cmml" xref="Ch2.F6.1.1.m1.1.1.3.1"></times><ci id="Ch2.F6.1.1.m1.1.1.3.2.cmml" xref="Ch2.F6.1.1.m1.1.1.3.2">𝑝</ci><ci id="Ch2.F6.1.1.m1.1.1.3.3.cmml" xref="Ch2.F6.1.1.m1.1.1.3.3">𝑟</ci><ci id="Ch2.F6.1.1.m1.1.1.3.4.cmml" xref="Ch2.F6.1.1.m1.1.1.3.4">𝑒</ci><ci id="Ch2.F6.1.1.m1.1.1.3.5.cmml" xref="Ch2.F6.1.1.m1.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F6.1.1.m1.1e">y_{pred}</annotation><annotation encoding="application/x-llamapun" id="Ch2.F6.1.1.m1.1f">italic_y start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math>). The prediction is compared to the true value (<math alttext="y" class="ltx_Math" display="inline" id="Ch2.F6.2.2.m2.1"><semantics id="Ch2.F6.2.2.m2.1c"><mi id="Ch2.F6.2.2.m2.1.1" xref="Ch2.F6.2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Ch2.F6.2.2.m2.1d"><ci id="Ch2.F6.2.2.m2.1.1.cmml" xref="Ch2.F6.2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F6.2.2.m2.1e">y</annotation><annotation encoding="application/x-llamapun" id="Ch2.F6.2.2.m2.1f">italic_y</annotation></semantics></math>) using a loss function (<math alttext="L(y_{pred},y)" class="ltx_Math" display="inline" id="Ch2.F6.3.3.m3.2"><semantics id="Ch2.F6.3.3.m3.2c"><mrow id="Ch2.F6.3.3.m3.2.2" xref="Ch2.F6.3.3.m3.2.2.cmml"><mi id="Ch2.F6.3.3.m3.2.2.3" xref="Ch2.F6.3.3.m3.2.2.3.cmml">L</mi><mo id="Ch2.F6.3.3.m3.2.2.2" xref="Ch2.F6.3.3.m3.2.2.2.cmml">⁢</mo><mrow id="Ch2.F6.3.3.m3.2.2.1.1" xref="Ch2.F6.3.3.m3.2.2.1.2.cmml"><mo id="Ch2.F6.3.3.m3.2.2.1.1.2" stretchy="false" xref="Ch2.F6.3.3.m3.2.2.1.2.cmml">(</mo><msub id="Ch2.F6.3.3.m3.2.2.1.1.1" xref="Ch2.F6.3.3.m3.2.2.1.1.1.cmml"><mi id="Ch2.F6.3.3.m3.2.2.1.1.1.2" xref="Ch2.F6.3.3.m3.2.2.1.1.1.2.cmml">y</mi><mrow id="Ch2.F6.3.3.m3.2.2.1.1.1.3" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.cmml"><mi id="Ch2.F6.3.3.m3.2.2.1.1.1.3.2" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.2.cmml">p</mi><mo id="Ch2.F6.3.3.m3.2.2.1.1.1.3.1" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.3.3.m3.2.2.1.1.1.3.3" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.3.cmml">r</mi><mo id="Ch2.F6.3.3.m3.2.2.1.1.1.3.1c" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.3.3.m3.2.2.1.1.1.3.4" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.4.cmml">e</mi><mo id="Ch2.F6.3.3.m3.2.2.1.1.1.3.1d" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="Ch2.F6.3.3.m3.2.2.1.1.1.3.5" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.5.cmml">d</mi></mrow></msub><mo id="Ch2.F6.3.3.m3.2.2.1.1.3" xref="Ch2.F6.3.3.m3.2.2.1.2.cmml">,</mo><mi id="Ch2.F6.3.3.m3.1.1" xref="Ch2.F6.3.3.m3.1.1.cmml">y</mi><mo id="Ch2.F6.3.3.m3.2.2.1.1.4" stretchy="false" xref="Ch2.F6.3.3.m3.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.F6.3.3.m3.2d"><apply id="Ch2.F6.3.3.m3.2.2.cmml" xref="Ch2.F6.3.3.m3.2.2"><times id="Ch2.F6.3.3.m3.2.2.2.cmml" xref="Ch2.F6.3.3.m3.2.2.2"></times><ci id="Ch2.F6.3.3.m3.2.2.3.cmml" xref="Ch2.F6.3.3.m3.2.2.3">𝐿</ci><interval closure="open" id="Ch2.F6.3.3.m3.2.2.1.2.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1"><apply id="Ch2.F6.3.3.m3.2.2.1.1.1.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="Ch2.F6.3.3.m3.2.2.1.1.1.1.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1">subscript</csymbol><ci id="Ch2.F6.3.3.m3.2.2.1.1.1.2.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.2">𝑦</ci><apply id="Ch2.F6.3.3.m3.2.2.1.1.1.3.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3"><times id="Ch2.F6.3.3.m3.2.2.1.1.1.3.1.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.1"></times><ci id="Ch2.F6.3.3.m3.2.2.1.1.1.3.2.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.2">𝑝</ci><ci id="Ch2.F6.3.3.m3.2.2.1.1.1.3.3.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.3">𝑟</ci><ci id="Ch2.F6.3.3.m3.2.2.1.1.1.3.4.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.4">𝑒</ci><ci id="Ch2.F6.3.3.m3.2.2.1.1.1.3.5.cmml" xref="Ch2.F6.3.3.m3.2.2.1.1.1.3.5">𝑑</ci></apply></apply><ci id="Ch2.F6.3.3.m3.1.1.cmml" xref="Ch2.F6.3.3.m3.1.1">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.F6.3.3.m3.2e">L(y_{pred},y)</annotation><annotation encoding="application/x-llamapun" id="Ch2.F6.3.3.m3.2f">italic_L ( italic_y start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT , italic_y )</annotation></semantics></math>), and the error is propagated back through the network during backpropagation to adjust and improve the model weights.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch2.S2.SS4.p1">
<p class="ltx_p" id="Ch2.S2.SS4.p1.1">The training process of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNNs</span></a> for object detection and segmentation includes forward and backward propagation, as illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2.F6" title="Figure 2.6 ‣ 2.2.4 Training Process ‣ 2.2 Deep-Learning-Based Object Recognition ‣ Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2.6</span></a>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS4.p2">
<p class="ltx_p" id="Ch2.S2.SS4.p2.1">During forward propagation, the input data is fed through the network to output a prediction. The choice of loss function is crucial for model performance. Commonly used loss functions for different tasks include Binary Cross-Entropy Loss, Focal Loss, Bounding Box Loss, Objectness Loss, and Pixel-wise Cross-Entropy. Each of these loss functions addresses specific aspects of the prediction problem, such as class imbalance (Focal Loss) or spatial localization accuracy (Bounding Box Loss).
These loss functions, sometimes used in combination or with other loss functions, help the model learn from its mistakes and achieve optimal performance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib31" title="">talaei2023deep </a></cite>.
Further description and mathematical definitions of these loss functions can be found in reference <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS4.p3">
<p class="ltx_p" id="Ch2.S2.SS4.p3.1">Backward propagation, based on a loss function, then adjusts the network weights to minimize discrepancies between the predictions and the ground truth <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib38" title="">lecun2015deep </a></cite>. This adjustment process involves calculating gradients of the loss function with respect to the network parameters. These gradients indicate the direction and magnitude of the changes needed to reduce the loss.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS4.p4">
<p class="ltx_p" id="Ch2.S2.SS4.p4.1">Optimization algorithms use these gradients to update the network parameters iteratively. Common optimization algorithms include Stochastic Gradient Descent (SGD), Adam, RMSprop, and AdaGrad <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib30" title="">goodfellow2016deep </a></cite>. Each algorithm has its strengths and is chosen based on the specific requirements of the task. The goal is to minimize the loss function, thereby improving the performance of the model. The learning rate, a key factor in this process, determines the size of the updates. Optimization involves multiple passes through the dataset, known as epochs, where the network parameters are refined to achieve better accuracy and generalization <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib30" title="">goodfellow2016deep </a></cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S2.SS5">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">2.2.5   Evaluation Metrics</h4>
<div class="ltx_para" id="Ch2.S2.SS5.p1">
<p class="ltx_p" id="Ch2.S2.SS5.p1.1">Evaluating the performance of object recognition models is critical to understanding their effectiveness and accuracy.
The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mean Average Precision (mAP)</span></a> is a commonly-used metric to evaluate object detection and segmentation performance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite>.
Expressed in percentage, it is calculated as the mean of all the Average Precisions (AP) for all classes present in the dataset at a given <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> threshold. This is, mathematically:</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p2">
<table class="ltx_equation ltx_eqn_table" id="Ch2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="mAP_{\tau}=\frac{1}{C}\sum_{c=1}^{C}AP_{\tau,c}" class="ltx_Math" display="block" id="Ch2.E4.m1.2"><semantics id="Ch2.E4.m1.2a"><mrow id="Ch2.E4.m1.2.3" xref="Ch2.E4.m1.2.3.cmml"><mrow id="Ch2.E4.m1.2.3.2" xref="Ch2.E4.m1.2.3.2.cmml"><mi id="Ch2.E4.m1.2.3.2.2" xref="Ch2.E4.m1.2.3.2.2.cmml">m</mi><mo id="Ch2.E4.m1.2.3.2.1" xref="Ch2.E4.m1.2.3.2.1.cmml">⁢</mo><mi id="Ch2.E4.m1.2.3.2.3" xref="Ch2.E4.m1.2.3.2.3.cmml">A</mi><mo id="Ch2.E4.m1.2.3.2.1a" xref="Ch2.E4.m1.2.3.2.1.cmml">⁢</mo><msub id="Ch2.E4.m1.2.3.2.4" xref="Ch2.E4.m1.2.3.2.4.cmml"><mi id="Ch2.E4.m1.2.3.2.4.2" xref="Ch2.E4.m1.2.3.2.4.2.cmml">P</mi><mi id="Ch2.E4.m1.2.3.2.4.3" xref="Ch2.E4.m1.2.3.2.4.3.cmml">τ</mi></msub></mrow><mo id="Ch2.E4.m1.2.3.1" xref="Ch2.E4.m1.2.3.1.cmml">=</mo><mrow id="Ch2.E4.m1.2.3.3" xref="Ch2.E4.m1.2.3.3.cmml"><mfrac id="Ch2.E4.m1.2.3.3.2" xref="Ch2.E4.m1.2.3.3.2.cmml"><mn id="Ch2.E4.m1.2.3.3.2.2" xref="Ch2.E4.m1.2.3.3.2.2.cmml">1</mn><mi id="Ch2.E4.m1.2.3.3.2.3" xref="Ch2.E4.m1.2.3.3.2.3.cmml">C</mi></mfrac><mo id="Ch2.E4.m1.2.3.3.1" xref="Ch2.E4.m1.2.3.3.1.cmml">⁢</mo><mrow id="Ch2.E4.m1.2.3.3.3" xref="Ch2.E4.m1.2.3.3.3.cmml"><munderover id="Ch2.E4.m1.2.3.3.3.1" xref="Ch2.E4.m1.2.3.3.3.1.cmml"><mo id="Ch2.E4.m1.2.3.3.3.1.2.2" movablelimits="false" xref="Ch2.E4.m1.2.3.3.3.1.2.2.cmml">∑</mo><mrow id="Ch2.E4.m1.2.3.3.3.1.2.3" xref="Ch2.E4.m1.2.3.3.3.1.2.3.cmml"><mi id="Ch2.E4.m1.2.3.3.3.1.2.3.2" xref="Ch2.E4.m1.2.3.3.3.1.2.3.2.cmml">c</mi><mo id="Ch2.E4.m1.2.3.3.3.1.2.3.1" xref="Ch2.E4.m1.2.3.3.3.1.2.3.1.cmml">=</mo><mn id="Ch2.E4.m1.2.3.3.3.1.2.3.3" xref="Ch2.E4.m1.2.3.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="Ch2.E4.m1.2.3.3.3.1.3" xref="Ch2.E4.m1.2.3.3.3.1.3.cmml">C</mi></munderover><mrow id="Ch2.E4.m1.2.3.3.3.2" xref="Ch2.E4.m1.2.3.3.3.2.cmml"><mi id="Ch2.E4.m1.2.3.3.3.2.2" xref="Ch2.E4.m1.2.3.3.3.2.2.cmml">A</mi><mo id="Ch2.E4.m1.2.3.3.3.2.1" xref="Ch2.E4.m1.2.3.3.3.2.1.cmml">⁢</mo><msub id="Ch2.E4.m1.2.3.3.3.2.3" xref="Ch2.E4.m1.2.3.3.3.2.3.cmml"><mi id="Ch2.E4.m1.2.3.3.3.2.3.2" xref="Ch2.E4.m1.2.3.3.3.2.3.2.cmml">P</mi><mrow id="Ch2.E4.m1.2.2.2.4" xref="Ch2.E4.m1.2.2.2.3.cmml"><mi id="Ch2.E4.m1.1.1.1.1" xref="Ch2.E4.m1.1.1.1.1.cmml">τ</mi><mo id="Ch2.E4.m1.2.2.2.4.1" xref="Ch2.E4.m1.2.2.2.3.cmml">,</mo><mi id="Ch2.E4.m1.2.2.2.2" xref="Ch2.E4.m1.2.2.2.2.cmml">c</mi></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.E4.m1.2b"><apply id="Ch2.E4.m1.2.3.cmml" xref="Ch2.E4.m1.2.3"><eq id="Ch2.E4.m1.2.3.1.cmml" xref="Ch2.E4.m1.2.3.1"></eq><apply id="Ch2.E4.m1.2.3.2.cmml" xref="Ch2.E4.m1.2.3.2"><times id="Ch2.E4.m1.2.3.2.1.cmml" xref="Ch2.E4.m1.2.3.2.1"></times><ci id="Ch2.E4.m1.2.3.2.2.cmml" xref="Ch2.E4.m1.2.3.2.2">𝑚</ci><ci id="Ch2.E4.m1.2.3.2.3.cmml" xref="Ch2.E4.m1.2.3.2.3">𝐴</ci><apply id="Ch2.E4.m1.2.3.2.4.cmml" xref="Ch2.E4.m1.2.3.2.4"><csymbol cd="ambiguous" id="Ch2.E4.m1.2.3.2.4.1.cmml" xref="Ch2.E4.m1.2.3.2.4">subscript</csymbol><ci id="Ch2.E4.m1.2.3.2.4.2.cmml" xref="Ch2.E4.m1.2.3.2.4.2">𝑃</ci><ci id="Ch2.E4.m1.2.3.2.4.3.cmml" xref="Ch2.E4.m1.2.3.2.4.3">𝜏</ci></apply></apply><apply id="Ch2.E4.m1.2.3.3.cmml" xref="Ch2.E4.m1.2.3.3"><times id="Ch2.E4.m1.2.3.3.1.cmml" xref="Ch2.E4.m1.2.3.3.1"></times><apply id="Ch2.E4.m1.2.3.3.2.cmml" xref="Ch2.E4.m1.2.3.3.2"><divide id="Ch2.E4.m1.2.3.3.2.1.cmml" xref="Ch2.E4.m1.2.3.3.2"></divide><cn id="Ch2.E4.m1.2.3.3.2.2.cmml" type="integer" xref="Ch2.E4.m1.2.3.3.2.2">1</cn><ci id="Ch2.E4.m1.2.3.3.2.3.cmml" xref="Ch2.E4.m1.2.3.3.2.3">𝐶</ci></apply><apply id="Ch2.E4.m1.2.3.3.3.cmml" xref="Ch2.E4.m1.2.3.3.3"><apply id="Ch2.E4.m1.2.3.3.3.1.cmml" xref="Ch2.E4.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="Ch2.E4.m1.2.3.3.3.1.1.cmml" xref="Ch2.E4.m1.2.3.3.3.1">superscript</csymbol><apply id="Ch2.E4.m1.2.3.3.3.1.2.cmml" xref="Ch2.E4.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="Ch2.E4.m1.2.3.3.3.1.2.1.cmml" xref="Ch2.E4.m1.2.3.3.3.1">subscript</csymbol><sum id="Ch2.E4.m1.2.3.3.3.1.2.2.cmml" xref="Ch2.E4.m1.2.3.3.3.1.2.2"></sum><apply id="Ch2.E4.m1.2.3.3.3.1.2.3.cmml" xref="Ch2.E4.m1.2.3.3.3.1.2.3"><eq id="Ch2.E4.m1.2.3.3.3.1.2.3.1.cmml" xref="Ch2.E4.m1.2.3.3.3.1.2.3.1"></eq><ci id="Ch2.E4.m1.2.3.3.3.1.2.3.2.cmml" xref="Ch2.E4.m1.2.3.3.3.1.2.3.2">𝑐</ci><cn id="Ch2.E4.m1.2.3.3.3.1.2.3.3.cmml" type="integer" xref="Ch2.E4.m1.2.3.3.3.1.2.3.3">1</cn></apply></apply><ci id="Ch2.E4.m1.2.3.3.3.1.3.cmml" xref="Ch2.E4.m1.2.3.3.3.1.3">𝐶</ci></apply><apply id="Ch2.E4.m1.2.3.3.3.2.cmml" xref="Ch2.E4.m1.2.3.3.3.2"><times id="Ch2.E4.m1.2.3.3.3.2.1.cmml" xref="Ch2.E4.m1.2.3.3.3.2.1"></times><ci id="Ch2.E4.m1.2.3.3.3.2.2.cmml" xref="Ch2.E4.m1.2.3.3.3.2.2">𝐴</ci><apply id="Ch2.E4.m1.2.3.3.3.2.3.cmml" xref="Ch2.E4.m1.2.3.3.3.2.3"><csymbol cd="ambiguous" id="Ch2.E4.m1.2.3.3.3.2.3.1.cmml" xref="Ch2.E4.m1.2.3.3.3.2.3">subscript</csymbol><ci id="Ch2.E4.m1.2.3.3.3.2.3.2.cmml" xref="Ch2.E4.m1.2.3.3.3.2.3.2">𝑃</ci><list id="Ch2.E4.m1.2.2.2.3.cmml" xref="Ch2.E4.m1.2.2.2.4"><ci id="Ch2.E4.m1.1.1.1.1.cmml" xref="Ch2.E4.m1.1.1.1.1">𝜏</ci><ci id="Ch2.E4.m1.2.2.2.2.cmml" xref="Ch2.E4.m1.2.2.2.2">𝑐</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.E4.m1.2c">mAP_{\tau}=\frac{1}{C}\sum_{c=1}^{C}AP_{\tau,c}</annotation><annotation encoding="application/x-llamapun" id="Ch2.E4.m1.2d">italic_m italic_A italic_P start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_C end_ARG ∑ start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT italic_A italic_P start_POSTSUBSCRIPT italic_τ , italic_c end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p3">
<p class="ltx_p" id="Ch2.S2.SS5.p3.4">Here, <math alttext="mAP_{\tau}" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p3.1.m1.1"><semantics id="Ch2.S2.SS5.p3.1.m1.1a"><mrow id="Ch2.S2.SS5.p3.1.m1.1.1" xref="Ch2.S2.SS5.p3.1.m1.1.1.cmml"><mi id="Ch2.S2.SS5.p3.1.m1.1.1.2" xref="Ch2.S2.SS5.p3.1.m1.1.1.2.cmml">m</mi><mo id="Ch2.S2.SS5.p3.1.m1.1.1.1" xref="Ch2.S2.SS5.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.SS5.p3.1.m1.1.1.3" xref="Ch2.S2.SS5.p3.1.m1.1.1.3.cmml">A</mi><mo id="Ch2.S2.SS5.p3.1.m1.1.1.1a" xref="Ch2.S2.SS5.p3.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.SS5.p3.1.m1.1.1.4" xref="Ch2.S2.SS5.p3.1.m1.1.1.4.cmml"><mi id="Ch2.S2.SS5.p3.1.m1.1.1.4.2" xref="Ch2.S2.SS5.p3.1.m1.1.1.4.2.cmml">P</mi><mi id="Ch2.S2.SS5.p3.1.m1.1.1.4.3" xref="Ch2.S2.SS5.p3.1.m1.1.1.4.3.cmml">τ</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p3.1.m1.1b"><apply id="Ch2.S2.SS5.p3.1.m1.1.1.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1"><times id="Ch2.S2.SS5.p3.1.m1.1.1.1.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.1"></times><ci id="Ch2.S2.SS5.p3.1.m1.1.1.2.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.2">𝑚</ci><ci id="Ch2.S2.SS5.p3.1.m1.1.1.3.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.3">𝐴</ci><apply id="Ch2.S2.SS5.p3.1.m1.1.1.4.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.SS5.p3.1.m1.1.1.4.1.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.4">subscript</csymbol><ci id="Ch2.S2.SS5.p3.1.m1.1.1.4.2.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.4.2">𝑃</ci><ci id="Ch2.S2.SS5.p3.1.m1.1.1.4.3.cmml" xref="Ch2.S2.SS5.p3.1.m1.1.1.4.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p3.1.m1.1c">mAP_{\tau}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p3.1.m1.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math> represents the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> at an IoU threshold <math alttext="\tau" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p3.2.m2.1"><semantics id="Ch2.S2.SS5.p3.2.m2.1a"><mi id="Ch2.S2.SS5.p3.2.m2.1.1" xref="Ch2.S2.SS5.p3.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p3.2.m2.1b"><ci id="Ch2.S2.SS5.p3.2.m2.1.1.cmml" xref="Ch2.S2.SS5.p3.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p3.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p3.2.m2.1d">italic_τ</annotation></semantics></math>, calculated by averaging the AP values across all <math alttext="C" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p3.3.m3.1"><semantics id="Ch2.S2.SS5.p3.3.m3.1a"><mi id="Ch2.S2.SS5.p3.3.m3.1.1" xref="Ch2.S2.SS5.p3.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p3.3.m3.1b"><ci id="Ch2.S2.SS5.p3.3.m3.1.1.cmml" xref="Ch2.S2.SS5.p3.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p3.3.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p3.3.m3.1d">italic_C</annotation></semantics></math> classes. For the calculation of AP for each class, true positives are counted when the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> of the prediction exceeds the given threshold <math alttext="\tau" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p3.4.m4.1"><semantics id="Ch2.S2.SS5.p3.4.m4.1a"><mi id="Ch2.S2.SS5.p3.4.m4.1.1" xref="Ch2.S2.SS5.p3.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p3.4.m4.1b"><ci id="Ch2.S2.SS5.p3.4.m4.1.1.cmml" xref="Ch2.S2.SS5.p3.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p3.4.m4.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p3.4.m4.1d">italic_τ</annotation></semantics></math>. In the case of object detection, a true positive is confirmed when the  <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> of the predicted bounding box exceeds the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> threshold. For instance segmentation, true positives are based on the overlap between the predicted mask and the ground truth mask at the IoU threshold. This distinction in true positive calculation signifies the different evaluation approaches between object detection and instance segmentation.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p4">
<p class="ltx_p" id="Ch2.S2.SS5.p4.2">It is common in the field of object recognition to refer to <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> as a short form of <math alttext="mAP_{0.5:0.95}" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p4.1.m1.1"><semantics id="Ch2.S2.SS5.p4.1.m1.1a"><mrow id="Ch2.S2.SS5.p4.1.m1.1.1" xref="Ch2.S2.SS5.p4.1.m1.1.1.cmml"><mi id="Ch2.S2.SS5.p4.1.m1.1.1.2" xref="Ch2.S2.SS5.p4.1.m1.1.1.2.cmml">m</mi><mo id="Ch2.S2.SS5.p4.1.m1.1.1.1" xref="Ch2.S2.SS5.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.SS5.p4.1.m1.1.1.3" xref="Ch2.S2.SS5.p4.1.m1.1.1.3.cmml">A</mi><mo id="Ch2.S2.SS5.p4.1.m1.1.1.1a" xref="Ch2.S2.SS5.p4.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.SS5.p4.1.m1.1.1.4" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.cmml"><mi id="Ch2.S2.SS5.p4.1.m1.1.1.4.2" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.2.cmml">P</mi><mrow id="Ch2.S2.SS5.p4.1.m1.1.1.4.3" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.cmml"><mn id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.2" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.2.cmml">0.5</mn><mo id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.1" lspace="0.278em" rspace="0.278em" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.1.cmml">:</mo><mn id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.3" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.3.cmml">0.95</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p4.1.m1.1b"><apply id="Ch2.S2.SS5.p4.1.m1.1.1.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1"><times id="Ch2.S2.SS5.p4.1.m1.1.1.1.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.1"></times><ci id="Ch2.S2.SS5.p4.1.m1.1.1.2.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.2">𝑚</ci><ci id="Ch2.S2.SS5.p4.1.m1.1.1.3.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.3">𝐴</ci><apply id="Ch2.S2.SS5.p4.1.m1.1.1.4.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.SS5.p4.1.m1.1.1.4.1.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.4">subscript</csymbol><ci id="Ch2.S2.SS5.p4.1.m1.1.1.4.2.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.2">𝑃</ci><apply id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3"><ci id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.1.cmml" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.1">:</ci><cn id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.2.cmml" type="float" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.2">0.5</cn><cn id="Ch2.S2.SS5.p4.1.m1.1.1.4.3.3.cmml" type="float" xref="Ch2.S2.SS5.p4.1.m1.1.1.4.3.3">0.95</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p4.1.m1.1c">mAP_{0.5:0.95}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p4.1.m1.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT 0.5 : 0.95 end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>. The <math alttext="mAP_{0.5:0.95}" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p4.2.m2.1"><semantics id="Ch2.S2.SS5.p4.2.m2.1a"><mrow id="Ch2.S2.SS5.p4.2.m2.1.1" xref="Ch2.S2.SS5.p4.2.m2.1.1.cmml"><mi id="Ch2.S2.SS5.p4.2.m2.1.1.2" xref="Ch2.S2.SS5.p4.2.m2.1.1.2.cmml">m</mi><mo id="Ch2.S2.SS5.p4.2.m2.1.1.1" xref="Ch2.S2.SS5.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.SS5.p4.2.m2.1.1.3" xref="Ch2.S2.SS5.p4.2.m2.1.1.3.cmml">A</mi><mo id="Ch2.S2.SS5.p4.2.m2.1.1.1a" xref="Ch2.S2.SS5.p4.2.m2.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.SS5.p4.2.m2.1.1.4" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.cmml"><mi id="Ch2.S2.SS5.p4.2.m2.1.1.4.2" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.2.cmml">P</mi><mrow id="Ch2.S2.SS5.p4.2.m2.1.1.4.3" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.cmml"><mn id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.2" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.2.cmml">0.5</mn><mo id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.1" lspace="0.278em" rspace="0.278em" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.1.cmml">:</mo><mn id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.3" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.3.cmml">0.95</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p4.2.m2.1b"><apply id="Ch2.S2.SS5.p4.2.m2.1.1.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1"><times id="Ch2.S2.SS5.p4.2.m2.1.1.1.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.1"></times><ci id="Ch2.S2.SS5.p4.2.m2.1.1.2.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.2">𝑚</ci><ci id="Ch2.S2.SS5.p4.2.m2.1.1.3.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.3">𝐴</ci><apply id="Ch2.S2.SS5.p4.2.m2.1.1.4.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.SS5.p4.2.m2.1.1.4.1.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.4">subscript</csymbol><ci id="Ch2.S2.SS5.p4.2.m2.1.1.4.2.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.2">𝑃</ci><apply id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3"><ci id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.1.cmml" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.1">:</ci><cn id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.2.cmml" type="float" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.2">0.5</cn><cn id="Ch2.S2.SS5.p4.2.m2.1.1.4.3.3.cmml" type="float" xref="Ch2.S2.SS5.p4.2.m2.1.1.4.3.3">0.95</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p4.2.m2.1c">mAP_{0.5:0.95}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p4.2.m2.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT 0.5 : 0.95 end_POSTSUBSCRIPT</annotation></semantics></math> accounts for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> values at <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> thresholds that range from 0.5 to 0.95, in increments of 0.05. The formula would therefore be defined as:</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p5">
<table class="ltx_equation ltx_eqn_table" id="Ch2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="mAP=\frac{1}{N}\sum_{\tau=0.5}^{0.95}mAP_{\tau}" class="ltx_Math" display="block" id="Ch2.E5.m1.1"><semantics id="Ch2.E5.m1.1a"><mrow id="Ch2.E5.m1.1.1" xref="Ch2.E5.m1.1.1.cmml"><mrow id="Ch2.E5.m1.1.1.2" xref="Ch2.E5.m1.1.1.2.cmml"><mi id="Ch2.E5.m1.1.1.2.2" xref="Ch2.E5.m1.1.1.2.2.cmml">m</mi><mo id="Ch2.E5.m1.1.1.2.1" xref="Ch2.E5.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch2.E5.m1.1.1.2.3" xref="Ch2.E5.m1.1.1.2.3.cmml">A</mi><mo id="Ch2.E5.m1.1.1.2.1a" xref="Ch2.E5.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch2.E5.m1.1.1.2.4" xref="Ch2.E5.m1.1.1.2.4.cmml">P</mi></mrow><mo id="Ch2.E5.m1.1.1.1" xref="Ch2.E5.m1.1.1.1.cmml">=</mo><mrow id="Ch2.E5.m1.1.1.3" xref="Ch2.E5.m1.1.1.3.cmml"><mfrac id="Ch2.E5.m1.1.1.3.2" xref="Ch2.E5.m1.1.1.3.2.cmml"><mn id="Ch2.E5.m1.1.1.3.2.2" xref="Ch2.E5.m1.1.1.3.2.2.cmml">1</mn><mi id="Ch2.E5.m1.1.1.3.2.3" xref="Ch2.E5.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo id="Ch2.E5.m1.1.1.3.1" xref="Ch2.E5.m1.1.1.3.1.cmml">⁢</mo><mrow id="Ch2.E5.m1.1.1.3.3" xref="Ch2.E5.m1.1.1.3.3.cmml"><munderover id="Ch2.E5.m1.1.1.3.3.1" xref="Ch2.E5.m1.1.1.3.3.1.cmml"><mo id="Ch2.E5.m1.1.1.3.3.1.2.2" movablelimits="false" xref="Ch2.E5.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="Ch2.E5.m1.1.1.3.3.1.2.3" xref="Ch2.E5.m1.1.1.3.3.1.2.3.cmml"><mi id="Ch2.E5.m1.1.1.3.3.1.2.3.2" xref="Ch2.E5.m1.1.1.3.3.1.2.3.2.cmml">τ</mi><mo id="Ch2.E5.m1.1.1.3.3.1.2.3.1" xref="Ch2.E5.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="Ch2.E5.m1.1.1.3.3.1.2.3.3" xref="Ch2.E5.m1.1.1.3.3.1.2.3.3.cmml">0.5</mn></mrow><mn id="Ch2.E5.m1.1.1.3.3.1.3" xref="Ch2.E5.m1.1.1.3.3.1.3.cmml">0.95</mn></munderover><mrow id="Ch2.E5.m1.1.1.3.3.2" xref="Ch2.E5.m1.1.1.3.3.2.cmml"><mi id="Ch2.E5.m1.1.1.3.3.2.2" xref="Ch2.E5.m1.1.1.3.3.2.2.cmml">m</mi><mo id="Ch2.E5.m1.1.1.3.3.2.1" xref="Ch2.E5.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="Ch2.E5.m1.1.1.3.3.2.3" xref="Ch2.E5.m1.1.1.3.3.2.3.cmml">A</mi><mo id="Ch2.E5.m1.1.1.3.3.2.1a" xref="Ch2.E5.m1.1.1.3.3.2.1.cmml">⁢</mo><msub id="Ch2.E5.m1.1.1.3.3.2.4" xref="Ch2.E5.m1.1.1.3.3.2.4.cmml"><mi id="Ch2.E5.m1.1.1.3.3.2.4.2" xref="Ch2.E5.m1.1.1.3.3.2.4.2.cmml">P</mi><mi id="Ch2.E5.m1.1.1.3.3.2.4.3" xref="Ch2.E5.m1.1.1.3.3.2.4.3.cmml">τ</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch2.E5.m1.1b"><apply id="Ch2.E5.m1.1.1.cmml" xref="Ch2.E5.m1.1.1"><eq id="Ch2.E5.m1.1.1.1.cmml" xref="Ch2.E5.m1.1.1.1"></eq><apply id="Ch2.E5.m1.1.1.2.cmml" xref="Ch2.E5.m1.1.1.2"><times id="Ch2.E5.m1.1.1.2.1.cmml" xref="Ch2.E5.m1.1.1.2.1"></times><ci id="Ch2.E5.m1.1.1.2.2.cmml" xref="Ch2.E5.m1.1.1.2.2">𝑚</ci><ci id="Ch2.E5.m1.1.1.2.3.cmml" xref="Ch2.E5.m1.1.1.2.3">𝐴</ci><ci id="Ch2.E5.m1.1.1.2.4.cmml" xref="Ch2.E5.m1.1.1.2.4">𝑃</ci></apply><apply id="Ch2.E5.m1.1.1.3.cmml" xref="Ch2.E5.m1.1.1.3"><times id="Ch2.E5.m1.1.1.3.1.cmml" xref="Ch2.E5.m1.1.1.3.1"></times><apply id="Ch2.E5.m1.1.1.3.2.cmml" xref="Ch2.E5.m1.1.1.3.2"><divide id="Ch2.E5.m1.1.1.3.2.1.cmml" xref="Ch2.E5.m1.1.1.3.2"></divide><cn id="Ch2.E5.m1.1.1.3.2.2.cmml" type="integer" xref="Ch2.E5.m1.1.1.3.2.2">1</cn><ci id="Ch2.E5.m1.1.1.3.2.3.cmml" xref="Ch2.E5.m1.1.1.3.2.3">𝑁</ci></apply><apply id="Ch2.E5.m1.1.1.3.3.cmml" xref="Ch2.E5.m1.1.1.3.3"><apply id="Ch2.E5.m1.1.1.3.3.1.cmml" xref="Ch2.E5.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Ch2.E5.m1.1.1.3.3.1.1.cmml" xref="Ch2.E5.m1.1.1.3.3.1">superscript</csymbol><apply id="Ch2.E5.m1.1.1.3.3.1.2.cmml" xref="Ch2.E5.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="Ch2.E5.m1.1.1.3.3.1.2.1.cmml" xref="Ch2.E5.m1.1.1.3.3.1">subscript</csymbol><sum id="Ch2.E5.m1.1.1.3.3.1.2.2.cmml" xref="Ch2.E5.m1.1.1.3.3.1.2.2"></sum><apply id="Ch2.E5.m1.1.1.3.3.1.2.3.cmml" xref="Ch2.E5.m1.1.1.3.3.1.2.3"><eq id="Ch2.E5.m1.1.1.3.3.1.2.3.1.cmml" xref="Ch2.E5.m1.1.1.3.3.1.2.3.1"></eq><ci id="Ch2.E5.m1.1.1.3.3.1.2.3.2.cmml" xref="Ch2.E5.m1.1.1.3.3.1.2.3.2">𝜏</ci><cn id="Ch2.E5.m1.1.1.3.3.1.2.3.3.cmml" type="float" xref="Ch2.E5.m1.1.1.3.3.1.2.3.3">0.5</cn></apply></apply><cn id="Ch2.E5.m1.1.1.3.3.1.3.cmml" type="float" xref="Ch2.E5.m1.1.1.3.3.1.3">0.95</cn></apply><apply id="Ch2.E5.m1.1.1.3.3.2.cmml" xref="Ch2.E5.m1.1.1.3.3.2"><times id="Ch2.E5.m1.1.1.3.3.2.1.cmml" xref="Ch2.E5.m1.1.1.3.3.2.1"></times><ci id="Ch2.E5.m1.1.1.3.3.2.2.cmml" xref="Ch2.E5.m1.1.1.3.3.2.2">𝑚</ci><ci id="Ch2.E5.m1.1.1.3.3.2.3.cmml" xref="Ch2.E5.m1.1.1.3.3.2.3">𝐴</ci><apply id="Ch2.E5.m1.1.1.3.3.2.4.cmml" xref="Ch2.E5.m1.1.1.3.3.2.4"><csymbol cd="ambiguous" id="Ch2.E5.m1.1.1.3.3.2.4.1.cmml" xref="Ch2.E5.m1.1.1.3.3.2.4">subscript</csymbol><ci id="Ch2.E5.m1.1.1.3.3.2.4.2.cmml" xref="Ch2.E5.m1.1.1.3.3.2.4.2">𝑃</ci><ci id="Ch2.E5.m1.1.1.3.3.2.4.3.cmml" xref="Ch2.E5.m1.1.1.3.3.2.4.3">𝜏</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.E5.m1.1c">mAP=\frac{1}{N}\sum_{\tau=0.5}^{0.95}mAP_{\tau}</annotation><annotation encoding="application/x-llamapun" id="Ch2.E5.m1.1d">italic_m italic_A italic_P = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_τ = 0.5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0.95 end_POSTSUPERSCRIPT italic_m italic_A italic_P start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p6">
<p class="ltx_p" id="Ch2.S2.SS5.p6.2">Where <math alttext="N" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p6.1.m1.1"><semantics id="Ch2.S2.SS5.p6.1.m1.1a"><mi id="Ch2.S2.SS5.p6.1.m1.1.1" xref="Ch2.S2.SS5.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p6.1.m1.1b"><ci id="Ch2.S2.SS5.p6.1.m1.1.1.cmml" xref="Ch2.S2.SS5.p6.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p6.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p6.1.m1.1d">italic_N</annotation></semantics></math> represents the number of thresholds, which is 10 in the case of the range <math alttext="0.5:0.95" class="ltx_Math" display="inline" id="Ch2.S2.SS5.p6.2.m2.1"><semantics id="Ch2.S2.SS5.p6.2.m2.1a"><mrow id="Ch2.S2.SS5.p6.2.m2.1.1" xref="Ch2.S2.SS5.p6.2.m2.1.1.cmml"><mn id="Ch2.S2.SS5.p6.2.m2.1.1.2" xref="Ch2.S2.SS5.p6.2.m2.1.1.2.cmml">0.5</mn><mo id="Ch2.S2.SS5.p6.2.m2.1.1.1" lspace="0.278em" rspace="0.278em" xref="Ch2.S2.SS5.p6.2.m2.1.1.1.cmml">:</mo><mn id="Ch2.S2.SS5.p6.2.m2.1.1.3" xref="Ch2.S2.SS5.p6.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.SS5.p6.2.m2.1b"><apply id="Ch2.S2.SS5.p6.2.m2.1.1.cmml" xref="Ch2.S2.SS5.p6.2.m2.1.1"><ci id="Ch2.S2.SS5.p6.2.m2.1.1.1.cmml" xref="Ch2.S2.SS5.p6.2.m2.1.1.1">:</ci><cn id="Ch2.S2.SS5.p6.2.m2.1.1.2.cmml" type="float" xref="Ch2.S2.SS5.p6.2.m2.1.1.2">0.5</cn><cn id="Ch2.S2.SS5.p6.2.m2.1.1.3.cmml" type="float" xref="Ch2.S2.SS5.p6.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.SS5.p6.2.m2.1c">0.5:0.95</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.SS5.p6.2.m2.1d">0.5 : 0.95</annotation></semantics></math>. This comprehensive evaluation across several <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.iou" title="Intersection over Union">IoU</span></a> thresholds provides insights into the performance of the model at different levels of strictness in object localization against the ground truth.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p7">
<p class="ltx_p" id="Ch2.S2.SS5.p7.1">Additionally, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> can accomodate objects of varying sizes by further categorizing it based on the pixel area of the detected objects <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite>:</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p8">
<ul class="ltx_itemize" id="Ch2.S2.I2">
<li class="ltx_item" id="Ch2.S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I2.i1.p1">
<p class="ltx_p" id="Ch2.S2.I2.i1.p1.2"><math alttext="mAP_{s}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i1.p1.1.m1.1"><semantics id="Ch2.S2.I2.i1.p1.1.m1.1a"><mrow id="Ch2.S2.I2.i1.p1.1.m1.1.1" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.cmml"><mi id="Ch2.S2.I2.i1.p1.1.m1.1.1.2" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.2.cmml">m</mi><mo id="Ch2.S2.I2.i1.p1.1.m1.1.1.1" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i1.p1.1.m1.1.1.3" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.3.cmml">A</mi><mo id="Ch2.S2.I2.i1.p1.1.m1.1.1.1a" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.I2.i1.p1.1.m1.1.1.4" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4.cmml"><mi id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.2" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4.2.cmml">P</mi><mi id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.3" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i1.p1.1.m1.1b"><apply id="Ch2.S2.I2.i1.p1.1.m1.1.1.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1"><times id="Ch2.S2.I2.i1.p1.1.m1.1.1.1.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.1"></times><ci id="Ch2.S2.I2.i1.p1.1.m1.1.1.2.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.2">𝑚</ci><ci id="Ch2.S2.I2.i1.p1.1.m1.1.1.3.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.3">𝐴</ci><apply id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.1.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4">subscript</csymbol><ci id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.2.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4.2">𝑃</ci><ci id="Ch2.S2.I2.i1.p1.1.m1.1.1.4.3.cmml" xref="Ch2.S2.I2.i1.p1.1.m1.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i1.p1.1.m1.1c">mAP_{s}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i1.p1.1.m1.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> (small) if <math alttext="area\leq 32^{2}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i1.p1.2.m2.1"><semantics id="Ch2.S2.I2.i1.p1.2.m2.1a"><mrow id="Ch2.S2.I2.i1.p1.2.m2.1.1" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.cmml"><mrow id="Ch2.S2.I2.i1.p1.2.m2.1.1.2" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.cmml"><mi id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.2" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.2.cmml">a</mi><mo id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.3" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.3.cmml">r</mi><mo id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1a" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.4" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.4.cmml">e</mi><mo id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1b" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.5" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.5.cmml">a</mi></mrow><mo id="Ch2.S2.I2.i1.p1.2.m2.1.1.1" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.1.cmml">≤</mo><msup id="Ch2.S2.I2.i1.p1.2.m2.1.1.3" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3.cmml"><mn id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.2" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3.2.cmml">32</mn><mn id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.3" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i1.p1.2.m2.1b"><apply id="Ch2.S2.I2.i1.p1.2.m2.1.1.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1"><leq id="Ch2.S2.I2.i1.p1.2.m2.1.1.1.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.1"></leq><apply id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2"><times id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.1"></times><ci id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.2.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.2">𝑎</ci><ci id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.3.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.3">𝑟</ci><ci id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.4.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.4">𝑒</ci><ci id="Ch2.S2.I2.i1.p1.2.m2.1.1.2.5.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.2.5">𝑎</ci></apply><apply id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.1.cmml" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3">superscript</csymbol><cn id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.2.cmml" type="integer" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3.2">32</cn><cn id="Ch2.S2.I2.i1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="Ch2.S2.I2.i1.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i1.p1.2.m2.1c">area\leq 32^{2}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i1.p1.2.m2.1d">italic_a italic_r italic_e italic_a ≤ 32 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> pixels</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I2.i2.p1">
<p class="ltx_p" id="Ch2.S2.I2.i2.p1.2"><math alttext="mAP_{m}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i2.p1.1.m1.1"><semantics id="Ch2.S2.I2.i2.p1.1.m1.1a"><mrow id="Ch2.S2.I2.i2.p1.1.m1.1.1" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.cmml"><mi id="Ch2.S2.I2.i2.p1.1.m1.1.1.2" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.2.cmml">m</mi><mo id="Ch2.S2.I2.i2.p1.1.m1.1.1.1" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i2.p1.1.m1.1.1.3" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.3.cmml">A</mi><mo id="Ch2.S2.I2.i2.p1.1.m1.1.1.1a" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.I2.i2.p1.1.m1.1.1.4" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4.cmml"><mi id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.2" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4.2.cmml">P</mi><mi id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.3" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4.3.cmml">m</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i2.p1.1.m1.1b"><apply id="Ch2.S2.I2.i2.p1.1.m1.1.1.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1"><times id="Ch2.S2.I2.i2.p1.1.m1.1.1.1.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.1"></times><ci id="Ch2.S2.I2.i2.p1.1.m1.1.1.2.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.2">𝑚</ci><ci id="Ch2.S2.I2.i2.p1.1.m1.1.1.3.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.3">𝐴</ci><apply id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.1.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4">subscript</csymbol><ci id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.2.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4.2">𝑃</ci><ci id="Ch2.S2.I2.i2.p1.1.m1.1.1.4.3.cmml" xref="Ch2.S2.I2.i2.p1.1.m1.1.1.4.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i2.p1.1.m1.1c">mAP_{m}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i2.p1.1.m1.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> (medium) if <math alttext="32^{2}&lt;area\leq 96^{2}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i2.p1.2.m2.1"><semantics id="Ch2.S2.I2.i2.p1.2.m2.1a"><mrow id="Ch2.S2.I2.i2.p1.2.m2.1.1" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.cmml"><msup id="Ch2.S2.I2.i2.p1.2.m2.1.1.2" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2.cmml"><mn id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.2" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2.2.cmml">32</mn><mn id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.3" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2.3.cmml">2</mn></msup><mo id="Ch2.S2.I2.i2.p1.2.m2.1.1.3" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.3.cmml">&lt;</mo><mrow id="Ch2.S2.I2.i2.p1.2.m2.1.1.4" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.cmml"><mi id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.2" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.2.cmml">a</mi><mo id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.3" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.3.cmml">r</mi><mo id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1a" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.4" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.4.cmml">e</mi><mo id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1b" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.5" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.5.cmml">a</mi></mrow><mo id="Ch2.S2.I2.i2.p1.2.m2.1.1.5" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.5.cmml">≤</mo><msup id="Ch2.S2.I2.i2.p1.2.m2.1.1.6" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6.cmml"><mn id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.2" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6.2.cmml">96</mn><mn id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.3" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i2.p1.2.m2.1b"><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1"><and id="Ch2.S2.I2.i2.p1.2.m2.1.1a.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1"></and><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1b.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1"><lt id="Ch2.S2.I2.i2.p1.2.m2.1.1.3.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.3"></lt><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.1.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2">superscript</csymbol><cn id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.2.cmml" type="integer" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2.2">32</cn><cn id="Ch2.S2.I2.i2.p1.2.m2.1.1.2.3.cmml" type="integer" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.2.3">2</cn></apply><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4"><times id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.1"></times><ci id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.2.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.2">𝑎</ci><ci id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.3.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.3">𝑟</ci><ci id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.4.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.4">𝑒</ci><ci id="Ch2.S2.I2.i2.p1.2.m2.1.1.4.5.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.4.5">𝑎</ci></apply></apply><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1c.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1"><leq id="Ch2.S2.I2.i2.p1.2.m2.1.1.5.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.5"></leq><share href="https://arxiv.org/html/2410.04946v1#Ch2.S2.I2.i2.p1.2.m2.1.1.4.cmml" id="Ch2.S2.I2.i2.p1.2.m2.1.1d.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1"></share><apply id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6"><csymbol cd="ambiguous" id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.1.cmml" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6">superscript</csymbol><cn id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.2.cmml" type="integer" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6.2">96</cn><cn id="Ch2.S2.I2.i2.p1.2.m2.1.1.6.3.cmml" type="integer" xref="Ch2.S2.I2.i2.p1.2.m2.1.1.6.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i2.p1.2.m2.1c">32^{2}&lt;area\leq 96^{2}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i2.p1.2.m2.1d">32 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT &lt; italic_a italic_r italic_e italic_a ≤ 96 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> pixels</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S2.I2.i3.p1">
<p class="ltx_p" id="Ch2.S2.I2.i3.p1.2"><math alttext="mAP_{l}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i3.p1.1.m1.1"><semantics id="Ch2.S2.I2.i3.p1.1.m1.1a"><mrow id="Ch2.S2.I2.i3.p1.1.m1.1.1" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.cmml"><mi id="Ch2.S2.I2.i3.p1.1.m1.1.1.2" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.2.cmml">m</mi><mo id="Ch2.S2.I2.i3.p1.1.m1.1.1.1" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i3.p1.1.m1.1.1.3" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.3.cmml">A</mi><mo id="Ch2.S2.I2.i3.p1.1.m1.1.1.1a" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="Ch2.S2.I2.i3.p1.1.m1.1.1.4" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4.cmml"><mi id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.2" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4.2.cmml">P</mi><mi id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.3" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i3.p1.1.m1.1b"><apply id="Ch2.S2.I2.i3.p1.1.m1.1.1.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1"><times id="Ch2.S2.I2.i3.p1.1.m1.1.1.1.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.1"></times><ci id="Ch2.S2.I2.i3.p1.1.m1.1.1.2.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.2">𝑚</ci><ci id="Ch2.S2.I2.i3.p1.1.m1.1.1.3.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.3">𝐴</ci><apply id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.1.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4">subscript</csymbol><ci id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.2.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4.2">𝑃</ci><ci id="Ch2.S2.I2.i3.p1.1.m1.1.1.4.3.cmml" xref="Ch2.S2.I2.i3.p1.1.m1.1.1.4.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i3.p1.1.m1.1c">mAP_{l}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i3.p1.1.m1.1d">italic_m italic_A italic_P start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> (large) if <math alttext="area&gt;96^{2}" class="ltx_Math" display="inline" id="Ch2.S2.I2.i3.p1.2.m2.1"><semantics id="Ch2.S2.I2.i3.p1.2.m2.1a"><mrow id="Ch2.S2.I2.i3.p1.2.m2.1.1" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.cmml"><mrow id="Ch2.S2.I2.i3.p1.2.m2.1.1.2" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.cmml"><mi id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.2" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.2.cmml">a</mi><mo id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.3" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.3.cmml">r</mi><mo id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1a" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.4" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.4.cmml">e</mi><mo id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1b" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.5" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.5.cmml">a</mi></mrow><mo id="Ch2.S2.I2.i3.p1.2.m2.1.1.1" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.1.cmml">&gt;</mo><msup id="Ch2.S2.I2.i3.p1.2.m2.1.1.3" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3.cmml"><mn id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.2" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3.2.cmml">96</mn><mn id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.3" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Ch2.S2.I2.i3.p1.2.m2.1b"><apply id="Ch2.S2.I2.i3.p1.2.m2.1.1.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1"><gt id="Ch2.S2.I2.i3.p1.2.m2.1.1.1.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.1"></gt><apply id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2"><times id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.1"></times><ci id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.2.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.2">𝑎</ci><ci id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.3.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.3">𝑟</ci><ci id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.4.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.4">𝑒</ci><ci id="Ch2.S2.I2.i3.p1.2.m2.1.1.2.5.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.2.5">𝑎</ci></apply><apply id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.1.cmml" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3">superscript</csymbol><cn id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.2.cmml" type="integer" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3.2">96</cn><cn id="Ch2.S2.I2.i3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="Ch2.S2.I2.i3.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S2.I2.i3.p1.2.m2.1c">area&gt;96^{2}</annotation><annotation encoding="application/x-llamapun" id="Ch2.S2.I2.i3.p1.2.m2.1d">italic_a italic_r italic_e italic_a &gt; 96 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> pixels</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p9">
<p class="ltx_p" id="Ch2.S2.SS5.p9.1">This distinction per object size allows for a more detailed analysis of performance, especially in datasets with a wide range of object sizes, by highlighting its ability to detect small, medium, and large objects.</p>
</div>
<div class="ltx_para" id="Ch2.S2.SS5.p10">
<p class="ltx_p" id="Ch2.S2.SS5.p10.1">In order to compare results with existing standards, datasets such as COCO <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite>, with over 330,000 images and detailed annotations, are used as a resource for training and evaluating computer vision models in object detection and instance segmentation. Its diverse image collection makes it a valuable resource for researchers and developers. In the literature of experimental general purpose object recognition, it is a standard practice to evaluate general-purpose object recognition models performance using COCO as a benchmark <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite> with the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> as metric.</p>
</div>
</section>
</section>
</section>
<section class="ltx_chapter" id="Ch3" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 3 </span>Relevant State of the Art</h2>
<div class="ltx_para" id="Ch3.p1">
<p class="ltx_p" id="Ch3.p1.1">Expanding on the foundations of Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch2" title="Chapter 2 Fundamentals of Modern Object Recognition ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">2</span></a>, this chapter identifies key limitations in approaches to maritime situational awareness prior to this thesis. Specifically, this chapter reviews existing maritime datasets, real-time ship recognition algorithms, georeferencing techniques and gives an overview of the technologies available for deployment on embedded systems. By analyzing limitations, this chapter lays the groundwork for the studies and developments of a novel approach presented in later chapters.</p>
</div>
<section class="ltx_section" id="Ch3.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">3.1   Real-world Maritime Datasets</h3>
<div class="ltx_para" id="Ch3.S1.p1">
<p class="ltx_p" id="Ch3.S1.p1.1">The accuracy of a supervised learning model is greatly dependent on the quality and volume of the annotated data it is trained on, especially for real-world applications <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib34" title="">Cunningham2008 </a></cite>. As deep-learning-based ship detection and segmentation rely on supervised learning, it is necessary to use domain-specific training datasets <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib33" title="">chai2021deep </a></cite>. The training set and annotations must accurately represent the variety of ways objects can appear in different conditions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib57" title="">sarker2021deep </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch3.S1.p2">
<p class="ltx_p" id="Ch3.S1.p2.1">Real-world maritime monitoring requires image data with precise annotations for a broad range of ships and ship classes <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib58" title="">qiao2021marine </a></cite>. General-purpose detection and segmentation datasets, such as COCO <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite> or PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib59" title="">everingham2015pascal </a></cite>, therefore, do not suit the task of ship recognition and georeferencing as benchmark datasets for maritime awareness.
Relevant datasets in the literature for ship detection on video monitoring cameras are the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd" title="Singapore Maritime Dataset">Singapore Maritime Dataset (SMD)</span></a> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite>, Seaships7000 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib60" title="">shao2018seaships </a></cite>, and a dataset introduced by Chen et al. <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib61" title="">chen2020video </a></cite>. Moreover, other private datasets exist <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib62" title="">ghahremani2019multi </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib63" title="">nita2020cnn </a></cite>, however the restricted access makes the experimental validation using them not possible.
The accessible datasets, lack a variety of ship classes in their annotations and do not provide ship masks, necessary for ship georeferencing.
The MarSyn dataset <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib64" title="">ribeiro2022real </a></cite> is a synthetic ship dataset that contains images rendered from synthetic 3D scenes for instance segmentation in six ship classes, without georeference from the ships annotated.</p>
</div>
<div class="ltx_para" id="Ch3.S1.p3">
<p class="ltx_p" id="Ch3.S1.p3.1">A literature review on ship detection and localization <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib65" title="">teixeira2022literature </a></cite> highlights the fact that while annotations for ship datasets should include more complex data such as latitude and longitude of the ship, available datasets primarily focus on the object classes and bounding boxes, without masks or geographic positions. However, as motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, ship segmentation provides a more suitable solution for georeferencing. Therefore, we find the need for a publicly available real-world dataset, for ship segmentation and georeferencing, that includes footage of a maritime infrastructure as well as mask and georeferencing annotations of several classes of ships.
This dataset, should aim towards the advancement and evaluation of ship recognition methods for the improvement of maritime situational awareness.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">3.2   Ship Recognition Using Maritime Monitoring Footage</h3>
<div class="ltx_para" id="Ch3.S2.p1">
<p class="ltx_p" id="Ch3.S2.p1.1">To enhance maritime situational awareness, it is crucial to use methods that perfom ship recognition on maritime footage <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite>. However, these methods should not only recognize ships but also allow the gathering of essential information about them, such as their class and geographic location (georeference). It is vital to present this information in a simplified format to maritime operators for quick and effective decision-making <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>. Additionally, deploying an embedded system, with a monitoring camera on board, enables deep-learning object recognition directly on-site. This approach reduces network bandwidth, minimizes latency, improves security, and offers cost-efficiency <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib27" title="">ning2020heterogeneous </a></cite>, but comes with the trade-off of lower computational power compared to high-end systems <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib25" title="">mittal2019survey </a></cite>.
To address these complexities effectively, it is important that ship recognition methodologies not only ensure high accuracy across various ship sizes and types but are also optimized for the constraints of embedded hardware. Furthermore, the inference speed of video-based ship segmentation is paramount, as it significantly contributes to the improvement of data fusion with other sensor data, leading towards more cohesive maritime situational awareness systems <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<div class="ltx_figure ltx_transformed_outer" id="Ch3.F1" style="width:253.9pt;height:435.5pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:435.5pt;transform:translate(-90.78pt,-90.28pt) rotate(-90deg) ;"><figure><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="350" id="Ch3.F1.g1" src="extracted/5906916/fig/YOLOv8Architecture.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch3.F1.2.1.1" style="font-size:90%;">Figure 3.1</span>: </span><span class="ltx_text" id="Ch3.F1.3.2" style="font-size:80%;">The YOLOv8 architecture by Ultralytics <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib66" title="">jocheryolov8 </a></cite>. (a) Backbone, neck and head of the architecture. The numbers next to every block represent the sequential order followed by the implementation, from input to output. (b) Standard YOLOv8 convolutional block. (c) Spatial Pyramid Pooling Fast module of YOLOv8. (d) C2f with split channel operation and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp" title="Cross Stage Partial">Cross Stage Partial (CSP)</span></a> Bottleneck. (e) Segment block of YOLOv8 that performs segmentation. Modified from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> ©2023 IEEE.</span></figcaption>
</figure></div></div>
<div class="ltx_para" id="Ch3.S2.p2">
<p class="ltx_p" id="Ch3.S2.p2.1">These challenges underline the need to search for effective object recognition methods in the literature. The following list provides a brief overview of state-of-the-art object detection and instance segmentation methods that are particularly relevant:</p>
</div>
<div class="ltx_para" id="Ch3.S2.p3">
<ul class="ltx_itemize" id="Ch3.S2.I1">
<li class="ltx_item" id="Ch3.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i1.p1">
<p class="ltx_p" id="Ch3.S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i1.p1.1.1">YOLOv4-CSP <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib67" title="">wang2021scaled </a></cite></span>. The You-Only-Look-Once (YOLO) algorithm was introduced for real-time object detection <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib68" title="">redmon2016you </a></cite>. It divides the image into a grid and predicts bounding boxes and class probabilities for each grid cell, based on pre-defined anchors that act as reference points. These anchors are calculated by applying k-means clustering <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib32" title="">sammut2011encyclopedia </a></cite>, and serve a set of provisional bounding boxes for the object detection task.
YOLOv4-CSP presented a substantial enhancement in speed and accuracy by leveraging two main innovations. The first innovation is the use of CSPDarknet53 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib69" title="">bochkovskiy2020yolov4 </a></cite> as backbone, based on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp" title="Cross Stage Partial">CSP</span></a> network <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib70" title="">wang2020cspnet </a></cite>. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp" title="Cross Stage Partial">CSP</span></a> network partitions the feature map into two parts: one part is processed through a series of layers while the other part bypasses these layers, ensuring better gradient flow and capturing patterns more effectively <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib70" title="">wang2020cspnet </a></cite>. The second innovation is the Bag of Freebies technique, which includes data augmentation, label smoothing, and additional regularization methods. These improvements reached a detection mAP of 47.5% on COCO.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i2.p1">
<p class="ltx_p" id="Ch3.S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i2.p1.1.1">Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib71" title="">ren2015faster </a></cite> and Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib72" title="">he2017mask </a></cite></span>. Mask R-CNN is a two-stage instance segmentation method that was developed as an extension of the object detector Faster R-CNN.
They use the Region Proposal Network introduced in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib71" title="">ren2015faster </a></cite> to identify object candidates and then refine these detections by classifying them and fitting precise bounding boxes. In Mask R-CNN, a fully convolutional network was added to regress the mask from the detected bounding boxes, reaching a mask mAP of 39.8% on the COCO dataset with the ResNeXt-101 backbone <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib73" title="">xie2017aggregated </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i3.p1">
<p class="ltx_p" id="Ch3.S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i3.p1.1.1">DetectoRS <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib74" title="">qiao2021detectors </a></cite></span> is a multi-stage instance segmentation method that enhances the use of recursive feature pyramid network <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib41" title="">lin2017feature </a></cite> and feedback connections <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib75" title="">NIPS2014_19de10ad </a></cite> for improved performance. It features an atrous convolution, a type of dilated convolution used to expand the receptive field, allowing it to capture larger areas of the input <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib76" title="">yu2015multi </a></cite>. DetectoRS uses ResNet-50 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib40" title="">he2016deep </a></cite> as its backbone, and achieves a 44.4% mask mAP on the COCO dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i4.p1">
<p class="ltx_p" id="Ch3.S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i4.p1.1.1">YOLACT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib77" title="">bolya2019yolact </a></cite></span> emerged as one of the first real-time instance segmentation approach, operating in one-stage. It generates prototype masks through an independent fully convolutional network <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib78" title="">long2015fully </a></cite> and computes coefficients for adjusting these masks to the predicted bounding boxes. After suppressing overlapping detections with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">Non-Maximum Suppression (NMS)</span></a>, it filters the masks using anchor boxes. With a ResNet-101 backbone <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib40" title="">he2016deep </a></cite>, YOLACT attains a mask mAP of 34.1% on the COCO dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i5.p1">
<p class="ltx_p" id="Ch3.S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i5.p1.1.1">Centermask-Lite <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib79" title="">lee2020centermask </a></cite></span> is a one-stage instance segmentation method, optimized for real-time applications. It utilizes a spatial attention-guided mask branch within a fully convolutional object detector <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib80" title="">guo2020fully </a></cite> to refine proposed regions. It incorporates a novel backbone, VoVNet <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib81" title="">lee2019energy </a></cite>, which enhances feature map integration and, with VoVNet-39, achieves a 36.3% mask mAP on the COCO dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i6.p1">
<p class="ltx_p" id="Ch3.S2.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i6.p1.1.1">YOLOv5 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib82" title="">jocheryolov5 </a></cite></span>, developed within Ultralytics framework<span class="ltx_note ltx_role_footnote" id="Ch3.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ultralytics/" title="">https://github.com/ultralytics/</a></span></span></span>, builds on YOLOv4 but utilizes PyTorch and introduces the AutoAnchor algorithm to automatically fine-tune anchor boxes over multiple iterations. It achieves a detection mAP of 50.7% on COCO.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i7.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.p1.1.1">YOLOv8 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib66" title="">jocheryolov8 </a></cite></span>, also developed by Ultralytics, builds upon previous YOLOv5. With a focus on real-time applications, this version supports a full range of vision tasks, including detection and instance segmentation. In this thesis, YOLOv8 plays a central role in the customized instance segmentation architecture proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> and improved in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>, ScatYOLOv8+CBAM (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>). Therefore, YOLOv8 is described further than the previous state-of-the-art methods. The YOLOv8 architecture is divided into three main parts: Backbone, Neck, and Head, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>. The model uses the backbone CSPDarknet53 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib69" title="">bochkovskiy2020yolov4 </a></cite> as previous YOLO versions, but includes the novel C2f module (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(a)). The blocks found in the backbone are:</p>
</div>
<div class="ltx_para" id="Ch3.S2.I1.i7.p2">
<ul class="ltx_itemize" id="Ch3.S2.I1.i7.I1">
<li class="ltx_item" id="Ch3.S2.I1.i7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I1.i1.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i1.p1.1.1">Conv Block (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(b))</span>: Each Conv block includes a 2D convolution, followed by batch normalization and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.silu" title="Sigmoid-Weighted Linear Unit">Sigmoid-Weighted Linear Unit (SiLU)</span></a> activation function. This block reduces the spatial dimensions (width and height) and increases the number of channels.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I1.i2.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i2.p1.1.1">SPPF Block (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(c))</span>: The Spatial Pyramid Pooling Fast (SPPF) block performs multiple max-pooling operations at different scales, concatenates the results, and then applies a convolution. This block maintains the number of channels.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I1.i3.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I1.i3.p1.1.1">C2f Module (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(d))</span>: This module contains a series of convolutional layers, a channel split operation, and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp" title="Cross Stage Partial">CSP</span></a> bottleneck. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.csp" title="Cross Stage Partial">CSP</span></a> bottleneck splits the input feature map into two parts: one part goes through a series of convolutional layers (bottleneck), while the other part bypasses these layers. The outputs are then concatenated, which helps in maintaining gradient flow and reducing computational load. This design enhances feature extraction and learning efficiency. The C2f module increases the number of channels.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S2.I1.i7.p3">
<p class="ltx_p" id="Ch3.S2.I1.i7.p3.1">The neck of YOLOv8 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(a)) is designed to generate feature pyramids that help the model handle objects at different scales. Further blocks of the neck are:</p>
<ul class="ltx_itemize" id="Ch3.S2.I1.i7.I2">
<li class="ltx_item" id="Ch3.S2.I1.i7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I2.i1.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I2.i1.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I2.i1.p1.1.1">Upsample</span>: This block increases the spatial dimensions (height and width) of the feature maps.</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I2.i2.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I2.i2.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I2.i2.p1.1.1">Concat</span>: The Concat blocks merge feature maps from different stages, increasing the number of channels.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S2.I1.i7.p4">
<p class="ltx_p" id="Ch3.S2.I1.i7.p4.1">The head (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(a)) is responsible for producing the final output of the model. It includes the following:</p>
<ul class="ltx_itemize" id="Ch3.S2.I1.i7.I3">
<li class="ltx_item" id="Ch3.S2.I1.i7.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I3.i1.1.1.1">–</span></span>
<div class="ltx_para" id="Ch3.S2.I1.i7.I3.i1.p1">
<p class="ltx_p" id="Ch3.S2.I1.i7.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch3.S2.I1.i7.I3.i1.p1.1.1">Segment Block (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>(e))</span>: This block consists of a series of convolutional layers and generates segmentation masks and classifies each detected object. The postprocessing of YOLOv8, not shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a>, combines the output of both the pixel-level masks and class labels for each object.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S2.I1.i7.p5">
<p class="ltx_p" id="Ch3.S2.I1.i7.p5.5">YOLOv8 also offers five model sizes, these being, from the lightest and fastest to the deepest and most accurate: YOLOv8<math alttext="n" class="ltx_Math" display="inline" id="Ch3.S2.I1.i7.p5.1.m1.1"><semantics id="Ch3.S2.I1.i7.p5.1.m1.1a"><mi id="Ch3.S2.I1.i7.p5.1.m1.1.1" xref="Ch3.S2.I1.i7.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.I1.i7.p5.1.m1.1b"><ci id="Ch3.S2.I1.i7.p5.1.m1.1.1.cmml" xref="Ch3.S2.I1.i7.p5.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.I1.i7.p5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.I1.i7.p5.1.m1.1d">italic_n</annotation></semantics></math>, YOLOv8<math alttext="s" class="ltx_Math" display="inline" id="Ch3.S2.I1.i7.p5.2.m2.1"><semantics id="Ch3.S2.I1.i7.p5.2.m2.1a"><mi id="Ch3.S2.I1.i7.p5.2.m2.1.1" xref="Ch3.S2.I1.i7.p5.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.I1.i7.p5.2.m2.1b"><ci id="Ch3.S2.I1.i7.p5.2.m2.1.1.cmml" xref="Ch3.S2.I1.i7.p5.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.I1.i7.p5.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.I1.i7.p5.2.m2.1d">italic_s</annotation></semantics></math>, YOLOv8<math alttext="m" class="ltx_Math" display="inline" id="Ch3.S2.I1.i7.p5.3.m3.1"><semantics id="Ch3.S2.I1.i7.p5.3.m3.1a"><mi id="Ch3.S2.I1.i7.p5.3.m3.1.1" xref="Ch3.S2.I1.i7.p5.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.I1.i7.p5.3.m3.1b"><ci id="Ch3.S2.I1.i7.p5.3.m3.1.1.cmml" xref="Ch3.S2.I1.i7.p5.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.I1.i7.p5.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.I1.i7.p5.3.m3.1d">italic_m</annotation></semantics></math>, YOLOv8<math alttext="l" class="ltx_Math" display="inline" id="Ch3.S2.I1.i7.p5.4.m4.1"><semantics id="Ch3.S2.I1.i7.p5.4.m4.1a"><mi id="Ch3.S2.I1.i7.p5.4.m4.1.1" xref="Ch3.S2.I1.i7.p5.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.I1.i7.p5.4.m4.1b"><ci id="Ch3.S2.I1.i7.p5.4.m4.1.1.cmml" xref="Ch3.S2.I1.i7.p5.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.I1.i7.p5.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.I1.i7.p5.4.m4.1d">italic_l</annotation></semantics></math>, and YOLOv8<math alttext="x" class="ltx_Math" display="inline" id="Ch3.S2.I1.i7.p5.5.m5.1"><semantics id="Ch3.S2.I1.i7.p5.5.m5.1a"><mi id="Ch3.S2.I1.i7.p5.5.m5.1.1" xref="Ch3.S2.I1.i7.p5.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.I1.i7.p5.5.m5.1b"><ci id="Ch3.S2.I1.i7.p5.5.m5.1.1.cmml" xref="Ch3.S2.I1.i7.p5.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.I1.i7.p5.5.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.I1.i7.p5.5.m5.1d">italic_x</annotation></semantics></math>. It achieves a detection mAP of 53.9% and a mask mAP of 43.4%.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S2.p4">
<p class="ltx_p" id="Ch3.S2.p4.1">The above-listed methods were originally designed by their authors using COCO <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite> as benchmark dataset, serving this as a way of identifying robust and real-time models in the literature that could be suited for ship recognition tasks.</p>
</div>
<div class="ltx_para" id="Ch3.S2.p5">
<p class="ltx_p" id="Ch3.S2.p5.1">An effective object recognition method for ship recognition should offer potential for integration with additional tasks, such as georeferencing, which, is vital for enhancing maritime situational awareness <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.
This necessitates a move beyond the utilization of existing state-of-the-art object detection and segmentation models, towards developing an advanced approach. Such an approach must be lightweight enough for embedded system deployment while maintaining or enhancing precision and speed. This highlights the need for innovative, efficient solutions capable of meeting the stringent demands of both performance and practicality in maritime situational awareness.</p>
</div>
<div class="ltx_para" id="Ch3.S2.p6">
<p class="ltx_p" id="Ch3.S2.p6.1">To tackle the challenges mentioned and boost performance in real-world scenarios with scarce data, real-time ship recognition can be improved by leveraging advanced methods like attention mechanisms and techniques from other fields, such as the 2D scattering transform. The 2D scattering transform, which uses wavelets, has been widely used in signal processing tasks such as speech recognition, time series analysis, astrophysics, and geosciences <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib83" title="">singh2021deep </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib84" title="">pan2020spatio </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib85" title="">cheng2020new </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib86" title="">rodriguez2021recurrent </a></cite>.
Prior research has utilized wavelets in the computer vision field to perform multi-resolution geometric processing, multi-scale oriented filtering and image denoising <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib29" title="">szeliski2022computer </a></cite>.
In combination with deep learning and computer vision, the scattering transform has been used for image classification tasks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib87" title="">bruna2013invariant </a></cite>, proving to provide a deep systematic understanding of how invariant features can be captured and utilized. It captures the essence of geometric and structural properties, which are crucial for recognizing complex patterns under various conditions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib88" title="">oyallon2018compressing </a></cite>.
The integration of the 2D scattering transform and attention to an object recognition method should aim to enhance ship recognition performance, offering an efficient and targeted solution to the previously outlined challenges.
The technicalities of the 2D scattering transform are given in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="Ch3.S2.p7">
<p class="ltx_p" id="Ch3.S2.p7.1">Enhancing the performance of ship recognition systems through the integration of advanced techniques represents a significant step forward in maritime situational awareness. However, the practical application of these advancements in real-world monitoring scenarios brings to the forefront additional challenges, particularly in the recognition of small and distant ships. The effective recognition of small and distant ships ensures protection at the infrastructure by enabling early threat detection and accident prevention at maritime infrastructure <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib20" title="">chen2020deep </a></cite>. Using images at their original full-resolution, or even high-resolution cameras, is essential for this task <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib21" title="">rekavandi2022guide </a></cite>. However, deep learning techniques for object recognition on high-resolution images consume significant memory and necessitate larger neural networks, which complicates their real-time deployment <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib89" title="">saponara2021impact </a></cite>. Moreover, high-resolution processing on embedded systems with limited memory presents additional challenges, impacting performance and latency <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib90" title="">mao2016towards </a></cite>.
Image super-resolution, which consists of the synthetic increase of the input image resolution, has been used in the literature <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib21" title="">rekavandi2022guide </a></cite>. In reference <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib91" title="">wang2023uav </a></cite>, their work introduced additional blocks and layers using transformers, which leverage self-attention extensively <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib92" title="">zhu2023biformer </a></cite> for better small object accuracy from aerial views <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib91" title="">wang2023uav </a></cite>. These solutions increase computational complexity beyond the capacity of embedded systems for real-time operation.
The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">Slicing Aided Hyper Inference (SAHI)</span></a> method, introduced in reference <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib93" title="">sahi23akyon </a></cite>, splits high-resolution images into slices, enabling detection and segmentation of small objects.
Slicing mechanisms, therefore, allow the processing of high-resolution images on embedded systems by dividing the images into manageable sections, thus reducing the computational load and memory requirements.
However, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> lacks native batch inference processing, and instead, processes slices sequentially. This highlights the need for a slicing method which can benefit real-time applications such as small ship segmentation, as proposed in this thesis.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">3.3   Georeferencing of Recognized Ships</h3>
<div class="ltx_para" id="Ch3.S3.p1">
<p class="ltx_p" id="Ch3.S3.p1.1">The field of object georeferencing from images extends across various domains, from aerial vehicle tracking using airborne cameras <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib94" title="">han2011geolocation </a></cite> to the vehicle geolocation in urban environments for autonomous driving <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib95" title="">shami2024geo </a></cite>. These methods, while effective within their respective ranges of operation (dozens of meters), require positioning systems on board for calibration.
The method proposed by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib96" title="">milosavljevic2017method </a></cite> introduced an alternative that estimates georeferences from surveillance camera videos by aligning video frame points with geographic locations using a homography transformation to project the camera space onto orthophoto maps, which are geometrically corrected aerial photos, and Digital Elevation Models (DEMs). However, the application of high-resolution orthophotos and DEMs for maritime environments presents significant challenges, as these methods primarily model terrestrial elevations and are less effective for water surfaces, where the dynamic nature of water and its reflective properties complicate the creation of DEMs that could be used for ship georeferencing.</p>
</div>
<figure class="ltx_table" id="Ch3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T1.4.1.1" style="font-size:90%;">Table 3.1</span>: </span><span class="ltx_text" id="Ch3.T1.5.2" style="font-size:90%;">Ship georeferencing accuracy in existing literature. Note: Some entries lack reported uncertainty values for the positioning error.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="Ch3.T1.2" style="width:433.6pt;height:86.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.0pt,10.9pt) scale(0.797603293282502,0.797603293282502) ;">
<table class="ltx_tabular ltx_align_middle" id="Ch3.T1.2.2">
<tr class="ltx_tr" id="Ch3.T1.2.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.3.1"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.2.3.1.1">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.3.2"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.2.3.2.1">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.3.3"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.2.3.3.1">Range to Object</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.3.4"><span class="ltx_text ltx_font_bold" id="Ch3.T1.2.2.3.4.1">Error (m)</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.2.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.4.1"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib97" title="">naus2021assessment </a></cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.4.2">Radar Antenna + GPS*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.4.3">1000 m</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T1.2.2.4.4">6.5</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.1.1">
<td class="ltx_td ltx_align_center" id="Ch3.T1.1.1.1.2"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib98" title="">livingstone2014ship </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.1.1.1.3">Synthetic Aperture Radar</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.1.1.1.4">800 km</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.1.1.1.1">13 <math alttext="\pm" class="ltx_Math" display="inline" id="Ch3.T1.1.1.1.1.m1.1"><semantics id="Ch3.T1.1.1.1.1.m1.1a"><mo id="Ch3.T1.1.1.1.1.m1.1.1" xref="Ch3.T1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch3.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="Ch3.T1.1.1.1.1.m1.1.1.cmml" xref="Ch3.T1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T1.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Ch3.T1.1.1.1.1.m1.1d">±</annotation></semantics></math> 23</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.2.2"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib99" title="">wei2020geolocation </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.2.3">Opt. Remote Sensing</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.2.4">36000 km</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.2.1">165 <math alttext="\pm" class="ltx_Math" display="inline" id="Ch3.T1.2.2.2.1.m1.1"><semantics id="Ch3.T1.2.2.2.1.m1.1a"><mo id="Ch3.T1.2.2.2.1.m1.1.1" xref="Ch3.T1.2.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch3.T1.2.2.2.1.m1.1b"><csymbol cd="latexml" id="Ch3.T1.2.2.2.1.m1.1.1.cmml" xref="Ch3.T1.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch3.T1.2.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Ch3.T1.2.2.2.1.m1.1d">±</annotation></semantics></math> 109</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.2.5">
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.5.1"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib24" title="">helgesen2020low </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.5.2">Opt. Camera + GPS + IMU**</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.5.3">400 m</td>
<td class="ltx_td ltx_align_center" id="Ch3.T1.2.2.5.4">20</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" id="Ch3.T1.2.2.6.1"><span class="ltx_text" id="Ch3.T1.2.2.6.1.1" style="font-size:80%;">*Global Positioning System, **Inertial Measurement Unit</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Ch3.S3.p2">
<p class="ltx_p" id="Ch3.S3.p2.1">To transition from terrestrial to maritime applications, as discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, ship georeferencing is a critical aspect. This process involves the assignment of geographic coordinates to ships detected in various data sources. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.T1" title="Table 3.1 ‣ 3.3 Georeferencing of Recognized Ships ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a> shows a summary of ship georeferencing accuracies in the literature using different technologies.
Radar technologies have been a cornerstone in this field, providing real-time georeferencing at a speed of 1 Hz, as detailed in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib97" title="">naus2021assessment </a></cite>. Despite their accuracy, radar systems often involve high costs and complex deployment requirements.
Parallel to radar, satellite technologies including optical remote sensing <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib99" title="">wei2020geolocation </a></cite> and synthetic aperture radar (SAR) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib98" title="">livingstone2014ship </a></cite>, extend georeferencing capabilities over larger coverage areas. However, the effectiveness of these methods is constrained by their data cycle times (<math alttext="\sim" class="ltx_Math" display="inline" id="Ch3.S3.p2.1.m1.1"><semantics id="Ch3.S3.p2.1.m1.1a"><mo id="Ch3.S3.p2.1.m1.1.1" xref="Ch3.S3.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Ch3.S3.p2.1.m1.1b"><csymbol cd="latexml" id="Ch3.S3.p2.1.m1.1.1.cmml" xref="Ch3.S3.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S3.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="Ch3.S3.p2.1.m1.1d">∼</annotation></semantics></math>minutes) and the satellite revisit schedules, limiting their temporal resolution.
Recent advancements have explored the use of video sequences for ship georeferencing. The work in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib24" title="">helgesen2020low </a></cite> proposed a method which relies on the pinhole camera model calibration matrix <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib100" title="">zhang2000flexible </a></cite> to georeference ships detected in video frames. This approach, while innovative, requires prior knowledge of camera calibration, and its application has been limited to controlled conditions with a single video sequence of two small ships.</p>
</div>
<div class="ltx_para" id="Ch3.S3.p3">
<p class="ltx_p" id="Ch3.S3.p3.1">The methodologies and technologies reviewed reveal a landscape where accuracy, range, and cost are in constant negotiation. While radar and satellite methods offer comprehensive coverage, their practical deployment is often hindered by high costs and technical complexities. Conversely, camera-based approaches present a cost-effective alternative but are limited by the need for prior calibration or additional sources, such as orthophotos or DEMs. A solution that uses cameras without pose calibration would facilitate scalability in the deployment of the georeferencing method to existing monitoring cameras at the maritime infrastructure.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S4">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">3.4   Deployment on Embedded Systems</h3>
<div class="ltx_para" id="Ch3.S4.p1">
<p class="ltx_p" id="Ch3.S4.p1.1">Utilizing a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">Graphics Processing Unit (GPU)</span></a> on an embedded system, equipped with a monitoring camera, can allow for on-site deep-learning object recognition, streamlining the process significantly <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib25" title="">mittal2019survey </a></cite>. Processing images directly on the embedded system, rather than transferring them to a cloud or server, produces notable reduction in network bandwidth and latency, alongside cost savings and enhanced security <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib27" title="">ning2020heterogeneous </a></cite>. This integration facilitates real-time access to recognized and georeferenced ships through web services, enabling their display on maps for operators, boosting maritime monitoring <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib19" title="">flenker2021marlin </a></cite>.</p>
</div>
<figure class="ltx_table" id="Ch3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch3.T2.2.1.1" style="font-size:90%;">Table 3.2</span>: </span><span class="ltx_text" id="Ch3.T2.3.2" style="font-size:90%;">Comparison of NVIDIA GPU modules, with focus on the Jetson family and high-end GPU-powered systems.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="Ch3.T2.4" style="width:433.6pt;height:138.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(11.7pt,-3.7pt) scale(1.05707097543824,1.05707097543824) ;">
<table class="ltx_tabular ltx_align_middle" id="Ch3.T2.4.1">
<tr class="ltx_tr" id="Ch3.T2.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.1.1">System Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.2.1">Module</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.3.1">CUDA Cores</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.4.1">Memory</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.4.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="Ch3.T2.4.1.1.5.1">
<tr class="ltx_tr" id="Ch3.T2.4.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Ch3.T2.4.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.5.1.1.1.1">Max Power</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Ch3.T2.4.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.4.1.1.5.1.2.1.1">Consumption</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.2.1" rowspan="3"><span class="ltx_text" id="Ch3.T2.4.1.2.1.1">Edge Computing Device</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.2.2">Jetson Nano</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.2.3">128</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.2.4">4 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.4.1.2.5">10 W</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.3.1">Jetson TX2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.3.2">256</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.3.3">8 GB</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.4.1.3.4">15 W</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.4.1">Jetson AGX Xavier</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.4.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.4.3">16GB</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.4.1.4.4">30 W</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.5.1" rowspan="2"><span class="ltx_text" id="Ch3.T2.4.1.5.1.1">High-End Device</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.5.2">GV100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.5.3">5120</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch3.T2.4.1.5.4">32 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.4.1.5.5">250 W</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.6.1">A100</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.6.2">6912</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch3.T2.4.1.6.3">80 GB</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.4.1.6.4">400 W</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Ch3.S4.p2">
<p class="ltx_p" id="Ch3.S4.p2.1">Within the spectrum of embedded systems widely utilized for deep learning and computer vision, the NVIDIA Jetson family<span class="ltx_note ltx_role_footnote" id="Ch3.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/embedded/jetson-modules" title="">https://developer.nvidia.com/embedded/jetson-modules</a></span></span></span> stands out in the literature, offering both mobile and energy-efficient embedded <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-systems <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib101" title="">chen2019deep </a></cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.T2" title="Table 3.2 ‣ 3.4 Deployment on Embedded Systems ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.2</span></a> shows a comparison of three modules of the Jetson family compared against high-end server-based <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> systems to contextualize their capabilities. We observe that the Jetson modules provide optimized balance between performance and energy efficiency, marking them as an optimal solution for vision-based systems where the on-site deployment is a requirement. Larger servers, are typically used for the training of the models that are later deployed on the Jetson.</p>
</div>
<div class="ltx_para" id="Ch3.S4.p3">
<p class="ltx_p" id="Ch3.S4.p3.1">Jetson modules allow the use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> computing for deep learning models developed with PyTorch. Additionally, to enhance deep learning efficiency, models can be converted into optimized engines using TensorRT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib102" title="">NVIDIATensorRT2024 </a></cite>, a practice recommended for deploying models on NVIDIA hardware, which leads to faster inference speeds <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib103" title="">stepanenko2019using </a></cite>.
TensorRT is an NVIDIA library designed for high-performance deep learning inference, which includes optimizations for NVIDIA hardware.</p>
</div>
<div class="ltx_para" id="Ch3.S4.p4">
<p class="ltx_p" id="Ch3.S4.p4.1">The transition to export weights from PyTorch-trained models to TensorRT involves converting the trained deep learning models into a format that is optimized for inference on NVIDIA <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> s. This process begins with the trained model in PyTorch. The model is then exported to an intermediate representation, often using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.onnx"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.onnx" title="Open Neural Network Exchange">Open Neural Network Exchange (ONNX)</span></a> <span class="ltx_note ltx_role_footnote" id="Ch3.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/onnx/onnx" title="">https://github.com/onnx/onnx</a></span></span></span>, which standardizes the model format for use across different deep learning frameworks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib104" title="">onnxruntime </a></cite>. Once in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.onnx"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.onnx" title="Open Neural Network Exchange">ONNX</span></a> format, the model is ready to be optimized by TensorRT, which analyzes the network to fuse layers, optimize kernel selection, and apply other enhancements that reduce memory footprint. The optimization process is automatically tailored to the unique architecture of the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>, making it more effective when it is performed directly on the intended target system.
By converting PyTorch models to TensorRT, deep learning models can achieve faster inference times, reduced memory usage, and the ability to choose precision formats (such as FP16 or INT8) that balance speed and accuracy.</p>
</div>
<div class="ltx_para" id="Ch3.S4.p5">
<p class="ltx_p" id="Ch3.S4.p5.1">Several studies have leveraged NVIDIA Jetson modules for deploying deep learning models in various computer vision applications. For instance, in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib26" title="">zhao2019embedded </a></cite>, the Jetson TX2 is employed for ship detection, showcasing the utility of Jetson modules in maritime object detection. The work in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib105" title="">heller2022marine </a></cite> explored a comparison of marine object detection methods using the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd" title="Singapore Maritime Dataset">SMD</span></a> dataset <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite> on the NVIDIA Jetson Xavier AGX. In the field of instance segmentation, the work in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib106" title="">panero2021real </a></cite> utilized the NVIDIA Jetson AGX Xavier for real-time instance segmentation in driving traffic videos, showing the capability of the module to handle complex vision tasks in real-time scenarios.</p>
</div>
<div class="ltx_para" id="Ch3.S4.p6">
<p class="ltx_p" id="Ch3.S4.p6.1">While the NVIDIA Jetson modules have been effectively utilized in various object detection and recognition tasks, there is a notable absence of research focusing on tailored architectures for real-time ship segmentation deployable on these embedded systems. This highlights a significant opportunity for innovation in developing efficient, real-time processing solutions specifically designed for maritime monitoring applications.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch4" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 4 </span>ShipSG: Ship Segmentation and Georeferencing Dataset</h2>
<div class="ltx_para" id="Ch4.p1">
<p class="ltx_p" id="Ch4.p1.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3</span></a>, we explored the current state of maritime situational awareness, highlighting the critical need for robust and efficient ship recognition methodologies and the challenges associated with georeferencing ships using maritime monitoring footage. This exploration underlined the limitations of existing datasets in supporting the development and evaluation of advanced ship recognition and georeferencing techniques. Motivated by these insights, the creation of a comprehensive dataset that includes precise annotations for ship segmentation and accurate georeferencing has become paramount.
This chapter presents ShipSG, a novel dataset for ship segmentation and georeferencing using images from a fixed oblique perspective at maritime facilities.
ShipSG serves as a foundational component of this thesis, enabling the evaluation of existing instance segmentation methods as detailed in Chapters <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a> compared against the custom architecture proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.
Additionally, the dataset has been instrumental in the quantitative assessment of our georeferencing approaches, as outlined in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>.
A further description of the dataset is given in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>.
<span class="ltx_text ltx_font_bold" id="Ch4.p1.1.1">The dataset was made public and is accessible upon request<span class="ltx_note ltx_role_footnote" id="Ch4.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="Ch4.footnote1.1.1.1">1</span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dlr.de/mi/shipsg" title="">https://dlr.de/mi/shipsg</a></span></span></span></span>.</p>
</div>
<section class="ltx_section" id="Ch4.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">4.1   Dataset Overview</h3>
<div class="ltx_para" id="Ch4.S1.p1">
<p class="ltx_p" id="Ch4.S1.p1.1">The ShipSG dataset dataset was introduced in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> for the development and evaluation of instance segmentation and georeferencing methods using computer vision and deep learning, thus advancing the research field of ship recognition for maritime situational awareness. Some samples of ShipSG with annotated ship masks can be seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F1" title="Figure 4.1 ‣ 4.1 Dataset Overview ‣ Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4.1</span></a>. The dataset contains:</p>
</div>
<div class="ltx_para" id="Ch4.S1.p2">
<ul class="ltx_itemize" id="Ch4.S1.I1">
<li class="ltx_item" id="Ch4.S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i1.p1">
<p class="ltx_p" id="Ch4.S1.I1.i1.p1.1">3505 images (<math alttext="2028\times 1520" class="ltx_Math" display="inline" id="Ch4.S1.I1.i1.p1.1.m1.1"><semantics id="Ch4.S1.I1.i1.p1.1.m1.1a"><mrow id="Ch4.S1.I1.i1.p1.1.m1.1.1" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.cmml"><mn id="Ch4.S1.I1.i1.p1.1.m1.1.1.2" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.2.cmml">2028</mn><mo id="Ch4.S1.I1.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.1.cmml">×</mo><mn id="Ch4.S1.I1.i1.p1.1.m1.1.1.3" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.3.cmml">1520</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch4.S1.I1.i1.p1.1.m1.1b"><apply id="Ch4.S1.I1.i1.p1.1.m1.1.1.cmml" xref="Ch4.S1.I1.i1.p1.1.m1.1.1"><times id="Ch4.S1.I1.i1.p1.1.m1.1.1.1.cmml" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.1"></times><cn id="Ch4.S1.I1.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.2">2028</cn><cn id="Ch4.S1.I1.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="Ch4.S1.I1.i1.p1.1.m1.1.1.3">1520</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S1.I1.i1.p1.1.m1.1c">2028\times 1520</annotation><annotation encoding="application/x-llamapun" id="Ch4.S1.I1.i1.p1.1.m1.1d">2028 × 1520</annotation></semantics></math> pixels) from two cameras with static oblique view to the Doppelschleuse, Bremerhaven, Germany.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i2.p1">
<p class="ltx_p" id="Ch4.S1.I1.i2.p1.1">11625 annotated ship masks grouped in seven classes (see Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F3" title="Figure 4.3 ‣ 4.2 Acquisition and Annotation ‣ Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4.3</span></a>) with COCO format <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i3.p1">
<p class="ltx_p" id="Ch4.S1.I1.i3.p1.1">3505 geographic positions, consisting of the latitude and longitude of one of the masks within each image.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i4.p1">
<p class="ltx_p" id="Ch4.S1.I1.i4.p1.1">3505 <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">Automatic Identification System (AIS)</span></a> ship types<span class="ltx_note ltx_role_footnote" id="Ch4.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://coast.noaa.gov/data/marinecadastre/ais/VesselTypeCodes2018.pdf" title="">https://coast.noaa.gov/data/marinecadastre/ais/VesselTypeCodes2018.pdf</a></span></span></span>, one per geographic position annotated.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S1.I1.i5.p1">
<p class="ltx_p" id="Ch4.S1.I1.i5.p1.1">3505 ship lengths, one per geographic position annotated.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="Ch4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="304" id="Ch4.F1.g1" src="extracted/5906916/fig/dataset_mi_website.jpg" width="608"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F1.2.1.1" style="font-size:90%;">Figure 4.1</span>: </span><span class="ltx_text" id="Ch4.F1.3.2" style="font-size:90%;">Visualisation of ShipSG dataset samples with annotated ship masks, classes, and one ship position per image. Reprinted from the dataset site with permission from <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dlr"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dlr" title="German Aerospace Center">German Aerospace Center (DLR)</span></a>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S1.p3">
<p class="ltx_p" id="Ch4.S1.p3.1">The dataset was split into two sets: training and validation.
The training set contains 80% of the dataset, with 2804 images, and the remaining 20% is used for validation, with 701 images.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">4.2   Acquisition and Annotation</h3>
<div class="ltx_para" id="Ch4.S2.p1">
<p class="ltx_p" id="Ch4.S2.p1.1">The ShipSG dataset was collected through two strategically positioned cameras at the Fischereihafen-Doppelschleuse in Bremerhaven, Germany, aiming to cover a broad view of the lock’s entrance and adjacent Weser river area (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F2" title="Figure 4.2 ‣ 4.2 Acquisition and Annotation ‣ Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
With a height above water level of 23 meters, these cameras captured the dynamic maritime activities within the port basin, ranging distance up to 400 meters from the cameras, and on a distance of up to 1200 meters on the Weser river.
The images were captured under various weather conditions including sunny, cloudy, windy, and rainy days during Autumn 2020.
The dataset also ensures privacy by anonymizing non-relevant entities like vehicles and people.
This comprehensive collection approach ensures a diverse and realistic dataset for maritime research.</p>
</div>
<figure class="ltx_figure" id="Ch4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="364" id="Ch4.F2.g1" src="extracted/5906916/fig/cam_view.jpg" width="412"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F2.2.1.1" style="font-size:90%;">Figure 4.2</span>: </span><span class="ltx_text" id="Ch4.F2.3.2" style="font-size:90%;">View of each camera and identification of important elements in the scene. (a) View of first camera. (b) View of second camera. (c) Notable elements in the scene (OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib107" title="">OpenStreetMap </a></cite>). Modified from [BCP-II] (CC BY 4.0).</span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S2.p2">
<p class="ltx_p" id="Ch4.S2.p2.1">The dataset utilized <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> data to identify ships in each image, accessing real-time positional and static information from ships to annotate images accurately.
This included both the exact locations and lengths of the ships.
By matching the timestamps of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> messages with those of captured images, the dataset ensures high precision in ship positioning, limiting the time offset to 100 ms to achieve a close correspondence.
The recommended speed within the port of Bremerhaven is 10 knots (18.5 km/h) to ensure safe and efficient navigation, in accordance with the guidelines provided by the port authority <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib108" title="">Bremerhaven2024 </a></cite>, which means a displacement of approximately 0.5 m in 100 ms.
Therefore, the impact of the time offset between <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> and image capture can be considered negligible for the precision of the dataset ground truth.
This approach allowed for the annotation of 3505 images using accurate <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> data.</p>
</div>
<figure class="ltx_figure" id="Ch4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="306" id="Ch4.F3.g1" src="extracted/5906916/fig/classes_samples.jpg" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch4.F3.2.1.1" style="font-size:90%;">Figure 4.3</span>: </span><span class="ltx_text" id="Ch4.F3.3.2" style="font-size:90%;">Examples extracted from the dataset that show the seven ship classes. Each class contains a variety of sizes and orientations of the ships. (a) Cargo, (b) Tug, (c) Special 1, (d) Tanker, (e) Law Enforcement, (f) Passenger/Pleasure, (g) Special 2. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> (CC BY 4.0). </span></figcaption>
</figure>
<div class="ltx_para" id="Ch4.S2.p3">
<p class="ltx_p" id="Ch4.S2.p3.1">By providing <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> ship types with the dataset, we enable users to create their own tailored classes. In our case, we categorized seven ship classes (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4.F3" title="Figure 4.3 ‣ 4.2 Acquisition and Annotation ‣ Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4.3</span></a>) for the dataset based on an observation of their purpose and visual similarities:</p>
</div>
<div class="ltx_para" id="Ch4.S2.p4">
<ul class="ltx_itemize" id="Ch4.S2.I1">
<li class="ltx_item" id="Ch4.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i1.p1">
<p class="ltx_p" id="Ch4.S2.I1.i1.p1.1">Cargo: All types of cargo ships.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i2.p1">
<p class="ltx_p" id="Ch4.S2.I1.i2.p1.1">Law Enforcement: Police watercrafts and coast guard ships.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i3.p1">
<p class="ltx_p" id="Ch4.S2.I1.i3.p1.1">Passenger/Pleasure: Ferries, yachts, pleasure and sailing crafts.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i4.p1">
<p class="ltx_p" id="Ch4.S2.I1.i4.p1.1">Special 1: Crane vessels, dredgers and fishing boats.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i5.p1">
<p class="ltx_p" id="Ch4.S2.I1.i5.p1.1">Special 2: Research and survey ships, search and rescue ships and pilot vessels.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i6.p1">
<p class="ltx_p" id="Ch4.S2.I1.i6.p1.1">Tanker: All types of tankers.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i7.p1">
<p class="ltx_p" id="Ch4.S2.I1.i7.p1.1">Tug: All types of tugboats.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch4.S2.p5">
<p class="ltx_p" id="Ch4.S2.p5.1">To train and validate instance segmentation algorithms, ship masks were manually annotated in each image, identifying the ships and their classes using the LabelMe software <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib109" title="">labelme2016 </a></cite>. Moreover, ShipSG can also be used not only for the development of ship segmentation but also ship detection algorithms, by considering the surrounding bounding box of annotated masks.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">4.3   Summary and Discussion</h3>
<div class="ltx_para" id="Ch4.S3.p1">
<p class="ltx_p" id="Ch4.S3.p1.1">Featuring 3505 images, 11625 ship masks and the corresponding georeferences, a novel dataset, ShipSG, for ship segmentation and georeferencing using a static oblique view of a port has been presented. This dataset contains images with mask annotations of ships present, and their corresponding class, position and length.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p2">
<p class="ltx_p" id="Ch4.S3.p2.1">ShipSG stands as a pivotal contribution to the field of maritime research, setting a new benchmark for ship segmentation and georeferencing.
The validation of innovative methodologies using ShipSG lays the groundwork for future advancements in maritime situational awareness.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p3">
<p class="ltx_p" id="Ch4.S3.p3.1">The creation and use of ShipSG is an essential pillar for this thesis, as it allowed the validation of the recognition methods proposed in Chapters <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.
Our proposed georeferencing methods are also quantitatively validated using ShipSG, as presented in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>.
In total, ShipSG has been used in publications <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p4">
<p class="ltx_p" id="Ch4.S3.p4.1">While methods trained on ShipSG were cross-validated with other similar datasets in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> to study generalizability to other maritime scenes (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>), a key limitation is its reliance on only two views of the same area, coupled with the high costs and logistical challenges of new image capture and manual annotation.
Therefore, improvements of the dataset will focus on introducing a broader spectrum of data, crucial for mitigating issues caused by the limited variability in real-world annotated data.</p>
</div>
<div class="ltx_para" id="Ch4.S3.p5">
<p class="ltx_p" id="Ch4.S3.p5.1">Moreover, future iterations of ShipSG could enhance ship recognition algorithms by leveraging <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> data for annotating ship heading, in addition to the ship lengths.
The incorporation of this, combined with further annotations such as ship cuboids or keypoints, would offer valuable insights into the development of algorithms that automatically recognize ship heading and dimensions.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch5" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 5 </span>Ship Recognition for Improved Maritime Awareness</h2>
<div class="ltx_para" id="Ch5.p1">
<p class="ltx_p" id="Ch5.p1.1">We now delve into the initial exploration of deep learning techniques for ship detection and instance segmentation, that allowed further development of tailored solutions for ship recognition in the maritime domain as discussed in subsequent chapters.</p>
</div>
<div class="ltx_para" id="Ch5.p2">
<p class="ltx_p" id="Ch5.p2.1">Firstly, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1" title="5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1</span></a> shows that ship detection serves as a proof of concept for the feasibility of using deep-learning-based object detection and georeferencing.
This proof of concept reveals its potential to be applied to existing problems, as proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>: abnormal vessel behaviour detection, camera integrity assessment and 3D reconstruction.
Secondly, the chapter continues with the journey through standard instance segmentation methods shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>, performed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, setting the stage for the custom developments for real-time ship segmentation and georeferencing provided in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>. Therefore, this chapter outlines the impact of ship detection and segmentation in the development of advanced methodologies for the improvement of maritime situational awareness.</p>
</div>
<section class="ltx_section" id="Ch5.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">5.1   Ship Detection for Maritime Applications</h3>
<div class="ltx_para" id="Ch5.S1.p1">
<p class="ltx_p" id="Ch5.S1.p1.1">In this section we navigate through the implementations for this thesis in the field of ship detection from monitoring video and images as proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.
We explore how the automatic recognition of the bounding box of ships provides information that can be used by further processes for three different applications.
The first application is the detection of abnormal vessel behavior, which is crucial for maritime safety and security, as it enables early identification and mitigation of potential threats or navigational hazards <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib110" title="">riveiro2018maritime </a></cite>.
The second is the assessment of optical camera obstruction using ship detection, vital to maintain the reliability of surveillance systems, ensuring consistent monitoring quality under various environmental conditions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib111" title="">de2022partial </a></cite>.
Finally, the third application discussed in this section is 3D reconstruction of detected ships, which plays a pivotal role in enhancing situational awareness by offering three-dimensional visualizations which improve available semantic information of the situation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib112" title="">FraunhoferCML2021 </a></cite>.</p>
</div>
<section class="ltx_subsection" id="Ch5.S1.SS1">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">5.1.1   Abnormal Vessel Behaviour from Video <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>
</h4>
<figure class="ltx_figure" id="Ch5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="333" id="Ch5.F1.g1" src="extracted/5906916/fig/paper1_diagram.png" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F1.2.1.1" style="font-size:90%;">Figure 5.1</span>: </span><span class="ltx_text" id="Ch5.F1.3.2" style="font-size:90%;">Framework proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> for maritime anomaly detection from video, including my contributions (Vessel + Motion Detector and Geovisualization). The framework interprets the anomalies using the detections and georeferences for the geovisualization. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. ©2021 IEEE. </span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.SS1.p1">
<p class="ltx_p" id="Ch5.S1.SS1.p1.1">The proof of concept for ship detection and georeferencing as tool to support maritime situational awareness, paving the way for this thesis, was conceived in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>.
The publication presents a framework (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F1" title="Figure 5.1 ‣ 5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1</span></a>) for detecting and geovisualizing abnormal vessel behavior using video.
This framework aims to enhance maritime situational awareness by offering a tool that leverages <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai" title="Artificial Intelligence">Artificial Intelligence (AI)</span></a> for monitoring and interpreting anomalous vessel activities, thereby improving safety and security in maritime environments.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS1.p2">
<p class="ltx_p" id="Ch5.S1.SS1.p2.1">In my contribution to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, the focus was on ship detection, motion analysis, and georeferencing, to facilitate global geospatial localization and visualization of abnormal behavior (in latitudes and longitudes).
The georeferenced ships are then represented on a real-world coordinate map, and the motion is used to represent the direction of movement in the form of ship heading. Details for the motion and georeferencing are given in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>.
The anomaly detection module, which was not part of my contribution, uses a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gan"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gan" title="Generative Adversarial Network">Generative Adversarial Network (GAN)</span></a> for the identification of abnormal behavior. The behaviour is interpreted by combining the output of the anomaly detector with the ship detections, motion and georeferencing. The abnormal vessel behavior was defined by categorizing anomalies based on significant vessel fluxes and depletions, thereby establishing a nuanced criteria that captures a wide range of anomalous patterns without relying on supervised training.
Both anomaly detection and ship detection model were trained and validated on an optical sequence video with a resolution of <math alttext="1280\times 720" class="ltx_Math" display="inline" id="Ch5.S1.SS1.p2.1.m1.1"><semantics id="Ch5.S1.SS1.p2.1.m1.1a"><mrow id="Ch5.S1.SS1.p2.1.m1.1.1" xref="Ch5.S1.SS1.p2.1.m1.1.1.cmml"><mn id="Ch5.S1.SS1.p2.1.m1.1.1.2" xref="Ch5.S1.SS1.p2.1.m1.1.1.2.cmml">1280</mn><mo id="Ch5.S1.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch5.S1.SS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="Ch5.S1.SS1.p2.1.m1.1.1.3" xref="Ch5.S1.SS1.p2.1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S1.SS1.p2.1.m1.1b"><apply id="Ch5.S1.SS1.p2.1.m1.1.1.cmml" xref="Ch5.S1.SS1.p2.1.m1.1.1"><times id="Ch5.S1.SS1.p2.1.m1.1.1.1.cmml" xref="Ch5.S1.SS1.p2.1.m1.1.1.1"></times><cn id="Ch5.S1.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch5.S1.SS1.p2.1.m1.1.1.2">1280</cn><cn id="Ch5.S1.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="Ch5.S1.SS1.p2.1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.SS1.p2.1.m1.1c">1280\times 720</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.SS1.p2.1.m1.1d">1280 × 720</annotation></semantics></math> pixels and high density of vessels at the port of Sydney. The scene can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F2" title="Figure 5.2 ‣ 5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="298" id="Ch5.F2.g1" src="extracted/5906916/fig/formal_detections.png" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F2.2.1.1" style="font-size:90%;">Figure 5.2</span>: </span><span class="ltx_text" id="Ch5.F2.3.2" style="font-size:90%;">Inference of the YOLOv4-CSP based vessel detector. The orange bounding boxes correspond to the detected vessels. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. ©2021 IEEE. </span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.SS1.p3">
<p class="ltx_p" id="Ch5.S1.SS1.p3.1">Focusing this section on the object detector used for the framework (motion and georeferencing are explained in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a>), I defined a custom dataset from the video using 75 random frames for training and 20 for validation. Then, the ships bounding boxes were annotated manually.
Given the high density of vessels in each frame, this lead to a total of 4922 and 1387 bounding boxes on the training and validation set, respectively.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS1.p4">
<p class="ltx_p" id="Ch5.S1.SS1.p4.1">The object detector selected was YOLOv4-CSP <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib67" title="">wang2021scaled </a></cite>, with CSP-Darknet53 backbone <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib67" title="">wang2021scaled </a></cite>, to train and validate with the generated custom dataset.
After 234 epochs, the model achieved a peak <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mean Average Precision (mAP)</span></a> of 75.86%.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F2" title="Figure 5.2 ‣ 5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a> shows the inference of vessels with the resulting model on a frame that was not part of either the training or validation set for the ship detector.
Among the configuration parameters for training, there was the need to increase the image resolution to <math alttext="1536\times 1536" class="ltx_Math" display="inline" id="Ch5.S1.SS1.p4.1.m1.1"><semantics id="Ch5.S1.SS1.p4.1.m1.1a"><mrow id="Ch5.S1.SS1.p4.1.m1.1.1" xref="Ch5.S1.SS1.p4.1.m1.1.1.cmml"><mn id="Ch5.S1.SS1.p4.1.m1.1.1.2" xref="Ch5.S1.SS1.p4.1.m1.1.1.2.cmml">1536</mn><mo id="Ch5.S1.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch5.S1.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="Ch5.S1.SS1.p4.1.m1.1.1.3" xref="Ch5.S1.SS1.p4.1.m1.1.1.3.cmml">1536</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S1.SS1.p4.1.m1.1b"><apply id="Ch5.S1.SS1.p4.1.m1.1.1.cmml" xref="Ch5.S1.SS1.p4.1.m1.1.1"><times id="Ch5.S1.SS1.p4.1.m1.1.1.1.cmml" xref="Ch5.S1.SS1.p4.1.m1.1.1.1"></times><cn id="Ch5.S1.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="Ch5.S1.SS1.p4.1.m1.1.1.2">1536</cn><cn id="Ch5.S1.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="Ch5.S1.SS1.p4.1.m1.1.1.3">1536</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.SS1.p4.1.m1.1c">1536\times 1536</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.SS1.p4.1.m1.1d">1536 × 1536</annotation></semantics></math> pixels to obtain meaningful results with small ships or those located far away from the camera (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F2" title="Figure 5.2 ‣ 5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>), to the detriment of real-time performance.
The goal of the publication was the proof of concept for a framework for ship detection and georeferencing for the identification of abnormal behaviour.
Therefore, real-time processing was not a concern and all modules were run on high-end servers in an off-line manner.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS1.p5">
<p class="ltx_p" id="Ch5.S1.SS1.p5.1">The vessel detector presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> plays a key role within the framework.
It identifies vessels and ships in video data, enabling further analysis such as motion detection through optical flow and accurate mapping of vessel locations using georeferencing.
This process allows for the identification of vessel movements and anomalies on maps using web services, crucial for improving maritime safety and security.
The methodology of bounding box georeferencing and optical-flow based course calculation, which form a significant part of the contribution of <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, are discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS1.p6">
<p class="ltx_p" id="Ch5.S1.SS1.p6.1">Moreover, while the presented framework demonstrates promising results in a controlled setting, transitioning to a real-time, real-world application remained unexplored in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>.
This thesis further explores solutions to bridge this gap with regards to the recognition and georeferencing of ships.
As motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, georeferencing results are evidently superior when using the mask of ships rather than the bounding box, due to the unnecessary background included within bounding boxes and the inaccuracies of using the bounding box center as the georeferencing point.
Selecting an incorrect pixel from the bounding box for georeferencing introduces more error, leading to the consideration of instance segmentation over object detection in the following chapters of this thesis, aiming for more precise georeferencing.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S1.SS2">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">5.1.2   Ship Detection for Integrity Assessment of Camera Obstruction <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a>
</h4>
<div class="ltx_para" id="Ch5.S1.SS2.p1">
<p class="ltx_p" id="Ch5.S1.SS2.p1.1">The study presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a>, focuses on evaluating the resilience and reliability of maritime object detection algorithms under conditions of partial camera obstruction.
The work is based around the ShipSG dataset, to explore the effects of various simulated obstructions on detection performance.
An obstruction is defined as a physical anomaly, mostly static in nature, that obstructs the camera in close proximity to the lens, potentially requiring intervention to remove or clean.
The goal was to quantify the detrimental impact on the system’s ability to detect maritime objects correctly, treating the obstruction as a type of fault to investigate its effects on object detection performance.</p>
</div>
<figure class="ltx_figure" id="Ch5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="175" id="Ch5.F3.g1" src="extracted/5906916/fig/obstruction_samples.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F3.2.1.1" style="font-size:90%;">Figure 5.3</span>: </span><span class="ltx_text" id="Ch5.F3.3.2" style="font-size:90%;">Examples of a ship with different synthetic partial obstruction profiles. (a) No obstruction; (b) 30% bright obstruction at the bottom; (c) 30% dark
obstruction at the center; (d) 60% gray obstruction at the right. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a>. ©2023 IEEE. </span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.SS2.p2">
<p class="ltx_p" id="Ch5.S1.SS2.p2.1">My contributions to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> lie in the use of ShipSG dataset and the detection of ships, that allow an investigation on how the obstructions affect false positive, misclassification, and false negative ratios, along with the detection score distributions.
The work employs Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib71" title="">ren2015faster </a></cite> as the object detector for ship detection. While not specifically designed for real-time applications, its robust architecture offers a strong foundation for object detection tasks.
Faster R-CNN was trained on the seven different ship classes of the dataset, using 80% of images for training and 20% for validation, with <math alttext="1333\times 800" class="ltx_Math" display="inline" id="Ch5.S1.SS2.p2.1.m1.1"><semantics id="Ch5.S1.SS2.p2.1.m1.1a"><mrow id="Ch5.S1.SS2.p2.1.m1.1.1" xref="Ch5.S1.SS2.p2.1.m1.1.1.cmml"><mn id="Ch5.S1.SS2.p2.1.m1.1.1.2" xref="Ch5.S1.SS2.p2.1.m1.1.1.2.cmml">1333</mn><mo id="Ch5.S1.SS2.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch5.S1.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="Ch5.S1.SS2.p2.1.m1.1.1.3" xref="Ch5.S1.SS2.p2.1.m1.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S1.SS2.p2.1.m1.1b"><apply id="Ch5.S1.SS2.p2.1.m1.1.1.cmml" xref="Ch5.S1.SS2.p2.1.m1.1.1"><times id="Ch5.S1.SS2.p2.1.m1.1.1.1.cmml" xref="Ch5.S1.SS2.p2.1.m1.1.1.1"></times><cn id="Ch5.S1.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch5.S1.SS2.p2.1.m1.1.1.2">1333</cn><cn id="Ch5.S1.SS2.p2.1.m1.1.1.3.cmml" type="integer" xref="Ch5.S1.SS2.p2.1.m1.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.SS2.p2.1.m1.1c">1333\times 800</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.SS2.p2.1.m1.1d">1333 × 800</annotation></semantics></math> pixel resolution with the ResNeXt-101 backbone <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib73" title="">xie2017aggregated </a></cite>.
The training was initiated using pre-trained COCO weights <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib56" title="">lin2014microsoft </a></cite> and after additional training on ShipSG of 11 epochs, the model achieved a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> of 82.6%.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS2.p3">
<p class="ltx_p" id="Ch5.S1.SS2.p3.1">The work of <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> aligns with the expectations, that true positives (correct detections) decrease and false negatives rise linearly with obstruction. False positives peak from 50 to 60% obstruction, and beyond 60%, as the occlusion covers most of the ship, the detector often fails to recognize any object, leading to an increase in false negatives.
Incorporating an obstruction detection step can support maritime stakeholders in identifying camera faults, saving operational time and the subsequent costs.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS2.p4">
<p class="ltx_p" id="Ch5.S1.SS2.p4.1">Further advancements will employ instance segmentation instead of object detection to offer significant integrity assessment improvements, as it allows for precise delineation of ship contours, minimizing background noise in detections.
Therefore, segmentation could provide deeper insights into how lens obstructions specifically affect the visibility and classification of ships, by isolating the object from obstructive elements more effectively than bounding boxes.
Moreover, more realistic obstructions should be used to increase the understanding of their impact in the recognition.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch5.S1.SS3">
<h4 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_subsection">5.1.3   Ship Detection for 3D Reconstruction <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>
</h4>
<figure class="ltx_figure" id="Ch5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="292" id="Ch5.F4.g1" src="extracted/5906916/fig/paper4_diagram.png" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F4.2.1.1" style="font-size:90%;">Figure 5.4</span>: </span><span class="ltx_text" id="Ch5.F4.3.2" style="font-size:90%;">Framework proposed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a> for 3D reconstruction of ships using synthetic stereo images. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>. ©2023 Springer. </span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.SS3.p1">
<p class="ltx_p" id="Ch5.S1.SS3.p1.1">The work in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a> proposes a novel experimental framework (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F4" title="Figure 5.4 ‣ 5.1.3 Ship Detection for 3D Reconstruction [BCP-IV] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.4</span></a>) for real-time 3D reconstruction of a detected ship, using synthetic stereo images, and is deployed on an NVIDIA Jetson AGX Xavier.
The goal is to enhance maritime situational awareness by processing 2D video data for display into a single consistent 3D display using an embedded system. This transformation from 2D videos into 3D displays provides an intuitive, comprehensive maritime environment understanding with enhanced visualization.
The framework is validated using a synthetic and controlled environment created with Blender3D <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib113" title="">blender3D </a></cite>, that represents a simulated sequence of a tugboat.
It introduces a pipeline prototype for dynamic 3D reconstruction using virtual stereoscopic cameras on a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">Graphics Processing Unit (GPU)</span></a>-accelerated embedded device, where object detection plays the role of locating the tugboat on the frames.</p>
</div>
<figure class="ltx_figure" id="Ch5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="193" id="Ch5.F5.g1" src="extracted/5906916/fig/synthetic_yolov5.jpg" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F5.2.1.1" style="font-size:90%;">Figure 5.5</span>: </span><span class="ltx_text" id="Ch5.F5.3.2" style="font-size:90%;">Object detection example for 3D reconstruction. (a) Four samples of the rendered dataset for object detection training. (b) An example of tugboat detection using YOLOv5 on one frame of the synthetic sequence for reconstruction. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>. ©2023 Springer. </span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.SS3.p2">
<p class="ltx_p" id="Ch5.S1.SS3.p2.2">As part of my contributions to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>, YOLOv5 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib82" title="">jocheryolov5 </a></cite> was selected as object detector, in its lightest configuration (nano or <math alttext="n" class="ltx_Math" display="inline" id="Ch5.S1.SS3.p2.1.m1.1"><semantics id="Ch5.S1.SS3.p2.1.m1.1a"><mi id="Ch5.S1.SS3.p2.1.m1.1.1" xref="Ch5.S1.SS3.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch5.S1.SS3.p2.1.m1.1b"><ci id="Ch5.S1.SS3.p2.1.m1.1.1.cmml" xref="Ch5.S1.SS3.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.SS3.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.SS3.p2.1.m1.1d">italic_n</annotation></semantics></math>).
It was trained on a custom synthetic dataset of a tugboat in various sizes and perspectives (see Figure<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F5" title="Figure 5.5 ‣ 5.1.3 Ship Detection for 3D Reconstruction [BCP-IV] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.5</span></a> (a)), for 50 epochs with an image resolution of <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch5.S1.SS3.p2.2.m2.1"><semantics id="Ch5.S1.SS3.p2.2.m2.1a"><mrow id="Ch5.S1.SS3.p2.2.m2.1.1" xref="Ch5.S1.SS3.p2.2.m2.1.1.cmml"><mn id="Ch5.S1.SS3.p2.2.m2.1.1.2" xref="Ch5.S1.SS3.p2.2.m2.1.1.2.cmml">640</mn><mo id="Ch5.S1.SS3.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch5.S1.SS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="Ch5.S1.SS3.p2.2.m2.1.1.3" xref="Ch5.S1.SS3.p2.2.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch5.S1.SS3.p2.2.m2.1b"><apply id="Ch5.S1.SS3.p2.2.m2.1.1.cmml" xref="Ch5.S1.SS3.p2.2.m2.1.1"><times id="Ch5.S1.SS3.p2.2.m2.1.1.1.cmml" xref="Ch5.S1.SS3.p2.2.m2.1.1.1"></times><cn id="Ch5.S1.SS3.p2.2.m2.1.1.2.cmml" type="integer" xref="Ch5.S1.SS3.p2.2.m2.1.1.2">640</cn><cn id="Ch5.S1.SS3.p2.2.m2.1.1.3.cmml" type="integer" xref="Ch5.S1.SS3.p2.2.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.SS3.p2.2.m2.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.SS3.p2.2.m2.1d">640 × 640</annotation></semantics></math> pixels.
For inference, the object detector was deployed, using Pytorch weights, on an NVIDIA Jetson Xavier AGX for real-time processing, using Pytorch <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib55" title="">NEURIPS2019_9015 </a></cite>, achieving a speed of 74 ms per frame and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> of 90.7%.
The tugboat detector, the fast and accurate detection, enables the framework to focus the rest of the pipeline on the content of the bounding box, therefore supporting the subsequent 3D reconstruction process.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS3.p3">
<p class="ltx_p" id="Ch5.S1.SS3.p3.1">Testing this framework on a synthetic dataset presents challenges in extrapolating results to real-world scenarios.
Real-world deployment faces issues such as varied lighting conditions, diverse ship designs, and environmental factors like sea state and weather, which can significantly impact detection and reconstruction accuracy.
We observe that the high <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> (90.7%) is a result of the dataset just being constituted by the same boat throughout the whole sequence, with no other boats or classes being present.
Addressing these challenges requires robust algorithmic improvements and real-world datasets to ensure the framework’s effectiveness in practical maritime monitoring and safety applications.</p>
</div>
<div class="ltx_para" id="Ch5.S1.SS3.p4">
<p class="ltx_p" id="Ch5.S1.SS3.p4.1">Moreover, exploring instance segmentation instead of object detection for future work could yield benefits, particularly for accurate 3D reconstruction, where ship segmentation is essential.
After the tugboat detector, the framework relies on traditional segmentation techniques, due to the absence of a real-time capable segmentation solution using deep learning deployable on the NVIDIA Jetson AGX Xavier at the time of the implementation of <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.
This underscores the potential of real-time instance segmentation developments deployed on embedded systems shown in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.
For example, the framework presented in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>, a deep-learning-based instance segmentation method could provide a more unified approach, enhancing 3D reconstruction efficiency and potentially accuracy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch5.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">5.2   Standard Ship Segmentation Using ShipSG <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>
</h3>
<div class="ltx_para" id="Ch5.S2.p1">
<p class="ltx_p" id="Ch5.S2.p1.1">Building on the foundational work of this thesis in ship detection and its implications for maritime applications, as outlined in the preceding sections, we look now into the experimental evaluation of standard instance segmentation methods on the ShipSG dataset, as presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>.
The use of instance segmentation is motivated by the significant enhancement that it would provide in applications such as abnormal vessel behavior detection with more accurate georeferencing, integrity assessment of camera obstruction with more accurate analysis, and 3D reconstruction of detected ships with the unification of parts in the pipeline.</p>
</div>
<div class="ltx_para" id="Ch5.S2.p2">
<p class="ltx_p" id="Ch5.S2.p2.1">The creation of the ShipSG dataset (see Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4</span></a>), provided a comprehensive basis to perform an evaluation of robust instance segmentation methods like Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib72" title="">he2017mask </a></cite> and DetectoRS <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib74" title="">qiao2021detectors </a></cite>, as well as real-time methods including YOLACT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib77" title="">bolya2019yolact </a></cite> and Centermask-Lite <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib79" title="">lee2020centermask </a></cite>.
The latter evaluations seeking real-time performance involved configurations that sought to balance inference speed with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a>.
Therefore, two configurations for each were selected, one deeper and another one lighter, as can be seen in Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T1" title="Table 5.1 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T2" title="Table 5.2 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
All methods initiated training on ShipSG with COCO pre-trained weights.
It is notable that in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, inference speed was measured using the NVIDIA GV100, a high-end <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>, boasts Tensor Cores for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ai" title="Artificial Intelligence">AI</span></a> acceleration, 32 GB of memory, and over 5000 <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cuda"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cuda" title="Compute Unified Device Architecture">Compute Unified Device Architecture (CUDA)</span></a> cores for unparalleled computational performance.
Embedded system based deployment will be discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T1.8.1.1" style="font-size:90%;">Table 5.1</span>: </span><span class="ltx_text" id="Ch5.T1.9.2" style="font-size:90%;">Configurations during training for each instance segmentation method evaluated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>. (CC BY 4.0)</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch5.T1.6">
<tr class="ltx_tr" id="Ch5.T1.6.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Ch5.T1.6.7.1"><span class="ltx_text ltx_font_bold" id="Ch5.T1.6.7.1.1" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T1.6.7.2"><span class="ltx_text ltx_font_bold" id="Ch5.T1.6.7.2.1" style="font-size:80%;">Input Size (Pixel)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T1.6.7.3"><span class="ltx_text ltx_font_bold" id="Ch5.T1.6.7.3.1" style="font-size:80%;">Backbone</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T1.6.7.4"><span class="ltx_text ltx_font_bold" id="Ch5.T1.6.7.4.1" style="font-size:80%;">Number of Epochs</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch5.T1.1.1.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.1.1.2.1" style="font-size:80%;">Mask R-CNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.1.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.1.1.1.1" style="font-size:80%;">1333 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.1.1.1.m1.1"><semantics id="Ch5.T1.1.1.1.m1.1a"><mo id="Ch5.T1.1.1.1.m1.1.1" mathsize="80%" xref="Ch5.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.1.1.1.m1.1b"><times id="Ch5.T1.1.1.1.m1.1.1.cmml" xref="Ch5.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.1.1.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.1.1.1.2" style="font-size:80%;"> 800</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.1.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.1.1.3.1" style="font-size:80%;">ResNeXt-101</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.1.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.1.1.4.1" style="font-size:80%;">11</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T1.2.2.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.2.2.2.1" style="font-size:80%;">DetectoRS</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.2.2.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.2.2.1.1" style="font-size:80%;">1333 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.2.2.1.m1.1"><semantics id="Ch5.T1.2.2.1.m1.1a"><mo id="Ch5.T1.2.2.1.m1.1.1" mathsize="80%" xref="Ch5.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.2.2.1.m1.1b"><times id="Ch5.T1.2.2.1.m1.1.1.cmml" xref="Ch5.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.2.2.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.2.2.1.2" style="font-size:80%;"> 800</span>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.2.2.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.2.2.3.1" style="font-size:80%;">ResNet-50</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.2.2.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.2.2.4.1" style="font-size:80%;">11</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T1.3.3.2" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.3.3.2.1" style="font-size:80%;">YOLACT</span><sub class="ltx_sub" id="Ch5.T1.3.3.2.2"><span class="ltx_text" id="Ch5.T1.3.3.2.2.1" style="font-size:80%;">550</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.3.3.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.3.3.1.1" style="font-size:80%;">550 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.3.3.1.m1.1"><semantics id="Ch5.T1.3.3.1.m1.1a"><mo id="Ch5.T1.3.3.1.m1.1.1" mathsize="80%" xref="Ch5.T1.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.3.3.1.m1.1b"><times id="Ch5.T1.3.3.1.m1.1.1.cmml" xref="Ch5.T1.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.3.3.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.3.3.1.2" style="font-size:80%;"> 550</span>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.3.3.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.3.3.3.1" style="font-size:80%;">ResNet-50</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.3.3.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.3.3.4.1" style="font-size:80%;">18</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T1.4.4.2" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.4.4.2.1" style="font-size:80%;">YOLACT</span><sub class="ltx_sub" id="Ch5.T1.4.4.2.2"><span class="ltx_text" id="Ch5.T1.4.4.2.2.1" style="font-size:80%;">700</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.4.4.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.4.4.1.1" style="font-size:80%;">700 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.4.4.1.m1.1"><semantics id="Ch5.T1.4.4.1.m1.1a"><mo id="Ch5.T1.4.4.1.m1.1.1" mathsize="80%" xref="Ch5.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.4.4.1.m1.1b"><times id="Ch5.T1.4.4.1.m1.1.1.cmml" xref="Ch5.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.4.4.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.4.4.1.2" style="font-size:80%;"> 700</span>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.4.4.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.4.4.3.1" style="font-size:80%;">ResNet-101</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.4.4.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.4.4.4.1" style="font-size:80%;">16</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T1.5.5.2" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.5.5.2.1" style="font-size:80%;">Centermask-Lite</span><sub class="ltx_sub" id="Ch5.T1.5.5.2.2"><span class="ltx_text" id="Ch5.T1.5.5.2.2.1" style="font-size:80%;">V19</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.5.5.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T1.5.5.1.1" style="font-size:80%;">800 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.5.5.1.m1.1"><semantics id="Ch5.T1.5.5.1.m1.1a"><mo id="Ch5.T1.5.5.1.m1.1.1" mathsize="80%" xref="Ch5.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.5.5.1.m1.1b"><times id="Ch5.T1.5.5.1.m1.1.1.cmml" xref="Ch5.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.5.5.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.5.5.1.2" style="font-size:80%;"> 600</span>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.5.5.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.5.5.3.1" style="font-size:80%;">Vovnet-19</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.5.5.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T1.5.5.4.1" style="font-size:80%;">17</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Ch5.T1.6.6.2">
<span class="ltx_text" id="Ch5.T1.6.6.2.1" style="font-size:80%;">Centermask-Lite</span><sub class="ltx_sub" id="Ch5.T1.6.6.2.2"><span class="ltx_text" id="Ch5.T1.6.6.2.2.1" style="font-size:80%;">V39</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.6.6.1">
<span class="ltx_text" id="Ch5.T1.6.6.1.1" style="font-size:80%;">800 </span><math alttext="\times" class="ltx_Math" display="inline" id="Ch5.T1.6.6.1.m1.1"><semantics id="Ch5.T1.6.6.1.m1.1a"><mo id="Ch5.T1.6.6.1.m1.1.1" mathsize="80%" xref="Ch5.T1.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch5.T1.6.6.1.m1.1b"><times id="Ch5.T1.6.6.1.m1.1.1.cmml" xref="Ch5.T1.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch5.T1.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch5.T1.6.6.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="Ch5.T1.6.6.1.2" style="font-size:80%;"> 600</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.6.6.3"><span class="ltx_text" id="Ch5.T1.6.6.3.1" style="font-size:80%;">Vovnet-39</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.6.6.4"><span class="ltx_text" id="Ch5.T1.6.6.4.1" style="font-size:80%;">17</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="Ch5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch5.T2.2.1.1" style="font-size:90%;">Table 5.2</span>: </span><span class="ltx_text" id="Ch5.T2.3.2" style="font-size:90%;">Resulting instance segmentation APs and inference speed per method evaluated. Inference times are measured on a high-end NVIDIA GV100 <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>. Adapted from [BCP-II] (CC BY 4.0).</span></figcaption>
<table class="ltx_tabular ltx_align_middle" id="Ch5.T2.4">
<tr class="ltx_tr" id="Ch5.T2.4.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="Ch5.T2.4.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.1.1" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T2.4.1.2"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.2.1" style="font-size:80%;">mAP (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T2.4.1.3"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.3.1" style="font-size:80%;">mAP<sub class="ltx_sub" id="Ch5.T2.4.1.3.1.1">s</sub> (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T2.4.1.4"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.4.1" style="font-size:80%;">mAP<sub class="ltx_sub" id="Ch5.T2.4.1.4.1.1">m</sub> (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T2.4.1.5"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.5.1" style="font-size:80%;">mAP<sub class="ltx_sub" id="Ch5.T2.4.1.5.1.1">l</sub> (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch5.T2.4.1.6"><span class="ltx_text ltx_font_bold" id="Ch5.T2.4.1.6.1" style="font-size:80%;">Inference (ms)</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch5.T2.4.2.1" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.1.1" style="font-size:80%;">Mask R-CNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.2.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.2.1" style="font-size:80%;">73.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.2.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.3.1" style="font-size:80%;">50.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.2.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.4.1" style="font-size:80%;">75.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.2.5" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.5.1" style="font-size:80%;">77.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.2.6" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.2.6.1" style="font-size:80%;">117</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T2.4.3.1" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.1.1" style="font-size:80%;">DetectoRS</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.3.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.2.1" style="font-size:80%;">74.7</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.3.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.3.1" style="font-size:80%;">55.6</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.3.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.4.1" style="font-size:80%;">75.7</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.3.5" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.5.1" style="font-size:80%;">79.2</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.3.6" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.3.6.1" style="font-size:80%;">151</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ch5.T2.4.4.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T2.4.4.1.1" style="font-size:80%;">YOLACT</span><sub class="ltx_sub" id="Ch5.T2.4.4.1.2"><span class="ltx_text" id="Ch5.T2.4.4.1.2.1" style="font-size:80%;">550</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.4.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.4.2.1" style="font-size:80%;">52.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.4.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.4.3.1" style="font-size:80%;">8.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.4.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.4.4.1" style="font-size:80%;">51.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.4.5" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.4.5.1" style="font-size:80%;">70.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T2.4.4.6" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.4.6.1" style="font-size:80%;">28</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T2.4.5.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T2.4.5.1.1" style="font-size:80%;">YOLACT</span><sub class="ltx_sub" id="Ch5.T2.4.5.1.2"><span class="ltx_text" id="Ch5.T2.4.5.1.2.1" style="font-size:80%;">700</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.5.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.5.2.1" style="font-size:80%;">58.2</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.5.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.5.3.1" style="font-size:80%;">14.0</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.5.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.5.4.1" style="font-size:80%;">58.2</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.5.5" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.5.5.1" style="font-size:80%;">75.1</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.5.6" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.5.6.1" style="font-size:80%;">36</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch5.T2.4.6.1" style="padding-bottom:2.84544pt;">
<span class="ltx_text" id="Ch5.T2.4.6.1.1" style="font-size:80%;">Centermask-Lite</span><sub class="ltx_sub" id="Ch5.T2.4.6.1.2"><span class="ltx_text" id="Ch5.T2.4.6.1.2.1" style="font-size:80%;">V19</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.6.2" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.6.2.1" style="font-size:80%;">63.5</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.6.3" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.6.3.1" style="font-size:80%;">45.5</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.6.4" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.6.4.1" style="font-size:80%;">64.0</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.6.5" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.6.5.1" style="font-size:80%;">65.7</span></td>
<td class="ltx_td ltx_align_center" id="Ch5.T2.4.6.6" style="padding-bottom:2.84544pt;"><span class="ltx_text" id="Ch5.T2.4.6.6.1" style="font-size:80%;">24</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T2.4.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Ch5.T2.4.7.1">
<span class="ltx_text" id="Ch5.T2.4.7.1.1" style="font-size:80%;">Centermask-Lite</span><sub class="ltx_sub" id="Ch5.T2.4.7.1.2"><span class="ltx_text" id="Ch5.T2.4.7.1.2.1" style="font-size:80%;">V39</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T2.4.7.2"><span class="ltx_text" id="Ch5.T2.4.7.2.1" style="font-size:80%;">64.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T2.4.7.3"><span class="ltx_text" id="Ch5.T2.4.7.3.1" style="font-size:80%;">46.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T2.4.7.4"><span class="ltx_text" id="Ch5.T2.4.7.4.1" style="font-size:80%;">64.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T2.4.7.5"><span class="ltx_text" id="Ch5.T2.4.7.5.1" style="font-size:80%;">66.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T2.4.7.6"><span class="ltx_text" id="Ch5.T2.4.7.6.1" style="font-size:80%;">28</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="Ch5.S2.p3">
<p class="ltx_p" id="Ch5.S2.p3.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T2" title="Table 5.2 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>, the robust methods, Mask R-CNN and DetectoRS, demonstrated superior mask <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> across all categories when compared to their real-time counterparts. DetectoRS, in particular, achieved the highest overall mAP, underscoring its effectiveness in accurate ship segmentation, however at the highest computational cost, even when using a high-end server.</p>
</div>
<div class="ltx_para" id="Ch5.S2.p4">
<p class="ltx_p" id="Ch5.S2.p4.1">For real-time applications, Centermask-Lite showcased better <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> performance, especially in handling small and medium-sized objects, while YOLACT was more adept at segmenting larger objects.
However, we observe that small-sized objects are segmented with a significant lower performance than the rest of object sizes for all methods.
As explained in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, small ship segmentation a critical problem in maritime monitoring and it will be tackled by this thesis in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="Ch5.S2.p5">
<p class="ltx_p" id="Ch5.S2.p5.1">While Centermask-Lite in its deeper form exhibits the best trade-off in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> and inference speed, deployment of instance segmentation on an embedded system, remained open.
This fact highlights the ongoing challenge of optimizing for both speed and accuracy in maritime object detection and segmentation.
The challenge stemmed from framework incompatibilities with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems, such as the NVIDIA Jetson AGX Xavier. Notably, this system utilizes an architecture based on ARM (Advanced Reduced Instruction Set Computing Machine) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib114" title="">sloss2004arm </a></cite>.
Memory constraints and the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm" title="Advanced Reduced instruction set computer Machine">ARM</span></a>-specific architecture compounded the difficulty of deploying Centermask-Lite.
The effective deployment of our custom real-time ship segmentation on the edge using an embedded system to fill this gap is shown in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="Ch5.F6.g1" src="extracted/5906916/fig/generalisation.png" width="530"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch5.F6.2.1.1" style="font-size:90%;">Figure 5.6</span>: </span><span class="ltx_text" id="Ch5.F6.3.2" style="font-size:90%;">Annotated masks on existing datasets to study the generalization of our models.
(a) Annotated examples of the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd" title="Singapore Maritime Dataset">Singapore Maritime Dataset (SMD)</span></a>. (b) Annotated examples of Seaships7000. (c) Annotated examples of the dataset by Chen et al. Reprinted from [BCP-II] (CC BY 4.0)</span></figcaption>
</figure>
<div class="ltx_para" id="Ch5.S2.p6">
<p class="ltx_p" id="Ch5.S2.p6.1">The generalization capability of models trained on ShipSG is crucial for deploying these models in diverse real-world maritime environments, where conditions and scenarios can vary significantly.
To assess the generalizability of the instance segmentation models presented in this section, I tested their performance on a mini-dataset of 100 images derived from other maritime datasets, namely the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.smd" title="Singapore Maritime Dataset">SMD</span></a> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib17" title="">prasad2017video </a></cite>, Seaships7000 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib60" title="">shao2018seaships </a></cite>, and the dataset by Chen et al <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib61" title="">chen2020video </a></cite>.
These datasets, which only used for testing in this work, provide a diverse range of maritime scenarios and vessel types to challenge the ability of the models to accurately segment ships in different conditions.
Since these datasets did not contain mask annotations, ship masks were annotated on the 100 images manually (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.F6" title="Figure 5.6 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.6</span></a>).
With DetectoRS leading the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> with 48.6%, the test revealed that models trained on the ShipSG dataset could predict ships from other datasets with reasonable accuracy, given the complex diversity of the mini-dataset.</p>
</div>
<div class="ltx_para" id="Ch5.S2.p7">
<p class="ltx_p" id="Ch5.S2.p7.1">The work presented in this section underpins the importance of instance segmentation in enhancing maritime safety and security applications.
By providing a detailed evaluation of different segmentation methods, it paves the way for the next research focus on optimizing instance segmentation models for real-time applications while deployed on an embedded system, which is tackled in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.
Moreover, georeferencing from the resulting masks of the evaluated methods of this section is quantitatively analyzed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">5.3   Summary and Discussion</h3>
<div class="ltx_para" id="Ch5.S3.p1">
<p class="ltx_p" id="Ch5.S3.p1.1">This chapter showcased the initial exploration in applying deep learning techniques for ship detection and instance segmentation within maritime applications for the improvement of situational awareness, presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.
Along the chapter, the pivotal role of ship detection in facilitating a range of applications has been highlighted, from abnormal vessel behavior detection or camera integrity assessment to 3D ship reconstruction.
Moreover, it underscored the enhanced capabilities that instance segmentation offers over bounding box detection, particularly in extracting detailed ship features crucial for applications like georeferencing, which is specially interesting for this thesis to improve maritime situational awareness.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p2">
<p class="ltx_p" id="Ch5.S3.p2.1">Despite the demonstrated potential of bounding box ship detection and success in controlled <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> or synthetic <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a> settings, several challenges and open tasks remain.
For example, the improvement of georeferencing and integrity assessment using segmented masks instead of bounding boxes or to unify detection and segmentation in the case of our 3D reconstruction framework.
Therefore, this opens the way to use instance segmentation instead of bounding box detection.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p3">
<p class="ltx_p" id="Ch5.S3.p3.1">In the initial exploration of standard instance segmentation techniques on the ShipSG dataset, we studied the precision of ship feature extraction, crucial for the various maritime applications.
This highlights the challenges associated with real-time processing.
The evaluation of real-time methods on the NVIDIA GV100 <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> revealed that the best trade-off between computational efficiency and segmentation accuracy, was given by Centermask-Lite.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p4">
<p class="ltx_p" id="Ch5.S3.p4.1">Though deployment of YOLOv5 (bounding box) using Pytorch weights is reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a> (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS3" title="5.1.3 Ship Detection for 3D Reconstruction [BCP-IV] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>), the deployment of instance segmentation on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems was not reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> (see Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>).
Deployment challenges arose from the incompatibility between deep learning and the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm" title="Advanced Reduced instruction set computer Machine">ARM</span></a> architectures of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems like the NVIDIA Jetson AGX Xavier.
This highlights the need for adaptable methods to enable advanced on-board instance segmentation processing.
The move towards embedded systems is essential for practical deployment in dynamic maritime environments, where processing speed and accuracy are paramount.
This transition to real-time instance segmentation on embedded systems is addressed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p5">
<p class="ltx_p" id="Ch5.S3.p5.1">Another critical aspect discussed is the importance of instance segmentation for accurate georeferencing of ships. We motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a> that while ship detection provides valuable insights for several maritime applications, instance segmentation offers a more detailed analysis crucial for precise georeferencing.
The ability to extract exact ship contours rather than relying on bounding boxes allows for more accurate positioning of vessels.
This capability is explored further in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>, which delves into the application of the methods proposed for improving maritime situational awareness through enhanced georeferencing.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p6">
<p class="ltx_p" id="Ch5.S3.p6.1">Furthermore, the initial instance segmentation study shown in this chapter, while promising, highlighted a precision decrease in segmenting small or distant ships.
This issue is particularly pertinent for maritime situational awareness, where the ability to accurately identify all vessels, independent their size and within the proximity of the port area is crucial.
Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S4" title="6.4 Enhanced Small Ship Segmentation Using Higher Resolution Images [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.4</span></a>, addresses this by introducing a solution that enhances the segmentation of small ships, thereby filling this gap in the initial methodology.</p>
</div>
<div class="ltx_para" id="Ch5.S3.p7">
<p class="ltx_p" id="Ch5.S3.p7.1">In essence, while the initial exploration into ship detection and instance segmentation revealed significant potential for enhancing maritime situational awareness, it also uncovered several challenges and areas for further development.
Specifically, the need for real-time processing on embedded systems, improved detection of small or distant ships, and the utilization of instance segmentation for accurate georeferencing.
The subsequent chapters of the thesis aim to address these gaps, presenting custom-tailored solutions that bring these advanced computer vision techniques closer to practical deployment in the maritime domain.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch6" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 6 </span>Advanced Ship Recognition for Real-time Operation</h2>
<div class="ltx_para" id="Ch6.p1">
<p class="ltx_p" id="Ch6.p1.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, we explored the impact of ship detection and segmentation in the improvement of maritime situational awareness and how the development of advanced methodologies can further improve results.
Moreover, the deployment of such algorithms on embedded systems has been proven important for practical deployment in dynamic maritime environments, where processing speed and accuracy are paramount.
We discussed the applicability of ship detection when deployed on an embedded system, as reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.
However, in the case of instance segmentation, task required for more accurate ship georeferencing, deployment on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems remained open. The deployment of instance segmentation methods highlighted the need for adaptable methods to enable advanced on-board processing.</p>
</div>
<div class="ltx_para" id="Ch6.p2">
<p class="ltx_p" id="Ch6.p2.1">We investigate in this chapter the proposed improvements for real-time ship segmentation proposed in this thesis, as introduced in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.
First, I present the ScatBlock, a 2D Scattering-transform-based block to be used in the proposed tailored deep-learning architecture.
Second, I delve into the design of the custom architecture, ScatYOLOv8+CBAM, that integrates the ScatBlock and attention mechanisms, and demonstrate its superior performance using ShipSG.
Thirdly, I propose an optimization to the architecture, followed by the deployment with TensoRT on the embedded system to measure inference times for real-time applicability.
Lastly, I address and propose a solution to the precision decrease in segmenting small and distant ships discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a> and essential for improved maritime situational awareness.</p>
</div>
<section class="ltx_section" id="Ch6.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">6.1   The ScatBlock <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>
</h3>
<div class="ltx_para" id="Ch6.S1.p1">
<p class="ltx_p" id="Ch6.S1.p1.1">The ScatBlock, introduced in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, is a custom designed 2D scattering-transform-based block that is key for the novel and tailored architecture for ship segmentation developed in this thesis.
The 2D scattering transform is a specialized operator that extracts invariant feature representations by decomposing the input image data into a set of scattering coefficients.
Each coefficient is a translation-invariant feature map representation that captures spatial and angular variations in an image.</p>
</div>
<div class="ltx_para" id="Ch6.S1.p2">
<p class="ltx_p" id="Ch6.S1.p2.3">Mathematically, the scattering transform is computed using a set of dilated and rotated versions of a mother wavelet <math alttext="\psi" class="ltx_Math" display="inline" id="Ch6.S1.p2.1.m1.1"><semantics id="Ch6.S1.p2.1.m1.1a"><mi id="Ch6.S1.p2.1.m1.1.1" xref="Ch6.S1.p2.1.m1.1.1.cmml">ψ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p2.1.m1.1b"><ci id="Ch6.S1.p2.1.m1.1.1.cmml" xref="Ch6.S1.p2.1.m1.1.1">𝜓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p2.1.m1.1c">\psi</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p2.1.m1.1d">italic_ψ</annotation></semantics></math> and a low-pass filter <math alttext="\phi_{J}" class="ltx_Math" display="inline" id="Ch6.S1.p2.2.m2.1"><semantics id="Ch6.S1.p2.2.m2.1a"><msub id="Ch6.S1.p2.2.m2.1.1" xref="Ch6.S1.p2.2.m2.1.1.cmml"><mi id="Ch6.S1.p2.2.m2.1.1.2" xref="Ch6.S1.p2.2.m2.1.1.2.cmml">ϕ</mi><mi id="Ch6.S1.p2.2.m2.1.1.3" xref="Ch6.S1.p2.2.m2.1.1.3.cmml">J</mi></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p2.2.m2.1b"><apply id="Ch6.S1.p2.2.m2.1.1.cmml" xref="Ch6.S1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p2.2.m2.1.1.1.cmml" xref="Ch6.S1.p2.2.m2.1.1">subscript</csymbol><ci id="Ch6.S1.p2.2.m2.1.1.2.cmml" xref="Ch6.S1.p2.2.m2.1.1.2">italic-ϕ</ci><ci id="Ch6.S1.p2.2.m2.1.1.3.cmml" xref="Ch6.S1.p2.2.m2.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p2.2.m2.1c">\phi_{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p2.2.m2.1d">italic_ϕ start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="J" class="ltx_Math" display="inline" id="Ch6.S1.p2.3.m3.1"><semantics id="Ch6.S1.p2.3.m3.1a"><mi id="Ch6.S1.p2.3.m3.1.1" xref="Ch6.S1.p2.3.m3.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p2.3.m3.1b"><ci id="Ch6.S1.p2.3.m3.1.1.cmml" xref="Ch6.S1.p2.3.m3.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p2.3.m3.1c">J</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p2.3.m3.1d">italic_J</annotation></semantics></math> being the spatial scale of the transform.
The process involves convolving the input image with a predefined filter bank, followed by an element-wise complex modulus operation:</p>
</div>
<div class="ltx_para" id="Ch6.S1.p3">
<table class="ltx_equation ltx_eqn_table" id="Ch6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="U_{\lambda}=\left|(x*\psi_{\lambda})\right|" class="ltx_Math" display="block" id="Ch6.E1.m1.1"><semantics id="Ch6.E1.m1.1a"><mrow id="Ch6.E1.m1.1.1" xref="Ch6.E1.m1.1.1.cmml"><msub id="Ch6.E1.m1.1.1.3" xref="Ch6.E1.m1.1.1.3.cmml"><mi id="Ch6.E1.m1.1.1.3.2" xref="Ch6.E1.m1.1.1.3.2.cmml">U</mi><mi id="Ch6.E1.m1.1.1.3.3" xref="Ch6.E1.m1.1.1.3.3.cmml">λ</mi></msub><mo id="Ch6.E1.m1.1.1.2" xref="Ch6.E1.m1.1.1.2.cmml">=</mo><mrow id="Ch6.E1.m1.1.1.1.1" xref="Ch6.E1.m1.1.1.1.2.cmml"><mo id="Ch6.E1.m1.1.1.1.1.2" xref="Ch6.E1.m1.1.1.1.2.1.cmml">|</mo><mrow id="Ch6.E1.m1.1.1.1.1.1.1" xref="Ch6.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="Ch6.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="Ch6.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.E1.m1.1.1.1.1.1.1.1" xref="Ch6.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="Ch6.E1.m1.1.1.1.1.1.1.1.2" xref="Ch6.E1.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="Ch6.E1.m1.1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.E1.m1.1.1.1.1.1.1.1.1.cmml">∗</mo><msub id="Ch6.E1.m1.1.1.1.1.1.1.1.3" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="Ch6.E1.m1.1.1.1.1.1.1.1.3.2" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3.2.cmml">ψ</mi><mi id="Ch6.E1.m1.1.1.1.1.1.1.1.3.3" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3.3.cmml">λ</mi></msub></mrow><mo id="Ch6.E1.m1.1.1.1.1.1.1.3" stretchy="false" xref="Ch6.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="Ch6.E1.m1.1.1.1.1.3" xref="Ch6.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.E1.m1.1b"><apply id="Ch6.E1.m1.1.1.cmml" xref="Ch6.E1.m1.1.1"><eq id="Ch6.E1.m1.1.1.2.cmml" xref="Ch6.E1.m1.1.1.2"></eq><apply id="Ch6.E1.m1.1.1.3.cmml" xref="Ch6.E1.m1.1.1.3"><csymbol cd="ambiguous" id="Ch6.E1.m1.1.1.3.1.cmml" xref="Ch6.E1.m1.1.1.3">subscript</csymbol><ci id="Ch6.E1.m1.1.1.3.2.cmml" xref="Ch6.E1.m1.1.1.3.2">𝑈</ci><ci id="Ch6.E1.m1.1.1.3.3.cmml" xref="Ch6.E1.m1.1.1.3.3">𝜆</ci></apply><apply id="Ch6.E1.m1.1.1.1.2.cmml" xref="Ch6.E1.m1.1.1.1.1"><abs id="Ch6.E1.m1.1.1.1.2.1.cmml" xref="Ch6.E1.m1.1.1.1.1.2"></abs><apply id="Ch6.E1.m1.1.1.1.1.1.1.1.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1"><times id="Ch6.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.1"></times><ci id="Ch6.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="Ch6.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Ch6.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Ch6.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3.2">𝜓</ci><ci id="Ch6.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="Ch6.E1.m1.1.1.1.1.1.1.1.3.3">𝜆</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.E1.m1.1c">U_{\lambda}=\left|(x*\psi_{\lambda})\right|</annotation><annotation encoding="application/x-llamapun" id="Ch6.E1.m1.1d">italic_U start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT = | ( italic_x ∗ italic_ψ start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ) |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch6.S1.p4">
<p class="ltx_p" id="Ch6.S1.p4.4">where <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S1.p4.1.m1.1"><semantics id="Ch6.S1.p4.1.m1.1a"><mi id="Ch6.S1.p4.1.m1.1.1" xref="Ch6.S1.p4.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p4.1.m1.1b"><ci id="Ch6.S1.p4.1.m1.1.1.cmml" xref="Ch6.S1.p4.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p4.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p4.1.m1.1d">italic_x</annotation></semantics></math> represents the original input image, and <math alttext="\psi_{\lambda}" class="ltx_Math" display="inline" id="Ch6.S1.p4.2.m2.1"><semantics id="Ch6.S1.p4.2.m2.1a"><msub id="Ch6.S1.p4.2.m2.1.1" xref="Ch6.S1.p4.2.m2.1.1.cmml"><mi id="Ch6.S1.p4.2.m2.1.1.2" xref="Ch6.S1.p4.2.m2.1.1.2.cmml">ψ</mi><mi id="Ch6.S1.p4.2.m2.1.1.3" xref="Ch6.S1.p4.2.m2.1.1.3.cmml">λ</mi></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p4.2.m2.1b"><apply id="Ch6.S1.p4.2.m2.1.1.cmml" xref="Ch6.S1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p4.2.m2.1.1.1.cmml" xref="Ch6.S1.p4.2.m2.1.1">subscript</csymbol><ci id="Ch6.S1.p4.2.m2.1.1.2.cmml" xref="Ch6.S1.p4.2.m2.1.1.2">𝜓</ci><ci id="Ch6.S1.p4.2.m2.1.1.3.cmml" xref="Ch6.S1.p4.2.m2.1.1.3">𝜆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p4.2.m2.1c">\psi_{\lambda}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p4.2.m2.1d">italic_ψ start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT</annotation></semantics></math> denotes the mother wavelet filter at a specific scale and orientation determined by <math alttext="\lambda" class="ltx_Math" display="inline" id="Ch6.S1.p4.3.m3.1"><semantics id="Ch6.S1.p4.3.m3.1a"><mi id="Ch6.S1.p4.3.m3.1.1" xref="Ch6.S1.p4.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p4.3.m3.1b"><ci id="Ch6.S1.p4.3.m3.1.1.cmml" xref="Ch6.S1.p4.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p4.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p4.3.m3.1d">italic_λ</annotation></semantics></math>. The output is obtained with a smoothing operation using the low-pass filter <math alttext="\phi" class="ltx_Math" display="inline" id="Ch6.S1.p4.4.m4.1"><semantics id="Ch6.S1.p4.4.m4.1a"><mi id="Ch6.S1.p4.4.m4.1.1" xref="Ch6.S1.p4.4.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p4.4.m4.1b"><ci id="Ch6.S1.p4.4.m4.1.1.cmml" xref="Ch6.S1.p4.4.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p4.4.m4.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p4.4.m4.1d">italic_ϕ</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="Ch6.S1.p5">
<table class="ltx_equation ltx_eqn_table" id="Ch6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="S_{\lambda}=U_{\lambda}*\phi_{J}" class="ltx_Math" display="block" id="Ch6.E2.m1.1"><semantics id="Ch6.E2.m1.1a"><mrow id="Ch6.E2.m1.1.1" xref="Ch6.E2.m1.1.1.cmml"><msub id="Ch6.E2.m1.1.1.2" xref="Ch6.E2.m1.1.1.2.cmml"><mi id="Ch6.E2.m1.1.1.2.2" xref="Ch6.E2.m1.1.1.2.2.cmml">S</mi><mi id="Ch6.E2.m1.1.1.2.3" xref="Ch6.E2.m1.1.1.2.3.cmml">λ</mi></msub><mo id="Ch6.E2.m1.1.1.1" xref="Ch6.E2.m1.1.1.1.cmml">=</mo><mrow id="Ch6.E2.m1.1.1.3" xref="Ch6.E2.m1.1.1.3.cmml"><msub id="Ch6.E2.m1.1.1.3.2" xref="Ch6.E2.m1.1.1.3.2.cmml"><mi id="Ch6.E2.m1.1.1.3.2.2" xref="Ch6.E2.m1.1.1.3.2.2.cmml">U</mi><mi id="Ch6.E2.m1.1.1.3.2.3" xref="Ch6.E2.m1.1.1.3.2.3.cmml">λ</mi></msub><mo id="Ch6.E2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="Ch6.E2.m1.1.1.3.1.cmml">∗</mo><msub id="Ch6.E2.m1.1.1.3.3" xref="Ch6.E2.m1.1.1.3.3.cmml"><mi id="Ch6.E2.m1.1.1.3.3.2" xref="Ch6.E2.m1.1.1.3.3.2.cmml">ϕ</mi><mi id="Ch6.E2.m1.1.1.3.3.3" xref="Ch6.E2.m1.1.1.3.3.3.cmml">J</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.E2.m1.1b"><apply id="Ch6.E2.m1.1.1.cmml" xref="Ch6.E2.m1.1.1"><eq id="Ch6.E2.m1.1.1.1.cmml" xref="Ch6.E2.m1.1.1.1"></eq><apply id="Ch6.E2.m1.1.1.2.cmml" xref="Ch6.E2.m1.1.1.2"><csymbol cd="ambiguous" id="Ch6.E2.m1.1.1.2.1.cmml" xref="Ch6.E2.m1.1.1.2">subscript</csymbol><ci id="Ch6.E2.m1.1.1.2.2.cmml" xref="Ch6.E2.m1.1.1.2.2">𝑆</ci><ci id="Ch6.E2.m1.1.1.2.3.cmml" xref="Ch6.E2.m1.1.1.2.3">𝜆</ci></apply><apply id="Ch6.E2.m1.1.1.3.cmml" xref="Ch6.E2.m1.1.1.3"><times id="Ch6.E2.m1.1.1.3.1.cmml" xref="Ch6.E2.m1.1.1.3.1"></times><apply id="Ch6.E2.m1.1.1.3.2.cmml" xref="Ch6.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="Ch6.E2.m1.1.1.3.2.1.cmml" xref="Ch6.E2.m1.1.1.3.2">subscript</csymbol><ci id="Ch6.E2.m1.1.1.3.2.2.cmml" xref="Ch6.E2.m1.1.1.3.2.2">𝑈</ci><ci id="Ch6.E2.m1.1.1.3.2.3.cmml" xref="Ch6.E2.m1.1.1.3.2.3">𝜆</ci></apply><apply id="Ch6.E2.m1.1.1.3.3.cmml" xref="Ch6.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="Ch6.E2.m1.1.1.3.3.1.cmml" xref="Ch6.E2.m1.1.1.3.3">subscript</csymbol><ci id="Ch6.E2.m1.1.1.3.3.2.cmml" xref="Ch6.E2.m1.1.1.3.3.2">italic-ϕ</ci><ci id="Ch6.E2.m1.1.1.3.3.3.cmml" xref="Ch6.E2.m1.1.1.3.3.3">𝐽</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.E2.m1.1c">S_{\lambda}=U_{\lambda}*\phi_{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.E2.m1.1d">italic_S start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT = italic_U start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ∗ italic_ϕ start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch6.S1.p6">
<p class="ltx_p" id="Ch6.S1.p6.5">where <math alttext="S_{\lambda}" class="ltx_Math" display="inline" id="Ch6.S1.p6.1.m1.1"><semantics id="Ch6.S1.p6.1.m1.1a"><msub id="Ch6.S1.p6.1.m1.1.1" xref="Ch6.S1.p6.1.m1.1.1.cmml"><mi id="Ch6.S1.p6.1.m1.1.1.2" xref="Ch6.S1.p6.1.m1.1.1.2.cmml">S</mi><mi id="Ch6.S1.p6.1.m1.1.1.3" xref="Ch6.S1.p6.1.m1.1.1.3.cmml">λ</mi></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p6.1.m1.1b"><apply id="Ch6.S1.p6.1.m1.1.1.cmml" xref="Ch6.S1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p6.1.m1.1.1.1.cmml" xref="Ch6.S1.p6.1.m1.1.1">subscript</csymbol><ci id="Ch6.S1.p6.1.m1.1.1.2.cmml" xref="Ch6.S1.p6.1.m1.1.1.2">𝑆</ci><ci id="Ch6.S1.p6.1.m1.1.1.3.cmml" xref="Ch6.S1.p6.1.m1.1.1.3">𝜆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p6.1.m1.1c">S_{\lambda}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p6.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT</annotation></semantics></math> are the scattering coefficients after the smoothing operation, which capture the invariant and descriptive features of the original image.
The total number of scattering coefficients (feature maps), <math alttext="J\times L+1" class="ltx_Math" display="inline" id="Ch6.S1.p6.2.m2.1"><semantics id="Ch6.S1.p6.2.m2.1a"><mrow id="Ch6.S1.p6.2.m2.1.1" xref="Ch6.S1.p6.2.m2.1.1.cmml"><mrow id="Ch6.S1.p6.2.m2.1.1.2" xref="Ch6.S1.p6.2.m2.1.1.2.cmml"><mi id="Ch6.S1.p6.2.m2.1.1.2.2" xref="Ch6.S1.p6.2.m2.1.1.2.2.cmml">J</mi><mo id="Ch6.S1.p6.2.m2.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S1.p6.2.m2.1.1.2.1.cmml">×</mo><mi id="Ch6.S1.p6.2.m2.1.1.2.3" xref="Ch6.S1.p6.2.m2.1.1.2.3.cmml">L</mi></mrow><mo id="Ch6.S1.p6.2.m2.1.1.1" xref="Ch6.S1.p6.2.m2.1.1.1.cmml">+</mo><mn id="Ch6.S1.p6.2.m2.1.1.3" xref="Ch6.S1.p6.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p6.2.m2.1b"><apply id="Ch6.S1.p6.2.m2.1.1.cmml" xref="Ch6.S1.p6.2.m2.1.1"><plus id="Ch6.S1.p6.2.m2.1.1.1.cmml" xref="Ch6.S1.p6.2.m2.1.1.1"></plus><apply id="Ch6.S1.p6.2.m2.1.1.2.cmml" xref="Ch6.S1.p6.2.m2.1.1.2"><times id="Ch6.S1.p6.2.m2.1.1.2.1.cmml" xref="Ch6.S1.p6.2.m2.1.1.2.1"></times><ci id="Ch6.S1.p6.2.m2.1.1.2.2.cmml" xref="Ch6.S1.p6.2.m2.1.1.2.2">𝐽</ci><ci id="Ch6.S1.p6.2.m2.1.1.2.3.cmml" xref="Ch6.S1.p6.2.m2.1.1.2.3">𝐿</ci></apply><cn id="Ch6.S1.p6.2.m2.1.1.3.cmml" type="integer" xref="Ch6.S1.p6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p6.2.m2.1c">J\times L+1</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p6.2.m2.1d">italic_J × italic_L + 1</annotation></semantics></math>, is determined by <math alttext="L" class="ltx_Math" display="inline" id="Ch6.S1.p6.3.m3.1"><semantics id="Ch6.S1.p6.3.m3.1a"><mi id="Ch6.S1.p6.3.m3.1.1" xref="Ch6.S1.p6.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p6.3.m3.1b"><ci id="Ch6.S1.p6.3.m3.1.1.cmml" xref="Ch6.S1.p6.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p6.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p6.3.m3.1d">italic_L</annotation></semantics></math>, the number of orientations or rotations of the mother wavelet <math alttext="\psi" class="ltx_Math" display="inline" id="Ch6.S1.p6.4.m4.1"><semantics id="Ch6.S1.p6.4.m4.1a"><mi id="Ch6.S1.p6.4.m4.1.1" xref="Ch6.S1.p6.4.m4.1.1.cmml">ψ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p6.4.m4.1b"><ci id="Ch6.S1.p6.4.m4.1.1.cmml" xref="Ch6.S1.p6.4.m4.1.1">𝜓</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p6.4.m4.1c">\psi</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p6.4.m4.1d">italic_ψ</annotation></semantics></math>, and <math alttext="J" class="ltx_Math" display="inline" id="Ch6.S1.p6.5.m5.1"><semantics id="Ch6.S1.p6.5.m5.1a"><mi id="Ch6.S1.p6.5.m5.1.1" xref="Ch6.S1.p6.5.m5.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p6.5.m5.1b"><ci id="Ch6.S1.p6.5.m5.1.1.cmml" xref="Ch6.S1.p6.5.m5.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p6.5.m5.1c">J</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p6.5.m5.1d">italic_J</annotation></semantics></math> the scale, or also known as order.</p>
</div>
<figure class="ltx_figure" id="Ch6.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="145" id="Ch6.F1.g1" src="extracted/5906916/fig/wavelet_examples.png" width="510"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F1.2.1.1" style="font-size:90%;">Figure 6.1</span>: </span><span class="ltx_text" id="Ch6.F1.3.2" style="font-size:90%;">Examples of commonly used 2D wavelets. From left to right: Gabor, Morlet, Daubechies and Symlet wavelets.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S1.p7">
<p class="ltx_p" id="Ch6.S1.p7.1">Commonly used 2D wavelets <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib115" title="">guo2022review </a></cite> are represented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F1" title="Figure 6.1 ‣ 6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>. When used in the 2D scattering transform, Gabor wavelets, though computationally expensive, are very sensitive to spatial frequencies and variations in textures. Morlet wavelets are characterized by their sinusoidal shape and can perform better with periodical patterns. Daubechies wavelets, can be useful for images with specific geometric patterns. Lastly, Symlet wavelets offer symmetry to preserve features and minimize distortion, which prevents artifacts in the scattering coefficients.</p>
</div>
<figure class="ltx_figure" id="Ch6.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="271" id="Ch6.F2.g1" src="extracted/5906916/fig/scattering_decomposition.png" width="608"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F2.8.7.1" style="font-size:90%;">Figure 6.2</span>: </span><span class="ltx_text" id="Ch6.F2.6.6" style="font-size:90%;">Scattering coefficient decomposition of an image <math alttext="x" class="ltx_Math" display="inline" id="Ch6.F2.1.1.m1.1"><semantics id="Ch6.F2.1.1.m1.1c"><mi id="Ch6.F2.1.1.m1.1.1" xref="Ch6.F2.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.F2.1.1.m1.1d"><ci id="Ch6.F2.1.1.m1.1.1.cmml" xref="Ch6.F2.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.1.1.m1.1e">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.1.1.m1.1f">italic_x</annotation></semantics></math>, showing low-pass filtering to obtain <math alttext="S_{0}" class="ltx_Math" display="inline" id="Ch6.F2.2.2.m2.1"><semantics id="Ch6.F2.2.2.m2.1c"><msub id="Ch6.F2.2.2.m2.1.1" xref="Ch6.F2.2.2.m2.1.1.cmml"><mi id="Ch6.F2.2.2.m2.1.1.2" xref="Ch6.F2.2.2.m2.1.1.2.cmml">S</mi><mn id="Ch6.F2.2.2.m2.1.1.3" xref="Ch6.F2.2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Ch6.F2.2.2.m2.1d"><apply id="Ch6.F2.2.2.m2.1.1.cmml" xref="Ch6.F2.2.2.m2.1.1"><csymbol cd="ambiguous" id="Ch6.F2.2.2.m2.1.1.1.cmml" xref="Ch6.F2.2.2.m2.1.1">subscript</csymbol><ci id="Ch6.F2.2.2.m2.1.1.2.cmml" xref="Ch6.F2.2.2.m2.1.1.2">𝑆</ci><cn id="Ch6.F2.2.2.m2.1.1.3.cmml" type="integer" xref="Ch6.F2.2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.2.2.m2.1e">S_{0}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.2.2.m2.1f">italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and wavelet modulus operations at orientations 0°, 45°, 90°, and 135° for scale <math alttext="2^{1}" class="ltx_Math" display="inline" id="Ch6.F2.3.3.m3.1"><semantics id="Ch6.F2.3.3.m3.1c"><msup id="Ch6.F2.3.3.m3.1.1" xref="Ch6.F2.3.3.m3.1.1.cmml"><mn id="Ch6.F2.3.3.m3.1.1.2" xref="Ch6.F2.3.3.m3.1.1.2.cmml">2</mn><mn id="Ch6.F2.3.3.m3.1.1.3" xref="Ch6.F2.3.3.m3.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="Ch6.F2.3.3.m3.1d"><apply id="Ch6.F2.3.3.m3.1.1.cmml" xref="Ch6.F2.3.3.m3.1.1"><csymbol cd="ambiguous" id="Ch6.F2.3.3.m3.1.1.1.cmml" xref="Ch6.F2.3.3.m3.1.1">superscript</csymbol><cn id="Ch6.F2.3.3.m3.1.1.2.cmml" type="integer" xref="Ch6.F2.3.3.m3.1.1.2">2</cn><cn id="Ch6.F2.3.3.m3.1.1.3.cmml" type="integer" xref="Ch6.F2.3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.3.3.m3.1e">2^{1}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.3.3.m3.1f">2 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> to produce first-order coefficients <math alttext="S_{\lambda_{1}}" class="ltx_Math" display="inline" id="Ch6.F2.4.4.m4.1"><semantics id="Ch6.F2.4.4.m4.1c"><msub id="Ch6.F2.4.4.m4.1.1" xref="Ch6.F2.4.4.m4.1.1.cmml"><mi id="Ch6.F2.4.4.m4.1.1.2" xref="Ch6.F2.4.4.m4.1.1.2.cmml">S</mi><msub id="Ch6.F2.4.4.m4.1.1.3" xref="Ch6.F2.4.4.m4.1.1.3.cmml"><mi id="Ch6.F2.4.4.m4.1.1.3.2" xref="Ch6.F2.4.4.m4.1.1.3.2.cmml">λ</mi><mn id="Ch6.F2.4.4.m4.1.1.3.3" xref="Ch6.F2.4.4.m4.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="Ch6.F2.4.4.m4.1d"><apply id="Ch6.F2.4.4.m4.1.1.cmml" xref="Ch6.F2.4.4.m4.1.1"><csymbol cd="ambiguous" id="Ch6.F2.4.4.m4.1.1.1.cmml" xref="Ch6.F2.4.4.m4.1.1">subscript</csymbol><ci id="Ch6.F2.4.4.m4.1.1.2.cmml" xref="Ch6.F2.4.4.m4.1.1.2">𝑆</ci><apply id="Ch6.F2.4.4.m4.1.1.3.cmml" xref="Ch6.F2.4.4.m4.1.1.3"><csymbol cd="ambiguous" id="Ch6.F2.4.4.m4.1.1.3.1.cmml" xref="Ch6.F2.4.4.m4.1.1.3">subscript</csymbol><ci id="Ch6.F2.4.4.m4.1.1.3.2.cmml" xref="Ch6.F2.4.4.m4.1.1.3.2">𝜆</ci><cn id="Ch6.F2.4.4.m4.1.1.3.3.cmml" type="integer" xref="Ch6.F2.4.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.4.4.m4.1e">S_{\lambda_{1}}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.4.4.m4.1f">italic_S start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>. Higher-order coefficients <math alttext="S_{\lambda_{1},\lambda_{2},...}" class="ltx_Math" display="inline" id="Ch6.F2.5.5.m5.3"><semantics id="Ch6.F2.5.5.m5.3c"><msub id="Ch6.F2.5.5.m5.3.4" xref="Ch6.F2.5.5.m5.3.4.cmml"><mi id="Ch6.F2.5.5.m5.3.4.2" xref="Ch6.F2.5.5.m5.3.4.2.cmml">S</mi><mrow id="Ch6.F2.5.5.m5.3.3.3.3" xref="Ch6.F2.5.5.m5.3.3.3.4.cmml"><msub id="Ch6.F2.5.5.m5.2.2.2.2.1" xref="Ch6.F2.5.5.m5.2.2.2.2.1.cmml"><mi id="Ch6.F2.5.5.m5.2.2.2.2.1.2" xref="Ch6.F2.5.5.m5.2.2.2.2.1.2.cmml">λ</mi><mn id="Ch6.F2.5.5.m5.2.2.2.2.1.3" xref="Ch6.F2.5.5.m5.2.2.2.2.1.3.cmml">1</mn></msub><mo id="Ch6.F2.5.5.m5.3.3.3.3.3" xref="Ch6.F2.5.5.m5.3.3.3.4.cmml">,</mo><msub id="Ch6.F2.5.5.m5.3.3.3.3.2" xref="Ch6.F2.5.5.m5.3.3.3.3.2.cmml"><mi id="Ch6.F2.5.5.m5.3.3.3.3.2.2" xref="Ch6.F2.5.5.m5.3.3.3.3.2.2.cmml">λ</mi><mn id="Ch6.F2.5.5.m5.3.3.3.3.2.3" xref="Ch6.F2.5.5.m5.3.3.3.3.2.3.cmml">2</mn></msub><mo id="Ch6.F2.5.5.m5.3.3.3.3.4" xref="Ch6.F2.5.5.m5.3.3.3.4.cmml">,</mo><mi id="Ch6.F2.5.5.m5.1.1.1.1" mathvariant="normal" xref="Ch6.F2.5.5.m5.1.1.1.1.cmml">…</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch6.F2.5.5.m5.3d"><apply id="Ch6.F2.5.5.m5.3.4.cmml" xref="Ch6.F2.5.5.m5.3.4"><csymbol cd="ambiguous" id="Ch6.F2.5.5.m5.3.4.1.cmml" xref="Ch6.F2.5.5.m5.3.4">subscript</csymbol><ci id="Ch6.F2.5.5.m5.3.4.2.cmml" xref="Ch6.F2.5.5.m5.3.4.2">𝑆</ci><list id="Ch6.F2.5.5.m5.3.3.3.4.cmml" xref="Ch6.F2.5.5.m5.3.3.3.3"><apply id="Ch6.F2.5.5.m5.2.2.2.2.1.cmml" xref="Ch6.F2.5.5.m5.2.2.2.2.1"><csymbol cd="ambiguous" id="Ch6.F2.5.5.m5.2.2.2.2.1.1.cmml" xref="Ch6.F2.5.5.m5.2.2.2.2.1">subscript</csymbol><ci id="Ch6.F2.5.5.m5.2.2.2.2.1.2.cmml" xref="Ch6.F2.5.5.m5.2.2.2.2.1.2">𝜆</ci><cn id="Ch6.F2.5.5.m5.2.2.2.2.1.3.cmml" type="integer" xref="Ch6.F2.5.5.m5.2.2.2.2.1.3">1</cn></apply><apply id="Ch6.F2.5.5.m5.3.3.3.3.2.cmml" xref="Ch6.F2.5.5.m5.3.3.3.3.2"><csymbol cd="ambiguous" id="Ch6.F2.5.5.m5.3.3.3.3.2.1.cmml" xref="Ch6.F2.5.5.m5.3.3.3.3.2">subscript</csymbol><ci id="Ch6.F2.5.5.m5.3.3.3.3.2.2.cmml" xref="Ch6.F2.5.5.m5.3.3.3.3.2.2">𝜆</ci><cn id="Ch6.F2.5.5.m5.3.3.3.3.2.3.cmml" type="integer" xref="Ch6.F2.5.5.m5.3.3.3.3.2.3">2</cn></apply><ci id="Ch6.F2.5.5.m5.1.1.1.1.cmml" xref="Ch6.F2.5.5.m5.1.1.1.1">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.5.5.m5.3e">S_{\lambda_{1},\lambda_{2},...}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.5.5.m5.3f">italic_S start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … end_POSTSUBSCRIPT</annotation></semantics></math> are obtained at scale <math alttext="2^{J}" class="ltx_Math" display="inline" id="Ch6.F2.6.6.m6.1"><semantics id="Ch6.F2.6.6.m6.1c"><msup id="Ch6.F2.6.6.m6.1.1" xref="Ch6.F2.6.6.m6.1.1.cmml"><mn id="Ch6.F2.6.6.m6.1.1.2" xref="Ch6.F2.6.6.m6.1.1.2.cmml">2</mn><mi id="Ch6.F2.6.6.m6.1.1.3" xref="Ch6.F2.6.6.m6.1.1.3.cmml">J</mi></msup><annotation-xml encoding="MathML-Content" id="Ch6.F2.6.6.m6.1d"><apply id="Ch6.F2.6.6.m6.1.1.cmml" xref="Ch6.F2.6.6.m6.1.1"><csymbol cd="ambiguous" id="Ch6.F2.6.6.m6.1.1.1.cmml" xref="Ch6.F2.6.6.m6.1.1">superscript</csymbol><cn id="Ch6.F2.6.6.m6.1.1.2.cmml" type="integer" xref="Ch6.F2.6.6.m6.1.1.2">2</cn><ci id="Ch6.F2.6.6.m6.1.1.3.cmml" xref="Ch6.F2.6.6.m6.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F2.6.6.m6.1e">2^{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F2.6.6.m6.1f">2 start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S1.p8">
<p class="ltx_p" id="Ch6.S1.p8.7">An illustrative example of the multi-scale decomposition in a scattering network is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F2" title="Figure 6.2 ‣ 6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>. The input image <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S1.p8.1.m1.1"><semantics id="Ch6.S1.p8.1.m1.1a"><mi id="Ch6.S1.p8.1.m1.1.1" xref="Ch6.S1.p8.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.1.m1.1b"><ci id="Ch6.S1.p8.1.m1.1.1.cmml" xref="Ch6.S1.p8.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.1.m1.1d">italic_x</annotation></semantics></math> is subjected to a low-pass filtering to produce <math alttext="S_{0}" class="ltx_Math" display="inline" id="Ch6.S1.p8.2.m2.1"><semantics id="Ch6.S1.p8.2.m2.1a"><msub id="Ch6.S1.p8.2.m2.1.1" xref="Ch6.S1.p8.2.m2.1.1.cmml"><mi id="Ch6.S1.p8.2.m2.1.1.2" xref="Ch6.S1.p8.2.m2.1.1.2.cmml">S</mi><mn id="Ch6.S1.p8.2.m2.1.1.3" xref="Ch6.S1.p8.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.2.m2.1b"><apply id="Ch6.S1.p8.2.m2.1.1.cmml" xref="Ch6.S1.p8.2.m2.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p8.2.m2.1.1.1.cmml" xref="Ch6.S1.p8.2.m2.1.1">subscript</csymbol><ci id="Ch6.S1.p8.2.m2.1.1.2.cmml" xref="Ch6.S1.p8.2.m2.1.1.2">𝑆</ci><cn id="Ch6.S1.p8.2.m2.1.1.3.cmml" type="integer" xref="Ch6.S1.p8.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.2.m2.1c">S_{0}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.2.m2.1d">italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, and to <math alttext="Gabor" class="ltx_Math" display="inline" id="Ch6.S1.p8.3.m3.1"><semantics id="Ch6.S1.p8.3.m3.1a"><mrow id="Ch6.S1.p8.3.m3.1.1" xref="Ch6.S1.p8.3.m3.1.1.cmml"><mi id="Ch6.S1.p8.3.m3.1.1.2" xref="Ch6.S1.p8.3.m3.1.1.2.cmml">G</mi><mo id="Ch6.S1.p8.3.m3.1.1.1" xref="Ch6.S1.p8.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p8.3.m3.1.1.3" xref="Ch6.S1.p8.3.m3.1.1.3.cmml">a</mi><mo id="Ch6.S1.p8.3.m3.1.1.1a" xref="Ch6.S1.p8.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p8.3.m3.1.1.4" xref="Ch6.S1.p8.3.m3.1.1.4.cmml">b</mi><mo id="Ch6.S1.p8.3.m3.1.1.1b" xref="Ch6.S1.p8.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p8.3.m3.1.1.5" xref="Ch6.S1.p8.3.m3.1.1.5.cmml">o</mi><mo id="Ch6.S1.p8.3.m3.1.1.1c" xref="Ch6.S1.p8.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p8.3.m3.1.1.6" xref="Ch6.S1.p8.3.m3.1.1.6.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.3.m3.1b"><apply id="Ch6.S1.p8.3.m3.1.1.cmml" xref="Ch6.S1.p8.3.m3.1.1"><times id="Ch6.S1.p8.3.m3.1.1.1.cmml" xref="Ch6.S1.p8.3.m3.1.1.1"></times><ci id="Ch6.S1.p8.3.m3.1.1.2.cmml" xref="Ch6.S1.p8.3.m3.1.1.2">𝐺</ci><ci id="Ch6.S1.p8.3.m3.1.1.3.cmml" xref="Ch6.S1.p8.3.m3.1.1.3">𝑎</ci><ci id="Ch6.S1.p8.3.m3.1.1.4.cmml" xref="Ch6.S1.p8.3.m3.1.1.4">𝑏</ci><ci id="Ch6.S1.p8.3.m3.1.1.5.cmml" xref="Ch6.S1.p8.3.m3.1.1.5">𝑜</ci><ci id="Ch6.S1.p8.3.m3.1.1.6.cmml" xref="Ch6.S1.p8.3.m3.1.1.6">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.3.m3.1c">Gabor</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.3.m3.1d">italic_G italic_a italic_b italic_o italic_r</annotation></semantics></math> wavelet convolutions with four orientations at the first scale <math alttext="2^{1}" class="ltx_Math" display="inline" id="Ch6.S1.p8.4.m4.1"><semantics id="Ch6.S1.p8.4.m4.1a"><msup id="Ch6.S1.p8.4.m4.1.1" xref="Ch6.S1.p8.4.m4.1.1.cmml"><mn id="Ch6.S1.p8.4.m4.1.1.2" xref="Ch6.S1.p8.4.m4.1.1.2.cmml">2</mn><mn id="Ch6.S1.p8.4.m4.1.1.3" xref="Ch6.S1.p8.4.m4.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.4.m4.1b"><apply id="Ch6.S1.p8.4.m4.1.1.cmml" xref="Ch6.S1.p8.4.m4.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p8.4.m4.1.1.1.cmml" xref="Ch6.S1.p8.4.m4.1.1">superscript</csymbol><cn id="Ch6.S1.p8.4.m4.1.1.2.cmml" type="integer" xref="Ch6.S1.p8.4.m4.1.1.2">2</cn><cn id="Ch6.S1.p8.4.m4.1.1.3.cmml" type="integer" xref="Ch6.S1.p8.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.4.m4.1c">2^{1}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.4.m4.1d">2 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>. The modulus of each convolved image is taken, followed by another low-pass filtering to yield the first-order scattering coefficients <math alttext="S_{\lambda_{1}}" class="ltx_Math" display="inline" id="Ch6.S1.p8.5.m5.1"><semantics id="Ch6.S1.p8.5.m5.1a"><msub id="Ch6.S1.p8.5.m5.1.1" xref="Ch6.S1.p8.5.m5.1.1.cmml"><mi id="Ch6.S1.p8.5.m5.1.1.2" xref="Ch6.S1.p8.5.m5.1.1.2.cmml">S</mi><msub id="Ch6.S1.p8.5.m5.1.1.3" xref="Ch6.S1.p8.5.m5.1.1.3.cmml"><mi id="Ch6.S1.p8.5.m5.1.1.3.2" xref="Ch6.S1.p8.5.m5.1.1.3.2.cmml">λ</mi><mn id="Ch6.S1.p8.5.m5.1.1.3.3" xref="Ch6.S1.p8.5.m5.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.5.m5.1b"><apply id="Ch6.S1.p8.5.m5.1.1.cmml" xref="Ch6.S1.p8.5.m5.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p8.5.m5.1.1.1.cmml" xref="Ch6.S1.p8.5.m5.1.1">subscript</csymbol><ci id="Ch6.S1.p8.5.m5.1.1.2.cmml" xref="Ch6.S1.p8.5.m5.1.1.2">𝑆</ci><apply id="Ch6.S1.p8.5.m5.1.1.3.cmml" xref="Ch6.S1.p8.5.m5.1.1.3"><csymbol cd="ambiguous" id="Ch6.S1.p8.5.m5.1.1.3.1.cmml" xref="Ch6.S1.p8.5.m5.1.1.3">subscript</csymbol><ci id="Ch6.S1.p8.5.m5.1.1.3.2.cmml" xref="Ch6.S1.p8.5.m5.1.1.3.2">𝜆</ci><cn id="Ch6.S1.p8.5.m5.1.1.3.3.cmml" type="integer" xref="Ch6.S1.p8.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.5.m5.1c">S_{\lambda_{1}}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.5.m5.1d">italic_S start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>. This process is repeated iteratively to produce higher-order coefficients <math alttext="S_{\lambda_{1},\lambda_{2}}" class="ltx_Math" display="inline" id="Ch6.S1.p8.6.m6.2"><semantics id="Ch6.S1.p8.6.m6.2a"><msub id="Ch6.S1.p8.6.m6.2.3" xref="Ch6.S1.p8.6.m6.2.3.cmml"><mi id="Ch6.S1.p8.6.m6.2.3.2" xref="Ch6.S1.p8.6.m6.2.3.2.cmml">S</mi><mrow id="Ch6.S1.p8.6.m6.2.2.2.2" xref="Ch6.S1.p8.6.m6.2.2.2.3.cmml"><msub id="Ch6.S1.p8.6.m6.1.1.1.1.1" xref="Ch6.S1.p8.6.m6.1.1.1.1.1.cmml"><mi id="Ch6.S1.p8.6.m6.1.1.1.1.1.2" xref="Ch6.S1.p8.6.m6.1.1.1.1.1.2.cmml">λ</mi><mn id="Ch6.S1.p8.6.m6.1.1.1.1.1.3" xref="Ch6.S1.p8.6.m6.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Ch6.S1.p8.6.m6.2.2.2.2.3" xref="Ch6.S1.p8.6.m6.2.2.2.3.cmml">,</mo><msub id="Ch6.S1.p8.6.m6.2.2.2.2.2" xref="Ch6.S1.p8.6.m6.2.2.2.2.2.cmml"><mi id="Ch6.S1.p8.6.m6.2.2.2.2.2.2" xref="Ch6.S1.p8.6.m6.2.2.2.2.2.2.cmml">λ</mi><mn id="Ch6.S1.p8.6.m6.2.2.2.2.2.3" xref="Ch6.S1.p8.6.m6.2.2.2.2.2.3.cmml">2</mn></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.6.m6.2b"><apply id="Ch6.S1.p8.6.m6.2.3.cmml" xref="Ch6.S1.p8.6.m6.2.3"><csymbol cd="ambiguous" id="Ch6.S1.p8.6.m6.2.3.1.cmml" xref="Ch6.S1.p8.6.m6.2.3">subscript</csymbol><ci id="Ch6.S1.p8.6.m6.2.3.2.cmml" xref="Ch6.S1.p8.6.m6.2.3.2">𝑆</ci><list id="Ch6.S1.p8.6.m6.2.2.2.3.cmml" xref="Ch6.S1.p8.6.m6.2.2.2.2"><apply id="Ch6.S1.p8.6.m6.1.1.1.1.1.cmml" xref="Ch6.S1.p8.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p8.6.m6.1.1.1.1.1.1.cmml" xref="Ch6.S1.p8.6.m6.1.1.1.1.1">subscript</csymbol><ci id="Ch6.S1.p8.6.m6.1.1.1.1.1.2.cmml" xref="Ch6.S1.p8.6.m6.1.1.1.1.1.2">𝜆</ci><cn id="Ch6.S1.p8.6.m6.1.1.1.1.1.3.cmml" type="integer" xref="Ch6.S1.p8.6.m6.1.1.1.1.1.3">1</cn></apply><apply id="Ch6.S1.p8.6.m6.2.2.2.2.2.cmml" xref="Ch6.S1.p8.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="Ch6.S1.p8.6.m6.2.2.2.2.2.1.cmml" xref="Ch6.S1.p8.6.m6.2.2.2.2.2">subscript</csymbol><ci id="Ch6.S1.p8.6.m6.2.2.2.2.2.2.cmml" xref="Ch6.S1.p8.6.m6.2.2.2.2.2.2">𝜆</ci><cn id="Ch6.S1.p8.6.m6.2.2.2.2.2.3.cmml" type="integer" xref="Ch6.S1.p8.6.m6.2.2.2.2.2.3">2</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.6.m6.2c">S_{\lambda_{1},\lambda_{2}}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.6.m6.2d">italic_S start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, with wavelet convolutions at increasing scales <math alttext="2^{J}" class="ltx_Math" display="inline" id="Ch6.S1.p8.7.m7.1"><semantics id="Ch6.S1.p8.7.m7.1a"><msup id="Ch6.S1.p8.7.m7.1.1" xref="Ch6.S1.p8.7.m7.1.1.cmml"><mn id="Ch6.S1.p8.7.m7.1.1.2" xref="Ch6.S1.p8.7.m7.1.1.2.cmml">2</mn><mi id="Ch6.S1.p8.7.m7.1.1.3" xref="Ch6.S1.p8.7.m7.1.1.3.cmml">J</mi></msup><annotation-xml encoding="MathML-Content" id="Ch6.S1.p8.7.m7.1b"><apply id="Ch6.S1.p8.7.m7.1.1.cmml" xref="Ch6.S1.p8.7.m7.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p8.7.m7.1.1.1.cmml" xref="Ch6.S1.p8.7.m7.1.1">superscript</csymbol><cn id="Ch6.S1.p8.7.m7.1.1.2.cmml" type="integer" xref="Ch6.S1.p8.7.m7.1.1.2">2</cn><ci id="Ch6.S1.p8.7.m7.1.1.3.cmml" xref="Ch6.S1.p8.7.m7.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p8.7.m7.1c">2^{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p8.7.m7.1d">2 start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT</annotation></semantics></math>, capturing progressively coarser image features. The network cascades through multiple scales to extract robust, invariant features for image analysis.
We observe that each scattering coefficient is a translation-invariant feature map representation that captures spatial and angular variations in the input image.</p>
</div>
<figure class="ltx_figure" id="Ch6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="97" id="Ch6.F3.g1" src="extracted/5906916/fig/scatblock.jpg" width="255"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F3.2.1.1" style="font-size:90%;">Figure 6.3</span>: </span><span class="ltx_text" id="Ch6.F3.3.2" style="font-size:90%;">ScatBlock, as conceived in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, contains an upsample operation, followed by the scattering layer and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu" title="Rectified Linear Unit">Rectified Linear Unit (ReLU)</span></a> activation. Adapted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S1.p9">
<p class="ltx_p" id="Ch6.S1.p9.4">To incorporate the scattering transform, the ScatBlock architecture (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F3" title="Figure 6.3 ‣ 6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>) begins by upsampling the input image to counteract the resolution decrease inherent to the 2D scattering transform, which typically downsamples the input image by a factor of <math alttext="2^{J}" class="ltx_Math" display="inline" id="Ch6.S1.p9.1.m1.1"><semantics id="Ch6.S1.p9.1.m1.1a"><msup id="Ch6.S1.p9.1.m1.1.1" xref="Ch6.S1.p9.1.m1.1.1.cmml"><mn id="Ch6.S1.p9.1.m1.1.1.2" xref="Ch6.S1.p9.1.m1.1.1.2.cmml">2</mn><mi id="Ch6.S1.p9.1.m1.1.1.3" xref="Ch6.S1.p9.1.m1.1.1.3.cmml">J</mi></msup><annotation-xml encoding="MathML-Content" id="Ch6.S1.p9.1.m1.1b"><apply id="Ch6.S1.p9.1.m1.1.1.cmml" xref="Ch6.S1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.S1.p9.1.m1.1.1.1.cmml" xref="Ch6.S1.p9.1.m1.1.1">superscript</csymbol><cn id="Ch6.S1.p9.1.m1.1.1.2.cmml" type="integer" xref="Ch6.S1.p9.1.m1.1.1.2">2</cn><ci id="Ch6.S1.p9.1.m1.1.1.3.cmml" xref="Ch6.S1.p9.1.m1.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p9.1.m1.1c">2^{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p9.1.m1.1d">2 start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT</annotation></semantics></math> to reduce computational complexity across scales.
Specifically, the input image <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S1.p9.2.m2.1"><semantics id="Ch6.S1.p9.2.m2.1a"><mi id="Ch6.S1.p9.2.m2.1.1" xref="Ch6.S1.p9.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S1.p9.2.m2.1b"><ci id="Ch6.S1.p9.2.m2.1.1.cmml" xref="Ch6.S1.p9.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p9.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p9.2.m2.1d">italic_x</annotation></semantics></math> is upscaled to <math alttext="(2\times H)\times(2\times W)" class="ltx_Math" display="inline" id="Ch6.S1.p9.3.m3.2"><semantics id="Ch6.S1.p9.3.m3.2a"><mrow id="Ch6.S1.p9.3.m3.2.2" xref="Ch6.S1.p9.3.m3.2.2.cmml"><mrow id="Ch6.S1.p9.3.m3.1.1.1.1" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.cmml"><mo id="Ch6.S1.p9.3.m3.1.1.1.1.2" stretchy="false" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.S1.p9.3.m3.1.1.1.1.1" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.cmml"><mn id="Ch6.S1.p9.3.m3.1.1.1.1.1.2" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.2.cmml">2</mn><mo id="Ch6.S1.p9.3.m3.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.1.cmml">×</mo><mi id="Ch6.S1.p9.3.m3.1.1.1.1.1.3" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.3.cmml">H</mi></mrow><mo id="Ch6.S1.p9.3.m3.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mo id="Ch6.S1.p9.3.m3.2.2.3" rspace="0.222em" xref="Ch6.S1.p9.3.m3.2.2.3.cmml">×</mo><mrow id="Ch6.S1.p9.3.m3.2.2.2.1" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.cmml"><mo id="Ch6.S1.p9.3.m3.2.2.2.1.2" stretchy="false" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.cmml">(</mo><mrow id="Ch6.S1.p9.3.m3.2.2.2.1.1" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.cmml"><mn id="Ch6.S1.p9.3.m3.2.2.2.1.1.2" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.2.cmml">2</mn><mo id="Ch6.S1.p9.3.m3.2.2.2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.1.cmml">×</mo><mi id="Ch6.S1.p9.3.m3.2.2.2.1.1.3" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.3.cmml">W</mi></mrow><mo id="Ch6.S1.p9.3.m3.2.2.2.1.3" stretchy="false" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p9.3.m3.2b"><apply id="Ch6.S1.p9.3.m3.2.2.cmml" xref="Ch6.S1.p9.3.m3.2.2"><times id="Ch6.S1.p9.3.m3.2.2.3.cmml" xref="Ch6.S1.p9.3.m3.2.2.3"></times><apply id="Ch6.S1.p9.3.m3.1.1.1.1.1.cmml" xref="Ch6.S1.p9.3.m3.1.1.1.1"><times id="Ch6.S1.p9.3.m3.1.1.1.1.1.1.cmml" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.1"></times><cn id="Ch6.S1.p9.3.m3.1.1.1.1.1.2.cmml" type="integer" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.2">2</cn><ci id="Ch6.S1.p9.3.m3.1.1.1.1.1.3.cmml" xref="Ch6.S1.p9.3.m3.1.1.1.1.1.3">𝐻</ci></apply><apply id="Ch6.S1.p9.3.m3.2.2.2.1.1.cmml" xref="Ch6.S1.p9.3.m3.2.2.2.1"><times id="Ch6.S1.p9.3.m3.2.2.2.1.1.1.cmml" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.1"></times><cn id="Ch6.S1.p9.3.m3.2.2.2.1.1.2.cmml" type="integer" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.2">2</cn><ci id="Ch6.S1.p9.3.m3.2.2.2.1.1.3.cmml" xref="Ch6.S1.p9.3.m3.2.2.2.1.1.3">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p9.3.m3.2c">(2\times H)\times(2\times W)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p9.3.m3.2d">( 2 × italic_H ) × ( 2 × italic_W )</annotation></semantics></math>, ensuring the output dimensionality aligns with the input image size, a requirement for subsequent processing layers in the backbone.
These sparse feature maps resulting from the scattering transform are then forwarded to a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu" title="Rectified Linear Unit">ReLU</span></a> activation function.
The ScatBlock conceived in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> uses the first-order coefficients (<math alttext="J=1" class="ltx_Math" display="inline" id="Ch6.S1.p9.4.m4.1"><semantics id="Ch6.S1.p9.4.m4.1a"><mrow id="Ch6.S1.p9.4.m4.1.1" xref="Ch6.S1.p9.4.m4.1.1.cmml"><mi id="Ch6.S1.p9.4.m4.1.1.2" xref="Ch6.S1.p9.4.m4.1.1.2.cmml">J</mi><mo id="Ch6.S1.p9.4.m4.1.1.1" xref="Ch6.S1.p9.4.m4.1.1.1.cmml">=</mo><mn id="Ch6.S1.p9.4.m4.1.1.3" xref="Ch6.S1.p9.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p9.4.m4.1b"><apply id="Ch6.S1.p9.4.m4.1.1.cmml" xref="Ch6.S1.p9.4.m4.1.1"><eq id="Ch6.S1.p9.4.m4.1.1.1.cmml" xref="Ch6.S1.p9.4.m4.1.1.1"></eq><ci id="Ch6.S1.p9.4.m4.1.1.2.cmml" xref="Ch6.S1.p9.4.m4.1.1.2">𝐽</ci><cn id="Ch6.S1.p9.4.m4.1.1.3.cmml" type="integer" xref="Ch6.S1.p9.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p9.4.m4.1c">J=1</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p9.4.m4.1d">italic_J = 1</annotation></semantics></math>).
Although the computation of subsequent orders is achievable by incorporating more layers, the first-order coefficients hold significant information for mainstream tasks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib87" title="">bruna2013invariant </a></cite>.
In this case, the first order was found to be sufficient to enrich the capability of the model to discern and segment ships while maintaining fast computation time.</p>
</div>
<div class="ltx_para" id="Ch6.S1.p10">
<p class="ltx_p" id="Ch6.S1.p10.3">Regarding implementation, the ScatBlock developed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> uses the approach developed in reference <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite> to achieve the scattering transform, using the open-source Python module <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S1.p10.1.m1.1"><semantics id="Ch6.S1.p10.1.m1.1a"><mrow id="Ch6.S1.p10.1.m1.1.1" xref="Ch6.S1.p10.1.m1.1.1.cmml"><mi id="Ch6.S1.p10.1.m1.1.1.2" xref="Ch6.S1.p10.1.m1.1.1.2.cmml">p</mi><mo id="Ch6.S1.p10.1.m1.1.1.1" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.3" xref="Ch6.S1.p10.1.m1.1.1.3.cmml">y</mi><mo id="Ch6.S1.p10.1.m1.1.1.1a" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.4" xref="Ch6.S1.p10.1.m1.1.1.4.cmml">t</mi><mo id="Ch6.S1.p10.1.m1.1.1.1b" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.5" xref="Ch6.S1.p10.1.m1.1.1.5.cmml">o</mi><mo id="Ch6.S1.p10.1.m1.1.1.1c" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.6" xref="Ch6.S1.p10.1.m1.1.1.6.cmml">r</mi><mo id="Ch6.S1.p10.1.m1.1.1.1d" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.7" xref="Ch6.S1.p10.1.m1.1.1.7.cmml">c</mi><mo id="Ch6.S1.p10.1.m1.1.1.1e" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.8" xref="Ch6.S1.p10.1.m1.1.1.8.cmml">h</mi><mo id="Ch6.S1.p10.1.m1.1.1.1f" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.9" mathvariant="normal" xref="Ch6.S1.p10.1.m1.1.1.9.cmml">_</mi><mo id="Ch6.S1.p10.1.m1.1.1.1g" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.10" xref="Ch6.S1.p10.1.m1.1.1.10.cmml">w</mi><mo id="Ch6.S1.p10.1.m1.1.1.1h" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.11" xref="Ch6.S1.p10.1.m1.1.1.11.cmml">a</mi><mo id="Ch6.S1.p10.1.m1.1.1.1i" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.12" xref="Ch6.S1.p10.1.m1.1.1.12.cmml">v</mi><mo id="Ch6.S1.p10.1.m1.1.1.1j" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.13" xref="Ch6.S1.p10.1.m1.1.1.13.cmml">e</mi><mo id="Ch6.S1.p10.1.m1.1.1.1k" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.14" xref="Ch6.S1.p10.1.m1.1.1.14.cmml">l</mi><mo id="Ch6.S1.p10.1.m1.1.1.1l" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.15" xref="Ch6.S1.p10.1.m1.1.1.15.cmml">e</mi><mo id="Ch6.S1.p10.1.m1.1.1.1m" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.16" xref="Ch6.S1.p10.1.m1.1.1.16.cmml">t</mi><mo id="Ch6.S1.p10.1.m1.1.1.1n" xref="Ch6.S1.p10.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.1.m1.1.1.17" xref="Ch6.S1.p10.1.m1.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p10.1.m1.1b"><apply id="Ch6.S1.p10.1.m1.1.1.cmml" xref="Ch6.S1.p10.1.m1.1.1"><times id="Ch6.S1.p10.1.m1.1.1.1.cmml" xref="Ch6.S1.p10.1.m1.1.1.1"></times><ci id="Ch6.S1.p10.1.m1.1.1.2.cmml" xref="Ch6.S1.p10.1.m1.1.1.2">𝑝</ci><ci id="Ch6.S1.p10.1.m1.1.1.3.cmml" xref="Ch6.S1.p10.1.m1.1.1.3">𝑦</ci><ci id="Ch6.S1.p10.1.m1.1.1.4.cmml" xref="Ch6.S1.p10.1.m1.1.1.4">𝑡</ci><ci id="Ch6.S1.p10.1.m1.1.1.5.cmml" xref="Ch6.S1.p10.1.m1.1.1.5">𝑜</ci><ci id="Ch6.S1.p10.1.m1.1.1.6.cmml" xref="Ch6.S1.p10.1.m1.1.1.6">𝑟</ci><ci id="Ch6.S1.p10.1.m1.1.1.7.cmml" xref="Ch6.S1.p10.1.m1.1.1.7">𝑐</ci><ci id="Ch6.S1.p10.1.m1.1.1.8.cmml" xref="Ch6.S1.p10.1.m1.1.1.8">ℎ</ci><ci id="Ch6.S1.p10.1.m1.1.1.9.cmml" xref="Ch6.S1.p10.1.m1.1.1.9">_</ci><ci id="Ch6.S1.p10.1.m1.1.1.10.cmml" xref="Ch6.S1.p10.1.m1.1.1.10">𝑤</ci><ci id="Ch6.S1.p10.1.m1.1.1.11.cmml" xref="Ch6.S1.p10.1.m1.1.1.11">𝑎</ci><ci id="Ch6.S1.p10.1.m1.1.1.12.cmml" xref="Ch6.S1.p10.1.m1.1.1.12">𝑣</ci><ci id="Ch6.S1.p10.1.m1.1.1.13.cmml" xref="Ch6.S1.p10.1.m1.1.1.13">𝑒</ci><ci id="Ch6.S1.p10.1.m1.1.1.14.cmml" xref="Ch6.S1.p10.1.m1.1.1.14">𝑙</ci><ci id="Ch6.S1.p10.1.m1.1.1.15.cmml" xref="Ch6.S1.p10.1.m1.1.1.15">𝑒</ci><ci id="Ch6.S1.p10.1.m1.1.1.16.cmml" xref="Ch6.S1.p10.1.m1.1.1.16">𝑡</ci><ci id="Ch6.S1.p10.1.m1.1.1.17.cmml" xref="Ch6.S1.p10.1.m1.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p10.1.m1.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p10.1.m1.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="Ch6.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/fbcotter/pytorch_wavelets/" title="">https://github.com/fbcotter/pytorch_wavelets/</a></span></span></span>.
The scattering transform implementation in <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S1.p10.2.m2.1"><semantics id="Ch6.S1.p10.2.m2.1a"><mrow id="Ch6.S1.p10.2.m2.1.1" xref="Ch6.S1.p10.2.m2.1.1.cmml"><mi id="Ch6.S1.p10.2.m2.1.1.2" xref="Ch6.S1.p10.2.m2.1.1.2.cmml">p</mi><mo id="Ch6.S1.p10.2.m2.1.1.1" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.3" xref="Ch6.S1.p10.2.m2.1.1.3.cmml">y</mi><mo id="Ch6.S1.p10.2.m2.1.1.1a" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.4" xref="Ch6.S1.p10.2.m2.1.1.4.cmml">t</mi><mo id="Ch6.S1.p10.2.m2.1.1.1b" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.5" xref="Ch6.S1.p10.2.m2.1.1.5.cmml">o</mi><mo id="Ch6.S1.p10.2.m2.1.1.1c" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.6" xref="Ch6.S1.p10.2.m2.1.1.6.cmml">r</mi><mo id="Ch6.S1.p10.2.m2.1.1.1d" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.7" xref="Ch6.S1.p10.2.m2.1.1.7.cmml">c</mi><mo id="Ch6.S1.p10.2.m2.1.1.1e" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.8" xref="Ch6.S1.p10.2.m2.1.1.8.cmml">h</mi><mo id="Ch6.S1.p10.2.m2.1.1.1f" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.9" mathvariant="normal" xref="Ch6.S1.p10.2.m2.1.1.9.cmml">_</mi><mo id="Ch6.S1.p10.2.m2.1.1.1g" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.10" xref="Ch6.S1.p10.2.m2.1.1.10.cmml">w</mi><mo id="Ch6.S1.p10.2.m2.1.1.1h" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.11" xref="Ch6.S1.p10.2.m2.1.1.11.cmml">a</mi><mo id="Ch6.S1.p10.2.m2.1.1.1i" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.12" xref="Ch6.S1.p10.2.m2.1.1.12.cmml">v</mi><mo id="Ch6.S1.p10.2.m2.1.1.1j" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.13" xref="Ch6.S1.p10.2.m2.1.1.13.cmml">e</mi><mo id="Ch6.S1.p10.2.m2.1.1.1k" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.14" xref="Ch6.S1.p10.2.m2.1.1.14.cmml">l</mi><mo id="Ch6.S1.p10.2.m2.1.1.1l" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.15" xref="Ch6.S1.p10.2.m2.1.1.15.cmml">e</mi><mo id="Ch6.S1.p10.2.m2.1.1.1m" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.16" xref="Ch6.S1.p10.2.m2.1.1.16.cmml">t</mi><mo id="Ch6.S1.p10.2.m2.1.1.1n" xref="Ch6.S1.p10.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.2.m2.1.1.17" xref="Ch6.S1.p10.2.m2.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p10.2.m2.1b"><apply id="Ch6.S1.p10.2.m2.1.1.cmml" xref="Ch6.S1.p10.2.m2.1.1"><times id="Ch6.S1.p10.2.m2.1.1.1.cmml" xref="Ch6.S1.p10.2.m2.1.1.1"></times><ci id="Ch6.S1.p10.2.m2.1.1.2.cmml" xref="Ch6.S1.p10.2.m2.1.1.2">𝑝</ci><ci id="Ch6.S1.p10.2.m2.1.1.3.cmml" xref="Ch6.S1.p10.2.m2.1.1.3">𝑦</ci><ci id="Ch6.S1.p10.2.m2.1.1.4.cmml" xref="Ch6.S1.p10.2.m2.1.1.4">𝑡</ci><ci id="Ch6.S1.p10.2.m2.1.1.5.cmml" xref="Ch6.S1.p10.2.m2.1.1.5">𝑜</ci><ci id="Ch6.S1.p10.2.m2.1.1.6.cmml" xref="Ch6.S1.p10.2.m2.1.1.6">𝑟</ci><ci id="Ch6.S1.p10.2.m2.1.1.7.cmml" xref="Ch6.S1.p10.2.m2.1.1.7">𝑐</ci><ci id="Ch6.S1.p10.2.m2.1.1.8.cmml" xref="Ch6.S1.p10.2.m2.1.1.8">ℎ</ci><ci id="Ch6.S1.p10.2.m2.1.1.9.cmml" xref="Ch6.S1.p10.2.m2.1.1.9">_</ci><ci id="Ch6.S1.p10.2.m2.1.1.10.cmml" xref="Ch6.S1.p10.2.m2.1.1.10">𝑤</ci><ci id="Ch6.S1.p10.2.m2.1.1.11.cmml" xref="Ch6.S1.p10.2.m2.1.1.11">𝑎</ci><ci id="Ch6.S1.p10.2.m2.1.1.12.cmml" xref="Ch6.S1.p10.2.m2.1.1.12">𝑣</ci><ci id="Ch6.S1.p10.2.m2.1.1.13.cmml" xref="Ch6.S1.p10.2.m2.1.1.13">𝑒</ci><ci id="Ch6.S1.p10.2.m2.1.1.14.cmml" xref="Ch6.S1.p10.2.m2.1.1.14">𝑙</ci><ci id="Ch6.S1.p10.2.m2.1.1.15.cmml" xref="Ch6.S1.p10.2.m2.1.1.15">𝑒</ci><ci id="Ch6.S1.p10.2.m2.1.1.16.cmml" xref="Ch6.S1.p10.2.m2.1.1.16">𝑡</ci><ci id="Ch6.S1.p10.2.m2.1.1.17.cmml" xref="Ch6.S1.p10.2.m2.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p10.2.m2.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p10.2.m2.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> is based on the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">Dual-Tree Complex Wavelet Transform (DTCWT)</span></a>. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">DTCWT</span></a>, initially introduced by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib117" title="">selesnick2005dual </a></cite>, computes the scattering transform with enhanced efficiency while ensuring theoretical consistency with the traditional approach. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">DTCWT</span></a>, employs wavelet trees for signal decomposition in frequencies, facilitating information capture and directional selectivity.
This decomposition allows for a detailed analysis of the isolated frequency content across both horizontal and vertical dimensions of the image, revealing textures and patterns. The approach proposed by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite> in <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S1.p10.3.m3.1"><semantics id="Ch6.S1.p10.3.m3.1a"><mrow id="Ch6.S1.p10.3.m3.1.1" xref="Ch6.S1.p10.3.m3.1.1.cmml"><mi id="Ch6.S1.p10.3.m3.1.1.2" xref="Ch6.S1.p10.3.m3.1.1.2.cmml">p</mi><mo id="Ch6.S1.p10.3.m3.1.1.1" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.3" xref="Ch6.S1.p10.3.m3.1.1.3.cmml">y</mi><mo id="Ch6.S1.p10.3.m3.1.1.1a" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.4" xref="Ch6.S1.p10.3.m3.1.1.4.cmml">t</mi><mo id="Ch6.S1.p10.3.m3.1.1.1b" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.5" xref="Ch6.S1.p10.3.m3.1.1.5.cmml">o</mi><mo id="Ch6.S1.p10.3.m3.1.1.1c" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.6" xref="Ch6.S1.p10.3.m3.1.1.6.cmml">r</mi><mo id="Ch6.S1.p10.3.m3.1.1.1d" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.7" xref="Ch6.S1.p10.3.m3.1.1.7.cmml">c</mi><mo id="Ch6.S1.p10.3.m3.1.1.1e" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.8" xref="Ch6.S1.p10.3.m3.1.1.8.cmml">h</mi><mo id="Ch6.S1.p10.3.m3.1.1.1f" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.9" mathvariant="normal" xref="Ch6.S1.p10.3.m3.1.1.9.cmml">_</mi><mo id="Ch6.S1.p10.3.m3.1.1.1g" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.10" xref="Ch6.S1.p10.3.m3.1.1.10.cmml">w</mi><mo id="Ch6.S1.p10.3.m3.1.1.1h" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.11" xref="Ch6.S1.p10.3.m3.1.1.11.cmml">a</mi><mo id="Ch6.S1.p10.3.m3.1.1.1i" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.12" xref="Ch6.S1.p10.3.m3.1.1.12.cmml">v</mi><mo id="Ch6.S1.p10.3.m3.1.1.1j" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.13" xref="Ch6.S1.p10.3.m3.1.1.13.cmml">e</mi><mo id="Ch6.S1.p10.3.m3.1.1.1k" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.14" xref="Ch6.S1.p10.3.m3.1.1.14.cmml">l</mi><mo id="Ch6.S1.p10.3.m3.1.1.1l" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.15" xref="Ch6.S1.p10.3.m3.1.1.15.cmml">e</mi><mo id="Ch6.S1.p10.3.m3.1.1.1m" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.16" xref="Ch6.S1.p10.3.m3.1.1.16.cmml">t</mi><mo id="Ch6.S1.p10.3.m3.1.1.1n" xref="Ch6.S1.p10.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p10.3.m3.1.1.17" xref="Ch6.S1.p10.3.m3.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p10.3.m3.1b"><apply id="Ch6.S1.p10.3.m3.1.1.cmml" xref="Ch6.S1.p10.3.m3.1.1"><times id="Ch6.S1.p10.3.m3.1.1.1.cmml" xref="Ch6.S1.p10.3.m3.1.1.1"></times><ci id="Ch6.S1.p10.3.m3.1.1.2.cmml" xref="Ch6.S1.p10.3.m3.1.1.2">𝑝</ci><ci id="Ch6.S1.p10.3.m3.1.1.3.cmml" xref="Ch6.S1.p10.3.m3.1.1.3">𝑦</ci><ci id="Ch6.S1.p10.3.m3.1.1.4.cmml" xref="Ch6.S1.p10.3.m3.1.1.4">𝑡</ci><ci id="Ch6.S1.p10.3.m3.1.1.5.cmml" xref="Ch6.S1.p10.3.m3.1.1.5">𝑜</ci><ci id="Ch6.S1.p10.3.m3.1.1.6.cmml" xref="Ch6.S1.p10.3.m3.1.1.6">𝑟</ci><ci id="Ch6.S1.p10.3.m3.1.1.7.cmml" xref="Ch6.S1.p10.3.m3.1.1.7">𝑐</ci><ci id="Ch6.S1.p10.3.m3.1.1.8.cmml" xref="Ch6.S1.p10.3.m3.1.1.8">ℎ</ci><ci id="Ch6.S1.p10.3.m3.1.1.9.cmml" xref="Ch6.S1.p10.3.m3.1.1.9">_</ci><ci id="Ch6.S1.p10.3.m3.1.1.10.cmml" xref="Ch6.S1.p10.3.m3.1.1.10">𝑤</ci><ci id="Ch6.S1.p10.3.m3.1.1.11.cmml" xref="Ch6.S1.p10.3.m3.1.1.11">𝑎</ci><ci id="Ch6.S1.p10.3.m3.1.1.12.cmml" xref="Ch6.S1.p10.3.m3.1.1.12">𝑣</ci><ci id="Ch6.S1.p10.3.m3.1.1.13.cmml" xref="Ch6.S1.p10.3.m3.1.1.13">𝑒</ci><ci id="Ch6.S1.p10.3.m3.1.1.14.cmml" xref="Ch6.S1.p10.3.m3.1.1.14">𝑙</ci><ci id="Ch6.S1.p10.3.m3.1.1.15.cmml" xref="Ch6.S1.p10.3.m3.1.1.15">𝑒</ci><ci id="Ch6.S1.p10.3.m3.1.1.16.cmml" xref="Ch6.S1.p10.3.m3.1.1.16">𝑡</ci><ci id="Ch6.S1.p10.3.m3.1.1.17.cmml" xref="Ch6.S1.p10.3.m3.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p10.3.m3.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p10.3.m3.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> to achieve the scattering transform not only enhances efficiency on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> s due to the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">DTCWT</span></a> fast convolution capabilities and suitability for parallel processing, but also ensures the robustness and invariance of the extracted scattering coefficients. A detailed technical description of how the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">DTCWT</span></a> is used for a faster scattering transform can be found in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite>.</p>
</div>
<div class="ltx_para" id="Ch6.S1.p11">
<p class="ltx_p" id="Ch6.S1.p11.2">Another option to implement the 2D scattering transform is the package Kymatio <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib118" title="">andreux2020kymatio </a></cite>, which closely mirrors the traditional scattering transform in its approach but falls short in computational speed compared to <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S1.p11.1.m1.1"><semantics id="Ch6.S1.p11.1.m1.1a"><mrow id="Ch6.S1.p11.1.m1.1.1" xref="Ch6.S1.p11.1.m1.1.1.cmml"><mi id="Ch6.S1.p11.1.m1.1.1.2" xref="Ch6.S1.p11.1.m1.1.1.2.cmml">p</mi><mo id="Ch6.S1.p11.1.m1.1.1.1" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.3" xref="Ch6.S1.p11.1.m1.1.1.3.cmml">y</mi><mo id="Ch6.S1.p11.1.m1.1.1.1a" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.4" xref="Ch6.S1.p11.1.m1.1.1.4.cmml">t</mi><mo id="Ch6.S1.p11.1.m1.1.1.1b" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.5" xref="Ch6.S1.p11.1.m1.1.1.5.cmml">o</mi><mo id="Ch6.S1.p11.1.m1.1.1.1c" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.6" xref="Ch6.S1.p11.1.m1.1.1.6.cmml">r</mi><mo id="Ch6.S1.p11.1.m1.1.1.1d" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.7" xref="Ch6.S1.p11.1.m1.1.1.7.cmml">c</mi><mo id="Ch6.S1.p11.1.m1.1.1.1e" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.8" xref="Ch6.S1.p11.1.m1.1.1.8.cmml">h</mi><mo id="Ch6.S1.p11.1.m1.1.1.1f" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.9" mathvariant="normal" xref="Ch6.S1.p11.1.m1.1.1.9.cmml">_</mi><mo id="Ch6.S1.p11.1.m1.1.1.1g" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.10" xref="Ch6.S1.p11.1.m1.1.1.10.cmml">w</mi><mo id="Ch6.S1.p11.1.m1.1.1.1h" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.11" xref="Ch6.S1.p11.1.m1.1.1.11.cmml">a</mi><mo id="Ch6.S1.p11.1.m1.1.1.1i" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.12" xref="Ch6.S1.p11.1.m1.1.1.12.cmml">v</mi><mo id="Ch6.S1.p11.1.m1.1.1.1j" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.13" xref="Ch6.S1.p11.1.m1.1.1.13.cmml">e</mi><mo id="Ch6.S1.p11.1.m1.1.1.1k" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.14" xref="Ch6.S1.p11.1.m1.1.1.14.cmml">l</mi><mo id="Ch6.S1.p11.1.m1.1.1.1l" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.15" xref="Ch6.S1.p11.1.m1.1.1.15.cmml">e</mi><mo id="Ch6.S1.p11.1.m1.1.1.1m" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.16" xref="Ch6.S1.p11.1.m1.1.1.16.cmml">t</mi><mo id="Ch6.S1.p11.1.m1.1.1.1n" xref="Ch6.S1.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.1.m1.1.1.17" xref="Ch6.S1.p11.1.m1.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p11.1.m1.1b"><apply id="Ch6.S1.p11.1.m1.1.1.cmml" xref="Ch6.S1.p11.1.m1.1.1"><times id="Ch6.S1.p11.1.m1.1.1.1.cmml" xref="Ch6.S1.p11.1.m1.1.1.1"></times><ci id="Ch6.S1.p11.1.m1.1.1.2.cmml" xref="Ch6.S1.p11.1.m1.1.1.2">𝑝</ci><ci id="Ch6.S1.p11.1.m1.1.1.3.cmml" xref="Ch6.S1.p11.1.m1.1.1.3">𝑦</ci><ci id="Ch6.S1.p11.1.m1.1.1.4.cmml" xref="Ch6.S1.p11.1.m1.1.1.4">𝑡</ci><ci id="Ch6.S1.p11.1.m1.1.1.5.cmml" xref="Ch6.S1.p11.1.m1.1.1.5">𝑜</ci><ci id="Ch6.S1.p11.1.m1.1.1.6.cmml" xref="Ch6.S1.p11.1.m1.1.1.6">𝑟</ci><ci id="Ch6.S1.p11.1.m1.1.1.7.cmml" xref="Ch6.S1.p11.1.m1.1.1.7">𝑐</ci><ci id="Ch6.S1.p11.1.m1.1.1.8.cmml" xref="Ch6.S1.p11.1.m1.1.1.8">ℎ</ci><ci id="Ch6.S1.p11.1.m1.1.1.9.cmml" xref="Ch6.S1.p11.1.m1.1.1.9">_</ci><ci id="Ch6.S1.p11.1.m1.1.1.10.cmml" xref="Ch6.S1.p11.1.m1.1.1.10">𝑤</ci><ci id="Ch6.S1.p11.1.m1.1.1.11.cmml" xref="Ch6.S1.p11.1.m1.1.1.11">𝑎</ci><ci id="Ch6.S1.p11.1.m1.1.1.12.cmml" xref="Ch6.S1.p11.1.m1.1.1.12">𝑣</ci><ci id="Ch6.S1.p11.1.m1.1.1.13.cmml" xref="Ch6.S1.p11.1.m1.1.1.13">𝑒</ci><ci id="Ch6.S1.p11.1.m1.1.1.14.cmml" xref="Ch6.S1.p11.1.m1.1.1.14">𝑙</ci><ci id="Ch6.S1.p11.1.m1.1.1.15.cmml" xref="Ch6.S1.p11.1.m1.1.1.15">𝑒</ci><ci id="Ch6.S1.p11.1.m1.1.1.16.cmml" xref="Ch6.S1.p11.1.m1.1.1.16">𝑡</ci><ci id="Ch6.S1.p11.1.m1.1.1.17.cmml" xref="Ch6.S1.p11.1.m1.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p11.1.m1.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p11.1.m1.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math>. The use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dtcwt" title="Dual-Tree Complex Wavelet Transform">DTCWT</span></a> by the latter for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-optimized computations significantly accelerates performance, making <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S1.p11.2.m2.1"><semantics id="Ch6.S1.p11.2.m2.1a"><mrow id="Ch6.S1.p11.2.m2.1.1" xref="Ch6.S1.p11.2.m2.1.1.cmml"><mi id="Ch6.S1.p11.2.m2.1.1.2" xref="Ch6.S1.p11.2.m2.1.1.2.cmml">p</mi><mo id="Ch6.S1.p11.2.m2.1.1.1" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.3" xref="Ch6.S1.p11.2.m2.1.1.3.cmml">y</mi><mo id="Ch6.S1.p11.2.m2.1.1.1a" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.4" xref="Ch6.S1.p11.2.m2.1.1.4.cmml">t</mi><mo id="Ch6.S1.p11.2.m2.1.1.1b" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.5" xref="Ch6.S1.p11.2.m2.1.1.5.cmml">o</mi><mo id="Ch6.S1.p11.2.m2.1.1.1c" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.6" xref="Ch6.S1.p11.2.m2.1.1.6.cmml">r</mi><mo id="Ch6.S1.p11.2.m2.1.1.1d" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.7" xref="Ch6.S1.p11.2.m2.1.1.7.cmml">c</mi><mo id="Ch6.S1.p11.2.m2.1.1.1e" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.8" xref="Ch6.S1.p11.2.m2.1.1.8.cmml">h</mi><mo id="Ch6.S1.p11.2.m2.1.1.1f" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.9" mathvariant="normal" xref="Ch6.S1.p11.2.m2.1.1.9.cmml">_</mi><mo id="Ch6.S1.p11.2.m2.1.1.1g" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.10" xref="Ch6.S1.p11.2.m2.1.1.10.cmml">w</mi><mo id="Ch6.S1.p11.2.m2.1.1.1h" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.11" xref="Ch6.S1.p11.2.m2.1.1.11.cmml">a</mi><mo id="Ch6.S1.p11.2.m2.1.1.1i" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.12" xref="Ch6.S1.p11.2.m2.1.1.12.cmml">v</mi><mo id="Ch6.S1.p11.2.m2.1.1.1j" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.13" xref="Ch6.S1.p11.2.m2.1.1.13.cmml">e</mi><mo id="Ch6.S1.p11.2.m2.1.1.1k" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.14" xref="Ch6.S1.p11.2.m2.1.1.14.cmml">l</mi><mo id="Ch6.S1.p11.2.m2.1.1.1l" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.15" xref="Ch6.S1.p11.2.m2.1.1.15.cmml">e</mi><mo id="Ch6.S1.p11.2.m2.1.1.1m" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.16" xref="Ch6.S1.p11.2.m2.1.1.16.cmml">t</mi><mo id="Ch6.S1.p11.2.m2.1.1.1n" xref="Ch6.S1.p11.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S1.p11.2.m2.1.1.17" xref="Ch6.S1.p11.2.m2.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S1.p11.2.m2.1b"><apply id="Ch6.S1.p11.2.m2.1.1.cmml" xref="Ch6.S1.p11.2.m2.1.1"><times id="Ch6.S1.p11.2.m2.1.1.1.cmml" xref="Ch6.S1.p11.2.m2.1.1.1"></times><ci id="Ch6.S1.p11.2.m2.1.1.2.cmml" xref="Ch6.S1.p11.2.m2.1.1.2">𝑝</ci><ci id="Ch6.S1.p11.2.m2.1.1.3.cmml" xref="Ch6.S1.p11.2.m2.1.1.3">𝑦</ci><ci id="Ch6.S1.p11.2.m2.1.1.4.cmml" xref="Ch6.S1.p11.2.m2.1.1.4">𝑡</ci><ci id="Ch6.S1.p11.2.m2.1.1.5.cmml" xref="Ch6.S1.p11.2.m2.1.1.5">𝑜</ci><ci id="Ch6.S1.p11.2.m2.1.1.6.cmml" xref="Ch6.S1.p11.2.m2.1.1.6">𝑟</ci><ci id="Ch6.S1.p11.2.m2.1.1.7.cmml" xref="Ch6.S1.p11.2.m2.1.1.7">𝑐</ci><ci id="Ch6.S1.p11.2.m2.1.1.8.cmml" xref="Ch6.S1.p11.2.m2.1.1.8">ℎ</ci><ci id="Ch6.S1.p11.2.m2.1.1.9.cmml" xref="Ch6.S1.p11.2.m2.1.1.9">_</ci><ci id="Ch6.S1.p11.2.m2.1.1.10.cmml" xref="Ch6.S1.p11.2.m2.1.1.10">𝑤</ci><ci id="Ch6.S1.p11.2.m2.1.1.11.cmml" xref="Ch6.S1.p11.2.m2.1.1.11">𝑎</ci><ci id="Ch6.S1.p11.2.m2.1.1.12.cmml" xref="Ch6.S1.p11.2.m2.1.1.12">𝑣</ci><ci id="Ch6.S1.p11.2.m2.1.1.13.cmml" xref="Ch6.S1.p11.2.m2.1.1.13">𝑒</ci><ci id="Ch6.S1.p11.2.m2.1.1.14.cmml" xref="Ch6.S1.p11.2.m2.1.1.14">𝑙</ci><ci id="Ch6.S1.p11.2.m2.1.1.15.cmml" xref="Ch6.S1.p11.2.m2.1.1.15">𝑒</ci><ci id="Ch6.S1.p11.2.m2.1.1.16.cmml" xref="Ch6.S1.p11.2.m2.1.1.16">𝑡</ci><ci id="Ch6.S1.p11.2.m2.1.1.17.cmml" xref="Ch6.S1.p11.2.m2.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S1.p11.2.m2.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S1.p11.2.m2.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> the preferred choice for the scattering transform implementations of the ScatBlock due to its efficiency.</p>
</div>
<div class="ltx_para" id="Ch6.S1.p12">
<p class="ltx_p" id="Ch6.S1.p12.1">As introduced in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3" title="Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3</span></a>, the work proposed in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib87" title="">bruna2013invariant </a></cite> demonstrated that the features extracted by the scattering transform are quite meaningful for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">Convolutional Neural Networks (CNNs)</span></a>.
The motivation drawn in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib87" title="">bruna2013invariant </a></cite> lies in the ability of the transform to provide a deep systematic understanding of how invariant features can be captured and utilized for improved deep-learning-based image classification.
This approach is particularly relevant for maritime awareness, where the recognition of ships across diverse sizes and types, under varying lighting and weather conditions, demands a robust feature extraction mechanism that can handle the complexities of real-world scenarios.
The scattering transform using wavelets are particularly suited for ship recognition because the output coefficients excel at capturing multi-scale geometric and structural features, and the relatively uniform water background provides a good contrast for highlighting ships against the static background.
Having this in mind, the ScatBlock has been designed to capture the shape of ships by extracting their inherent geometric and structural properties, and will be added to the custom architecture for ship recognition explained in the following section.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">6.2   ScatYOLOv8+CBAM <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>
</h3>
<div class="ltx_para" id="Ch6.S2.p1">
<p class="ltx_p" id="Ch6.S2.p1.1">To conform the customized ship segmentation architecture presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, named ScatYOLOv8+CBAM, two additions were implemented.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p2">
<p class="ltx_p" id="Ch6.S2.p2.1">Firstly, the ScatBlock was blended (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>) at the beginning of the backbone of YOLOv8 to enhance the input image for instance segmentation, replacing the first convolutional block of YOLOv8.
This was motivated by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib88" title="">oyallon2018compressing </a></cite>. In their work, the authors explore the efficiency of using the scattering transform to preprocess images before feeding them into <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a> s, showcasing how this technique can significantly enhance the quality of feature representations. The relevance of the findings of <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib88" title="">oyallon2018compressing </a></cite> to the ScatBlock lie in the practical application of the scattering transform to streamline the processing pipeline for real-time instance segmentation and object recognition tasks.
The advantage of YOLOv8-like architectures is their deployability on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems, which enables deployment of the custom architecture as well.</p>
</div>
<figure class="ltx_figure" id="Ch6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="138" id="Ch6.F4.g1" src="extracted/5906916/fig/cbam.jpg" width="255"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F4.2.1.1" style="font-size:90%;">Figure 6.4</span>: </span><a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" style="font-size:90%;" title="Convolutional Block Attention Module">Convolutional Block Attention Module (CBAM)</span></a><span class="ltx_text" id="Ch6.F4.3.2" style="font-size:90%;"> module introduced by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>, depicting the channel and
spatial attention mechanisms. Adapted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
</figure>
<figure class="ltx_figure" id="Ch6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="267" id="Ch6.F5.g1" src="extracted/5906916/fig/cam_sam_cbam.png" width="510"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F5.2.1.1" style="font-size:90%;">Figure 6.5</span>: </span><span class="ltx_text" id="Ch6.F5.3.2" style="font-size:90%;">Diagram of each attention sub-module in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a>, taken from <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>. The channel attention module (top) uses both max-pooling and average-pooling operations, followed by a shared <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.mlp"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.mlp" title="Multi-layer Perceptron">Multi-layer Perceptron (MLP)</span></a>, to generate channel attention. The spatial attention module (bottom) applies max-pooling and average-pooling across the channels, then uses a convolutional layer to generate spatial attention.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S2.p3">
<p class="ltx_p" id="Ch6.S2.p3.1">The second addition to the network is the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a>. Introduced by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>, this module advances <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a> s by embedding attention mechanisms with minimal computational overhead. It is structured around two key components (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F4" title="Figure 6.4 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.4</span></a>). The channel attention module, being the initial component, discerns the importance of each feature channel, effectively determining “what” is significant in the given feature map and enhancing these channels while suppressing less relevant ones. This is achieved using global average pooling and max pooling, which aggregate information efficiently without heavy computation. The channel attention map <math alttext="M_{c}" class="ltx_Math" display="inline" id="Ch6.S2.p3.1.m1.1"><semantics id="Ch6.S2.p3.1.m1.1a"><msub id="Ch6.S2.p3.1.m1.1.1" xref="Ch6.S2.p3.1.m1.1.1.cmml"><mi id="Ch6.S2.p3.1.m1.1.1.2" xref="Ch6.S2.p3.1.m1.1.1.2.cmml">M</mi><mi id="Ch6.S2.p3.1.m1.1.1.3" xref="Ch6.S2.p3.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.1.m1.1b"><apply id="Ch6.S2.p3.1.m1.1.1.cmml" xref="Ch6.S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.S2.p3.1.m1.1.1.1.cmml" xref="Ch6.S2.p3.1.m1.1.1">subscript</csymbol><ci id="Ch6.S2.p3.1.m1.1.1.2.cmml" xref="Ch6.S2.p3.1.m1.1.1.2">𝑀</ci><ci id="Ch6.S2.p3.1.m1.1.1.3.cmml" xref="Ch6.S2.p3.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.1.m1.1c">M_{c}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.1.m1.1d">italic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is computed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="Ch6.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M_{c}(F)=\sigma(W_{1}\cdot(\text{ReLU}(W_{0}\cdot\text{AvgPool}(F)))+W_{1}%
\cdot(\text{ReLU}(W_{0}\cdot\text{MaxPool}(F))))" class="ltx_Math" display="block" id="Ch6.E3.m1.4"><semantics id="Ch6.E3.m1.4a"><mrow id="Ch6.E3.m1.4.4" xref="Ch6.E3.m1.4.4.cmml"><mrow id="Ch6.E3.m1.4.4.3" xref="Ch6.E3.m1.4.4.3.cmml"><msub id="Ch6.E3.m1.4.4.3.2" xref="Ch6.E3.m1.4.4.3.2.cmml"><mi id="Ch6.E3.m1.4.4.3.2.2" xref="Ch6.E3.m1.4.4.3.2.2.cmml">M</mi><mi id="Ch6.E3.m1.4.4.3.2.3" xref="Ch6.E3.m1.4.4.3.2.3.cmml">c</mi></msub><mo id="Ch6.E3.m1.4.4.3.1" xref="Ch6.E3.m1.4.4.3.1.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.3.3.2" xref="Ch6.E3.m1.4.4.3.cmml"><mo id="Ch6.E3.m1.4.4.3.3.2.1" stretchy="false" xref="Ch6.E3.m1.4.4.3.cmml">(</mo><mi id="Ch6.E3.m1.1.1" xref="Ch6.E3.m1.1.1.cmml">F</mi><mo id="Ch6.E3.m1.4.4.3.3.2.2" stretchy="false" xref="Ch6.E3.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.2" xref="Ch6.E3.m1.4.4.2.cmml">=</mo><mrow id="Ch6.E3.m1.4.4.1" xref="Ch6.E3.m1.4.4.1.cmml"><mi id="Ch6.E3.m1.4.4.1.3" xref="Ch6.E3.m1.4.4.1.3.cmml">σ</mi><mo id="Ch6.E3.m1.4.4.1.2" xref="Ch6.E3.m1.4.4.1.2.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.cmml"><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.cmml"><msub id="Ch6.E3.m1.4.4.1.1.1.1.1.3" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3.cmml"><mi id="Ch6.E3.m1.4.4.1.1.1.1.1.3.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3.2.cmml">W</mi><mn id="Ch6.E3.m1.4.4.1.1.1.1.1.3.3" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3.3.cmml">1</mn></msub><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="Ch6.E3.m1.4.4.1.1.1.1.1.2.cmml">⋅</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3a.cmml">ReLU</mtext><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">W</mi><mn id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">0</mn></msub><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⋅</mo><mtext id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">AvgPool</mtext></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="Ch6.E3.m1.2.2" xref="Ch6.E3.m1.2.2.cmml">F</mi><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.3" xref="Ch6.E3.m1.4.4.1.1.1.1.3.cmml">+</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.cmml"><msub id="Ch6.E3.m1.4.4.1.1.1.1.2.3" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3.cmml"><mi id="Ch6.E3.m1.4.4.1.1.1.1.2.3.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3.2.cmml">W</mi><mn id="Ch6.E3.m1.4.4.1.1.1.1.2.3.3" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3.3.cmml">1</mn></msub><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.2" lspace="0.222em" rspace="0.222em" xref="Ch6.E3.m1.4.4.1.1.1.1.2.2.cmml">⋅</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.cmml"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3a.cmml">ReLU</mtext><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.2.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml"><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.cmml"><msub id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.cmml"><mi id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.2.cmml">W</mi><mn id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.3" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.3.cmml">0</mn></msub><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.1.cmml">⋅</mo><mtext id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3a.cmml">MaxPool</mtext></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.1" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.3.2" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml"><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.3.2.1" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml">(</mo><mi id="Ch6.E3.m1.3.3" xref="Ch6.E3.m1.3.3.cmml">F</mi><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.3.2.2" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.3" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.3" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Ch6.E3.m1.4.4.1.1.1.3" stretchy="false" xref="Ch6.E3.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.E3.m1.4b"><apply id="Ch6.E3.m1.4.4.cmml" xref="Ch6.E3.m1.4.4"><eq id="Ch6.E3.m1.4.4.2.cmml" xref="Ch6.E3.m1.4.4.2"></eq><apply id="Ch6.E3.m1.4.4.3.cmml" xref="Ch6.E3.m1.4.4.3"><times id="Ch6.E3.m1.4.4.3.1.cmml" xref="Ch6.E3.m1.4.4.3.1"></times><apply id="Ch6.E3.m1.4.4.3.2.cmml" xref="Ch6.E3.m1.4.4.3.2"><csymbol cd="ambiguous" id="Ch6.E3.m1.4.4.3.2.1.cmml" xref="Ch6.E3.m1.4.4.3.2">subscript</csymbol><ci id="Ch6.E3.m1.4.4.3.2.2.cmml" xref="Ch6.E3.m1.4.4.3.2.2">𝑀</ci><ci id="Ch6.E3.m1.4.4.3.2.3.cmml" xref="Ch6.E3.m1.4.4.3.2.3">𝑐</ci></apply><ci id="Ch6.E3.m1.1.1.cmml" xref="Ch6.E3.m1.1.1">𝐹</ci></apply><apply id="Ch6.E3.m1.4.4.1.cmml" xref="Ch6.E3.m1.4.4.1"><times id="Ch6.E3.m1.4.4.1.2.cmml" xref="Ch6.E3.m1.4.4.1.2"></times><ci id="Ch6.E3.m1.4.4.1.3.cmml" xref="Ch6.E3.m1.4.4.1.3">𝜎</ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1"><plus id="Ch6.E3.m1.4.4.1.1.1.1.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.3"></plus><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1"><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.2">⋅</ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Ch6.E3.m1.4.4.1.1.1.1.1.3.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.3.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3.2">𝑊</ci><cn id="Ch6.E3.m1.4.4.1.1.1.1.1.3.3.cmml" type="integer" xref="Ch6.E3.m1.4.4.1.1.1.1.1.3.3">1</cn></apply><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1"><times id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.2"></times><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3a.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.3">ReLU</mtext></ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1"><times id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1">⋅</ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑊</ci><cn id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" type="integer" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3">0</cn></apply><ci id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3">AvgPool</mtext></ci></apply><ci id="Ch6.E3.m1.2.2.cmml" xref="Ch6.E3.m1.2.2">𝐹</ci></apply></apply></apply><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2"><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.2">⋅</ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3"><csymbol cd="ambiguous" id="Ch6.E3.m1.4.4.1.1.1.1.2.3.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3">subscript</csymbol><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.3.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3.2">𝑊</ci><cn id="Ch6.E3.m1.4.4.1.1.1.1.2.3.3.cmml" type="integer" xref="Ch6.E3.m1.4.4.1.1.1.1.2.3.3">1</cn></apply><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1"><times id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.2"></times><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3a.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.3">ReLU</mtext></ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1"><times id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.1"></times><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2"><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.1">⋅</ci><apply id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.1.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.2.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.2">𝑊</ci><cn id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.3.cmml" type="integer" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.2.3">0</cn></apply><ci id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3a.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3"><mtext id="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3.cmml" xref="Ch6.E3.m1.4.4.1.1.1.1.2.1.1.1.1.1.1.2.3">MaxPool</mtext></ci></apply><ci id="Ch6.E3.m1.3.3.cmml" xref="Ch6.E3.m1.3.3">𝐹</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.E3.m1.4c">M_{c}(F)=\sigma(W_{1}\cdot(\text{ReLU}(W_{0}\cdot\text{AvgPool}(F)))+W_{1}%
\cdot(\text{ReLU}(W_{0}\cdot\text{MaxPool}(F))))</annotation><annotation encoding="application/x-llamapun" id="Ch6.E3.m1.4d">italic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_F ) = italic_σ ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ ( ReLU ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ⋅ AvgPool ( italic_F ) ) ) + italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ ( ReLU ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ⋅ MaxPool ( italic_F ) ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Ch6.S2.p3.8">where <math alttext="F" class="ltx_Math" display="inline" id="Ch6.S2.p3.2.m1.1"><semantics id="Ch6.S2.p3.2.m1.1a"><mi id="Ch6.S2.p3.2.m1.1.1" xref="Ch6.S2.p3.2.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.2.m1.1b"><ci id="Ch6.S2.p3.2.m1.1.1.cmml" xref="Ch6.S2.p3.2.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.2.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.2.m1.1d">italic_F</annotation></semantics></math> is the input feature map, <math alttext="\text{AvgPool}(F)" class="ltx_Math" display="inline" id="Ch6.S2.p3.3.m2.1"><semantics id="Ch6.S2.p3.3.m2.1a"><mrow id="Ch6.S2.p3.3.m2.1.2" xref="Ch6.S2.p3.3.m2.1.2.cmml"><mtext id="Ch6.S2.p3.3.m2.1.2.2" xref="Ch6.S2.p3.3.m2.1.2.2a.cmml">AvgPool</mtext><mo id="Ch6.S2.p3.3.m2.1.2.1" xref="Ch6.S2.p3.3.m2.1.2.1.cmml">⁢</mo><mrow id="Ch6.S2.p3.3.m2.1.2.3.2" xref="Ch6.S2.p3.3.m2.1.2.cmml"><mo id="Ch6.S2.p3.3.m2.1.2.3.2.1" stretchy="false" xref="Ch6.S2.p3.3.m2.1.2.cmml">(</mo><mi id="Ch6.S2.p3.3.m2.1.1" xref="Ch6.S2.p3.3.m2.1.1.cmml">F</mi><mo id="Ch6.S2.p3.3.m2.1.2.3.2.2" stretchy="false" xref="Ch6.S2.p3.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.3.m2.1b"><apply id="Ch6.S2.p3.3.m2.1.2.cmml" xref="Ch6.S2.p3.3.m2.1.2"><times id="Ch6.S2.p3.3.m2.1.2.1.cmml" xref="Ch6.S2.p3.3.m2.1.2.1"></times><ci id="Ch6.S2.p3.3.m2.1.2.2a.cmml" xref="Ch6.S2.p3.3.m2.1.2.2"><mtext id="Ch6.S2.p3.3.m2.1.2.2.cmml" xref="Ch6.S2.p3.3.m2.1.2.2">AvgPool</mtext></ci><ci id="Ch6.S2.p3.3.m2.1.1.cmml" xref="Ch6.S2.p3.3.m2.1.1">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.3.m2.1c">\text{AvgPool}(F)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.3.m2.1d">AvgPool ( italic_F )</annotation></semantics></math> and <math alttext="\text{MaxPool}(F)" class="ltx_Math" display="inline" id="Ch6.S2.p3.4.m3.1"><semantics id="Ch6.S2.p3.4.m3.1a"><mrow id="Ch6.S2.p3.4.m3.1.2" xref="Ch6.S2.p3.4.m3.1.2.cmml"><mtext id="Ch6.S2.p3.4.m3.1.2.2" xref="Ch6.S2.p3.4.m3.1.2.2a.cmml">MaxPool</mtext><mo id="Ch6.S2.p3.4.m3.1.2.1" xref="Ch6.S2.p3.4.m3.1.2.1.cmml">⁢</mo><mrow id="Ch6.S2.p3.4.m3.1.2.3.2" xref="Ch6.S2.p3.4.m3.1.2.cmml"><mo id="Ch6.S2.p3.4.m3.1.2.3.2.1" stretchy="false" xref="Ch6.S2.p3.4.m3.1.2.cmml">(</mo><mi id="Ch6.S2.p3.4.m3.1.1" xref="Ch6.S2.p3.4.m3.1.1.cmml">F</mi><mo id="Ch6.S2.p3.4.m3.1.2.3.2.2" stretchy="false" xref="Ch6.S2.p3.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.4.m3.1b"><apply id="Ch6.S2.p3.4.m3.1.2.cmml" xref="Ch6.S2.p3.4.m3.1.2"><times id="Ch6.S2.p3.4.m3.1.2.1.cmml" xref="Ch6.S2.p3.4.m3.1.2.1"></times><ci id="Ch6.S2.p3.4.m3.1.2.2a.cmml" xref="Ch6.S2.p3.4.m3.1.2.2"><mtext id="Ch6.S2.p3.4.m3.1.2.2.cmml" xref="Ch6.S2.p3.4.m3.1.2.2">MaxPool</mtext></ci><ci id="Ch6.S2.p3.4.m3.1.1.cmml" xref="Ch6.S2.p3.4.m3.1.1">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.4.m3.1c">\text{MaxPool}(F)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.4.m3.1d">MaxPool ( italic_F )</annotation></semantics></math> are the global average pooled and global max pooled feature maps, <math alttext="W_{0}\in\mathbb{R}^{C/r\times C}" class="ltx_Math" display="inline" id="Ch6.S2.p3.5.m4.1"><semantics id="Ch6.S2.p3.5.m4.1a"><mrow id="Ch6.S2.p3.5.m4.1.1" xref="Ch6.S2.p3.5.m4.1.1.cmml"><msub id="Ch6.S2.p3.5.m4.1.1.2" xref="Ch6.S2.p3.5.m4.1.1.2.cmml"><mi id="Ch6.S2.p3.5.m4.1.1.2.2" xref="Ch6.S2.p3.5.m4.1.1.2.2.cmml">W</mi><mn id="Ch6.S2.p3.5.m4.1.1.2.3" xref="Ch6.S2.p3.5.m4.1.1.2.3.cmml">0</mn></msub><mo id="Ch6.S2.p3.5.m4.1.1.1" xref="Ch6.S2.p3.5.m4.1.1.1.cmml">∈</mo><msup id="Ch6.S2.p3.5.m4.1.1.3" xref="Ch6.S2.p3.5.m4.1.1.3.cmml"><mi id="Ch6.S2.p3.5.m4.1.1.3.2" xref="Ch6.S2.p3.5.m4.1.1.3.2.cmml">ℝ</mi><mrow id="Ch6.S2.p3.5.m4.1.1.3.3" xref="Ch6.S2.p3.5.m4.1.1.3.3.cmml"><mrow id="Ch6.S2.p3.5.m4.1.1.3.3.2" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.cmml"><mi id="Ch6.S2.p3.5.m4.1.1.3.3.2.2" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.2.cmml">C</mi><mo id="Ch6.S2.p3.5.m4.1.1.3.3.2.1" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.1.cmml">/</mo><mi id="Ch6.S2.p3.5.m4.1.1.3.3.2.3" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.3.cmml">r</mi></mrow><mo id="Ch6.S2.p3.5.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S2.p3.5.m4.1.1.3.3.1.cmml">×</mo><mi id="Ch6.S2.p3.5.m4.1.1.3.3.3" xref="Ch6.S2.p3.5.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.5.m4.1b"><apply id="Ch6.S2.p3.5.m4.1.1.cmml" xref="Ch6.S2.p3.5.m4.1.1"><in id="Ch6.S2.p3.5.m4.1.1.1.cmml" xref="Ch6.S2.p3.5.m4.1.1.1"></in><apply id="Ch6.S2.p3.5.m4.1.1.2.cmml" xref="Ch6.S2.p3.5.m4.1.1.2"><csymbol cd="ambiguous" id="Ch6.S2.p3.5.m4.1.1.2.1.cmml" xref="Ch6.S2.p3.5.m4.1.1.2">subscript</csymbol><ci id="Ch6.S2.p3.5.m4.1.1.2.2.cmml" xref="Ch6.S2.p3.5.m4.1.1.2.2">𝑊</ci><cn id="Ch6.S2.p3.5.m4.1.1.2.3.cmml" type="integer" xref="Ch6.S2.p3.5.m4.1.1.2.3">0</cn></apply><apply id="Ch6.S2.p3.5.m4.1.1.3.cmml" xref="Ch6.S2.p3.5.m4.1.1.3"><csymbol cd="ambiguous" id="Ch6.S2.p3.5.m4.1.1.3.1.cmml" xref="Ch6.S2.p3.5.m4.1.1.3">superscript</csymbol><ci id="Ch6.S2.p3.5.m4.1.1.3.2.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.2">ℝ</ci><apply id="Ch6.S2.p3.5.m4.1.1.3.3.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3"><times id="Ch6.S2.p3.5.m4.1.1.3.3.1.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.1"></times><apply id="Ch6.S2.p3.5.m4.1.1.3.3.2.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.2"><divide id="Ch6.S2.p3.5.m4.1.1.3.3.2.1.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.1"></divide><ci id="Ch6.S2.p3.5.m4.1.1.3.3.2.2.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.2">𝐶</ci><ci id="Ch6.S2.p3.5.m4.1.1.3.3.2.3.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.2.3">𝑟</ci></apply><ci id="Ch6.S2.p3.5.m4.1.1.3.3.3.cmml" xref="Ch6.S2.p3.5.m4.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.5.m4.1c">W_{0}\in\mathbb{R}^{C/r\times C}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.5.m4.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C / italic_r × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="W_{1}\in\mathbb{R}^{C\times C/r}" class="ltx_Math" display="inline" id="Ch6.S2.p3.6.m5.1"><semantics id="Ch6.S2.p3.6.m5.1a"><mrow id="Ch6.S2.p3.6.m5.1.1" xref="Ch6.S2.p3.6.m5.1.1.cmml"><msub id="Ch6.S2.p3.6.m5.1.1.2" xref="Ch6.S2.p3.6.m5.1.1.2.cmml"><mi id="Ch6.S2.p3.6.m5.1.1.2.2" xref="Ch6.S2.p3.6.m5.1.1.2.2.cmml">W</mi><mn id="Ch6.S2.p3.6.m5.1.1.2.3" xref="Ch6.S2.p3.6.m5.1.1.2.3.cmml">1</mn></msub><mo id="Ch6.S2.p3.6.m5.1.1.1" xref="Ch6.S2.p3.6.m5.1.1.1.cmml">∈</mo><msup id="Ch6.S2.p3.6.m5.1.1.3" xref="Ch6.S2.p3.6.m5.1.1.3.cmml"><mi id="Ch6.S2.p3.6.m5.1.1.3.2" xref="Ch6.S2.p3.6.m5.1.1.3.2.cmml">ℝ</mi><mrow id="Ch6.S2.p3.6.m5.1.1.3.3" xref="Ch6.S2.p3.6.m5.1.1.3.3.cmml"><mrow id="Ch6.S2.p3.6.m5.1.1.3.3.2" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.cmml"><mi id="Ch6.S2.p3.6.m5.1.1.3.3.2.2" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.2.cmml">C</mi><mo id="Ch6.S2.p3.6.m5.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.1.cmml">×</mo><mi id="Ch6.S2.p3.6.m5.1.1.3.3.2.3" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.3.cmml">C</mi></mrow><mo id="Ch6.S2.p3.6.m5.1.1.3.3.1" xref="Ch6.S2.p3.6.m5.1.1.3.3.1.cmml">/</mo><mi id="Ch6.S2.p3.6.m5.1.1.3.3.3" xref="Ch6.S2.p3.6.m5.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.6.m5.1b"><apply id="Ch6.S2.p3.6.m5.1.1.cmml" xref="Ch6.S2.p3.6.m5.1.1"><in id="Ch6.S2.p3.6.m5.1.1.1.cmml" xref="Ch6.S2.p3.6.m5.1.1.1"></in><apply id="Ch6.S2.p3.6.m5.1.1.2.cmml" xref="Ch6.S2.p3.6.m5.1.1.2"><csymbol cd="ambiguous" id="Ch6.S2.p3.6.m5.1.1.2.1.cmml" xref="Ch6.S2.p3.6.m5.1.1.2">subscript</csymbol><ci id="Ch6.S2.p3.6.m5.1.1.2.2.cmml" xref="Ch6.S2.p3.6.m5.1.1.2.2">𝑊</ci><cn id="Ch6.S2.p3.6.m5.1.1.2.3.cmml" type="integer" xref="Ch6.S2.p3.6.m5.1.1.2.3">1</cn></apply><apply id="Ch6.S2.p3.6.m5.1.1.3.cmml" xref="Ch6.S2.p3.6.m5.1.1.3"><csymbol cd="ambiguous" id="Ch6.S2.p3.6.m5.1.1.3.1.cmml" xref="Ch6.S2.p3.6.m5.1.1.3">superscript</csymbol><ci id="Ch6.S2.p3.6.m5.1.1.3.2.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.2">ℝ</ci><apply id="Ch6.S2.p3.6.m5.1.1.3.3.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3"><divide id="Ch6.S2.p3.6.m5.1.1.3.3.1.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.1"></divide><apply id="Ch6.S2.p3.6.m5.1.1.3.3.2.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.2"><times id="Ch6.S2.p3.6.m5.1.1.3.3.2.1.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.1"></times><ci id="Ch6.S2.p3.6.m5.1.1.3.3.2.2.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.2">𝐶</ci><ci id="Ch6.S2.p3.6.m5.1.1.3.3.2.3.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.2.3">𝐶</ci></apply><ci id="Ch6.S2.p3.6.m5.1.1.3.3.3.cmml" xref="Ch6.S2.p3.6.m5.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.6.m5.1c">W_{1}\in\mathbb{R}^{C\times C/r}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.6.m5.1d">italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_C / italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> are the shared weight matrices of the fully connected layers, <span class="ltx_text ltx_markedasmath" id="Ch6.S2.p3.8.1">ReLU</span> is the Rectified Linear Unit activation function, and <math alttext="\sigma" class="ltx_Math" display="inline" id="Ch6.S2.p3.8.m7.1"><semantics id="Ch6.S2.p3.8.m7.1a"><mi id="Ch6.S2.p3.8.m7.1.1" xref="Ch6.S2.p3.8.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p3.8.m7.1b"><ci id="Ch6.S2.p3.8.m7.1.1.cmml" xref="Ch6.S2.p3.8.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p3.8.m7.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p3.8.m7.1d">italic_σ</annotation></semantics></math> is the sigmoid activation function.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p4">
<p class="ltx_p" id="Ch6.S2.p4.1">The subsequent component, the spatial attention module, focuses on crucial spatial regions within the feature map, identifying “where” the important information is located. It applies average pooling and max pooling across the channels, followed by a convolutional layer, which is computationally light. The spatial attention map <math alttext="M_{s}" class="ltx_Math" display="inline" id="Ch6.S2.p4.1.m1.1"><semantics id="Ch6.S2.p4.1.m1.1a"><msub id="Ch6.S2.p4.1.m1.1.1" xref="Ch6.S2.p4.1.m1.1.1.cmml"><mi id="Ch6.S2.p4.1.m1.1.1.2" xref="Ch6.S2.p4.1.m1.1.1.2.cmml">M</mi><mi id="Ch6.S2.p4.1.m1.1.1.3" xref="Ch6.S2.p4.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.1.m1.1b"><apply id="Ch6.S2.p4.1.m1.1.1.cmml" xref="Ch6.S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.S2.p4.1.m1.1.1.1.cmml" xref="Ch6.S2.p4.1.m1.1.1">subscript</csymbol><ci id="Ch6.S2.p4.1.m1.1.1.2.cmml" xref="Ch6.S2.p4.1.m1.1.1.2">𝑀</ci><ci id="Ch6.S2.p4.1.m1.1.1.3.cmml" xref="Ch6.S2.p4.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.1.m1.1c">M_{s}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.1.m1.1d">italic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is computed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="Ch6.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M_{s}(F)=\sigma(f^{7\times 7}([\text{AvgPool}(F);\text{MaxPool}(F)]))" class="ltx_Math" display="block" id="Ch6.E4.m1.4"><semantics id="Ch6.E4.m1.4a"><mrow id="Ch6.E4.m1.4.4" xref="Ch6.E4.m1.4.4.cmml"><mrow id="Ch6.E4.m1.4.4.3" xref="Ch6.E4.m1.4.4.3.cmml"><msub id="Ch6.E4.m1.4.4.3.2" xref="Ch6.E4.m1.4.4.3.2.cmml"><mi id="Ch6.E4.m1.4.4.3.2.2" xref="Ch6.E4.m1.4.4.3.2.2.cmml">M</mi><mi id="Ch6.E4.m1.4.4.3.2.3" xref="Ch6.E4.m1.4.4.3.2.3.cmml">s</mi></msub><mo id="Ch6.E4.m1.4.4.3.1" xref="Ch6.E4.m1.4.4.3.1.cmml">⁢</mo><mrow id="Ch6.E4.m1.4.4.3.3.2" xref="Ch6.E4.m1.4.4.3.cmml"><mo id="Ch6.E4.m1.4.4.3.3.2.1" stretchy="false" xref="Ch6.E4.m1.4.4.3.cmml">(</mo><mi id="Ch6.E4.m1.1.1" xref="Ch6.E4.m1.1.1.cmml">F</mi><mo id="Ch6.E4.m1.4.4.3.3.2.2" stretchy="false" xref="Ch6.E4.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="Ch6.E4.m1.4.4.2" xref="Ch6.E4.m1.4.4.2.cmml">=</mo><mrow id="Ch6.E4.m1.4.4.1" xref="Ch6.E4.m1.4.4.1.cmml"><mi id="Ch6.E4.m1.4.4.1.3" xref="Ch6.E4.m1.4.4.1.3.cmml">σ</mi><mo id="Ch6.E4.m1.4.4.1.2" xref="Ch6.E4.m1.4.4.1.2.cmml">⁢</mo><mrow id="Ch6.E4.m1.4.4.1.1.1" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml"><mo id="Ch6.E4.m1.4.4.1.1.1.2" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml"><msup id="Ch6.E4.m1.4.4.1.1.1.1.3" xref="Ch6.E4.m1.4.4.1.1.1.1.3.cmml"><mi id="Ch6.E4.m1.4.4.1.1.1.1.3.2" xref="Ch6.E4.m1.4.4.1.1.1.1.3.2.cmml">f</mi><mrow id="Ch6.E4.m1.4.4.1.1.1.1.3.3" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.cmml"><mn id="Ch6.E4.m1.4.4.1.1.1.1.3.3.2" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.2.cmml">7</mn><mo id="Ch6.E4.m1.4.4.1.1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.1.cmml">×</mo><mn id="Ch6.E4.m1.4.4.1.1.1.1.3.3.3" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.3.cmml">7</mn></mrow></msup><mo id="Ch6.E4.m1.4.4.1.1.1.1.2" xref="Ch6.E4.m1.4.4.1.1.1.1.2.cmml">⁢</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml"><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.2" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.3" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml">[</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mtext id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2a.cmml">AvgPool</mtext><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="Ch6.E4.m1.2.2" xref="Ch6.E4.m1.2.2.cmml">F</mi><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.4" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml">;</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml"><mtext id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2a.cmml">MaxPool</mtext><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.1" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mrow id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3.2" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml"><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3.2.1" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">(</mo><mi id="Ch6.E4.m1.3.3" xref="Ch6.E4.m1.3.3.cmml">F</mi><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3.2.2" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">)</mo></mrow></mrow><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.5" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo id="Ch6.E4.m1.4.4.1.1.1.1.1.1.3" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch6.E4.m1.4.4.1.1.1.3" stretchy="false" xref="Ch6.E4.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.E4.m1.4b"><apply id="Ch6.E4.m1.4.4.cmml" xref="Ch6.E4.m1.4.4"><eq id="Ch6.E4.m1.4.4.2.cmml" xref="Ch6.E4.m1.4.4.2"></eq><apply id="Ch6.E4.m1.4.4.3.cmml" xref="Ch6.E4.m1.4.4.3"><times id="Ch6.E4.m1.4.4.3.1.cmml" xref="Ch6.E4.m1.4.4.3.1"></times><apply id="Ch6.E4.m1.4.4.3.2.cmml" xref="Ch6.E4.m1.4.4.3.2"><csymbol cd="ambiguous" id="Ch6.E4.m1.4.4.3.2.1.cmml" xref="Ch6.E4.m1.4.4.3.2">subscript</csymbol><ci id="Ch6.E4.m1.4.4.3.2.2.cmml" xref="Ch6.E4.m1.4.4.3.2.2">𝑀</ci><ci id="Ch6.E4.m1.4.4.3.2.3.cmml" xref="Ch6.E4.m1.4.4.3.2.3">𝑠</ci></apply><ci id="Ch6.E4.m1.1.1.cmml" xref="Ch6.E4.m1.1.1">𝐹</ci></apply><apply id="Ch6.E4.m1.4.4.1.cmml" xref="Ch6.E4.m1.4.4.1"><times id="Ch6.E4.m1.4.4.1.2.cmml" xref="Ch6.E4.m1.4.4.1.2"></times><ci id="Ch6.E4.m1.4.4.1.3.cmml" xref="Ch6.E4.m1.4.4.1.3">𝜎</ci><apply id="Ch6.E4.m1.4.4.1.1.1.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1"><times id="Ch6.E4.m1.4.4.1.1.1.1.2.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.2"></times><apply id="Ch6.E4.m1.4.4.1.1.1.1.3.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="Ch6.E4.m1.4.4.1.1.1.1.3.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.3">superscript</csymbol><ci id="Ch6.E4.m1.4.4.1.1.1.1.3.2.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.3.2">𝑓</ci><apply id="Ch6.E4.m1.4.4.1.1.1.1.3.3.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3"><times id="Ch6.E4.m1.4.4.1.1.1.1.3.3.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.1"></times><cn id="Ch6.E4.m1.4.4.1.1.1.1.3.3.2.cmml" type="integer" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.2">7</cn><cn id="Ch6.E4.m1.4.4.1.1.1.1.3.3.3.cmml" type="integer" xref="Ch6.E4.m1.4.4.1.1.1.1.3.3.3">7</cn></apply></apply><list id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2"><apply id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1"><times id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1"></times><ci id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2a.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2"><mtext id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2">AvgPool</mtext></ci><ci id="Ch6.E4.m1.2.2.cmml" xref="Ch6.E4.m1.2.2">𝐹</ci></apply><apply id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2"><times id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.1.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.1"></times><ci id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2a.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2"><mtext id="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml" xref="Ch6.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2">MaxPool</mtext></ci><ci id="Ch6.E4.m1.3.3.cmml" xref="Ch6.E4.m1.3.3">𝐹</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.E4.m1.4c">M_{s}(F)=\sigma(f^{7\times 7}([\text{AvgPool}(F);\text{MaxPool}(F)]))</annotation><annotation encoding="application/x-llamapun" id="Ch6.E4.m1.4d">italic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_F ) = italic_σ ( italic_f start_POSTSUPERSCRIPT 7 × 7 end_POSTSUPERSCRIPT ( [ AvgPool ( italic_F ) ; MaxPool ( italic_F ) ] ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Ch6.S2.p4.8">where <math alttext="F" class="ltx_Math" display="inline" id="Ch6.S2.p4.2.m1.1"><semantics id="Ch6.S2.p4.2.m1.1a"><mi id="Ch6.S2.p4.2.m1.1.1" xref="Ch6.S2.p4.2.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.2.m1.1b"><ci id="Ch6.S2.p4.2.m1.1.1.cmml" xref="Ch6.S2.p4.2.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.2.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.2.m1.1d">italic_F</annotation></semantics></math> is the input feature map, <math alttext="\text{AvgPool}(F)" class="ltx_Math" display="inline" id="Ch6.S2.p4.3.m2.1"><semantics id="Ch6.S2.p4.3.m2.1a"><mrow id="Ch6.S2.p4.3.m2.1.2" xref="Ch6.S2.p4.3.m2.1.2.cmml"><mtext id="Ch6.S2.p4.3.m2.1.2.2" xref="Ch6.S2.p4.3.m2.1.2.2a.cmml">AvgPool</mtext><mo id="Ch6.S2.p4.3.m2.1.2.1" xref="Ch6.S2.p4.3.m2.1.2.1.cmml">⁢</mo><mrow id="Ch6.S2.p4.3.m2.1.2.3.2" xref="Ch6.S2.p4.3.m2.1.2.cmml"><mo id="Ch6.S2.p4.3.m2.1.2.3.2.1" stretchy="false" xref="Ch6.S2.p4.3.m2.1.2.cmml">(</mo><mi id="Ch6.S2.p4.3.m2.1.1" xref="Ch6.S2.p4.3.m2.1.1.cmml">F</mi><mo id="Ch6.S2.p4.3.m2.1.2.3.2.2" stretchy="false" xref="Ch6.S2.p4.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.3.m2.1b"><apply id="Ch6.S2.p4.3.m2.1.2.cmml" xref="Ch6.S2.p4.3.m2.1.2"><times id="Ch6.S2.p4.3.m2.1.2.1.cmml" xref="Ch6.S2.p4.3.m2.1.2.1"></times><ci id="Ch6.S2.p4.3.m2.1.2.2a.cmml" xref="Ch6.S2.p4.3.m2.1.2.2"><mtext id="Ch6.S2.p4.3.m2.1.2.2.cmml" xref="Ch6.S2.p4.3.m2.1.2.2">AvgPool</mtext></ci><ci id="Ch6.S2.p4.3.m2.1.1.cmml" xref="Ch6.S2.p4.3.m2.1.1">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.3.m2.1c">\text{AvgPool}(F)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.3.m2.1d">AvgPool ( italic_F )</annotation></semantics></math> and <math alttext="\text{MaxPool}(F)" class="ltx_Math" display="inline" id="Ch6.S2.p4.4.m3.1"><semantics id="Ch6.S2.p4.4.m3.1a"><mrow id="Ch6.S2.p4.4.m3.1.2" xref="Ch6.S2.p4.4.m3.1.2.cmml"><mtext id="Ch6.S2.p4.4.m3.1.2.2" xref="Ch6.S2.p4.4.m3.1.2.2a.cmml">MaxPool</mtext><mo id="Ch6.S2.p4.4.m3.1.2.1" xref="Ch6.S2.p4.4.m3.1.2.1.cmml">⁢</mo><mrow id="Ch6.S2.p4.4.m3.1.2.3.2" xref="Ch6.S2.p4.4.m3.1.2.cmml"><mo id="Ch6.S2.p4.4.m3.1.2.3.2.1" stretchy="false" xref="Ch6.S2.p4.4.m3.1.2.cmml">(</mo><mi id="Ch6.S2.p4.4.m3.1.1" xref="Ch6.S2.p4.4.m3.1.1.cmml">F</mi><mo id="Ch6.S2.p4.4.m3.1.2.3.2.2" stretchy="false" xref="Ch6.S2.p4.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.4.m3.1b"><apply id="Ch6.S2.p4.4.m3.1.2.cmml" xref="Ch6.S2.p4.4.m3.1.2"><times id="Ch6.S2.p4.4.m3.1.2.1.cmml" xref="Ch6.S2.p4.4.m3.1.2.1"></times><ci id="Ch6.S2.p4.4.m3.1.2.2a.cmml" xref="Ch6.S2.p4.4.m3.1.2.2"><mtext id="Ch6.S2.p4.4.m3.1.2.2.cmml" xref="Ch6.S2.p4.4.m3.1.2.2">MaxPool</mtext></ci><ci id="Ch6.S2.p4.4.m3.1.1.cmml" xref="Ch6.S2.p4.4.m3.1.1">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.4.m3.1c">\text{MaxPool}(F)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.4.m3.1d">MaxPool ( italic_F )</annotation></semantics></math> are the average pooled and max pooled feature maps across the channel dimension, <math alttext="[;]" class="ltx_math_unparsed" display="inline" id="Ch6.S2.p4.5.m4.1"><semantics id="Ch6.S2.p4.5.m4.1a"><mrow id="Ch6.S2.p4.5.m4.1b"><mo id="Ch6.S2.p4.5.m4.1.1" stretchy="false">[</mo><mo id="Ch6.S2.p4.5.m4.1.2">;</mo><mo id="Ch6.S2.p4.5.m4.1.3" stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" id="Ch6.S2.p4.5.m4.1c">[;]</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.5.m4.1d">[ ; ]</annotation></semantics></math> denotes the concatenation operation along the channel axis, <math alttext="f^{7\times 7}" class="ltx_Math" display="inline" id="Ch6.S2.p4.6.m5.1"><semantics id="Ch6.S2.p4.6.m5.1a"><msup id="Ch6.S2.p4.6.m5.1.1" xref="Ch6.S2.p4.6.m5.1.1.cmml"><mi id="Ch6.S2.p4.6.m5.1.1.2" xref="Ch6.S2.p4.6.m5.1.1.2.cmml">f</mi><mrow id="Ch6.S2.p4.6.m5.1.1.3" xref="Ch6.S2.p4.6.m5.1.1.3.cmml"><mn id="Ch6.S2.p4.6.m5.1.1.3.2" xref="Ch6.S2.p4.6.m5.1.1.3.2.cmml">7</mn><mo id="Ch6.S2.p4.6.m5.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S2.p4.6.m5.1.1.3.1.cmml">×</mo><mn id="Ch6.S2.p4.6.m5.1.1.3.3" xref="Ch6.S2.p4.6.m5.1.1.3.3.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.6.m5.1b"><apply id="Ch6.S2.p4.6.m5.1.1.cmml" xref="Ch6.S2.p4.6.m5.1.1"><csymbol cd="ambiguous" id="Ch6.S2.p4.6.m5.1.1.1.cmml" xref="Ch6.S2.p4.6.m5.1.1">superscript</csymbol><ci id="Ch6.S2.p4.6.m5.1.1.2.cmml" xref="Ch6.S2.p4.6.m5.1.1.2">𝑓</ci><apply id="Ch6.S2.p4.6.m5.1.1.3.cmml" xref="Ch6.S2.p4.6.m5.1.1.3"><times id="Ch6.S2.p4.6.m5.1.1.3.1.cmml" xref="Ch6.S2.p4.6.m5.1.1.3.1"></times><cn id="Ch6.S2.p4.6.m5.1.1.3.2.cmml" type="integer" xref="Ch6.S2.p4.6.m5.1.1.3.2">7</cn><cn id="Ch6.S2.p4.6.m5.1.1.3.3.cmml" type="integer" xref="Ch6.S2.p4.6.m5.1.1.3.3">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.6.m5.1c">f^{7\times 7}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.6.m5.1d">italic_f start_POSTSUPERSCRIPT 7 × 7 end_POSTSUPERSCRIPT</annotation></semantics></math> represents a convolution operation with a filter size of <math alttext="7\times 7" class="ltx_Math" display="inline" id="Ch6.S2.p4.7.m6.1"><semantics id="Ch6.S2.p4.7.m6.1a"><mrow id="Ch6.S2.p4.7.m6.1.1" xref="Ch6.S2.p4.7.m6.1.1.cmml"><mn id="Ch6.S2.p4.7.m6.1.1.2" xref="Ch6.S2.p4.7.m6.1.1.2.cmml">7</mn><mo id="Ch6.S2.p4.7.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S2.p4.7.m6.1.1.1.cmml">×</mo><mn id="Ch6.S2.p4.7.m6.1.1.3" xref="Ch6.S2.p4.7.m6.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.7.m6.1b"><apply id="Ch6.S2.p4.7.m6.1.1.cmml" xref="Ch6.S2.p4.7.m6.1.1"><times id="Ch6.S2.p4.7.m6.1.1.1.cmml" xref="Ch6.S2.p4.7.m6.1.1.1"></times><cn id="Ch6.S2.p4.7.m6.1.1.2.cmml" type="integer" xref="Ch6.S2.p4.7.m6.1.1.2">7</cn><cn id="Ch6.S2.p4.7.m6.1.1.3.cmml" type="integer" xref="Ch6.S2.p4.7.m6.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.7.m6.1c">7\times 7</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.7.m6.1d">7 × 7</annotation></semantics></math>, and <math alttext="\sigma" class="ltx_Math" display="inline" id="Ch6.S2.p4.8.m7.1"><semantics id="Ch6.S2.p4.8.m7.1a"><mi id="Ch6.S2.p4.8.m7.1.1" xref="Ch6.S2.p4.8.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p4.8.m7.1b"><ci id="Ch6.S2.p4.8.m7.1.1.cmml" xref="Ch6.S2.p4.8.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p4.8.m7.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p4.8.m7.1d">italic_σ</annotation></semantics></math> is the sigmoid activation function.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p5">
<p class="ltx_p" id="Ch6.S2.p5.1">By sequentially applying these attention mechanisms, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> refines the feature representation. The efficiency of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> comes from its simple yet effective design that avoids complex operations, ensuring fast processing and minimal computational overhead. Since the attention maps of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> are broadcast and applied element-wise, they do not alter the height, width, or number of channels of the input feature map.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p6">
<p class="ltx_p" id="Ch6.S2.p6.1">Through the fusion of channel and spatial attention mechanisms with the YOLOv8 backbone and the 2D scattering transform, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> enables the network to concentrate on pertinent spatial areas while highlighting critical channels, enhancing feature depiction and localization accuracy. According to <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>, integrating <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> into various deep learning frameworks has yielded notable enhancements across a wide array of classification and detection tasks. Specifically, in tasks related to instance segmentation, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> has been instrumental in refining object perimeters and improving the precision of segmented individual objects in images <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite>.
The integration of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> into ScatYOLOv8+CBAM at the head enhances feature extraction through both channel and spatial attention mechanisms. These mechanisms leverage scattering transforms to concentrate on significant regions and features, thereby improving the capability to precisely segment ships within intricate maritime settings.</p>
</div>
<div class="ltx_figure ltx_transformed_outer" id="Ch6.F6" style="width:261.9pt;height:435.5pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:435.5pt;transform:translate(-86.81pt,-86.31pt) rotate(-90deg) ;"><figure><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="Ch6.F6.g1" src="extracted/5906916/fig/scatyolov8+cbam.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F6.2.1.1" style="font-size:90%;">Figure 6.6</span>: </span><span class="ltx_text" id="Ch6.F6.3.2" style="font-size:80%;">The proposed architecture in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, with the ScatBlock in the backbone of YOLOv8 and CBAM modules in the head. (a) We place ScatBlock at the beginning of the YOLOv8 backbone to render a set of sparse feature maps, replacing the first Conv block of the original YOLOv8 backbone. The CBAM module is placed at the output of the head blocks of YOLOv8 to distill valuable object shape information for the segmentation task. The numbers next to every block represent the sequential order followed by the implementation, from input to output (b) The scattering block contains an upsample operation, followed by the scattering layer and a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.relu" title="Rectified Linear Unit">ReLU</span></a> activation (c) CBAM module depicting the channel and spatial attention mechanisms<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib45" title="">woo2018cbam </a></cite> (d) Standard YOLOv8 convolutional block (e) Spatial Pyramid Pooling Fast module of YOLOv8 (f) C2f with split channel operation and a CSP Bottleneck. (g) Segment block of YOLOv8 that performs segmentation. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
</figure></div></div>
<div class="ltx_para" id="Ch6.S2.p7">
<p class="ltx_p" id="Ch6.S2.p7.1">The proposed ScatYOLOv8+CBAM, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F6" title="Figure 6.6 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.6</span></a> (a), substitutes the initial convolutional block of the YOLOv8 backbone with a ScatBlock. Unlike the original first <math alttext="Conv" class="ltx_Math" display="inline" id="Ch6.S2.p7.1.m1.1"><semantics id="Ch6.S2.p7.1.m1.1a"><mrow id="Ch6.S2.p7.1.m1.1.1" xref="Ch6.S2.p7.1.m1.1.1.cmml"><mi id="Ch6.S2.p7.1.m1.1.1.2" xref="Ch6.S2.p7.1.m1.1.1.2.cmml">C</mi><mo id="Ch6.S2.p7.1.m1.1.1.1" xref="Ch6.S2.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p7.1.m1.1.1.3" xref="Ch6.S2.p7.1.m1.1.1.3.cmml">o</mi><mo id="Ch6.S2.p7.1.m1.1.1.1a" xref="Ch6.S2.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p7.1.m1.1.1.4" xref="Ch6.S2.p7.1.m1.1.1.4.cmml">n</mi><mo id="Ch6.S2.p7.1.m1.1.1.1b" xref="Ch6.S2.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p7.1.m1.1.1.5" xref="Ch6.S2.p7.1.m1.1.1.5.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p7.1.m1.1b"><apply id="Ch6.S2.p7.1.m1.1.1.cmml" xref="Ch6.S2.p7.1.m1.1.1"><times id="Ch6.S2.p7.1.m1.1.1.1.cmml" xref="Ch6.S2.p7.1.m1.1.1.1"></times><ci id="Ch6.S2.p7.1.m1.1.1.2.cmml" xref="Ch6.S2.p7.1.m1.1.1.2">𝐶</ci><ci id="Ch6.S2.p7.1.m1.1.1.3.cmml" xref="Ch6.S2.p7.1.m1.1.1.3">𝑜</ci><ci id="Ch6.S2.p7.1.m1.1.1.4.cmml" xref="Ch6.S2.p7.1.m1.1.1.4">𝑛</ci><ci id="Ch6.S2.p7.1.m1.1.1.5.cmml" xref="Ch6.S2.p7.1.m1.1.1.5">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p7.1.m1.1c">Conv</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p7.1.m1.1d">italic_C italic_o italic_n italic_v</annotation></semantics></math> block of CSPDarknet53 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib69" title="">bochkovskiy2020yolov4 </a></cite> (YOLOv8 backbone), the ScatBlock employs the first-order 2D scattering transform.
The ScatBlock is operated only in forward mode, since the rotated wavelets and low-pass filter are fixed, and it does not allow for the backpropagation and filter parameter updates during the training phase.
Following the insights from <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib119" title="">zhu2021tph </a></cite>, which applied CBAM to enhance the head of YOLOv5 for aerial object detection, the CBAM block is integrated after the C2f blocks (see C2f block explanation in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.S2" title="3.2 Ship Recognition Using Maritime Monitoring Footage ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.2</span></a>) within the YOLOv8 neck. This integration serves a dual purpose: assisting the network in identifying areas of interest, specifically ships, and utilizing these identified regions as inputs for subsequent blocks in the head.</p>
</div>
<figure class="ltx_figure" id="Ch6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="101" id="Ch6.F7.g1" src="extracted/5906916/fig/filters_plot.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F7.2.1.1" style="font-size:90%;">Figure 6.7</span>: </span><span class="ltx_text" id="Ch6.F7.3.2" style="font-size:90%;">Visualization of 2D Wavelet Filters used in the ScatBlock. The first on the left shows the low-pass filter. The rest display the oriented filters, generated by combining and rotating to represent different directional sensitivities of the Symlet wavelet.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S2.p8">
<p class="ltx_p" id="Ch6.S2.p8.2">As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>, the implementation of the ScatBlock (see block number <math alttext="0" class="ltx_Math" display="inline" id="Ch6.S2.p8.1.m1.1"><semantics id="Ch6.S2.p8.1.m1.1a"><mn id="Ch6.S2.p8.1.m1.1.1" xref="Ch6.S2.p8.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="Ch6.S2.p8.1.m1.1b"><cn id="Ch6.S2.p8.1.m1.1.1.cmml" type="integer" xref="Ch6.S2.p8.1.m1.1.1">0</cn></annotation-xml></semantics></math> of Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F6" title="Figure 6.6 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.6</span></a> (a)) uses the 2D Wavelet transformations by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite>, with the open-source Python module <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S2.p8.2.m2.1"><semantics id="Ch6.S2.p8.2.m2.1a"><mrow id="Ch6.S2.p8.2.m2.1.1" xref="Ch6.S2.p8.2.m2.1.1.cmml"><mi id="Ch6.S2.p8.2.m2.1.1.2" xref="Ch6.S2.p8.2.m2.1.1.2.cmml">p</mi><mo id="Ch6.S2.p8.2.m2.1.1.1" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.3" xref="Ch6.S2.p8.2.m2.1.1.3.cmml">y</mi><mo id="Ch6.S2.p8.2.m2.1.1.1a" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.4" xref="Ch6.S2.p8.2.m2.1.1.4.cmml">t</mi><mo id="Ch6.S2.p8.2.m2.1.1.1b" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.5" xref="Ch6.S2.p8.2.m2.1.1.5.cmml">o</mi><mo id="Ch6.S2.p8.2.m2.1.1.1c" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.6" xref="Ch6.S2.p8.2.m2.1.1.6.cmml">r</mi><mo id="Ch6.S2.p8.2.m2.1.1.1d" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.7" xref="Ch6.S2.p8.2.m2.1.1.7.cmml">c</mi><mo id="Ch6.S2.p8.2.m2.1.1.1e" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.8" xref="Ch6.S2.p8.2.m2.1.1.8.cmml">h</mi><mo id="Ch6.S2.p8.2.m2.1.1.1f" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.9" mathvariant="normal" xref="Ch6.S2.p8.2.m2.1.1.9.cmml">_</mi><mo id="Ch6.S2.p8.2.m2.1.1.1g" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.10" xref="Ch6.S2.p8.2.m2.1.1.10.cmml">w</mi><mo id="Ch6.S2.p8.2.m2.1.1.1h" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.11" xref="Ch6.S2.p8.2.m2.1.1.11.cmml">a</mi><mo id="Ch6.S2.p8.2.m2.1.1.1i" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.12" xref="Ch6.S2.p8.2.m2.1.1.12.cmml">v</mi><mo id="Ch6.S2.p8.2.m2.1.1.1j" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.13" xref="Ch6.S2.p8.2.m2.1.1.13.cmml">e</mi><mo id="Ch6.S2.p8.2.m2.1.1.1k" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.14" xref="Ch6.S2.p8.2.m2.1.1.14.cmml">l</mi><mo id="Ch6.S2.p8.2.m2.1.1.1l" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.15" xref="Ch6.S2.p8.2.m2.1.1.15.cmml">e</mi><mo id="Ch6.S2.p8.2.m2.1.1.1m" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.16" xref="Ch6.S2.p8.2.m2.1.1.16.cmml">t</mi><mo id="Ch6.S2.p8.2.m2.1.1.1n" xref="Ch6.S2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S2.p8.2.m2.1.1.17" xref="Ch6.S2.p8.2.m2.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p8.2.m2.1b"><apply id="Ch6.S2.p8.2.m2.1.1.cmml" xref="Ch6.S2.p8.2.m2.1.1"><times id="Ch6.S2.p8.2.m2.1.1.1.cmml" xref="Ch6.S2.p8.2.m2.1.1.1"></times><ci id="Ch6.S2.p8.2.m2.1.1.2.cmml" xref="Ch6.S2.p8.2.m2.1.1.2">𝑝</ci><ci id="Ch6.S2.p8.2.m2.1.1.3.cmml" xref="Ch6.S2.p8.2.m2.1.1.3">𝑦</ci><ci id="Ch6.S2.p8.2.m2.1.1.4.cmml" xref="Ch6.S2.p8.2.m2.1.1.4">𝑡</ci><ci id="Ch6.S2.p8.2.m2.1.1.5.cmml" xref="Ch6.S2.p8.2.m2.1.1.5">𝑜</ci><ci id="Ch6.S2.p8.2.m2.1.1.6.cmml" xref="Ch6.S2.p8.2.m2.1.1.6">𝑟</ci><ci id="Ch6.S2.p8.2.m2.1.1.7.cmml" xref="Ch6.S2.p8.2.m2.1.1.7">𝑐</ci><ci id="Ch6.S2.p8.2.m2.1.1.8.cmml" xref="Ch6.S2.p8.2.m2.1.1.8">ℎ</ci><ci id="Ch6.S2.p8.2.m2.1.1.9.cmml" xref="Ch6.S2.p8.2.m2.1.1.9">_</ci><ci id="Ch6.S2.p8.2.m2.1.1.10.cmml" xref="Ch6.S2.p8.2.m2.1.1.10">𝑤</ci><ci id="Ch6.S2.p8.2.m2.1.1.11.cmml" xref="Ch6.S2.p8.2.m2.1.1.11">𝑎</ci><ci id="Ch6.S2.p8.2.m2.1.1.12.cmml" xref="Ch6.S2.p8.2.m2.1.1.12">𝑣</ci><ci id="Ch6.S2.p8.2.m2.1.1.13.cmml" xref="Ch6.S2.p8.2.m2.1.1.13">𝑒</ci><ci id="Ch6.S2.p8.2.m2.1.1.14.cmml" xref="Ch6.S2.p8.2.m2.1.1.14">𝑙</ci><ci id="Ch6.S2.p8.2.m2.1.1.15.cmml" xref="Ch6.S2.p8.2.m2.1.1.15">𝑒</ci><ci id="Ch6.S2.p8.2.m2.1.1.16.cmml" xref="Ch6.S2.p8.2.m2.1.1.16">𝑡</ci><ci id="Ch6.S2.p8.2.m2.1.1.17.cmml" xref="Ch6.S2.p8.2.m2.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p8.2.m2.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p8.2.m2.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math>, which has <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cuda"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cuda" title="Compute Unified Device Architecture">CUDA</span></a> support for <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> operations.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p9">
<p class="ltx_p" id="Ch6.S2.p9.1">With regards to feature map resolution and channel number changes in the proposed architecture, changes in resolution occur primarily due to the Conv blocks, which reduce the height and width, and the Upsample blocks, which increase them. The other blocks (ScatBlock, C2f, SPPF, CBAM, and Segment) do not alter the height and width resolution. Regarding the number of channels, Conv blocks increase the number of channels, while CBAM blocks decrease them. The ScatBlock increases the number of channels (output scattering coefficients). The SPPF and CBAM block maintain the number of channels. The Concat blocks increase the number of channels by merging feature maps, and the Segment block adjusts the number of channels to match the number of output classes.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p10">
<p class="ltx_p" id="Ch6.S2.p10.1">To evaluate ScatYOLOv8+CBAM, in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, the ShipSG dataset was used.
Following the common practice in the field and for comparison with the results obtained in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mean Average Precision (mAP)</span></a> was reported as performance metric.
For a fair comparison with the state-of-the-art, YOLOv8 and ScatYOLOv8+CBAM models were trained using an NVIDIA A100 <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> with random weight initialization for all models. The number of training epochs was 300, with the default settings provided by YOLOv8 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib66" title="">jocheryolov8 </a></cite>. The input size used for all models is <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch6.S2.p10.1.m1.1"><semantics id="Ch6.S2.p10.1.m1.1a"><mrow id="Ch6.S2.p10.1.m1.1.1" xref="Ch6.S2.p10.1.m1.1.1.cmml"><mn id="Ch6.S2.p10.1.m1.1.1.2" xref="Ch6.S2.p10.1.m1.1.1.2.cmml">640</mn><mo id="Ch6.S2.p10.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S2.p10.1.m1.1.1.1.cmml">×</mo><mn id="Ch6.S2.p10.1.m1.1.1.3" xref="Ch6.S2.p10.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S2.p10.1.m1.1b"><apply id="Ch6.S2.p10.1.m1.1.1.cmml" xref="Ch6.S2.p10.1.m1.1.1"><times id="Ch6.S2.p10.1.m1.1.1.1.cmml" xref="Ch6.S2.p10.1.m1.1.1.1"></times><cn id="Ch6.S2.p10.1.m1.1.1.2.cmml" type="integer" xref="Ch6.S2.p10.1.m1.1.1.2">640</cn><cn id="Ch6.S2.p10.1.m1.1.1.3.cmml" type="integer" xref="Ch6.S2.p10.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p10.1.m1.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p10.1.m1.1d">640 × 640</annotation></semantics></math> pixels.</p>
</div>
<figure class="ltx_figure" id="Ch6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="450" id="Ch6.F8.g1" src="extracted/5906916/fig/scat_in_action.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F8.3.2.1" style="font-size:90%;">Figure 6.8</span>: </span><span class="ltx_text" id="Ch6.F8.1.1" style="font-size:90%;">Instance segmentation process of ScatYOLOv8+CBAM on ShipSG. (a) ShipSG input sample image. (b) Output of the ScatBlock (here, for visualization, the mean of output channels without <math alttext="S_{0}" class="ltx_Math" display="inline" id="Ch6.F8.1.1.m1.1"><semantics id="Ch6.F8.1.1.m1.1c"><msub id="Ch6.F8.1.1.m1.1.1" xref="Ch6.F8.1.1.m1.1.1.cmml"><mi id="Ch6.F8.1.1.m1.1.1.2" xref="Ch6.F8.1.1.m1.1.1.2.cmml">S</mi><mn id="Ch6.F8.1.1.m1.1.1.3" xref="Ch6.F8.1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Ch6.F8.1.1.m1.1d"><apply id="Ch6.F8.1.1.m1.1.1.cmml" xref="Ch6.F8.1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.F8.1.1.m1.1.1.1.cmml" xref="Ch6.F8.1.1.m1.1.1">subscript</csymbol><ci id="Ch6.F8.1.1.m1.1.1.2.cmml" xref="Ch6.F8.1.1.m1.1.1.2">𝑆</ci><cn id="Ch6.F8.1.1.m1.1.1.3.cmml" type="integer" xref="Ch6.F8.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F8.1.1.m1.1e">S_{0}</annotation><annotation encoding="application/x-llamapun" id="Ch6.F8.1.1.m1.1f">italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>). (c) Output of CBAM (module number 21 in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F6" title="Figure 6.6 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.6</span></a>). (d) ShipSG image with segmented and classified ships using the proposed architecture ScatYOLOv8n+CBAM. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S2.p11">
<p class="ltx_p" id="Ch6.S2.p11.1">In the example of ship segmentation inference on ShipSG, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F8" title="Figure 6.8 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.8</span></a>, it is observable that the inclusion of the ScatBlock enhances the ships within the image. This enhancement significantly improves the clarity and visibility of the ship edges, leading to a more defined and distinct outline of the ships present. Regarding the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> output, it is noted that the implemented attention mechanisms (both channel and spatial) effectively complement the scattering transform. The attention maps enhance the location of ships within the image while minimizing background influence.</p>
</div>
<figure class="ltx_table" id="Ch6.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch6.T1.4.1.1" style="font-size:90%;">Table 6.1</span>: </span><span class="ltx_text" id="Ch6.T1.5.2" style="font-size:90%;">Comparison of state-of-the-art segmentation performances on ShipSG with YOLOv8n and ScatYOLOv8n+CBAM. Adapted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch6.T1.2">
<tr class="ltx_tr" id="Ch6.T1.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.2.3.1"><span class="ltx_text ltx_font_bold" id="Ch6.T1.2.3.1.1">Segmentation model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T1.2.3.2"><span class="ltx_text ltx_font_bold" id="Ch6.T1.2.3.2.1">mAP (%)</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.2.4.1">Mask R-CNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T1.2.4.2">73.3</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.2.5">
<td class="ltx_td ltx_align_left" id="Ch6.T1.2.5.1">DetectoRS</td>
<td class="ltx_td ltx_align_center" id="Ch6.T1.2.5.2">74.7</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.1.1">
<td class="ltx_td ltx_align_left" id="Ch6.T1.1.1.1">YOLACT<sub class="ltx_sub" id="Ch6.T1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="Ch6.T1.1.1.1.1.1">700</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch6.T1.1.1.2">58.20</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.2.2">
<td class="ltx_td ltx_align_left" id="Ch6.T1.2.2.1">Centermask-Lite<sub class="ltx_sub" id="Ch6.T1.2.2.1.1"><span class="ltx_text ltx_font_italic" id="Ch6.T1.2.2.1.1.1">V39</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="Ch6.T1.2.2.2">64.40</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.2.6.1">YOLOv8n</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T1.2.6.2">70.15</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.2.7">
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch6.T1.2.7.1"><span class="ltx_text ltx_font_bold" id="Ch6.T1.2.7.1.1">ScatYOLOv8n + CBAM</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Ch6.T1.2.7.2"><span class="ltx_text ltx_font_bold" id="Ch6.T1.2.7.2.1">75.46</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="Ch6.S2.p12">
<p class="ltx_p" id="Ch6.S2.p12.1">In <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, I focused on the lightest version of YOLOv8 for the implementation ScatYOLOv8+CBAM, version <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S2.p12.1.m1.1"><semantics id="Ch6.S2.p12.1.m1.1a"><mi id="Ch6.S2.p12.1.m1.1.1" xref="Ch6.S2.p12.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p12.1.m1.1b"><ci id="Ch6.S2.p12.1.m1.1.1.cmml" xref="Ch6.S2.p12.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p12.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p12.1.m1.1d">italic_n</annotation></semantics></math>, due to its potential for real-time operation.
As seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T1" title="Table 6.1 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>, the baseline YOLOv8n shows improvement compared to the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> of previous real-time approaches for ship segmentation on ShipSG studied in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, YOLACT and Centermask-Lite. Yet, it does not show advantage against more robust methods like Mask R-CNN and DetectoRS.
ScatYOLOv8n+CBAM achieves 5.31% improvement with respect to standard YOLOv8n, 11.06% improvement compared to Centermask-Lite and 0.76% with respect to the most robust of the previous study, DetectoRS.
While the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> increase is modest when compared to DetectoRS, DetectoRS provided an inference time of 151 ms using a high-end <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>.
As discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, the deployment of instance segmentation on <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems was not reported for the methods presented in the initial study due to the incompatibility between deep learning and the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.arm" title="Advanced Reduced instruction set computer Machine">Advanced Reduced instruction set computer Machine (ARM)</span></a> architectures of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a>-powered embedded systems.
The advantage of YOLOv8-like architectures, such as ScatYOLOv8+CBAM, is the deployability on embedded systems. In <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, the NVIDIA Jetson AGX Xavier was selected as the target embedded system for deployment.
The ScatYOLOv8+CBAM model was deployed using native Pytorch weights.
This allow us to measure inference times of the new architecture on the system, which will show a great advantage against the previously studied instance segmentation methods.</p>
</div>
<figure class="ltx_table" id="Ch6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch6.T2.16.1.1" style="font-size:90%;">Table 6.2</span>: </span><span class="ltx_text" id="Ch6.T2.17.2" style="font-size:90%;">Ablation study of YOLOv8 segmentation models and ScatYOLOv8+CBAM additions after training on ShipSG and inference times on the NVIDIA Jetson AGX Xavier. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch6.T2.14">
<tr class="ltx_tr" id="Ch6.T2.14.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T2.14.15.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.15.1.1">Segmentation model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="Ch6.T2.14.15.2">
<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref ltx_font_bold" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.15.2.1"> (%)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Ch6.T2.14.15.3"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.15.3.1">Inference (ms)</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.14.16">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T2.14.16.1">YOLOv8n</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.14.16.2">70.35</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T2.14.16.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.14.16.4">28.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.14.16.5">-</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.2.2.3">YOLOv8s</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.2.2.4">71.99</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.1.1.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.1.1.1.m1.1"><semantics id="Ch6.T2.1.1.1.m1.1a"><mo id="Ch6.T2.1.1.1.m1.1.1" stretchy="false" xref="Ch6.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.1.1.1.m1.1b"><ci id="Ch6.T2.1.1.1.m1.1.1.cmml" xref="Ch6.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.1.1.1.m1.1d">↑</annotation></semantics></math>1.64)</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.2.2.5">32.2</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.2.2.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.2.2.2.m1.1"><semantics id="Ch6.T2.2.2.2.m1.1a"><mo id="Ch6.T2.2.2.2.m1.1.1" stretchy="false" xref="Ch6.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.2.2.2.m1.1b"><ci id="Ch6.T2.2.2.2.m1.1.1.cmml" xref="Ch6.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.2.2.2.m1.1d">↑</annotation></semantics></math>3.5)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.4.4.3">YOLOv8m</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.4.4.4">74.84</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.3.3.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.3.3.1.m1.1"><semantics id="Ch6.T2.3.3.1.m1.1a"><mo id="Ch6.T2.3.3.1.m1.1.1" stretchy="false" xref="Ch6.T2.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.3.3.1.m1.1b"><ci id="Ch6.T2.3.3.1.m1.1.1.cmml" xref="Ch6.T2.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.3.3.1.m1.1d">↑</annotation></semantics></math>4.49)</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.4.4.5">72.4</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.4.4.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.4.4.2.m1.1"><semantics id="Ch6.T2.4.4.2.m1.1a"><mo id="Ch6.T2.4.4.2.m1.1.1" stretchy="false" xref="Ch6.T2.4.4.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.4.4.2.m1.1b"><ci id="Ch6.T2.4.4.2.m1.1.1.cmml" xref="Ch6.T2.4.4.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.4.4.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.4.4.2.m1.1d">↑</annotation></semantics></math>43.7)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.6.6.3">YOLOv8l</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.6.6.4">75.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ch6.T2.5.5.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.5.5.1.m1.1"><semantics id="Ch6.T2.5.5.1.m1.1a"><mo id="Ch6.T2.5.5.1.m1.1.1" stretchy="false" xref="Ch6.T2.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.5.5.1.m1.1b"><ci id="Ch6.T2.5.5.1.m1.1.1.cmml" xref="Ch6.T2.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.5.5.1.m1.1d">↑</annotation></semantics></math>5.54)</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.6.6.5">127.1</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.6.6.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.6.6.2.m1.1"><semantics id="Ch6.T2.6.6.2.m1.1a"><mo id="Ch6.T2.6.6.2.m1.1.1" stretchy="false" xref="Ch6.T2.6.6.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.6.6.2.m1.1b"><ci id="Ch6.T2.6.6.2.m1.1.1.cmml" xref="Ch6.T2.6.6.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.6.6.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.6.6.2.m1.1d">↑</annotation></semantics></math>98.4)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.8.8.3">YOLOv8x</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.8.8.4">76.45</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="Ch6.T2.7.7.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.7.7.1.m1.1"><semantics id="Ch6.T2.7.7.1.m1.1a"><mo id="Ch6.T2.7.7.1.m1.1.1" stretchy="false" xref="Ch6.T2.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.7.7.1.m1.1b"><ci id="Ch6.T2.7.7.1.m1.1.1.cmml" xref="Ch6.T2.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.7.7.1.m1.1d">↑</annotation></semantics></math>6.10)</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.8.8.5">196.6</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.8.8.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.8.8.2.m1.1"><semantics id="Ch6.T2.8.8.2.m1.1a"><mo id="Ch6.T2.8.8.2.m1.1.1" stretchy="false" xref="Ch6.T2.8.8.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.8.8.2.m1.1b"><ci id="Ch6.T2.8.8.2.m1.1.1.cmml" xref="Ch6.T2.8.8.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.8.8.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.8.8.2.m1.1d">↑</annotation></semantics></math>167.9)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.10.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T2.10.10.3">ScatYOLOv8n</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.10.10.4">74.42</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="Ch6.T2.9.9.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.9.9.1.m1.1"><semantics id="Ch6.T2.9.9.1.m1.1a"><mo id="Ch6.T2.9.9.1.m1.1.1" stretchy="false" xref="Ch6.T2.9.9.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.9.9.1.m1.1b"><ci id="Ch6.T2.9.9.1.m1.1.1.cmml" xref="Ch6.T2.9.9.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.9.9.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.9.9.1.m1.1d">↑</annotation></semantics></math>4.07)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.10.10.5">58.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T2.10.10.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.10.10.2.m1.1"><semantics id="Ch6.T2.10.10.2.m1.1a"><mo id="Ch6.T2.10.10.2.m1.1.1" stretchy="false" xref="Ch6.T2.10.10.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.10.10.2.m1.1b"><ci id="Ch6.T2.10.10.2.m1.1.1.cmml" xref="Ch6.T2.10.10.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.10.10.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.10.10.2.m1.1d">↑</annotation></semantics></math>29.5)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.12.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T2.12.12.3">YOLOv8n + CBAM</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.12.12.4">70.75</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="Ch6.T2.11.11.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.11.11.1.m1.1"><semantics id="Ch6.T2.11.11.1.m1.1a"><mo id="Ch6.T2.11.11.1.m1.1.1" stretchy="false" xref="Ch6.T2.11.11.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.11.11.1.m1.1b"><ci id="Ch6.T2.11.11.1.m1.1.1.cmml" xref="Ch6.T2.11.11.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.11.11.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.11.11.1.m1.1d">↑</annotation></semantics></math>0.40)</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.12.12.5">29.9</td>
<td class="ltx_td ltx_align_left" id="Ch6.T2.12.12.2">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.12.12.2.m1.1"><semantics id="Ch6.T2.12.12.2.m1.1a"><mo id="Ch6.T2.12.12.2.m1.1.1" stretchy="false" xref="Ch6.T2.12.12.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.12.12.2.m1.1b"><ci id="Ch6.T2.12.12.2.m1.1.1.cmml" xref="Ch6.T2.12.12.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.12.12.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.12.12.2.m1.1d">↑</annotation></semantics></math>1.2)</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.14.14">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="Ch6.T2.14.14.3"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.14.3.1">ScatYOLOv8n + CBAM</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch6.T2.14.14.4"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.14.4.1">75.46</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="Ch6.T2.13.13.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.13.13.1.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.13.13.1.1.m1.1"><semantics id="Ch6.T2.13.13.1.1.m1.1a"><mo id="Ch6.T2.13.13.1.1.m1.1.1" stretchy="false" xref="Ch6.T2.13.13.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.13.13.1.1.m1.1b"><ci id="Ch6.T2.13.13.1.1.m1.1.1.cmml" xref="Ch6.T2.13.13.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.13.13.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.13.13.1.1.m1.1d">↑</annotation></semantics></math>5.11)</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch6.T2.14.14.5"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.14.5.1">59.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="Ch6.T2.14.14.2"><span class="ltx_text ltx_font_bold" id="Ch6.T2.14.14.2.1">(<math alttext="\uparrow" class="ltx_Math" display="inline" id="Ch6.T2.14.14.2.1.m1.1"><semantics id="Ch6.T2.14.14.2.1.m1.1a"><mo id="Ch6.T2.14.14.2.1.m1.1.1" stretchy="false" xref="Ch6.T2.14.14.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="Ch6.T2.14.14.2.1.m1.1b"><ci id="Ch6.T2.14.14.2.1.m1.1.1.cmml" xref="Ch6.T2.14.14.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.T2.14.14.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.T2.14.14.2.1.m1.1d">↑</annotation></semantics></math>30.6)</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="Ch6.S2.p13">
<p class="ltx_p" id="Ch6.S2.p13.1">In assessing the inference times on the embedded system for the proposed enhancements within ScatYOLOv8+CBAM, specifically the ScatBlock and <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a>, their individual impacts were also examined in the ablation study presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T2" title="Table 6.2 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>. The first part of the table outlines the performance metrics of each YOLOv8 model variant. The second part provides the individual and combined contributions of the enhancements introduced in this work. The increments are noted in comparison to the baseline performance of the YOLOv8n model.
It can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T2" title="Table 6.2 ‣ 6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a> that the addition of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cbam" title="Convolutional Block Attention Module">CBAM</span></a> produces an increased <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> at a very minimal computational cost.
The proposed architecture ScatYOLOv8+CBAM, in the lightest version <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S2.p13.1.m1.1"><semantics id="Ch6.S2.p13.1.m1.1a"><mi id="Ch6.S2.p13.1.m1.1.1" xref="Ch6.S2.p13.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S2.p13.1.m1.1b"><ci id="Ch6.S2.p13.1.m1.1.1.cmml" xref="Ch6.S2.p13.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S2.p13.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S2.p13.1.m1.1d">italic_n</annotation></semantics></math>, provides a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> comparable to the deeper and heavier YOLOv8l (75.46% vs 75.89%).
However, the proposed model demonstrates a substantially faster inference speed (59.3 ms versus 127.1 ms) on the NVIDIA Jetson AGX Xavier. This marks a significant improvement over the preliminary findings discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p14">
<p class="ltx_p" id="Ch6.S2.p14.1">It has been shown that ScatYOLOv8+CBAM enables the efficient handling of images in maritime environments deployed on embedded systems, facilitating faster and more accurate real-time ship segmentation by leveraging the capabilities of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a> s with the added robustness provided by the scattering transform and attention mechanisms.
This indicates its potential viability to enhance real-world maritime situational awareness applications.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p15">
<p class="ltx_p" id="Ch6.S2.p15.1">It is important to note that one of the goals of this thesis is the improvement of maritime situational awareness leveraging ship segmentation for accurate georeferencing.
Therefore, to further validate the ScatYOLOv8+CBAM architecture, the output masks of this architecture are evaluated for ship georeferencing Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S3" title="7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.3</span></a>, as presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. This evaluation shows, with higher segmentation <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a>, consistent results for georeferencing compared to the georeferencing evaluation of the standard segmentation methods studied in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">6.3   Optimized ScatYOLOv8+CBAM <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>
</h3>
<div class="ltx_para" id="Ch6.S3.p1">
<p class="ltx_p" id="Ch6.S3.p1.1">As it was motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, to enhance maritime situational awareness, optimizing for real-time processing capabilities is key. Therefore, ship recognition should operate with the highest possible accuracy and the shortest inference times on embedded systems.
This section studies optimizations to ScatYOLOv8+CBAM for a more time-efficient ship segmentation, by circumventing redundancies in the original ScatBlock.</p>
</div>
<div class="ltx_para" id="Ch6.S3.p2">
<p class="ltx_p" id="Ch6.S3.p2.2">As shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>, the 2D scattering transform typically downsamples the input image by a factor of <math alttext="2^{J}" class="ltx_Math" display="inline" id="Ch6.S3.p2.1.m1.1"><semantics id="Ch6.S3.p2.1.m1.1a"><msup id="Ch6.S3.p2.1.m1.1.1" xref="Ch6.S3.p2.1.m1.1.1.cmml"><mn id="Ch6.S3.p2.1.m1.1.1.2" xref="Ch6.S3.p2.1.m1.1.1.2.cmml">2</mn><mi id="Ch6.S3.p2.1.m1.1.1.3" xref="Ch6.S3.p2.1.m1.1.1.3.cmml">J</mi></msup><annotation-xml encoding="MathML-Content" id="Ch6.S3.p2.1.m1.1b"><apply id="Ch6.S3.p2.1.m1.1.1.cmml" xref="Ch6.S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch6.S3.p2.1.m1.1.1.1.cmml" xref="Ch6.S3.p2.1.m1.1.1">superscript</csymbol><cn id="Ch6.S3.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch6.S3.p2.1.m1.1.1.2">2</cn><ci id="Ch6.S3.p2.1.m1.1.1.3.cmml" xref="Ch6.S3.p2.1.m1.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p2.1.m1.1c">2^{J}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p2.1.m1.1d">2 start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT</annotation></semantics></math> to reduce computational complexity across scales.
For the ScatBlock, which uses only first-order coefficients, this translates to an output resolution that is half the input resolution. To address this, the ScatBlock upsamples the input image to <math alttext="(2\times H)\times(2\times W)" class="ltx_Math" display="inline" id="Ch6.S3.p2.2.m2.2"><semantics id="Ch6.S3.p2.2.m2.2a"><mrow id="Ch6.S3.p2.2.m2.2.2" xref="Ch6.S3.p2.2.m2.2.2.cmml"><mrow id="Ch6.S3.p2.2.m2.1.1.1.1" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.cmml"><mo id="Ch6.S3.p2.2.m2.1.1.1.1.2" stretchy="false" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="Ch6.S3.p2.2.m2.1.1.1.1.1" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.cmml"><mn id="Ch6.S3.p2.2.m2.1.1.1.1.1.2" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.2.cmml">2</mn><mo id="Ch6.S3.p2.2.m2.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.1.cmml">×</mo><mi id="Ch6.S3.p2.2.m2.1.1.1.1.1.3" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.3.cmml">H</mi></mrow><mo id="Ch6.S3.p2.2.m2.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow><mo id="Ch6.S3.p2.2.m2.2.2.3" rspace="0.222em" xref="Ch6.S3.p2.2.m2.2.2.3.cmml">×</mo><mrow id="Ch6.S3.p2.2.m2.2.2.2.1" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.cmml"><mo id="Ch6.S3.p2.2.m2.2.2.2.1.2" stretchy="false" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.cmml">(</mo><mrow id="Ch6.S3.p2.2.m2.2.2.2.1.1" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.cmml"><mn id="Ch6.S3.p2.2.m2.2.2.2.1.1.2" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.2.cmml">2</mn><mo id="Ch6.S3.p2.2.m2.2.2.2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.1.cmml">×</mo><mi id="Ch6.S3.p2.2.m2.2.2.2.1.1.3" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.3.cmml">W</mi></mrow><mo id="Ch6.S3.p2.2.m2.2.2.2.1.3" stretchy="false" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S3.p2.2.m2.2b"><apply id="Ch6.S3.p2.2.m2.2.2.cmml" xref="Ch6.S3.p2.2.m2.2.2"><times id="Ch6.S3.p2.2.m2.2.2.3.cmml" xref="Ch6.S3.p2.2.m2.2.2.3"></times><apply id="Ch6.S3.p2.2.m2.1.1.1.1.1.cmml" xref="Ch6.S3.p2.2.m2.1.1.1.1"><times id="Ch6.S3.p2.2.m2.1.1.1.1.1.1.cmml" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.1"></times><cn id="Ch6.S3.p2.2.m2.1.1.1.1.1.2.cmml" type="integer" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.2">2</cn><ci id="Ch6.S3.p2.2.m2.1.1.1.1.1.3.cmml" xref="Ch6.S3.p2.2.m2.1.1.1.1.1.3">𝐻</ci></apply><apply id="Ch6.S3.p2.2.m2.2.2.2.1.1.cmml" xref="Ch6.S3.p2.2.m2.2.2.2.1"><times id="Ch6.S3.p2.2.m2.2.2.2.1.1.1.cmml" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.1"></times><cn id="Ch6.S3.p2.2.m2.2.2.2.1.1.2.cmml" type="integer" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.2">2</cn><ci id="Ch6.S3.p2.2.m2.2.2.2.1.1.3.cmml" xref="Ch6.S3.p2.2.m2.2.2.2.1.1.3">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p2.2.m2.2c">(2\times H)\times(2\times W)</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p2.2.m2.2d">( 2 × italic_H ) × ( 2 × italic_W )</annotation></semantics></math> (see fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F3" title="Figure 6.3 ‣ 6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>). This ensures that the output dimensions match the size of the input image, which is crucial for the compatibility with subsequent YOLOv8 backbone blocks.
However, the sequential upsampling and downsampling in image resolution increases computational burden.
Furthermore, while the deployment on the NVIDIA Jetson AGX Xavier was documented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>) using Pytorch weights, the potential for model optimization with TensorRT to achieve more efficient real-time inference was not investigated.
The contributions of <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a> address these areas of improvement, by optimizing the custom architecture ScatYOLOv8+CBAM and performing a comprehensive evaluation for ship segmentation with all model sizes, focusing on real-time processing on the embedded system.</p>
</div>
<div class="ltx_para" id="Ch6.S3.p3">
<p class="ltx_p" id="Ch6.S3.p3.2">The main optimization focuses on the ScatBlock. As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S1" title="6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.1</span></a>, the ScatBlock was implemented using the open-source Python module <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S3.p3.1.m1.1"><semantics id="Ch6.S3.p3.1.m1.1a"><mrow id="Ch6.S3.p3.1.m1.1.1" xref="Ch6.S3.p3.1.m1.1.1.cmml"><mi id="Ch6.S3.p3.1.m1.1.1.2" xref="Ch6.S3.p3.1.m1.1.1.2.cmml">p</mi><mo id="Ch6.S3.p3.1.m1.1.1.1" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.3" xref="Ch6.S3.p3.1.m1.1.1.3.cmml">y</mi><mo id="Ch6.S3.p3.1.m1.1.1.1a" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.4" xref="Ch6.S3.p3.1.m1.1.1.4.cmml">t</mi><mo id="Ch6.S3.p3.1.m1.1.1.1b" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.5" xref="Ch6.S3.p3.1.m1.1.1.5.cmml">o</mi><mo id="Ch6.S3.p3.1.m1.1.1.1c" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.6" xref="Ch6.S3.p3.1.m1.1.1.6.cmml">r</mi><mo id="Ch6.S3.p3.1.m1.1.1.1d" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.7" xref="Ch6.S3.p3.1.m1.1.1.7.cmml">c</mi><mo id="Ch6.S3.p3.1.m1.1.1.1e" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.8" xref="Ch6.S3.p3.1.m1.1.1.8.cmml">h</mi><mo id="Ch6.S3.p3.1.m1.1.1.1f" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.9" mathvariant="normal" xref="Ch6.S3.p3.1.m1.1.1.9.cmml">_</mi><mo id="Ch6.S3.p3.1.m1.1.1.1g" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.10" xref="Ch6.S3.p3.1.m1.1.1.10.cmml">w</mi><mo id="Ch6.S3.p3.1.m1.1.1.1h" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.11" xref="Ch6.S3.p3.1.m1.1.1.11.cmml">a</mi><mo id="Ch6.S3.p3.1.m1.1.1.1i" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.12" xref="Ch6.S3.p3.1.m1.1.1.12.cmml">v</mi><mo id="Ch6.S3.p3.1.m1.1.1.1j" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.13" xref="Ch6.S3.p3.1.m1.1.1.13.cmml">e</mi><mo id="Ch6.S3.p3.1.m1.1.1.1k" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.14" xref="Ch6.S3.p3.1.m1.1.1.14.cmml">l</mi><mo id="Ch6.S3.p3.1.m1.1.1.1l" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.15" xref="Ch6.S3.p3.1.m1.1.1.15.cmml">e</mi><mo id="Ch6.S3.p3.1.m1.1.1.1m" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.16" xref="Ch6.S3.p3.1.m1.1.1.16.cmml">t</mi><mo id="Ch6.S3.p3.1.m1.1.1.1n" xref="Ch6.S3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.1.m1.1.1.17" xref="Ch6.S3.p3.1.m1.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S3.p3.1.m1.1b"><apply id="Ch6.S3.p3.1.m1.1.1.cmml" xref="Ch6.S3.p3.1.m1.1.1"><times id="Ch6.S3.p3.1.m1.1.1.1.cmml" xref="Ch6.S3.p3.1.m1.1.1.1"></times><ci id="Ch6.S3.p3.1.m1.1.1.2.cmml" xref="Ch6.S3.p3.1.m1.1.1.2">𝑝</ci><ci id="Ch6.S3.p3.1.m1.1.1.3.cmml" xref="Ch6.S3.p3.1.m1.1.1.3">𝑦</ci><ci id="Ch6.S3.p3.1.m1.1.1.4.cmml" xref="Ch6.S3.p3.1.m1.1.1.4">𝑡</ci><ci id="Ch6.S3.p3.1.m1.1.1.5.cmml" xref="Ch6.S3.p3.1.m1.1.1.5">𝑜</ci><ci id="Ch6.S3.p3.1.m1.1.1.6.cmml" xref="Ch6.S3.p3.1.m1.1.1.6">𝑟</ci><ci id="Ch6.S3.p3.1.m1.1.1.7.cmml" xref="Ch6.S3.p3.1.m1.1.1.7">𝑐</ci><ci id="Ch6.S3.p3.1.m1.1.1.8.cmml" xref="Ch6.S3.p3.1.m1.1.1.8">ℎ</ci><ci id="Ch6.S3.p3.1.m1.1.1.9.cmml" xref="Ch6.S3.p3.1.m1.1.1.9">_</ci><ci id="Ch6.S3.p3.1.m1.1.1.10.cmml" xref="Ch6.S3.p3.1.m1.1.1.10">𝑤</ci><ci id="Ch6.S3.p3.1.m1.1.1.11.cmml" xref="Ch6.S3.p3.1.m1.1.1.11">𝑎</ci><ci id="Ch6.S3.p3.1.m1.1.1.12.cmml" xref="Ch6.S3.p3.1.m1.1.1.12">𝑣</ci><ci id="Ch6.S3.p3.1.m1.1.1.13.cmml" xref="Ch6.S3.p3.1.m1.1.1.13">𝑒</ci><ci id="Ch6.S3.p3.1.m1.1.1.14.cmml" xref="Ch6.S3.p3.1.m1.1.1.14">𝑙</ci><ci id="Ch6.S3.p3.1.m1.1.1.15.cmml" xref="Ch6.S3.p3.1.m1.1.1.15">𝑒</ci><ci id="Ch6.S3.p3.1.m1.1.1.16.cmml" xref="Ch6.S3.p3.1.m1.1.1.16">𝑡</ci><ci id="Ch6.S3.p3.1.m1.1.1.17.cmml" xref="Ch6.S3.p3.1.m1.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p3.1.m1.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p3.1.m1.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite> to achieve the 2D scattering transform.
In the optimization, the initial step involves removing the downsampling associated with the scattering transform. This refinement is achieved by bypassing, within the <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S3.p3.2.m2.1"><semantics id="Ch6.S3.p3.2.m2.1a"><mrow id="Ch6.S3.p3.2.m2.1.1" xref="Ch6.S3.p3.2.m2.1.1.cmml"><mi id="Ch6.S3.p3.2.m2.1.1.2" xref="Ch6.S3.p3.2.m2.1.1.2.cmml">p</mi><mo id="Ch6.S3.p3.2.m2.1.1.1" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.3" xref="Ch6.S3.p3.2.m2.1.1.3.cmml">y</mi><mo id="Ch6.S3.p3.2.m2.1.1.1a" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.4" xref="Ch6.S3.p3.2.m2.1.1.4.cmml">t</mi><mo id="Ch6.S3.p3.2.m2.1.1.1b" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.5" xref="Ch6.S3.p3.2.m2.1.1.5.cmml">o</mi><mo id="Ch6.S3.p3.2.m2.1.1.1c" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.6" xref="Ch6.S3.p3.2.m2.1.1.6.cmml">r</mi><mo id="Ch6.S3.p3.2.m2.1.1.1d" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.7" xref="Ch6.S3.p3.2.m2.1.1.7.cmml">c</mi><mo id="Ch6.S3.p3.2.m2.1.1.1e" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.8" xref="Ch6.S3.p3.2.m2.1.1.8.cmml">h</mi><mo id="Ch6.S3.p3.2.m2.1.1.1f" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.9" mathvariant="normal" xref="Ch6.S3.p3.2.m2.1.1.9.cmml">_</mi><mo id="Ch6.S3.p3.2.m2.1.1.1g" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.10" xref="Ch6.S3.p3.2.m2.1.1.10.cmml">w</mi><mo id="Ch6.S3.p3.2.m2.1.1.1h" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.11" xref="Ch6.S3.p3.2.m2.1.1.11.cmml">a</mi><mo id="Ch6.S3.p3.2.m2.1.1.1i" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.12" xref="Ch6.S3.p3.2.m2.1.1.12.cmml">v</mi><mo id="Ch6.S3.p3.2.m2.1.1.1j" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.13" xref="Ch6.S3.p3.2.m2.1.1.13.cmml">e</mi><mo id="Ch6.S3.p3.2.m2.1.1.1k" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.14" xref="Ch6.S3.p3.2.m2.1.1.14.cmml">l</mi><mo id="Ch6.S3.p3.2.m2.1.1.1l" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.15" xref="Ch6.S3.p3.2.m2.1.1.15.cmml">e</mi><mo id="Ch6.S3.p3.2.m2.1.1.1m" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.16" xref="Ch6.S3.p3.2.m2.1.1.16.cmml">t</mi><mo id="Ch6.S3.p3.2.m2.1.1.1n" xref="Ch6.S3.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p3.2.m2.1.1.17" xref="Ch6.S3.p3.2.m2.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S3.p3.2.m2.1b"><apply id="Ch6.S3.p3.2.m2.1.1.cmml" xref="Ch6.S3.p3.2.m2.1.1"><times id="Ch6.S3.p3.2.m2.1.1.1.cmml" xref="Ch6.S3.p3.2.m2.1.1.1"></times><ci id="Ch6.S3.p3.2.m2.1.1.2.cmml" xref="Ch6.S3.p3.2.m2.1.1.2">𝑝</ci><ci id="Ch6.S3.p3.2.m2.1.1.3.cmml" xref="Ch6.S3.p3.2.m2.1.1.3">𝑦</ci><ci id="Ch6.S3.p3.2.m2.1.1.4.cmml" xref="Ch6.S3.p3.2.m2.1.1.4">𝑡</ci><ci id="Ch6.S3.p3.2.m2.1.1.5.cmml" xref="Ch6.S3.p3.2.m2.1.1.5">𝑜</ci><ci id="Ch6.S3.p3.2.m2.1.1.6.cmml" xref="Ch6.S3.p3.2.m2.1.1.6">𝑟</ci><ci id="Ch6.S3.p3.2.m2.1.1.7.cmml" xref="Ch6.S3.p3.2.m2.1.1.7">𝑐</ci><ci id="Ch6.S3.p3.2.m2.1.1.8.cmml" xref="Ch6.S3.p3.2.m2.1.1.8">ℎ</ci><ci id="Ch6.S3.p3.2.m2.1.1.9.cmml" xref="Ch6.S3.p3.2.m2.1.1.9">_</ci><ci id="Ch6.S3.p3.2.m2.1.1.10.cmml" xref="Ch6.S3.p3.2.m2.1.1.10">𝑤</ci><ci id="Ch6.S3.p3.2.m2.1.1.11.cmml" xref="Ch6.S3.p3.2.m2.1.1.11">𝑎</ci><ci id="Ch6.S3.p3.2.m2.1.1.12.cmml" xref="Ch6.S3.p3.2.m2.1.1.12">𝑣</ci><ci id="Ch6.S3.p3.2.m2.1.1.13.cmml" xref="Ch6.S3.p3.2.m2.1.1.13">𝑒</ci><ci id="Ch6.S3.p3.2.m2.1.1.14.cmml" xref="Ch6.S3.p3.2.m2.1.1.14">𝑙</ci><ci id="Ch6.S3.p3.2.m2.1.1.15.cmml" xref="Ch6.S3.p3.2.m2.1.1.15">𝑒</ci><ci id="Ch6.S3.p3.2.m2.1.1.16.cmml" xref="Ch6.S3.p3.2.m2.1.1.16">𝑡</ci><ci id="Ch6.S3.p3.2.m2.1.1.17.cmml" xref="Ch6.S3.p3.2.m2.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p3.2.m2.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p3.2.m2.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> package, the division of the image into distinct frequencies and considering all frequency components simultaneously.
Following this, the downsampling step of the scattering transform is omitted. The absence of quadrant division means that downsampling would inappropriately cause a mismatch in resolution by presupposing a quadrant-based reduction. As a result, the enhanced ScatBlock does not need the upsample to retain the original resolution in its output feature map. This ensures the preservation of vital image details crucial for accurate segmentation, while simultaneously boosting inference speed.</p>
</div>
<div class="ltx_para" id="Ch6.S3.p4">
<p class="ltx_p" id="Ch6.S3.p4.7">The optimization of the ScatYOLOv8+CBAM architecture introduced <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> was presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>, which removes the upsample from the Scatblock (see upsample in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F3" title="Figure 6.3 ‣ 6.1 The ScatBlock [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>) and the downsampling operation eliminated from the <math alttext="pytorch\_wavelets" class="ltx_Math" display="inline" id="Ch6.S3.p4.1.m1.1"><semantics id="Ch6.S3.p4.1.m1.1a"><mrow id="Ch6.S3.p4.1.m1.1.1" xref="Ch6.S3.p4.1.m1.1.1.cmml"><mi id="Ch6.S3.p4.1.m1.1.1.2" xref="Ch6.S3.p4.1.m1.1.1.2.cmml">p</mi><mo id="Ch6.S3.p4.1.m1.1.1.1" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.3" xref="Ch6.S3.p4.1.m1.1.1.3.cmml">y</mi><mo id="Ch6.S3.p4.1.m1.1.1.1a" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.4" xref="Ch6.S3.p4.1.m1.1.1.4.cmml">t</mi><mo id="Ch6.S3.p4.1.m1.1.1.1b" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.5" xref="Ch6.S3.p4.1.m1.1.1.5.cmml">o</mi><mo id="Ch6.S3.p4.1.m1.1.1.1c" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.6" xref="Ch6.S3.p4.1.m1.1.1.6.cmml">r</mi><mo id="Ch6.S3.p4.1.m1.1.1.1d" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.7" xref="Ch6.S3.p4.1.m1.1.1.7.cmml">c</mi><mo id="Ch6.S3.p4.1.m1.1.1.1e" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.8" xref="Ch6.S3.p4.1.m1.1.1.8.cmml">h</mi><mo id="Ch6.S3.p4.1.m1.1.1.1f" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.9" mathvariant="normal" xref="Ch6.S3.p4.1.m1.1.1.9.cmml">_</mi><mo id="Ch6.S3.p4.1.m1.1.1.1g" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.10" xref="Ch6.S3.p4.1.m1.1.1.10.cmml">w</mi><mo id="Ch6.S3.p4.1.m1.1.1.1h" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.11" xref="Ch6.S3.p4.1.m1.1.1.11.cmml">a</mi><mo id="Ch6.S3.p4.1.m1.1.1.1i" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.12" xref="Ch6.S3.p4.1.m1.1.1.12.cmml">v</mi><mo id="Ch6.S3.p4.1.m1.1.1.1j" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.13" xref="Ch6.S3.p4.1.m1.1.1.13.cmml">e</mi><mo id="Ch6.S3.p4.1.m1.1.1.1k" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.14" xref="Ch6.S3.p4.1.m1.1.1.14.cmml">l</mi><mo id="Ch6.S3.p4.1.m1.1.1.1l" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.15" xref="Ch6.S3.p4.1.m1.1.1.15.cmml">e</mi><mo id="Ch6.S3.p4.1.m1.1.1.1m" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.16" xref="Ch6.S3.p4.1.m1.1.1.16.cmml">t</mi><mo id="Ch6.S3.p4.1.m1.1.1.1n" xref="Ch6.S3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch6.S3.p4.1.m1.1.1.17" xref="Ch6.S3.p4.1.m1.1.1.17.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.1.m1.1b"><apply id="Ch6.S3.p4.1.m1.1.1.cmml" xref="Ch6.S3.p4.1.m1.1.1"><times id="Ch6.S3.p4.1.m1.1.1.1.cmml" xref="Ch6.S3.p4.1.m1.1.1.1"></times><ci id="Ch6.S3.p4.1.m1.1.1.2.cmml" xref="Ch6.S3.p4.1.m1.1.1.2">𝑝</ci><ci id="Ch6.S3.p4.1.m1.1.1.3.cmml" xref="Ch6.S3.p4.1.m1.1.1.3">𝑦</ci><ci id="Ch6.S3.p4.1.m1.1.1.4.cmml" xref="Ch6.S3.p4.1.m1.1.1.4">𝑡</ci><ci id="Ch6.S3.p4.1.m1.1.1.5.cmml" xref="Ch6.S3.p4.1.m1.1.1.5">𝑜</ci><ci id="Ch6.S3.p4.1.m1.1.1.6.cmml" xref="Ch6.S3.p4.1.m1.1.1.6">𝑟</ci><ci id="Ch6.S3.p4.1.m1.1.1.7.cmml" xref="Ch6.S3.p4.1.m1.1.1.7">𝑐</ci><ci id="Ch6.S3.p4.1.m1.1.1.8.cmml" xref="Ch6.S3.p4.1.m1.1.1.8">ℎ</ci><ci id="Ch6.S3.p4.1.m1.1.1.9.cmml" xref="Ch6.S3.p4.1.m1.1.1.9">_</ci><ci id="Ch6.S3.p4.1.m1.1.1.10.cmml" xref="Ch6.S3.p4.1.m1.1.1.10">𝑤</ci><ci id="Ch6.S3.p4.1.m1.1.1.11.cmml" xref="Ch6.S3.p4.1.m1.1.1.11">𝑎</ci><ci id="Ch6.S3.p4.1.m1.1.1.12.cmml" xref="Ch6.S3.p4.1.m1.1.1.12">𝑣</ci><ci id="Ch6.S3.p4.1.m1.1.1.13.cmml" xref="Ch6.S3.p4.1.m1.1.1.13">𝑒</ci><ci id="Ch6.S3.p4.1.m1.1.1.14.cmml" xref="Ch6.S3.p4.1.m1.1.1.14">𝑙</ci><ci id="Ch6.S3.p4.1.m1.1.1.15.cmml" xref="Ch6.S3.p4.1.m1.1.1.15">𝑒</ci><ci id="Ch6.S3.p4.1.m1.1.1.16.cmml" xref="Ch6.S3.p4.1.m1.1.1.16">𝑡</ci><ci id="Ch6.S3.p4.1.m1.1.1.17.cmml" xref="Ch6.S3.p4.1.m1.1.1.17">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.1.m1.1c">pytorch\_wavelets</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.1.m1.1d">italic_p italic_y italic_t italic_o italic_r italic_c italic_h _ italic_w italic_a italic_v italic_e italic_l italic_e italic_t italic_s</annotation></semantics></math> package <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib116" title="">cotter_2020 </a></cite>.
To examine the optimization, the original ScatYOLOv8+CBAM serves as a benchmark for comparison, as well as standard YOLOv8 (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F9" title="Figure 6.9 ‣ 6.3 Optimized ScatYOLOv8+CBAM [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.9</span></a>).
The newly optimized ScatYOLOv8+CBAM was trained across all model sizes (<math alttext="n" class="ltx_Math" display="inline" id="Ch6.S3.p4.2.m2.1"><semantics id="Ch6.S3.p4.2.m2.1a"><mi id="Ch6.S3.p4.2.m2.1.1" xref="Ch6.S3.p4.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.2.m2.1b"><ci id="Ch6.S3.p4.2.m2.1.1.cmml" xref="Ch6.S3.p4.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.2.m2.1d">italic_n</annotation></semantics></math>, <math alttext="s" class="ltx_Math" display="inline" id="Ch6.S3.p4.3.m3.1"><semantics id="Ch6.S3.p4.3.m3.1a"><mi id="Ch6.S3.p4.3.m3.1.1" xref="Ch6.S3.p4.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.3.m3.1b"><ci id="Ch6.S3.p4.3.m3.1.1.cmml" xref="Ch6.S3.p4.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.3.m3.1d">italic_s</annotation></semantics></math>, <math alttext="m" class="ltx_Math" display="inline" id="Ch6.S3.p4.4.m4.1"><semantics id="Ch6.S3.p4.4.m4.1a"><mi id="Ch6.S3.p4.4.m4.1.1" xref="Ch6.S3.p4.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.4.m4.1b"><ci id="Ch6.S3.p4.4.m4.1.1.cmml" xref="Ch6.S3.p4.4.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.4.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.4.m4.1d">italic_m</annotation></semantics></math>, <math alttext="l" class="ltx_Math" display="inline" id="Ch6.S3.p4.5.m5.1"><semantics id="Ch6.S3.p4.5.m5.1a"><mi id="Ch6.S3.p4.5.m5.1.1" xref="Ch6.S3.p4.5.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.5.m5.1b"><ci id="Ch6.S3.p4.5.m5.1.1.cmml" xref="Ch6.S3.p4.5.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.5.m5.1c">l</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.5.m5.1d">italic_l</annotation></semantics></math>, <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S3.p4.6.m6.1"><semantics id="Ch6.S3.p4.6.m6.1a"><mi id="Ch6.S3.p4.6.m6.1.1" xref="Ch6.S3.p4.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.6.m6.1b"><ci id="Ch6.S3.p4.6.m6.1.1.cmml" xref="Ch6.S3.p4.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.6.m6.1d">italic_x</annotation></semantics></math>), employing the original parameters specified in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. This includes using an input size of <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch6.S3.p4.7.m7.1"><semantics id="Ch6.S3.p4.7.m7.1a"><mrow id="Ch6.S3.p4.7.m7.1.1" xref="Ch6.S3.p4.7.m7.1.1.cmml"><mn id="Ch6.S3.p4.7.m7.1.1.2" xref="Ch6.S3.p4.7.m7.1.1.2.cmml">640</mn><mo id="Ch6.S3.p4.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S3.p4.7.m7.1.1.1.cmml">×</mo><mn id="Ch6.S3.p4.7.m7.1.1.3" xref="Ch6.S3.p4.7.m7.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S3.p4.7.m7.1b"><apply id="Ch6.S3.p4.7.m7.1.1.cmml" xref="Ch6.S3.p4.7.m7.1.1"><times id="Ch6.S3.p4.7.m7.1.1.1.cmml" xref="Ch6.S3.p4.7.m7.1.1.1"></times><cn id="Ch6.S3.p4.7.m7.1.1.2.cmml" type="integer" xref="Ch6.S3.p4.7.m7.1.1.2">640</cn><cn id="Ch6.S3.p4.7.m7.1.1.3.cmml" type="integer" xref="Ch6.S3.p4.7.m7.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p4.7.m7.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p4.7.m7.1d">640 × 640</annotation></semantics></math> pixels, initializing weights randomly, and setting the training period to 300 epochs. To measure inference times, the duration from when an image is inputted to when predictions are obtained is recorded, now using TensorRT-exported weights on the Jetson AGX Xavier. The approach in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a> of using TensorRT contrasts with the prior utilization of Pytorch weights in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>).</p>
</div>
<figure class="ltx_figure" id="Ch6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="Ch6.F9.g1" src="extracted/5906916/fig/scat_comparison.png" width="452"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F9.2.1.1" style="font-size:90%;">Figure 6.9</span>: </span><a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" style="font-size:90%;" title="mean Average Precision">mAP</span></a><span class="ltx_text" id="Ch6.F9.3.2" style="font-size:90%;"> vs TensorRT inference times on NVIDIA Jetson AGX Xavier, showing the improved performance speed of the optimized ScatYOLOv8+CBAM relative to the predecessor. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>. ©2024 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S3.p5">
<p class="ltx_p" id="Ch6.S3.p5.6">A comparison of mAP for ship segmentation and TensorRT inference times on the Jetson AGX Xavier can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F9" title="Figure 6.9 ‣ 6.3 Optimized ScatYOLOv8+CBAM [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.9</span></a>.
The comparison covers the optimized ScatYOLOv8+CBAM, its predecessor, and the standard YOLOv8.
The optimized ScatYOLOv8+CBAM <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S3.p5.1.m1.1"><semantics id="Ch6.S3.p5.1.m1.1a"><mi id="Ch6.S3.p5.1.m1.1.1" xref="Ch6.S3.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.1.m1.1b"><ci id="Ch6.S3.p5.1.m1.1.1.cmml" xref="Ch6.S3.p5.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.1.m1.1d">italic_n</annotation></semantics></math> model provides comparable precision compared with the earlier version and the larger <math alttext="l" class="ltx_Math" display="inline" id="Ch6.S3.p5.2.m2.1"><semantics id="Ch6.S3.p5.2.m2.1a"><mi id="Ch6.S3.p5.2.m2.1.1" xref="Ch6.S3.p5.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.2.m2.1b"><ci id="Ch6.S3.p5.2.m2.1.1.cmml" xref="Ch6.S3.p5.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.2.m2.1d">italic_l</annotation></semantics></math> model of YOLOv8, with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> s of 75.39%, 75.46%, and 75.89%, respectively.
Yet, the optimized model is much faster, with an inference time of 25.3 ms versus 39.9 ms and 77.5 ms for the other models.
This shows that the <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S3.p5.3.m3.1"><semantics id="Ch6.S3.p5.3.m3.1a"><mi id="Ch6.S3.p5.3.m3.1.1" xref="Ch6.S3.p5.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.3.m3.1b"><ci id="Ch6.S3.p5.3.m3.1.1.cmml" xref="Ch6.S3.p5.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.3.m3.1d">italic_n</annotation></semantics></math> model of the optimized ScatYOLOv8+CBAM is 36.5% faster than its predecessor with a minor drop in mAP compared to the original ScatBlock-equipped architecture (0.06%).
Additionally, the <math alttext="s" class="ltx_Math" display="inline" id="Ch6.S3.p5.4.m4.1"><semantics id="Ch6.S3.p5.4.m4.1a"><mi id="Ch6.S3.p5.4.m4.1.1" xref="Ch6.S3.p5.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.4.m4.1b"><ci id="Ch6.S3.p5.4.m4.1.1.cmml" xref="Ch6.S3.p5.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.4.m4.1d">italic_s</annotation></semantics></math> and <math alttext="m" class="ltx_Math" display="inline" id="Ch6.S3.p5.5.m5.1"><semantics id="Ch6.S3.p5.5.m5.1a"><mi id="Ch6.S3.p5.5.m5.1.1" xref="Ch6.S3.p5.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.5.m5.1b"><ci id="Ch6.S3.p5.5.m5.1.1.cmml" xref="Ch6.S3.p5.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.5.m5.1d">italic_m</annotation></semantics></math> models exceed in mAP over the largest YOLOv8<math alttext="x" class="ltx_Math" display="inline" id="Ch6.S3.p5.6.m6.1"><semantics id="Ch6.S3.p5.6.m6.1a"><mi id="Ch6.S3.p5.6.m6.1.1" xref="Ch6.S3.p5.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p5.6.m6.1b"><ci id="Ch6.S3.p5.6.m6.1.1.cmml" xref="Ch6.S3.p5.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p5.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p5.6.m6.1d">italic_x</annotation></semantics></math> but offering quicker inference.</p>
</div>
<div class="ltx_para" id="Ch6.S3.p6">
<p class="ltx_p" id="Ch6.S3.p6.2">The optimized ScatYOLOv8+CBAM speeds up inference against its predecessor, making it more suitable for real-time use.
Although the <math alttext="l" class="ltx_Math" display="inline" id="Ch6.S3.p6.1.m1.1"><semantics id="Ch6.S3.p6.1.m1.1a"><mi id="Ch6.S3.p6.1.m1.1.1" xref="Ch6.S3.p6.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p6.1.m1.1b"><ci id="Ch6.S3.p6.1.m1.1.1.cmml" xref="Ch6.S3.p6.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p6.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p6.1.m1.1d">italic_l</annotation></semantics></math> and <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S3.p6.2.m2.1"><semantics id="Ch6.S3.p6.2.m2.1a"><mi id="Ch6.S3.p6.2.m2.1.1" xref="Ch6.S3.p6.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S3.p6.2.m2.1b"><ci id="Ch6.S3.p6.2.m2.1.1.cmml" xref="Ch6.S3.p6.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.p6.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.p6.2.m2.1d">italic_x</annotation></semantics></math> sizes of the optimized model lead in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a>, their slower inference speeds limit their real-time deployment.</p>
</div>
<div class="ltx_para" id="Ch6.S3.p7">
<p class="ltx_p" id="Ch6.S3.p7.1">In summary, the optimized ScatYOLOv8+CBAM has achieved faster inference speeds on the NVIDIA Jetson AGX Xavier with TensorRT, outpacing the previous version. Moreover, it surpasses standard YOLOv8 models in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a>.
However, while shallower models demonstrate significant improvements in accuracy and speed compared to state-of-the-art ship segmentation on ShipSG, deeper and larger models exhibit a noticeable slowdown. This slowdown suggests an increase in computational complexity within the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.cnn" title="Convolutional Neural Network">CNN</span></a> when it processes scattering coefficients in larger models, opening room for further improvements that will be discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S5" title="6.5 Summary and Discussion ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.5</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S4">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">6.4   Enhanced Small Ship Segmentation Using Higher Resolution Images <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>
</h3>
<div class="ltx_para" id="Ch6.S4.p1">
<p class="ltx_p" id="Ch6.S4.p1.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, we observed a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> decrease in segmenting small and distant ships, especially for the initially considered as real-time methods (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T2" title="Table 5.2 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>).
As motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, the recognition of small and distant ships in maritime footage holds significant implications for navigation, safety, and security.
Typically, object detectors and instance segmentation methods reduce image size for quicker inference, losing critical details in the image.
On the other hand, high-resolution deep learning approaches strain memory and computational resources, particularly on embedded systems with limited capacity.
Innovating segmentation architectures for such systems is crucial to overcome these challenges.
In <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>, the proposed batch-processed <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">Slicing Aided Hyper Inference (SAHI)</span></a>, combined with the optimized version of ScatYOLOv8+CBAM, advance the state-of-the-art in small ship segmentation using embedded platforms.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p2">
<p class="ltx_p" id="Ch6.S4.p2.1">Existing deep-learning approaches use image super-resolution or incorporate additional network blocks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib21" title="">rekavandi2022guide </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib91" title="">wang2023uav </a></cite>, which is not ideal for embedded systems due to memory constrains.
The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> framework <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib93" title="">sahi23akyon </a></cite> improves the recognition of small objects in high-resolution images by dividing images into overlapping patches that retain their original size. Then, object detection or segmentation is performed in sequentially on the patches using a compatible method, such as YOLOv8. The resulting detections are then merged with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">Non-Maximum Suppression (NMS)</span></a>. In the case of the segmented masks, they are merged using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.nms" title="Non-Maximum Suppression">NMS</span></a> and then combined appropriately with a logical OR operator.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p3">
<p class="ltx_p" id="Ch6.S4.p3.1">The sequential inference of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> limits speed, suggesting batch inference integration could boost efficiency on the embedded system.
The work in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a> modifies the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> framework<span class="ltx_note ltx_role_footnote" id="Ch6.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/obss/sahi/" title="">https://github.com/obss/sahi/</a></span></span></span> by adding batch processing for the inference stage, addressing the original sequential slice processing constraint as documented in <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib93" title="">sahi23akyon </a></cite>. By enabling ScatYOLOv8+CBAM (and YOLOv8) to infer masks on multiple slices simultaneously (<math alttext="batch=N_{slices}" class="ltx_Math" display="inline" id="Ch6.S4.p3.1.m1.1"><semantics id="Ch6.S4.p3.1.m1.1a"><mrow id="Ch6.S4.p3.1.m1.1.1" xref="Ch6.S4.p3.1.m1.1.1.cmml"><mrow id="Ch6.S4.p3.1.m1.1.1.2" xref="Ch6.S4.p3.1.m1.1.1.2.cmml"><mi id="Ch6.S4.p3.1.m1.1.1.2.2" xref="Ch6.S4.p3.1.m1.1.1.2.2.cmml">b</mi><mo id="Ch6.S4.p3.1.m1.1.1.2.1" xref="Ch6.S4.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.2.3" xref="Ch6.S4.p3.1.m1.1.1.2.3.cmml">a</mi><mo id="Ch6.S4.p3.1.m1.1.1.2.1a" xref="Ch6.S4.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.2.4" xref="Ch6.S4.p3.1.m1.1.1.2.4.cmml">t</mi><mo id="Ch6.S4.p3.1.m1.1.1.2.1b" xref="Ch6.S4.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.2.5" xref="Ch6.S4.p3.1.m1.1.1.2.5.cmml">c</mi><mo id="Ch6.S4.p3.1.m1.1.1.2.1c" xref="Ch6.S4.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.2.6" xref="Ch6.S4.p3.1.m1.1.1.2.6.cmml">h</mi></mrow><mo id="Ch6.S4.p3.1.m1.1.1.1" xref="Ch6.S4.p3.1.m1.1.1.1.cmml">=</mo><msub id="Ch6.S4.p3.1.m1.1.1.3" xref="Ch6.S4.p3.1.m1.1.1.3.cmml"><mi id="Ch6.S4.p3.1.m1.1.1.3.2" xref="Ch6.S4.p3.1.m1.1.1.3.2.cmml">N</mi><mrow id="Ch6.S4.p3.1.m1.1.1.3.3" xref="Ch6.S4.p3.1.m1.1.1.3.3.cmml"><mi id="Ch6.S4.p3.1.m1.1.1.3.3.2" xref="Ch6.S4.p3.1.m1.1.1.3.3.2.cmml">s</mi><mo id="Ch6.S4.p3.1.m1.1.1.3.3.1" xref="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.3.3.3" xref="Ch6.S4.p3.1.m1.1.1.3.3.3.cmml">l</mi><mo id="Ch6.S4.p3.1.m1.1.1.3.3.1a" xref="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.3.3.4" xref="Ch6.S4.p3.1.m1.1.1.3.3.4.cmml">i</mi><mo id="Ch6.S4.p3.1.m1.1.1.3.3.1b" xref="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.3.3.5" xref="Ch6.S4.p3.1.m1.1.1.3.3.5.cmml">c</mi><mo id="Ch6.S4.p3.1.m1.1.1.3.3.1c" xref="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.3.3.6" xref="Ch6.S4.p3.1.m1.1.1.3.3.6.cmml">e</mi><mo id="Ch6.S4.p3.1.m1.1.1.3.3.1d" xref="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch6.S4.p3.1.m1.1.1.3.3.7" xref="Ch6.S4.p3.1.m1.1.1.3.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p3.1.m1.1b"><apply id="Ch6.S4.p3.1.m1.1.1.cmml" xref="Ch6.S4.p3.1.m1.1.1"><eq id="Ch6.S4.p3.1.m1.1.1.1.cmml" xref="Ch6.S4.p3.1.m1.1.1.1"></eq><apply id="Ch6.S4.p3.1.m1.1.1.2.cmml" xref="Ch6.S4.p3.1.m1.1.1.2"><times id="Ch6.S4.p3.1.m1.1.1.2.1.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.1"></times><ci id="Ch6.S4.p3.1.m1.1.1.2.2.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.2">𝑏</ci><ci id="Ch6.S4.p3.1.m1.1.1.2.3.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.3">𝑎</ci><ci id="Ch6.S4.p3.1.m1.1.1.2.4.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.4">𝑡</ci><ci id="Ch6.S4.p3.1.m1.1.1.2.5.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.5">𝑐</ci><ci id="Ch6.S4.p3.1.m1.1.1.2.6.cmml" xref="Ch6.S4.p3.1.m1.1.1.2.6">ℎ</ci></apply><apply id="Ch6.S4.p3.1.m1.1.1.3.cmml" xref="Ch6.S4.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="Ch6.S4.p3.1.m1.1.1.3.1.cmml" xref="Ch6.S4.p3.1.m1.1.1.3">subscript</csymbol><ci id="Ch6.S4.p3.1.m1.1.1.3.2.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.2">𝑁</ci><apply id="Ch6.S4.p3.1.m1.1.1.3.3.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3"><times id="Ch6.S4.p3.1.m1.1.1.3.3.1.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.1"></times><ci id="Ch6.S4.p3.1.m1.1.1.3.3.2.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.2">𝑠</ci><ci id="Ch6.S4.p3.1.m1.1.1.3.3.3.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.3">𝑙</ci><ci id="Ch6.S4.p3.1.m1.1.1.3.3.4.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.4">𝑖</ci><ci id="Ch6.S4.p3.1.m1.1.1.3.3.5.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.5">𝑐</ci><ci id="Ch6.S4.p3.1.m1.1.1.3.3.6.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.6">𝑒</ci><ci id="Ch6.S4.p3.1.m1.1.1.3.3.7.cmml" xref="Ch6.S4.p3.1.m1.1.1.3.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p3.1.m1.1c">batch=N_{slices}</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p3.1.m1.1d">italic_b italic_a italic_t italic_c italic_h = italic_N start_POSTSUBSCRIPT italic_s italic_l italic_i italic_c italic_e italic_s end_POSTSUBSCRIPT</annotation></semantics></math>), the inference phase is made more resource-efficient. Preprocessing (slicing) and postprocessing (merging) within <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> remain unchanged.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p4">
<p class="ltx_p" id="Ch6.S4.p4.1">Splitting an image into slices, to form a batch, optimizes both memory usage and computational efficiency, especially on embedded systems with limited <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> memory. Performing object recognition on the high-resolution images may exceed <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> memory capacity if processed as a whole, leading to out-of-memory errors. By splitting the image, each slice can be processed independently within memory constraints. Batch processing these slices allows the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gpu" title="Graphics Processing Unit">GPU</span></a> to handle multiple slices concurrently, leveraging its parallel processing capabilities. This approach balances the need to manage memory effectively while maximizing computational throughput, making it ideal for resource-constrained environments like the NVIDIA Jetson AGX Xavier.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p5">
<p class="ltx_p" id="Ch6.S4.p5.1">Additionally, the slicing mechanism was used for model fine-tuning, with slices of the full-resolution ShipSG images as new training set. This allows the model to focus on improving the ability to segment small ships from full-resolution image slices of the dataset. By incorporating batch inference for inference and targeted fine-tuning during training, the optimized ScatYOLOv8+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> not only maintains quality for small ship segmentation but also boosts real-time performance on embedded systems.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p6">
<p class="ltx_p" id="Ch6.S4.p6.5">For the fine-tuning process, a new training dataset comprising 33648 slices from ShipSG images was created, each of <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch6.S4.p6.1.m1.1"><semantics id="Ch6.S4.p6.1.m1.1a"><mrow id="Ch6.S4.p6.1.m1.1.1" xref="Ch6.S4.p6.1.m1.1.1.cmml"><mn id="Ch6.S4.p6.1.m1.1.1.2" xref="Ch6.S4.p6.1.m1.1.1.2.cmml">640</mn><mo id="Ch6.S4.p6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S4.p6.1.m1.1.1.1.cmml">×</mo><mn id="Ch6.S4.p6.1.m1.1.1.3" xref="Ch6.S4.p6.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p6.1.m1.1b"><apply id="Ch6.S4.p6.1.m1.1.1.cmml" xref="Ch6.S4.p6.1.m1.1.1"><times id="Ch6.S4.p6.1.m1.1.1.1.cmml" xref="Ch6.S4.p6.1.m1.1.1.1"></times><cn id="Ch6.S4.p6.1.m1.1.1.2.cmml" type="integer" xref="Ch6.S4.p6.1.m1.1.1.2">640</cn><cn id="Ch6.S4.p6.1.m1.1.1.3.cmml" type="integer" xref="Ch6.S4.p6.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p6.1.m1.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p6.1.m1.1d">640 × 640</annotation></semantics></math> pixels and incorporating a <math alttext="20\%" class="ltx_Math" display="inline" id="Ch6.S4.p6.2.m2.1"><semantics id="Ch6.S4.p6.2.m2.1a"><mrow id="Ch6.S4.p6.2.m2.1.1" xref="Ch6.S4.p6.2.m2.1.1.cmml"><mn id="Ch6.S4.p6.2.m2.1.1.2" xref="Ch6.S4.p6.2.m2.1.1.2.cmml">20</mn><mo id="Ch6.S4.p6.2.m2.1.1.1" xref="Ch6.S4.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p6.2.m2.1b"><apply id="Ch6.S4.p6.2.m2.1.1.cmml" xref="Ch6.S4.p6.2.m2.1.1"><csymbol cd="latexml" id="Ch6.S4.p6.2.m2.1.1.1.cmml" xref="Ch6.S4.p6.2.m2.1.1.1">percent</csymbol><cn id="Ch6.S4.p6.2.m2.1.1.2.cmml" type="integer" xref="Ch6.S4.p6.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p6.2.m2.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p6.2.m2.1d">20 %</annotation></semantics></math> overlap between slices. This overlap guarantees that sufficient contextual information is retained for objects at the edges, yielding the most favorable experimental outcomes.
Per image of full-resolution in ShipSG (<math alttext="2028\times 1520" class="ltx_Math" display="inline" id="Ch6.S4.p6.3.m3.1"><semantics id="Ch6.S4.p6.3.m3.1a"><mrow id="Ch6.S4.p6.3.m3.1.1" xref="Ch6.S4.p6.3.m3.1.1.cmml"><mn id="Ch6.S4.p6.3.m3.1.1.2" xref="Ch6.S4.p6.3.m3.1.1.2.cmml">2028</mn><mo id="Ch6.S4.p6.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S4.p6.3.m3.1.1.1.cmml">×</mo><mn id="Ch6.S4.p6.3.m3.1.1.3" xref="Ch6.S4.p6.3.m3.1.1.3.cmml">1520</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p6.3.m3.1b"><apply id="Ch6.S4.p6.3.m3.1.1.cmml" xref="Ch6.S4.p6.3.m3.1.1"><times id="Ch6.S4.p6.3.m3.1.1.1.cmml" xref="Ch6.S4.p6.3.m3.1.1.1"></times><cn id="Ch6.S4.p6.3.m3.1.1.2.cmml" type="integer" xref="Ch6.S4.p6.3.m3.1.1.2">2028</cn><cn id="Ch6.S4.p6.3.m3.1.1.3.cmml" type="integer" xref="Ch6.S4.p6.3.m3.1.1.3">1520</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p6.3.m3.1c">2028\times 1520</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p6.3.m3.1d">2028 × 1520</annotation></semantics></math> pixels), the total number of slices of <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch6.S4.p6.4.m4.1"><semantics id="Ch6.S4.p6.4.m4.1a"><mrow id="Ch6.S4.p6.4.m4.1.1" xref="Ch6.S4.p6.4.m4.1.1.cmml"><mn id="Ch6.S4.p6.4.m4.1.1.2" xref="Ch6.S4.p6.4.m4.1.1.2.cmml">640</mn><mo id="Ch6.S4.p6.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S4.p6.4.m4.1.1.1.cmml">×</mo><mn id="Ch6.S4.p6.4.m4.1.1.3" xref="Ch6.S4.p6.4.m4.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p6.4.m4.1b"><apply id="Ch6.S4.p6.4.m4.1.1.cmml" xref="Ch6.S4.p6.4.m4.1.1"><times id="Ch6.S4.p6.4.m4.1.1.1.cmml" xref="Ch6.S4.p6.4.m4.1.1.1"></times><cn id="Ch6.S4.p6.4.m4.1.1.2.cmml" type="integer" xref="Ch6.S4.p6.4.m4.1.1.2">640</cn><cn id="Ch6.S4.p6.4.m4.1.1.3.cmml" type="integer" xref="Ch6.S4.p6.4.m4.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p6.4.m4.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p6.4.m4.1d">640 × 640</annotation></semantics></math> pixels is <math alttext="12" class="ltx_Math" display="inline" id="Ch6.S4.p6.5.m5.1"><semantics id="Ch6.S4.p6.5.m5.1a"><mn id="Ch6.S4.p6.5.m5.1.1" xref="Ch6.S4.p6.5.m5.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="Ch6.S4.p6.5.m5.1b"><cn id="Ch6.S4.p6.5.m5.1.1.cmml" type="integer" xref="Ch6.S4.p6.5.m5.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p6.5.m5.1c">12</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p6.5.m5.1d">12</annotation></semantics></math>.
This augmented dataset facilitated the training of the optimized ScatYOLOv8+CBAM model, initiating as pre-trained weights the models of Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S3" title="6.3 Optimized ScatYOLOv8+CBAM [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>, and extending with an extra 30 epochs of training. Analogously, this fine-tuning approach was applied as well to standard YOLOv8 models for comparative analysis.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p7">
<p class="ltx_p" id="Ch6.S4.p7.3">At the inference stage, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> preprocessing includes the real-time slicing of the image from which ships are being segmented.
This means that inference times when using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> encompass the total time from inputting an image to obtaining predictions, with all processing steps, using TensorRT-exported weights on the Jetson AGX Xavier.
For inference, the framework uses the same slicing parameters as during the fine-tune training stage, that is, <math alttext="12" class="ltx_Math" display="inline" id="Ch6.S4.p7.1.m1.1"><semantics id="Ch6.S4.p7.1.m1.1a"><mn id="Ch6.S4.p7.1.m1.1.1" xref="Ch6.S4.p7.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="Ch6.S4.p7.1.m1.1b"><cn id="Ch6.S4.p7.1.m1.1.1.cmml" type="integer" xref="Ch6.S4.p7.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p7.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p7.1.m1.1d">12</annotation></semantics></math> slices of <math alttext="640\times 640" class="ltx_Math" display="inline" id="Ch6.S4.p7.2.m2.1"><semantics id="Ch6.S4.p7.2.m2.1a"><mrow id="Ch6.S4.p7.2.m2.1.1" xref="Ch6.S4.p7.2.m2.1.1.cmml"><mn id="Ch6.S4.p7.2.m2.1.1.2" xref="Ch6.S4.p7.2.m2.1.1.2.cmml">640</mn><mo id="Ch6.S4.p7.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch6.S4.p7.2.m2.1.1.1.cmml">×</mo><mn id="Ch6.S4.p7.2.m2.1.1.3" xref="Ch6.S4.p7.2.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p7.2.m2.1b"><apply id="Ch6.S4.p7.2.m2.1.1.cmml" xref="Ch6.S4.p7.2.m2.1.1"><times id="Ch6.S4.p7.2.m2.1.1.1.cmml" xref="Ch6.S4.p7.2.m2.1.1.1"></times><cn id="Ch6.S4.p7.2.m2.1.1.2.cmml" type="integer" xref="Ch6.S4.p7.2.m2.1.1.2">640</cn><cn id="Ch6.S4.p7.2.m2.1.1.3.cmml" type="integer" xref="Ch6.S4.p7.2.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p7.2.m2.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p7.2.m2.1d">640 × 640</annotation></semantics></math> pixels per full-resolution image, with <math alttext="20\%" class="ltx_Math" display="inline" id="Ch6.S4.p7.3.m3.1"><semantics id="Ch6.S4.p7.3.m3.1a"><mrow id="Ch6.S4.p7.3.m3.1.1" xref="Ch6.S4.p7.3.m3.1.1.cmml"><mn id="Ch6.S4.p7.3.m3.1.1.2" xref="Ch6.S4.p7.3.m3.1.1.2.cmml">20</mn><mo id="Ch6.S4.p7.3.m3.1.1.1" xref="Ch6.S4.p7.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p7.3.m3.1b"><apply id="Ch6.S4.p7.3.m3.1.1.cmml" xref="Ch6.S4.p7.3.m3.1.1"><csymbol cd="latexml" id="Ch6.S4.p7.3.m3.1.1.1.cmml" xref="Ch6.S4.p7.3.m3.1.1.1">percent</csymbol><cn id="Ch6.S4.p7.3.m3.1.1.2.cmml" type="integer" xref="Ch6.S4.p7.3.m3.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p7.3.m3.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p7.3.m3.1d">20 %</annotation></semantics></math> overlap.</p>
</div>
<figure class="ltx_figure" id="Ch6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="578" id="Ch6.F10.g1" src="extracted/5906916/fig/small_example_vertical.jpg" width="295"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F10.4.3.1" style="font-size:90%;">Figure 6.10</span>: </span><span class="ltx_text" id="Ch6.F10.2.2" style="font-size:90%;">Small ship segmentation on ShipSG. (a) Original ShipSG image, with a small ship zoomed in for visualization. (b) Inference using the optimized ScatYOLOv8<math alttext="x" class="ltx_Math" display="inline" id="Ch6.F10.1.1.m1.1"><semantics id="Ch6.F10.1.1.m1.1c"><mi id="Ch6.F10.1.1.m1.1.1" xref="Ch6.F10.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.F10.1.1.m1.1d"><ci id="Ch6.F10.1.1.m1.1.1.cmml" xref="Ch6.F10.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F10.1.1.m1.1e">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.F10.1.1.m1.1f">italic_x</annotation></semantics></math>+CBAM, where the small ship has been undetected. (c) Inference using the proposed optimized ScatYOLOv8<math alttext="x" class="ltx_Math" display="inline" id="Ch6.F10.2.2.m2.1"><semantics id="Ch6.F10.2.2.m2.1c"><mi id="Ch6.F10.2.2.m2.1.1" xref="Ch6.F10.2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.F10.2.2.m2.1d"><ci id="Ch6.F10.2.2.m2.1.1.cmml" xref="Ch6.F10.2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.F10.2.2.m2.1e">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.F10.2.2.m2.1f">italic_x</annotation></semantics></math>+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, where the small ship appears segmented, at a high confidence score. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>. ©2024 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S4.p8">
<p class="ltx_p" id="Ch6.S4.p8.1">We can see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F10" title="Figure 6.10 ‣ 6.4 Enhanced Small Ship Segmentation Using Higher Resolution Images [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.10</span></a> the effectiveness of the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> approach, illustrating how a small ship, undetected without <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, is accurately segmented, highlighting its significance in boosting maritime situational awareness. Furthermore, the enhanced confidence scores for larger ships in the same figure underscore the robustness of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>.</p>
</div>
<figure class="ltx_table" id="Ch6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch6.T3.2.1.1" style="font-size:90%;">Table 6.3</span>: </span><span class="ltx_text" id="Ch6.T3.3.2" style="font-size:90%;">Comparison of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> scores for small objects with all model sizes using standard YOLOv8, our proposed optimized ScatYOLOv8+CBAM, and the addition of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>. ©2024 IEEE.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch6.T3.4">
<tr class="ltx_tr" id="Ch6.T3.4.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T3.4.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center" colspan="5" id="Ch6.T3.4.1.2"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.1.2.1">mAP small objects (%)</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.4.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.2.1"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.2.1.1">n</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.2.2"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.2.2.1">s</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.2.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.2.3.1">m</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.2.4"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.2.4.1">l</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.2.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.2.5.1">x</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T3.4.3.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>YOLOv8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.3.2">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.3.3">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.3.4">41.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.3.5">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.3.6">43.4</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T3.4.4.1">Opt. ScatYOLOv8+CBAM</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.4.2">45.8</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.4.3">47.1</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.4.4">47.2</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.4.5">47.9</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.4.6">48.0</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.4.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Ch6.T3.4.5.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>YOLOv8 &amp; <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.5.2">53.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.5.3">54.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.5.4">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.5.5">55.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.4.5.6">55.7</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.4.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="Ch6.T3.4.6.1">Opt. ScatYOLOv8+CBAM &amp; <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>
</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.6.2"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.6.2.1">54.7</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.6.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.6.3.1">55.6</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.6.4"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.6.4.1">56.7</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.6.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.6.5.1">57.9</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.4.6.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.4.6.6.1">58.9</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="Ch6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="316" id="Ch6.F11.g1" src="extracted/5906916/fig/sahi_comparison.png" width="452"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch6.F11.2.1.1" style="font-size:90%;">Figure 6.11</span>: </span><span class="ltx_text" id="Ch6.F11.3.2" style="font-size:90%;">mAP vs TensorRT inference times on NVIDIA Jetson AGX Xavier.
ScatYOLOv8+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> provides the best accuracy for ship segmentation. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>. ©2024 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch6.S4.p9">
<p class="ltx_p" id="Ch6.S4.p9.5">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T3" title="Table 6.3 ‣ 6.4 Enhanced Small Ship Segmentation Using Higher Resolution Images [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a> presents the results of the analysis on small ship segmentation across different model sizes.
The optimized ScatYOLOv8+CBAM outperforms the standard YOLOv8 for small objects across all sizes.
<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> significantly boosts performance for both standard YOLOv8 and the optimized ScatYOLOv8+CBAM, showing gains between <math alttext="8\%" class="ltx_Math" display="inline" id="Ch6.S4.p9.1.m1.1"><semantics id="Ch6.S4.p9.1.m1.1a"><mrow id="Ch6.S4.p9.1.m1.1.1" xref="Ch6.S4.p9.1.m1.1.1.cmml"><mn id="Ch6.S4.p9.1.m1.1.1.2" xref="Ch6.S4.p9.1.m1.1.1.2.cmml">8</mn><mo id="Ch6.S4.p9.1.m1.1.1.1" xref="Ch6.S4.p9.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p9.1.m1.1b"><apply id="Ch6.S4.p9.1.m1.1.1.cmml" xref="Ch6.S4.p9.1.m1.1.1"><csymbol cd="latexml" id="Ch6.S4.p9.1.m1.1.1.1.cmml" xref="Ch6.S4.p9.1.m1.1.1.1">percent</csymbol><cn id="Ch6.S4.p9.1.m1.1.1.2.cmml" type="integer" xref="Ch6.S4.p9.1.m1.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p9.1.m1.1c">8\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p9.1.m1.1d">8 %</annotation></semantics></math> and <math alttext="11\%" class="ltx_Math" display="inline" id="Ch6.S4.p9.2.m2.1"><semantics id="Ch6.S4.p9.2.m2.1a"><mrow id="Ch6.S4.p9.2.m2.1.1" xref="Ch6.S4.p9.2.m2.1.1.cmml"><mn id="Ch6.S4.p9.2.m2.1.1.2" xref="Ch6.S4.p9.2.m2.1.1.2.cmml">11</mn><mo id="Ch6.S4.p9.2.m2.1.1.1" xref="Ch6.S4.p9.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p9.2.m2.1b"><apply id="Ch6.S4.p9.2.m2.1.1.cmml" xref="Ch6.S4.p9.2.m2.1.1"><csymbol cd="latexml" id="Ch6.S4.p9.2.m2.1.1.1.cmml" xref="Ch6.S4.p9.2.m2.1.1.1">percent</csymbol><cn id="Ch6.S4.p9.2.m2.1.1.2.cmml" type="integer" xref="Ch6.S4.p9.2.m2.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p9.2.m2.1c">11\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p9.2.m2.1d">11 %</annotation></semantics></math> over configurations without <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>. Specifically, the optimized ScatYOLOv8+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> achieves the highest mAP improvements for small objects, ranging from <math alttext="1.4\%" class="ltx_Math" display="inline" id="Ch6.S4.p9.3.m3.1"><semantics id="Ch6.S4.p9.3.m3.1a"><mrow id="Ch6.S4.p9.3.m3.1.1" xref="Ch6.S4.p9.3.m3.1.1.cmml"><mn id="Ch6.S4.p9.3.m3.1.1.2" xref="Ch6.S4.p9.3.m3.1.1.2.cmml">1.4</mn><mo id="Ch6.S4.p9.3.m3.1.1.1" xref="Ch6.S4.p9.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p9.3.m3.1b"><apply id="Ch6.S4.p9.3.m3.1.1.cmml" xref="Ch6.S4.p9.3.m3.1.1"><csymbol cd="latexml" id="Ch6.S4.p9.3.m3.1.1.1.cmml" xref="Ch6.S4.p9.3.m3.1.1.1">percent</csymbol><cn id="Ch6.S4.p9.3.m3.1.1.2.cmml" type="float" xref="Ch6.S4.p9.3.m3.1.1.2">1.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p9.3.m3.1c">1.4\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p9.3.m3.1d">1.4 %</annotation></semantics></math> to <math alttext="3.2\%" class="ltx_Math" display="inline" id="Ch6.S4.p9.4.m4.1"><semantics id="Ch6.S4.p9.4.m4.1a"><mrow id="Ch6.S4.p9.4.m4.1.1" xref="Ch6.S4.p9.4.m4.1.1.cmml"><mn id="Ch6.S4.p9.4.m4.1.1.2" xref="Ch6.S4.p9.4.m4.1.1.2.cmml">3.2</mn><mo id="Ch6.S4.p9.4.m4.1.1.1" xref="Ch6.S4.p9.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Ch6.S4.p9.4.m4.1b"><apply id="Ch6.S4.p9.4.m4.1.1.cmml" xref="Ch6.S4.p9.4.m4.1.1"><csymbol cd="latexml" id="Ch6.S4.p9.4.m4.1.1.1.cmml" xref="Ch6.S4.p9.4.m4.1.1.1">percent</csymbol><cn id="Ch6.S4.p9.4.m4.1.1.2.cmml" type="float" xref="Ch6.S4.p9.4.m4.1.1.2">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p9.4.m4.1c">3.2\%</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p9.4.m4.1d">3.2 %</annotation></semantics></math> over standard YOLOv8 equipped with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>. Moreover, the advantage of integrating our optimized model with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> becomes more pronounced with model depth, highlighting a scalable improvement in small ship segmentation with increased network complexity.
When comparing to the small ship segmentation performance provided in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.T2" title="Table 5.2 ‣ 5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>, the optimized ScatYOLOv8+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> in model size <math alttext="s" class="ltx_Math" display="inline" id="Ch6.S4.p9.5.m5.1"><semantics id="Ch6.S4.p9.5.m5.1a"><mi id="Ch6.S4.p9.5.m5.1.1" xref="Ch6.S4.p9.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch6.S4.p9.5.m5.1b"><ci id="Ch6.S4.p9.5.m5.1.1.cmml" xref="Ch6.S4.p9.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p9.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p9.5.m5.1d">italic_s</annotation></semantics></math> onwards performs as good or better than the best one from the initial study, DetectoRS. It is important to recall that DetectoRS was not compatible with embedded system deployment, which underlines another superiority of the customized architecture.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p10">
<p class="ltx_p" id="Ch6.S4.p10.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.F11" title="Figure 6.11 ‣ 6.4 Enhanced Small Ship Segmentation Using Higher Resolution Images [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.11</span></a> compares the overall mAP (all mask sizes) of models using batch-processed <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> against those without it on the Jetson AGX Xavier.
The ScatYOLOv8+CBAM model with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> outperforms all standard YOLOv8 models in ship segmentation accuracy.
The ScatYOLOv8+CBAM <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S4.p10.1.m1.1"><semantics id="Ch6.S4.p10.1.m1.1a"><mi id="Ch6.S4.p10.1.m1.1.1" xref="Ch6.S4.p10.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S4.p10.1.m1.1b"><ci id="Ch6.S4.p10.1.m1.1.1.cmml" xref="Ch6.S4.p10.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p10.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p10.1.m1.1d">italic_n</annotation></semantics></math> with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> not only achieves higher mAP than the largest YOLOv8<math alttext="x" class="ltx_Math" display="inline" id="Ch6.S4.p10.2.m2.1"><semantics id="Ch6.S4.p10.2.m2.1a"><mi id="Ch6.S4.p10.2.m2.1.1" xref="Ch6.S4.p10.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S4.p10.2.m2.1b"><ci id="Ch6.S4.p10.2.m2.1.1.cmml" xref="Ch6.S4.p10.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p10.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p10.2.m2.1d">italic_x</annotation></semantics></math> with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> but also performs substantially faster, sparing resources of the embedded system even on full-resolution images.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p11">
<p class="ltx_p" id="Ch6.S4.p11.3">Compared to its own non-<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> version, the optimized ScatYOLOv8+CBAM with <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> in <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S4.p11.1.m1.1"><semantics id="Ch6.S4.p11.1.m1.1a"><mi id="Ch6.S4.p11.1.m1.1.1" xref="Ch6.S4.p11.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S4.p11.1.m1.1b"><ci id="Ch6.S4.p11.1.m1.1.1.cmml" xref="Ch6.S4.p11.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p11.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p11.1.m1.1d">italic_n</annotation></semantics></math> size shows equivalent mAP to a the <math alttext="m" class="ltx_Math" display="inline" id="Ch6.S4.p11.2.m2.1"><semantics id="Ch6.S4.p11.2.m2.1a"><mi id="Ch6.S4.p11.2.m2.1.1" xref="Ch6.S4.p11.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch6.S4.p11.2.m2.1b"><ci id="Ch6.S4.p11.2.m2.1.1.cmml" xref="Ch6.S4.p11.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p11.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p11.2.m2.1d">italic_m</annotation></semantics></math> model without <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, albeit with increased computation time of <math alttext="\sim" class="ltx_Math" display="inline" id="Ch6.S4.p11.3.m3.1"><semantics id="Ch6.S4.p11.3.m3.1a"><mo id="Ch6.S4.p11.3.m3.1.1" xref="Ch6.S4.p11.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Ch6.S4.p11.3.m3.1b"><csymbol cd="latexml" id="Ch6.S4.p11.3.m3.1.1.cmml" xref="Ch6.S4.p11.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S4.p11.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="Ch6.S4.p11.3.m3.1d">∼</annotation></semantics></math>170 ms, significantly enhancing small ship segmentation by 7.5%, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.T3" title="Table 6.3 ‣ 6.4 Enhanced Small Ship Segmentation Using Higher Resolution Images [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>.
This shows that the benefits of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> in densely populated maritime areas where monitoring small, distant ships in real-time is crucial for safety and security, carry considerable associated computational costs.</p>
</div>
<div class="ltx_para" id="Ch6.S4.p12">
<p class="ltx_p" id="Ch6.S4.p12.1">During the tests, it was measured that 25% of the processing time during <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> batch-inference goes to slicing and merging the masks form the slices, suggesting room for optimization.</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S5">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">6.5   Summary and Discussion</h3>
<div class="ltx_para" id="Ch6.S5.p1">
<p class="ltx_p" id="Ch6.S5.p1.2">This chapter presented an advancement in the field of real-time ship segmentation with the design of ScatYOLOv8+CBAM. In lightest version <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S5.p1.1.m1.1"><semantics id="Ch6.S5.p1.1.m1.1a"><mi id="Ch6.S5.p1.1.m1.1.1" xref="Ch6.S5.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p1.1.m1.1b"><ci id="Ch6.S5.p1.1.m1.1.1.cmml" xref="Ch6.S5.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p1.1.m1.1d">italic_n</annotation></semantics></math>, the architecture demonstrated its capability on ShipSG with a <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> of 75.46%, 5% higher than baseline, comparable to the performance of larger baseline models but at half inference speed per frame (59.3 ms).
We analyzed an optimization of the architecture, that removes the upsampling and downsampling from the ScatBlock to save computing time, and deployed it with TensoRT on the Jetson AGX Xavier to measure inference times for real-time applicability. The optimized ScatYOLOv8+CBAM in model size <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S5.p1.2.m2.1"><semantics id="Ch6.S5.p1.2.m2.1a"><mi id="Ch6.S5.p1.2.m2.1.1" xref="Ch6.S5.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p1.2.m2.1b"><ci id="Ch6.S5.p1.2.m2.1.1.cmml" xref="Ch6.S5.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p1.2.m2.1d">italic_n</annotation></semantics></math> performs 36.5% faster than its predecessor, achieving 25.3 ms per frame.
Finally, I proposed a batch-processed <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> to increase the segmentation of small and distant ships that is able to run within embedded system resources. Though bringing along increased computation, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> for small ships increased in ranges from 8% to 11% in comparison with the baseline without <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>.
The work presented bridges the transition from standard methods to real-time instance segmentation on embedded systems, and addresses the ability to accurately identify all ships, independent from their size, and within the vicinity of the port area.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p2">
<p class="ltx_p" id="Ch6.S5.p2.1">Additionally, the chapter has uncovered various areas for enhancement. Further research could focus on improving ScatYOLOv8+CBAM and advancing the performance of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>.
As seen in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S3" title="6.3 Optimized ScatYOLOv8+CBAM [BCP-VI] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.3</span></a>, shallower models show notable improvements in both accuracy and speed over the standard ship segmentation models explored with ShipSG.
However, larger models become slower. This indicates that larger models face increased computational demands when handling scattering coefficients, hinting at potential strategies for refinement to be explored.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p3">
<p class="ltx_p" id="Ch6.S5.p3.1">One possible strategy is making the scattering transform learnable. This would involve introducing adjustable parameters within the wavelet filters (parametrization) by merging deep learning adaptability with theoretical wavelet analysis. However, this is a challenging task due to the complicated balance between maintaining the translation and deformation invariant properties of the transform while allowing for sufficient flexibility to learn from data.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p4">
<p class="ltx_p" id="Ch6.S5.p4.1">The second possible strategy is enhancing the transform with learnable downsampling and attention mechanisms for a more practical approach for real-time use. For example, learnable downsampling, such as strided convolutions with batch normalization, can compress scattering coefficients while learning to preserve key information.
On the other hand, attention mechanisms focus computational resources on significant features, improving processing efficiency. An example is the integration of transformers, that are based on attention and could enable models to combine the invariant feature extraction of the scattering transform with the transformer contextual learning, offering a focus on the most relevant features for the segmentation.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p5">
<p class="ltx_p" id="Ch6.S5.p5.1">Moreover, it was measured during the experiments that 25% of the processing time during <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> batch-inference is dedicated to the slicing (preprocessing) and merging of the masks after segmentation (postprocessing). In the pursuit of enhancing <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, by using parallel processing, the slicing and merging tasks could be improved. An example is the division of the workload with multi-threading or a pool of processes to enable concurrent execution of slicing and merging, optimizing computational resources and improving efficiency.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p6">
<p class="ltx_p" id="Ch6.S5.p6.7">Given the slowdown in larger models, is pertinent to address how potential practitioners and users of ScatYOLOv8+CBAM should select the size of the architecture (from <math alttext="n" class="ltx_Math" display="inline" id="Ch6.S5.p6.1.m1.1"><semantics id="Ch6.S5.p6.1.m1.1a"><mi id="Ch6.S5.p6.1.m1.1.1" xref="Ch6.S5.p6.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.1.m1.1b"><ci id="Ch6.S5.p6.1.m1.1.1.cmml" xref="Ch6.S5.p6.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.1.m1.1d">italic_n</annotation></semantics></math> to <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S5.p6.2.m2.1"><semantics id="Ch6.S5.p6.2.m2.1a"><mi id="Ch6.S5.p6.2.m2.1.1" xref="Ch6.S5.p6.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.2.m2.1b"><ci id="Ch6.S5.p6.2.m2.1.1.cmml" xref="Ch6.S5.p6.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.2.m2.1d">italic_x</annotation></semantics></math>), and the use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, in their specific application.
The decision should consider the critical balance between real-time processing demands and computational limitations. The optimized ScatYOLOv8+CBAM, with its <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> adaptation for small and distant ships, serves as a guide for such selections. Users should weigh the operational complexity, the necessity for rapid data processing, and accuracy requirements against the computational resources available. By aligning architecture choices with these considerations, practitioners and users can ensure the an effective deployment tailored to their unique application needs.
For instance, the optimized ScatYOLOv8+CBAM in smaller configurations (<math alttext="n" class="ltx_Math" display="inline" id="Ch6.S5.p6.3.m3.1"><semantics id="Ch6.S5.p6.3.m3.1a"><mi id="Ch6.S5.p6.3.m3.1.1" xref="Ch6.S5.p6.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.3.m3.1b"><ci id="Ch6.S5.p6.3.m3.1.1.cmml" xref="Ch6.S5.p6.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.3.m3.1d">italic_n</annotation></semantics></math> and <math alttext="s" class="ltx_Math" display="inline" id="Ch6.S5.p6.4.m4.1"><semantics id="Ch6.S5.p6.4.m4.1a"><mi id="Ch6.S5.p6.4.m4.1.1" xref="Ch6.S5.p6.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.4.m4.1b"><ci id="Ch6.S5.p6.4.m4.1.1.cmml" xref="Ch6.S5.p6.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.4.m4.1d">italic_s</annotation></semantics></math>) is ideal for scenarios demanding rapid response with considerable accuracy, such as real-time port surveillance or navigation aid systems.
Additionally, though optional, the use of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>, could enhance access to small and distant ships approach the maritime infrastructure, when the application requires higher levels of responsiveness.
Conversely, in scenarios where computational resources are less constrained, larger configurations (<math alttext="m" class="ltx_Math" display="inline" id="Ch6.S5.p6.5.m5.1"><semantics id="Ch6.S5.p6.5.m5.1a"><mi id="Ch6.S5.p6.5.m5.1.1" xref="Ch6.S5.p6.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.5.m5.1b"><ci id="Ch6.S5.p6.5.m5.1.1.cmml" xref="Ch6.S5.p6.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.5.m5.1d">italic_m</annotation></semantics></math>, <math alttext="l" class="ltx_Math" display="inline" id="Ch6.S5.p6.6.m6.1"><semantics id="Ch6.S5.p6.6.m6.1a"><mi id="Ch6.S5.p6.6.m6.1.1" xref="Ch6.S5.p6.6.m6.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.6.m6.1b"><ci id="Ch6.S5.p6.6.m6.1.1.cmml" xref="Ch6.S5.p6.6.m6.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.6.m6.1c">l</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.6.m6.1d">italic_l</annotation></semantics></math> and <math alttext="x" class="ltx_Math" display="inline" id="Ch6.S5.p6.7.m7.1"><semantics id="Ch6.S5.p6.7.m7.1a"><mi id="Ch6.S5.p6.7.m7.1.1" xref="Ch6.S5.p6.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch6.S5.p6.7.m7.1b"><ci id="Ch6.S5.p6.7.m7.1.1.cmml" xref="Ch6.S5.p6.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S5.p6.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch6.S5.p6.7.m7.1d">italic_x</annotation></semantics></math>) could be leveraged for enhanced precision. This choice underscores the necessity for a strategic approach in deploying these technologies, where understanding the specific maritime application context and corresponding computational trade-offs guides the optimal use of the architecture.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p7">
<p class="ltx_p" id="Ch6.S5.p7.1">Integrating the methodologies presented in this chapter with other processing chains and sensors can also further enhance maritime situational awareness.
This could include displaying real-time georeferenced ships on maps through web services, merging various data sources such as ship tracking, 3D reconstruction and anomaly detection or information from other sensors as thermal imaging or radar.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p8">
<p class="ltx_p" id="Ch6.S5.p8.1">In conclusion, the results presented in this Chapter establish a new standard for maritime monitoring on embedded systems and create a foundation for future work aimed at enhancing real-time, high-resolution processing within the limits of resource-restricted settings.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch7" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 7 </span>Ship Georeferencing for Maritime Situational Awareness</h2>
<div class="ltx_para" id="Ch7.p1">
<p class="ltx_p" id="Ch7.p1.1">In Chapters <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>, we explored the creation of a custom maritime dataset, the impact of ship recognition on maritime applications, and the advancements and optimizations for real-time processing on embedded systems with a customized architecture, even with higher resolutions to enhance detail in small ship recognition.
Building on these foundations, this chapter studies the georeferencing of recognized ships using monocular images.
The aim is the implementation of a method that provides meaningful information from the recognized ships to the situational awareness picture for a better semantic understanding of the maritime situation. This process involves the development of methods for the representation of the recognized ship on a global scale using single images and without prior knowledge of the camera.
We call this ship georeferencing.</p>
</div>
<div class="ltx_para" id="Ch7.p2">
<p class="ltx_p" id="Ch7.p2.1">First, we delve into understanding the concept of homographies for georeferencing, which serves as fundamental for the ship georeferencing techniques proposed.</p>
</div>
<div class="ltx_para" id="Ch7.p3">
<p class="ltx_p" id="Ch7.p3.1">Then, this chapter presents my proposed ship bounding box georeferencing method and calculation of ship heading direction from optical flow, which form a significant part of my contributions to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, in addition to what has been presented in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.p4">
<p class="ltx_p" id="Ch7.p4.1">Moreover, I expand upon the georeferencing methodology and show an in depth quantitative studies of the use of homographies for ship georeferencing using ShipSG and the results from Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>, based on <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, respectively.</p>
</div>
<section class="ltx_section" id="Ch7.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">7.1   Homographies for Image Georeferencing <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>
</h3>
<div class="ltx_para" id="Ch7.S1.p1">
<p class="ltx_p" id="Ch7.S1.p1.1">Homography <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib120" title="">hartley2003multiple </a></cite> is a mathematical concept widely used in the fields of computer vision and image processing to describe the transformation of points between two planes (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F1" title="Figure 7.1 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.1</span></a>).
This transformation, encapsulated by the homography matrix, allows for the conversion of coordinates between these two planes, highlighting the essence of homography in linking different spatial perspectives.</p>
</div>
<figure class="ltx_figure" id="Ch7.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="276" id="Ch7.F1.g1" src="extracted/5906916/fig/homography.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F1.9.8.1" style="font-size:90%;">Figure 7.1</span>: </span><span class="ltx_text" id="Ch7.F1.7.7" style="font-size:90%;">Homography between two planes. Plane <math alttext="P" class="ltx_Math" display="inline" id="Ch7.F1.1.1.m1.1"><semantics id="Ch7.F1.1.1.m1.1c"><mi id="Ch7.F1.1.1.m1.1.1" xref="Ch7.F1.1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Ch7.F1.1.1.m1.1d"><ci id="Ch7.F1.1.1.m1.1.1.cmml" xref="Ch7.F1.1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.1.1.m1.1e">P</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.1.1.m1.1f">italic_P</annotation></semantics></math> and <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="Ch7.F1.2.2.m2.1"><semantics id="Ch7.F1.2.2.m2.1c"><msup id="Ch7.F1.2.2.m2.1.1" xref="Ch7.F1.2.2.m2.1.1.cmml"><mi id="Ch7.F1.2.2.m2.1.1.2" xref="Ch7.F1.2.2.m2.1.1.2.cmml">P</mi><mo id="Ch7.F1.2.2.m2.1.1.3" xref="Ch7.F1.2.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Ch7.F1.2.2.m2.1d"><apply id="Ch7.F1.2.2.m2.1.1.cmml" xref="Ch7.F1.2.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.F1.2.2.m2.1.1.1.cmml" xref="Ch7.F1.2.2.m2.1.1">superscript</csymbol><ci id="Ch7.F1.2.2.m2.1.1.2.cmml" xref="Ch7.F1.2.2.m2.1.1.2">𝑃</ci><ci id="Ch7.F1.2.2.m2.1.1.3.cmml" xref="Ch7.F1.2.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.2.2.m2.1e">P^{\prime}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.2.2.m2.1f">italic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> represent two different surfaces or image planes. The point <math alttext="p" class="ltx_Math" display="inline" id="Ch7.F1.3.3.m3.1"><semantics id="Ch7.F1.3.3.m3.1c"><mi id="Ch7.F1.3.3.m3.1.1" xref="Ch7.F1.3.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Ch7.F1.3.3.m3.1d"><ci id="Ch7.F1.3.3.m3.1.1.cmml" xref="Ch7.F1.3.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.3.3.m3.1e">p</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.3.3.m3.1f">italic_p</annotation></semantics></math> on <math alttext="P" class="ltx_Math" display="inline" id="Ch7.F1.4.4.m4.1"><semantics id="Ch7.F1.4.4.m4.1c"><mi id="Ch7.F1.4.4.m4.1.1" xref="Ch7.F1.4.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Ch7.F1.4.4.m4.1d"><ci id="Ch7.F1.4.4.m4.1.1.cmml" xref="Ch7.F1.4.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.4.4.m4.1e">P</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.4.4.m4.1f">italic_P</annotation></semantics></math> and <math alttext="p^{\prime}" class="ltx_Math" display="inline" id="Ch7.F1.5.5.m5.1"><semantics id="Ch7.F1.5.5.m5.1c"><msup id="Ch7.F1.5.5.m5.1.1" xref="Ch7.F1.5.5.m5.1.1.cmml"><mi id="Ch7.F1.5.5.m5.1.1.2" xref="Ch7.F1.5.5.m5.1.1.2.cmml">p</mi><mo id="Ch7.F1.5.5.m5.1.1.3" xref="Ch7.F1.5.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Ch7.F1.5.5.m5.1d"><apply id="Ch7.F1.5.5.m5.1.1.cmml" xref="Ch7.F1.5.5.m5.1.1"><csymbol cd="ambiguous" id="Ch7.F1.5.5.m5.1.1.1.cmml" xref="Ch7.F1.5.5.m5.1.1">superscript</csymbol><ci id="Ch7.F1.5.5.m5.1.1.2.cmml" xref="Ch7.F1.5.5.m5.1.1.2">𝑝</ci><ci id="Ch7.F1.5.5.m5.1.1.3.cmml" xref="Ch7.F1.5.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.5.5.m5.1e">p^{\prime}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.5.5.m5.1f">italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> on <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="Ch7.F1.6.6.m6.1"><semantics id="Ch7.F1.6.6.m6.1c"><msup id="Ch7.F1.6.6.m6.1.1" xref="Ch7.F1.6.6.m6.1.1.cmml"><mi id="Ch7.F1.6.6.m6.1.1.2" xref="Ch7.F1.6.6.m6.1.1.2.cmml">P</mi><mo id="Ch7.F1.6.6.m6.1.1.3" xref="Ch7.F1.6.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Ch7.F1.6.6.m6.1d"><apply id="Ch7.F1.6.6.m6.1.1.cmml" xref="Ch7.F1.6.6.m6.1.1"><csymbol cd="ambiguous" id="Ch7.F1.6.6.m6.1.1.1.cmml" xref="Ch7.F1.6.6.m6.1.1">superscript</csymbol><ci id="Ch7.F1.6.6.m6.1.1.2.cmml" xref="Ch7.F1.6.6.m6.1.1.2">𝑃</ci><ci id="Ch7.F1.6.6.m6.1.1.3.cmml" xref="Ch7.F1.6.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.F1.6.6.m6.1e">P^{\prime}</annotation><annotation encoding="application/x-llamapun" id="Ch7.F1.6.6.m6.1f">italic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> illustrate a pair of points that are related by a homography (<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref ltx_markedasmath ltx_font_italic" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">Homography(H)</span></a>), showcasing how a point in one plane can be mapped to another plane through a projective transformation (dashed arrow).</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S1.p2">
<p class="ltx_p" id="Ch7.S1.p2.1">Essentially, a homography is represented by a <math alttext="3\times 3" class="ltx_Math" display="inline" id="Ch7.S1.p2.1.m1.1"><semantics id="Ch7.S1.p2.1.m1.1a"><mrow id="Ch7.S1.p2.1.m1.1.1" xref="Ch7.S1.p2.1.m1.1.1.cmml"><mn id="Ch7.S1.p2.1.m1.1.1.2" xref="Ch7.S1.p2.1.m1.1.1.2.cmml">3</mn><mo id="Ch7.S1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch7.S1.p2.1.m1.1.1.1.cmml">×</mo><mn id="Ch7.S1.p2.1.m1.1.1.3" xref="Ch7.S1.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p2.1.m1.1b"><apply id="Ch7.S1.p2.1.m1.1.1.cmml" xref="Ch7.S1.p2.1.m1.1.1"><times id="Ch7.S1.p2.1.m1.1.1.1.cmml" xref="Ch7.S1.p2.1.m1.1.1.1"></times><cn id="Ch7.S1.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch7.S1.p2.1.m1.1.1.2">3</cn><cn id="Ch7.S1.p2.1.m1.1.1.3.cmml" type="integer" xref="Ch7.S1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p2.1.m1.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p2.1.m1.1d">3 × 3</annotation></semantics></math> matrix:</p>
</div>
<div class="ltx_para" id="Ch7.S1.p3">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H=\begin{bmatrix}h_{11}&amp;h_{12}&amp;h_{13}\\
h_{21}&amp;h_{22}&amp;h_{23}\\
h_{31}&amp;h_{32}&amp;1\\
\end{bmatrix}" class="ltx_Math" display="block" id="Ch7.E1.m1.1"><semantics id="Ch7.E1.m1.1a"><mrow id="Ch7.E1.m1.1.2" xref="Ch7.E1.m1.1.2.cmml"><mi id="Ch7.E1.m1.1.2.2" xref="Ch7.E1.m1.1.2.2.cmml">H</mi><mo id="Ch7.E1.m1.1.2.1" xref="Ch7.E1.m1.1.2.1.cmml">=</mo><mrow id="Ch7.E1.m1.1.1.3" xref="Ch7.E1.m1.1.1.2.cmml"><mo id="Ch7.E1.m1.1.1.3.1" xref="Ch7.E1.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" id="Ch7.E1.m1.1.1.1.1" rowspacing="0pt" xref="Ch7.E1.m1.1.1.1.1.cmml"><mtr id="Ch7.E1.m1.1.1.1.1a" xref="Ch7.E1.m1.1.1.1.1.cmml"><mtd id="Ch7.E1.m1.1.1.1.1b" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.1.1.1" xref="Ch7.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.1.1.1.2" xref="Ch7.E1.m1.1.1.1.1.1.1.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.1.1.1.3" xref="Ch7.E1.m1.1.1.1.1.1.1.1.3.cmml">11</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1c" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.1.2.1" xref="Ch7.E1.m1.1.1.1.1.1.2.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.1.2.1.2" xref="Ch7.E1.m1.1.1.1.1.1.2.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.1.2.1.3" xref="Ch7.E1.m1.1.1.1.1.1.2.1.3.cmml">12</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1d" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.1.3.1" xref="Ch7.E1.m1.1.1.1.1.1.3.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.1.3.1.2" xref="Ch7.E1.m1.1.1.1.1.1.3.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.1.3.1.3" xref="Ch7.E1.m1.1.1.1.1.1.3.1.3.cmml">13</mn></msub></mtd></mtr><mtr id="Ch7.E1.m1.1.1.1.1e" xref="Ch7.E1.m1.1.1.1.1.cmml"><mtd id="Ch7.E1.m1.1.1.1.1f" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.2.1.1" xref="Ch7.E1.m1.1.1.1.1.2.1.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.2.1.1.2" xref="Ch7.E1.m1.1.1.1.1.2.1.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.2.1.1.3" xref="Ch7.E1.m1.1.1.1.1.2.1.1.3.cmml">21</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1g" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.2.2.1" xref="Ch7.E1.m1.1.1.1.1.2.2.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.2.2.1.2" xref="Ch7.E1.m1.1.1.1.1.2.2.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.2.2.1.3" xref="Ch7.E1.m1.1.1.1.1.2.2.1.3.cmml">22</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1h" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.2.3.1" xref="Ch7.E1.m1.1.1.1.1.2.3.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.2.3.1.2" xref="Ch7.E1.m1.1.1.1.1.2.3.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.2.3.1.3" xref="Ch7.E1.m1.1.1.1.1.2.3.1.3.cmml">23</mn></msub></mtd></mtr><mtr id="Ch7.E1.m1.1.1.1.1i" xref="Ch7.E1.m1.1.1.1.1.cmml"><mtd id="Ch7.E1.m1.1.1.1.1j" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.3.1.1" xref="Ch7.E1.m1.1.1.1.1.3.1.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.3.1.1.2" xref="Ch7.E1.m1.1.1.1.1.3.1.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.3.1.1.3" xref="Ch7.E1.m1.1.1.1.1.3.1.1.3.cmml">31</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1k" xref="Ch7.E1.m1.1.1.1.1.cmml"><msub id="Ch7.E1.m1.1.1.1.1.3.2.1" xref="Ch7.E1.m1.1.1.1.1.3.2.1.cmml"><mi id="Ch7.E1.m1.1.1.1.1.3.2.1.2" xref="Ch7.E1.m1.1.1.1.1.3.2.1.2.cmml">h</mi><mn id="Ch7.E1.m1.1.1.1.1.3.2.1.3" xref="Ch7.E1.m1.1.1.1.1.3.2.1.3.cmml">32</mn></msub></mtd><mtd id="Ch7.E1.m1.1.1.1.1l" xref="Ch7.E1.m1.1.1.1.1.cmml"><mn id="Ch7.E1.m1.1.1.1.1.3.3.1" xref="Ch7.E1.m1.1.1.1.1.3.3.1.cmml">1</mn></mtd></mtr></mtable><mo id="Ch7.E1.m1.1.1.3.2" xref="Ch7.E1.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E1.m1.1b"><apply id="Ch7.E1.m1.1.2.cmml" xref="Ch7.E1.m1.1.2"><eq id="Ch7.E1.m1.1.2.1.cmml" xref="Ch7.E1.m1.1.2.1"></eq><ci id="Ch7.E1.m1.1.2.2.cmml" xref="Ch7.E1.m1.1.2.2">𝐻</ci><apply id="Ch7.E1.m1.1.1.2.cmml" xref="Ch7.E1.m1.1.1.3"><csymbol cd="latexml" id="Ch7.E1.m1.1.1.2.1.cmml" xref="Ch7.E1.m1.1.1.3.1">matrix</csymbol><matrix id="Ch7.E1.m1.1.1.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1"><matrixrow id="Ch7.E1.m1.1.1.1.1a.cmml" xref="Ch7.E1.m1.1.1.1.1"><apply id="Ch7.E1.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.1.1.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.1.1.1.3">11</cn></apply><apply id="Ch7.E1.m1.1.1.1.1.1.2.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.1.2.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.2.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.1.2.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.1.2.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.1.2.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.1.2.1.3">12</cn></apply><apply id="Ch7.E1.m1.1.1.1.1.1.3.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.1.3.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.1.3.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.1.3.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.1.3.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.1.3.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.1.3.1.3">13</cn></apply></matrixrow><matrixrow id="Ch7.E1.m1.1.1.1.1b.cmml" xref="Ch7.E1.m1.1.1.1.1"><apply id="Ch7.E1.m1.1.1.1.1.2.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.2.1.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.1.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.2.1.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.2.1.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.2.1.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.2.1.1.3">21</cn></apply><apply id="Ch7.E1.m1.1.1.1.1.2.2.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.2.2.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.2.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.2.2.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.2.2.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.2.2.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.2.2.1.3">22</cn></apply><apply id="Ch7.E1.m1.1.1.1.1.2.3.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.2.3.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.2.3.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.2.3.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.2.3.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.2.3.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.2.3.1.3">23</cn></apply></matrixrow><matrixrow id="Ch7.E1.m1.1.1.1.1c.cmml" xref="Ch7.E1.m1.1.1.1.1"><apply id="Ch7.E1.m1.1.1.1.1.3.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.3.1.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.3.1.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.3.1.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.3.1.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.3.1.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.3.1.1.3">31</cn></apply><apply id="Ch7.E1.m1.1.1.1.1.3.2.1.cmml" xref="Ch7.E1.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="Ch7.E1.m1.1.1.1.1.3.2.1.1.cmml" xref="Ch7.E1.m1.1.1.1.1.3.2.1">subscript</csymbol><ci id="Ch7.E1.m1.1.1.1.1.3.2.1.2.cmml" xref="Ch7.E1.m1.1.1.1.1.3.2.1.2">ℎ</ci><cn id="Ch7.E1.m1.1.1.1.1.3.2.1.3.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.3.2.1.3">32</cn></apply><cn id="Ch7.E1.m1.1.1.1.1.3.3.1.cmml" type="integer" xref="Ch7.E1.m1.1.1.1.1.3.3.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E1.m1.1c">H=\begin{bmatrix}h_{11}&amp;h_{12}&amp;h_{13}\\
h_{21}&amp;h_{22}&amp;h_{23}\\
h_{31}&amp;h_{32}&amp;1\\
\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="Ch7.E1.m1.1d">italic_H = [ start_ARG start_ROW start_CELL italic_h start_POSTSUBSCRIPT 11 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 13 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 21 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 22 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 23 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 31 end_POSTSUBSCRIPT end_CELL start_CELL italic_h start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S1.p4">
<p class="ltx_p" id="Ch7.S1.p4.2">When the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">H</span></a> matrix is multiplied by a point in homogeneous coordinates, it maps it from one plane to another.
It offers a powerful tool for projective transformations, allowing for the mapping of the geometric correspondence between points <math alttext="p" class="ltx_Math" display="inline" id="Ch7.S1.p4.1.m1.1"><semantics id="Ch7.S1.p4.1.m1.1a"><mi id="Ch7.S1.p4.1.m1.1.1" xref="Ch7.S1.p4.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p4.1.m1.1b"><ci id="Ch7.S1.p4.1.m1.1.1.cmml" xref="Ch7.S1.p4.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p4.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p4.1.m1.1d">italic_p</annotation></semantics></math> and <math alttext="p^{\prime}" class="ltx_Math" display="inline" id="Ch7.S1.p4.2.m2.1"><semantics id="Ch7.S1.p4.2.m2.1a"><msup id="Ch7.S1.p4.2.m2.1.1" xref="Ch7.S1.p4.2.m2.1.1.cmml"><mi id="Ch7.S1.p4.2.m2.1.1.2" xref="Ch7.S1.p4.2.m2.1.1.2.cmml">p</mi><mo id="Ch7.S1.p4.2.m2.1.1.3" xref="Ch7.S1.p4.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Ch7.S1.p4.2.m2.1b"><apply id="Ch7.S1.p4.2.m2.1.1.cmml" xref="Ch7.S1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S1.p4.2.m2.1.1.1.cmml" xref="Ch7.S1.p4.2.m2.1.1">superscript</csymbol><ci id="Ch7.S1.p4.2.m2.1.1.2.cmml" xref="Ch7.S1.p4.2.m2.1.1.2">𝑝</ci><ci id="Ch7.S1.p4.2.m2.1.1.3.cmml" xref="Ch7.S1.p4.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p4.2.m2.1c">p^{\prime}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p4.2.m2.1d">italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> in different planes, such that</p>
</div>
<div class="ltx_para" id="Ch7.S1.p5">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p^{\prime}=H\cdot p" class="ltx_Math" display="block" id="Ch7.E2.m1.1"><semantics id="Ch7.E2.m1.1a"><mrow id="Ch7.E2.m1.1.1" xref="Ch7.E2.m1.1.1.cmml"><msup id="Ch7.E2.m1.1.1.2" xref="Ch7.E2.m1.1.1.2.cmml"><mi id="Ch7.E2.m1.1.1.2.2" xref="Ch7.E2.m1.1.1.2.2.cmml">p</mi><mo id="Ch7.E2.m1.1.1.2.3" xref="Ch7.E2.m1.1.1.2.3.cmml">′</mo></msup><mo id="Ch7.E2.m1.1.1.1" xref="Ch7.E2.m1.1.1.1.cmml">=</mo><mrow id="Ch7.E2.m1.1.1.3" xref="Ch7.E2.m1.1.1.3.cmml"><mi id="Ch7.E2.m1.1.1.3.2" xref="Ch7.E2.m1.1.1.3.2.cmml">H</mi><mo id="Ch7.E2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E2.m1.1.1.3.1.cmml">⋅</mo><mi id="Ch7.E2.m1.1.1.3.3" xref="Ch7.E2.m1.1.1.3.3.cmml">p</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E2.m1.1b"><apply id="Ch7.E2.m1.1.1.cmml" xref="Ch7.E2.m1.1.1"><eq id="Ch7.E2.m1.1.1.1.cmml" xref="Ch7.E2.m1.1.1.1"></eq><apply id="Ch7.E2.m1.1.1.2.cmml" xref="Ch7.E2.m1.1.1.2"><csymbol cd="ambiguous" id="Ch7.E2.m1.1.1.2.1.cmml" xref="Ch7.E2.m1.1.1.2">superscript</csymbol><ci id="Ch7.E2.m1.1.1.2.2.cmml" xref="Ch7.E2.m1.1.1.2.2">𝑝</ci><ci id="Ch7.E2.m1.1.1.2.3.cmml" xref="Ch7.E2.m1.1.1.2.3">′</ci></apply><apply id="Ch7.E2.m1.1.1.3.cmml" xref="Ch7.E2.m1.1.1.3"><ci id="Ch7.E2.m1.1.1.3.1.cmml" xref="Ch7.E2.m1.1.1.3.1">⋅</ci><ci id="Ch7.E2.m1.1.1.3.2.cmml" xref="Ch7.E2.m1.1.1.3.2">𝐻</ci><ci id="Ch7.E2.m1.1.1.3.3.cmml" xref="Ch7.E2.m1.1.1.3.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E2.m1.1c">p^{\prime}=H\cdot p</annotation><annotation encoding="application/x-llamapun" id="Ch7.E2.m1.1d">italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_H ⋅ italic_p</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S1.p6">
<p class="ltx_p" id="Ch7.S1.p6.2">where <math alttext="p=\begin{bmatrix}x\\
y\\
1\end{bmatrix}" class="ltx_Math" display="inline" id="Ch7.S1.p6.1.m1.1"><semantics id="Ch7.S1.p6.1.m1.1a"><mrow id="Ch7.S1.p6.1.m1.1.2" xref="Ch7.S1.p6.1.m1.1.2.cmml"><mi id="Ch7.S1.p6.1.m1.1.2.2" xref="Ch7.S1.p6.1.m1.1.2.2.cmml">p</mi><mo id="Ch7.S1.p6.1.m1.1.2.1" xref="Ch7.S1.p6.1.m1.1.2.1.cmml">=</mo><mrow id="Ch7.S1.p6.1.m1.1.1.3" xref="Ch7.S1.p6.1.m1.1.1.2.cmml"><mo id="Ch7.S1.p6.1.m1.1.1.3.1" xref="Ch7.S1.p6.1.m1.1.1.2.1.cmml">[</mo><mtable id="Ch7.S1.p6.1.m1.1.1.1.1" rowspacing="0pt" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mtr id="Ch7.S1.p6.1.m1.1.1.1.1a" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.1.m1.1.1.1.1b" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mi id="Ch7.S1.p6.1.m1.1.1.1.1.1.1.1" xref="Ch7.S1.p6.1.m1.1.1.1.1.1.1.1.cmml">x</mi></mtd></mtr><mtr id="Ch7.S1.p6.1.m1.1.1.1.1c" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.1.m1.1.1.1.1d" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mi id="Ch7.S1.p6.1.m1.1.1.1.1.2.1.1" xref="Ch7.S1.p6.1.m1.1.1.1.1.2.1.1.cmml">y</mi></mtd></mtr><mtr id="Ch7.S1.p6.1.m1.1.1.1.1e" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.1.m1.1.1.1.1f" xref="Ch7.S1.p6.1.m1.1.1.1.1.cmml"><mn id="Ch7.S1.p6.1.m1.1.1.1.1.3.1.1" xref="Ch7.S1.p6.1.m1.1.1.1.1.3.1.1.cmml">1</mn></mtd></mtr></mtable><mo id="Ch7.S1.p6.1.m1.1.1.3.2" xref="Ch7.S1.p6.1.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p6.1.m1.1b"><apply id="Ch7.S1.p6.1.m1.1.2.cmml" xref="Ch7.S1.p6.1.m1.1.2"><eq id="Ch7.S1.p6.1.m1.1.2.1.cmml" xref="Ch7.S1.p6.1.m1.1.2.1"></eq><ci id="Ch7.S1.p6.1.m1.1.2.2.cmml" xref="Ch7.S1.p6.1.m1.1.2.2">𝑝</ci><apply id="Ch7.S1.p6.1.m1.1.1.2.cmml" xref="Ch7.S1.p6.1.m1.1.1.3"><csymbol cd="latexml" id="Ch7.S1.p6.1.m1.1.1.2.1.cmml" xref="Ch7.S1.p6.1.m1.1.1.3.1">matrix</csymbol><matrix id="Ch7.S1.p6.1.m1.1.1.1.1.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1"><matrixrow id="Ch7.S1.p6.1.m1.1.1.1.1a.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1"><ci id="Ch7.S1.p6.1.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1.1.1.1">𝑥</ci></matrixrow><matrixrow id="Ch7.S1.p6.1.m1.1.1.1.1b.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1"><ci id="Ch7.S1.p6.1.m1.1.1.1.1.2.1.1.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1.2.1.1">𝑦</ci></matrixrow><matrixrow id="Ch7.S1.p6.1.m1.1.1.1.1c.cmml" xref="Ch7.S1.p6.1.m1.1.1.1.1"><cn id="Ch7.S1.p6.1.m1.1.1.1.1.3.1.1.cmml" type="integer" xref="Ch7.S1.p6.1.m1.1.1.1.1.3.1.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p6.1.m1.1c">p=\begin{bmatrix}x\\
y\\
1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p6.1.m1.1d">italic_p = [ start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math> and <math alttext="p^{\prime}=\begin{bmatrix}x^{\prime}\\
y^{\prime}\\
1\end{bmatrix}" class="ltx_Math" display="inline" id="Ch7.S1.p6.2.m2.1"><semantics id="Ch7.S1.p6.2.m2.1a"><mrow id="Ch7.S1.p6.2.m2.1.2" xref="Ch7.S1.p6.2.m2.1.2.cmml"><msup id="Ch7.S1.p6.2.m2.1.2.2" xref="Ch7.S1.p6.2.m2.1.2.2.cmml"><mi id="Ch7.S1.p6.2.m2.1.2.2.2" xref="Ch7.S1.p6.2.m2.1.2.2.2.cmml">p</mi><mo id="Ch7.S1.p6.2.m2.1.2.2.3" xref="Ch7.S1.p6.2.m2.1.2.2.3.cmml">′</mo></msup><mo id="Ch7.S1.p6.2.m2.1.2.1" xref="Ch7.S1.p6.2.m2.1.2.1.cmml">=</mo><mrow id="Ch7.S1.p6.2.m2.1.1.3" xref="Ch7.S1.p6.2.m2.1.1.2.cmml"><mo id="Ch7.S1.p6.2.m2.1.1.3.1" xref="Ch7.S1.p6.2.m2.1.1.2.1.cmml">[</mo><mtable id="Ch7.S1.p6.2.m2.1.1.1.1" rowspacing="0pt" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><mtr id="Ch7.S1.p6.2.m2.1.1.1.1a" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.2.m2.1.1.1.1b" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><msup id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.cmml"><mi id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.2" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.3" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.3.cmml">′</mo></msup></mtd></mtr><mtr id="Ch7.S1.p6.2.m2.1.1.1.1c" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.2.m2.1.1.1.1d" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><msup id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.cmml"><mi id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.2" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.2.cmml">y</mi><mo id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.3" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.3.cmml">′</mo></msup></mtd></mtr><mtr id="Ch7.S1.p6.2.m2.1.1.1.1e" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><mtd id="Ch7.S1.p6.2.m2.1.1.1.1f" xref="Ch7.S1.p6.2.m2.1.1.1.1.cmml"><mn id="Ch7.S1.p6.2.m2.1.1.1.1.3.1.1" xref="Ch7.S1.p6.2.m2.1.1.1.1.3.1.1.cmml">1</mn></mtd></mtr></mtable><mo id="Ch7.S1.p6.2.m2.1.1.3.2" xref="Ch7.S1.p6.2.m2.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p6.2.m2.1b"><apply id="Ch7.S1.p6.2.m2.1.2.cmml" xref="Ch7.S1.p6.2.m2.1.2"><eq id="Ch7.S1.p6.2.m2.1.2.1.cmml" xref="Ch7.S1.p6.2.m2.1.2.1"></eq><apply id="Ch7.S1.p6.2.m2.1.2.2.cmml" xref="Ch7.S1.p6.2.m2.1.2.2"><csymbol cd="ambiguous" id="Ch7.S1.p6.2.m2.1.2.2.1.cmml" xref="Ch7.S1.p6.2.m2.1.2.2">superscript</csymbol><ci id="Ch7.S1.p6.2.m2.1.2.2.2.cmml" xref="Ch7.S1.p6.2.m2.1.2.2.2">𝑝</ci><ci id="Ch7.S1.p6.2.m2.1.2.2.3.cmml" xref="Ch7.S1.p6.2.m2.1.2.2.3">′</ci></apply><apply id="Ch7.S1.p6.2.m2.1.1.2.cmml" xref="Ch7.S1.p6.2.m2.1.1.3"><csymbol cd="latexml" id="Ch7.S1.p6.2.m2.1.1.2.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.3.1">matrix</csymbol><matrix id="Ch7.S1.p6.2.m2.1.1.1.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1"><matrixrow id="Ch7.S1.p6.2.m2.1.1.1.1a.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1"><apply id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1">superscript</csymbol><ci id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.2.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.2">𝑥</ci><ci id="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.3.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.1.1.1.3">′</ci></apply></matrixrow><matrixrow id="Ch7.S1.p6.2.m2.1.1.1.1b.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1"><apply id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.1.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1">superscript</csymbol><ci id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.2.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.2">𝑦</ci><ci id="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.3.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1.2.1.1.3">′</ci></apply></matrixrow><matrixrow id="Ch7.S1.p6.2.m2.1.1.1.1c.cmml" xref="Ch7.S1.p6.2.m2.1.1.1.1"><cn id="Ch7.S1.p6.2.m2.1.1.1.1.3.1.1.cmml" type="integer" xref="Ch7.S1.p6.2.m2.1.1.1.1.3.1.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p6.2.m2.1c">p^{\prime}=\begin{bmatrix}x^{\prime}\\
y^{\prime}\\
1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p6.2.m2.1d">italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math>, as seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F1" title="Figure 7.1 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.1</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p7">
<p class="ltx_p" id="Ch7.S1.p7.1">The creation of a homography matrix requires the identification of correspondences between points in the two planes, typically two images.
Once a sufficient number of point pairs (usually at least four non-collinear points) is established, the homography matrix can be solved using linear algebra techniques.
The process involves setting up a system of equations based on the point correspondences and solving for the eight unknowns of the homography matrix (the ninth element is conventionally set to 1 for normalization).</p>
</div>
<div class="ltx_para" id="Ch7.S1.p8">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}x_{1}&amp;y_{1}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{1}x_{1}&amp;-x^{\prime}_{1}y_{1}\\
0&amp;0&amp;0&amp;x_{1}&amp;y_{1}&amp;1&amp;-y^{\prime}_{1}x_{1}&amp;-y^{\prime}_{1}y_{1}\\
x_{2}&amp;y_{2}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{2}x_{2}&amp;-x^{\prime}_{2}y_{2}\\
0&amp;0&amp;0&amp;x_{2}&amp;y_{2}&amp;1&amp;-y^{\prime}_{2}x_{2}&amp;-y^{\prime}_{2}y_{2}\\
x_{3}&amp;y_{3}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{3}x_{3}&amp;-x^{\prime}_{3}y_{3}\\
0&amp;0&amp;0&amp;x_{3}&amp;y_{3}&amp;1&amp;-y^{\prime}_{3}x_{3}&amp;-y^{\prime}_{3}y_{3}\\
x_{4}&amp;y_{4}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{4}x_{4}&amp;-x^{\prime}_{4}y_{4}\\
0&amp;0&amp;0&amp;x_{4}&amp;y_{4}&amp;1&amp;-y^{\prime}_{4}x_{4}&amp;-y^{\prime}_{4}y_{4}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_{n}&amp;y_{n}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{n}x_{n}&amp;-x^{\prime}_{n}y_{n}\\
0&amp;0&amp;0&amp;x_{n}&amp;y_{n}&amp;1&amp;-y^{\prime}_{n}x_{n}&amp;-y^{\prime}_{n}y_{n}\\
\end{bmatrix}\begin{bmatrix}h_{11}\\
h_{12}\\
h_{13}\\
h_{21}\\
h_{22}\\
h_{23}\\
h_{31}\\
h_{32}\\
\end{bmatrix}=\begin{bmatrix}x^{\prime}_{1}\\
y^{\prime}_{1}\\
x^{\prime}_{2}\\
y^{\prime}_{2}\\
x^{\prime}_{3}\\
y^{\prime}_{3}\\
x^{\prime}_{4}\\
y^{\prime}_{4}\\
\vdots\\
x^{\prime}_{n}\\
y^{\prime}_{n}\\
\end{bmatrix}" class="ltx_Math" display="block" id="Ch7.E3.m1.3"><semantics id="Ch7.E3.m1.3a"><mrow id="Ch7.E3.m1.3.4" xref="Ch7.E3.m1.3.4.cmml"><mrow id="Ch7.E3.m1.3.4.2" xref="Ch7.E3.m1.3.4.2.cmml"><mrow id="Ch7.E3.m1.1.1.3" xref="Ch7.E3.m1.1.1.2.cmml"><mo id="Ch7.E3.m1.1.1.3.1" xref="Ch7.E3.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" id="Ch7.E3.m1.1.1.1.1" rowspacing="0pt" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtr id="Ch7.E3.m1.1.1.1.1a" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1b" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.1.1.1" xref="Ch7.E3.m1.1.1.1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.1.1.2" xref="Ch7.E3.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.1.1.1.3" xref="Ch7.E3.m1.1.1.1.1.1.1.1.3.cmml">1</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1c" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.1.2.1" xref="Ch7.E3.m1.1.1.1.1.1.2.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.2.1.2" xref="Ch7.E3.m1.1.1.1.1.1.2.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.1.2.1.3" xref="Ch7.E3.m1.1.1.1.1.1.2.1.3.cmml">1</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1d" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.1.3.1" xref="Ch7.E3.m1.1.1.1.1.1.3.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1e" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.1.4.1" xref="Ch7.E3.m1.1.1.1.1.1.4.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1f" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.1.5.1" xref="Ch7.E3.m1.1.1.1.1.1.5.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1g" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.1.6.1" xref="Ch7.E3.m1.1.1.1.1.1.6.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1h" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.1.7.1" xref="Ch7.E3.m1.1.1.1.1.1.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.1.7.1a" xref="Ch7.E3.m1.1.1.1.1.1.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.1.7.1.2" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.3.cmml">1</mn><mo id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.1.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.3.cmml">1</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1i" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.1.8.1" xref="Ch7.E3.m1.1.1.1.1.1.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.1.8.1a" xref="Ch7.E3.m1.1.1.1.1.1.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.1.8.1.2" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.3.cmml">1</mn><mo id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.1.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.3.cmml">1</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1j" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1k" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.2.1.1" xref="Ch7.E3.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1l" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.2.2.1" xref="Ch7.E3.m1.1.1.1.1.2.2.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1m" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.2.3.1" xref="Ch7.E3.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1n" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.2.4.1" xref="Ch7.E3.m1.1.1.1.1.2.4.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.4.1.2" xref="Ch7.E3.m1.1.1.1.1.2.4.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.2.4.1.3" xref="Ch7.E3.m1.1.1.1.1.2.4.1.3.cmml">1</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1o" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.2.5.1" xref="Ch7.E3.m1.1.1.1.1.2.5.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.5.1.2" xref="Ch7.E3.m1.1.1.1.1.2.5.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.2.5.1.3" xref="Ch7.E3.m1.1.1.1.1.2.5.1.3.cmml">1</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1p" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.2.6.1" xref="Ch7.E3.m1.1.1.1.1.2.6.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1q" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.2.7.1" xref="Ch7.E3.m1.1.1.1.1.2.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.2.7.1a" xref="Ch7.E3.m1.1.1.1.1.2.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.2.7.1.2" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.3.cmml">1</mn><mo id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.2.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.3.cmml">1</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1r" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.2.8.1" xref="Ch7.E3.m1.1.1.1.1.2.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.2.8.1a" xref="Ch7.E3.m1.1.1.1.1.2.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.2.8.1.2" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.3.cmml">1</mn><mo id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.2.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.3.cmml">1</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1s" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1t" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.3.1.1" xref="Ch7.E3.m1.1.1.1.1.3.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.1.1.2" xref="Ch7.E3.m1.1.1.1.1.3.1.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.3.1.1.3" xref="Ch7.E3.m1.1.1.1.1.3.1.1.3.cmml">2</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1u" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.3.2.1" xref="Ch7.E3.m1.1.1.1.1.3.2.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.2.1.2" xref="Ch7.E3.m1.1.1.1.1.3.2.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.3.2.1.3" xref="Ch7.E3.m1.1.1.1.1.3.2.1.3.cmml">2</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1v" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.3.3.1" xref="Ch7.E3.m1.1.1.1.1.3.3.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1w" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.3.4.1" xref="Ch7.E3.m1.1.1.1.1.3.4.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1x" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.3.5.1" xref="Ch7.E3.m1.1.1.1.1.3.5.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1y" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.3.6.1" xref="Ch7.E3.m1.1.1.1.1.3.6.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1z" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.3.7.1" xref="Ch7.E3.m1.1.1.1.1.3.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.3.7.1a" xref="Ch7.E3.m1.1.1.1.1.3.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.3.7.1.2" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.3.cmml">2</mn><mo id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.3.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.3.cmml">2</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1aa" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.3.8.1" xref="Ch7.E3.m1.1.1.1.1.3.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.3.8.1a" xref="Ch7.E3.m1.1.1.1.1.3.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.3.8.1.2" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.3.cmml">2</mn><mo id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.3.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.3.cmml">2</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1ab" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1ac" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.4.1.1" xref="Ch7.E3.m1.1.1.1.1.4.1.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ad" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.4.2.1" xref="Ch7.E3.m1.1.1.1.1.4.2.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ae" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.4.3.1" xref="Ch7.E3.m1.1.1.1.1.4.3.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1af" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.4.4.1" xref="Ch7.E3.m1.1.1.1.1.4.4.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.4.1.2" xref="Ch7.E3.m1.1.1.1.1.4.4.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.4.4.1.3" xref="Ch7.E3.m1.1.1.1.1.4.4.1.3.cmml">2</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1ag" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.4.5.1" xref="Ch7.E3.m1.1.1.1.1.4.5.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.5.1.2" xref="Ch7.E3.m1.1.1.1.1.4.5.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.4.5.1.3" xref="Ch7.E3.m1.1.1.1.1.4.5.1.3.cmml">2</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1ah" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.4.6.1" xref="Ch7.E3.m1.1.1.1.1.4.6.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ai" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.4.7.1" xref="Ch7.E3.m1.1.1.1.1.4.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.4.7.1a" xref="Ch7.E3.m1.1.1.1.1.4.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.4.7.1.2" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.3.cmml">2</mn><mo id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.4.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.3.cmml">2</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1aj" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.4.8.1" xref="Ch7.E3.m1.1.1.1.1.4.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.4.8.1a" xref="Ch7.E3.m1.1.1.1.1.4.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.4.8.1.2" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.3.cmml">2</mn><mo id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.4.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.3.cmml">2</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1ak" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1al" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.5.1.1" xref="Ch7.E3.m1.1.1.1.1.5.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.1.1.2" xref="Ch7.E3.m1.1.1.1.1.5.1.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.5.1.1.3" xref="Ch7.E3.m1.1.1.1.1.5.1.1.3.cmml">3</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1am" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.5.2.1" xref="Ch7.E3.m1.1.1.1.1.5.2.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.2.1.2" xref="Ch7.E3.m1.1.1.1.1.5.2.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.5.2.1.3" xref="Ch7.E3.m1.1.1.1.1.5.2.1.3.cmml">3</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1an" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.5.3.1" xref="Ch7.E3.m1.1.1.1.1.5.3.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ao" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.5.4.1" xref="Ch7.E3.m1.1.1.1.1.5.4.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ap" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.5.5.1" xref="Ch7.E3.m1.1.1.1.1.5.5.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1aq" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.5.6.1" xref="Ch7.E3.m1.1.1.1.1.5.6.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ar" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.5.7.1" xref="Ch7.E3.m1.1.1.1.1.5.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.5.7.1a" xref="Ch7.E3.m1.1.1.1.1.5.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.5.7.1.2" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.3.cmml">3</mn><mo id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.5.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.3.cmml">3</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1as" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.5.8.1" xref="Ch7.E3.m1.1.1.1.1.5.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.5.8.1a" xref="Ch7.E3.m1.1.1.1.1.5.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.5.8.1.2" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.3.cmml">3</mn><mo id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.5.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.3.cmml">3</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1at" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1au" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.6.1.1" xref="Ch7.E3.m1.1.1.1.1.6.1.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1av" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.6.2.1" xref="Ch7.E3.m1.1.1.1.1.6.2.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1aw" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.6.3.1" xref="Ch7.E3.m1.1.1.1.1.6.3.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ax" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.6.4.1" xref="Ch7.E3.m1.1.1.1.1.6.4.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.4.1.2" xref="Ch7.E3.m1.1.1.1.1.6.4.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.6.4.1.3" xref="Ch7.E3.m1.1.1.1.1.6.4.1.3.cmml">3</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1ay" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.6.5.1" xref="Ch7.E3.m1.1.1.1.1.6.5.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.5.1.2" xref="Ch7.E3.m1.1.1.1.1.6.5.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.6.5.1.3" xref="Ch7.E3.m1.1.1.1.1.6.5.1.3.cmml">3</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1az" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.6.6.1" xref="Ch7.E3.m1.1.1.1.1.6.6.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ba" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.6.7.1" xref="Ch7.E3.m1.1.1.1.1.6.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.6.7.1a" xref="Ch7.E3.m1.1.1.1.1.6.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.6.7.1.2" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.3.cmml">3</mn><mo id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.6.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.3.cmml">3</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1bb" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.6.8.1" xref="Ch7.E3.m1.1.1.1.1.6.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.6.8.1a" xref="Ch7.E3.m1.1.1.1.1.6.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.6.8.1.2" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.3.cmml">3</mn><mo id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.6.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.3.cmml">3</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1bc" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1bd" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.7.1.1" xref="Ch7.E3.m1.1.1.1.1.7.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.1.1.2" xref="Ch7.E3.m1.1.1.1.1.7.1.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.7.1.1.3" xref="Ch7.E3.m1.1.1.1.1.7.1.1.3.cmml">4</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1be" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.7.2.1" xref="Ch7.E3.m1.1.1.1.1.7.2.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.2.1.2" xref="Ch7.E3.m1.1.1.1.1.7.2.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.7.2.1.3" xref="Ch7.E3.m1.1.1.1.1.7.2.1.3.cmml">4</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1bf" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.7.3.1" xref="Ch7.E3.m1.1.1.1.1.7.3.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bg" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.7.4.1" xref="Ch7.E3.m1.1.1.1.1.7.4.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bh" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.7.5.1" xref="Ch7.E3.m1.1.1.1.1.7.5.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bi" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.7.6.1" xref="Ch7.E3.m1.1.1.1.1.7.6.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bj" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.7.7.1" xref="Ch7.E3.m1.1.1.1.1.7.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.7.7.1a" xref="Ch7.E3.m1.1.1.1.1.7.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.7.7.1.2" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.3.cmml">4</mn><mo id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.7.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.3.cmml">4</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1bk" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.7.8.1" xref="Ch7.E3.m1.1.1.1.1.7.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.7.8.1a" xref="Ch7.E3.m1.1.1.1.1.7.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.7.8.1.2" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.3.cmml">4</mn><mo id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.7.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.3.cmml">4</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1bl" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1bm" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.8.1.1" xref="Ch7.E3.m1.1.1.1.1.8.1.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bn" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.8.2.1" xref="Ch7.E3.m1.1.1.1.1.8.2.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bo" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.8.3.1" xref="Ch7.E3.m1.1.1.1.1.8.3.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bp" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.8.4.1" xref="Ch7.E3.m1.1.1.1.1.8.4.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.4.1.2" xref="Ch7.E3.m1.1.1.1.1.8.4.1.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.8.4.1.3" xref="Ch7.E3.m1.1.1.1.1.8.4.1.3.cmml">4</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1bq" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.8.5.1" xref="Ch7.E3.m1.1.1.1.1.8.5.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.5.1.2" xref="Ch7.E3.m1.1.1.1.1.8.5.1.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.8.5.1.3" xref="Ch7.E3.m1.1.1.1.1.8.5.1.3.cmml">4</mn></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1br" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.8.6.1" xref="Ch7.E3.m1.1.1.1.1.8.6.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1bs" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.8.7.1" xref="Ch7.E3.m1.1.1.1.1.8.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.8.7.1a" xref="Ch7.E3.m1.1.1.1.1.8.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.8.7.1.2" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.3.cmml">4</mn><mo id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.8.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.2.cmml">x</mi><mn id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.3.cmml">4</mn></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1bt" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.8.8.1" xref="Ch7.E3.m1.1.1.1.1.8.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.8.8.1a" xref="Ch7.E3.m1.1.1.1.1.8.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.8.8.1.2" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.3.cmml">4</mn><mo id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.8.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.2.cmml">y</mi><mn id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.3.cmml">4</mn></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1bu" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1bv" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.1.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.1.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1bw" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.2.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.2.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1bx" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.3.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.3.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1by" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.4.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.4.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1bz" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.5.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.5.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1ca" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.6.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.6.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1cb" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.7.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.7.1.cmml">⋮</mi></mtd><mtd id="Ch7.E3.m1.1.1.1.1cc" xref="Ch7.E3.m1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.9.8.1" mathvariant="normal" xref="Ch7.E3.m1.1.1.1.1.9.8.1.cmml">⋮</mi></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1cd" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1ce" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.10.1.1" xref="Ch7.E3.m1.1.1.1.1.10.1.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.1.1.2" xref="Ch7.E3.m1.1.1.1.1.10.1.1.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.10.1.1.3" xref="Ch7.E3.m1.1.1.1.1.10.1.1.3.cmml">n</mi></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1cf" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.10.2.1" xref="Ch7.E3.m1.1.1.1.1.10.2.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.2.1.2" xref="Ch7.E3.m1.1.1.1.1.10.2.1.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.10.2.1.3" xref="Ch7.E3.m1.1.1.1.1.10.2.1.3.cmml">n</mi></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1cg" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.10.3.1" xref="Ch7.E3.m1.1.1.1.1.10.3.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ch" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.10.4.1" xref="Ch7.E3.m1.1.1.1.1.10.4.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ci" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.10.5.1" xref="Ch7.E3.m1.1.1.1.1.10.5.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1cj" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.10.6.1" xref="Ch7.E3.m1.1.1.1.1.10.6.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ck" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.10.7.1" xref="Ch7.E3.m1.1.1.1.1.10.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.10.7.1a" xref="Ch7.E3.m1.1.1.1.1.10.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.10.7.1.2" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.3.cmml">n</mi><mo id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.10.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.3.cmml">n</mi></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1cl" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.10.8.1" xref="Ch7.E3.m1.1.1.1.1.10.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.10.8.1a" xref="Ch7.E3.m1.1.1.1.1.10.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.10.8.1.2" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.3.cmml">n</mi><mo id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.10.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.3.cmml">n</mi></msub></mrow></mrow></mtd></mtr><mtr id="Ch7.E3.m1.1.1.1.1cm" xref="Ch7.E3.m1.1.1.1.1.cmml"><mtd id="Ch7.E3.m1.1.1.1.1cn" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.11.1.1" xref="Ch7.E3.m1.1.1.1.1.11.1.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1co" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.11.2.1" xref="Ch7.E3.m1.1.1.1.1.11.2.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1cp" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.11.3.1" xref="Ch7.E3.m1.1.1.1.1.11.3.1.cmml">0</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1cq" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.11.4.1" xref="Ch7.E3.m1.1.1.1.1.11.4.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.4.1.2" xref="Ch7.E3.m1.1.1.1.1.11.4.1.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.11.4.1.3" xref="Ch7.E3.m1.1.1.1.1.11.4.1.3.cmml">n</mi></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1cr" xref="Ch7.E3.m1.1.1.1.1.cmml"><msub id="Ch7.E3.m1.1.1.1.1.11.5.1" xref="Ch7.E3.m1.1.1.1.1.11.5.1.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.5.1.2" xref="Ch7.E3.m1.1.1.1.1.11.5.1.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.11.5.1.3" xref="Ch7.E3.m1.1.1.1.1.11.5.1.3.cmml">n</mi></msub></mtd><mtd id="Ch7.E3.m1.1.1.1.1cs" xref="Ch7.E3.m1.1.1.1.1.cmml"><mn id="Ch7.E3.m1.1.1.1.1.11.6.1" xref="Ch7.E3.m1.1.1.1.1.11.6.1.cmml">1</mn></mtd><mtd id="Ch7.E3.m1.1.1.1.1ct" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.11.7.1" xref="Ch7.E3.m1.1.1.1.1.11.7.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.11.7.1a" xref="Ch7.E3.m1.1.1.1.1.11.7.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.11.7.1.2" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.3.cmml">n</mi><mo id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.11.7.1.2.1" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.2.cmml">x</mi><mi id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.3.cmml">n</mi></msub></mrow></mrow></mtd><mtd id="Ch7.E3.m1.1.1.1.1cu" xref="Ch7.E3.m1.1.1.1.1.cmml"><mrow id="Ch7.E3.m1.1.1.1.1.11.8.1" xref="Ch7.E3.m1.1.1.1.1.11.8.1.cmml"><mo id="Ch7.E3.m1.1.1.1.1.11.8.1a" xref="Ch7.E3.m1.1.1.1.1.11.8.1.cmml">−</mo><mrow id="Ch7.E3.m1.1.1.1.1.11.8.1.2" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.cmml"><msubsup id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.2" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.3" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.3.cmml">n</mi><mo id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.3" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.3.cmml">′</mo></msubsup><mo id="Ch7.E3.m1.1.1.1.1.11.8.1.2.1" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.1.cmml">⁢</mo><msub id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.cmml"><mi id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.2" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.2.cmml">y</mi><mi id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.3" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.3.cmml">n</mi></msub></mrow></mrow></mtd></mtr></mtable><mo id="Ch7.E3.m1.1.1.3.2" xref="Ch7.E3.m1.1.1.2.1.cmml">]</mo></mrow><mo id="Ch7.E3.m1.3.4.2.1" xref="Ch7.E3.m1.3.4.2.1.cmml">⁢</mo><mrow id="Ch7.E3.m1.2.2.3" xref="Ch7.E3.m1.2.2.2.cmml"><mo id="Ch7.E3.m1.2.2.3.1" xref="Ch7.E3.m1.2.2.2.1.cmml">[</mo><mtable displaystyle="true" id="Ch7.E3.m1.2.2.1.1" rowspacing="0pt" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtr id="Ch7.E3.m1.2.2.1.1a" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1b" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.1.1.1" xref="Ch7.E3.m1.2.2.1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.1.1.1.2" xref="Ch7.E3.m1.2.2.1.1.1.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.1.1.1.3" xref="Ch7.E3.m1.2.2.1.1.1.1.1.3.cmml">11</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1c" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1d" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.2.1.1" xref="Ch7.E3.m1.2.2.1.1.2.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.2.1.1.2" xref="Ch7.E3.m1.2.2.1.1.2.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.2.1.1.3" xref="Ch7.E3.m1.2.2.1.1.2.1.1.3.cmml">12</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1e" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1f" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.3.1.1" xref="Ch7.E3.m1.2.2.1.1.3.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.3.1.1.2" xref="Ch7.E3.m1.2.2.1.1.3.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.3.1.1.3" xref="Ch7.E3.m1.2.2.1.1.3.1.1.3.cmml">13</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1g" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1h" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.4.1.1" xref="Ch7.E3.m1.2.2.1.1.4.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.4.1.1.2" xref="Ch7.E3.m1.2.2.1.1.4.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.4.1.1.3" xref="Ch7.E3.m1.2.2.1.1.4.1.1.3.cmml">21</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1i" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1j" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.5.1.1" xref="Ch7.E3.m1.2.2.1.1.5.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.5.1.1.2" xref="Ch7.E3.m1.2.2.1.1.5.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.5.1.1.3" xref="Ch7.E3.m1.2.2.1.1.5.1.1.3.cmml">22</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1k" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1l" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.6.1.1" xref="Ch7.E3.m1.2.2.1.1.6.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.6.1.1.2" xref="Ch7.E3.m1.2.2.1.1.6.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.6.1.1.3" xref="Ch7.E3.m1.2.2.1.1.6.1.1.3.cmml">23</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1m" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1n" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.7.1.1" xref="Ch7.E3.m1.2.2.1.1.7.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.7.1.1.2" xref="Ch7.E3.m1.2.2.1.1.7.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.7.1.1.3" xref="Ch7.E3.m1.2.2.1.1.7.1.1.3.cmml">31</mn></msub></mtd></mtr><mtr id="Ch7.E3.m1.2.2.1.1o" xref="Ch7.E3.m1.2.2.1.1.cmml"><mtd id="Ch7.E3.m1.2.2.1.1p" xref="Ch7.E3.m1.2.2.1.1.cmml"><msub id="Ch7.E3.m1.2.2.1.1.8.1.1" xref="Ch7.E3.m1.2.2.1.1.8.1.1.cmml"><mi id="Ch7.E3.m1.2.2.1.1.8.1.1.2" xref="Ch7.E3.m1.2.2.1.1.8.1.1.2.cmml">h</mi><mn id="Ch7.E3.m1.2.2.1.1.8.1.1.3" xref="Ch7.E3.m1.2.2.1.1.8.1.1.3.cmml">32</mn></msub></mtd></mtr></mtable><mo id="Ch7.E3.m1.2.2.3.2" xref="Ch7.E3.m1.2.2.2.1.cmml">]</mo></mrow></mrow><mo id="Ch7.E3.m1.3.4.1" xref="Ch7.E3.m1.3.4.1.cmml">=</mo><mrow id="Ch7.E3.m1.3.3.3" xref="Ch7.E3.m1.3.3.2.cmml"><mo id="Ch7.E3.m1.3.3.3.1" xref="Ch7.E3.m1.3.3.2.1.cmml">[</mo><mtable displaystyle="true" id="Ch7.E3.m1.3.3.1.1" rowspacing="0pt" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtr id="Ch7.E3.m1.3.3.1.1a" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1b" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.1.1.1" xref="Ch7.E3.m1.3.3.1.1.1.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.1.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.1.1.1.2.2.cmml">x</mi><mn id="Ch7.E3.m1.3.3.1.1.1.1.1.3" xref="Ch7.E3.m1.3.3.1.1.1.1.1.3.cmml">1</mn><mo id="Ch7.E3.m1.3.3.1.1.1.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.1.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1c" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1d" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.2.1.1" xref="Ch7.E3.m1.3.3.1.1.2.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.2.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.2.1.1.2.2.cmml">y</mi><mn id="Ch7.E3.m1.3.3.1.1.2.1.1.3" xref="Ch7.E3.m1.3.3.1.1.2.1.1.3.cmml">1</mn><mo id="Ch7.E3.m1.3.3.1.1.2.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.2.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1e" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1f" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.3.1.1" xref="Ch7.E3.m1.3.3.1.1.3.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.3.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.3.1.1.2.2.cmml">x</mi><mn id="Ch7.E3.m1.3.3.1.1.3.1.1.3" xref="Ch7.E3.m1.3.3.1.1.3.1.1.3.cmml">2</mn><mo id="Ch7.E3.m1.3.3.1.1.3.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.3.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1g" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1h" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.4.1.1" xref="Ch7.E3.m1.3.3.1.1.4.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.4.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.4.1.1.2.2.cmml">y</mi><mn id="Ch7.E3.m1.3.3.1.1.4.1.1.3" xref="Ch7.E3.m1.3.3.1.1.4.1.1.3.cmml">2</mn><mo id="Ch7.E3.m1.3.3.1.1.4.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.4.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1i" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1j" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.5.1.1" xref="Ch7.E3.m1.3.3.1.1.5.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.5.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.5.1.1.2.2.cmml">x</mi><mn id="Ch7.E3.m1.3.3.1.1.5.1.1.3" xref="Ch7.E3.m1.3.3.1.1.5.1.1.3.cmml">3</mn><mo id="Ch7.E3.m1.3.3.1.1.5.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.5.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1k" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1l" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.6.1.1" xref="Ch7.E3.m1.3.3.1.1.6.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.6.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.6.1.1.2.2.cmml">y</mi><mn id="Ch7.E3.m1.3.3.1.1.6.1.1.3" xref="Ch7.E3.m1.3.3.1.1.6.1.1.3.cmml">3</mn><mo id="Ch7.E3.m1.3.3.1.1.6.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.6.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1m" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1n" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.7.1.1" xref="Ch7.E3.m1.3.3.1.1.7.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.7.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.7.1.1.2.2.cmml">x</mi><mn id="Ch7.E3.m1.3.3.1.1.7.1.1.3" xref="Ch7.E3.m1.3.3.1.1.7.1.1.3.cmml">4</mn><mo id="Ch7.E3.m1.3.3.1.1.7.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.7.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1o" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1p" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.8.1.1" xref="Ch7.E3.m1.3.3.1.1.8.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.8.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.8.1.1.2.2.cmml">y</mi><mn id="Ch7.E3.m1.3.3.1.1.8.1.1.3" xref="Ch7.E3.m1.3.3.1.1.8.1.1.3.cmml">4</mn><mo id="Ch7.E3.m1.3.3.1.1.8.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.8.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1q" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1r" xref="Ch7.E3.m1.3.3.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.9.1.1" mathvariant="normal" xref="Ch7.E3.m1.3.3.1.1.9.1.1.cmml">⋮</mi></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1s" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1t" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.10.1.1" xref="Ch7.E3.m1.3.3.1.1.10.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.10.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.10.1.1.2.2.cmml">x</mi><mi id="Ch7.E3.m1.3.3.1.1.10.1.1.3" xref="Ch7.E3.m1.3.3.1.1.10.1.1.3.cmml">n</mi><mo id="Ch7.E3.m1.3.3.1.1.10.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.10.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr><mtr id="Ch7.E3.m1.3.3.1.1u" xref="Ch7.E3.m1.3.3.1.1.cmml"><mtd id="Ch7.E3.m1.3.3.1.1v" xref="Ch7.E3.m1.3.3.1.1.cmml"><msubsup id="Ch7.E3.m1.3.3.1.1.11.1.1" xref="Ch7.E3.m1.3.3.1.1.11.1.1.cmml"><mi id="Ch7.E3.m1.3.3.1.1.11.1.1.2.2" xref="Ch7.E3.m1.3.3.1.1.11.1.1.2.2.cmml">y</mi><mi id="Ch7.E3.m1.3.3.1.1.11.1.1.3" xref="Ch7.E3.m1.3.3.1.1.11.1.1.3.cmml">n</mi><mo id="Ch7.E3.m1.3.3.1.1.11.1.1.2.3" xref="Ch7.E3.m1.3.3.1.1.11.1.1.2.3.cmml">′</mo></msubsup></mtd></mtr></mtable><mo id="Ch7.E3.m1.3.3.3.2" xref="Ch7.E3.m1.3.3.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E3.m1.3b"><apply id="Ch7.E3.m1.3.4.cmml" xref="Ch7.E3.m1.3.4"><eq id="Ch7.E3.m1.3.4.1.cmml" xref="Ch7.E3.m1.3.4.1"></eq><apply id="Ch7.E3.m1.3.4.2.cmml" xref="Ch7.E3.m1.3.4.2"><times id="Ch7.E3.m1.3.4.2.1.cmml" xref="Ch7.E3.m1.3.4.2.1"></times><apply id="Ch7.E3.m1.1.1.2.cmml" xref="Ch7.E3.m1.1.1.3"><csymbol cd="latexml" id="Ch7.E3.m1.1.1.2.1.cmml" xref="Ch7.E3.m1.1.1.3.1">matrix</csymbol><matrix id="Ch7.E3.m1.1.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1"><matrixrow id="Ch7.E3.m1.1.1.1.1a.cmml" xref="Ch7.E3.m1.1.1.1.1"><apply id="Ch7.E3.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.2.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.2.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.2.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.2.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.1.2.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.2.1.3">1</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.1.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.3.1">1</cn><cn id="Ch7.E3.m1.1.1.1.1.1.4.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.4.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.1.5.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.5.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.1.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.6.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.1.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1"><minus id="Ch7.E3.m1.1.1.1.1.1.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.1.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.1.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.2.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.7.1.2.3.3">1</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.1.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1"><minus id="Ch7.E3.m1.1.1.1.1.1.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.1.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.1.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.2.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.1.8.1.2.3.3">1</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1b.cmml" xref="Ch7.E3.m1.1.1.1.1"><cn id="Ch7.E3.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.1.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.2.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.2.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.3.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.2.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.4.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.4.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.4.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.4.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.4.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.2.4.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.4.1.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.2.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.5.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.5.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.5.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.5.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.2.5.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.5.1.3">1</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.2.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.6.1">1</cn><apply id="Ch7.E3.m1.1.1.1.1.2.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1"><minus id="Ch7.E3.m1.1.1.1.1.2.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.2.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.2.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.2.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.7.1.2.3.3">1</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.2.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1"><minus id="Ch7.E3.m1.1.1.1.1.2.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.2.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.2.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.2.3">1</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.2.8.1.2.3.3">1</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1c.cmml" xref="Ch7.E3.m1.1.1.1.1"><apply id="Ch7.E3.m1.1.1.1.1.3.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.1.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.1.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.1.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.3.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.1.1.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.3.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.2.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.2.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.2.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.2.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.3.2.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.2.1.3">2</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.3.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.3.1">1</cn><cn id="Ch7.E3.m1.1.1.1.1.3.4.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.4.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.3.5.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.5.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.3.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.6.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.3.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1"><minus id="Ch7.E3.m1.1.1.1.1.3.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.3.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.3.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.2.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.7.1.2.3.3">2</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.3.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1"><minus id="Ch7.E3.m1.1.1.1.1.3.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.3.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.3.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.2.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.3.8.1.2.3.3">2</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1d.cmml" xref="Ch7.E3.m1.1.1.1.1"><cn id="Ch7.E3.m1.1.1.1.1.4.1.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.1.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.4.2.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.2.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.4.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.3.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.4.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.4.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.4.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.4.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.4.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.4.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.4.4.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.4.1.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.4.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.5.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.5.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.5.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.5.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.4.5.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.5.1.3">2</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.4.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.6.1">1</cn><apply id="Ch7.E3.m1.1.1.1.1.4.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1"><minus id="Ch7.E3.m1.1.1.1.1.4.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.4.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.4.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.2.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.7.1.2.3.3">2</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.4.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1"><minus id="Ch7.E3.m1.1.1.1.1.4.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.4.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.4.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.2.3">2</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.4.8.1.2.3.3">2</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1e.cmml" xref="Ch7.E3.m1.1.1.1.1"><apply id="Ch7.E3.m1.1.1.1.1.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.1.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.1.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.1.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.5.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.1.1.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.5.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.2.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.2.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.2.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.2.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.2.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.5.2.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.2.1.3">3</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.5.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.3.1">1</cn><cn id="Ch7.E3.m1.1.1.1.1.5.4.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.4.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.5.5.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.5.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.5.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.6.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.5.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1"><minus id="Ch7.E3.m1.1.1.1.1.5.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.5.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.5.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.2.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.7.1.2.3.3">3</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.5.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1"><minus id="Ch7.E3.m1.1.1.1.1.5.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.5.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.5.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.2.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.5.8.1.2.3.3">3</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1f.cmml" xref="Ch7.E3.m1.1.1.1.1"><cn id="Ch7.E3.m1.1.1.1.1.6.1.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.1.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.6.2.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.2.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.6.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.3.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.6.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.4.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.4.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.4.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.4.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.4.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.6.4.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.4.1.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.6.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.5.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.5.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.5.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.5.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.6.5.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.5.1.3">3</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.6.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.6.1">1</cn><apply id="Ch7.E3.m1.1.1.1.1.6.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1"><minus id="Ch7.E3.m1.1.1.1.1.6.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.6.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.6.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.2.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.7.1.2.3.3">3</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.6.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1"><minus id="Ch7.E3.m1.1.1.1.1.6.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.6.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.6.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.2.3">3</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.6.8.1.2.3.3">3</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1g.cmml" xref="Ch7.E3.m1.1.1.1.1"><apply id="Ch7.E3.m1.1.1.1.1.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.1.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.1.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.1.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.7.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.1.1.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.7.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.2.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.2.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.2.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.2.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.2.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.7.2.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.2.1.3">4</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.7.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.3.1">1</cn><cn id="Ch7.E3.m1.1.1.1.1.7.4.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.4.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.7.5.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.5.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.7.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.6.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.7.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1"><minus id="Ch7.E3.m1.1.1.1.1.7.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.7.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.7.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.2.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.7.1.2.3.3">4</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.7.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1"><minus id="Ch7.E3.m1.1.1.1.1.7.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.7.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.7.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.2.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.7.8.1.2.3.3">4</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1h.cmml" xref="Ch7.E3.m1.1.1.1.1"><cn id="Ch7.E3.m1.1.1.1.1.8.1.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.1.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.8.2.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.2.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.8.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.3.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.8.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.4.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.4.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.4.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.4.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.4.1.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.8.4.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.4.1.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.8.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.5.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.5.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.5.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.5.1.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.8.5.1.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.5.1.3">4</cn></apply><cn id="Ch7.E3.m1.1.1.1.1.8.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.6.1">1</cn><apply id="Ch7.E3.m1.1.1.1.1.8.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1"><minus id="Ch7.E3.m1.1.1.1.1.8.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.8.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.8.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.2.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.2">𝑥</ci><cn id="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.7.1.2.3.3">4</cn></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.8.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1"><minus id="Ch7.E3.m1.1.1.1.1.8.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.8.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.8.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.2.3">′</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.2.3">4</cn></apply><apply id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.2">𝑦</ci><cn id="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.3.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.8.8.1.2.3.3">4</cn></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1i.cmml" xref="Ch7.E3.m1.1.1.1.1"><ci id="Ch7.E3.m1.1.1.1.1.9.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.1.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.2.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.3.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.4.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.5.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.6.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.6.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.7.1">⋮</ci><ci id="Ch7.E3.m1.1.1.1.1.9.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.9.8.1">⋮</ci></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1j.cmml" xref="Ch7.E3.m1.1.1.1.1"><apply id="Ch7.E3.m1.1.1.1.1.10.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.1.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.1.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.1.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.1.1.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.10.1.1.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.1.1.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.10.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.2.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.2.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.2.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.2.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.2.1.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.10.2.1.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.2.1.3">𝑛</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.10.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.10.3.1">1</cn><cn id="Ch7.E3.m1.1.1.1.1.10.4.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.10.4.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.10.5.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.10.5.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.10.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.10.6.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.10.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1"><minus id="Ch7.E3.m1.1.1.1.1.10.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.10.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.10.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.2.3">′</ci></apply><ci id="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.2.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.7.1.2.3.3">𝑛</ci></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.10.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1"><minus id="Ch7.E3.m1.1.1.1.1.10.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.10.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.10.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.2.3">′</ci></apply><ci id="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.2.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.3.cmml" xref="Ch7.E3.m1.1.1.1.1.10.8.1.2.3.3">𝑛</ci></apply></apply></apply></matrixrow><matrixrow id="Ch7.E3.m1.1.1.1.1k.cmml" xref="Ch7.E3.m1.1.1.1.1"><cn id="Ch7.E3.m1.1.1.1.1.11.1.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.11.1.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.11.2.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.11.2.1">0</cn><cn id="Ch7.E3.m1.1.1.1.1.11.3.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.11.3.1">0</cn><apply id="Ch7.E3.m1.1.1.1.1.11.4.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.4.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.4.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.4.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.4.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.4.1.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.11.4.1.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.4.1.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.11.5.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.5.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.5.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.5.1">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.5.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.5.1.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.11.5.1.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.5.1.3">𝑛</ci></apply><cn id="Ch7.E3.m1.1.1.1.1.11.6.1.cmml" type="integer" xref="Ch7.E3.m1.1.1.1.1.11.6.1">1</cn><apply id="Ch7.E3.m1.1.1.1.1.11.7.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1"><minus id="Ch7.E3.m1.1.1.1.1.11.7.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.11.7.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2"><times id="Ch7.E3.m1.1.1.1.1.11.7.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.2.3">′</ci></apply><ci id="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.2.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.2">𝑥</ci><ci id="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.7.1.2.3.3">𝑛</ci></apply></apply></apply><apply id="Ch7.E3.m1.1.1.1.1.11.8.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1"><minus id="Ch7.E3.m1.1.1.1.1.11.8.1.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1"></minus><apply id="Ch7.E3.m1.1.1.1.1.11.8.1.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2"><times id="Ch7.E3.m1.1.1.1.1.11.8.1.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.1"></times><apply id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2">subscript</csymbol><apply id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2">superscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.2.3">′</ci></apply><ci id="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.2.3">𝑛</ci></apply><apply id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3"><csymbol cd="ambiguous" id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.1.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3">subscript</csymbol><ci id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.2.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.2">𝑦</ci><ci id="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.3.cmml" xref="Ch7.E3.m1.1.1.1.1.11.8.1.2.3.3">𝑛</ci></apply></apply></apply></matrixrow></matrix></apply><apply id="Ch7.E3.m1.2.2.2.cmml" xref="Ch7.E3.m1.2.2.3"><csymbol cd="latexml" id="Ch7.E3.m1.2.2.2.1.cmml" xref="Ch7.E3.m1.2.2.3.1">matrix</csymbol><matrix id="Ch7.E3.m1.2.2.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1"><matrixrow id="Ch7.E3.m1.2.2.1.1a.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.1.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.1.1.1.3">11</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1b.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.2.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.2.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.2.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.2.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.2.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.2.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.2.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.2.1.1.3">12</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1c.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.3.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.3.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.3.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.3.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.3.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.3.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.3.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.3.1.1.3">13</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1d.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.4.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.4.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.4.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.4.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.4.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.4.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.4.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.4.1.1.3">21</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1e.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.5.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.5.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.5.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.5.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.5.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.5.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.5.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.5.1.1.3">22</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1f.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.6.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.6.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.6.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.6.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.6.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.6.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.6.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.6.1.1.3">23</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1g.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.7.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.7.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.7.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.7.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.7.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.7.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.7.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.7.1.1.3">31</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.2.2.1.1h.cmml" xref="Ch7.E3.m1.2.2.1.1"><apply id="Ch7.E3.m1.2.2.1.1.8.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.8.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.2.2.1.1.8.1.1.1.cmml" xref="Ch7.E3.m1.2.2.1.1.8.1.1">subscript</csymbol><ci id="Ch7.E3.m1.2.2.1.1.8.1.1.2.cmml" xref="Ch7.E3.m1.2.2.1.1.8.1.1.2">ℎ</ci><cn id="Ch7.E3.m1.2.2.1.1.8.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.2.2.1.1.8.1.1.3">32</cn></apply></matrixrow></matrix></apply></apply><apply id="Ch7.E3.m1.3.3.2.cmml" xref="Ch7.E3.m1.3.3.3"><csymbol cd="latexml" id="Ch7.E3.m1.3.3.2.1.cmml" xref="Ch7.E3.m1.3.3.3.1">matrix</csymbol><matrix id="Ch7.E3.m1.3.3.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1"><matrixrow id="Ch7.E3.m1.3.3.1.1a.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.1.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.1.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.1.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1.2.2">𝑥</ci><ci id="Ch7.E3.m1.3.3.1.1.1.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.1.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.1.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.1.1.1.3">1</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1b.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.2.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.2.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.2.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.2.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.2.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1.2.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.2.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.2.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.2.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.2.1.1.3">1</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1c.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.3.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.3.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.3.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.3.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.3.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1.2.2">𝑥</ci><ci id="Ch7.E3.m1.3.3.1.1.3.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.3.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.3.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.3.1.1.3">2</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1d.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.4.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.4.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.4.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.4.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.4.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1.2.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.4.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.4.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.4.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.4.1.1.3">2</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1e.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.5.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.5.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.5.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.5.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.5.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1.2.2">𝑥</ci><ci id="Ch7.E3.m1.3.3.1.1.5.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.5.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.5.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.5.1.1.3">3</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1f.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.6.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.6.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.6.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.6.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.6.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1.2.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.6.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.6.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.6.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.6.1.1.3">3</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1g.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.7.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.7.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.7.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.7.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.7.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1.2.2">𝑥</ci><ci id="Ch7.E3.m1.3.3.1.1.7.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.7.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.7.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.7.1.1.3">4</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1h.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.8.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.8.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.8.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.8.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.8.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1.2.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.8.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.8.1.1.2.3">′</ci></apply><cn id="Ch7.E3.m1.3.3.1.1.8.1.1.3.cmml" type="integer" xref="Ch7.E3.m1.3.3.1.1.8.1.1.3">4</cn></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1i.cmml" xref="Ch7.E3.m1.3.3.1.1"><ci id="Ch7.E3.m1.3.3.1.1.9.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.9.1.1">⋮</ci></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1j.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.10.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.10.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.10.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.10.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.10.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1.2.2">𝑥</ci><ci id="Ch7.E3.m1.3.3.1.1.10.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1.2.3">′</ci></apply><ci id="Ch7.E3.m1.3.3.1.1.10.1.1.3.cmml" xref="Ch7.E3.m1.3.3.1.1.10.1.1.3">𝑛</ci></apply></matrixrow><matrixrow id="Ch7.E3.m1.3.3.1.1k.cmml" xref="Ch7.E3.m1.3.3.1.1"><apply id="Ch7.E3.m1.3.3.1.1.11.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.11.1.1.1.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1">subscript</csymbol><apply id="Ch7.E3.m1.3.3.1.1.11.1.1.2.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1"><csymbol cd="ambiguous" id="Ch7.E3.m1.3.3.1.1.11.1.1.2.1.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1">superscript</csymbol><ci id="Ch7.E3.m1.3.3.1.1.11.1.1.2.2.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1.2.2">𝑦</ci><ci id="Ch7.E3.m1.3.3.1.1.11.1.1.2.3.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1.2.3">′</ci></apply><ci id="Ch7.E3.m1.3.3.1.1.11.1.1.3.cmml" xref="Ch7.E3.m1.3.3.1.1.11.1.1.3">𝑛</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E3.m1.3c">\begin{bmatrix}x_{1}&amp;y_{1}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{1}x_{1}&amp;-x^{\prime}_{1}y_{1}\\
0&amp;0&amp;0&amp;x_{1}&amp;y_{1}&amp;1&amp;-y^{\prime}_{1}x_{1}&amp;-y^{\prime}_{1}y_{1}\\
x_{2}&amp;y_{2}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{2}x_{2}&amp;-x^{\prime}_{2}y_{2}\\
0&amp;0&amp;0&amp;x_{2}&amp;y_{2}&amp;1&amp;-y^{\prime}_{2}x_{2}&amp;-y^{\prime}_{2}y_{2}\\
x_{3}&amp;y_{3}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{3}x_{3}&amp;-x^{\prime}_{3}y_{3}\\
0&amp;0&amp;0&amp;x_{3}&amp;y_{3}&amp;1&amp;-y^{\prime}_{3}x_{3}&amp;-y^{\prime}_{3}y_{3}\\
x_{4}&amp;y_{4}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{4}x_{4}&amp;-x^{\prime}_{4}y_{4}\\
0&amp;0&amp;0&amp;x_{4}&amp;y_{4}&amp;1&amp;-y^{\prime}_{4}x_{4}&amp;-y^{\prime}_{4}y_{4}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_{n}&amp;y_{n}&amp;1&amp;0&amp;0&amp;0&amp;-x^{\prime}_{n}x_{n}&amp;-x^{\prime}_{n}y_{n}\\
0&amp;0&amp;0&amp;x_{n}&amp;y_{n}&amp;1&amp;-y^{\prime}_{n}x_{n}&amp;-y^{\prime}_{n}y_{n}\\
\end{bmatrix}\begin{bmatrix}h_{11}\\
h_{12}\\
h_{13}\\
h_{21}\\
h_{22}\\
h_{23}\\
h_{31}\\
h_{32}\\
\end{bmatrix}=\begin{bmatrix}x^{\prime}_{1}\\
y^{\prime}_{1}\\
x^{\prime}_{2}\\
y^{\prime}_{2}\\
x^{\prime}_{3}\\
y^{\prime}_{3}\\
x^{\prime}_{4}\\
y^{\prime}_{4}\\
\vdots\\
x^{\prime}_{n}\\
y^{\prime}_{n}\\
\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="Ch7.E3.m1.3d">[ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL - italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL 1 end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL start_CELL - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_h start_POSTSUBSCRIPT 11 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 13 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 21 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 22 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 23 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 31 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_h start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S1.p9">
<p class="ltx_p" id="Ch7.S1.p9.3">To solve the homography, these equations are stacked for all correspondences to form a system <math alttext="Ah=b" class="ltx_Math" display="inline" id="Ch7.S1.p9.1.m1.1"><semantics id="Ch7.S1.p9.1.m1.1a"><mrow id="Ch7.S1.p9.1.m1.1.1" xref="Ch7.S1.p9.1.m1.1.1.cmml"><mrow id="Ch7.S1.p9.1.m1.1.1.2" xref="Ch7.S1.p9.1.m1.1.1.2.cmml"><mi id="Ch7.S1.p9.1.m1.1.1.2.2" xref="Ch7.S1.p9.1.m1.1.1.2.2.cmml">A</mi><mo id="Ch7.S1.p9.1.m1.1.1.2.1" xref="Ch7.S1.p9.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S1.p9.1.m1.1.1.2.3" xref="Ch7.S1.p9.1.m1.1.1.2.3.cmml">h</mi></mrow><mo id="Ch7.S1.p9.1.m1.1.1.1" xref="Ch7.S1.p9.1.m1.1.1.1.cmml">=</mo><mi id="Ch7.S1.p9.1.m1.1.1.3" xref="Ch7.S1.p9.1.m1.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.1.m1.1b"><apply id="Ch7.S1.p9.1.m1.1.1.cmml" xref="Ch7.S1.p9.1.m1.1.1"><eq id="Ch7.S1.p9.1.m1.1.1.1.cmml" xref="Ch7.S1.p9.1.m1.1.1.1"></eq><apply id="Ch7.S1.p9.1.m1.1.1.2.cmml" xref="Ch7.S1.p9.1.m1.1.1.2"><times id="Ch7.S1.p9.1.m1.1.1.2.1.cmml" xref="Ch7.S1.p9.1.m1.1.1.2.1"></times><ci id="Ch7.S1.p9.1.m1.1.1.2.2.cmml" xref="Ch7.S1.p9.1.m1.1.1.2.2">𝐴</ci><ci id="Ch7.S1.p9.1.m1.1.1.2.3.cmml" xref="Ch7.S1.p9.1.m1.1.1.2.3">ℎ</ci></apply><ci id="Ch7.S1.p9.1.m1.1.1.3.cmml" xref="Ch7.S1.p9.1.m1.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.1.m1.1c">Ah=b</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.1.m1.1d">italic_A italic_h = italic_b</annotation></semantics></math>.
Typically, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.dlt"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.dlt" title="Direct Linear Transformation">Direct Linear Transformation (DLT)</span></a>, <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ransac"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ransac" title="Random Sample Consensus">Random Sample Consensus (RANSAC)</span></a>, or <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ls"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ls" title="Least Squares">Least Squares (LS)</span></a> are employed to find the optimal homography matrix that minimizes the re-projection error between the observed and predicted points.
<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ls"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ls" title="Least Squares">LS</span></a> provided the most accurate results for this application and so was exclusively considered for the remainder of this work.
The LS solution to <math alttext="Ah=b" class="ltx_Math" display="inline" id="Ch7.S1.p9.2.m2.1"><semantics id="Ch7.S1.p9.2.m2.1a"><mrow id="Ch7.S1.p9.2.m2.1.1" xref="Ch7.S1.p9.2.m2.1.1.cmml"><mrow id="Ch7.S1.p9.2.m2.1.1.2" xref="Ch7.S1.p9.2.m2.1.1.2.cmml"><mi id="Ch7.S1.p9.2.m2.1.1.2.2" xref="Ch7.S1.p9.2.m2.1.1.2.2.cmml">A</mi><mo id="Ch7.S1.p9.2.m2.1.1.2.1" xref="Ch7.S1.p9.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S1.p9.2.m2.1.1.2.3" xref="Ch7.S1.p9.2.m2.1.1.2.3.cmml">h</mi></mrow><mo id="Ch7.S1.p9.2.m2.1.1.1" xref="Ch7.S1.p9.2.m2.1.1.1.cmml">=</mo><mi id="Ch7.S1.p9.2.m2.1.1.3" xref="Ch7.S1.p9.2.m2.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.2.m2.1b"><apply id="Ch7.S1.p9.2.m2.1.1.cmml" xref="Ch7.S1.p9.2.m2.1.1"><eq id="Ch7.S1.p9.2.m2.1.1.1.cmml" xref="Ch7.S1.p9.2.m2.1.1.1"></eq><apply id="Ch7.S1.p9.2.m2.1.1.2.cmml" xref="Ch7.S1.p9.2.m2.1.1.2"><times id="Ch7.S1.p9.2.m2.1.1.2.1.cmml" xref="Ch7.S1.p9.2.m2.1.1.2.1"></times><ci id="Ch7.S1.p9.2.m2.1.1.2.2.cmml" xref="Ch7.S1.p9.2.m2.1.1.2.2">𝐴</ci><ci id="Ch7.S1.p9.2.m2.1.1.2.3.cmml" xref="Ch7.S1.p9.2.m2.1.1.2.3">ℎ</ci></apply><ci id="Ch7.S1.p9.2.m2.1.1.3.cmml" xref="Ch7.S1.p9.2.m2.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.2.m2.1c">Ah=b</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.2.m2.1d">italic_A italic_h = italic_b</annotation></semantics></math> is given by minimizing <math alttext="\|Ah-b\|^{2}" class="ltx_Math" display="inline" id="Ch7.S1.p9.3.m3.1"><semantics id="Ch7.S1.p9.3.m3.1a"><msup id="Ch7.S1.p9.3.m3.1.1" xref="Ch7.S1.p9.3.m3.1.1.cmml"><mrow id="Ch7.S1.p9.3.m3.1.1.1.1" xref="Ch7.S1.p9.3.m3.1.1.1.2.cmml"><mo id="Ch7.S1.p9.3.m3.1.1.1.1.2" stretchy="false" xref="Ch7.S1.p9.3.m3.1.1.1.2.1.cmml">‖</mo><mrow id="Ch7.S1.p9.3.m3.1.1.1.1.1" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.cmml"><mrow id="Ch7.S1.p9.3.m3.1.1.1.1.1.2" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.cmml"><mi id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.2" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.2.cmml">A</mi><mo id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.1" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.3" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.3.cmml">h</mi></mrow><mo id="Ch7.S1.p9.3.m3.1.1.1.1.1.1" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.1.cmml">−</mo><mi id="Ch7.S1.p9.3.m3.1.1.1.1.1.3" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="Ch7.S1.p9.3.m3.1.1.1.1.3" stretchy="false" xref="Ch7.S1.p9.3.m3.1.1.1.2.1.cmml">‖</mo></mrow><mn id="Ch7.S1.p9.3.m3.1.1.3" xref="Ch7.S1.p9.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.3.m3.1b"><apply id="Ch7.S1.p9.3.m3.1.1.cmml" xref="Ch7.S1.p9.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S1.p9.3.m3.1.1.2.cmml" xref="Ch7.S1.p9.3.m3.1.1">superscript</csymbol><apply id="Ch7.S1.p9.3.m3.1.1.1.2.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1"><csymbol cd="latexml" id="Ch7.S1.p9.3.m3.1.1.1.2.1.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.2">norm</csymbol><apply id="Ch7.S1.p9.3.m3.1.1.1.1.1.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1"><minus id="Ch7.S1.p9.3.m3.1.1.1.1.1.1.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.1"></minus><apply id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2"><times id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.1.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.1"></times><ci id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.2.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.2">𝐴</ci><ci id="Ch7.S1.p9.3.m3.1.1.1.1.1.2.3.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.2.3">ℎ</ci></apply><ci id="Ch7.S1.p9.3.m3.1.1.1.1.1.3.cmml" xref="Ch7.S1.p9.3.m3.1.1.1.1.1.3">𝑏</ci></apply></apply><cn id="Ch7.S1.p9.3.m3.1.1.3.cmml" type="integer" xref="Ch7.S1.p9.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.3.m3.1c">\|Ah-b\|^{2}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.3.m3.1d">∥ italic_A italic_h - italic_b ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, which leads to the solution:</p>
<table class="ltx_equation ltx_eqn_table" id="Ch7.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h=(A^{T}A)^{-1}A^{T}b" class="ltx_Math" display="block" id="Ch7.E4.m1.1"><semantics id="Ch7.E4.m1.1a"><mrow id="Ch7.E4.m1.1.1" xref="Ch7.E4.m1.1.1.cmml"><mi id="Ch7.E4.m1.1.1.3" xref="Ch7.E4.m1.1.1.3.cmml">h</mi><mo id="Ch7.E4.m1.1.1.2" xref="Ch7.E4.m1.1.1.2.cmml">=</mo><mrow id="Ch7.E4.m1.1.1.1" xref="Ch7.E4.m1.1.1.1.cmml"><msup id="Ch7.E4.m1.1.1.1.1" xref="Ch7.E4.m1.1.1.1.1.cmml"><mrow id="Ch7.E4.m1.1.1.1.1.1.1" xref="Ch7.E4.m1.1.1.1.1.1.1.1.cmml"><mo id="Ch7.E4.m1.1.1.1.1.1.1.2" stretchy="false" xref="Ch7.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Ch7.E4.m1.1.1.1.1.1.1.1" xref="Ch7.E4.m1.1.1.1.1.1.1.1.cmml"><msup id="Ch7.E4.m1.1.1.1.1.1.1.1.2" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2.cmml"><mi id="Ch7.E4.m1.1.1.1.1.1.1.1.2.2" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2.2.cmml">A</mi><mi id="Ch7.E4.m1.1.1.1.1.1.1.1.2.3" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo id="Ch7.E4.m1.1.1.1.1.1.1.1.1" xref="Ch7.E4.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch7.E4.m1.1.1.1.1.1.1.1.3" xref="Ch7.E4.m1.1.1.1.1.1.1.1.3.cmml">A</mi></mrow><mo id="Ch7.E4.m1.1.1.1.1.1.1.3" stretchy="false" xref="Ch7.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="Ch7.E4.m1.1.1.1.1.3" xref="Ch7.E4.m1.1.1.1.1.3.cmml"><mo id="Ch7.E4.m1.1.1.1.1.3a" xref="Ch7.E4.m1.1.1.1.1.3.cmml">−</mo><mn id="Ch7.E4.m1.1.1.1.1.3.2" xref="Ch7.E4.m1.1.1.1.1.3.2.cmml">1</mn></mrow></msup><mo id="Ch7.E4.m1.1.1.1.2" xref="Ch7.E4.m1.1.1.1.2.cmml">⁢</mo><msup id="Ch7.E4.m1.1.1.1.3" xref="Ch7.E4.m1.1.1.1.3.cmml"><mi id="Ch7.E4.m1.1.1.1.3.2" xref="Ch7.E4.m1.1.1.1.3.2.cmml">A</mi><mi id="Ch7.E4.m1.1.1.1.3.3" xref="Ch7.E4.m1.1.1.1.3.3.cmml">T</mi></msup><mo id="Ch7.E4.m1.1.1.1.2a" xref="Ch7.E4.m1.1.1.1.2.cmml">⁢</mo><mi id="Ch7.E4.m1.1.1.1.4" xref="Ch7.E4.m1.1.1.1.4.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E4.m1.1b"><apply id="Ch7.E4.m1.1.1.cmml" xref="Ch7.E4.m1.1.1"><eq id="Ch7.E4.m1.1.1.2.cmml" xref="Ch7.E4.m1.1.1.2"></eq><ci id="Ch7.E4.m1.1.1.3.cmml" xref="Ch7.E4.m1.1.1.3">ℎ</ci><apply id="Ch7.E4.m1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1"><times id="Ch7.E4.m1.1.1.1.2.cmml" xref="Ch7.E4.m1.1.1.1.2"></times><apply id="Ch7.E4.m1.1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.E4.m1.1.1.1.1.2.cmml" xref="Ch7.E4.m1.1.1.1.1">superscript</csymbol><apply id="Ch7.E4.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1"><times id="Ch7.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.1"></times><apply id="Ch7.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Ch7.E4.m1.1.1.1.1.1.1.1.2.1.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="Ch7.E4.m1.1.1.1.1.1.1.1.2.2.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2.2">𝐴</ci><ci id="Ch7.E4.m1.1.1.1.1.1.1.1.2.3.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="Ch7.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="Ch7.E4.m1.1.1.1.1.1.1.1.3">𝐴</ci></apply><apply id="Ch7.E4.m1.1.1.1.1.3.cmml" xref="Ch7.E4.m1.1.1.1.1.3"><minus id="Ch7.E4.m1.1.1.1.1.3.1.cmml" xref="Ch7.E4.m1.1.1.1.1.3"></minus><cn id="Ch7.E4.m1.1.1.1.1.3.2.cmml" type="integer" xref="Ch7.E4.m1.1.1.1.1.3.2">1</cn></apply></apply><apply id="Ch7.E4.m1.1.1.1.3.cmml" xref="Ch7.E4.m1.1.1.1.3"><csymbol cd="ambiguous" id="Ch7.E4.m1.1.1.1.3.1.cmml" xref="Ch7.E4.m1.1.1.1.3">superscript</csymbol><ci id="Ch7.E4.m1.1.1.1.3.2.cmml" xref="Ch7.E4.m1.1.1.1.3.2">𝐴</ci><ci id="Ch7.E4.m1.1.1.1.3.3.cmml" xref="Ch7.E4.m1.1.1.1.3.3">𝑇</ci></apply><ci id="Ch7.E4.m1.1.1.1.4.cmml" xref="Ch7.E4.m1.1.1.1.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E4.m1.1c">h=(A^{T}A)^{-1}A^{T}b</annotation><annotation encoding="application/x-llamapun" id="Ch7.E4.m1.1d">italic_h = ( italic_A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_b</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Ch7.S1.p9.8">where <math alttext="h" class="ltx_Math" display="inline" id="Ch7.S1.p9.4.m1.1"><semantics id="Ch7.S1.p9.4.m1.1a"><mi id="Ch7.S1.p9.4.m1.1.1" xref="Ch7.S1.p9.4.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.4.m1.1b"><ci id="Ch7.S1.p9.4.m1.1.1.cmml" xref="Ch7.S1.p9.4.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.4.m1.1c">h</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.4.m1.1d">italic_h</annotation></semantics></math> is the vector containing the eight unknown elements of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref ltx_markedasmath ltx_font_italic" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">H</span></a>.
Finally, <math alttext="h" class="ltx_Math" display="inline" id="Ch7.S1.p9.6.m3.1"><semantics id="Ch7.S1.p9.6.m3.1a"><mi id="Ch7.S1.p9.6.m3.1.1" xref="Ch7.S1.p9.6.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.6.m3.1b"><ci id="Ch7.S1.p9.6.m3.1.1.cmml" xref="Ch7.S1.p9.6.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.6.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.6.m3.1d">italic_h</annotation></semantics></math> is reshaped back into the <math alttext="3\times 3" class="ltx_Math" display="inline" id="Ch7.S1.p9.7.m4.1"><semantics id="Ch7.S1.p9.7.m4.1a"><mrow id="Ch7.S1.p9.7.m4.1.1" xref="Ch7.S1.p9.7.m4.1.1.cmml"><mn id="Ch7.S1.p9.7.m4.1.1.2" xref="Ch7.S1.p9.7.m4.1.1.2.cmml">3</mn><mo id="Ch7.S1.p9.7.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch7.S1.p9.7.m4.1.1.1.cmml">×</mo><mn id="Ch7.S1.p9.7.m4.1.1.3" xref="Ch7.S1.p9.7.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p9.7.m4.1b"><apply id="Ch7.S1.p9.7.m4.1.1.cmml" xref="Ch7.S1.p9.7.m4.1.1"><times id="Ch7.S1.p9.7.m4.1.1.1.cmml" xref="Ch7.S1.p9.7.m4.1.1.1"></times><cn id="Ch7.S1.p9.7.m4.1.1.2.cmml" type="integer" xref="Ch7.S1.p9.7.m4.1.1.2">3</cn><cn id="Ch7.S1.p9.7.m4.1.1.3.cmml" type="integer" xref="Ch7.S1.p9.7.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p9.7.m4.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p9.7.m4.1d">3 × 3</annotation></semantics></math> homography matrix <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref ltx_markedasmath ltx_font_italic" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">H</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p10">
<p class="ltx_p" id="Ch7.S1.p10.1">Once the homography matrix <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref ltx_markedasmath ltx_font_italic" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">H</span></a> is created, it can be used as shown in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.E2" title="Equation 7.2 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a>.
In computer vision, homography has been extensively utilized across a range of applications.
These include image stitching in panoramic photography, feature matching, camera calibration, 3D reconstruction, augmented reality for overlaying virtual objects onto real-world scenes, motion estimation in video sequences, and perspective correction for rectifying images of planar surfaces.
In the field of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gis"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gis" title="Geographic Information System">Geographic Information System (GIS)</span></a>, it has been used to map ground surfaces in surveillance video frames to topographies that are small enough (few hundred meters range) to be approximated as a plane <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib121" title="">xie2017integration </a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib122" title="">shao2020accurate </a></cite>.
However they do not provide quantitative assessments of their accuracy.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p11">
<p class="ltx_p" id="Ch7.S1.p11.5">The ship georeferencing methodology proposed in this thesis also treats the Earth’s surface observed by the camera as a planar area for simplification. For example, in the case of ShipSG, given the 23 meter altitude of the cameras and considering the georeferencing range of up to 1200 meters on the Weser river, the curvature of the Earth introduces a minimal height difference. This difference can be approximated by <math alttext="h\approx\frac{d^{2}}{2r}" class="ltx_Math" display="inline" id="Ch7.S1.p11.1.m1.1"><semantics id="Ch7.S1.p11.1.m1.1a"><mrow id="Ch7.S1.p11.1.m1.1.1" xref="Ch7.S1.p11.1.m1.1.1.cmml"><mi id="Ch7.S1.p11.1.m1.1.1.2" xref="Ch7.S1.p11.1.m1.1.1.2.cmml">h</mi><mo id="Ch7.S1.p11.1.m1.1.1.1" xref="Ch7.S1.p11.1.m1.1.1.1.cmml">≈</mo><mfrac id="Ch7.S1.p11.1.m1.1.1.3" xref="Ch7.S1.p11.1.m1.1.1.3.cmml"><msup id="Ch7.S1.p11.1.m1.1.1.3.2" xref="Ch7.S1.p11.1.m1.1.1.3.2.cmml"><mi id="Ch7.S1.p11.1.m1.1.1.3.2.2" xref="Ch7.S1.p11.1.m1.1.1.3.2.2.cmml">d</mi><mn id="Ch7.S1.p11.1.m1.1.1.3.2.3" xref="Ch7.S1.p11.1.m1.1.1.3.2.3.cmml">2</mn></msup><mrow id="Ch7.S1.p11.1.m1.1.1.3.3" xref="Ch7.S1.p11.1.m1.1.1.3.3.cmml"><mn id="Ch7.S1.p11.1.m1.1.1.3.3.2" xref="Ch7.S1.p11.1.m1.1.1.3.3.2.cmml">2</mn><mo id="Ch7.S1.p11.1.m1.1.1.3.3.1" xref="Ch7.S1.p11.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="Ch7.S1.p11.1.m1.1.1.3.3.3" xref="Ch7.S1.p11.1.m1.1.1.3.3.3.cmml">r</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S1.p11.1.m1.1b"><apply id="Ch7.S1.p11.1.m1.1.1.cmml" xref="Ch7.S1.p11.1.m1.1.1"><approx id="Ch7.S1.p11.1.m1.1.1.1.cmml" xref="Ch7.S1.p11.1.m1.1.1.1"></approx><ci id="Ch7.S1.p11.1.m1.1.1.2.cmml" xref="Ch7.S1.p11.1.m1.1.1.2">ℎ</ci><apply id="Ch7.S1.p11.1.m1.1.1.3.cmml" xref="Ch7.S1.p11.1.m1.1.1.3"><divide id="Ch7.S1.p11.1.m1.1.1.3.1.cmml" xref="Ch7.S1.p11.1.m1.1.1.3"></divide><apply id="Ch7.S1.p11.1.m1.1.1.3.2.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="Ch7.S1.p11.1.m1.1.1.3.2.1.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.2">superscript</csymbol><ci id="Ch7.S1.p11.1.m1.1.1.3.2.2.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.2.2">𝑑</ci><cn id="Ch7.S1.p11.1.m1.1.1.3.2.3.cmml" type="integer" xref="Ch7.S1.p11.1.m1.1.1.3.2.3">2</cn></apply><apply id="Ch7.S1.p11.1.m1.1.1.3.3.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.3"><times id="Ch7.S1.p11.1.m1.1.1.3.3.1.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.3.1"></times><cn id="Ch7.S1.p11.1.m1.1.1.3.3.2.cmml" type="integer" xref="Ch7.S1.p11.1.m1.1.1.3.3.2">2</cn><ci id="Ch7.S1.p11.1.m1.1.1.3.3.3.cmml" xref="Ch7.S1.p11.1.m1.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p11.1.m1.1c">h\approx\frac{d^{2}}{2r}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p11.1.m1.1d">italic_h ≈ divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_r end_ARG</annotation></semantics></math>, where <math alttext="d" class="ltx_Math" display="inline" id="Ch7.S1.p11.2.m2.1"><semantics id="Ch7.S1.p11.2.m2.1a"><mi id="Ch7.S1.p11.2.m2.1.1" xref="Ch7.S1.p11.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p11.2.m2.1b"><ci id="Ch7.S1.p11.2.m2.1.1.cmml" xref="Ch7.S1.p11.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p11.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p11.2.m2.1d">italic_d</annotation></semantics></math> is the horizontal distance between two points on the Earth’s surface, and <math alttext="r" class="ltx_Math" display="inline" id="Ch7.S1.p11.3.m3.1"><semantics id="Ch7.S1.p11.3.m3.1a"><mi id="Ch7.S1.p11.3.m3.1.1" xref="Ch7.S1.p11.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p11.3.m3.1b"><ci id="Ch7.S1.p11.3.m3.1.1.cmml" xref="Ch7.S1.p11.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p11.3.m3.1c">r</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p11.3.m3.1d">italic_r</annotation></semantics></math> is the radius of the Earth. This formula derives from the geometric properties of a circle, where the Earth’s curvature over a small distance can be represented as the segment height (<math alttext="h" class="ltx_Math" display="inline" id="Ch7.S1.p11.4.m4.1"><semantics id="Ch7.S1.p11.4.m4.1a"><mi id="Ch7.S1.p11.4.m4.1.1" xref="Ch7.S1.p11.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p11.4.m4.1b"><ci id="Ch7.S1.p11.4.m4.1.1.cmml" xref="Ch7.S1.p11.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p11.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p11.4.m4.1d">italic_h</annotation></semantics></math>) of a circular segment with radius <math alttext="r" class="ltx_Math" display="inline" id="Ch7.S1.p11.5.m5.1"><semantics id="Ch7.S1.p11.5.m5.1a"><mi id="Ch7.S1.p11.5.m5.1.1" xref="Ch7.S1.p11.5.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="Ch7.S1.p11.5.m5.1b"><ci id="Ch7.S1.p11.5.m5.1.1.cmml" xref="Ch7.S1.p11.5.m5.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S1.p11.5.m5.1c">r</annotation><annotation encoding="application/x-llamapun" id="Ch7.S1.p11.5.m5.1d">italic_r</annotation></semantics></math>. For the observation range of 1200 meters, this formula gives the height difference due to curvature to be approximately 0.113 meters. This value is significantly small, especially when compared to the elevation of the camera and and length of the ships. Consequently, we simplify the water surface visible from the cameras as the tangent plane to the Earth’s curvature.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p12">
<p class="ltx_p" id="Ch7.S1.p12.1">By establishing correspondences between known geographic points on the water surface (e.g., buoys, landmarks <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, or <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">Automatic Identification System (AIS)</span></a> signals from ships <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>) and their representations in the image frame, a homography matrix can be computed.
This matrix then allows for any point captured on the water surface to be mapped to geographic coordinates.
This method not only facilitates the accurate mapping of static water surfaces but also paves the way for dynamic georeferencing applications, such as real-time ship recognition.
By leveraging the homography matrix, ships on the water can be precisely georeferenced, offering valuable insights for maritime situational awareness and bridging the digital and physical worlds.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p13">
<p class="ltx_p" id="Ch7.S1.p13.1">The static view of a monitoring camera allows the water surface captured in the image, with pixel coordinates, to be transformed to the water surface with geographic latitude and longitude coordinates.
Homography offers therefore a potent solution that enables ship georeferencing without the need for detailed camera calibration (i.e., intrinsic or extrinsic parameters).
The work in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> explores the homography-based method, and an experimental analysis of the method is presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.
A significant advantage of the proposed georeferencing method, lies in its applicability to any existing camera setup, provided there are identifiable reference points on the surface to create the homography.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p14">
<p class="ltx_p" id="Ch7.S1.p14.1">In the following sections of this chapter, we explore my proposed method to use homography for the mapping of ships on water surfaces captured by monitoring camera images.
We will delve into the detailed methodologies for recognizing and georeferencing ships on the water, further illustrating the practical implications and benefits of homography in real-world scenarios.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">7.2   Ship Detection and Georeferencing Using Homographies <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>
</h3>
<div class="ltx_para" id="Ch7.S2.p1">
<p class="ltx_p" id="Ch7.S2.p1.1">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>, the vessel detector plays a key role within the anomaly detection framework presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> as it identifies vessels and ships in video data, enabling accurate mapping of vessel locations using georeferencing. The motion detector leverages the ship’s bounding box and uses optical flow for motion detection. Optical flow analyzes pixel intensity changes between sequential frames to quantify displacement vectors, indicating motion. I use this displacement to indicate the course of the ship (heading).</p>
</div>
<div class="ltx_para" id="Ch7.S2.p2">
<p class="ltx_p" id="Ch7.S2.p2.1">Once the vessels are detected per video frame using YOLOv4-CSP <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib67" title="">wang2021scaled </a></cite> (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>), the pixel-based locations and course estimations (heading) are translated to a geographic coordinate system using a homography.
This georeferencing process allows for further analytics pursuing vessel abnormal behavior interpretation and for visualization on the situational awareness tool in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch7.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="251" id="Ch7.F2.g1" src="extracted/5906916/fig/homography_paper1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F2.2.1.1" style="font-size:90%;">Figure 7.2</span>: </span><span class="ltx_text" id="Ch7.F2.3.2" style="font-size:90%;">Representation of the two planes used to create the homography of publication <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. Left: camera view. Right: geographic coordinate system. The white surfaces show the same water plane on both views. The red pins correspond with the selected points on both planes to calculate our homography matrix H. The yellow pin shows the location of the camera. Modified from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. ©2021 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S2.p3">
<p class="ltx_p" id="Ch7.S2.p3.1">Given that the video used in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a> has a static perspective, the pixels representing the water surface within its field of view can be treated as a planar area (as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S1" title="7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.1</span></a>), where all detections of vessels occur.
The generation of <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.h"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.h" title="Homography">H</span></a> from these two planes is done with selected visual landmarks as reference points (see red pins in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F2" title="Figure 7.2 ‣ 7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a>), that are used to solve the linear system specified by Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.E3" title="Equation 7.3 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.3</span></a>.
These references are pixel coordinates from the camera view and latitudes and longitudes, in decimal degrees, of the geographic coordinate system.
Once the homography is solved, by taking the center of the bounding boxes provided by the YOLOv4-CSP detector, the georeferencing of vessel positions is possible.
The georeferencing, together with the YOLOv4-CSP object detector, allows the geo-location of observed anomalies in the scene and the calculation of vessel heading for their display on a map.</p>
</div>
<div class="ltx_para" id="Ch7.S2.p4">
<p class="ltx_p" id="Ch7.S2.p4.1">As discussed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a>, an additional contribution to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, along with vessel detection and georeferencing, is the calculation of the heading direction of the vessels. The course of a vessel is defined by its steering direction with respect to the geographic north pole, also called the heading angle.</p>
</div>
<figure class="ltx_figure" id="Ch7.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="237" id="Ch7.F3.g1" src="extracted/5906916/fig/opt_flow.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F3.2.1.1" style="font-size:90%;">Figure 7.3</span>: </span><span class="ltx_text" id="Ch7.F3.3.2" style="font-size:90%;">Illustrative representation of detected and georeferenced vessel with heading performed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. Left: example of detection using YOLOv4-CSP. The blue arrows represent displacement vectors (optical flow) with respect to the previous frame, and the green arrow the median direction within the bounding box. The red dots represent the georeferenced points and are defined by the median direction, from the center to the cutting point with the bounding box edge. Right: geographic representation (OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib107" title="">OpenStreetMap </a></cite>) of the vessel heading angle with respect to north using the created homography.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S2.p5">
<p class="ltx_p" id="Ch7.S2.p5.1">In the framework presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, Brox’s optical flow <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib123" title="">brox2004high </a></cite> is calculated to train the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gan"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gan" title="Generative Adversarial Network">Generative Adversarial Network (GAN)</span></a> that performs unsupervised anomaly detection.
The optical flow was used to determine the direction of the displacement vectors of the detected bounding boxes, represented by blue arrows in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F3" title="Figure 7.3 ‣ 7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.3</span></a> (left). This allows the estimation of the angle of the course of the vessels (heading) using georeferencing.
From the displacement vectors, the main direction of motion is determined from the median of all displacement directions within the bounding box, represented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F3" title="Figure 7.3 ‣ 7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.3</span></a> by the green arrow.
The homography created is then multiplied by the bounding box center and the tip of the displacement arrow to obtain their corresponding geographic coordinates.
The tip of the arrow is defined by the cutting point of the median displacement direction with the bounding box edge.
Once the two points (center and tip) are georeferenced, the heading angle (<math alttext="\theta" class="ltx_Math" display="inline" id="Ch7.S2.p5.1.m1.1"><semantics id="Ch7.S2.p5.1.m1.1a"><mi id="Ch7.S2.p5.1.m1.1.1" xref="Ch7.S2.p5.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Ch7.S2.p5.1.m1.1b"><ci id="Ch7.S2.p5.1.m1.1.1.cmml" xref="Ch7.S2.p5.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p5.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p5.1.m1.1d">italic_θ</annotation></semantics></math>) is calculated to obtain the course of the vessel using:</p>
</div>
<div class="ltx_para" id="Ch7.S2.p6">
<table class="ltx_equation ltx_eqn_table" id="Ch7.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta=\text{atan2}\left(\sin\Delta\lambda\cdot\cos\phi_{2},\cos\phi_{1}\cdot%
\sin\phi_{2}-\sin\phi_{1}\cdot\cos\phi_{2}\cdot\cos\Delta\lambda\right)" class="ltx_Math" display="block" id="Ch7.E5.m1.2"><semantics id="Ch7.E5.m1.2a"><mrow id="Ch7.E5.m1.2.2" xref="Ch7.E5.m1.2.2.cmml"><mi id="Ch7.E5.m1.2.2.4" xref="Ch7.E5.m1.2.2.4.cmml">θ</mi><mo id="Ch7.E5.m1.2.2.3" xref="Ch7.E5.m1.2.2.3.cmml">=</mo><mrow id="Ch7.E5.m1.2.2.2" xref="Ch7.E5.m1.2.2.2.cmml"><mtext id="Ch7.E5.m1.2.2.2.4" xref="Ch7.E5.m1.2.2.2.4a.cmml">atan2</mtext><mo id="Ch7.E5.m1.2.2.2.3" xref="Ch7.E5.m1.2.2.2.3.cmml">⁢</mo><mrow id="Ch7.E5.m1.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.3.cmml"><mo id="Ch7.E5.m1.2.2.2.2.2.3" xref="Ch7.E5.m1.2.2.2.2.3.cmml">(</mo><mrow id="Ch7.E5.m1.1.1.1.1.1.1" xref="Ch7.E5.m1.1.1.1.1.1.1.cmml"><mrow id="Ch7.E5.m1.1.1.1.1.1.1.2" xref="Ch7.E5.m1.1.1.1.1.1.1.2.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.2.1" xref="Ch7.E5.m1.1.1.1.1.1.1.2.1.cmml">sin</mi><mo id="Ch7.E5.m1.1.1.1.1.1.1.2a" lspace="0.167em" xref="Ch7.E5.m1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="Ch7.E5.m1.1.1.1.1.1.1.2.2" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.2.2.2" mathvariant="normal" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.2.cmml">Δ</mi><mo id="Ch7.E5.m1.1.1.1.1.1.1.2.2.1" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="Ch7.E5.m1.1.1.1.1.1.1.2.2.3" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.3.cmml">λ</mi></mrow></mrow><mo id="Ch7.E5.m1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E5.m1.1.1.1.1.1.1.1.cmml">⋅</mo><mrow id="Ch7.E5.m1.1.1.1.1.1.1.3" xref="Ch7.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.3.1" xref="Ch7.E5.m1.1.1.1.1.1.1.3.1.cmml">cos</mi><mo id="Ch7.E5.m1.1.1.1.1.1.1.3a" lspace="0.167em" xref="Ch7.E5.m1.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="Ch7.E5.m1.1.1.1.1.1.1.3.2" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.cmml"><mi id="Ch7.E5.m1.1.1.1.1.1.1.3.2.2" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.2.cmml">ϕ</mi><mn id="Ch7.E5.m1.1.1.1.1.1.1.3.2.3" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.3.cmml">2</mn></msub></mrow></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.4" xref="Ch7.E5.m1.2.2.2.2.3.cmml">,</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.cmml"><mrow id="Ch7.E5.m1.2.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.cmml"><mrow id="Ch7.E5.m1.2.2.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.2.2.1" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.1.cmml">cos</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.2.2a" lspace="0.167em" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.cmml">⁡</mo><msub id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.2.cmml">ϕ</mi><mn id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.3.cmml">1</mn></msub></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E5.m1.2.2.2.2.2.2.2.1.cmml">⋅</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.2.3.1" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.1.cmml">sin</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.2.3a" lspace="0.167em" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.cmml">⁡</mo><msub id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.2.cmml">ϕ</mi><mn id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.3.cmml">2</mn></msub></mrow></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.2.1" xref="Ch7.E5.m1.2.2.2.2.2.2.1.cmml">−</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.cmml"><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.2.1" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.1.cmml">sin</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.2a" lspace="0.167em" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.cmml">⁡</mo><msub id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.2.cmml">ϕ</mi><mn id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.3.cmml">1</mn></msub></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E5.m1.2.2.2.2.2.2.3.1.cmml">⋅</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.3.1" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.1.cmml">cos</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.3a" lspace="0.167em" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.cmml">⁡</mo><msub id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.2.cmml">ϕ</mi><mn id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.3.cmml">2</mn></msub></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.1a" lspace="0.222em" rspace="0.222em" xref="Ch7.E5.m1.2.2.2.2.2.2.3.1.cmml">⋅</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3.4" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.4.1" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.1.cmml">cos</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.4a" lspace="0.167em" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.cmml">⁡</mo><mrow id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.cmml"><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.2" mathvariant="normal" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.2.cmml">Δ</mi><mo id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.1" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.1.cmml">⁢</mo><mi id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.3" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.3.cmml">λ</mi></mrow></mrow></mrow></mrow><mo id="Ch7.E5.m1.2.2.2.2.2.5" xref="Ch7.E5.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E5.m1.2b"><apply id="Ch7.E5.m1.2.2.cmml" xref="Ch7.E5.m1.2.2"><eq id="Ch7.E5.m1.2.2.3.cmml" xref="Ch7.E5.m1.2.2.3"></eq><ci id="Ch7.E5.m1.2.2.4.cmml" xref="Ch7.E5.m1.2.2.4">𝜃</ci><apply id="Ch7.E5.m1.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2"><times id="Ch7.E5.m1.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.3"></times><ci id="Ch7.E5.m1.2.2.2.4a.cmml" xref="Ch7.E5.m1.2.2.2.4"><mtext id="Ch7.E5.m1.2.2.2.4.cmml" xref="Ch7.E5.m1.2.2.2.4">atan2</mtext></ci><interval closure="open" id="Ch7.E5.m1.2.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2"><apply id="Ch7.E5.m1.1.1.1.1.1.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1"><ci id="Ch7.E5.m1.1.1.1.1.1.1.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.1">⋅</ci><apply id="Ch7.E5.m1.1.1.1.1.1.1.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2"><sin id="Ch7.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2.1"></sin><apply id="Ch7.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2"><times id="Ch7.E5.m1.1.1.1.1.1.1.2.2.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.1"></times><ci id="Ch7.E5.m1.1.1.1.1.1.1.2.2.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.2">Δ</ci><ci id="Ch7.E5.m1.1.1.1.1.1.1.2.2.3.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.2.2.3">𝜆</ci></apply></apply><apply id="Ch7.E5.m1.1.1.1.1.1.1.3.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3"><cos id="Ch7.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.1"></cos><apply id="Ch7.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.1.1.1.1.1.1.3.2.1.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="Ch7.E5.m1.1.1.1.1.1.1.3.2.2.cmml" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.2">italic-ϕ</ci><cn id="Ch7.E5.m1.1.1.1.1.1.1.3.2.3.cmml" type="integer" xref="Ch7.E5.m1.1.1.1.1.1.1.3.2.3">2</cn></apply></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2"><minus id="Ch7.E5.m1.2.2.2.2.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.1"></minus><apply id="Ch7.E5.m1.2.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2"><ci id="Ch7.E5.m1.2.2.2.2.2.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.1">⋅</ci><apply id="Ch7.E5.m1.2.2.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2"><cos id="Ch7.E5.m1.2.2.2.2.2.2.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.1"></cos><apply id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.2">italic-ϕ</ci><cn id="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.3.cmml" type="integer" xref="Ch7.E5.m1.2.2.2.2.2.2.2.2.2.3">1</cn></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3"><sin id="Ch7.E5.m1.2.2.2.2.2.2.2.3.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.1"></sin><apply id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.2">italic-ϕ</ci><cn id="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.3.cmml" type="integer" xref="Ch7.E5.m1.2.2.2.2.2.2.2.3.2.3">2</cn></apply></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3"><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.1">⋅</ci><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2"><sin id="Ch7.E5.m1.2.2.2.2.2.2.3.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.1"></sin><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.2">italic-ϕ</ci><cn id="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.3.cmml" type="integer" xref="Ch7.E5.m1.2.2.2.2.2.2.3.2.2.3">1</cn></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3"><cos id="Ch7.E5.m1.2.2.2.2.2.2.3.3.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.1"></cos><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2"><csymbol cd="ambiguous" id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2">subscript</csymbol><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.2">italic-ϕ</ci><cn id="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.3.cmml" type="integer" xref="Ch7.E5.m1.2.2.2.2.2.2.3.3.2.3">2</cn></apply></apply><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.4.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4"><cos id="Ch7.E5.m1.2.2.2.2.2.2.3.4.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.1"></cos><apply id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2"><times id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.1.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.1"></times><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.2.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.2">Δ</ci><ci id="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.3.cmml" xref="Ch7.E5.m1.2.2.2.2.2.2.3.4.2.3">𝜆</ci></apply></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E5.m1.2c">\theta=\text{atan2}\left(\sin\Delta\lambda\cdot\cos\phi_{2},\cos\phi_{1}\cdot%
\sin\phi_{2}-\sin\phi_{1}\cdot\cos\phi_{2}\cdot\cos\Delta\lambda\right)</annotation><annotation encoding="application/x-llamapun" id="Ch7.E5.m1.2d">italic_θ = atan2 ( roman_sin roman_Δ italic_λ ⋅ roman_cos italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , roman_cos italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ roman_sin italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - roman_sin italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ roman_cos italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⋅ roman_cos roman_Δ italic_λ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S2.p7">
<p class="ltx_p" id="Ch7.S2.p7.5">where (<math alttext="\phi_{1}" class="ltx_Math" display="inline" id="Ch7.S2.p7.1.m1.1"><semantics id="Ch7.S2.p7.1.m1.1a"><msub id="Ch7.S2.p7.1.m1.1.1" xref="Ch7.S2.p7.1.m1.1.1.cmml"><mi id="Ch7.S2.p7.1.m1.1.1.2" xref="Ch7.S2.p7.1.m1.1.1.2.cmml">ϕ</mi><mn id="Ch7.S2.p7.1.m1.1.1.3" xref="Ch7.S2.p7.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch7.S2.p7.1.m1.1b"><apply id="Ch7.S2.p7.1.m1.1.1.cmml" xref="Ch7.S2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="Ch7.S2.p7.1.m1.1.1.1.cmml" xref="Ch7.S2.p7.1.m1.1.1">subscript</csymbol><ci id="Ch7.S2.p7.1.m1.1.1.2.cmml" xref="Ch7.S2.p7.1.m1.1.1.2">italic-ϕ</ci><cn id="Ch7.S2.p7.1.m1.1.1.3.cmml" type="integer" xref="Ch7.S2.p7.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p7.1.m1.1c">\phi_{1}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p7.1.m1.1d">italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\lambda_{1}" class="ltx_Math" display="inline" id="Ch7.S2.p7.2.m2.1"><semantics id="Ch7.S2.p7.2.m2.1a"><msub id="Ch7.S2.p7.2.m2.1.1" xref="Ch7.S2.p7.2.m2.1.1.cmml"><mi id="Ch7.S2.p7.2.m2.1.1.2" xref="Ch7.S2.p7.2.m2.1.1.2.cmml">λ</mi><mn id="Ch7.S2.p7.2.m2.1.1.3" xref="Ch7.S2.p7.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch7.S2.p7.2.m2.1b"><apply id="Ch7.S2.p7.2.m2.1.1.cmml" xref="Ch7.S2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S2.p7.2.m2.1.1.1.cmml" xref="Ch7.S2.p7.2.m2.1.1">subscript</csymbol><ci id="Ch7.S2.p7.2.m2.1.1.2.cmml" xref="Ch7.S2.p7.2.m2.1.1.2">𝜆</ci><cn id="Ch7.S2.p7.2.m2.1.1.3.cmml" type="integer" xref="Ch7.S2.p7.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p7.2.m2.1c">\lambda_{1}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p7.2.m2.1d">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) and (<math alttext="\phi_{2}" class="ltx_Math" display="inline" id="Ch7.S2.p7.3.m3.1"><semantics id="Ch7.S2.p7.3.m3.1a"><msub id="Ch7.S2.p7.3.m3.1.1" xref="Ch7.S2.p7.3.m3.1.1.cmml"><mi id="Ch7.S2.p7.3.m3.1.1.2" xref="Ch7.S2.p7.3.m3.1.1.2.cmml">ϕ</mi><mn id="Ch7.S2.p7.3.m3.1.1.3" xref="Ch7.S2.p7.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch7.S2.p7.3.m3.1b"><apply id="Ch7.S2.p7.3.m3.1.1.cmml" xref="Ch7.S2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S2.p7.3.m3.1.1.1.cmml" xref="Ch7.S2.p7.3.m3.1.1">subscript</csymbol><ci id="Ch7.S2.p7.3.m3.1.1.2.cmml" xref="Ch7.S2.p7.3.m3.1.1.2">italic-ϕ</ci><cn id="Ch7.S2.p7.3.m3.1.1.3.cmml" type="integer" xref="Ch7.S2.p7.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p7.3.m3.1c">\phi_{2}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p7.3.m3.1d">italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\lambda_{2}" class="ltx_Math" display="inline" id="Ch7.S2.p7.4.m4.1"><semantics id="Ch7.S2.p7.4.m4.1a"><msub id="Ch7.S2.p7.4.m4.1.1" xref="Ch7.S2.p7.4.m4.1.1.cmml"><mi id="Ch7.S2.p7.4.m4.1.1.2" xref="Ch7.S2.p7.4.m4.1.1.2.cmml">λ</mi><mn id="Ch7.S2.p7.4.m4.1.1.3" xref="Ch7.S2.p7.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch7.S2.p7.4.m4.1b"><apply id="Ch7.S2.p7.4.m4.1.1.cmml" xref="Ch7.S2.p7.4.m4.1.1"><csymbol cd="ambiguous" id="Ch7.S2.p7.4.m4.1.1.1.cmml" xref="Ch7.S2.p7.4.m4.1.1">subscript</csymbol><ci id="Ch7.S2.p7.4.m4.1.1.2.cmml" xref="Ch7.S2.p7.4.m4.1.1.2">𝜆</ci><cn id="Ch7.S2.p7.4.m4.1.1.3.cmml" type="integer" xref="Ch7.S2.p7.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p7.4.m4.1c">\lambda_{2}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p7.4.m4.1d">italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>) represent the coordinates of the georeferenced points (latitude and longitude, respectively), and <math alttext="\Delta" class="ltx_Math" display="inline" id="Ch7.S2.p7.5.m5.1"><semantics id="Ch7.S2.p7.5.m5.1a"><mi id="Ch7.S2.p7.5.m5.1.1" mathvariant="normal" xref="Ch7.S2.p7.5.m5.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="Ch7.S2.p7.5.m5.1b"><ci id="Ch7.S2.p7.5.m5.1.1.cmml" xref="Ch7.S2.p7.5.m5.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S2.p7.5.m5.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="Ch7.S2.p7.5.m5.1d">roman_Δ</annotation></semantics></math> represents difference.</p>
</div>
<figure class="ltx_figure" id="Ch7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="252" id="Ch7.F4.g1" src="extracted/5906916/fig/worldwind.jpg" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F4.2.1.1" style="font-size:90%;">Figure 7.4</span>: </span><span class="ltx_text" id="Ch7.F4.3.2" style="font-size:90%;">Visualization of detected and georeferenced vessels with heading. The vessels detected are provided to a web service map (WorldWind <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib124" title="">bell2007nasa </a></cite>) in the form of triangles pointing towards their motion direction. Vessels for which the motion displacement was near zero are represented by circles. The rest of the figure represents the anomaly detection and visualization pipeline presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. The red cell represents the interpreted area in which the anomalous ship is navigating. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>. ©2021 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S2.p8">
<p class="ltx_p" id="Ch7.S2.p8.1">For display purposes, each vessel detected and georeferenced, is converted to an isosceles triangle, centered around the detection coordinate and whose vertex is pointing in the direction determined by the heading angle.
Lastly, the three vertices are provided to a web service for map visualization (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F4" title="Figure 7.4 ‣ 7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.4</span></a>).</p>
</div>
<div class="ltx_para" id="Ch7.S2.p9">
<p class="ltx_p" id="Ch7.S2.p9.1">This section completes my contributions to <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>.
These include vessel detection using YOLOv4-CSP (see Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S1.SS1" title="5.1.1 Abnormal Vessel Behaviour from Video [BCP-I] ‣ 5.1 Ship Detection for Maritime Applications ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>), vessel heading calculation using optical flow and georeference for map visualization using a homography.
These contributions are paramount to contextualize the identification of the abnormal vessel behaviour, providing useful geographic and spatial information regarding the anomaly.</p>
</div>
<div class="ltx_para" id="Ch7.S2.p10">
<p class="ltx_p" id="Ch7.S2.p10.1">Since ground truth latitudes and longitudes of the vessels present in the video were not available, the quantitative georeferencing error of the methodology was not reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>.
The results, therefore, serve as a qualitative proof of concept of how ship recognition and georeferencing can improve maritime situational awareness.
The lack of availability of ground truth for vessel geographic positions motivates the creation of ShipSG (see Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4</span></a>), where vessel geographic positions (using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>) were collected together with the images.
The quantitative analysis of homographies for ship georeferencing is shown in the following section.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">7.3   Analysis of Ship Segmentation and Georeferencing Using Homographies <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>
</h3>
<div class="ltx_para" id="Ch7.S3.p1">
<p class="ltx_p" id="Ch7.S3.p1.1">As motivated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch1" title="Chapter 1 Introduction ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">1</span></a>, georeferencing results are superior from the mask of ships, due to the unnecessary background of bounding boxes as well as the inaccurate georeferencing result when using the bounding box center.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p2">
<p class="ltx_p" id="Ch7.S3.p2.1">I expand upon the georeferencing methodology of the Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a> and show in depth quantitative studies of the use of homographies for ship georeferencing using ShipSG.
These studies use the resulting masks from Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>, based on <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, respectively.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p3">
<p class="ltx_p" id="Ch7.S3.p3.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5.S2" title="5.2 Standard Ship Segmentation Using ShipSG [BCP-II] ‣ Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we have seen the results of an initial evaluation of different segmentation methods for the recognition of ships on the ShipSG dataset.
The annotation, using <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>, of latitudes and longitudes of ships present in the images allow to discuss now the evaluation of georeferencing from the resulting masks provided in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p4">
<p class="ltx_p" id="Ch7.S3.p4.2">Following the same principle presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a>, the ships, after being segmented, are georeferenced to provide their location to the situational awareness system in the form of latitude and longitude.
Since the views of the cameras on ShipSG are static, we can perform a transformation between the camera pixel coordinates (<math alttext="C_{x},C_{y}" class="ltx_Math" display="inline" id="Ch7.S3.p4.1.m1.2"><semantics id="Ch7.S3.p4.1.m1.2a"><mrow id="Ch7.S3.p4.1.m1.2.2.2" xref="Ch7.S3.p4.1.m1.2.2.3.cmml"><msub id="Ch7.S3.p4.1.m1.1.1.1.1" xref="Ch7.S3.p4.1.m1.1.1.1.1.cmml"><mi id="Ch7.S3.p4.1.m1.1.1.1.1.2" xref="Ch7.S3.p4.1.m1.1.1.1.1.2.cmml">C</mi><mi id="Ch7.S3.p4.1.m1.1.1.1.1.3" xref="Ch7.S3.p4.1.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="Ch7.S3.p4.1.m1.2.2.2.3" xref="Ch7.S3.p4.1.m1.2.2.3.cmml">,</mo><msub id="Ch7.S3.p4.1.m1.2.2.2.2" xref="Ch7.S3.p4.1.m1.2.2.2.2.cmml"><mi id="Ch7.S3.p4.1.m1.2.2.2.2.2" xref="Ch7.S3.p4.1.m1.2.2.2.2.2.cmml">C</mi><mi id="Ch7.S3.p4.1.m1.2.2.2.2.3" xref="Ch7.S3.p4.1.m1.2.2.2.2.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p4.1.m1.2b"><list id="Ch7.S3.p4.1.m1.2.2.3.cmml" xref="Ch7.S3.p4.1.m1.2.2.2"><apply id="Ch7.S3.p4.1.m1.1.1.1.1.cmml" xref="Ch7.S3.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p4.1.m1.1.1.1.1.1.cmml" xref="Ch7.S3.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="Ch7.S3.p4.1.m1.1.1.1.1.2.cmml" xref="Ch7.S3.p4.1.m1.1.1.1.1.2">𝐶</ci><ci id="Ch7.S3.p4.1.m1.1.1.1.1.3.cmml" xref="Ch7.S3.p4.1.m1.1.1.1.1.3">𝑥</ci></apply><apply id="Ch7.S3.p4.1.m1.2.2.2.2.cmml" xref="Ch7.S3.p4.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="Ch7.S3.p4.1.m1.2.2.2.2.1.cmml" xref="Ch7.S3.p4.1.m1.2.2.2.2">subscript</csymbol><ci id="Ch7.S3.p4.1.m1.2.2.2.2.2.cmml" xref="Ch7.S3.p4.1.m1.2.2.2.2.2">𝐶</ci><ci id="Ch7.S3.p4.1.m1.2.2.2.2.3.cmml" xref="Ch7.S3.p4.1.m1.2.2.2.2.3">𝑦</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p4.1.m1.2c">C_{x},C_{y}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p4.1.m1.2d">italic_C start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math>) and Earth’s geographic latitude and longitude (<math alttext="\phi,\lambda" class="ltx_Math" display="inline" id="Ch7.S3.p4.2.m2.2"><semantics id="Ch7.S3.p4.2.m2.2a"><mrow id="Ch7.S3.p4.2.m2.2.3.2" xref="Ch7.S3.p4.2.m2.2.3.1.cmml"><mi id="Ch7.S3.p4.2.m2.1.1" xref="Ch7.S3.p4.2.m2.1.1.cmml">ϕ</mi><mo id="Ch7.S3.p4.2.m2.2.3.2.1" xref="Ch7.S3.p4.2.m2.2.3.1.cmml">,</mo><mi id="Ch7.S3.p4.2.m2.2.2" xref="Ch7.S3.p4.2.m2.2.2.cmml">λ</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p4.2.m2.2b"><list id="Ch7.S3.p4.2.m2.2.3.1.cmml" xref="Ch7.S3.p4.2.m2.2.3.2"><ci id="Ch7.S3.p4.2.m2.1.1.cmml" xref="Ch7.S3.p4.2.m2.1.1">italic-ϕ</ci><ci id="Ch7.S3.p4.2.m2.2.2.cmml" xref="Ch7.S3.p4.2.m2.2.2">𝜆</ci></list></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p4.2.m2.2c">\phi,\lambda</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p4.2.m2.2d">italic_ϕ , italic_λ</annotation></semantics></math>) in decimal degrees using a homography.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p5">
<p class="ltx_p" id="Ch7.S3.p5.1">I took <math alttext="200" class="ltx_Math" display="inline" id="Ch7.S3.p5.1.m1.1"><semantics id="Ch7.S3.p5.1.m1.1a"><mn id="Ch7.S3.p5.1.m1.1.1" xref="Ch7.S3.p5.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="Ch7.S3.p5.1.m1.1b"><cn id="Ch7.S3.p5.1.m1.1.1.cmml" type="integer" xref="Ch7.S3.p5.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p5.1.m1.1c">200</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p5.1.m1.1d">200</annotation></semantics></math> samples of the training set of ShipSG to create the homographies for the two camera views, and solved Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.E3" title="Equation 7.3 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.3</span></a> to obtain H.
The validation set is later used to quantitatively analyse how well the georeferencing performs.
The separation of homographies by high or low tides was not found to provide a significant improvement in results. Likewise, the correction of lens distortion prior to the homography calculation did not show an experimental improvement of the method.
Therefore, due to their negligible impact, both tidal conditions and lens distortion were excluded from consideration in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p6">
<p class="ltx_p" id="Ch7.S3.p6.1">Upon the creation of the homographies, I proposed a method to automatically determine the pixel (<math alttext="C_{x},C_{y}" class="ltx_Math" display="inline" id="Ch7.S3.p6.1.m1.2"><semantics id="Ch7.S3.p6.1.m1.2a"><mrow id="Ch7.S3.p6.1.m1.2.2.2" xref="Ch7.S3.p6.1.m1.2.2.3.cmml"><msub id="Ch7.S3.p6.1.m1.1.1.1.1" xref="Ch7.S3.p6.1.m1.1.1.1.1.cmml"><mi id="Ch7.S3.p6.1.m1.1.1.1.1.2" xref="Ch7.S3.p6.1.m1.1.1.1.1.2.cmml">C</mi><mi id="Ch7.S3.p6.1.m1.1.1.1.1.3" xref="Ch7.S3.p6.1.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="Ch7.S3.p6.1.m1.2.2.2.3" xref="Ch7.S3.p6.1.m1.2.2.3.cmml">,</mo><msub id="Ch7.S3.p6.1.m1.2.2.2.2" xref="Ch7.S3.p6.1.m1.2.2.2.2.cmml"><mi id="Ch7.S3.p6.1.m1.2.2.2.2.2" xref="Ch7.S3.p6.1.m1.2.2.2.2.2.cmml">C</mi><mi id="Ch7.S3.p6.1.m1.2.2.2.2.3" xref="Ch7.S3.p6.1.m1.2.2.2.2.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p6.1.m1.2b"><list id="Ch7.S3.p6.1.m1.2.2.3.cmml" xref="Ch7.S3.p6.1.m1.2.2.2"><apply id="Ch7.S3.p6.1.m1.1.1.1.1.cmml" xref="Ch7.S3.p6.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p6.1.m1.1.1.1.1.1.cmml" xref="Ch7.S3.p6.1.m1.1.1.1.1">subscript</csymbol><ci id="Ch7.S3.p6.1.m1.1.1.1.1.2.cmml" xref="Ch7.S3.p6.1.m1.1.1.1.1.2">𝐶</ci><ci id="Ch7.S3.p6.1.m1.1.1.1.1.3.cmml" xref="Ch7.S3.p6.1.m1.1.1.1.1.3">𝑥</ci></apply><apply id="Ch7.S3.p6.1.m1.2.2.2.2.cmml" xref="Ch7.S3.p6.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="Ch7.S3.p6.1.m1.2.2.2.2.1.cmml" xref="Ch7.S3.p6.1.m1.2.2.2.2">subscript</csymbol><ci id="Ch7.S3.p6.1.m1.2.2.2.2.2.cmml" xref="Ch7.S3.p6.1.m1.2.2.2.2.2">𝐶</ci><ci id="Ch7.S3.p6.1.m1.2.2.2.2.3.cmml" xref="Ch7.S3.p6.1.m1.2.2.2.2.3">𝑦</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p6.1.m1.2c">C_{x},C_{y}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p6.1.m1.2d">italic_C start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math>) from the masks that most accurately represents the geographic position of the ship.</p>
</div>
<figure class="ltx_figure" id="Ch7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="183" id="Ch7.F5.g1" src="extracted/5906916/fig/georeferencing_new.png" width="412"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F5.2.1.1" style="font-size:90%;">Figure 7.5</span>: </span><span class="ltx_text" id="Ch7.F5.3.2" style="font-size:90%;">Example of segmented ship mask with calculated pixel to be georeferenced (in red, enlarged for visualization). Reprinted from [BCP-II] (CC BY 4.0).</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S3.p7">
<p class="ltx_p" id="Ch7.S3.p7.1">The pixel to be georeferenced represents the intersection between the ship’s hull and the waterline, located beneath the bridge or wheelhouse where the navigation antenna is positioned.
This is achieved by identifying the bottom-most pixel within the ship mask in the vertical direction (Y) that corresponds to the statistical mode along the horizontal axis (X) (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F5" title="Figure 7.5 ‣ 7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.5</span></a>).
Then, this pixel is georeferenced using the homography transformation defined in Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.E2" title="Equation 7.2 ‣ 7.1 Homographies for Image Georeferencing [BCP-II] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a>, facilitating the conversion of image pixel coordinates into real-world latitudes and longitudes.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p8">
<p class="ltx_p" id="Ch7.S3.p8.1">For the evaluation, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> uses the resulting masks of DetectoRS, which was the method that provided the best <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> result during the initial study (74.7%).</p>
</div>
<div class="ltx_para" id="Ch7.S3.p9">
<p class="ltx_p" id="Ch7.S3.p9.4">The true latitudes and longitudes obtained via AIS (<math alttext="\phi_{AIS}" class="ltx_Math" display="inline" id="Ch7.S3.p9.1.m1.1"><semantics id="Ch7.S3.p9.1.m1.1a"><msub id="Ch7.S3.p9.1.m1.1.1" xref="Ch7.S3.p9.1.m1.1.1.cmml"><mi id="Ch7.S3.p9.1.m1.1.1.2" xref="Ch7.S3.p9.1.m1.1.1.2.cmml">ϕ</mi><mrow id="Ch7.S3.p9.1.m1.1.1.3" xref="Ch7.S3.p9.1.m1.1.1.3.cmml"><mi id="Ch7.S3.p9.1.m1.1.1.3.2" xref="Ch7.S3.p9.1.m1.1.1.3.2.cmml">A</mi><mo id="Ch7.S3.p9.1.m1.1.1.3.1" xref="Ch7.S3.p9.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p9.1.m1.1.1.3.3" xref="Ch7.S3.p9.1.m1.1.1.3.3.cmml">I</mi><mo id="Ch7.S3.p9.1.m1.1.1.3.1a" xref="Ch7.S3.p9.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p9.1.m1.1.1.3.4" xref="Ch7.S3.p9.1.m1.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p9.1.m1.1b"><apply id="Ch7.S3.p9.1.m1.1.1.cmml" xref="Ch7.S3.p9.1.m1.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p9.1.m1.1.1.1.cmml" xref="Ch7.S3.p9.1.m1.1.1">subscript</csymbol><ci id="Ch7.S3.p9.1.m1.1.1.2.cmml" xref="Ch7.S3.p9.1.m1.1.1.2">italic-ϕ</ci><apply id="Ch7.S3.p9.1.m1.1.1.3.cmml" xref="Ch7.S3.p9.1.m1.1.1.3"><times id="Ch7.S3.p9.1.m1.1.1.3.1.cmml" xref="Ch7.S3.p9.1.m1.1.1.3.1"></times><ci id="Ch7.S3.p9.1.m1.1.1.3.2.cmml" xref="Ch7.S3.p9.1.m1.1.1.3.2">𝐴</ci><ci id="Ch7.S3.p9.1.m1.1.1.3.3.cmml" xref="Ch7.S3.p9.1.m1.1.1.3.3">𝐼</ci><ci id="Ch7.S3.p9.1.m1.1.1.3.4.cmml" xref="Ch7.S3.p9.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p9.1.m1.1c">\phi_{AIS}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p9.1.m1.1d">italic_ϕ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\lambda_{AIS}" class="ltx_Math" display="inline" id="Ch7.S3.p9.2.m2.1"><semantics id="Ch7.S3.p9.2.m2.1a"><msub id="Ch7.S3.p9.2.m2.1.1" xref="Ch7.S3.p9.2.m2.1.1.cmml"><mi id="Ch7.S3.p9.2.m2.1.1.2" xref="Ch7.S3.p9.2.m2.1.1.2.cmml">λ</mi><mrow id="Ch7.S3.p9.2.m2.1.1.3" xref="Ch7.S3.p9.2.m2.1.1.3.cmml"><mi id="Ch7.S3.p9.2.m2.1.1.3.2" xref="Ch7.S3.p9.2.m2.1.1.3.2.cmml">A</mi><mo id="Ch7.S3.p9.2.m2.1.1.3.1" xref="Ch7.S3.p9.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p9.2.m2.1.1.3.3" xref="Ch7.S3.p9.2.m2.1.1.3.3.cmml">I</mi><mo id="Ch7.S3.p9.2.m2.1.1.3.1a" xref="Ch7.S3.p9.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p9.2.m2.1.1.3.4" xref="Ch7.S3.p9.2.m2.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p9.2.m2.1b"><apply id="Ch7.S3.p9.2.m2.1.1.cmml" xref="Ch7.S3.p9.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p9.2.m2.1.1.1.cmml" xref="Ch7.S3.p9.2.m2.1.1">subscript</csymbol><ci id="Ch7.S3.p9.2.m2.1.1.2.cmml" xref="Ch7.S3.p9.2.m2.1.1.2">𝜆</ci><apply id="Ch7.S3.p9.2.m2.1.1.3.cmml" xref="Ch7.S3.p9.2.m2.1.1.3"><times id="Ch7.S3.p9.2.m2.1.1.3.1.cmml" xref="Ch7.S3.p9.2.m2.1.1.3.1"></times><ci id="Ch7.S3.p9.2.m2.1.1.3.2.cmml" xref="Ch7.S3.p9.2.m2.1.1.3.2">𝐴</ci><ci id="Ch7.S3.p9.2.m2.1.1.3.3.cmml" xref="Ch7.S3.p9.2.m2.1.1.3.3">𝐼</ci><ci id="Ch7.S3.p9.2.m2.1.1.3.4.cmml" xref="Ch7.S3.p9.2.m2.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p9.2.m2.1c">\lambda_{AIS}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p9.2.m2.1d">italic_λ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT</annotation></semantics></math>) on the validation set of ShipSG, were quantitatively compared with those georeferenced via homography (<math alttext="\phi_{H}" class="ltx_Math" display="inline" id="Ch7.S3.p9.3.m3.1"><semantics id="Ch7.S3.p9.3.m3.1a"><msub id="Ch7.S3.p9.3.m3.1.1" xref="Ch7.S3.p9.3.m3.1.1.cmml"><mi id="Ch7.S3.p9.3.m3.1.1.2" xref="Ch7.S3.p9.3.m3.1.1.2.cmml">ϕ</mi><mi id="Ch7.S3.p9.3.m3.1.1.3" xref="Ch7.S3.p9.3.m3.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p9.3.m3.1b"><apply id="Ch7.S3.p9.3.m3.1.1.cmml" xref="Ch7.S3.p9.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p9.3.m3.1.1.1.cmml" xref="Ch7.S3.p9.3.m3.1.1">subscript</csymbol><ci id="Ch7.S3.p9.3.m3.1.1.2.cmml" xref="Ch7.S3.p9.3.m3.1.1.2">italic-ϕ</ci><ci id="Ch7.S3.p9.3.m3.1.1.3.cmml" xref="Ch7.S3.p9.3.m3.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p9.3.m3.1c">\phi_{H}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p9.3.m3.1d">italic_ϕ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\lambda_{H}" class="ltx_Math" display="inline" id="Ch7.S3.p9.4.m4.1"><semantics id="Ch7.S3.p9.4.m4.1a"><msub id="Ch7.S3.p9.4.m4.1.1" xref="Ch7.S3.p9.4.m4.1.1.cmml"><mi id="Ch7.S3.p9.4.m4.1.1.2" xref="Ch7.S3.p9.4.m4.1.1.2.cmml">λ</mi><mi id="Ch7.S3.p9.4.m4.1.1.3" xref="Ch7.S3.p9.4.m4.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p9.4.m4.1b"><apply id="Ch7.S3.p9.4.m4.1.1.cmml" xref="Ch7.S3.p9.4.m4.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p9.4.m4.1.1.1.cmml" xref="Ch7.S3.p9.4.m4.1.1">subscript</csymbol><ci id="Ch7.S3.p9.4.m4.1.1.2.cmml" xref="Ch7.S3.p9.4.m4.1.1.2">𝜆</ci><ci id="Ch7.S3.p9.4.m4.1.1.3.cmml" xref="Ch7.S3.p9.4.m4.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p9.4.m4.1c">\lambda_{H}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p9.4.m4.1d">italic_λ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math>).
To facilitate this comparison, both sets of latitudes and longitudes were converted from decimal degrees to <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.utm"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.utm" title="Universal Transverse Mercator">Universal Transverse Mercator (UTM)</span></a> coordinates, allowing for all results to be expressed in meters.
Among the metrics employed for the quantitative assessment in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">Georeferencing Distance Error (GDE)</span></a> is the one that best represents the accuracy of the method as it directly measures the distance in meters between the actual and estimated positions.
Therefore, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> measures the distance between true (<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>) and georeferenced (H) positions.
The haversine equation (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.E6" title="Equation 7.6 ‣ 7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.6</span></a>) is used instead of euclidean distance to take into account the radius (R) of the Earth</p>
<table class="ltx_equation ltx_eqn_table" id="Ch7.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="GDE=2\leavevmode\nobreak\ \cdot R\cdot\arcsin\sqrt{\sin^{2}\dfrac{\abs{\phi_{%
AIS}-\phi_{H}}}{2}+\cos\phi_{AIS}\cdot\cos\phi_{H}\cdot\sin^{2}\dfrac{\abs{%
\lambda_{AIS}-\lambda_{H}}}{2}}" class="ltx_Math" display="block" id="Ch7.E6.m1.1"><semantics id="Ch7.E6.m1.1a"><mrow id="Ch7.E6.m1.1.1" xref="Ch7.E6.m1.1.1.cmml"><mrow id="Ch7.E6.m1.1.1.2" xref="Ch7.E6.m1.1.1.2.cmml"><mi id="Ch7.E6.m1.1.1.2.2" xref="Ch7.E6.m1.1.1.2.2.cmml">G</mi><mo id="Ch7.E6.m1.1.1.2.1" xref="Ch7.E6.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.2.3" xref="Ch7.E6.m1.1.1.2.3.cmml">D</mi><mo id="Ch7.E6.m1.1.1.2.1a" xref="Ch7.E6.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.2.4" xref="Ch7.E6.m1.1.1.2.4.cmml">E</mi></mrow><mo id="Ch7.E6.m1.1.1.1" xref="Ch7.E6.m1.1.1.1.cmml">=</mo><mrow id="Ch7.E6.m1.1.1.3" xref="Ch7.E6.m1.1.1.3.cmml"><mn id="Ch7.E6.m1.1.1.3.2" xref="Ch7.E6.m1.1.1.3.2.cmml">2</mn><mo id="Ch7.E6.m1.1.1.3.1" lspace="0.722em" rspace="0.222em" xref="Ch7.E6.m1.1.1.3.1.cmml">⋅</mo><mi id="Ch7.E6.m1.1.1.3.3" xref="Ch7.E6.m1.1.1.3.3.cmml">R</mi><mo id="Ch7.E6.m1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="Ch7.E6.m1.1.1.3.1.cmml">⋅</mo><mrow id="Ch7.E6.m1.1.1.3.4" xref="Ch7.E6.m1.1.1.3.4.cmml"><mi id="Ch7.E6.m1.1.1.3.4.1" xref="Ch7.E6.m1.1.1.3.4.1.cmml">arcsin</mi><mo id="Ch7.E6.m1.1.1.3.4a" lspace="0.167em" xref="Ch7.E6.m1.1.1.3.4.cmml">⁡</mo><msqrt id="Ch7.E6.m1.1.1.3.4.2" xref="Ch7.E6.m1.1.1.3.4.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.cmml"><msup id="Ch7.E6.m1.1.1.3.4.2.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.1.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1.2.cmml">sin</mi><mn id="Ch7.E6.m1.1.1.3.4.2.2.2.1.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1.3.cmml">2</mn></msup><mo id="Ch7.E6.m1.1.1.3.4.2.2.2a" lspace="0.167em" xref="Ch7.E6.m1.1.1.3.4.2.2.2.cmml">⁡</mo><mfrac id="Ch7.E6.m1.1.1.3.4.2.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2b.cmml"><mtext id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2a" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2b.cmml">\abs</mtext></merror><mo id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.1.cmml">⁢</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.2.cmml">ϕ</mi><mrow id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.2.cmml">A</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.3.cmml">I</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1a" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.4" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.4.cmml">S</mi></mrow></msub></mrow><mo id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.1.cmml">−</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.2.cmml">ϕ</mi><mi id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.3.cmml">H</mi></msub></mrow><mn id="Ch7.E6.m1.1.1.3.4.2.2.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.3.cmml">2</mn></mfrac></mrow><mo id="Ch7.E6.m1.1.1.3.4.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.1.cmml">+</mo><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.1.cmml">cos</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.2a" lspace="0.167em" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.cmml">⁡</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.2.cmml">ϕ</mi><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.2.cmml">A</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.3.cmml">I</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1a" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.4" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.4.cmml">S</mi></mrow></msub></mrow><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.1" lspace="0.222em" rspace="0.222em" xref="Ch7.E6.m1.1.1.3.4.2.2.3.1.cmml">⋅</mo><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.3.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.1.cmml">cos</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.3a" lspace="0.167em" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.cmml">⁡</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.2.cmml">ϕ</mi><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.3.cmml">H</mi></msub></mrow><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.1a" lspace="0.222em" rspace="0.222em" xref="Ch7.E6.m1.1.1.3.4.2.2.3.1.cmml">⋅</mo><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.4" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.cmml"><msup id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.2.cmml">sin</mi><mn id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.3.cmml">2</mn></msup><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.4a" lspace="0.167em" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.cmml">⁡</mo><mfrac id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.cmml"><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2b.cmml"><mtext id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2a" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2b.cmml">\abs</mtext></merror><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.1.cmml">⁢</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.2.cmml">λ</mi><mrow id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.2.cmml">A</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.3.cmml">I</mi><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1a" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1.cmml">⁢</mo><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.4" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.4.cmml">S</mi></mrow></msub></mrow><mo id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.1" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.1.cmml">−</mo><msub id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.cmml"><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.2" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.2.cmml">λ</mi><mi id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.3.cmml">H</mi></msub></mrow><mn id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.3" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.3.cmml">2</mn></mfrac></mrow></mrow></mrow></msqrt></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.E6.m1.1b"><apply id="Ch7.E6.m1.1.1.cmml" xref="Ch7.E6.m1.1.1"><eq id="Ch7.E6.m1.1.1.1.cmml" xref="Ch7.E6.m1.1.1.1"></eq><apply id="Ch7.E6.m1.1.1.2.cmml" xref="Ch7.E6.m1.1.1.2"><times id="Ch7.E6.m1.1.1.2.1.cmml" xref="Ch7.E6.m1.1.1.2.1"></times><ci id="Ch7.E6.m1.1.1.2.2.cmml" xref="Ch7.E6.m1.1.1.2.2">𝐺</ci><ci id="Ch7.E6.m1.1.1.2.3.cmml" xref="Ch7.E6.m1.1.1.2.3">𝐷</ci><ci id="Ch7.E6.m1.1.1.2.4.cmml" xref="Ch7.E6.m1.1.1.2.4">𝐸</ci></apply><apply id="Ch7.E6.m1.1.1.3.cmml" xref="Ch7.E6.m1.1.1.3"><ci id="Ch7.E6.m1.1.1.3.1.cmml" xref="Ch7.E6.m1.1.1.3.1">⋅</ci><cn id="Ch7.E6.m1.1.1.3.2.cmml" type="integer" xref="Ch7.E6.m1.1.1.3.2">2</cn><ci id="Ch7.E6.m1.1.1.3.3.cmml" xref="Ch7.E6.m1.1.1.3.3">𝑅</ci><apply id="Ch7.E6.m1.1.1.3.4.cmml" xref="Ch7.E6.m1.1.1.3.4"><arcsin id="Ch7.E6.m1.1.1.3.4.1.cmml" xref="Ch7.E6.m1.1.1.3.4.1"></arcsin><apply id="Ch7.E6.m1.1.1.3.4.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2"><root id="Ch7.E6.m1.1.1.3.4.2a.cmml" xref="Ch7.E6.m1.1.1.3.4.2"></root><apply id="Ch7.E6.m1.1.1.3.4.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2"><plus id="Ch7.E6.m1.1.1.3.4.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.1"></plus><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2"><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.2.1.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1">superscript</csymbol><sin id="Ch7.E6.m1.1.1.3.4.2.2.2.1.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1.2"></sin><cn id="Ch7.E6.m1.1.1.3.4.2.2.2.1.3.cmml" type="integer" xref="Ch7.E6.m1.1.1.3.4.2.2.2.1.3">2</cn></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2"><divide id="Ch7.E6.m1.1.1.3.4.2.2.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2"></divide><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2"><minus id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.1"></minus><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2"><times id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.1"></times><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2b.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2"><merror class="ltx_ERROR undefined undefined" id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2"><mtext id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2a.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.2">\abs</mtext></merror></ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.2">italic-ϕ</ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3"><times id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.1"></times><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.2">𝐴</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.3">𝐼</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.4.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.2.3.3.4">𝑆</ci></apply></apply></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.2">italic-ϕ</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.2.3.3">𝐻</ci></apply></apply><cn id="Ch7.E6.m1.1.1.3.4.2.2.2.2.3.cmml" type="integer" xref="Ch7.E6.m1.1.1.3.4.2.2.2.2.3">2</cn></apply></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3"><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.1">⋅</ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2"><cos id="Ch7.E6.m1.1.1.3.4.2.2.3.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.1"></cos><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.2">italic-ϕ</ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3"><times id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.1"></times><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.2">𝐴</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.3">𝐼</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.4.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.2.2.3.4">𝑆</ci></apply></apply></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3"><cos id="Ch7.E6.m1.1.1.3.4.2.2.3.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.1"></cos><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.2">italic-ϕ</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.3.2.3">𝐻</ci></apply></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4"><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1">superscript</csymbol><sin id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.2"></sin><cn id="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.3.cmml" type="integer" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.1.3">2</cn></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2"><divide id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2"></divide><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2"><minus id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.1"></minus><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2"><times id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.1"></times><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2b.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2"><merror class="ltx_ERROR undefined undefined" id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2"><mtext id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2a.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.2">\abs</mtext></merror></ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.2">𝜆</ci><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3"><times id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.1"></times><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.2">𝐴</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.3">𝐼</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.4.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.2.3.3.4">𝑆</ci></apply></apply></apply><apply id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3"><csymbol cd="ambiguous" id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.1.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3">subscript</csymbol><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.2.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.2">𝜆</ci><ci id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.3.cmml" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.2.3.3">𝐻</ci></apply></apply><cn id="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.3.cmml" type="integer" xref="Ch7.E6.m1.1.1.3.4.2.2.3.4.2.3">2</cn></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.E6.m1.1c">GDE=2\leavevmode\nobreak\ \cdot R\cdot\arcsin\sqrt{\sin^{2}\dfrac{\abs{\phi_{%
AIS}-\phi_{H}}}{2}+\cos\phi_{AIS}\cdot\cos\phi_{H}\cdot\sin^{2}\dfrac{\abs{%
\lambda_{AIS}-\lambda_{H}}}{2}}</annotation><annotation encoding="application/x-llamapun" id="Ch7.E6.m1.1d">italic_G italic_D italic_E = 2 ⋅ italic_R ⋅ roman_arcsin square-root start_ARG roman_sin start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG italic_ϕ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT - italic_ϕ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG + roman_cos italic_ϕ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT ⋅ roman_cos italic_ϕ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ⋅ roman_sin start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG italic_λ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT - italic_λ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Ch7.S3.p10">
<p class="ltx_p" id="Ch7.S3.p10.4">where <math alttext="\phi_{AIS}" class="ltx_Math" display="inline" id="Ch7.S3.p10.1.m1.1"><semantics id="Ch7.S3.p10.1.m1.1a"><msub id="Ch7.S3.p10.1.m1.1.1" xref="Ch7.S3.p10.1.m1.1.1.cmml"><mi id="Ch7.S3.p10.1.m1.1.1.2" xref="Ch7.S3.p10.1.m1.1.1.2.cmml">ϕ</mi><mrow id="Ch7.S3.p10.1.m1.1.1.3" xref="Ch7.S3.p10.1.m1.1.1.3.cmml"><mi id="Ch7.S3.p10.1.m1.1.1.3.2" xref="Ch7.S3.p10.1.m1.1.1.3.2.cmml">A</mi><mo id="Ch7.S3.p10.1.m1.1.1.3.1" xref="Ch7.S3.p10.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p10.1.m1.1.1.3.3" xref="Ch7.S3.p10.1.m1.1.1.3.3.cmml">I</mi><mo id="Ch7.S3.p10.1.m1.1.1.3.1a" xref="Ch7.S3.p10.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p10.1.m1.1.1.3.4" xref="Ch7.S3.p10.1.m1.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p10.1.m1.1b"><apply id="Ch7.S3.p10.1.m1.1.1.cmml" xref="Ch7.S3.p10.1.m1.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p10.1.m1.1.1.1.cmml" xref="Ch7.S3.p10.1.m1.1.1">subscript</csymbol><ci id="Ch7.S3.p10.1.m1.1.1.2.cmml" xref="Ch7.S3.p10.1.m1.1.1.2">italic-ϕ</ci><apply id="Ch7.S3.p10.1.m1.1.1.3.cmml" xref="Ch7.S3.p10.1.m1.1.1.3"><times id="Ch7.S3.p10.1.m1.1.1.3.1.cmml" xref="Ch7.S3.p10.1.m1.1.1.3.1"></times><ci id="Ch7.S3.p10.1.m1.1.1.3.2.cmml" xref="Ch7.S3.p10.1.m1.1.1.3.2">𝐴</ci><ci id="Ch7.S3.p10.1.m1.1.1.3.3.cmml" xref="Ch7.S3.p10.1.m1.1.1.3.3">𝐼</ci><ci id="Ch7.S3.p10.1.m1.1.1.3.4.cmml" xref="Ch7.S3.p10.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p10.1.m1.1c">\phi_{AIS}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p10.1.m1.1d">italic_ϕ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\phi_{H}" class="ltx_Math" display="inline" id="Ch7.S3.p10.2.m2.1"><semantics id="Ch7.S3.p10.2.m2.1a"><msub id="Ch7.S3.p10.2.m2.1.1" xref="Ch7.S3.p10.2.m2.1.1.cmml"><mi id="Ch7.S3.p10.2.m2.1.1.2" xref="Ch7.S3.p10.2.m2.1.1.2.cmml">ϕ</mi><mi id="Ch7.S3.p10.2.m2.1.1.3" xref="Ch7.S3.p10.2.m2.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p10.2.m2.1b"><apply id="Ch7.S3.p10.2.m2.1.1.cmml" xref="Ch7.S3.p10.2.m2.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p10.2.m2.1.1.1.cmml" xref="Ch7.S3.p10.2.m2.1.1">subscript</csymbol><ci id="Ch7.S3.p10.2.m2.1.1.2.cmml" xref="Ch7.S3.p10.2.m2.1.1.2">italic-ϕ</ci><ci id="Ch7.S3.p10.2.m2.1.1.3.cmml" xref="Ch7.S3.p10.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p10.2.m2.1c">\phi_{H}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p10.2.m2.1d">italic_ϕ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> represent the longitudes from the ground truth and georeferencing method using homography, respectively, <math alttext="\lambda_{AIS}" class="ltx_Math" display="inline" id="Ch7.S3.p10.3.m3.1"><semantics id="Ch7.S3.p10.3.m3.1a"><msub id="Ch7.S3.p10.3.m3.1.1" xref="Ch7.S3.p10.3.m3.1.1.cmml"><mi id="Ch7.S3.p10.3.m3.1.1.2" xref="Ch7.S3.p10.3.m3.1.1.2.cmml">λ</mi><mrow id="Ch7.S3.p10.3.m3.1.1.3" xref="Ch7.S3.p10.3.m3.1.1.3.cmml"><mi id="Ch7.S3.p10.3.m3.1.1.3.2" xref="Ch7.S3.p10.3.m3.1.1.3.2.cmml">A</mi><mo id="Ch7.S3.p10.3.m3.1.1.3.1" xref="Ch7.S3.p10.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p10.3.m3.1.1.3.3" xref="Ch7.S3.p10.3.m3.1.1.3.3.cmml">I</mi><mo id="Ch7.S3.p10.3.m3.1.1.3.1a" xref="Ch7.S3.p10.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p10.3.m3.1.1.3.4" xref="Ch7.S3.p10.3.m3.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p10.3.m3.1b"><apply id="Ch7.S3.p10.3.m3.1.1.cmml" xref="Ch7.S3.p10.3.m3.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p10.3.m3.1.1.1.cmml" xref="Ch7.S3.p10.3.m3.1.1">subscript</csymbol><ci id="Ch7.S3.p10.3.m3.1.1.2.cmml" xref="Ch7.S3.p10.3.m3.1.1.2">𝜆</ci><apply id="Ch7.S3.p10.3.m3.1.1.3.cmml" xref="Ch7.S3.p10.3.m3.1.1.3"><times id="Ch7.S3.p10.3.m3.1.1.3.1.cmml" xref="Ch7.S3.p10.3.m3.1.1.3.1"></times><ci id="Ch7.S3.p10.3.m3.1.1.3.2.cmml" xref="Ch7.S3.p10.3.m3.1.1.3.2">𝐴</ci><ci id="Ch7.S3.p10.3.m3.1.1.3.3.cmml" xref="Ch7.S3.p10.3.m3.1.1.3.3">𝐼</ci><ci id="Ch7.S3.p10.3.m3.1.1.3.4.cmml" xref="Ch7.S3.p10.3.m3.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p10.3.m3.1c">\lambda_{AIS}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p10.3.m3.1d">italic_λ start_POSTSUBSCRIPT italic_A italic_I italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\lambda_{H}" class="ltx_Math" display="inline" id="Ch7.S3.p10.4.m4.1"><semantics id="Ch7.S3.p10.4.m4.1a"><msub id="Ch7.S3.p10.4.m4.1.1" xref="Ch7.S3.p10.4.m4.1.1.cmml"><mi id="Ch7.S3.p10.4.m4.1.1.2" xref="Ch7.S3.p10.4.m4.1.1.2.cmml">λ</mi><mi id="Ch7.S3.p10.4.m4.1.1.3" xref="Ch7.S3.p10.4.m4.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="Ch7.S3.p10.4.m4.1b"><apply id="Ch7.S3.p10.4.m4.1.1.cmml" xref="Ch7.S3.p10.4.m4.1.1"><csymbol cd="ambiguous" id="Ch7.S3.p10.4.m4.1.1.1.cmml" xref="Ch7.S3.p10.4.m4.1.1">subscript</csymbol><ci id="Ch7.S3.p10.4.m4.1.1.2.cmml" xref="Ch7.S3.p10.4.m4.1.1.2">𝜆</ci><ci id="Ch7.S3.p10.4.m4.1.1.3.cmml" xref="Ch7.S3.p10.4.m4.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p10.4.m4.1c">\lambda_{H}</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p10.4.m4.1d">italic_λ start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> correspond to the latitudes, and R is the Earth’s radius at Bremerhaven.</p>
</div>
<figure class="ltx_figure" id="Ch7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="250" id="Ch7.F6.g1" src="extracted/5906916/fig/gde.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F6.2.1.1" style="font-size:90%;">Figure 7.6</span>: </span><span class="ltx_text" id="Ch7.F6.3.2" style="font-size:90%;">Georeferencing distance error per ship length. GDEs and their uncertainties fall within the bounds of the ship length. Reprinted from [BCP-II] (CC BY 4.0).</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S3.p11">
<p class="ltx_p" id="Ch7.S3.p11.3">The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a>, given as <math alttext="mean\leavevmode\nobreak\ \pm\leavevmode\nobreak\ standard\leavevmode\nobreak\ deviation" class="ltx_Math" display="inline" id="Ch7.S3.p11.1.m1.1"><semantics id="Ch7.S3.p11.1.m1.1a"><mrow id="Ch7.S3.p11.1.m1.1.1" xref="Ch7.S3.p11.1.m1.1.1.cmml"><mrow id="Ch7.S3.p11.1.m1.1.1.2" xref="Ch7.S3.p11.1.m1.1.1.2.cmml"><mi id="Ch7.S3.p11.1.m1.1.1.2.2" xref="Ch7.S3.p11.1.m1.1.1.2.2.cmml">m</mi><mo id="Ch7.S3.p11.1.m1.1.1.2.1" xref="Ch7.S3.p11.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.2.3" xref="Ch7.S3.p11.1.m1.1.1.2.3.cmml">e</mi><mo id="Ch7.S3.p11.1.m1.1.1.2.1a" xref="Ch7.S3.p11.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.2.4" xref="Ch7.S3.p11.1.m1.1.1.2.4.cmml">a</mi><mo id="Ch7.S3.p11.1.m1.1.1.2.1b" xref="Ch7.S3.p11.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.2.5" xref="Ch7.S3.p11.1.m1.1.1.2.5.cmml">n</mi></mrow><mo id="Ch7.S3.p11.1.m1.1.1.1" lspace="0.722em" rspace="0.722em" xref="Ch7.S3.p11.1.m1.1.1.1.cmml">±</mo><mrow id="Ch7.S3.p11.1.m1.1.1.3" xref="Ch7.S3.p11.1.m1.1.1.3.cmml"><mi id="Ch7.S3.p11.1.m1.1.1.3.2" xref="Ch7.S3.p11.1.m1.1.1.3.2.cmml">s</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.3" xref="Ch7.S3.p11.1.m1.1.1.3.3.cmml">t</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1a" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.4" xref="Ch7.S3.p11.1.m1.1.1.3.4.cmml">a</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1b" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.5" xref="Ch7.S3.p11.1.m1.1.1.3.5.cmml">n</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1c" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.6" xref="Ch7.S3.p11.1.m1.1.1.3.6.cmml">d</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1d" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.7" xref="Ch7.S3.p11.1.m1.1.1.3.7.cmml">a</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1e" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.8" xref="Ch7.S3.p11.1.m1.1.1.3.8.cmml">r</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1f" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.9" xref="Ch7.S3.p11.1.m1.1.1.3.9.cmml">d</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1g" lspace="0.500em" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.10" xref="Ch7.S3.p11.1.m1.1.1.3.10.cmml">d</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1h" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.11" xref="Ch7.S3.p11.1.m1.1.1.3.11.cmml">e</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1i" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.12" xref="Ch7.S3.p11.1.m1.1.1.3.12.cmml">v</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1j" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.13" xref="Ch7.S3.p11.1.m1.1.1.3.13.cmml">i</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1k" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.14" xref="Ch7.S3.p11.1.m1.1.1.3.14.cmml">a</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1l" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.15" xref="Ch7.S3.p11.1.m1.1.1.3.15.cmml">t</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1m" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.16" xref="Ch7.S3.p11.1.m1.1.1.3.16.cmml">i</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1n" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.17" xref="Ch7.S3.p11.1.m1.1.1.3.17.cmml">o</mi><mo id="Ch7.S3.p11.1.m1.1.1.3.1o" xref="Ch7.S3.p11.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.1.m1.1.1.3.18" xref="Ch7.S3.p11.1.m1.1.1.3.18.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p11.1.m1.1b"><apply id="Ch7.S3.p11.1.m1.1.1.cmml" xref="Ch7.S3.p11.1.m1.1.1"><csymbol cd="latexml" id="Ch7.S3.p11.1.m1.1.1.1.cmml" xref="Ch7.S3.p11.1.m1.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S3.p11.1.m1.1.1.2.cmml" xref="Ch7.S3.p11.1.m1.1.1.2"><times id="Ch7.S3.p11.1.m1.1.1.2.1.cmml" xref="Ch7.S3.p11.1.m1.1.1.2.1"></times><ci id="Ch7.S3.p11.1.m1.1.1.2.2.cmml" xref="Ch7.S3.p11.1.m1.1.1.2.2">𝑚</ci><ci id="Ch7.S3.p11.1.m1.1.1.2.3.cmml" xref="Ch7.S3.p11.1.m1.1.1.2.3">𝑒</ci><ci id="Ch7.S3.p11.1.m1.1.1.2.4.cmml" xref="Ch7.S3.p11.1.m1.1.1.2.4">𝑎</ci><ci id="Ch7.S3.p11.1.m1.1.1.2.5.cmml" xref="Ch7.S3.p11.1.m1.1.1.2.5">𝑛</ci></apply><apply id="Ch7.S3.p11.1.m1.1.1.3.cmml" xref="Ch7.S3.p11.1.m1.1.1.3"><times id="Ch7.S3.p11.1.m1.1.1.3.1.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.1"></times><ci id="Ch7.S3.p11.1.m1.1.1.3.2.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.2">𝑠</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.3.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.3">𝑡</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.4.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.4">𝑎</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.5.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.5">𝑛</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.6.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.6">𝑑</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.7.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.7">𝑎</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.8.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.8">𝑟</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.9.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.9">𝑑</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.10.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.10">𝑑</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.11.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.11">𝑒</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.12.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.12">𝑣</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.13.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.13">𝑖</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.14.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.14">𝑎</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.15.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.15">𝑡</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.16.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.16">𝑖</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.17.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.17">𝑜</ci><ci id="Ch7.S3.p11.1.m1.1.1.3.18.cmml" xref="Ch7.S3.p11.1.m1.1.1.3.18">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p11.1.m1.1c">mean\leavevmode\nobreak\ \pm\leavevmode\nobreak\ standard\leavevmode\nobreak\ deviation</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p11.1.m1.1d">italic_m italic_e italic_a italic_n ± italic_s italic_t italic_a italic_n italic_d italic_a italic_r italic_d italic_d italic_e italic_v italic_i italic_a italic_t italic_i italic_o italic_n</annotation></semantics></math> in meters, reaches <math alttext="22\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S3.p11.2.m2.1"><semantics id="Ch7.S3.p11.2.m2.1a"><mrow id="Ch7.S3.p11.2.m2.1.1" xref="Ch7.S3.p11.2.m2.1.1.cmml"><mrow id="Ch7.S3.p11.2.m2.1.1.2" xref="Ch7.S3.p11.2.m2.1.1.2.cmml"><mn id="Ch7.S3.p11.2.m2.1.1.2.2" xref="Ch7.S3.p11.2.m2.1.1.2.2.cmml">22</mn><mo id="Ch7.S3.p11.2.m2.1.1.2.1" lspace="0.500em" xref="Ch7.S3.p11.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p11.2.m2.1.1.2.3" xref="Ch7.S3.p11.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S3.p11.2.m2.1.1.1" lspace="0.722em" xref="Ch7.S3.p11.2.m2.1.1.1.cmml">±</mo><mrow id="Ch7.S3.p11.2.m2.1.1.3" xref="Ch7.S3.p11.2.m2.1.1.3.cmml"><mn id="Ch7.S3.p11.2.m2.1.1.3.2" xref="Ch7.S3.p11.2.m2.1.1.3.2.cmml"> 10</mn><mo id="Ch7.S3.p11.2.m2.1.1.3.1" lspace="0.500em" xref="Ch7.S3.p11.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.2.m2.1.1.3.3" xref="Ch7.S3.p11.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p11.2.m2.1b"><apply id="Ch7.S3.p11.2.m2.1.1.cmml" xref="Ch7.S3.p11.2.m2.1.1"><csymbol cd="latexml" id="Ch7.S3.p11.2.m2.1.1.1.cmml" xref="Ch7.S3.p11.2.m2.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S3.p11.2.m2.1.1.2.cmml" xref="Ch7.S3.p11.2.m2.1.1.2"><times id="Ch7.S3.p11.2.m2.1.1.2.1.cmml" xref="Ch7.S3.p11.2.m2.1.1.2.1"></times><cn id="Ch7.S3.p11.2.m2.1.1.2.2.cmml" type="integer" xref="Ch7.S3.p11.2.m2.1.1.2.2">22</cn><ci id="Ch7.S3.p11.2.m2.1.1.2.3.cmml" xref="Ch7.S3.p11.2.m2.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S3.p11.2.m2.1.1.3.cmml" xref="Ch7.S3.p11.2.m2.1.1.3"><times id="Ch7.S3.p11.2.m2.1.1.3.1.cmml" xref="Ch7.S3.p11.2.m2.1.1.3.1"></times><cn id="Ch7.S3.p11.2.m2.1.1.3.2.cmml" type="integer" xref="Ch7.S3.p11.2.m2.1.1.3.2">10</cn><ci id="Ch7.S3.p11.2.m2.1.1.3.3.cmml" xref="Ch7.S3.p11.2.m2.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p11.2.m2.1c">22\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p11.2.m2.1d">22 italic_m ± 10 italic_m</annotation></semantics></math> for ranges inside the port basin (up to 400 m to the camera) and <math alttext="53\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 24%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S3.p11.3.m3.1"><semantics id="Ch7.S3.p11.3.m3.1a"><mrow id="Ch7.S3.p11.3.m3.1.1" xref="Ch7.S3.p11.3.m3.1.1.cmml"><mrow id="Ch7.S3.p11.3.m3.1.1.2" xref="Ch7.S3.p11.3.m3.1.1.2.cmml"><mn id="Ch7.S3.p11.3.m3.1.1.2.2" xref="Ch7.S3.p11.3.m3.1.1.2.2.cmml">53</mn><mo id="Ch7.S3.p11.3.m3.1.1.2.1" lspace="0.500em" xref="Ch7.S3.p11.3.m3.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p11.3.m3.1.1.2.3" xref="Ch7.S3.p11.3.m3.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S3.p11.3.m3.1.1.1" lspace="0.722em" xref="Ch7.S3.p11.3.m3.1.1.1.cmml">±</mo><mrow id="Ch7.S3.p11.3.m3.1.1.3" xref="Ch7.S3.p11.3.m3.1.1.3.cmml"><mn id="Ch7.S3.p11.3.m3.1.1.3.2" xref="Ch7.S3.p11.3.m3.1.1.3.2.cmml"> 24</mn><mo id="Ch7.S3.p11.3.m3.1.1.3.1" lspace="0.500em" xref="Ch7.S3.p11.3.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p11.3.m3.1.1.3.3" xref="Ch7.S3.p11.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p11.3.m3.1b"><apply id="Ch7.S3.p11.3.m3.1.1.cmml" xref="Ch7.S3.p11.3.m3.1.1"><csymbol cd="latexml" id="Ch7.S3.p11.3.m3.1.1.1.cmml" xref="Ch7.S3.p11.3.m3.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S3.p11.3.m3.1.1.2.cmml" xref="Ch7.S3.p11.3.m3.1.1.2"><times id="Ch7.S3.p11.3.m3.1.1.2.1.cmml" xref="Ch7.S3.p11.3.m3.1.1.2.1"></times><cn id="Ch7.S3.p11.3.m3.1.1.2.2.cmml" type="integer" xref="Ch7.S3.p11.3.m3.1.1.2.2">53</cn><ci id="Ch7.S3.p11.3.m3.1.1.2.3.cmml" xref="Ch7.S3.p11.3.m3.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S3.p11.3.m3.1.1.3.cmml" xref="Ch7.S3.p11.3.m3.1.1.3"><times id="Ch7.S3.p11.3.m3.1.1.3.1.cmml" xref="Ch7.S3.p11.3.m3.1.1.3.1"></times><cn id="Ch7.S3.p11.3.m3.1.1.3.2.cmml" type="integer" xref="Ch7.S3.p11.3.m3.1.1.3.2">24</cn><ci id="Ch7.S3.p11.3.m3.1.1.3.3.cmml" xref="Ch7.S3.p11.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p11.3.m3.1c">53\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 24%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p11.3.m3.1d">53 italic_m ± 24 italic_m</annotation></semantics></math> on the river (from 400 m to 1200 m).
In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4</span></a> it was described that ship lengths along with the ship
positions from <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> messages were collected and are used to observe how the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> changes with ship length and range.
As seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F6" title="Figure 7.6 ‣ 7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.6</span></a>, for the smallest ship lengths (0 to 20 m), the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> within the port basin and river are similar.
This demonstrates that the accuracy of pinpointing the georeferenced pixel of the mask increases with the decrease in ship size, regardless of the distance between the ship and the camera.
Distance to the ship from the camera is the primary factor influencing the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a>.
For ships longer than 20 meters, a marked rise in <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> occurs at distances beyond 400 meters (on the river).
In contrast, at distances shorter than 400 meters (within the port basin), the length of the ship has a smaller effect on the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> compared to distances on the river beyond 400 meters.
Despite the challenge of identifying the precise pixel of the mask for georeferencing increasing with the distance to the ship, where each pixel spans a broader geographical area, the <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> remains consistent within uncertainties for each ship length.
This suggests that the method provides estimations with a level of accuracy that can be considered contextually appropriate, when the specific operational contexts allow a a deviation of this magnitude within acceptable safety or operational thresholds.</p>
</div>
<figure class="ltx_figure" id="Ch7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="223" id="Ch7.F7.g1" src="extracted/5906916/fig/georef_paper5.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Ch7.F7.2.1.1" style="font-size:90%;">Figure 7.7</span>: </span><span class="ltx_text" id="Ch7.F7.3.2" style="font-size:90%;">Segmented and georeferenced ships using ScatYOLOv8+CBAM and homographies to improve maritime awareness. (a) ShipSG image with segmented and classified ships using the ScatYOLOv8n+CBAM architecture presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. (b) Georeferenced ships displayed on OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib107" title="">OpenStreetMap </a></cite> using the homography-based method of <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> on segmented masks. Reprinted from <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. ©2023 IEEE.</span></figcaption>
</figure>
<div class="ltx_para" id="Ch7.S3.p12">
<p class="ltx_p" id="Ch7.S3.p12.2">An additional quantitative georeferencing exploration, was done in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> with the resulting masks of the proposed ScatYOLOv8n+CBAM (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6.S2" title="6.2 ScatYOLOv8+CBAM [BCP-V] ‣ Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6.2</span></a>).
The georeferencing of the masks resulting from ScatYOLOv8+CBAM, studied in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, follows the same procedure to automatically identify the pixel to be georeferenced, and with the same homographies calculated in the previous evaluation with standard methods.
The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.gde" title="Georeferencing Distance Error">GDE</span></a> yielded by the masks predicted with ScatYOLOv8+CBAM was of <math alttext="18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 13%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S3.p12.1.m1.1"><semantics id="Ch7.S3.p12.1.m1.1a"><mrow id="Ch7.S3.p12.1.m1.1.1" xref="Ch7.S3.p12.1.m1.1.1.cmml"><mrow id="Ch7.S3.p12.1.m1.1.1.2" xref="Ch7.S3.p12.1.m1.1.1.2.cmml"><mn id="Ch7.S3.p12.1.m1.1.1.2.2" xref="Ch7.S3.p12.1.m1.1.1.2.2.cmml">18</mn><mo id="Ch7.S3.p12.1.m1.1.1.2.1" lspace="0.500em" xref="Ch7.S3.p12.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p12.1.m1.1.1.2.3" xref="Ch7.S3.p12.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S3.p12.1.m1.1.1.1" lspace="0.722em" xref="Ch7.S3.p12.1.m1.1.1.1.cmml">±</mo><mrow id="Ch7.S3.p12.1.m1.1.1.3" xref="Ch7.S3.p12.1.m1.1.1.3.cmml"><mn id="Ch7.S3.p12.1.m1.1.1.3.2" xref="Ch7.S3.p12.1.m1.1.1.3.2.cmml"> 13</mn><mo id="Ch7.S3.p12.1.m1.1.1.3.1" lspace="0.500em" xref="Ch7.S3.p12.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p12.1.m1.1.1.3.3" xref="Ch7.S3.p12.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p12.1.m1.1b"><apply id="Ch7.S3.p12.1.m1.1.1.cmml" xref="Ch7.S3.p12.1.m1.1.1"><csymbol cd="latexml" id="Ch7.S3.p12.1.m1.1.1.1.cmml" xref="Ch7.S3.p12.1.m1.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S3.p12.1.m1.1.1.2.cmml" xref="Ch7.S3.p12.1.m1.1.1.2"><times id="Ch7.S3.p12.1.m1.1.1.2.1.cmml" xref="Ch7.S3.p12.1.m1.1.1.2.1"></times><cn id="Ch7.S3.p12.1.m1.1.1.2.2.cmml" type="integer" xref="Ch7.S3.p12.1.m1.1.1.2.2">18</cn><ci id="Ch7.S3.p12.1.m1.1.1.2.3.cmml" xref="Ch7.S3.p12.1.m1.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S3.p12.1.m1.1.1.3.cmml" xref="Ch7.S3.p12.1.m1.1.1.3"><times id="Ch7.S3.p12.1.m1.1.1.3.1.cmml" xref="Ch7.S3.p12.1.m1.1.1.3.1"></times><cn id="Ch7.S3.p12.1.m1.1.1.3.2.cmml" type="integer" xref="Ch7.S3.p12.1.m1.1.1.3.2">13</cn><ci id="Ch7.S3.p12.1.m1.1.1.3.3.cmml" xref="Ch7.S3.p12.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p12.1.m1.1c">18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 13%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p12.1.m1.1d">18 italic_m ± 13 italic_m</annotation></semantics></math> within the port basin (up to 400 m range). On the river (range from 400 m to 1200 m), the measured GDE using ScatYOLOv8+CBAM was <math alttext="44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S3.p12.2.m2.1"><semantics id="Ch7.S3.p12.2.m2.1a"><mrow id="Ch7.S3.p12.2.m2.1.1" xref="Ch7.S3.p12.2.m2.1.1.cmml"><mrow id="Ch7.S3.p12.2.m2.1.1.2" xref="Ch7.S3.p12.2.m2.1.1.2.cmml"><mn id="Ch7.S3.p12.2.m2.1.1.2.2" xref="Ch7.S3.p12.2.m2.1.1.2.2.cmml">44</mn><mo id="Ch7.S3.p12.2.m2.1.1.2.1" lspace="0.500em" xref="Ch7.S3.p12.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S3.p12.2.m2.1.1.2.3" xref="Ch7.S3.p12.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S3.p12.2.m2.1.1.1" lspace="0.722em" xref="Ch7.S3.p12.2.m2.1.1.1.cmml">±</mo><mrow id="Ch7.S3.p12.2.m2.1.1.3" xref="Ch7.S3.p12.2.m2.1.1.3.cmml"><mn id="Ch7.S3.p12.2.m2.1.1.3.2" xref="Ch7.S3.p12.2.m2.1.1.3.2.cmml"> 27</mn><mo id="Ch7.S3.p12.2.m2.1.1.3.1" lspace="0.500em" xref="Ch7.S3.p12.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S3.p12.2.m2.1.1.3.3" xref="Ch7.S3.p12.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S3.p12.2.m2.1b"><apply id="Ch7.S3.p12.2.m2.1.1.cmml" xref="Ch7.S3.p12.2.m2.1.1"><csymbol cd="latexml" id="Ch7.S3.p12.2.m2.1.1.1.cmml" xref="Ch7.S3.p12.2.m2.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S3.p12.2.m2.1.1.2.cmml" xref="Ch7.S3.p12.2.m2.1.1.2"><times id="Ch7.S3.p12.2.m2.1.1.2.1.cmml" xref="Ch7.S3.p12.2.m2.1.1.2.1"></times><cn id="Ch7.S3.p12.2.m2.1.1.2.2.cmml" type="integer" xref="Ch7.S3.p12.2.m2.1.1.2.2">44</cn><ci id="Ch7.S3.p12.2.m2.1.1.2.3.cmml" xref="Ch7.S3.p12.2.m2.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S3.p12.2.m2.1.1.3.cmml" xref="Ch7.S3.p12.2.m2.1.1.3"><times id="Ch7.S3.p12.2.m2.1.1.3.1.cmml" xref="Ch7.S3.p12.2.m2.1.1.3.1"></times><cn id="Ch7.S3.p12.2.m2.1.1.3.2.cmml" type="integer" xref="Ch7.S3.p12.2.m2.1.1.3.2">27</cn><ci id="Ch7.S3.p12.2.m2.1.1.3.3.cmml" xref="Ch7.S3.p12.2.m2.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S3.p12.2.m2.1c">44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S3.p12.2.m2.1d">44 italic_m ± 27 italic_m</annotation></semantics></math>.
These results confirm the applicability of the optimized model allowing the benefits described in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a> without any degradation in georeferencing performance.
As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.F7" title="Figure 7.7 ‣ 7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.7</span></a>, when the georeferenced masks are accessed using web services, this work can support real-time decision making, thus improving maritime awareness.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p13">
<p class="ltx_p" id="Ch7.S3.p13.1">It is important to note the lack of complex operations in the georeferencing method, which allows an easy deployment on an embedded system.
Specifically, these georeferencing steps were measured 0.5 ms on average on the NVIDIA Jetson AGX Xavier, a stark contrast to the inference times of the instance segmentation.
This shows that the bulk of the computation time is devoted to mask segmentation.
The subsequent pixel searching and coordinate transformation using a homography adds a minimal amount of time to the overall procedure.</p>
</div>
<figure class="ltx_table" id="Ch7.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ch7.T1.8.1.1" style="font-size:90%;">Table 7.1</span>: </span><span class="ltx_text" id="Ch7.T1.9.2" style="font-size:90%;">Comparison of the proposed method for ship georeferencing accuracy with existing works.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="Ch7.T1.6" style="width:433.6pt;height:146.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.0pt,16.9pt) scale(0.812549423095091,0.812549423095091) ;">
<table class="ltx_tabular ltx_align_middle" id="Ch7.T1.6.6">
<tr class="ltx_tr" id="Ch7.T1.6.6.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.7.1"><span class="ltx_text ltx_font_bold" id="Ch7.T1.6.6.7.1.1">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.7.2"><span class="ltx_text ltx_font_bold" id="Ch7.T1.6.6.7.2.1">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.7.3"><span class="ltx_text ltx_font_bold" id="Ch7.T1.6.6.7.3.1">Range to Object</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.7.4"><span class="ltx_text ltx_font_bold" id="Ch7.T1.6.6.7.4.1">Error (m)</span></td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.6.6.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.8.1"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib97" title="">naus2021assessment </a></cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.8.2">Radar Antenna + GPS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.8.3">1000 m</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch7.T1.6.6.8.4">6.5</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.1.1.1">
<td class="ltx_td ltx_align_center" id="Ch7.T1.1.1.1.2"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib98" title="">livingstone2014ship </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.1.1.1.3">Synthetic Aperture Radar</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.1.1.1.4">800 km</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.1.1.1.1">13 <math alttext="\pm" class="ltx_Math" display="inline" id="Ch7.T1.1.1.1.1.m1.1"><semantics id="Ch7.T1.1.1.1.1.m1.1a"><mo id="Ch7.T1.1.1.1.1.m1.1.1" xref="Ch7.T1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.1.1.1.1.m1.1.1.cmml" xref="Ch7.T1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.1.1.1.1.m1.1d">±</annotation></semantics></math> 23</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="Ch7.T1.2.2.2.2"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib99" title="">wei2020geolocation </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.2.2.2.3">Opt. Remote Sensing</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.2.2.2.4">36000 km</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.2.2.2.1">165 <math alttext="\pm" class="ltx_Math" display="inline" id="Ch7.T1.2.2.2.1.m1.1"><semantics id="Ch7.T1.2.2.2.1.m1.1a"><mo id="Ch7.T1.2.2.2.1.m1.1.1" xref="Ch7.T1.2.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.2.2.2.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.2.2.2.1.m1.1.1.cmml" xref="Ch7.T1.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.2.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.2.2.2.1.m1.1d">±</annotation></semantics></math> 109</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.6.6.9">
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.9.1"><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib24" title="">helgesen2020low </a></cite></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.9.2">Opt. Camera + GPS + IMU</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.9.3">400 m</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.9.4">20</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.3.3.3">
<td class="ltx_td ltx_align_center" id="Ch7.T1.3.3.3.2"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.3.3.3.3">Opt. Camera</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.3.3.3.4">400 m</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.3.3.3.1">22<math alttext="\leavevmode\nobreak\ \pm\leavevmode\nobreak\ " class="ltx_Math" display="inline" id="Ch7.T1.3.3.3.1.m1.1"><semantics id="Ch7.T1.3.3.3.1.m1.1a"><mo id="Ch7.T1.3.3.3.1.m1.1.1" xref="Ch7.T1.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.3.3.3.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.3.3.3.1.m1.1.1.cmml" xref="Ch7.T1.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.3.3.3.1.m1.1c">\leavevmode\nobreak\ \pm\leavevmode\nobreak\ </annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.3.3.3.1.m1.1d">±</annotation></semantics></math>10</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.4.4.4">
<td class="ltx_td ltx_align_center" id="Ch7.T1.4.4.4.2"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.4.4.4.3">Opt. Camera</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.4.4.4.4">1200 m</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.4.4.4.1">53<math alttext="\leavevmode\nobreak\ \pm\leavevmode\nobreak\ " class="ltx_Math" display="inline" id="Ch7.T1.4.4.4.1.m1.1"><semantics id="Ch7.T1.4.4.4.1.m1.1a"><mo id="Ch7.T1.4.4.4.1.m1.1.1" xref="Ch7.T1.4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.4.4.4.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.4.4.4.1.m1.1.1.cmml" xref="Ch7.T1.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.4.4.4.1.m1.1c">\leavevmode\nobreak\ \pm\leavevmode\nobreak\ </annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.4.4.4.1.m1.1d">±</annotation></semantics></math>24</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.5.5.5">
<td class="ltx_td ltx_align_center" id="Ch7.T1.5.5.5.2"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a></td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.5.5.5.3">Opt. Camera</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.5.5.5.4">400 m</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.5.5.5.1">18<math alttext="\leavevmode\nobreak\ \pm\leavevmode\nobreak\ " class="ltx_Math" display="inline" id="Ch7.T1.5.5.5.1.m1.1"><semantics id="Ch7.T1.5.5.5.1.m1.1a"><mo id="Ch7.T1.5.5.5.1.m1.1.1" xref="Ch7.T1.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.5.5.5.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.5.5.5.1.m1.1.1.cmml" xref="Ch7.T1.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.5.5.5.1.m1.1c">\leavevmode\nobreak\ \pm\leavevmode\nobreak\ </annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.5.5.5.1.m1.1d">±</annotation></semantics></math>10</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.6.6.6">
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.6.2">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>*</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.6.3">Opt. Camera</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.6.4">1200 m</td>
<td class="ltx_td ltx_align_center" id="Ch7.T1.6.6.6.1">44<math alttext="\leavevmode\nobreak\ \pm\leavevmode\nobreak\ " class="ltx_Math" display="inline" id="Ch7.T1.6.6.6.1.m1.1"><semantics id="Ch7.T1.6.6.6.1.m1.1a"><mo id="Ch7.T1.6.6.6.1.m1.1.1" xref="Ch7.T1.6.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Ch7.T1.6.6.6.1.m1.1b"><csymbol cd="latexml" id="Ch7.T1.6.6.6.1.m1.1.1.cmml" xref="Ch7.T1.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch7.T1.6.6.6.1.m1.1c">\leavevmode\nobreak\ \pm\leavevmode\nobreak\ </annotation><annotation encoding="application/x-llamapun" id="Ch7.T1.6.6.6.1.m1.1d">±</annotation></semantics></math>27</td>
</tr>
<tr class="ltx_tr" id="Ch7.T1.6.6.10">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" id="Ch7.T1.6.6.10.1"><span class="ltx_text" id="Ch7.T1.6.6.10.1.1" style="font-size:80%;">*Value not reported in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> but calculated for this table.</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Ch7.S3.p14">
<p class="ltx_p" id="Ch7.S3.p14.1">Given the absence of directly comparable methodologies that simultaneously address the use of monocular cameras without prior camera pose knowledge for fast ship georeferencing, this approach establishes a benchmark in the literature. A comparison of the obtained results with existing ship georeferencing accuracies that use other technologies can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.T1" title="Table 7.1 ‣ 7.3 Analysis of Ship Segmentation and Georeferencing Using Homographies [BCP-II] [BCP-V] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.1</span></a>, which is an updated version of Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch3.T1" title="Table 3.1 ‣ 3.3 Georeferencing of Recognized Ships ‣ Chapter 3 Relevant State of the Art ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">3.1</span></a> for ship georeferencing. The most comparable set-up and result is given by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#bib.bib24" title="">helgesen2020low </a></cite>, that utilizes prior knowledge of camera calibration, and its application was limited to controlled conditions with a single video sequence of a two small ships. Our method obtains similar positioning error, however providing a much more comprehensive study: results using two views, longer ranges, uncertainties, a large variety of ships of different categories and sizes, and does not need prior camera pose knowledge.</p>
</div>
<div class="ltx_para" id="Ch7.S3.p15">
<p class="ltx_p" id="Ch7.S3.p15.1">In this section, I expanded upon the georeferencing methodology of Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7.S2" title="7.2 Ship Detection and Georeferencing Using Homographies [BCP-I] ‣ Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7.2</span></a> to show quantitative results of the use of homographies for ship georeferencing on ShipSG, based on <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.
The results prove that the approach provides useful information from the segmented ships to the situational awareness picture.
This information involves the representation of the ship on a global scale using single images and without prior knowledge of the camera, standing out in the literature.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S4">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">7.4   Summary and Discussion</h3>
<div class="ltx_para" id="Ch7.S4.p1">
<p class="ltx_p" id="Ch7.S4.p1.1">Ship georeferencing using maritime footage, as discussed in this chapter, stands as a pivotal element in improving situational awareness.
We have seen an in-depth exploration of ship georeferencing techniques using single images for enhancing maritime situational awareness, as presented in  <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p2">
<p class="ltx_p" id="Ch7.S4.p2.1">This chapter started with a foundational understanding of homographies, essential for the proposed method and quantitative studies of ship georeferencing.
Through the utilization of homographies, we could qualitatively and quantitatively explore the mapping of recognized ships from monocular images to geographic coordinates without previous knowledge about the camera.
This is a significant benefit for its versatility; it can be applied any existing camera setup, as long as there are identifiable reference points on the surface to create the homography.
This fundamental understanding provided the basis for subsequent discussions on ship detection, heading calculation using optical flow, and the quantitative analysis of ship segmentation and georeferencing using the ShipSG dataset.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p3">
<p class="ltx_p" id="Ch7.S4.p3.1">We have seen the practical application of ship detection, georeferencing and heading to support abnormal behaviour detection in the maritime domain.
The process allows, using a created homography from static camera view, to subsequently map ship locations to geographic coordinates and obtain the heading direction for visualization.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p4">
<p class="ltx_p" id="Ch7.S4.p4.1">Bounding boxes, while useful for detection, introduce unnecessary background noise and inaccuracies, particularly when the center of the bounding box is used for georeferencing.
Masks, on the other hand, delineate the precise contours of ships, providing a more precise representation for geospatial mapping.
This precision is crucial for applications requiring exact geolocated data for improved situational awareness, such as assisted navigation, maritime monitoring or maritime safety and security operations.
Therefore, the complete pipeline first segments ships from images, yielding precise ship masks.
The segmentation is followed by the automatic search of the pixel within the mask that intersects the ship hull and the water below, at the point where the navigation antenna is located on the ship.
The pixel coordinates are then transformed into real-world geographic coordinates through a homography transformation.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p5">
<p class="ltx_p" id="Ch7.S4.p5.2">Quantitative analyses of ship segmentation and georeferencing using ShipSG uncovered promising accuracy, with a positioning error of <math alttext="18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 13%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S4.p5.1.m1.1"><semantics id="Ch7.S4.p5.1.m1.1a"><mrow id="Ch7.S4.p5.1.m1.1.1" xref="Ch7.S4.p5.1.m1.1.1.cmml"><mrow id="Ch7.S4.p5.1.m1.1.1.2" xref="Ch7.S4.p5.1.m1.1.1.2.cmml"><mn id="Ch7.S4.p5.1.m1.1.1.2.2" xref="Ch7.S4.p5.1.m1.1.1.2.2.cmml">18</mn><mo id="Ch7.S4.p5.1.m1.1.1.2.1" lspace="0.500em" xref="Ch7.S4.p5.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S4.p5.1.m1.1.1.2.3" xref="Ch7.S4.p5.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S4.p5.1.m1.1.1.1" lspace="0.722em" xref="Ch7.S4.p5.1.m1.1.1.1.cmml">±</mo><mrow id="Ch7.S4.p5.1.m1.1.1.3" xref="Ch7.S4.p5.1.m1.1.1.3.cmml"><mn id="Ch7.S4.p5.1.m1.1.1.3.2" xref="Ch7.S4.p5.1.m1.1.1.3.2.cmml"> 13</mn><mo id="Ch7.S4.p5.1.m1.1.1.3.1" lspace="0.500em" xref="Ch7.S4.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S4.p5.1.m1.1.1.3.3" xref="Ch7.S4.p5.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S4.p5.1.m1.1b"><apply id="Ch7.S4.p5.1.m1.1.1.cmml" xref="Ch7.S4.p5.1.m1.1.1"><csymbol cd="latexml" id="Ch7.S4.p5.1.m1.1.1.1.cmml" xref="Ch7.S4.p5.1.m1.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S4.p5.1.m1.1.1.2.cmml" xref="Ch7.S4.p5.1.m1.1.1.2"><times id="Ch7.S4.p5.1.m1.1.1.2.1.cmml" xref="Ch7.S4.p5.1.m1.1.1.2.1"></times><cn id="Ch7.S4.p5.1.m1.1.1.2.2.cmml" type="integer" xref="Ch7.S4.p5.1.m1.1.1.2.2">18</cn><ci id="Ch7.S4.p5.1.m1.1.1.2.3.cmml" xref="Ch7.S4.p5.1.m1.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S4.p5.1.m1.1.1.3.cmml" xref="Ch7.S4.p5.1.m1.1.1.3"><times id="Ch7.S4.p5.1.m1.1.1.3.1.cmml" xref="Ch7.S4.p5.1.m1.1.1.3.1"></times><cn id="Ch7.S4.p5.1.m1.1.1.3.2.cmml" type="integer" xref="Ch7.S4.p5.1.m1.1.1.3.2">13</cn><ci id="Ch7.S4.p5.1.m1.1.1.3.3.cmml" xref="Ch7.S4.p5.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S4.p5.1.m1.1c">18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 13%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S4.p5.1.m1.1d">18 italic_m ± 13 italic_m</annotation></semantics></math> m in range of up to 400 m and <math alttext="44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch7.S4.p5.2.m2.1"><semantics id="Ch7.S4.p5.2.m2.1a"><mrow id="Ch7.S4.p5.2.m2.1.1" xref="Ch7.S4.p5.2.m2.1.1.cmml"><mrow id="Ch7.S4.p5.2.m2.1.1.2" xref="Ch7.S4.p5.2.m2.1.1.2.cmml"><mn id="Ch7.S4.p5.2.m2.1.1.2.2" xref="Ch7.S4.p5.2.m2.1.1.2.2.cmml">44</mn><mo id="Ch7.S4.p5.2.m2.1.1.2.1" lspace="0.500em" xref="Ch7.S4.p5.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch7.S4.p5.2.m2.1.1.2.3" xref="Ch7.S4.p5.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="Ch7.S4.p5.2.m2.1.1.1" lspace="0.722em" xref="Ch7.S4.p5.2.m2.1.1.1.cmml">±</mo><mrow id="Ch7.S4.p5.2.m2.1.1.3" xref="Ch7.S4.p5.2.m2.1.1.3.cmml"><mn id="Ch7.S4.p5.2.m2.1.1.3.2" xref="Ch7.S4.p5.2.m2.1.1.3.2.cmml"> 27</mn><mo id="Ch7.S4.p5.2.m2.1.1.3.1" lspace="0.500em" xref="Ch7.S4.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch7.S4.p5.2.m2.1.1.3.3" xref="Ch7.S4.p5.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch7.S4.p5.2.m2.1b"><apply id="Ch7.S4.p5.2.m2.1.1.cmml" xref="Ch7.S4.p5.2.m2.1.1"><csymbol cd="latexml" id="Ch7.S4.p5.2.m2.1.1.1.cmml" xref="Ch7.S4.p5.2.m2.1.1.1">plus-or-minus</csymbol><apply id="Ch7.S4.p5.2.m2.1.1.2.cmml" xref="Ch7.S4.p5.2.m2.1.1.2"><times id="Ch7.S4.p5.2.m2.1.1.2.1.cmml" xref="Ch7.S4.p5.2.m2.1.1.2.1"></times><cn id="Ch7.S4.p5.2.m2.1.1.2.2.cmml" type="integer" xref="Ch7.S4.p5.2.m2.1.1.2.2">44</cn><ci id="Ch7.S4.p5.2.m2.1.1.2.3.cmml" xref="Ch7.S4.p5.2.m2.1.1.2.3">𝑚</ci></apply><apply id="Ch7.S4.p5.2.m2.1.1.3.cmml" xref="Ch7.S4.p5.2.m2.1.1.3"><times id="Ch7.S4.p5.2.m2.1.1.3.1.cmml" xref="Ch7.S4.p5.2.m2.1.1.3.1"></times><cn id="Ch7.S4.p5.2.m2.1.1.3.2.cmml" type="integer" xref="Ch7.S4.p5.2.m2.1.1.3.2">27</cn><ci id="Ch7.S4.p5.2.m2.1.1.3.3.cmml" xref="Ch7.S4.p5.2.m2.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch7.S4.p5.2.m2.1c">44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch7.S4.p5.2.m2.1d">44 italic_m ± 27 italic_m</annotation></semantics></math> from 400 m to 1200 m.
The computational time for the pixel search and homography multiplication, averaging 0.5 milliseconds, is remarkably low in comparison to the instance segmentation inference times.
This efficiency contrasts with the time-intensive nature of instance segmentation inference on an embedded system, highlighting that the major computational demand lies in mask segmentation.
The need for faster and more accurate ship segmentation was addressed by the proposed advanced methodologies for ship segmentation in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p6">
<p class="ltx_p" id="Ch7.S4.p6.1">The analyses underscored the reliability and efficiency of the proposed georeferencing methods, laying the ground for their integration into a real-time maritime situational awareness system.
The georeferenced latitudes and longitudes of ships derived from this pipeline can be integrated into web services for real-time decision-making, significantly bolstering maritime situational awareness.
This integration facilitates the dynamic monitoring of maritime traffic, enabling prompt responses to navigational hazards, environmental risks, and security threats.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p7">
<p class="ltx_p" id="Ch7.S4.p7.1">In light of the state-of-the-art methodologies in ship georeferencing prior to this research, the proposed method stands out by diverging from traditional reliance on complex camera calibration, high-resolution orthophotos, or Digital Elevation Models (DEMs). The proposed approach provides a scalable, camera-agnostic solution that significantly advances applicability of maritime situational awareness technologies.</p>
</div>
<div class="ltx_para" id="Ch7.S4.p8">
<p class="ltx_p" id="Ch7.S4.p8.1">Looking ahead, future work in ship georeferencing could delve into further refining accuracy by incorporating additional ship annotations from the ShipSG dataset, such as keypoints and cuboids.
These annotations could facilitate a more nuanced consideration of ship dimensions in the georeferencing process, potentially reducing error margins by accounting for the ship’s length.
Moreover, the exploration of cuboids in conjunction with ShipSG annotations opens new avenues for calculating ship orientations without relying on optical flow calculations.
This approach could eliminate the need for sequential image analysis in anomaly detection, which demands higher frame rates, thereby streamlining the process of determining ship headings and orientations.
Future research could also benefit from analyzing sequences of images or videos and different camera perspectives of the same maritime scene, enhancing the depth and accuracy of situational awareness beyond the capabilities demonstrated with the ShipSG dataset.
Moreover, further fusion of the proposed image-based real-time ship georeferences with data and processes from multiple sensors, e.g. infrared imaging, radar and <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>, will provide a more comprehensive understanding of maritime situations.
Overall, ship georeferencing represents a critical component in enhancing maritime situational awareness, with significant potential for further development and integration into maritime surveillance systems.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch8" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 8 </span>Summary and Conclusion</h2>
<div class="ltx_para" id="Ch8.p1">
<p class="ltx_p" id="Ch8.p1.1">This compilation thesis addresses the enhancement of maritime situational awareness through advanced ship recognition and georeferencing methodologies. With the creation and utilization of the ShipSG dataset, we have established a benchmark in the field, which facilitates the development of recognition and georeferencing methodologies for maritime applications. This work is driven by the intuition that the integration of deep-learning-based object recognition methods, ship georeferencing and embedded systems can significantly advance the state of real-time maritime monitoring. The chapters within this thesis offer a progression from the creation of a foundational dataset to the implementation and deployment of real-time recognition (detection and segmentation) and georeferencing methods, following strategies to overcome the limitations of the previous existing literature. The key findings from each chapter are summarized below:</p>
</div>
<div class="ltx_para" id="Ch8.p2">
<p class="ltx_p" id="Ch8.p2.1"><span class="ltx_text ltx_font_bold" id="Ch8.p2.1.1">Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch4" title="Chapter 4 ShipSG: Ship Segmentation and Georeferencing Dataset ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">4</span></a></span> shows the creation of a novel dataset, ShipSG, for ship segmentation and georeferencing with two views of a maritime infrastructure. The dataset contains 3505 images and 11625 ship masks with their corresponding class, position and length. ShipSG marks a significant advancement in the field by setting a new standard for ship segmentation and georeferencing research. Moreover, it plays a crucial role in validating innovative methodologies for maritime situational awareness presented in this thesis. The dataset has facilitated the verification of recognition methods discussed in the chapters dedicated to ship recognition and advanced ship recognition, along with the quantitative validation of the proposed georeferencing methods. The contribution of ShipSG is underscored with its use in publications <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a><a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a>, evidencing its importance and utility in pushing the boundaries of maritime situational awareness research.</p>
</div>
<div class="ltx_para" id="Ch8.p3">
<p class="ltx_p" id="Ch8.p3.1"><span class="ltx_text ltx_font_bold" id="Ch8.p3.1.1">Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch5" title="Chapter 5 Ship Recognition for Improved Maritime Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">5</span></a></span> showcased the initial investigation in applying deep learning techniques for ship detection and segmentation within maritime applications to enhance situational awareness, underscoring the pivotal role of these methods in facilitating a range of applications, such as abnormal vessel behavior detection <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S1" title="[BCP-I] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-I]</span></a>, camera integrity assessment <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S3" title="[BCP-III] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-III]</span></a> and 3D ship reconstruction <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>.
Despite the demonstrated potential and success for ship detection in controlled or synthetic settings, the exploration of standard ship segmentation methods <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> was highlighted for the superiority of instance segmentation over traditional bounding box detection in extracting detailed ship features crucial for applications like georeferencing. Challenges remained, notably in the precision, real-time processing, and the deployment of these technologies on GPU-powered embedded systems. The initial studies also pointed out a decrease in precision when segmenting small and distant ships, emphasizing the need for improved detection methods. Overall, while significant potential for enhancing maritime situational awareness has been revealed in this chapter, this exploration also uncovers the need for further development in real-time ship segmentation, the recognition of small and distant ships, and deployment on embedded systems, with subsequent chapters aiming to address these gaps through custom-tailored solutions for practical deployment in the maritime domain.</p>
</div>
<div class="ltx_para" id="Ch8.p4">
<p class="ltx_p" id="Ch8.p4.1"><span class="ltx_text ltx_font_bold" id="Ch8.p4.1.1">Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch6" title="Chapter 6 Advanced Ship Recognition for Real-time Operation ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">6</span></a></span> builds advances in the field real-time ship segmentation with the design of a customized architecture, ScatYOLOv8+CBAM <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>, and demonstrated its enhanced performance on ShipSG (<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> 75.46%).
The chapter analyzed an optimization of the architecture <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>, that removes the upsampling and downsampling from the ScatBlock to save computing time, and deployed it with TensoRT on the Jetson AGX Xavier to measure inference times for real-time applicability, which brought an acceleration of 36.5% of the inference speed.
Moreover, the chapter proposed a batched-processing <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> to increase the segmentation performance of small and distant ships that is able to run on embedded systems, emphasizing on the use of high-resolution images for a better understanding of the maritime situation. The <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.map"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.map" title="mean Average Precision">mAP</span></a> in small ship segmentation compared to the baseline achieved an improvement of 8% to 11%, however resulted also in a slowdown in inference speed.
Choosing the optimal ScatYOLOv8+CBAM model size and incorporating <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a> depends on balancing real-time processing needs with computational capabilities. For critical applications like port surveillance, smaller configurations offer quick, accurate responses. However, larger models improve precision both in general and specifically in small and distant ships. Understanding specific needs and computational trade-offs is key to effective deployment.
The presented advances bridge the transition from standard methods to real-time instance segmentation on embedded systems, and addresses the ability to accurately identify all ships, independent from their size, and within the proximity of the port area.</p>
</div>
<div class="ltx_para" id="Ch8.p5">
<p class="ltx_p" id="Ch8.p5.2"><span class="ltx_text ltx_font_bold" id="Ch8.p5.2.1">Chapter <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch7" title="Chapter 7 Ship Georeferencing for Maritime Situational Awareness ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">7</span></a></span> identifies ship georeferencing using maritime footage as a pivotal element for enhancing maritime situational awareness. This is done through the application of the ship georeferencing method developed in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and further validated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>. The chapter begins with a foundational understanding of homographies, crucial for the method, followed by qualitative and quantitative analysis of ship latitude and longitude positions from monocular images to geographic coordinates without prior camera knowledge.
The qualitative analysis was used in a practical application; abnormal behavior detection in the maritime domain, where georeferences where used for ship positioning and heading direction, leveraging optical flow.
In the quantitative analysis, the georeferencing, using ScatYOLOv8+CBAM masks, achieved a positioning error of <math alttext="18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch8.p5.1.m1.1"><semantics id="Ch8.p5.1.m1.1a"><mrow id="Ch8.p5.1.m1.1.1" xref="Ch8.p5.1.m1.1.1.cmml"><mrow id="Ch8.p5.1.m1.1.1.2" xref="Ch8.p5.1.m1.1.1.2.cmml"><mn id="Ch8.p5.1.m1.1.1.2.2" xref="Ch8.p5.1.m1.1.1.2.2.cmml">18</mn><mo id="Ch8.p5.1.m1.1.1.2.1" lspace="0.500em" xref="Ch8.p5.1.m1.1.1.2.1.cmml">⁢</mo><mi id="Ch8.p5.1.m1.1.1.2.3" xref="Ch8.p5.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="Ch8.p5.1.m1.1.1.1" lspace="0.722em" xref="Ch8.p5.1.m1.1.1.1.cmml">±</mo><mrow id="Ch8.p5.1.m1.1.1.3" xref="Ch8.p5.1.m1.1.1.3.cmml"><mn id="Ch8.p5.1.m1.1.1.3.2" xref="Ch8.p5.1.m1.1.1.3.2.cmml"> 10</mn><mo id="Ch8.p5.1.m1.1.1.3.1" lspace="0.500em" xref="Ch8.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch8.p5.1.m1.1.1.3.3" xref="Ch8.p5.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch8.p5.1.m1.1b"><apply id="Ch8.p5.1.m1.1.1.cmml" xref="Ch8.p5.1.m1.1.1"><csymbol cd="latexml" id="Ch8.p5.1.m1.1.1.1.cmml" xref="Ch8.p5.1.m1.1.1.1">plus-or-minus</csymbol><apply id="Ch8.p5.1.m1.1.1.2.cmml" xref="Ch8.p5.1.m1.1.1.2"><times id="Ch8.p5.1.m1.1.1.2.1.cmml" xref="Ch8.p5.1.m1.1.1.2.1"></times><cn id="Ch8.p5.1.m1.1.1.2.2.cmml" type="integer" xref="Ch8.p5.1.m1.1.1.2.2">18</cn><ci id="Ch8.p5.1.m1.1.1.2.3.cmml" xref="Ch8.p5.1.m1.1.1.2.3">𝑚</ci></apply><apply id="Ch8.p5.1.m1.1.1.3.cmml" xref="Ch8.p5.1.m1.1.1.3"><times id="Ch8.p5.1.m1.1.1.3.1.cmml" xref="Ch8.p5.1.m1.1.1.3.1"></times><cn id="Ch8.p5.1.m1.1.1.3.2.cmml" type="integer" xref="Ch8.p5.1.m1.1.1.3.2">10</cn><ci id="Ch8.p5.1.m1.1.1.3.3.cmml" xref="Ch8.p5.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.p5.1.m1.1c">18\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 10%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch8.p5.1.m1.1d">18 italic_m ± 10 italic_m</annotation></semantics></math> for ranges inside the port basin (up to 400 m) and <math alttext="44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m" class="ltx_Math" display="inline" id="Ch8.p5.2.m2.1"><semantics id="Ch8.p5.2.m2.1a"><mrow id="Ch8.p5.2.m2.1.1" xref="Ch8.p5.2.m2.1.1.cmml"><mrow id="Ch8.p5.2.m2.1.1.2" xref="Ch8.p5.2.m2.1.1.2.cmml"><mn id="Ch8.p5.2.m2.1.1.2.2" xref="Ch8.p5.2.m2.1.1.2.2.cmml">44</mn><mo id="Ch8.p5.2.m2.1.1.2.1" lspace="0.500em" xref="Ch8.p5.2.m2.1.1.2.1.cmml">⁢</mo><mi id="Ch8.p5.2.m2.1.1.2.3" xref="Ch8.p5.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="Ch8.p5.2.m2.1.1.1" lspace="0.722em" xref="Ch8.p5.2.m2.1.1.1.cmml">±</mo><mrow id="Ch8.p5.2.m2.1.1.3" xref="Ch8.p5.2.m2.1.1.3.cmml"><mn id="Ch8.p5.2.m2.1.1.3.2" xref="Ch8.p5.2.m2.1.1.3.2.cmml"> 27</mn><mo id="Ch8.p5.2.m2.1.1.3.1" lspace="0.500em" xref="Ch8.p5.2.m2.1.1.3.1.cmml">⁢</mo><mi id="Ch8.p5.2.m2.1.1.3.3" xref="Ch8.p5.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch8.p5.2.m2.1b"><apply id="Ch8.p5.2.m2.1.1.cmml" xref="Ch8.p5.2.m2.1.1"><csymbol cd="latexml" id="Ch8.p5.2.m2.1.1.1.cmml" xref="Ch8.p5.2.m2.1.1.1">plus-or-minus</csymbol><apply id="Ch8.p5.2.m2.1.1.2.cmml" xref="Ch8.p5.2.m2.1.1.2"><times id="Ch8.p5.2.m2.1.1.2.1.cmml" xref="Ch8.p5.2.m2.1.1.2.1"></times><cn id="Ch8.p5.2.m2.1.1.2.2.cmml" type="integer" xref="Ch8.p5.2.m2.1.1.2.2">44</cn><ci id="Ch8.p5.2.m2.1.1.2.3.cmml" xref="Ch8.p5.2.m2.1.1.2.3">𝑚</ci></apply><apply id="Ch8.p5.2.m2.1.1.3.cmml" xref="Ch8.p5.2.m2.1.1.3"><times id="Ch8.p5.2.m2.1.1.3.1.cmml" xref="Ch8.p5.2.m2.1.1.3.1"></times><cn id="Ch8.p5.2.m2.1.1.3.2.cmml" type="integer" xref="Ch8.p5.2.m2.1.1.3.2">27</cn><ci id="Ch8.p5.2.m2.1.1.3.3.cmml" xref="Ch8.p5.2.m2.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch8.p5.2.m2.1c">44\leavevmode\nobreak\ m\leavevmode\nobreak\ \pm\leavevmode\nobreak\ 27%
\leavevmode\nobreak\ m</annotation><annotation encoding="application/x-llamapun" id="Ch8.p5.2.m2.1d">44 italic_m ± 27 italic_m</annotation></semantics></math> on the river (from 400 m to 1200 m), which improves upon existing results in the literature.
Moreover, the measured average time for the georeferencing, 0.5 ms per frame on the Jetson AGX Xavier, is not significant when compared to the instance segmentation timings.
The versatility of this approach and its applicability to any camera setup with identifiable surface reference points, sets a solid groundwork for further work on ship recognition and georeferencing, where the ShipSG dataset can be used as a benchmark. The real-world applicability of georeferenced ship coordinates to support real-time decision-making processes, alone or in combination with other sensors and processes, shows the significant potential of this research to enhance maritime situational awareness.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="Ch8.p6">
<p class="ltx_p" id="Ch8.p6.1">The outcome of this thesis reflects a significant advancement in maritime situational awareness through the deployment of novel methods both for real-time ship segmentation and georeferencing that are able to run on an embedded system. The introduction of the ScatYoloV8+CBAM architecture, optimized for the ShipSG dataset, demonstrates improved performance metrics over existing methodologies.
Focusing on precise ship recognition and georeferencing regardless of class and size, the research has yielded a framework that, when applied, enhances the situational awareness of maritime stakeholders by displaying accurately located ships on digital maps, thereby consolidating knowledge into a user-friendly format which is easy to interpret and act upon.
The methodological choices have been centered on maximum accuracy and minimal processing times on embedded systems. This balance of speed and precision facilitates the integration with other sensor data and services, ultimately advancing maritime operations to be safer, more secure, and efficient.
This rationale ensures that the presented work has tangible impacts.
This thesis has thus contributed to the body of knowledge with validated approaches that aim to facilitate monitoring in the vicinity of maritime infrastructures. Moreover, the methods and findings presented here offer a solid foundation for future research directed at refining high-resolution, real-time processing in resource-limited maritime contexts.</p>
</div>
</section>
<section class="ltx_chapter" id="Ch9" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 9 </span>Future Work</h2>
<div class="ltx_para" id="Ch9.p1">
<p class="ltx_p" id="Ch9.p1.1">The outcomes and challenges presented in this thesis suggest several areas that are worth further research.</p>
</div>
<div class="ltx_para" id="Ch9.p2">
<p class="ltx_p" id="Ch9.p2.1">With regards to the ShipSG dataset, future iterations of the dataset will focus on expanding its diversity by incorporating images from various cameras, and including higher resolution images, for more detailed analysis. This initiative aims to address the current limitations related to the variability and detail of maritime scenes, including more adverse weather conditions. Moreover, further improvements will involve leveraging <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a> data for annotating ship heading, and enriching the dataset with additional annotations like ship cuboids or keypoints. These advancements will support the development of more sophisticated algorithms for automatically recognizing ship heading and dimensions, significantly benefiting maritime situational awareness research.</p>
</div>
<div class="ltx_para" id="Ch9.p3">
<p class="ltx_p" id="Ch9.p3.1">Focusing on improvements of ScatYOLOv8+CBAM and the overall performance of ship segmentation, key areas for advancement include refining the architecture to manage computational demands of larger models more effectively. Potential strategies for improvement involve making the ScatBlock learnable by introducing adjustable parameters within the wavelet filters, or enhancing the ScatBlock by adding learnable modules, such as strided convolutions and integration of transformers, to focus on significant features more efficiently. Additionally, optimizing the slicing and merging tasks in batch-inference processes through parallel processing techniques like multi-threading could significantly enhance efficiency when processing high-resolution images for small ship recognition.</p>
</div>
<div class="ltx_para" id="Ch9.p4">
<p class="ltx_p" id="Ch9.p4.1">Future work in ship georeferencing should focus on enhancing accuracy by using additional annotations on the ShipSG dataset, specifically keypoints and cuboids, to refine the georeferencing process by considering ship dimensions. Exploring cuboids alongside ShipSG annotations could also streamline ship orientation calculations, potentially eliminating the need for high frame-rate sequential image analysis in anomaly detection pipelines.</p>
</div>
<div class="ltx_para" id="Ch9.p5">
<p class="ltx_p" id="Ch9.p5.1">Finally, by integrating the methodologies presented in this thesis with additional processing chains and sensor data, we could contribute to the production of a broad and further enhanced situational awareness picture. This involves creating real-time visualizations of georeferenced ships using web services. For example, a dynamic map where ship positions update constantly, providing a clear picture of maritime situation. Furthermore, by merging diverse data sources such as ship tracking, 3D reconstruction, anomaly detection, and information from other sensors and data sources like thermal imaging, radar or <a href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.ais" title="Automatic Identification System">AIS</span></a>, we can gain a more comprehensive understanding of the maritime environment. This fusion of data will help authorities identify potential hazards, improve navigation safety, and optimize resource allocation for search and rescue operations.</p>
</div>
</section>
<section class="ltx_chapter" id="Ch10" lang="en">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 10 </span>Publications by the Author for this Thesis</h2>
<div class="ltx_para" id="Ch10.p1">
<p class="ltx_p" id="Ch10.p1.1">This chapter presents a chronological list of the publications used for this compilation thesis, together with my shares on the works and a short summary of my contributions to each publication.</p>
</div>
<section class="ltx_section" id="Ch10.S1">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-I]   </h3>
<div class="ltx_para" id="Ch10.S1.p1">
<p class="ltx_p" id="Ch10.S1.p1.1">E. Solano, <span class="ltx_text ltx_font_bold" id="Ch10.S1.p1.1.1">B. Carrillo-Perez</span>, T. Flenker, Y. Steiniger, and J. Stoppe, <span class="ltx_text ltx_font_bold" id="Ch10.S1.p1.1.2">”Detection and Geovisualization of Abnormal Vessel Behavior from Video,”</span> in 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 2021, pp. 2193–2199.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="Ch10.S1.p2">
<p class="ltx_p" id="Ch10.S1.p2.1">My share on this publication is 35%. Summary of contributions as <math alttext="2^{nd}" class="ltx_Math" display="inline" id="Ch10.S1.p2.1.m1.1"><semantics id="Ch10.S1.p2.1.m1.1a"><msup id="Ch10.S1.p2.1.m1.1.1" xref="Ch10.S1.p2.1.m1.1.1.cmml"><mn id="Ch10.S1.p2.1.m1.1.1.2" xref="Ch10.S1.p2.1.m1.1.1.2.cmml">2</mn><mrow id="Ch10.S1.p2.1.m1.1.1.3" xref="Ch10.S1.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S1.p2.1.m1.1.1.3.2" xref="Ch10.S1.p2.1.m1.1.1.3.2.cmml">n</mi><mo id="Ch10.S1.p2.1.m1.1.1.3.1" xref="Ch10.S1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S1.p2.1.m1.1.1.3.3" xref="Ch10.S1.p2.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S1.p2.1.m1.1b"><apply id="Ch10.S1.p2.1.m1.1.1.cmml" xref="Ch10.S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S1.p2.1.m1.1.1.1.cmml" xref="Ch10.S1.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S1.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S1.p2.1.m1.1.1.2">2</cn><apply id="Ch10.S1.p2.1.m1.1.1.3.cmml" xref="Ch10.S1.p2.1.m1.1.1.3"><times id="Ch10.S1.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S1.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S1.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S1.p2.1.m1.1.1.3.2">𝑛</ci><ci id="Ch10.S1.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S1.p2.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S1.p2.1.m1.1c">2^{nd}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S1.p2.1.m1.1d">2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S1.p3">
<ul class="ltx_itemize" id="Ch10.S1.I1">
<li class="ltx_item" id="Ch10.S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i1.p1">
<p class="ltx_p" id="Ch10.S1.I1.i1.p1.1">Training and validation of YOLOv4 using custom dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i2.p1">
<p class="ltx_p" id="Ch10.S1.I1.i2.p1.1">Implementation of pipeline for ship detection and georeferencing from Sydney harbour video sequence.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i3.p1">
<p class="ltx_p" id="Ch10.S1.I1.i3.p1.1">Development of a homography for ship georeferencing and adapted it for the pipeline.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i4.p1">
<p class="ltx_p" id="Ch10.S1.I1.i4.p1.1">Calculation of ship course estimation of detected ships using optical flow and homography.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i5.p1">
<p class="ltx_p" id="Ch10.S1.I1.i5.p1.1">I advised and participated in the writing of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i6.p1">
<p class="ltx_p" id="Ch10.S1.I1.i6.p1.1">Outcome: Proof of concept for ship detection and georeferencing to improve maritime situational awareness.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S1.I1.i7.p1">
<p class="ltx_p" id="Ch10.S1.I1.i7.p1.1">Next step: Quantitative analysis of the georeferencing method was not reported. Achieved in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S2" title="[BCP-II] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-II]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch10.S2">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-II]   </h3>
<div class="ltx_para" id="Ch10.S2.p1">
<p class="ltx_p" id="Ch10.S2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch10.S2.p1.1.1">B. Carrillo-Perez</span>, S. Barnes, and M. Stephan, <span class="ltx_text ltx_font_bold" id="Ch10.S2.p1.1.2">”Ship Segmentation and Georeferencing from Static Oblique View Images,”</span> Sensors, vol. 22, no. 7, p. 2713, Apr. 2022.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="Ch10.S2.p2">
<p class="ltx_p" id="Ch10.S2.p2.1">My share on this publication is 90%. Summary of contributions as <math alttext="1^{st}" class="ltx_Math" display="inline" id="Ch10.S2.p2.1.m1.1"><semantics id="Ch10.S2.p2.1.m1.1a"><msup id="Ch10.S2.p2.1.m1.1.1" xref="Ch10.S2.p2.1.m1.1.1.cmml"><mn id="Ch10.S2.p2.1.m1.1.1.2" xref="Ch10.S2.p2.1.m1.1.1.2.cmml">1</mn><mrow id="Ch10.S2.p2.1.m1.1.1.3" xref="Ch10.S2.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S2.p2.1.m1.1.1.3.2" xref="Ch10.S2.p2.1.m1.1.1.3.2.cmml">s</mi><mo id="Ch10.S2.p2.1.m1.1.1.3.1" xref="Ch10.S2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S2.p2.1.m1.1.1.3.3" xref="Ch10.S2.p2.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S2.p2.1.m1.1b"><apply id="Ch10.S2.p2.1.m1.1.1.cmml" xref="Ch10.S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S2.p2.1.m1.1.1.1.cmml" xref="Ch10.S2.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S2.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S2.p2.1.m1.1.1.2">1</cn><apply id="Ch10.S2.p2.1.m1.1.1.3.cmml" xref="Ch10.S2.p2.1.m1.1.1.3"><times id="Ch10.S2.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S2.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S2.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S2.p2.1.m1.1.1.3.2">𝑠</ci><ci id="Ch10.S2.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S2.p2.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S2.p2.1.m1.1c">1^{st}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S2.p2.1.m1.1d">1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S2.p3">
<ul class="ltx_itemize" id="Ch10.S2.I1">
<li class="ltx_item" id="Ch10.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i1.p1">
<p class="ltx_p" id="Ch10.S2.I1.i1.p1.1">Creation and publication of novel dataset for ship recognition and georeferencing (ShipSG).</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i2.p1">
<p class="ltx_p" id="Ch10.S2.I1.i2.p1.1">In-depth study of four state-of-the-art real-time ship segmentation methods, using ShipSG.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i3.p1">
<p class="ltx_p" id="Ch10.S2.I1.i3.p1.1">Quantitative analysis of the homography-based ship georeferencing method, using ShipSG.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i4.p1">
<p class="ltx_p" id="Ch10.S2.I1.i4.p1.1">Preparation of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i5.p1">
<p class="ltx_p" id="Ch10.S2.I1.i5.p1.1">Outcome: Definition of a pipeline for ship segmentation and georeferencing for ship display on situational awareness maps.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S2.I1.i6.p1">
<p class="ltx_p" id="Ch10.S2.I1.i6.p1.1">Next step: Deployment on embedded system was not reported. Achieved in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S4" title="[BCP-IV] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-IV]</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch10.S3">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-III]   </h3>
<div class="ltx_para" id="Ch10.S3.p1">
<p class="ltx_p" id="Ch10.S3.p1.1">F. A. Costa de Oliveira, <span class="ltx_text ltx_font_bold" id="Ch10.S3.p1.1.1">B. Carrillo-Perez</span>, A. García-Ortiz, and F. Sill-Torres, <span class="ltx_text ltx_font_bold" id="Ch10.S3.p1.1.2">”Integrity Assessment of Maritime Object Detection Impacted by Partial Camera Obstruction,”</span> in 2023 IEEE International Conference on System Reliability and Safety (ICSRS), Nov. 2023, pp. 474–480.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="Ch10.S3.p2">
<p class="ltx_p" id="Ch10.S3.p2.1">My share on this publication is 20%. Summary of contributions as <math alttext="2^{nd}" class="ltx_Math" display="inline" id="Ch10.S3.p2.1.m1.1"><semantics id="Ch10.S3.p2.1.m1.1a"><msup id="Ch10.S3.p2.1.m1.1.1" xref="Ch10.S3.p2.1.m1.1.1.cmml"><mn id="Ch10.S3.p2.1.m1.1.1.2" xref="Ch10.S3.p2.1.m1.1.1.2.cmml">2</mn><mrow id="Ch10.S3.p2.1.m1.1.1.3" xref="Ch10.S3.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S3.p2.1.m1.1.1.3.2" xref="Ch10.S3.p2.1.m1.1.1.3.2.cmml">n</mi><mo id="Ch10.S3.p2.1.m1.1.1.3.1" xref="Ch10.S3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S3.p2.1.m1.1.1.3.3" xref="Ch10.S3.p2.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S3.p2.1.m1.1b"><apply id="Ch10.S3.p2.1.m1.1.1.cmml" xref="Ch10.S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S3.p2.1.m1.1.1.1.cmml" xref="Ch10.S3.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S3.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S3.p2.1.m1.1.1.2">2</cn><apply id="Ch10.S3.p2.1.m1.1.1.3.cmml" xref="Ch10.S3.p2.1.m1.1.1.3"><times id="Ch10.S3.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S3.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S3.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S3.p2.1.m1.1.1.3.2">𝑛</ci><ci id="Ch10.S3.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S3.p2.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S3.p2.1.m1.1c">2^{nd}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S3.p2.1.m1.1d">2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S3.p3">
<ul class="ltx_itemize" id="Ch10.S3.I1">
<li class="ltx_item" id="Ch10.S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S3.I1.i1.p1">
<p class="ltx_p" id="Ch10.S3.I1.i1.p1.1">Training and validation of Faster R-CNN for ship detection using ShipSG. The study of obstructions impact was also validated using ShipSG.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S3.I1.i2.p1">
<p class="ltx_p" id="Ch10.S3.I1.i2.p1.1">I advised and participated in the writing of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S3.I1.i3.p1">
<p class="ltx_p" id="Ch10.S3.I1.i3.p1.1">Outcome: ShipSG dataset impacts other applications for the improvement of maritime situational awareness such as camera integrity assessment.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch10.S4">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-IV]   </h3>
<div class="ltx_para" id="Ch10.S4.p1">
<p class="ltx_p" id="Ch10.S4.p1.1">F. Sattler, <span class="ltx_text ltx_font_bold" id="Ch10.S4.p1.1.1">B. Carrillo-Perez</span>, S. Barnes, K. Stebner, M. Stephan, and G. Lux, <span class="ltx_text ltx_font_bold" id="Ch10.S4.p1.1.2">“Embedded 3D reconstruction of Dynamic Objects in Real Time for Maritime Situational Awareness Pictures,”</span> The Visual Computer, Springer, pp. 1–14, 2023. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="Ch10.S4.p2">
<p class="ltx_p" id="Ch10.S4.p2.1">My share on this publication is 15%. Summary of contributions as <math alttext="2^{nd}" class="ltx_Math" display="inline" id="Ch10.S4.p2.1.m1.1"><semantics id="Ch10.S4.p2.1.m1.1a"><msup id="Ch10.S4.p2.1.m1.1.1" xref="Ch10.S4.p2.1.m1.1.1.cmml"><mn id="Ch10.S4.p2.1.m1.1.1.2" xref="Ch10.S4.p2.1.m1.1.1.2.cmml">2</mn><mrow id="Ch10.S4.p2.1.m1.1.1.3" xref="Ch10.S4.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S4.p2.1.m1.1.1.3.2" xref="Ch10.S4.p2.1.m1.1.1.3.2.cmml">n</mi><mo id="Ch10.S4.p2.1.m1.1.1.3.1" xref="Ch10.S4.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S4.p2.1.m1.1.1.3.3" xref="Ch10.S4.p2.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S4.p2.1.m1.1b"><apply id="Ch10.S4.p2.1.m1.1.1.cmml" xref="Ch10.S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S4.p2.1.m1.1.1.1.cmml" xref="Ch10.S4.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S4.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S4.p2.1.m1.1.1.2">2</cn><apply id="Ch10.S4.p2.1.m1.1.1.3.cmml" xref="Ch10.S4.p2.1.m1.1.1.3"><times id="Ch10.S4.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S4.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S4.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S4.p2.1.m1.1.1.3.2">𝑛</ci><ci id="Ch10.S4.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S4.p2.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S4.p2.1.m1.1c">2^{nd}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S4.p2.1.m1.1d">2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S4.p3">
<ul class="ltx_itemize" id="Ch10.S4.I1">
<li class="ltx_item" id="Ch10.S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S4.I1.i1.p1">
<p class="ltx_p" id="Ch10.S4.I1.i1.p1.1">Training and validation of YOLOv5 for ship detection on synthetic dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S4.I1.i2.p1">
<p class="ltx_p" id="Ch10.S4.I1.i2.p1.1">Deployed detector on NVIDIA Jetson AGX Xavier, using Pytorch weights.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S4.I1.i3.p1">
<p class="ltx_p" id="Ch10.S4.I1.i3.p1.1">I advised and participated in the writing of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S4.I1.i4.p1">
<p class="ltx_p" id="Ch10.S4.I1.i4.p1.1">Outcome: Deployment of ship detector (bounding box) on embedded system for maritime situational awareness applications.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S4.I1.i5.p1">
<p class="ltx_p" id="Ch10.S4.I1.i5.p1.1">Next step: Deployment of instance segmentation (mask) on embedded system with fast inference speed and high accuracy on real dataset (ShipSG). Achieved in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S5" title="[BCP-V] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-V]</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch10.S5">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-V]   </h3>
<div class="ltx_para" id="Ch10.S5.p1">
<p class="ltx_p" id="Ch10.S5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch10.S5.p1.1.1">B. Carrillo-Perez</span>, A. Bueno Rodriguez, S. Barnes, and M. Stephan, <span class="ltx_text ltx_font_bold" id="Ch10.S5.p1.1.2">”Improving YOLOv8 with Scattering Transform and Attention for Maritime Awareness,”</span> in 2023 IEEE International Symposium on Image and Signal Processing and Analysis (ISPA), 2023, pp. 1–6.</p>
</div>
<div class="ltx_para" id="Ch10.S5.p2">
<p class="ltx_p" id="Ch10.S5.p2.1">My share on this publication is 90%. Summary of contributions as <math alttext="1^{st}" class="ltx_Math" display="inline" id="Ch10.S5.p2.1.m1.1"><semantics id="Ch10.S5.p2.1.m1.1a"><msup id="Ch10.S5.p2.1.m1.1.1" xref="Ch10.S5.p2.1.m1.1.1.cmml"><mn id="Ch10.S5.p2.1.m1.1.1.2" xref="Ch10.S5.p2.1.m1.1.1.2.cmml">1</mn><mrow id="Ch10.S5.p2.1.m1.1.1.3" xref="Ch10.S5.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S5.p2.1.m1.1.1.3.2" xref="Ch10.S5.p2.1.m1.1.1.3.2.cmml">s</mi><mo id="Ch10.S5.p2.1.m1.1.1.3.1" xref="Ch10.S5.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S5.p2.1.m1.1.1.3.3" xref="Ch10.S5.p2.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S5.p2.1.m1.1b"><apply id="Ch10.S5.p2.1.m1.1.1.cmml" xref="Ch10.S5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S5.p2.1.m1.1.1.1.cmml" xref="Ch10.S5.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S5.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S5.p2.1.m1.1.1.2">1</cn><apply id="Ch10.S5.p2.1.m1.1.1.3.cmml" xref="Ch10.S5.p2.1.m1.1.1.3"><times id="Ch10.S5.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S5.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S5.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S5.p2.1.m1.1.1.3.2">𝑠</ci><ci id="Ch10.S5.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S5.p2.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S5.p2.1.m1.1c">1^{st}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S5.p2.1.m1.1d">1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S5.p3">
<ul class="ltx_itemize" id="Ch10.S5.I1">
<li class="ltx_item" id="Ch10.S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i1.p1">
<p class="ltx_p" id="Ch10.S5.I1.i1.p1.1">Improved YOLOv8 for ship segmentation with the novel addition of 2D scattering transform and attention mechanism to conform ScatYOLOv8+CBAM. The model was validated on ShipSG.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i2.p1">
<p class="ltx_p" id="Ch10.S5.I1.i2.p1.1">Analysis of georeferencing results on ShipSG with the novel proposed architecture.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i3.p1">
<p class="ltx_p" id="Ch10.S5.I1.i3.p1.1">Deployment of instance segmentation on embedded system (NVIDIA Jetson AGX Xavier).</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i4.p1">
<p class="ltx_p" id="Ch10.S5.I1.i4.p1.1">Preparation of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i5.p1">
<p class="ltx_p" id="Ch10.S5.I1.i5.p1.1">Outcome: Faster and more accurate ship segmentation and georeferencing, deployed on embedded system.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S5.I1.i6.p1">
<p class="ltx_p" id="Ch10.S5.I1.i6.p1.1">Next step: Optimize architecture further and use higher image resolutions, specially for small or distant ship segmentation. Achieved in <a class="ltx_ref" href="https://arxiv.org/html/2410.04946v1#Ch10.S6" title="[BCP-VI] ‣ Chapter 10 Publications by the Author for this Thesis ‣ Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness"><span class="ltx_text ltx_ref_tag">[BCP-VI]</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch10.S6">
<h3 class="ltx_title ltx_font_bold ltx_font_smallcaps ltx_title_section" style="font-size:120%;">[BCP-VI]   </h3>
<div class="ltx_para" id="Ch10.S6.p1">
<p class="ltx_p" id="Ch10.S6.p1.1"><span class="ltx_text ltx_font_bold" id="Ch10.S6.p1.1.1">B. Carrillo-Perez</span>, A. Bueno Rodriguez, S. Barnes, and M. Stephan, <span class="ltx_text ltx_font_bold" id="Ch10.S6.p1.1.2">”Enhanced Small Ship Segmentation with Optimized ScatYOLOv8+CBAM on Embedded Systems,”</span> 2024. IEEE International Conference on Real-time Computing and Robotics (RCAR), 2024, pp. 1–6. (Accepted)</p>
</div>
<div class="ltx_para" id="Ch10.S6.p2">
<p class="ltx_p" id="Ch10.S6.p2.1">My share on this publication is 90%. Summary of contributions as <math alttext="1^{st}" class="ltx_Math" display="inline" id="Ch10.S6.p2.1.m1.1"><semantics id="Ch10.S6.p2.1.m1.1a"><msup id="Ch10.S6.p2.1.m1.1.1" xref="Ch10.S6.p2.1.m1.1.1.cmml"><mn id="Ch10.S6.p2.1.m1.1.1.2" xref="Ch10.S6.p2.1.m1.1.1.2.cmml">1</mn><mrow id="Ch10.S6.p2.1.m1.1.1.3" xref="Ch10.S6.p2.1.m1.1.1.3.cmml"><mi id="Ch10.S6.p2.1.m1.1.1.3.2" xref="Ch10.S6.p2.1.m1.1.1.3.2.cmml">s</mi><mo id="Ch10.S6.p2.1.m1.1.1.3.1" xref="Ch10.S6.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="Ch10.S6.p2.1.m1.1.1.3.3" xref="Ch10.S6.p2.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Ch10.S6.p2.1.m1.1b"><apply id="Ch10.S6.p2.1.m1.1.1.cmml" xref="Ch10.S6.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch10.S6.p2.1.m1.1.1.1.cmml" xref="Ch10.S6.p2.1.m1.1.1">superscript</csymbol><cn id="Ch10.S6.p2.1.m1.1.1.2.cmml" type="integer" xref="Ch10.S6.p2.1.m1.1.1.2">1</cn><apply id="Ch10.S6.p2.1.m1.1.1.3.cmml" xref="Ch10.S6.p2.1.m1.1.1.3"><times id="Ch10.S6.p2.1.m1.1.1.3.1.cmml" xref="Ch10.S6.p2.1.m1.1.1.3.1"></times><ci id="Ch10.S6.p2.1.m1.1.1.3.2.cmml" xref="Ch10.S6.p2.1.m1.1.1.3.2">𝑠</ci><ci id="Ch10.S6.p2.1.m1.1.1.3.3.cmml" xref="Ch10.S6.p2.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch10.S6.p2.1.m1.1c">1^{st}</annotation><annotation encoding="application/x-llamapun" id="Ch10.S6.p2.1.m1.1d">1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> author:</p>
</div>
<div class="ltx_para" id="Ch10.S6.p3">
<ul class="ltx_itemize" id="Ch10.S6.I1">
<li class="ltx_item" id="Ch10.S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i1.p1">
<p class="ltx_p" id="Ch10.S6.I1.i1.p1.1">Optimization of ScatBlock for faster inference with ScatYOLOv8+CBAM.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i2.p1">
<p class="ltx_p" id="Ch10.S6.I1.i2.p1.1">Comprehensive analysis of the use of ScatYOLOv8+CBAM for all model sizes.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i3.p1">
<p class="ltx_p" id="Ch10.S6.I1.i3.p1.1">Improvement of the slice prediction method (<a href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi"><span class="ltx_glossaryref" href="https://arxiv.org/html/2410.04946v1#glo.acronym.sahi" title="Slicing Aided Hyper Inference">SAHI</span></a>) to perform inference in batches, focusing on small or distant ship segmentation using ShipSG.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i4.p1">
<p class="ltx_p" id="Ch10.S6.I1.i4.p1.1">Weight serialization with TensorRT for Deployment on NVIDIA Jetson AGX Xavier, allowing faster inference.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i5.p1">
<p class="ltx_p" id="Ch10.S6.I1.i5.p1.1">Preparation of the manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ch10.S6.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch10.S6.I1.i6.p1">
<p class="ltx_p" id="Ch10.S6.I1.i6.p1.1">Outcome: More efficient deployment of ScatYOLOv8+CBAM on embedded system and improved the small and distant ship segmentation accuracy.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
E. Engler, D. Göge, and S. Brusch, “Resiliencen–a multi-dimensional
challenge for maritime infrastructures,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">NAŠE MORE: znanstveni
časopis za more i pomorstvo</em>, vol. 65, no. 2, pp. 123–129, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
F. S. Torres, N. Kulev, B. Skobiej, M. Meyer, O. Eichhorn, and
J. Schäfer-Frey, “Indicator-based safety and security assessment of
offshore wind farms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2020 Resilience Week (RWS)</em>.   IEEE, 2020, pp. 26–33.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
F. T. Cetin, B. Yilmaz, Y. Kabak, J.-H. Lee, C. Erbas, E. Akagunduz, S.-J. Lee,
and A. I. A. (TURKEY), “Increasing maritime situational awareness with
interoperating distributed information sources,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">18th Interantional
Command and Control Research and Technology Symposium</em>, 2013, pp. 9–22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
N. P. Ventikos and K. Louzis, “Risk dynamics for marine systems: towards a
bio-inspired framework for dynamic risk assessment,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Transportation
Safety and Environment</em>, vol. 4, no. 3, p. tdac018, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
K. Wang, M. Liang, Y. Li, J. Liu, and R. W. Liu, “Maritime traffic data
visualization: A brief review,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2019 IEEE 4th International
Conference on Big Data Analytics (ICBDA)</em>.   IEEE, 2019, pp. 67–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
B. Belmoukari, J.-F. Audy, and P. Forget, “Smart port: a systematic literature
review,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">European Transport Research Review</em>, vol. 15, no. 1, p. 4,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
International Maritime Organization (IMO), “Revised guidelines for the
onboard operational use of shipborne automatic identification systems
(ais),” Resolution A.1106(29), December 2015, adopted on December 2, 2015,
under agenda item 10 during the 29th session of the Assembly. [Online].
Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wwwcdn.imo.org/localresources/en/OurWork/Safety/Documents/AIS/Resolution%20A.1106(29).pdf" title="">https://wwwcdn.imo.org/localresources/en/OurWork/Safety/Documents/AIS/Resolution%20A.1106(29).pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
K.-i. Kim and K. M. Lee, “Adaptive information visualization for maritime
traffic stream sensor data with parallel context acquisition and machine
learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Sensors</em>, vol. 19, no. 5273, 2019. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1424-8220/19/23/5273" title="">https://www.mdpi.com/1424-8220/19/23/5273</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
S. Jakovlev, A. Daranda, M. Voznak, A. Lektauers, T. Eglynas, and M. Jusis,
“Analysis of the possibility to detect fake vessels in the automatic
identification system,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2020 61st International Scientific
Conference on Information Technology and Management Science of Riga Technical
University (ITMS)</em>.   IEEE, 2020, pp.
1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
M. C. Struck and J. Stoppe, “A backwards compatible approach to authenticate
automatic identification system messages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2021 IEEE International
Conference on Cyber Security and Resilience (CSR)</em>.   IEEE, 2021, pp. 524–529.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
G. Wimpenny, J. Safar, A. Grant, M. Bransby, and N. Ward, “Public key
authentication for ais and the vhf data exchange system (vdes),” in
<em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 31st International Technical Meeting of the
Satellite Division of The Institute of Navigation (ION GNSS+ 2018)</em>, 2018,
pp. 1841–1851.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
E. Alincourt, C. Ray, P.-M. Ricordel, D. Dare-Emzivat, and A. Boudraa,
“Methodology for ais signature identification through magnitude and temporal
characterization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">OCEANS 2016-Shanghai</em>.   IEEE, 2016, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
M. Balduzzi, A. Pasta, and K. Wilhoit, “A security evaluation of ais automated
identification system,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 30th annual computer
security applications conference</em>, 2014, pp. 436–445.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Z. Yan, Y. Xiao, L. Cheng, R. He, X. Ruan, X. Zhou, M. Li, and R. Bin,
“Exploring ais data for intelligent maritime routes extraction,”
<em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Applied Ocean Research</em>, vol. 101, p. 102271, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
M. Reggiannini, E. Salerno, C. Bacciu, A. D’Errico, A. Lo Duca, A. Marchetti,
M. Martinelli, C. Mercurio, A. Mistretta, M. Righi <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Remote
sensing for maritime traffic understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Remote Sensing</em>, vol. 16,
no. 3, p. 557, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
E. Schwarz, D. Krause, M. Berg, H. Daedelow, and H. Maass, “Near real time
applications for maritime situational awareness,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The International
Archives of the Photogrammetry, Remote Sensing and Spatial Information
Sciences</em>, vol. 40, pp. 999–1003, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabally, and C. Quek, “Video
processing from electro-optical sensors for object detection and tracking in
a maritime environment: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IEEE Transactions on Intelligent
Transportation Systems</em>, vol. 18, no. 8, pp. 1993–2016, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
F. Li, C.-H. Chen, G. Xu, D. Chang, and L. P. Khoo, “Causal factors and
symptoms of task-related human fatigue in vessel traffic service: A
task-driven approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Journal of Navigation</em>, vol. 73, no. 6, pp.
1340–1357, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
T. Flenker and J. Stoppe, “Marlin: An iot sensor network for improving
maritime situational awareness,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">MARESEC 2021</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Z. Chen, D. Chen, Y. Zhang, X. Cheng, M. Zhang, and C. Wu, “Deep learning for
autonomous ship-oriented small ship detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Safety Science</em>, vol.
130, p. 104812, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
A. M. Rekavandi, L. Xu, F. Boussaid, A.-K. Seghouane, S. Hoefs, and
M. Bennamoun, “A guide to image and video based small object detection using
deep learning: Case study of maritime surveillance,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint
arXiv:2207.12926</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
J. T. Hastings and L. L. Hill, <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Georeferencing</em>.   Boston, MA: Springer US, 2009, pp. 1246–1249. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-0-387-39940-9_181" title="">https://doi.org/10.1007/978-0-387-39940-9_181</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
N. Wawrzyniak, T. Hyla, and A. Popik, “Vessel detection and tracking method
based on video surveillance,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Sensors</em>, vol. 19, no. 23, p. 5230,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
Ø. K. Helgesen, E. F. Brekke, A. Stahl, and Ø. Engelhardtsen, “Low
altitude georeferencing for imaging sensors in maritime tracking,”
<em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IFAC-PapersOnLine</em>, vol. 53, no. 2, pp. 14 476–14 481, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
S. Mittal, “A survey on optimized implementation of deep learning models on
the nvidia jetson platform,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Journal of Systems Architecture</em>,
vol. 97, pp. 428–442, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
H. Zhao, W. Zhang, H. Sun, and B. Xue, “Embedded deep learning for ship
detection and recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Future Internet</em>, vol. 11, no. 2, p. 53,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
H. Ning, Y. Li, F. Shi, and L. T. Yang, “Heterogeneous edge computing open
platforms and tools for internet of things,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Future Generation
Computer Systems</em>, vol. 106, pp. 67–76, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
F. Sattler, S. Barnes, and M. Stephan, “A maritime situational awareness
framework using dynamic 3d reconstruction in real-time,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2023 27th
International Conference Information Visualisation (IV)</em>.   IEEE, 2023, pp. 334–339.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
R. Szeliski, <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Computer vision: algorithms and applications</em>.   Springer Nature, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
I. Goodfellow, Y. Bengio, and A. Courville, <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Deep learning</em>.   MIT press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
T. Talaei Khoei, H. Ould Slimane, and N. Kaabouch, “Deep learning: Systematic
review, models, challenges, and research directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Neural Computing
and Applications</em>, vol. 35, no. 31, pp. 23 103–23 124, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
C. Sammut and G. I. Webb, <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Encyclopedia of machine learning</em>.   Springer Science &amp; Business Media, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
J. Chai, H. Zeng, A. Li, and E. W. Ngai, “Deep learning in computer vision: A
critical review of emerging techniques and application scenarios,”
<em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Machine Learning with Applications</em>, vol. 6, p. 100134, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
P. Cunningham, M. Cord, and S. J. Delany, <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Supervised Learning</em>.   Berlin, Heidelberg: Springer Berlin Heidelberg,
2008, pp. 21–49. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-540-75171-7_2" title="">https://doi.org/10.1007/978-3-540-75171-7_2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
W. Rawat and Z. Wang, “Deep convolutional neural networks for image
classification: A comprehensive review,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Neural computation</em>, vol. 29,
no. 9, pp. 2352–2449, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
S. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, and B. Lee, “A
survey of modern deep learning based object detection models,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Digital
Signal Processing</em>, vol. 126, p. 103514, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
R. Shanmugamani, <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Deep Learning for Computer Vision: Expert techniques to
train advanced neural networks using TensorFlow and Keras</em>.   Packt Publishing Ltd, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Nature</em>, vol. 521,
no. 7553, pp. 436–444, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
J. Zhang, X. Li, L. Li, P. Sun, X. Su, T. Hu, and F. Chen, “Lightweight u-net
for cloud detection of visible and thermal infrared remote sensing images,”
<em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Optical and Quantum Electronics</em>, vol. 52, pp. 1–14, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of
the IEEE conference on computer vision and pattern recognition</em>, 2017, pp.
2117–2125.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
<em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
learning to align and translate,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:1409.0473</em>,
2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(44)</span>
<span class="ltx_bibblock">
M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to
attention-based neural machine translation,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint
arXiv:1508.04025</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block
attention module,” in <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the European conference on
computer vision (ECCV)</em>, 2018, pp. 3–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu, S.-H. Zhang,
R. R. Martin, M.-M. Cheng, and S.-M. Hu, “Attention mechanisms in computer
vision: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Computational visual media</em>, vol. 8, no. 3, pp.
331–368, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2018, pp. 7132–7141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(48)</span>
<span class="ltx_bibblock">
Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies</em>, 2016, pp.
1480–1489.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(49)</span>
<span class="ltx_bibblock">
N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran,
“Image transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">International Conference on Machine
Learning</em>.   PMLR, 2018, pp. 4055–4064.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(50)</span>
<span class="ltx_bibblock">
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and
Y. Bengio, “Show, attend and tell: Neural image caption generation with
visual attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:1502.03044</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(51)</span>
<span class="ltx_bibblock">
F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang,
“Residual attention network for image classification,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint
arXiv:1704.06904</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(52)</span>
<span class="ltx_bibblock">
L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, and T.-S. Chua, “Sca-cnn: Spatial
and channel-wise attention in convolutional networks for image captioning,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(53)</span>
<span class="ltx_bibblock">
G. Brauwers and F. Frasincar, “A general survey on attention mechanisms in
deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">IEEE Transactions on Knowledge and Data Engineering</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)</span>
<span class="ltx_bibblock">
M. Gong, D. Wang, X. Zhao, H. Guo, D. Luo, and M. Song, “A review of
non-maximum suppression algorithms for deep learning target detection,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Seventh Symposium on Novel Photoelectronic Detection Technology and
Applications</em>, vol. 11763.   SPIE, 2021,
pp. 821–828.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(55)</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,
M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and
S. Chintala, “Pytorch: An imperative style, high-performance deep learning
library,” in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in Neural Information Processing Systems
32</em>.   Curran Associates, Inc., 2019,
pp. 8024–8035. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(56)</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona,
D. Ramanan, C. L. Zitnick, and P. Dollár, “Microsoft coco: Common objects
in context,” 2014. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1405.0312" title="">http://arxiv.org/abs/1405.0312</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(57)</span>
<span class="ltx_bibblock">
I. H. Sarker, “Deep learning: a comprehensive overview on techniques,
taxonomy, applications and research directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">SN Computer Science</em>,
vol. 2, no. 6, p. 420, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(58)</span>
<span class="ltx_bibblock">
D. Qiao, G. Liu, T. Lv, W. Li, and J. Zhang, “Marine vision-based situational
awareness using discriminative deep learning: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Journal of
Marine Science and Engineering</em>, vol. 9, no. 4, p. 397, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman, “The pascal visual object classes challenge: A
retrospective,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">International journal of computer vision</em>, vol. 111,
no. 1, pp. 98–136, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Z. Shao, W. Wu, Z. Wang, W. Du, and C. Li, “Seaships: A large-scale precisely
annotated dataset for ship detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">IEEE transactions on
multimedia</em>, vol. 20, no. 10, pp. 2593–2604, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
X. Chen, L. Qi, Y. Yang, Q. Luo, O. Postolache, J. Tang, and H. Wu,
“Video-based detection infrastructure enhancement for automated ship
recognition and behavior analysis,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Journal of Advanced
Transportation</em>, vol. 2020, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
A. Ghahremani, Y. Kong, E. Bondarev <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">et al.</em>, “Multi-class detection and
orientation recognition of vessels in maritime surveillance,”
<em class="ltx_emph ltx_font_italic" id="bib.bib62.2.2">Electronic Imaging</em>, vol. 2019, no. 11, pp. 266–1, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(63)</span>
<span class="ltx_bibblock">
C. Nita and M. Vandewal, “Cnn-based object detection and segmentation for
maritime domain awareness,” in <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Artificial Intelligence and Machine
Learning in Defense Applications II</em>, vol. 11543.   International Society for Optics and Photonics, 2020, p. 1154306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(64)</span>
<span class="ltx_bibblock">
M. Ribeiro, B. Damas, and A. Bernardino, “Real-time ship segmentation in
maritime surveillance videos using automatically annotated synthetic
datasets,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Sensors</em>, vol. 22, no. 21, p. 8090, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
E. Teixeira, B. Araujo, V. Costa, S. Mafra, and F. Figueiredo, “Literature
review on ship localization, classification, and detection methods based on
optical sensors and neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Sensors</em>, vol. 22, no. 18, p.
6879, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
G. Jocher, A. Chaurasia, and J. Qiu, <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">YOLOv8 by Ultralytics</em>, Jan. 2023.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ultralytics/ultralytics" title="">https://github.com/ultralytics/ultralytics</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(67)</span>
<span class="ltx_bibblock">
C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “Scaled-yolov4: Scaling cross
stage partial network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE/cvf conference on
computer vision and pattern recognition</em>, 2021, pp. 13 029–13 038.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(68)</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(69)</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and
accuracy of object detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2004.10934</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(70)</span>
<span class="ltx_bibblock">
C.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.-H. Yeh,
“Cspnet: A new backbone that can enhance learning capability of cnn,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition workshops</em>, 2020, pp. 390–391.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(71)</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Advances in neural
information processing systems</em>, vol. 28, pp. 91–99, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(72)</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">2017
IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp.
2980–2988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(73)</span>
<span class="ltx_bibblock">
S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 1492–1500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(74)</span>
<span class="ltx_bibblock">
S. Qiao, L.-C. Chen, and A. Yuille, “Detectors: Detecting objects with
recursive feature pyramid and switchable atrous convolution,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 10 213–10 224.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(75)</span>
<span class="ltx_bibblock">
M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber, “Deep networks with
internal selective attention through feedback connections,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Advances in Neural Information Processing Systems</em>, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27.   Curran Associates, Inc., 2014. [Online].
Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2014/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2014/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(76)</span>
<span class="ltx_bibblock">
F. Yu and V. Koltun, “Multi-scale context aggregation by dilated
convolutions,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:1511.07122</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(77)</span>
<span class="ltx_bibblock">
D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, “Yolact: Real-time instance
segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the IEEE/CVF international conference
on computer vision</em>, 2019, pp. 9157–9166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(78)</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2015, pp. 3431–3440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(79)</span>
<span class="ltx_bibblock">
Y. Lee and J. Park, “Centermask: Real-time anchor-free instance
segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2020, pp. 13 906–13 915.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(80)</span>
<span class="ltx_bibblock">
Y. Guo, F. Chen, Q. Cheng, J. Wu, B. Wang, Y. Wu, and W. Zhao, “Fully
convolutional one-stage circular object detector on medical images,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">2020 4th International Conference on Advances in Image Processing</em>,
2020, pp. 21–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(81)</span>
<span class="ltx_bibblock">
Y. Lee, J.-w. Hwang, S. Lee, Y. Bae, and J. Park, “An energy and
gpu-computation efficient backbone network for real-time object detection,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(82)</span>
<span class="ltx_bibblock">
G. Jocher, A. Chaurasia, A. Stoken, J. Borovec, NanoCode012, Y. Kwon,
K. Michael, TaoXie, J. Fang, imyhxy, Lorna, Z. Yifu, C. Wong, A. V,
D. Montes, Z. Wang, C. Fati, J. Nadar, Laughing, UnglvKitDe, V. Sonck,
tkianai, yxNONG, P. Skalski, A. Hogan, D. Nair, M. Strobel, and M. Jain,
<em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation</em>,
Nov. 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.7347926" title="">https://doi.org/10.5281/zenodo.7347926</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(83)</span>
<span class="ltx_bibblock">
P. Singh, G. Saha, and M. Sahidullah, “Deep scattering network for speech
emotion recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">2021 29th European Signal Processing
Conference (EUSIPCO)</em>.   IEEE, 2021, pp.
131–135.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(84)</span>
<span class="ltx_bibblock">
C. Pan, S. Chen, and A. Ortega, “Spatio-temporal graph scattering transform,”
<em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2012.03363</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(85)</span>
<span class="ltx_bibblock">
S. Cheng, Y.-S. Ting, B. Ménard, and J. Bruna, “A new approach to
observational cosmology using the scattering transform,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Monthly
Notices of the Royal Astronomical Society</em>, vol. 499, no. 4, pp. 5902–5914,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(86)</span>
<span class="ltx_bibblock">
Á. B. Rodríguez, R. Balestriero, S. De Angelis, M. C. Benítez,
L. Zuccarello, R. Baraniuk, J. M. Ibanez, and M. V. de Hoop, “Recurrent
scattering network detects metastable behavior in polyphonic seismo-volcanic
signals for volcano eruption forecasting,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">IEEE Transactions on
Geoscience and Remote Sensing</em>, vol. 60, pp. 1–23, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(87)</span>
<span class="ltx_bibblock">
J. Bruna and S. Mallat, “Invariant scattering convolution networks,”
<em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">IEEE transactions on pattern analysis and machine intelligence</em>,
vol. 35, no. 8, pp. 1872–1886, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(88)</span>
<span class="ltx_bibblock">
E. Oyallon, E. Belilovsky, S. Zagoruyko, and M. Valko, “Compressing the input
for cnns with the first-order scattering transform,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of
the European Conference on Computer Vision (ECCV)</em>, 2018, pp. 301–316.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(89)</span>
<span class="ltx_bibblock">
S. Saponara and A. Elhanashi, “Impact of image resizing on deep learning
detectors for training time and model performance,” in <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">International
Conference on Applications in Electronics Pervading Industry, Environment and
Society</em>.   Springer, 2021, pp. 10–17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(90)</span>
<span class="ltx_bibblock">
H. Mao, S. Yao, T. Tang, B. Li, J. Yao, and Y. Wang, “Towards real-time object
detection on embedded systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">IEEE Transactions on Emerging Topics
in Computing</em>, vol. 6, no. 3, pp. 417–431, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(91)</span>
<span class="ltx_bibblock">
G. Wang, Y. Chen, P. An, H. Hong, J. Hu, and T. Huang, “Uav-yolov8: a
small-object-detection model based on improved yolov8 for uav aerial
photography scenarios,” <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Sensors</em>, vol. 23, no. 16, p. 7190, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(92)</span>
<span class="ltx_bibblock">
L. Zhu, X. Wang, Z. Ke, W. Zhang, and R. W. Lau, “Biformer: Vision transformer
with bi-level routing attention,” in <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2023, pp.
10 323–10 333.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(93)</span>
<span class="ltx_bibblock">
F. C. Akyon, S. Onur Altinuc, and A. Temizel, “Slicing aided hyper inference
and fine-tuning for small object detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">2022 IEEE
International Conference on Image Processing (ICIP)</em>, 2022, pp. 966–970.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(94)</span>
<span class="ltx_bibblock">
K. M. Han and G. N. DeSouza, “Geolocation of multiple targets from airborne
video without terrain data,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Journal of Intelligent &amp; Robotic
Systems</em>, vol. 62, no. 1, pp. 159–183, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(95)</span>
<span class="ltx_bibblock">
M. B. Shami, G. Kiss, T. A. Haakonsen, and F. Lindseth, “Geo-locating road
objects using inverse haversine formula with nvidia driveworks,” <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">arXiv
preprint arXiv:2401.07582</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(96)</span>
<span class="ltx_bibblock">
A. Milosavljević, D. Rančić, A. Dimitrijević, B. Predić,
and V. Mihajlović, “A method for estimating surveillance video
georeferences,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">ISPRS international journal of geo-information</em>,
vol. 6, no. 7, p. 211, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(97)</span>
<span class="ltx_bibblock">
K. Naus, M. W, P. Szymak, L. Gucma, and M. Gucma, “Assessment of ship position
estimation accuracy based on radar navigation mark echoes identified in an
electronic navigational chart,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Measurement</em>, vol. 169, p. 108630,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(98)</span>
<span class="ltx_bibblock">
C. E. Livingstone, M. Dragosevic, S. Chu, and I. Sikaneta, <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Ship detection
and measurement of ship motion by multi-aperture Synthetic Aperture
Radar</em>.   Defence Research and
Development Canada, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(99)</span>
<span class="ltx_bibblock">
Y. Wei, Z. Zhang, B. Mu, Y. Li, Q. Wang, and R. Liu, “Geolocation accuracy
evaluation of gf-4 geostationary high-resolution optical images over coastal
zones and offshore areas,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Journal of Coastal Research</em>, vol. 102,
no. SI, pp. 326–333, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(100)</span>
<span class="ltx_bibblock">
Z. Zhang, “A flexible new technique for camera calibration,” <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">IEEE
Transactions on pattern analysis and machine intelligence</em>, vol. 22, no. 11,
pp. 1330–1334, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(101)</span>
<span class="ltx_bibblock">
J. Chen and X. Ran, “Deep learning with edge computing: A review,”
<em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Proceedings of the IEEE</em>, vol. 107, no. 8, pp. 1655–1674, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(102)</span>
<span class="ltx_bibblock">
NVIDIA Corporation, “Nvidia tensorrt developer guide,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" title="">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html</a>,
2024, accessed: 2024-05-13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(103)</span>
<span class="ltx_bibblock">
S. Stepanenko and P. Yakimov, “Using high-performance deep learning platform
to accelerate object detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Proceedings of the International
Conference on Information Technology and Nanotechnology, Samara, Russia</em>,
2019, pp. 26–29.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(104)</span>
<span class="ltx_bibblock">
O. R. developers, “Onnx runtime,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onnxruntime.ai/" title="">https://onnxruntime.ai/</a>, 2021,
version: x.y.z.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(105)</span>
<span class="ltx_bibblock">
D. Heller, M. Rizk, R. Douguet, A. Baghdadi, and J.-P. Diguet, “Marine objects
detection using deep learning on embedded edge devices,” in <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">2022 IEEE
International Workshop on Rapid System Prototyping (RSP)</em>.   IEEE, 2022, pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(106)</span>
<span class="ltx_bibblock">
R. Panero Martinez, I. Schiopu, B. Cornelis, and A. Munteanu, “Real-time
instance segmentation of traffic videos for embedded devices,”
<em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Sensors</em>, vol. 21, no. 1, p. 275, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(107)</span>
<span class="ltx_bibblock">
OpenStreetMap contributors, “Planet dump retrieved from
https://planet.osm.org ,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.openstreetmap.org" title="">https://www.openstreetmap.org</a>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(108)</span>
<span class="ltx_bibblock">
“Port information guide bremerhaven,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.hbh.bremen.de/sixcms/media.php/13/PORT-INFORMATION-GUIDE-Bremerhaven.pdf" title="">http://www.hbh.bremen.de/sixcms/media.php/13/PORT-INFORMATION-GUIDE-Bremerhaven.pdf</a>,
Harbour Master Port of Bremerhaven.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(109)</span>
<span class="ltx_bibblock">
W. Kentaro, “labelme: Image Polygonal Annotation with Python,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/wkentaro/labelme" title="">https://github.com/wkentaro/labelme</a>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(110)</span>
<span class="ltx_bibblock">
M. Riveiro, G. Pallotta, and M. Vespe, “Maritime anomaly detection: A
review,” <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</em>, vol. 8, no. 5, p. e1266, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(111)</span>
<span class="ltx_bibblock">
F. A. C. de Oliveira, A. Niemi, A. García-Ortiz, and F. S. Torres,
“Partial camera obstruction detection using single value image metrics and
data augmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">2022 6th International Conference on System
Reliability and Safety (ICSRS)</em>.   IEEE,
2022, pp. 292–299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(112)</span>
<span class="ltx_bibblock">
H.-C. Burmeister, P. Grundmann, P. Hohnrath, and A. Ujkani, “Increasing
maritime situational awareness by augmented reality solutions,” Fraunhofer
Center for Maritime Logistics and Services CML, White paper, 2021, available
online:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cml.fraunhofer.de/content/dam/cml/en/documents/Studien/Burmeister%20Grundmann%20Hohnrath%20Ujkani%20%282021%29_Increasing-Situational-Awareness-by-Augmented-Reality-Solutions_White%20Paper.pdf" title="">https://www.cml.fraunhofer.de/content/dam/cml/en/documents/Studien/Burmeister%20Grundmann%20Hohnrath%20Ujkani%20%282021%29_Increasing-Situational-Awareness-by-Augmented-Reality-Solutions_White%20Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(113)</span>
<span class="ltx_bibblock">
<em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Blender - a 3D modelling and rendering package</em>, Blender Foundation,
Stichting Blender Foundation, Amsterdam, 2018. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.blender.org" title="">http://www.blender.org</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(114)</span>
<span class="ltx_bibblock">
A. Sloss, D. Symes, and C. Wright, <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">ARM system developer’s guide:
designing and optimizing system software</em>.   Elsevier, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(115)</span>
<span class="ltx_bibblock">
T. Guo, T. Zhang, E. Lim, M. Lopez-Benitez, F. Ma, and L. Yu, “A review of
wavelet analysis and its applications: Challenges and opportunities,”
<em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">IEEE Access</em>, vol. 10, pp. 58 869–58 903, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(116)</span>
<span class="ltx_bibblock">
F. Cotter, “Uses of complex wavelets in deep convolutional neural networks,”
Ph.D. dissertation, University of Cambridge, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(117)</span>
<span class="ltx_bibblock">
I. W. Selesnick, R. G. Baraniuk, and N. C. Kingsbury, “The dual-tree complex
wavelet transform,” <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">IEEE signal processing magazine</em>, vol. 22, no. 6,
pp. 123–151, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(118)</span>
<span class="ltx_bibblock">
M. Andreux, T. Angles, G. Exarchakis, R. Leonarduzzi, G. Rochette, L. Thiry,
J. Zarka, S. Mallat, J. Andén, E. Belilovsky <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">et al.</em>, “Kymatio:
Scattering transforms in python,” <em class="ltx_emph ltx_font_italic" id="bib.bib118.2.2">Journal of Machine Learning
Research</em>, vol. 21, no. 60, pp. 1–6, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(119)</span>
<span class="ltx_bibblock">
X. Zhu, S. Lyu, X. Wang, and Q. Zhao, “Tph-yolov5: Improved yolov5 based on
transformer prediction head for object detection on drone-captured
scenarios,” in <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Proceedings of the IEEE/CVF international conference on
computer vision</em>, 2021, pp. 2778–2788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(120)</span>
<span class="ltx_bibblock">
R. Hartley and A. Zisserman, <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Multiple view geometry in computer
vision</em>.   Cambridge university press,
2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(121)</span>
<span class="ltx_bibblock">
Y. Xie, M. Wang, X. Liu, and Y. Wu, “Integration of gis and moving objects in
surveillance video,” <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">ISPRS International Journal of Geo-Information</em>,
vol. 6, no. 4, p. 94, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(122)</span>
<span class="ltx_bibblock">
Z. Shao, C. Li, D. Li, O. Altan, L. Zhang, and L. Ding, “An accurate matching
method for projecting vector data into surveillance video to monitor and
protect cultivated land,” <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">ISPRS International Journal of
Geo-Information</em>, vol. 9, no. 7, p. 448, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(123)</span>
<span class="ltx_bibblock">
T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, “High accuracy optical flow
estimation based on a theory for warping,” in <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Computer Vision-ECCV
2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May
11-14, 2004. Proceedings, Part IV 8</em>.   Springer, 2004, pp. 25–36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(124)</span>
<span class="ltx_bibblock">
D. G. Bell, F. Kuehnel, C. Maxwell, R. Kim, K. Kasraie, T. Gaskins, P. Hogan,
and J. Coughlan, “Nasa world wind: Opensource gis for mission operations,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">2007 IEEE Aerospace Conference</em>.   IEEE, 2007, pp. 1–9.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 11:08:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
