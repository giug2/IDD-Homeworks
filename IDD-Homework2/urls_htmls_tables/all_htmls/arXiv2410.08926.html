<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</title>
<!--Generated on Fri Oct 11 11:30:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.08926v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S1" title="In Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S2" title="In Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S3" title="In Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S4" title="In Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S4.SS1" title="In 4 Discussion ‣ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Practical Lessons Learned</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S4.SS2" title="In 4 Discussion ‣ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Open Challenges, Limitations &amp; Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S5" title="In Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Virmarie Maquiling<sup class="ltx_sup" id="id1.1.id1">*</sup>
<br class="ltx_break"/>Human-Centered Technologies for Learning
<br class="ltx_break"/>Technical University of Munich
<br class="ltx_break"/>Munich Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">virmarie.maquiling@tum.de</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\And</span>Sean Anthony Byrne<sup class="ltx_sup" id="id4.4.id4">*</sup>
<br class="ltx_break"/>Dipartimento di Elettronica, Informazione e Bioingegneria
<br class="ltx_break"/>Politecnico di Milano
<br class="ltx_break"/>Piazza Leonardo da Vinci 
<br class="ltx_break"/>Milano Italy
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">seananthony.byrne@polimi.it</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id6.6.id6">\And</span>Diederick C. Niehorster 
<br class="ltx_break"/>Lund University Humanities Lab &amp; Dept. of Psychology
<br class="ltx_break"/>Lund University
<br class="ltx_break"/>Lund, Sweden
<span class="ltx_text ltx_font_typewriter" id="id7.7.id7">diederick_c.niehorster@humlab.lu.se</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id8.8.id8">\And</span>Marco Carminati 
<br class="ltx_break"/>Dipartimento di Elettronica, Informazione e Bioingegneria
<br class="ltx_break"/>Politecnico di Milano
<br class="ltx_break"/>Milano Italy
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id9">marco.carminati1@polimi.it</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id10.10.id10">\And</span>Enkelejda Kasneci 
<br class="ltx_break"/>Human-Centered Technologies for Learning
<br class="ltx_break"/>Technical University of Munich
<br class="ltx_break"/>Munich Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.11.id11">enkelejda.kasneci@tum.de</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input—a single click per video—we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the worlds largest unified dataset recorded using wearable eye trackers. Remarkably in pupil segmentation tasks, SAM 2 matches the performance of domain specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely-used datasets to promote further research.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration demonstrating the data annotation process with SAM 2: the user provides a single point prompt via a mouse click, and SAM 2 automatically handles the rest of the segmentation process. Optionally, the user can refine and add additional prompts in more difficult areas of the video to improve the model’s output.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The increasing integration of eye tracking into technologies like virtual reality (VR) devices and smart glasses<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx2" title="">Byrne et al<span class="ltx_text">.</span>(2024)</a>]</cite> has amplified the demand for robust gaze estimation systems, where a key task is the accurate localization of the pupil within an image or video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx14" title="">Maquiling et al<span class="ltx_text">.</span>(2024)</a>]</cite>. Traditional methods for pupil localization—including thresholding and center of mass calculations<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx15" title="">Nyström et al<span class="ltx_text">.</span>(2023)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx22" title="">Shortis et al<span class="ltx_text">.</span>(1994)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx17" title="">Peréz et al<span class="ltx_text">.</span>(2003)</a>]</cite> and ellipse-fitting algorithms<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx20" title="">Santini et al<span class="ltx_text">.</span>(2018a)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx21" title="">Santini et al<span class="ltx_text">.</span>(2018b)</a>]</cite>—while effective in controlled environments, suffer catastrophic errors in the presence of noise such as occlusions or reflections, limiting their utility in real-world settings<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx13" title="">Kothari et al<span class="ltx_text">.</span>(2022)</a>]</cite>. To overcome these limitations, deep learning-based approaches have emerged as powerful alternatives, addressing issues plaguing traditional methods like blinks or reflections<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>]</cite> and improving the robustness and accuracy of pupil detection under challenging conditions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx6" title="">Fuhl et al<span class="ltx_text">.</span>(2016a)</a>]</cite>. However, deploying these models requires vast amounts of annotated data and technical expertise. Data annotation can be very costly, requiring significant human labor depending on dataset size and complexity, and training gaze estimation models using supervised machine learning relies heavily on labeled data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx23" title="">Sun et al<span class="ltx_text">.</span>(2017)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx19" title="">Sambasivan et al<span class="ltx_text">.</span>(2021)</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Foundation models represent a paradigm shift in artificial intelligence, transforming how people interact with, develop, and deploy deep learning models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx1" title="">Bommasani et al<span class="ltx_text">.</span>(2021)</a>]</cite>. Characterized by vast numbers of trainable parameters and extensive training data, these models demonstrate impressive adaptability to downstream tasks and perform well on data distributions they have not encountered during training<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx1" title="">Bommasani et al<span class="ltx_text">.</span>(2021)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx11" title="">Kirillov et al<span class="ltx_text">.</span>(2023)</a>]</cite>. They have lowered barriers to entry for integrating AI into workflows, simplifying the use of AI-powered tools and handling complex tasks that once required specialized models and custom datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx1" title="">Bommasani et al<span class="ltx_text">.</span>(2021)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx26" title="">Zhou et al<span class="ltx_text">.</span>(2023)</a>]</cite>. Building on these advancements, Maquiling et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx14" title="">Maquiling et al<span class="ltx_text">.</span>(2024)</a>]</cite> showcased the potential of zero-shot vision foundation models in annotating eye tracking data by evaluating the Segment Anything Model (SAM)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx11" title="">Kirillov et al<span class="ltx_text">.</span>(2023)</a>]</cite>, a vision foundation model released by Meta AI, on the OpenEDS datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx16" title="">Palmero et al<span class="ltx_text">.</span>(2020)</a>]</cite>. However, SAM required at least one prompt per image, necessitating manual clicks on every image in the dataset—a time-consuming process. Its successor, SAM 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx18" title="">Ravi et al<span class="ltx_text">.</span>(2024)</a>]</cite>, addresses this limitation by enabling a single prompt to propagate across an entire video, allowing the model to track and segment objects even with occlusions. This improvement drastically reduces the need for manual interaction, making the annotation process significantly more efficient.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This case study explores the potential of SAM 2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx18" title="">Ravi et al<span class="ltx_text">.</span>(2024)</a>]</cite>, in advancing gaze estimation research. Importantly, it addresses long-standing challenges in gaze estimation: the need for large annotated datasets, the labor-intensive process of feature annotation, the high barrier of expertise for developing custom models, and the difficulty of domain adaptation where models struggle to generalize across datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx13" title="">Kothari et al<span class="ltx_text">.</span>(2022)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx3" title="">Byrne et al<span class="ltx_text">.</span>(2023)</a>]</cite>. This feature is particularly relevant in gaze estimation, where models often fail due to variations in differences across participant physiology, recording setups, and environmental lighting conditions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx13" title="">Kothari et al<span class="ltx_text">.</span>(2022)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx3" title="">Byrne et al<span class="ltx_text">.</span>(2023)</a>]</cite>.
To evaluate SAM 2, we deployed the model across diverse gaze estimation datasets, including VR environments and the world’s largest unified public dataset of eye images captured with head-mounted devices<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>. To evaluate and demonstrate SAM 2’s ease of use, we limited the annotation to just one click per video, regardless of its length. A frame where the pupil was clearly visible was selected, and a single point prompt was placed near the center of the pupil. For the OpenEDS2019 dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>]</cite>, which consists of non-sequential eye images, we applied a single prompt to the entire dataset by taking the prompt from the first image in the training set and propagating it to the test and validation sets, covering a total of 152 different participants. We then assess SAM 2’s segmentation performance using three key metrics: (1) the intersection-over-union (IoU) of the pupil masks to assess segmentation accuracy; (2) the ratio of frames where the pupil was not successfully tracked (when the pupil was visible) to the total number of frames (referred to as <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Pupil Lost</span>); and (3) the ratio of frames where a blink is correctly detected (i.e., the predicted mask is empty) to the total number of frames where the ground truth pupil mask is empty (referred to as <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">Blink Detected</span>). Our annotation process employed SAM 2’s smallest model, <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.3">SAM_hiera_tiny</span>, adapting the code released by SAM 2’s authors<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx18" title="">Ravi et al<span class="ltx_text">.</span>(2024)</a>]</cite> so that it could handle arbitrarily long videos without preprocessing or running into memory issues. This improved version of SAM 2 used for this paper is available from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dcnieho/segment-anything-2" title="">https://github.com/dcnieho/segment-anything-2</a> while the resulting masks can be downloaded from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13911636" title="">https://zenodo.org/records/13911636</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S2.F2.g1" src="x2.png" width="623"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison between Segment Anything Model 2 (top), a traditional segmentation model trained specifically on eye tracking datasets (left), and Segment Anything Model (right). The sample eye images are taken from the GW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx12" title="">Kothari et al<span class="ltx_text">.</span>(2020)</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We evaluated SAM 2’s performance on a diverse set of eye tracking datasets to test its generalizability across various domains. These datasets include both VR-based and mobile eye tracking environments, representing controlled and real-world settings. Specifically, we selected four VR-based datasets (including one synthetic) and three mobile eye tracking datasets captured in natural, uncontrolled environments. Two of these datasets are from the OpenEDS challenges<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx16" title="">Palmero et al<span class="ltx_text">.</span>(2020)</a>]</cite>, which focus on creating generalizable and robust semantic segmentations within VR settings. The synthetic NVGaze<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>]</cite> dataset includes its own pupil segmentations for gaze estimation. For the remaining datasets, ground truth segmentations were sourced from TEyeD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>, the world’s largest unified public dataset of eye images taken with head-mounted devices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Below is a brief description of the datasets used:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">OpenEDS2019</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>]</cite>: Contains 12,759 non-sequential images (<math alttext="400\times 640" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><mrow id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mn id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">400</mn><mo id="S2.I1.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><times id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.1"></times><cn id="S2.I1.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i1.p1.1.m1.1.1.2">400</cn><cn id="S2.I1.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i1.p1.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">400\times 640</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">400 × 640</annotation></semantics></math> pixels) acquired from 152 participants using a VR head-mounted display (HMD) with eye-facing cameras at 200 Hz under controlled lighting. Provides pixel-level annotations for the pupil, iris, and sclera.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">OpenEDS2020</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx16" title="">Palmero et al<span class="ltx_text">.</span>(2020)</a>]</cite>: Features eye-image sequences from 80 participants using a VR HMD at 100 Hz. The Eye Segmentation Dataset includes 200 sequences sampled at 5 Hz totalling to 29,500 images, of which 5% are manually annotated (<math alttext="640\times 400" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mn id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">640</mn><mo id="S2.I1.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></times><cn id="S2.I1.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i2.p1.1.m1.1.1.2">640</cn><cn id="S2.I1.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i2.p1.1.m1.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">640\times 400</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">640 × 400</annotation></semantics></math> pixels).</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.2.1">NVGaze</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>]</cite>: Comprises two datasets for near-eye gaze estimation under infrared illumination. The real-world dataset includes 264,279 images (<math alttext="640\times 480" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mn id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">640</mn><mo id="S2.I1.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><cn id="S2.I1.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2">640</cn><cn id="S2.I1.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">640\times 480</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">640 × 480</annotation></semantics></math> pixels) from 14 participants in a VR setting; the synthetic dataset contains 2 million images (<math alttext="1280\times 960" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><mrow id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><mn id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml">1280</mn><mo id="S2.I1.i3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml">960</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><times id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.1"></times><cn id="S2.I1.i3.p1.2.m2.1.1.2.cmml" type="integer" xref="S2.I1.i3.p1.2.m2.1.1.2">1280</cn><cn id="S2.I1.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.I1.i3.p1.2.m2.1.1.3">960</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">1280\times 960</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">1280 × 960</annotation></semantics></math> pixels). We evaluated SAM 2 on both.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Labelled Pupils in the Wild (LPW)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx24" title="">Tonsen et al<span class="ltx_text">.</span>(2016)</a>]</cite>: Consists of videos from 22 participants recorded in everyday environments using a head-mounted eye tracker at 120 Hz (<math alttext="640\times 480" class="ltx_Math" display="inline" id="S2.I1.i4.p1.1.m1.1"><semantics id="S2.I1.i4.p1.1.m1.1a"><mrow id="S2.I1.i4.p1.1.m1.1.1" xref="S2.I1.i4.p1.1.m1.1.1.cmml"><mn id="S2.I1.i4.p1.1.m1.1.1.2" xref="S2.I1.i4.p1.1.m1.1.1.2.cmml">640</mn><mo id="S2.I1.i4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i4.p1.1.m1.1.1.3" xref="S2.I1.i4.p1.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i4.p1.1.m1.1b"><apply id="S2.I1.i4.p1.1.m1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1"><times id="S2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1.1"></times><cn id="S2.I1.i4.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i4.p1.1.m1.1.1.2">640</cn><cn id="S2.I1.i4.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i4.p1.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i4.p1.1.m1.1c">640\times 480</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i4.p1.1.m1.1d">640 × 480</annotation></semantics></math> pixels), covering diverse lighting conditions and natural gaze distributions.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i5.p1.1.1">Gaze-in-Wild (GW)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx12" title="">Kothari et al<span class="ltx_text">.</span>(2020)</a>]</cite>: Provides naturalistic recordings from 19 participants performing everyday tasks with a mobile eye tracker at 120 Hz (<math alttext="640\times 480" class="ltx_Math" display="inline" id="S2.I1.i5.p1.1.m1.1"><semantics id="S2.I1.i5.p1.1.m1.1a"><mrow id="S2.I1.i5.p1.1.m1.1.1" xref="S2.I1.i5.p1.1.m1.1.1.cmml"><mn id="S2.I1.i5.p1.1.m1.1.1.2" xref="S2.I1.i5.p1.1.m1.1.1.2.cmml">640</mn><mo id="S2.I1.i5.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i5.p1.1.m1.1.1.3" xref="S2.I1.i5.p1.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i5.p1.1.m1.1b"><apply id="S2.I1.i5.p1.1.m1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1"><times id="S2.I1.i5.p1.1.m1.1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1.1"></times><cn id="S2.I1.i5.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i5.p1.1.m1.1.1.2">640</cn><cn id="S2.I1.i5.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i5.p1.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i5.p1.1.m1.1c">640\times 480</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i5.p1.1.m1.1d">640 × 480</annotation></semantics></math> pixels), including eye and head movements, infrared eye images, and scene imagery.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i6.p1">
<p class="ltx_p" id="S2.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i6.p1.1.1">Dikablis datasets</span>: A combination of datasets from ElSe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx7" title="">Fuhl et al<span class="ltx_text">.</span>(2016b)</a>]</cite>, ExCuSe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx5" title="">Fuhl et al<span class="ltx_text">.</span>(2015)</a>]</cite>, PNET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx6" title="">Fuhl et al<span class="ltx_text">.</span>(2016a)</a>]</cite>, and a driving study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx9" title="">Kasneci et al<span class="ltx_text">.</span>(2014)</a>]</cite>, compiled in TEyeD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>. Recorded at 25 Hz (<math alttext="384\times 288" class="ltx_Math" display="inline" id="S2.I1.i6.p1.1.m1.1"><semantics id="S2.I1.i6.p1.1.m1.1a"><mrow id="S2.I1.i6.p1.1.m1.1.1" xref="S2.I1.i6.p1.1.m1.1.1.cmml"><mn id="S2.I1.i6.p1.1.m1.1.1.2" xref="S2.I1.i6.p1.1.m1.1.1.2.cmml">384</mn><mo id="S2.I1.i6.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i6.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i6.p1.1.m1.1.1.3" xref="S2.I1.i6.p1.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i6.p1.1.m1.1b"><apply id="S2.I1.i6.p1.1.m1.1.1.cmml" xref="S2.I1.i6.p1.1.m1.1.1"><times id="S2.I1.i6.p1.1.m1.1.1.1.cmml" xref="S2.I1.i6.p1.1.m1.1.1.1"></times><cn id="S2.I1.i6.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.I1.i6.p1.1.m1.1.1.2">384</cn><cn id="S2.I1.i6.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i6.p1.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i6.p1.1.m1.1c">384\times 288</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i6.p1.1.m1.1d">384 × 288</annotation></semantics></math> pixels), it features eye recordings from 30 participants.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>SAM 2 results on various VR-(first and second rows) and mobile eye tracking (third and last rows) datasets. Images are taken from the OpenEDS2019 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>]</cite>, OpenEDS2020 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx16" title="">Palmero et al<span class="ltx_text">.</span>(2020)</a>]</cite>, LPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx24" title="">Tonsen et al<span class="ltx_text">.</span>(2016)</a>]</cite>, and the Dikablis datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>. SAM 2 handled occlusions remarkably well as observed in rows 1 and 4, and effectively segmented the pupil across a wide range of datasets, showing its robustness to different eye tracking conditions.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">OpenEDS</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">EllSeg-Gen</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">TEyeD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S3.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1">SAM 2</span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S3.T1.1.2.2.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S3.T1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.2.1">Mean IoU</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S3.T1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.3.1">Mean IoU</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S3.T1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.4.1">Mean IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.5.1">Mean IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.6.1">Pupil Lost</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.7.1">Blink Detected</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.1.1.1">OpenEDS2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.3.1.2">0.9528</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.3.1.3">0.956</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.3.1.4">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.1.5">0.8997</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.1.6">0.0075</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.1.7">0.9412</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.2.1.1">OpenEDS2020</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.2.2">0.9517</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.2.3">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.4.2.4">–</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2.5">0.9233</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2.6">0.0104</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2.7">0.9375</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.5.3.1.1">NVGaze (Synthetic)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.3.2">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.3.3">0.982</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.5.3.4">–</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.3.5">0.9259</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.3.6">0.0028</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.3.7">0.9602</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.4.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.4.1.1">NVGaze (Real)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.4.2">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.4.3">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.6.4.4">0.65</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.4.5">0.9079</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.4.6">0.0876</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.4.7">0.9889</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.5.1.1">LPW</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.7.5.2">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.7.5.3">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.7.5.4">*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.5.5">0.9023</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.5.6">0.0940</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.5.7">0.7127</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.8.6.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.6.1.1">GW</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.8.6.2">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.8.6.3">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.8.6.4">*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.6.5">0.9212</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.6.6">0.0184</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.6.7">0.8513</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.1.9.7.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.9.7.1.1">Dikablis</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.1.9.7.2">–</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.1.9.7.3">–</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.1.9.7.4">*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.9.7.5">0.8835</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.9.7.6">0.1846</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.9.7.7">0.9483</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of SAM 2 on multiple datasets compared to leaderboard scores published in OpenEDS2019 and OpenEDS2020 challenge pages, and baseline result from TEyeD using leave-one-out cross validation for each eye tracker. The asterisk (*) indicates datasets where a single baseline value was reported for all four datasets. As TEyeD provided the ground truth segmentation for NVGaze (real), LPW, GW, and the Dikablis datasets, no baseline mean IoU could be extracted from the original papers. Similarly, no baseline IoU was reported for the NVGaze synthetic dataset, therefore, we extracted the mIoU score from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx13" title="">Kothari et al<span class="ltx_text">.</span>(2022)</a>]</cite> who evaluated their model on NVgaze. Their mIoU score for OpenEDS2019 has been included as well.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Table<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#S3.T1" title="Table 1 ‣ 3 Results ‣ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes SAM 2’s performance metrics—mean IoU, pupil lost rate, and blink detection rate—compared to the leaderboard scores<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://eval.ai/web/challenges/challenge-page/353/leaderboard/1002</span></span></span><sup class="ltx_sup" id="S3.p1.1.1">,</sup><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://eval.ai/web/challenges/challenge-page/603/leaderboard/1680</span></span></span> of OpenEDS and the baseline score of TEyeD. For instance, on the OpenEDS2019 dataset, SAM 2 achieved a mean IoU of 89.97%, slightly lower than specialist models trained on OpenEDS, which reached at most 95.6%, with a pupil lost rate of 0.75% and a blink detection rate of 94.12% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx8" title="">Garbin et al<span class="ltx_text">.</span>(2019)</a>]</cite>. Similarly, for OpenEDS2020, SAM 2 attained a mean IoU of 92.33%, compared to the specialist model’s 95.17% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx16" title="">Palmero et al<span class="ltx_text">.</span>(2020)</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">On the NVGaze datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx10" title="">Kim et al<span class="ltx_text">.</span>(2019)</a>]</cite>, SAM 2 performed well on synthetic data with a mean IoU of 92.59% and on real data with 90.79%, despite a higher pupil lost rate of 8.76% on the real data. In mobile datasets, SAM 2 achieved mean IoUs of 90.23% on LPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx24" title="">Tonsen et al<span class="ltx_text">.</span>(2016)</a>]</cite> and 92.12% on GW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx12" title="">Kothari et al<span class="ltx_text">.</span>(2020)</a>]</cite>, with higher pupil lost rates due to increased noise and visual obstructions, as evidenced by the Dikablis datasets’ pupil lost rate of 18.46% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx7" title="">Fuhl et al<span class="ltx_text">.</span>(2016b)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx5" title="">Fuhl et al<span class="ltx_text">.</span>(2015)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx6" title="">Fuhl et al<span class="ltx_text">.</span>(2016a)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx9" title="">Kasneci et al<span class="ltx_text">.</span>(2014)</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Overall, SAM 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx18" title="">Ravi et al<span class="ltx_text">.</span>(2024)</a>]</cite> performed well on both VR and mobile eye tracking datasets, with VR datasets showing higher performance likely due to more controlled environments. Although SAM 2 slightly underperformed on the nonsequential OpenEDS2019 dataset—composed of individual images rather than continuous video frames—it still demonstrated the ability to generalize across multiple datasets. Notably, SAM 2 outperformed TEyeD’s top-performing model, which achieved a mean IoU of just 65% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx4" title="">Fuhl et al<span class="ltx_text">.</span>(2021)</a>]</cite>, and delivered competitive results compared to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx13" title="">Kothari et al<span class="ltx_text">.</span>(2022)</a>]</cite>, where a mean IoU of 98.2% was reported on NVGaze’s synthetic dataset. It is important to highlight that their model was trained on several datasets, including NVGaze’s training set, before being tested on the NVGaze test set. In contrast, SAM 2 achieved these results without any fine-tuning or sacrificing a subset of the datasets for training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">A major advantage of SAM 2 is its minimal human guidance requirement—just a single click per video to indicate that it should segment the pupil—compared to traditional manual annotation that would require thousands of clicks. For example, SAM 2 required only one click for the entire OpenEDS2019 dataset of 12,759 images. Similar reductions were observed across other datasets: 200 clicks for the 29,500 images of the OpenEDS2020 dataset, 66 clicks for LPW’s 130,856 images, 54 clicks for NVGaze’s 2,264,279 images, 423 clicks for the Dikablis datasets’ more than 5.6 million images, and 148 clicks for GW’s 6 million images. This significant reduction in human labor highlights SAM 2’s efficiency in annotating large datasets with minimal input while achieving high performance in pupil tracking without model fine-tuning or specialized training. Moreover, SAM 2’s inference process did not require powerful GPUs, making it feasible for use with low-end hardware. This level of accessibility was not possible before SAM 2 and demonstrates how vision foundation models can democratize large-scale data annotation in eye tracking research.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The quality of the dataset significantly impacts SAM 2’s performance. For instance, datasets with clear eye images, minimal noise and high resolution, such as OpenEDS and NVGaze, yielded higher IoU scores and lower pupil loss rates. However, SAM 2 encountered more difficulty in noisier datasets, like the Dikablis datasets, resulting in more pupil tracking failures.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In terms of human interaction, the effort required is mostly limited to monitoring the quality of the predicted masks and adding additional prompts only when necessary. While we limited our evaluation to a single point prompt per video, SAM 2 supports various other prompt types, such as multiple positive or negative point prompts (where the signs indicate areas that SAM 2 should and should not include in its segmentation) and bounding box prompts, offering flexibility for more complex and noisier datasets that require additional guidance. Below we highlight several practical lessons from conducting our study for researchers interested in implementing SAM 2 for eye tracking data segmentation tasks:</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Practical Lessons Learned</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Significant Time Savings:</span> SAM 2 significantly reduced the time necessary to annotate entire datasets. The datasets in this study were annotated by two of the authors within a couple of days, demonstrating SAM 2’s efficiency in annotating large volumes of data quickly. Importantly, these authors spent most of the time waiting for the model to finish producing its segmentation for the datasets, and only very little time setting up the model and checking its output.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Reduced Technical Barrier to Entry:</span> SAM 2 not only saves time in obtaining segmentation masks but also greatly simplifies the process for non-experts. Instead of developing custom pupil segmentation models, users can run SAM 2 with just a few lines of code. This lowers the barrier to entry, enabling more people to develop gaze estimation pipelines.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Minimal Human Interaction:</span> The annotation process required minimal human involvement beyond preparing the prompts and finding appropriate frames where the pupil is visible. SAM 2 handled the actual annotation, while the user only needs to perform quality checks and refine prompts when necessary.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Dealing with Noisy Data</span> While SAM 2 performed well in most cases, a decrease in performance was observed in noisier datasets. To mitigate this, further refining of prompts (e.g. using a more appropriate prompt strategy or adding prompts on more difficult frames) is necessary–although this still involves less human effort compared to traditional annotation processes.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Low Hardware Requirements</span> SAM 2’s low GPU requirements make it accessible for researchers with limited computational resources. We used both a high-end NVIDIA A100 (80GB VRAM) and a GeForce RTX 4090 (24 GB VRAM), achieving compute times of up to 40 frames per second (fps) for both. Impressively, SAM 2 also ran on more budget-friendly GPU’s such as the RTX 4060 Ti (16 GB VRAM) delivering around 12 fps, and was even functional on laptop-class GPU’s, albeit at significantly lower frame rates of just a few fps. However, it is worth noting that inference performance of the model appeared to be limited by CPU and not GPU performance in most of these cases.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">Strong Generalization</span> SAM 2 demonstrated robust performance across a diverse set of eye tracking datasets including VR, mobile, and even synthetic data, despite not being specifically trained on eye tracking data.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Open Challenges, Limitations &amp; Future Work</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">While SAM 2 excelled in pupil segmentation, challenges remain with segmenting less distinct eye features like the iris and sclera. To explore this, we evaluated SAM 2’s performance on the OpenEDS2020 dataset, where it achieved an mIoU of 76.53% for the iris and only 7.36% for the sclera with a single box prompt. Fine-tuning SAM 2 on specific eye features, especially under varying conditions like lighting, reflections, and noise, could improve performance by reducing its reliance on ideal conditions and simple prompts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Additionally, alternative prompting strategies, such as using bounding boxes or multiple point prompts, may yield better results in challenging cases. A limitation of our study is that we did not explore different prompt strategies, opting for a single point prompt to highlight the simplicity of annotation with SAM 2. While a single prompt proved to be effective for pupil segmentation in many cases, other prompts may improve results in difficult cases or for less well defined features.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Another consideration is the possibility that SAM 2 may have encountered similar images during training as all the datasets we have used are open to the public. Future work should test SAM 2 on completely novel datasets to validate its generalization capabilities. Additionally, as eye tracking moves to consumer devices, a key challenge will be adapting SAM 2 for low-power hardware like smart glasses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08926v1#bib.bibx25" title="">Zhang et al<span class="ltx_text">.</span>(2023)</a>]</cite>, making it crucial to balance performance with reduced computational requirements for real-time applications in VR, AR devices.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we assessed the practical segmentation capabilities of the SAM 2 Vision Foundation model. Using SAM 2, we efficiently annotated over 14 million pupil images across multiple datasets with just a few click prompts per dataset, significantly streamlining traditional annotation workflows. Our findings show that foundation models like SAM 2 effectively address key challenges in eye tracking research: data annotation, domain adaptation, and reducing training data requirements. Notably, SAM 2 achieves robust performance without fine-tuning, offering a user-friendly and accurate solution compared to its predecessor, SAM. Its ability to annotate entire datasets with minimal human input makes it suitable practically for large-scale applications. This work highlights the potential for general-purpose models to benefit other HCI fields where extensive labeled data is needed. As these models advance, we anticipate continued progress in both eye tracking research and broader human-computer interaction applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Bommasani et al<span class="ltx_text" id="bib.bibx1.1.1.1">.</span>(2021)]</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al<span class="ltx_text" id="bib.bibx1.4.1">.</span> 2021.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx1.5.1">arXiv preprint arXiv:2108.07258</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Byrne et al<span class="ltx_text" id="bib.bibx2.1.1.1">.</span>(2024)]</span>
<span class="ltx_bibblock">
Sean Anthony Byrne, Nora Castner, Efe Bozkir, Diederick C Niehorster, and Enkelejda Kasneci. 2024.

</span>
<span class="ltx_bibblock">From Lenses to Living Rooms: A Policy Brief on Eye Tracking in XR Before the Impending Boom. In <em class="ltx_emph ltx_font_italic" id="bib.bibx2.4.1">2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)</em>. IEEE, 90–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Byrne et al<span class="ltx_text" id="bib.bibx3.1.1.1">.</span>(2023)]</span>
<span class="ltx_bibblock">
Sean Anthony Byrne, Virmarie Maquiling, Marcus Nyström, Enkelejda Kasneci, and Diederick C. Niehorster. 2023.

</span>
<span class="ltx_bibblock">LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.06129 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.06129" title="">https://arxiv.org/abs/2309.06129</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Fuhl et al<span class="ltx_text" id="bib.bibx4.1.1.1">.</span>(2021)]</span>
<span class="ltx_bibblock">
Wolfgang Fuhl, Gjergji Kasneci, and Enkelejda Kasneci. 2021.

</span>
<span class="ltx_bibblock">Teyed: Over 20 million real-world eye images with pupil, eyelid, and iris 2d and 3d segmentations, 2d and 3d landmarks, 3d eyeball, gaze vector, and eye movement types. In <em class="ltx_emph ltx_font_italic" id="bib.bibx4.4.1">2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>. IEEE, 367–375.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Fuhl et al<span class="ltx_text" id="bib.bibx5.1.1.1">.</span>(2015)]</span>
<span class="ltx_bibblock">
Wolfgang Fuhl, Thomas Kübler, Katrin Sippel, Wolfgang Rosenstiel, and Enkelejda Kasneci. 2015.

</span>
<span class="ltx_bibblock">Excuse: Robust pupil detection in real-world scenarios. In <em class="ltx_emph ltx_font_italic" id="bib.bibx5.4.1">Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, 2015 Proceedings, Part I 16</em>. Springer, 39–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Fuhl et al<span class="ltx_text" id="bib.bibx6.1.1.1">.</span>(2016a)]</span>
<span class="ltx_bibblock">
Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, and Enkelejda Kasneci. 2016a.

</span>
<span class="ltx_bibblock">Pupilnet: Convolutional neural networks for robust pupil detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx6.4.1">arXiv preprint arXiv:1601.04902</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Fuhl et al<span class="ltx_text" id="bib.bibx7.1.1.1">.</span>(2016b)]</span>
<span class="ltx_bibblock">
Wolfgang Fuhl, Thiago C Santini, Thomas Kübler, and Enkelejda Kasneci. 2016b.

</span>
<span class="ltx_bibblock">Else: Ellipse selection for robust pupil detection in real-world environments. In <em class="ltx_emph ltx_font_italic" id="bib.bibx7.4.1">Proceedings of the ninth biennial ACM symposium on eye tracking research &amp; applications</em>. 123–130.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Garbin et al<span class="ltx_text" id="bib.bibx8.1.1.1">.</span>(2019)]</span>
<span class="ltx_bibblock">
Stephan J Garbin, Yiru Shen, Immo Schuetz, Robert Cavin, Gregory Hughes, and Sachin S Talathi. 2019.

</span>
<span class="ltx_bibblock">Openeds: Open eye dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx8.4.1">arXiv preprint arXiv:1905.03702</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Kasneci et al<span class="ltx_text" id="bib.bibx9.1.1.1">.</span>(2014)]</span>
<span class="ltx_bibblock">
Enkelejda Kasneci, Katrin Sippel, Kathrin Aehling, Martin Heister, Wolfgang Rosenstiel, Ulrich Schiefer, and Elena Papageorgiou. 2014.

</span>
<span class="ltx_bibblock">Driving with binocular visual field loss? A study on a supervised on-road parcours with simultaneous eye and head tracking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx9.4.1">PloS one</em> 9, 2 (2014), e87470.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Kim et al<span class="ltx_text" id="bib.bibx10.1.1.1">.</span>(2019)]</span>
<span class="ltx_bibblock">
Joohwan Kim, Michael Stengel, Alexander Majercik, Shalini De Mello, David Dunn, Samuli Laine, Morgan McGuire, and David Luebke. 2019.

</span>
<span class="ltx_bibblock">Nvgaze: An anatomically-informed dataset for low-latency, near-eye gaze estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bibx10.4.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Kirillov et al<span class="ltx_text" id="bib.bibx11.1.1.1">.</span>(2023)]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al<span class="ltx_text" id="bib.bibx11.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Segment anything. In <em class="ltx_emph ltx_font_italic" id="bib.bibx11.5.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 4015–4026.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Kothari et al<span class="ltx_text" id="bib.bibx12.1.1.1">.</span>(2020)]</span>
<span class="ltx_bibblock">
Rakshit Kothari, Zhizhuo Yang, Christopher Kanan, Reynold Bailey, Jeff B Pelz, and Gabriel J Diaz. 2020.

</span>
<span class="ltx_bibblock">Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx12.4.1">Scientific reports</em> 10, 1 (2020), 2539.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Kothari et al<span class="ltx_text" id="bib.bibx13.1.1.1">.</span>(2022)]</span>
<span class="ltx_bibblock">
Rakshit S Kothari, Reynold J Bailey, Christopher Kanan, Jeff B Pelz, and Gabriel J Diaz. 2022.

</span>
<span class="ltx_bibblock">EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx13.4.1">Proceedings of the ACM on human-computer interaction</em> 6, ETRA (2022), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Maquiling et al<span class="ltx_text" id="bib.bibx14.1.1.1">.</span>(2024)]</span>
<span class="ltx_bibblock">
Virmarie Maquiling, Sean Anthony Byrne, Diederick C Niehorster, Marcus Nyström, and Enkelejda Kasneci. 2024.

</span>
<span class="ltx_bibblock">Zero-shot segmentation of eye features using the segment anything model (sam).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx14.4.1">Proceedings of the ACM on Computer Graphics and Interactive Techniques</em> 7, 2 (2024), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Nyström et al<span class="ltx_text" id="bib.bibx15.1.1.1">.</span>(2023)]</span>
<span class="ltx_bibblock">
Marcus Nyström, Diederick C Niehorster, Richard Andersson, Roy S Hessels, and Ignace TC Hooge. 2023.

</span>
<span class="ltx_bibblock">The amplitude of small eye movements can be accurately estimated with video-based eye trackers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx15.4.1">Behavior Research Methods</em> 55, 2 (2023), 657–669.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Palmero et al<span class="ltx_text" id="bib.bibx16.1.1.1">.</span>(2020)]</span>
<span class="ltx_bibblock">
Cristina Palmero, Abhishek Sharma, Karsten Behrendt, Kapil Krishnakumar, Oleg V Komogortsev, and Sachin S Talathi. 2020.

</span>
<span class="ltx_bibblock">Openeds2020: Open eyes dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx16.4.1">arXiv preprint arXiv:2005.03876</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Peréz et al<span class="ltx_text" id="bib.bibx17.1.1.1">.</span>(2003)]</span>
<span class="ltx_bibblock">
Antonio Peréz, M Luisa Córdoba, A Garcia, Rafael Méndez, ML Munoz, José Luis Pedraza, and F Sanchez. 2003.

</span>
<span class="ltx_bibblock">A precise eye-gaze detection and tracking system.

</span>
<span class="ltx_bibblock">(2003).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Ravi et al<span class="ltx_text" id="bib.bibx18.1.1.1">.</span>(2024)]</span>
<span class="ltx_bibblock">
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al<span class="ltx_text" id="bib.bibx18.4.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Sam 2: Segment anything in images and videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx18.5.1">arXiv preprint arXiv:2408.00714</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Sambasivan et al<span class="ltx_text" id="bib.bibx19.1.1.1">.</span>(2021)]</span>
<span class="ltx_bibblock">
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021.

</span>
<span class="ltx_bibblock">“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI. In <em class="ltx_emph ltx_font_italic" id="bib.bibx19.4.1">proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Santini et al<span class="ltx_text" id="bib.bibx20.1.1.1">.</span>(2018a)]</span>
<span class="ltx_bibblock">
Thiago Santini, Wolfgang Fuhl, and Enkelejda Kasneci. 2018a.

</span>
<span class="ltx_bibblock">PuRe: Robust pupil detection for real-time pervasive eye tracking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx20.4.1">Computer Vision and Image Understanding</em> 170 (2018), 40–50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Santini et al<span class="ltx_text" id="bib.bibx21.1.1.1">.</span>(2018b)]</span>
<span class="ltx_bibblock">
Thiago Santini, Wolfgang Fuhl, and Enkelejda Kasneci. 2018b.

</span>
<span class="ltx_bibblock">PuReST: Robust pupil tracking for real-time pervasive eye tracking. In <em class="ltx_emph ltx_font_italic" id="bib.bibx21.4.1">Proceedings of the 2018 ACM symposium on eye tracking research &amp; applications</em>. 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Shortis et al<span class="ltx_text" id="bib.bibx22.1.1.1">.</span>(1994)]</span>
<span class="ltx_bibblock">
Mark R Shortis, Timothy A Clarke, and Tim Short. 1994.

</span>
<span class="ltx_bibblock">Comparison of some techniques for the subpixel location of discrete target images. In <em class="ltx_emph ltx_font_italic" id="bib.bibx22.4.1">Videometrics III</em>, Vol. 2350. SPIE, 239–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Sun et al<span class="ltx_text" id="bib.bibx23.1.1.1">.</span>(2017)]</span>
<span class="ltx_bibblock">
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017.

</span>
<span class="ltx_bibblock">Revisiting unreasonable effectiveness of data in deep learning era. In <em class="ltx_emph ltx_font_italic" id="bib.bibx23.4.1">Proceedings of the IEEE international conference on computer vision</em>. 843–852.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Tonsen et al<span class="ltx_text" id="bib.bibx24.1.1.1">.</span>(2016)]</span>
<span class="ltx_bibblock">
Marc Tonsen, Xucong Zhang, Yusuke Sugano, and Andreas Bulling. 2016.

</span>
<span class="ltx_bibblock">Labelled pupils in the wild: a dataset for studying pupil detection in unconstrained environments. In <em class="ltx_emph ltx_font_italic" id="bib.bibx24.4.1">Proceedings of the ninth biennial ACM symposium on eye tracking research &amp; applications</em>. 139–142.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al<span class="ltx_text" id="bib.bibx25.1.1.1">.</span>(2023)]</span>
<span class="ltx_bibblock">
Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. 2023.

</span>
<span class="ltx_bibblock">Faster segment anything: Towards lightweight sam for mobile applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx25.4.1">arXiv preprint arXiv:2306.14289</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Zhou et al<span class="ltx_text" id="bib.bibx26.1.1.1">.</span>(2023)]</span>
<span class="ltx_bibblock">
Yukun Zhou, Mark A Chia, Siegfried K Wagner, Murat S Ayhan, Dominic J Williamson, Robbert R Struyven, Timing Liu, Moucheng Xu, Mateo G Lozano, Peter Woodward-Court, et al<span class="ltx_text" id="bib.bibx26.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A foundation model for generalizable disease detection from retinal images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bibx26.5.1">Nature</em> 622, 7981 (2023), 156–163.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 11:30:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
