<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04190] Generative AI for Synthetic Data Generation: Methods, Challenges and the Future</title><meta property="og:description" content="The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generative AI for Synthetic Data Generation: Methods, Challenges and the Future">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generative AI for Synthetic Data Generation: Methods, Challenges and the Future">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04190">

<!--Generated on Fri Apr  5 13:35:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Generative AI,  Synthetic Data Generation,  Large Language Models.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generative AI for Synthetic Data Generation: Methods, Challenges and the Future</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xu Guo,¬†, and Yiqiang Chen
</span><span class="ltx_author_notes">Xu Guo is with Nanyang Technological University (NTU), Singapore and Yiqiang Chen is with Institute of Computing
Technology, Chinese Academy of Sciences.Emails: xu008@e.ntu.edu.sg</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Generative AI, Synthetic Data Generation, Large Language Models.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The introduction of Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in 2017, followed by groundbreaking LLMs like OpenAI‚Äôs GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Google‚Äôs BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, marked the beginning of a new era in language understanding and generation. More recently, generative LLMs (e.g., GPT-3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, LlaMa<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and ChatGPT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) have propelled this evolution to unprecedented heights, seamlessly converging with Generative AI and heralding a fresh era in the realm of synthetic data generation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The origins of Generative AI can be traced back to pivotal models such as Generative Adversarial Networks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (GANs) and Variational Autoencoders<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> (VAEs), which demonstrated the ability to generate realistic images and signals<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, it wasn‚Äôt until the advent of LLMs in recent years that Generative AI truly began to flourish. These LLMs, trained on vast datasets, showcased an unprecedented ability to produce coherent and contextually relevant text, pushing the boundaries of what AI could achieve in language-related tasks. The convergence of Generative AI and LLMs in the realm of synthetic data creation represents not merely a technological advancement, but a profound paradigm shift in our approach to data creation and the training of AI models.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Why do we need synthetic data?</span> The necessity for synthetic data arises from the inherent limitations of general-purpose Large Language Models (LLMs) in specialized and private domains, despite their significant achievements across various benchmarks. For instance, ClinicalBERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, adapted from BERT through pre-training on clinical texts, demonstrates superior performance in predicting hospital readmissions compared to the original BERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which was trained on Wikipedia and BookCorpus<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> text data. This highlights a crucial challenge: specialized domains often rely on domain-specific data that is not readily available or open to the public, thereby underscoring the importance of synthetic data in bridging these gaps.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Synergy between LLMs and synthetic data generation.</span>
Large Language Models (LLMs) for synthetic data generation marks a significant frontier in the field of AI. LLMs, such as ChatGPT, have revolutionized our approach to understanding and generating human-like text, providing a mechanism to create rich, contextually relevant synthetic data on an unprecedented scale. This synergy is pivotal in addressing data scarcity and privacy concerns, particularly in domains where real data is either limited or sensitive. By generating text that closely mirrors human language, LLMs facilitate the creation of robust, varied datasets necessary for training and refining AI models across various applications, from healthcare<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, eduction<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to business management<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Moreover, this collaboration opens new avenues for ethical AI development, allowing researchers to bypass the biases and ethical dilemmas often inherent in real-world datasets. The integration of LLMs in synthetic data generation not only pushes the boundaries of what‚Äôs achievable in AI but also ensures a more responsible and inclusive approach to AI development, aligning with evolving ethical standards and societal needs.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Other related survey papers.</span> Comprehensive surveys for Generative AI and LLMs exist, each revisits related works from a different perspective: Generative AI surveys provide a holistic view of this area starting from Generative Adversarial Networks (GANs) to ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and models developed for synthetic data generation in the past decade <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, with a special focus on text-to-image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or text-to-speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> generation as well as practical applications in Education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and Healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>; Surveys for LLMs provide systematic categorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> for NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and methods to adapt these LLMs to specific domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> through model optimization and personalization perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Surveys on LLMs for text generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> focus on developing generative LLMs including model architecture choices and training techniques and do not contain gigantic LLMs released in the past two years. Unlike these survey papers, this paper mainly focuses on recent technologies that employ generative LLMs <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">without training</span> them for synthetic training data generation and elicit their potential impact on practical adoption.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Outline of this paper.</span> The following of this paper is organized as follows. Section <a href="#S2" title="II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces recent methods for generating synthetic data from LLMs. Specifically, we summarize prompt engineering techniques that are particularly designed for probing LLMs to obtain desired data in sub-section <a href="#S2.SS1" title="II-A Prompt engineering ‚Ä£ II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a> while in sub-section <a href="#S2.SS2" title="II-B Parameter-efficient task adaptation ‚Ä£ II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>, we talk about how to employ parameter-efficient methods to adapt LLMs for generating task-related data; In sub-sections <a href="#S2.SS3" title="II-C Measuring data quality ‚Ä£ II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a> and <a href="#S2.SS4" title="II-D Training with synthetic data ‚Ä£ II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a> we introduce methods that can measure the quality of the synthetic dataset and how to effectively make use of the data for training. Section <a href="#S3" title="III Applications ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> details the application of synthetic data, focusing on its utilization in low-resource tasks in Sub-Section <a href="#S3.SS1" title="III-A Low-resource and long-tail problems ‚Ä£ III Applications ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> and practical deployment scenarios in Sub-Section <a href="#S3.SS2" title="III-B Fast inference and lightweight deployment ‚Ä£ III Applications ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. Additionally, Sub-Section <a href="#S3.SS3" title="III-C Medical Scenarios ‚Ä£ III Applications ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> provides a specific case study on the use of synthetic data within medical domains. Finally, in Section <a href="#S4" title="IV Challenges with Synthetic Data and Future Directions ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we underscore some prominent challenges in synthetic data and discuss potential avenues for future research.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.04190/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A general comparison between using LLMs for label-specific synthetic data generation (a) and label words prediction (b). In both cases, the LLMs are frozen and a task-related prompt is provided to condition the LLMs for task adaptation. <math id="S1.F1.3.m1.1" class="ltx_Math" alttext="\langle X\rangle" display="inline"><semantics id="S1.F1.3.m1.1b"><mrow id="S1.F1.3.m1.1.2.2" xref="S1.F1.3.m1.1.2.1.cmml"><mo stretchy="false" id="S1.F1.3.m1.1.2.2.1" xref="S1.F1.3.m1.1.2.1.1.cmml">‚ü®</mo><mi id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">X</mi><mo stretchy="false" id="S1.F1.3.m1.1.2.2.2" xref="S1.F1.3.m1.1.2.1.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><apply id="S1.F1.3.m1.1.2.1.cmml" xref="S1.F1.3.m1.1.2.2"><csymbol cd="latexml" id="S1.F1.3.m1.1.2.1.1.cmml" xref="S1.F1.3.m1.1.2.2.1">delimited-‚ü®‚ü©</csymbol><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">ùëã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\langle X\rangle</annotation></semantics></math> represents the text data and <math id="S1.F1.4.m2.1" class="ltx_Math" alttext="\langle Y\rangle" display="inline"><semantics id="S1.F1.4.m2.1b"><mrow id="S1.F1.4.m2.1.2.2" xref="S1.F1.4.m2.1.2.1.cmml"><mo stretchy="false" id="S1.F1.4.m2.1.2.2.1" xref="S1.F1.4.m2.1.2.1.1.cmml">‚ü®</mo><mi id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml">Y</mi><mo stretchy="false" id="S1.F1.4.m2.1.2.2.2" xref="S1.F1.4.m2.1.2.1.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><apply id="S1.F1.4.m2.1.2.1.cmml" xref="S1.F1.4.m2.1.2.2"><csymbol cd="latexml" id="S1.F1.4.m2.1.2.1.1.cmml" xref="S1.F1.4.m2.1.2.2.1">delimited-‚ü®‚ü©</csymbol><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">ùëå</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\langle Y\rangle</annotation></semantics></math> represents the label words. </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Generating synthetic training data from LLMs</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.5" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the major difference between using generative LLMs for synthetic data generation and the predominant <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{Prompting}" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">Prompting</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">Prompting</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\mathrm{Prompting}</annotation></semantics></math> technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> that directly applies LLMs for label prediction. In short, <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{Prompting}" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">Prompting</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">Prompting</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathrm{Prompting}</annotation></semantics></math> requires deploying the LLM model in practice to predict the label words <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\langle Y\rangle" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.2.2" xref="S2.p1.3.m3.1.2.1.cmml"><mo stretchy="false" id="S2.p1.3.m3.1.2.2.1" xref="S2.p1.3.m3.1.2.1.1.cmml">‚ü®</mo><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">Y</mi><mo stretchy="false" id="S2.p1.3.m3.1.2.2.2" xref="S2.p1.3.m3.1.2.1.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.2.1.cmml" xref="S2.p1.3.m3.1.2.2"><csymbol cd="latexml" id="S2.p1.3.m3.1.2.1.1.cmml" xref="S2.p1.3.m3.1.2.2.1">delimited-‚ü®‚ü©</csymbol><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ùëå</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\langle Y\rangle</annotation></semantics></math> (e.g., negative) from the input text data <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\langle X\rangle" display="inline"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.2.2" xref="S2.p1.4.m4.1.2.1.cmml"><mo stretchy="false" id="S2.p1.4.m4.1.2.2.1" xref="S2.p1.4.m4.1.2.1.1.cmml">‚ü®</mo><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">X</mi><mo stretchy="false" id="S2.p1.4.m4.1.2.2.2" xref="S2.p1.4.m4.1.2.1.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.2.1.cmml" xref="S2.p1.4.m4.1.2.2"><csymbol cd="latexml" id="S2.p1.4.m4.1.2.1.1.cmml" xref="S2.p1.4.m4.1.2.2.1">delimited-‚ü®‚ü©</csymbol><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ùëã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\langle X\rangle</annotation></semantics></math> with additional constraints from the prompt, e.g., ‚Äúthe sentiment of the movie review‚Äù indicates that the context is a movie review and the label shall describe its sentiment. On the contrary, synthetic data generation requires LLMs to generate text data <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\langle X\rangle" display="inline"><semantics id="S2.p1.5.m5.1a"><mrow id="S2.p1.5.m5.1.2.2" xref="S2.p1.5.m5.1.2.1.cmml"><mo stretchy="false" id="S2.p1.5.m5.1.2.2.1" xref="S2.p1.5.m5.1.2.1.1.cmml">‚ü®</mo><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">X</mi><mo stretchy="false" id="S2.p1.5.m5.1.2.2.2" xref="S2.p1.5.m5.1.2.1.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.2.1.cmml" xref="S2.p1.5.m5.1.2.2"><csymbol cd="latexml" id="S2.p1.5.m5.1.2.1.1.cmml" xref="S2.p1.5.m5.1.2.2.1">delimited-‚ü®‚ü©</csymbol><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ùëã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\langle X\rangle</annotation></semantics></math> based on label-conditional prompts. It is the synthetic data distilled from LLMs rather than the LLMs themselves that will be applied in downstream applications, enabling more diverse and unlimited use cases based on synthetic data. Table <a href="#S2.T1" title="TABLE I ‚Ä£ II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> lists the newly emerging methods for generating task-specific training data from LLMs proposed in the past two years.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Generator</span></td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Classifier</span></td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">ZeroGen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.T1.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">GPT2-XL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.1.3.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">LSTM</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S2.T1.1.3.2.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">SST-2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.T1.1.3.2.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.3.2.4.4" class="ltx_text" style="font-size:90%;">, IMDb</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S2.T1.1.3.2.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.3.2.4.7" class="ltx_text" style="font-size:90%;">, QNLI</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.2.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S2.T1.1.3.2.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">DistilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.4.3.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S2.T1.1.4.3.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.4.3.4.1" class="ltx_text" style="font-size:90%;">RTE</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.4.3.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S2.T1.1.4.3.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.4.3.4.4" class="ltx_text" style="font-size:90%;">, SQuAD</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.4.3.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S2.T1.1.4.3.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.5.4.4.1" class="ltx_text" style="font-size:90%;">AdversarialQA</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.5.4.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S2.T1.1.5.4.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.1.1" class="ltx_text" style="font-size:90%;">ZeroGen</span><sup id="S2.T1.1.1.1.2" class="ltx_sup"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">+</span></sup><span id="S2.T1.1.1.1.3" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.T1.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.2.1" class="ltx_text" style="font-size:90%;">GPT2-XL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.3.1" class="ltx_text" style="font-size:90%;">LSTM</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S2.T1.1.1.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.4.1" class="ltx_text" style="font-size:90%;">IMDb</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S2.T1.1.1.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.1.4.4" class="ltx_text" style="font-size:90%;">, SST-2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.T1.1.1.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.1.4.7" class="ltx_text" style="font-size:90%;">, Amazon</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S2.T1.1.1.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">DistilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.6.5.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S2.T1.1.6.5.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">Rotten Tomatoes</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.6.5.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S2.T1.1.6.5.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.6.5.4.4" class="ltx_text" style="font-size:90%;">, Yelp</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.6.5.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.6.5.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.7.6.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">Subj</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.7.6.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S2.T1.1.7.6.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.7.6.4.4" class="ltx_text" style="font-size:90%;">, AGNews</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.7.6.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.7.6.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.7.6.4.7" class="ltx_text" style="font-size:90%;">, DBpedia</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.7.6.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.7.6.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">SuperGen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.T1.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">CTRL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.8.7.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="S2.T1.1.8.7.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">COCO-LM</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.8.7.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S2.T1.1.8.7.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.8.7.4.1" class="ltx_text" style="font-size:90%;">GLUE</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.8.7.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S2.T1.1.8.7.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.9.8.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.9.8.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.9.8.3.1" class="ltx_text" style="font-size:90%;">RoBERTa</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.9.8.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S2.T1.1.9.8.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.9.8.4" class="ltx_td"></td>
</tr>
<tr id="S2.T1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.10.9.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.10.9.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.10.9.3.1" class="ltx_text" style="font-size:90%;">GPT-2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.10.9.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.1.10.9.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.10.9.4" class="ltx_td"></td>
</tr>
<tr id="S2.T1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.11.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.1.1" class="ltx_text" style="font-size:90%;">FewGen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.11.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.T1.1.11.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.2.1" class="ltx_text" style="font-size:90%;">CTRL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.11.10.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="S2.T1.1.11.10.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.3.1" class="ltx_text" style="font-size:90%;">RoBERTa</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.11.10.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S2.T1.1.11.10.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.11.10.4.1" class="ltx_text" style="font-size:90%;">GLUE</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.11.10.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S2.T1.1.11.10.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.12.11" class="ltx_tr">
<td id="S2.T1.1.12.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.1.1" class="ltx_text" style="font-size:90%;">ReGen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S2.T1.1.12.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.2.1" class="ltx_text" style="font-size:90%;">Condenser</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S2.T1.1.12.11.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.3.1" class="ltx_text" style="font-size:90%;">RoBERTa</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S2.T1.1.12.11.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.12.11.4.1" class="ltx_text" style="font-size:90%;">AGNews</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.12.11.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.12.11.4.4" class="ltx_text" style="font-size:90%;">,DBpedia</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.12.11.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.12.11.4.7" class="ltx_text" style="font-size:90%;">, MR</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.12.11.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S2.T1.1.12.11.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.13.12" class="ltx_tr">
<td id="S2.T1.1.13.12.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.13.12.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.13.12.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.13.12.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.13.12.4.1" class="ltx_text" style="font-size:90%;">NYT</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.13.12.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="S2.T1.1.13.12.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.13.12.4.4" class="ltx_text" style="font-size:90%;">, Yahoo</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.13.12.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.13.12.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.13.12.4.7" class="ltx_text" style="font-size:90%;">, Amazon</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.13.12.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S2.T1.1.13.12.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.14.13" class="ltx_tr">
<td id="S2.T1.1.14.13.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.14.13.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.14.13.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.14.13.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.14.13.4.1" class="ltx_text" style="font-size:90%;">Yelp</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.14.13.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.14.13.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.14.13.4.4" class="ltx_text" style="font-size:90%;">, SST-2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.14.13.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.T1.1.14.13.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.14.13.4.7" class="ltx_text" style="font-size:90%;">, IMDb</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.14.13.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S2.T1.1.14.13.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.15.14" class="ltx_tr">
<td id="S2.T1.1.15.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.15.14.1.1" class="ltx_text" style="font-size:90%;">ProGen </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.T1.1.15.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.15.14.2.1" class="ltx_text" style="font-size:90%;">GPT2-XL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.1.15.14.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.15.14.3.1" class="ltx_text" style="font-size:90%;">LSTM</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S2.T1.1.15.14.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.15.14.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.15.14.4.1" class="ltx_text" style="font-size:90%;">SST-2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.T1.1.15.14.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.15.14.4.4" class="ltx_text" style="font-size:90%;">, IMDb</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S2.T1.1.15.14.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.15.14.4.7" class="ltx_text" style="font-size:90%;">, Elec</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.15.14.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S2.T1.1.15.14.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.16.15" class="ltx_tr">
<td id="S2.T1.1.16.15.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.16.15.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.16.15.3.1" class="ltx_text" style="font-size:90%;">DistilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.16.15.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S2.T1.1.16.15.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.16.15.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.16.15.4.1" class="ltx_text" style="font-size:90%;">Rotten Tomatoes</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.16.15.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S2.T1.1.16.15.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.16.15.4.4" class="ltx_text" style="font-size:90%;">, Yelp</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.16.15.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S2.T1.1.16.15.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.17.16" class="ltx_tr">
<td id="S2.T1.1.17.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.17.16.1.1" class="ltx_text" style="font-size:90%;">AttrPrompt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.17.16.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S2.T1.1.17.16.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.17.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.17.16.2.1" class="ltx_text" style="font-size:90%;">ChatGPT</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.17.16.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S2.T1.1.17.16.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.17.16.3.1" class="ltx_text" style="font-size:90%;">BERT</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.17.16.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S2.T1.1.17.16.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.17.16.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.17.16.4.1" class="ltx_text" style="font-size:90%;">NYT</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.17.16.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="S2.T1.1.17.16.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.17.16.4.4" class="ltx_text" style="font-size:90%;">, Amazon</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.17.16.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S2.T1.1.17.16.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.18.17" class="ltx_tr">
<td id="S2.T1.1.18.17.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.18.17.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.18.17.3" class="ltx_td ltx_align_center ltx_border_r">
<span id="S2.T1.1.18.17.3.1" class="ltx_text" style="font-size:90%;">DistilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.18.17.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S2.T1.1.18.17.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.18.17.4" class="ltx_td ltx_align_center">
<span id="S2.T1.1.18.17.4.1" class="ltx_text" style="font-size:90%;">Reddit</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.18.17.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S2.T1.1.18.17.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.18.17.4.4" class="ltx_text" style="font-size:90%;">, StackExchange</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.18.17.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S2.T1.1.18.17.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.19.18" class="ltx_tr">
<td id="S2.T1.1.19.18.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.19.18.1.1" class="ltx_text" style="font-size:90%;">MixPrompt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.19.18.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.T1.1.19.18.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.19.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.19.18.2.1" class="ltx_text" style="font-size:90%;">FLAN-T5 XXL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.19.18.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S2.T1.1.19.18.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.19.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.19.18.3.1" class="ltx_text" style="font-size:90%;">GODEL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.19.18.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S2.T1.1.19.18.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T1.1.19.18.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.19.18.4.1" class="ltx_text" style="font-size:90%;">NLU++</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.19.18.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib58" title="" class="ltx_ref">58</a><span id="S2.T1.1.19.18.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.19.18.4.4" class="ltx_text" style="font-size:90%;">,TOPv2</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.19.18.4.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib59" title="" class="ltx_ref">59</a><span id="S2.T1.1.19.18.4.6.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S2.T1.1.20.19" class="ltx_tr">
<td id="S2.T1.1.20.19.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.1.20.19.2" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.1.20.19.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.1.20.19.4" class="ltx_td ltx_align_center ltx_border_b">
<span id="S2.T1.1.20.19.4.1" class="ltx_text" style="font-size:90%;">CrossNER </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.20.19.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib60" title="" class="ltx_ref">60</a><span id="S2.T1.1.20.19.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Data generation methods. Generator refers to LLMs that are used for synthetic data generation. Classifier refers to small-scale models that are trained on the synthetic data. These methods are limited to NLP models and tasks.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Prompt engineering</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Designing an informative prompt is the key to effective data generation with LLMs. A simple and straightforward approach is to embed the label information in the prompt to refrain LLMs from generating label-agnostic data as described in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a). However, due to the limited number of words in labels and the limited task information in the prompt, the data generated by LLMs still can be unrelated to the task and lack diversity, limiting the size of the synthetic dataset that can be generated from the same LLM. As such, more advanced prompt engineering techniques are expected to circumvent the limitations of traditional ones.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.4" class="ltx_p"><span id="S2.SS1.p2.4.1" class="ltx_text ltx_font_bold">Attribute-controlled prompt.</span> A clear definition for a specific task can be obtained by specifying a set of attributes. Take News classification as an example, one piece of News article can differ from another by providing the details of <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{location}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">location</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">location</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathrm{location}</annotation></semantics></math>, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{topic}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">topic</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">topic</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathrm{topic}</annotation></semantics></math>, <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathrm{text}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">text</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">text</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mathrm{text}</annotation></semantics></math> <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathrm{genre}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">genre</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">genre</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\mathrm{genre}</annotation></semantics></math> and so on. Inspired by this, MSP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> employs a mixture of attributes in the prompt template to obtain desired synthetic data. In AttrPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, authors show that such attribute-specific prompts can be directly extracted from ChatGPT and then applied to query ChatGPT for generating attribute-specific data. By expanding the simple class-conditional prompt with more attribute constraints, we can gather more diverse synthetic data from LLMs while ensuring relevance to the given task.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Verbalizer.</span> The verbalizer technique was originally proposed to enhance <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{Prompting}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">Prompting</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">Prompting</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\mathrm{Prompting}</annotation></semantics></math> performance, where the target label words are expanded with their neighbouring words that hold the same semantic meanings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. This strategy can be directly utilized to promote diverse data generation by expanding the class-conditional prompt into a set of semantically similar prompts. Besides, the verbalizer values can be extracted from LLMs themselves. For example, MetaPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> first obtains an expanded prompt from ChatGPT and further applies the enriched prompt to prompt LLMs for data generation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Parameter-efficient task adaptation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Parameter-efficient approaches in the era of LLMs generally refer to the tuning methods that only tune a small set of an LLM‚Äôs parameters (e.g., bias terms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, embeddings or last layer) or an extra set of parameters that are inserted to LLMs (e.g., Adapters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, Prompt Tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Prefix Tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> and LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>). In the tuning process, the parameters of the LLM backbone are not updated and only the small set of trainable parameters are learned on task-specific datasets to achieve domain adaptation. More parameter-efficient methods can be found in the survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. The advantage of parameter-efficient methods is that they grasp new task information while retaining powerful pre-trained knowledge.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To enable a general LLM to generate data for a specific task style, one promising approach is to aggregate a few-shot dataset (e.g., eight instances per class) and perform parameter-efficient adaptation for the LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. The method, FewGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, demonstrates that by tuning a few set of prefix vectors prepended to the CTRL model (1.6 Billion parameters) on few-shot datasets, the PrefixCTRL can generate more task-related training data. Similarly, MSP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> trains a set of soft prompt embeddings on few-shot task-specific training data and then applies the trained soft prompts to condition the FLAN-T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> (T5<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> further trained on instruction tuning datasets) for text generation. Compared with zero-shot generation, a small budget for few-shot task data can allow the general-purpose LLMs to quickly adapt to the target task under the parameter-efficient learning paradigm.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Measuring data quality</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.8" class="ltx_p">The quality of synthetic data is often measured by quantitative metrics. In ZeroGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, authors measured the quality of the generated data from three perspectives: <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{diversity}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">diversity</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">diversity</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\mathrm{diversity}</annotation></semantics></math>, <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{correctnes}" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">correctnes</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">correctnes</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\mathrm{correctnes}</annotation></semantics></math>, and <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathrm{naturalness}" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">naturalness</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">naturalness</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\mathrm{naturalness}</annotation></semantics></math>. <math id="S2.SS3.p1.4.m4.1" class="ltx_Math" alttext="\mathrm{Diversity}" display="inline"><semantics id="S2.SS3.p1.4.m4.1a"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">Diversity</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">Diversity</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">\mathrm{Diversity}</annotation></semantics></math> measures the difference between a chunk of text and another in the generated instances. For example, 4-<math id="S2.SS3.p1.5.m5.1" class="ltx_Math" alttext="\mathrm{gram}" display="inline"><semantics id="S2.SS3.p1.5.m5.1a"><mi id="S2.SS3.p1.5.m5.1.1" xref="S2.SS3.p1.5.m5.1.1.cmml">gram</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m5.1b"><ci id="S2.SS3.p1.5.m5.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1">gram</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m5.1c">\mathrm{gram}</annotation></semantics></math> Self-BLEU computes BLEU scores on every four consecutive tokens in the generated texts. <math id="S2.SS3.p1.6.m6.1" class="ltx_Math" alttext="\mathrm{Correctnes}" display="inline"><semantics id="S2.SS3.p1.6.m6.1a"><mi id="S2.SS3.p1.6.m6.1.1" xref="S2.SS3.p1.6.m6.1.1.cmml">Correctnes</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m6.1b"><ci id="S2.SS3.p1.6.m6.1.1.cmml" xref="S2.SS3.p1.6.m6.1.1">Correctnes</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m6.1c">\mathrm{Correctnes}</annotation></semantics></math> measures whether the data instance is related to the given label. Existing approaches for measuring <math id="S2.SS3.p1.7.m7.1" class="ltx_Math" alttext="\mathrm{correctnes}" display="inline"><semantics id="S2.SS3.p1.7.m7.1a"><mi id="S2.SS3.p1.7.m7.1.1" xref="S2.SS3.p1.7.m7.1.1.cmml">correctnes</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.7.m7.1b"><ci id="S2.SS3.p1.7.m7.1.1.cmml" xref="S2.SS3.p1.7.m7.1.1">correctnes</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.7.m7.1c">\mathrm{correctnes}</annotation></semantics></math> can be divided into two categories: automatic evaluation and human evaluation. Automatic evaluation methods train a model (e.g., RoBERTa-large) on the oracle training dataset in a fully-supervised full-model fine-tuning manner, and then apply the model to calculate the percentage of correctly predicted samples on the synthetic dataset. Human evaluation requires the availability of human annotators who will be assigned a random subset of the synthetic dataset and asked to judge whether the content is related to the label. <math id="S2.SS3.p1.8.m8.1" class="ltx_Math" alttext="\mathrm{Naturalness}" display="inline"><semantics id="S2.SS3.p1.8.m8.1a"><mi id="S2.SS3.p1.8.m8.1.1" xref="S2.SS3.p1.8.m8.1.1.cmml">Naturalness</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.8.m8.1b"><ci id="S2.SS3.p1.8.m8.1.1.cmml" xref="S2.SS3.p1.8.m8.1.1">Naturalness</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.8.m8.1c">\mathrm{Naturalness}</annotation></semantics></math> measurement requires human evaluators who can assess whether the generated text is fluent and similar to human-written texts by selecting a score from a given range.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To obtain high-quality synthetic data, ProGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposes to incorporate a quality estimation module in the data generation pipeline, where the firstly generated synthetic data are evaluated by a task-specific model that was trained on oracle data in advance. Then, the most influential synthetic samples are selected as in-context examples to prompt GPT2-XL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to generate a new set of synthetic data.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Training with synthetic data</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">In the process of training with synthetic data generated from LLMs, challenges such as inherent biases and hallucinations in the LLMs can introduce noise into the dataset, despite meticulous prompt design and supervised training. To mitigate these issues, the implementation of regularization techniques is crucial for stabilizing training with noisy datasets. Innovations like ZeroGen<sup id="S2.SS4.p1.1.1" class="ltx_sup"><span id="S2.SS4.p1.1.1.1" class="ltx_text ltx_font_italic">+</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> suggest the use of a small weight network trained through bilevel optimization to autonomously determine sample weights. Additionally, FewGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> incorporates a self-supervised training approach using temporal ensembling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. This method has been shown to offer superior performance enhancements compared to label smoothing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> when training downstream classifiers on synthetic data, highlighting its effectiveness in dealing with the unique challenges posed by synthetic datasets. Other techniques such as gradual annealing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> also demonstrates to be effective in enhancing the learning performance on synthetic data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Applications</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Synthetic data generated from LLMs can be used in a wide range of applications. In this section, we first introduce how to solve the long-standing low-resource and long-tail problems with synthetic data and its use cases for fast inference and deployment. Then, we present two practical examples of applying synthetic data in medical and education scenarios.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Low-resource and long-tail problems</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Low-resource problems are generally trapped by the lack of sufficient data and in some cases particularly impacted by long-tail classes in practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. Traditional research has predominantly leveraged transfer learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> to enhance performance in low-resource settings. Yet, these methods hinge on the availability of relevant source-domain datasets, which may not always be accessible. The impressive generative capabilities of LLMs and the production of highly realistic synthetic data signal a significant potential to reshape the traditional landscape of low-resource and long-tail problems.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">A primary challenge in merging the research directions of synthetic data generation and low-resource learning tasks is navigating the distribution disparity between real and synthetic data, as well as optimizing the use of synthetic data in training scenarios. Noteworthy approaches to address these issues include the application of regularization techniques. For instance, FewGen employs temporal ensembling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and CAMEL utilizes gradual learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. Additionally, innovative data selection techniques, as explored in Du et al. (2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, offer valuable insights. These methods are instrumental in harnessing the full potential of synthetic data to enhance learning performance, particularly in environments where real data is limited or imbalanced.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Fast inference and lightweight deployment</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Finetuning pre-trained language models on downstream tasks has been the predominant approach starting from the release of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. However, the growing size of these language models, while enhancing performance, imposes practical burdens on organizations requiring swift inference and prompt responses. The shift towards synthetic data generation opens up a realm of possibilities for downstream applications. By generating a curated synthetic dataset, it becomes feasible to train smaller, less complex models, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This approach not only facilitates easier deployment but also ensures faster inference, addressing the critical need for efficiency in real-world applications.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Medical Scenarios</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The medical domain presents unique challenges due to the confidential nature of patient data and the relative scarcity of medical data compared to the abundance of information available online. The use of LLMs and multi-modal LLMs has shown promising potential in medical domains such as dental diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, radiograph analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, and so on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. The exceptional data comprehension and generation capabilities of LLMs position synthetic data generation as an especially promising research avenue in the medical domain.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Data augmentation.</span> Synthetic data generation can help some medical tasks that lack sufficient data to train a strong predictive model. For instance, studies in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> demonstrated that augmenting real datasets with synthetic chest radiograph images generated by latent diffusion models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> can enhance classification performance. In medical language processing, Tang et al. (2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> demonstrated that tailored prompts provided to ChatGPT can yield task-specific synthetic data, significantly boosting the performance in tasks like biological named entity recognition and relation extraction. Additionally, GatorTronGPT, as explored in Peng et al. (2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which involved training GPT-3 from scratch on a dataset amalgamating 277-billion words from English and clinical texts, exhibited remarkable proficiency in generating synthetic clinical text. This data surpassed real data in performance across various biomedical tasks, including relation extraction and question answering, showcasing the potential of synthetic data in transforming medical AI applications.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Missing value imputation.</span> Medical data can be sparse in that patients may take different or do not take some examinations, leading to imbalanced attributes. Missing value imputation (MVI) methods are helpful in enhancing the density of medical attribute values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. Traditional MVI approaches typically involve random sampling from specified value ranges, as noted in Luo et al. (2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, essentially serving as a form of random data augmentation for certain attributes. With the advent of multi-modal LLMs, Ozbey et al. (2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> demonstrate that in cross-modality translation tasks, missing images under specific attributes can be effectively imputed using synthetic images generated from diffusion models. Such synthetic data, compared to traditional random imputation methods, offer more diverse information, thereby helping to mitigate the issue of overfitting in attributes with limited data.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Challenges with Synthetic Data and Future Directions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Many domains suffer from a lack of quality data, especially when it comes to rare events or minority classes. LLMs can augment existing datasets, creating balanced and comprehensive data sets that improve the training and performance of machine learning models. In this section, we highlight some challenges in the creation and use of synthetic data and discuss promising research directions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Overcoming Data Limitations</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Synthetic data generated from LLMs inherently faces several data limitations that must be acknowledged and addressed.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Correctness and Diversity.</span> In Section <a href="#S2" title="II Generating synthetic training data from LLMs ‚Ä£ Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we summarized existing approaches for monitoring the data quality and promoting data diversity in generation. They demonstrated effectiveness but do not entirely solved the problem. The challenge of ensuring the quality and accuracy of the generated data still remains profound. As an inherent nature, LLMs may inadvertently propagate inaccuracies or biases present in their pre-training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, leading to outputs that may not always align with factual or unbiased information. Additionally, the intra-class and inter-class data diversity and domain representativeness are a concern, especially in specialized or niche domains.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Hallucination.</span> Synthetic data generated by Large Language Models (LLMs) can sometimes be not only inaccurate but completely fictitious or disconnected from reality, a phenomenon often referred to as ‚Äùhallucination‚Äù <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. For instance, image generation based on specific captions can result in outputs with unrealistic features, such as a soldier depicted with three hands, as noted in the studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> for cross-modality generation. This hallucination issue is frequently linked to the quality of the training data, particularly if it contains inaccuracies that the LLM then overfits during the pre-training phase. The challenge is compounded due to the difficulty of either fine-tuning LLMs or modifying their pre-training data. Therefore, there‚Äôs a pressing need to develop new, more effective strategies to detect and address hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> in the context of synthetic data generation, ensuring the reliability and authenticity of the output.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Data privacy and ethical concerns</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">While synthetic data offers a way to leverage the power of AI without compromising individual privacy<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, the ethical implications of using synthetic data, particularly in sensitive domains, raise questions about privacy and consent, as the boundaries between real and synthetic data blur. Research in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> demonstrates that it is possible to extract specific information from the datasets used in training LLMs. Consequently, there exists a risk that synthetic data generation might inadvertently reveal elements of the underlying training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, some of which might be subject to licensing agreements. This scenario poses not only privacy issues but also potential financial implications for users, highlighting the need for careful management and consideration in the use and dissemination of synthetic data generated by LLMs.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Moreover, uploading data to LLM APIs also remains a data privacy concern. For instance, employing LLMs in clinical text mining poses significant privacy risks related to uploading patient information to LLM APIs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. This challenge necessitates a careful balance between leveraging the benefits of AI and respecting the confidentiality and privacy of individuals, particularly in healthcare and other sensitive areas. Addressing these concerns requires not just technological solutions, but also robust policy frameworks and ethical guidelines to ensure responsible use of synthetic data and AI technologies.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper reviews recent research on utilizing generative LLMs for synthetic data generation. With a focus on gigantic LLMs which are fixed for inference, we elicit the complexities of producing high-quality and diverse synthetic data and present some recent effective strategies to navigate these challenges, including attribute-controlled prompt engineering and verbalization strategies. Additionally, we also introduce some practical training techniques for training downstream models on the synthetic data presuming the data quality is inadequate. Then, we introduce some application scenarios for the use of synthetic data generation, extending from general low-resource issues to more specialized medical contexts. Finally, we conclude by spotlighting the significant ongoing challenges in the realm of synthetic data and proposing potential avenues for future research in this evolving field.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Xu Guo thanks the Wallenberg-NTU Presidential Postdoctoral Fellowship.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, L.¬†Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T.¬†Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†D. Kaplan, P.¬†Dhariwal, A.¬†Neelakantan, P.¬†Shyam, G.¬†Sastry, A.¬†Askell <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúLanguage models are few-shot learners,‚Äù <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†33, pp. 1877‚Äì1901, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.¬†Devlin, M.-W. Chang, K.¬†Lee, and K.¬†Toutanova, ‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding,‚Äù in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, J.¬†Burstein, C.¬†Doran, and T.¬†Solorio, Eds.¬†¬†¬†Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171‚Äì4186. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/N19-1423</span>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T.¬†Kojima, S.¬†S. Gu, M.¬†Reid, Y.¬†Matsuo, and Y.¬†Iwasawa, ‚ÄúLarge language models are zero-shot reasoners,‚Äù 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.¬†Touvron, T.¬†Lavril, G.¬†Izacard, X.¬†Martinet, M.-A. Lachaux, T.¬†Lacroix, B.¬†Rozi√®re, N.¬†Goyal, E.¬†Hambro, F.¬†Azhar, A.¬†Rodriguez, A.¬†Joulin, E.¬†Grave, and G.¬†Lample, ‚ÄúLlama: Open and efficient foundation language models,‚Äù 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
OpenAI, ‚ÄúIntroducing chatgpt,‚Äù 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.¬†Meng, M.¬†Michalski, J.¬†Huang, Y.¬†Zhang, T.¬†Abdelzaher, and J.¬†Han, ‚ÄúTuning language models as training data generators for augmentation-enhanced few-shot learning,‚Äù in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.¬†¬†¬†PMLR, 2023, pp. 24‚Äâ457‚Äì24‚Äâ477.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.¬†Meng, J.¬†Huang, Y.¬†Zhang, and J.¬†Han, ‚ÄúGenerating training data with language models: Towards zero-shot language understanding,‚Äù in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, A.¬†H. Oh, A.¬†Agarwal, D.¬†Belgrave, and K.¬†Cho, Eds., 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=4G1Sfp_1sz7</span>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.¬†Ye, J.¬†Gao, Q.¬†Li, H.¬†Xu, J.¬†Feng, Z.¬†Wu, T.¬†Yu, and L.¬†Kong, ‚ÄúZeroGen: Efficient zero-shot learning via dataset generation,‚Äù in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, Y.¬†Goldberg, Z.¬†Kozareva, and Y.¬†Zhang, Eds.¬†¬†¬†Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 11‚Äâ653‚Äì11‚Äâ669. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.emnlp-main.801</span>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Gao, R.¬†Pi, L.¬†Yong, H.¬†Xu, J.¬†Ye, Z.¬†Wu, W.¬†ZHANG, X.¬†Liang, Z.¬†Li, and L.¬†Kong, ‚ÄúSelf-guided noise-free data generation for efficient zero-shot learning,‚Äù in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=h5OpjGd_lo6</span>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.¬†Ye, J.¬†Gao, Z.¬†Wu, J.¬†Feng, T.¬†Yu, and L.¬†Kong, ‚ÄúProGen: Progressive zero-shot dataset generation via in-context feedback,‚Äù in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, Y.¬†Goldberg, Z.¬†Kozareva, and Y.¬†Zhang, Eds.¬†¬†¬†Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 3671‚Äì3683. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.findings-emnlp.269</span>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.¬†Yu, Y.¬†Zhuang, R.¬†Zhang, Y.¬†Meng, J.¬†Shen, and C.¬†Zhang, ‚ÄúReGen: Zero-shot text classification via training data generation with progressive dense retrieval,‚Äù in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, A.¬†Rogers, J.¬†Boyd-Graber, and N.¬†Okazaki, Eds.¬†¬†¬†Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 11‚Äâ782‚Äì11‚Äâ805. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2023.findings-acl.748</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D.¬†Chen, C.¬†Lee, Y.¬†Lu, D.¬†Rosati, and Z.¬†Yu, ‚ÄúMixture of soft prompts for controllable data generation,‚Äù in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, H.¬†Bouamor, J.¬†Pino, and K.¬†Bali, Eds.¬†¬†¬†Singapore: Association for Computational Linguistics, Dec. 2023, pp. 14‚Äâ815‚Äì14‚Äâ833. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2023.findings-emnlp.988</span>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I.¬†J. Goodfellow, J.¬†Pouget-Abadie, M.¬†Mirza, B.¬†Xu, D.¬†Warde-Farley, S.¬†Ozair, A.¬†Courville, and Y.¬†Bengio, ‚ÄúGenerative adversarial networks,‚Äù 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma and M.¬†Welling, ‚ÄúAuto-encoding variational bayes,‚Äù 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.¬†Wu, J.¬†Donahue, D.¬†Balduzzi, K.¬†Simonyan, and T.¬†Lillicrap, ‚ÄúLogan: Latent optimisation for generative adversarial networks,‚Äù 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K.¬†Huang, J.¬†Altosaar, and R.¬†Ranganath, ‚ÄúClinicalbert: Modeling clinical notes and predicting hospital readmission,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05342</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J.¬†Devlin, M.-W. Chang, K.¬†Lee, and K.¬†Toutanova, ‚ÄúBert: Pre-training of deep bidirectional transformers for language understanding,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y.¬†Zhu, R.¬†Kiros, R.¬†Zemel, R.¬†Salakhutdinov, R.¬†Urtasun, A.¬†Torralba, and S.¬†Fidler, ‚ÄúAligning books and movies: Towards story-like visual explanations by watching movies and reading books,‚Äù in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2015, pp. 19‚Äì27.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C.¬†Peng, X.¬†Yang, A.¬†Chen, K.¬†E. Smith, N.¬†PourNejatian, A.¬†B. Costa, C.¬†Martin, M.¬†G. Flores, Y.¬†Zhang, T.¬†Magoc <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúA study of generative large language model for medical research and healthcare,‚Äù <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13523</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S.¬†Moore, R.¬†Tong, A.¬†Singh, Z.¬†Liu, X.¬†Hu, Y.¬†Lu, J.¬†Liang, C.¬†Cao, H.¬†Khosravi, P.¬†Denny <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúEmpowering education with llms-the next-gen interface and content generation,‚Äù in <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence in Education</em>.¬†¬†¬†Springer, 2023, pp. 32‚Äì37.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
N.¬†Rane, ‚ÄúRole and challenges of chatgpt and similar generative artificial intelligence in business management,‚Äù <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Available at SSRN 4603227</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y.¬†Cao, S.¬†Li, Y.¬†Liu, Z.¬†Yan, Y.¬†Dai, P.¬†S. Yu, and L.¬†Sun, ‚ÄúA comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,‚Äù <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04226</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A.¬†Bauer, S.¬†Trapp, M.¬†Stenger, R.¬†Leppich, S.¬†Kounev, M.¬†Leznik, K.¬†Chard, and I.¬†Foster, ‚ÄúComprehensive exploration of synthetic data generation: A survey,‚Äù <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.02524</em>, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C.¬†Zhang, C.¬†Zhang, M.¬†Zhang, and I.¬†S. Kweon, ‚ÄúText-to-image diffusion model in generative ai: A survey,‚Äù <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.07909</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C.¬†Zhang, C.¬†Zhang, S.¬†Zheng, M.¬†Zhang, M.¬†Qamar, S.-H. Bae, and I.¬†S. Kweon, ‚ÄúA survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai,‚Äù <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.13336</em>, vol.¬†2, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D.¬†Baidoo-Anu and L.¬†O. Ansah, ‚ÄúEducation in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning,‚Äù <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Journal of AI</em>, vol.¬†7, no.¬†1, pp. 52‚Äì62, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P.¬†Yu, H.¬†Xu, X.¬†Hu, and C.¬†Deng, ‚ÄúLeveraging generative ai and large language models: A comprehensive roadmap for healthcare integration,‚Äù in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Healthcare</em>, vol.¬†11, no.¬†20.¬†¬†¬†MDPI, 2023, p. 2776.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X.¬†Qiu, T.¬†Sun, Y.¬†Xu, Y.¬†Shao, N.¬†Dai, and X.¬†Huang, ‚ÄúPre-trained models for natural language processing: A survey,‚Äù <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Science China Technological Sciences</em>, vol.¬†63, no.¬†10, pp. 1872‚Äì1897, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B.¬†Min, H.¬†Ross, E.¬†Sulem, A.¬†P.¬†B. Veyseh, T.¬†H. Nguyen, O.¬†Sainz, E.¬†Agirre, I.¬†Heintz, and D.¬†Roth, ‚ÄúRecent advances in natural language processing via large pre-trained language models: A survey,‚Äù <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol.¬†56, no.¬†2, pp. 1‚Äì40, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
X.¬†Guo, ‚ÄúData-efficient domain adaptation for pretrained language models,‚Äù 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X.¬†Guo and H.¬†Yu, ‚ÄúOn the domain adaptation and generalization of pretrained language models: A survey,‚Äù <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.03154</em>, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.¬†Li, T.¬†Tang, W.¬†X. Zhao, J.-Y. Nie, and J.-R. Wen, ‚ÄúPretrained language models for text generation: A survey,‚Äù <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.05273</em>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
C.¬†Raffel, N.¬†Shazeer, A.¬†Roberts, K.¬†Lee, S.¬†Narang, M.¬†Matena, Y.¬†Zhou, W.¬†Li, and P.¬†J. Liu, ‚ÄúExploring the limits of transfer learning with a unified text-to-text transformer,‚Äù <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol.¬†21, no.¬†1, pp. 5485‚Äì5551, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A.¬†Radford, J.¬†Wu, R.¬†Child, D.¬†Luan, D.¬†Amodei, I.¬†Sutskever <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúLanguage models are unsupervised multitask learners,‚Äù <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">OpenAI blog</em>, vol.¬†1, no.¬†8, p.¬†9, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S.¬†Hochreiter and J.¬†Schmidhuber, ‚ÄúLong short-term memory,‚Äù <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, vol.¬†9, no.¬†8, pp. 1735‚Äì1780, 1997.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
R.¬†Socher, A.¬†Perelygin, J.¬†Wu, J.¬†Chuang, C.¬†D. Manning, A.¬†Ng, and C.¬†Potts, ‚ÄúRecursive deep models for semantic compositionality over a sentiment treebank,‚Äù in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>, D.¬†Yarowsky, T.¬†Baldwin, A.¬†Korhonen, K.¬†Livescu, and S.¬†Bethard, Eds.¬†¬†¬†Seattle, Washington, USA: Association for Computational Linguistics, Oct. 2013, pp. 1631‚Äì1642. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/D13-1170</span>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A.¬†L. Maas, R.¬†E. Daly, P.¬†T. Pham, D.¬†Huang, A.¬†Y. Ng, and C.¬†Potts, ‚ÄúLearning word vectors for sentiment analysis,‚Äù in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, D.¬†Lin, Y.¬†Matsumoto, and R.¬†Mihalcea, Eds.¬†¬†¬†Portland, Oregon, USA: Association for Computational Linguistics, Jun. 2011, pp. 142‚Äì150. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/P11-1015</span>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
P.¬†Rajpurkar, J.¬†Zhang, K.¬†Lopyrev, and P.¬†Liang, ‚ÄúSQuAD: 100,000+ questions for machine comprehension of text,‚Äù in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, J.¬†Su, K.¬†Duh, and X.¬†Carreras, Eds.¬†¬†¬†Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383‚Äì2392. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/D16-1264</span>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
V.¬†Sanh, L.¬†Debut, J.¬†Chaumond, and T.¬†Wolf, ‚ÄúDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter,‚Äù 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
I.¬†Dagan, O.¬†Glickman, and B.¬†Magnini, ‚ÄúThe pascal recognising textual entailment challenge,‚Äù in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Machine learning challenges workshop</em>.¬†¬†¬†Springer, 2005, pp. 177‚Äì190.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M.¬†Bartolo, A.¬†Roberts, J.¬†Welbl, S.¬†Riedel, and P.¬†Stenetorp, ‚ÄúBeat the AI: Investigating adversarial human annotation for reading comprehension,‚Äù <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol.¬†8, pp. 662‚Äì678, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2020.tacl-1.43</span>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J.¬†McAuley and J.¬†Leskovec, ‚ÄúHidden factors and hidden topics: understanding rating dimensions with review text,‚Äù in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th ACM conference on Recommender systems</em>, 2013, pp. 165‚Äì172.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
B.¬†Pang and L.¬†Lee, ‚ÄúSeeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales,‚Äù in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL‚Äô05)</em>, K.¬†Knight, H.¬†T. Ng, and K.¬†Oflazer, Eds.¬†¬†¬†Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 115‚Äì124. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/P05-1015</span>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
X.¬†Zhang, J.¬†Zhao, and Y.¬†LeCun, ‚ÄúCharacter-level convolutional networks for text classification,‚Äù <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†28, 2015.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B.¬†Pang and L.¬†Lee, ‚ÄúA sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts,‚Äù in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</em>, Barcelona, Spain, Jul. 2004, pp. 271‚Äì278. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/P04-1035</span>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
N.¬†S. Keskar, B.¬†McCann, L.¬†R. Varshney, C.¬†Xiong, and R.¬†Socher, ‚ÄúCtrl: A conditional transformer language model for controllable generation,‚Äù <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05858</em>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Y.¬†Meng, C.¬†Xiong, P.¬†Bajaj, P.¬†Bennett, J.¬†Han, X.¬†Song <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúCoco-lm: Correcting and contrasting text sequences for language model pretraining,‚Äù <em id="bib.bib48.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.¬†34, pp. 23‚Äâ102‚Äì23‚Äâ114, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A.¬†Wang, A.¬†Singh, J.¬†Michael, F.¬†Hill, O.¬†Levy, and S.¬†Bowman, ‚ÄúGLUE: A multi-task benchmark and analysis platform for natural language understanding,‚Äù in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, T.¬†Linzen, G.¬†Chrupa≈Ça, and A.¬†Alishahi, Eds.¬†¬†¬†Brussels, Belgium: Association for Computational Linguistics, Nov. 2018, pp. 353‚Äì355. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/W18-5446</span>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Y.¬†Liu, M.¬†Ott, N.¬†Goyal, J.¬†Du, M.¬†Joshi, D.¬†Chen, O.¬†Levy, M.¬†Lewis, L.¬†Zettlemoyer, and V.¬†Stoyanov, ‚ÄúRoberta: A robustly optimized bert pretraining approach,‚Äù <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
L.¬†Gao and J.¬†Callan, ‚ÄúCondenser: a pre-training architecture for dense retrieval,‚Äù in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, M.-F. Moens, X.¬†Huang, L.¬†Specia, and S.¬†W.-t. Yih, Eds.¬†¬†¬†Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 981‚Äì993. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2021.emnlp-main.75</span>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y.¬†Meng, J.¬†Shen, C.¬†Zhang, and J.¬†Han, ‚ÄúWeakly-supervised hierarchical text classification,‚Äù <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.¬†33, no.¬†01, pp. 6826‚Äì6833, Jul. 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ojs.aaai.org/index.php/AAAI/article/view/4658</span>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Y.¬†Yu, Y.¬†Zhuang, J.¬†Zhang, Y.¬†Meng, A.¬†Ratner, R.¬†Krishna, J.¬†Shen, and C.¬†Zhang, ‚ÄúLarge language model as attributed training data generator: A tale of diversity and bias,‚Äù in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=6hZIfAY9GD</span>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J.¬†Blitzer, M.¬†Dredze, and F.¬†Pereira, ‚ÄúBiographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification,‚Äù in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</em>, A.¬†Zaenen and A.¬†van¬†den Bosch, Eds.¬†¬†¬†Prague, Czech Republic: Association for Computational Linguistics, Jun. 2007, pp. 440‚Äì447. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/P07-1056</span>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
G.¬†Geigle, N.¬†Reimers, A.¬†R√ºckl√©, and I.¬†Gurevych, ‚ÄúTweac: Transformer with extendable qa agent classifiers,‚Äù 2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
H.¬†W. Chung, L.¬†Hou, S.¬†Longpre, B.¬†Zoph, Y.¬†Tay, W.¬†Fedus, Y.¬†Li, X.¬†Wang, M.¬†Dehghani, S.¬†Brahma, A.¬†Webson, S.¬†S. Gu, Z.¬†Dai, M.¬†Suzgun, X.¬†Chen, A.¬†Chowdhery, A.¬†Castro-Ros, M.¬†Pellat, K.¬†Robinson, D.¬†Valter, S.¬†Narang, G.¬†Mishra, A.¬†Yu, V.¬†Zhao, Y.¬†Huang, A.¬†Dai, H.¬†Yu, S.¬†Petrov, E.¬†H. Chi, J.¬†Dean, J.¬†Devlin, A.¬†Roberts, D.¬†Zhou, Q.¬†V. Le, and J.¬†Wei, ‚ÄúScaling instruction-finetuned language models,‚Äù 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
B.¬†Peng, M.¬†Galley, P.¬†He, C.¬†Brockett, L.¬†Liden, E.¬†Nouri, Z.¬†Yu, B.¬†Dolan, and J.¬†Gao, ‚ÄúGodel: Large-scale pre-training for goal-directed dialog,‚Äù 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
I.¬†Casanueva, I.¬†Vuliƒá, G.¬†Spithourakis, and P.¬†Budzianowski, ‚ÄúNLU++: A multi-label, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue,‚Äù in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL 2022</em>, M.¬†Carpuat, M.-C. de¬†Marneffe, and I.¬†V. Meza¬†Ruiz, Eds.¬†¬†¬†Seattle, United States: Association for Computational Linguistics, Jul. 2022, pp. 1998‚Äì2013. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.findings-naacl.154</span>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
X.¬†Chen, A.¬†Ghoshal, Y.¬†Mehdad, L.¬†Zettlemoyer, and S.¬†Gupta, ‚ÄúLow-resource domain adaptation for compositional task-oriented semantic parsing,‚Äù in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, B.¬†Webber, T.¬†Cohn, Y.¬†He, and Y.¬†Liu, Eds.¬†¬†¬†Online: Association for Computational Linguistics, Nov. 2020, pp. 5090‚Äì5100. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2020.emnlp-main.413</span>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Z.¬†Liu, Y.¬†Xu, T.¬†Yu, W.¬†Dai, Z.¬†Ji, S.¬†Cahyawijaya, A.¬†Madotto, and P.¬†Fung, ‚ÄúCrossner: Evaluating cross-domain named entity recognition,‚Äù <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.¬†35, no.¬†15, pp. 13‚Äâ452‚Äì13‚Äâ460, May 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ojs.aaai.org/index.php/AAAI/article/view/17587</span>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
G.¬†Cui, S.¬†Hu, N.¬†Ding, L.¬†Huang, and Z.¬†Liu, ‚ÄúPrototypical verbalizer for prompt-based few-shot tuning,‚Äù in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, S.¬†Muresan, P.¬†Nakov, and A.¬†Villavicencio, Eds.¬†¬†¬†Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 7014‚Äì7024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.acl-long.483</span>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
S.¬†Hu, N.¬†Ding, H.¬†Wang, Z.¬†Liu, J.¬†Wang, J.¬†Li, W.¬†Wu, and M.¬†Sun, ‚ÄúKnowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification,‚Äù 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
L.¬†Reynolds and K.¬†McDonell, ‚ÄúPrompt programming for large language models: Beyond the few-shot paradigm,‚Äù 2021.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
E.¬†Ben¬†Zaken, Y.¬†Goldberg, and S.¬†Ravfogel, ‚ÄúBitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models,‚Äù in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, S.¬†Muresan, P.¬†Nakov, and A.¬†Villavicencio, Eds.¬†¬†¬†Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 1‚Äì9. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.acl-short.1</span>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
N.¬†Houlsby, A.¬†Giurgiu, S.¬†Jastrzebski, B.¬†Morrone, Q.¬†De¬†Laroussilhe, A.¬†Gesmundo, M.¬†Attariyan, and S.¬†Gelly, ‚ÄúParameter-efficient transfer learning for NLP,‚Äù in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em>, ser. Proceedings of Machine Learning Research, K.¬†Chaudhuri and R.¬†Salakhutdinov, Eds., vol.¬†97.¬†¬†¬†PMLR, 09‚Äì15 Jun 2019, pp. 2790‚Äì2799. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.mlr.press/v97/houlsby19a.html</span>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M.¬†Liu, X.¬†Guo, H.¬†Jiakai, J.¬†Chen, F.¬†Zhou, and S.¬†Hui, ‚ÄúInteMATs: Integrating granularity-specific multilingual adapters for cross-lingual transfer,‚Äù in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, H.¬†Bouamor, J.¬†Pino, and K.¬†Bali, Eds.¬†¬†¬†Singapore: Association for Computational Linguistics, Dec. 2023, pp. 5035‚Äì5049. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2023.findings-emnlp.335</span>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
B.¬†Lester, R.¬†Al-Rfou, and N.¬†Constant, ‚ÄúThe power of scale for parameter-efficient prompt tuning,‚Äù in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, M.-F. Moens, X.¬†Huang, L.¬†Specia, and S.¬†W.-t. Yih, Eds.¬†¬†¬†Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3045‚Äì3059. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2021.emnlp-main.243</span>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
X.¬†Guo, B.¬†Li, and H.¬†Yu, ‚ÄúImproving the sample efficiency of prompt tuning with domain adaptation,‚Äù in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, Y.¬†Goldberg, Z.¬†Kozareva, and Y.¬†Zhang, Eds.¬†¬†¬†Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 3523‚Äì3537. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2022.findings-emnlp.258</span>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
X.¬†L. Li and P.¬†Liang, ‚ÄúPrefix-tuning: Optimizing continuous prompts for generation,‚Äù in <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, C.¬†Zong, F.¬†Xia, W.¬†Li, and R.¬†Navigli, Eds.¬†¬†¬†Online: Association for Computational Linguistics, Aug. 2021, pp. 4582‚Äì4597. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2021.acl-long.353</span>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
E.¬†J. Hu, yelong shen, P.¬†Wallis, Z.¬†Allen-Zhu, Y.¬†Li, S.¬†Wang, L.¬†Wang, and W.¬†Chen, ‚ÄúLoRA: Low-rank adaptation of large language models,‚Äù in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=nZeVKeeFYf9</span>

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
N.¬†Ding, Y.¬†Qin, G.¬†Yang, F.¬†Wei, Z.¬†Yang, Y.¬†Su, S.¬†Hu, Y.¬†Chen, C.-M. Chan, W.¬†Chen, J.¬†Yi, W.¬†Zhao, X.¬†Wang, Z.¬†Liu, H.-T. Zheng, J.¬†Chen, Y.¬†Liu, J.¬†Tang, J.¬†Li, and M.¬†Sun, ‚ÄúDelta tuning: A comprehensive study of parameter efficient methods for pre-trained language models,‚Äù 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
S.¬†Laine and T.¬†Aila, ‚ÄúTemporal ensembling for semi-supervised learning,‚Äù <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02242</em>, 2016.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
R.¬†M√ºller, S.¬†Kornblith, and G.¬†Hinton, ‚ÄúWhen does label smoothing help?‚Äù 2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Z.¬†Du, Y.¬†Li, X.¬†Guo, Y.¬†Sun, and B.¬†Li, ‚ÄúTraining multimedia event extraction with generated images and captions,‚Äù <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.08966</em>, 2023.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
A.¬†M.¬†H. Tiong, J.¬†Li, G.¬†Lin, B.¬†Li, C.¬†Xiong, and S.¬†C.¬†H. Hoi, ‚ÄúImproving tail-class representation with centroid contrastive learning,‚Äù 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
X.¬†Guo, B.¬†Li, H.¬†Yu, and C.¬†Miao, ‚ÄúLatent-optimized adversarial neural transfer for sarcasm detection,‚Äù in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, K.¬†Toutanova, A.¬†Rumshisky, L.¬†Zettlemoyer, D.¬†Hakkani-Tur, I.¬†Beltagy, S.¬†Bethard, R.¬†Cotterell, T.¬†Chakraborty, and Y.¬†Zhou, Eds.¬†¬†¬†Online: Association for Computational Linguistics, Jun. 2021, pp. 5394‚Äì5407. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2021.naacl-main.425</span>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Z.¬†Du, H.¬†Li, X.¬†Guo, and B.¬†Li, ‚ÄúTraining on synthetic data beats real data in multimodal relation extraction,‚Äù 2023.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
H.¬†Huang, O.¬†Zheng, D.¬†Wang, J.¬†Yin, Z.¬†Wang, S.¬†Ding, H.¬†Yin, C.¬†Xu, R.¬†Yang, Q.¬†Zheng <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúChatgpt for shaping the future of dentistry: the potential of multi-modal large language model,‚Äù <em id="bib.bib78.2.2" class="ltx_emph ltx_font_italic">International Journal of Oral Science</em>, vol.¬†15, no.¬†1, p.¬†29, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
K.¬†Packh√§user, L.¬†Folle, F.¬†Thamm, and A.¬†Maier, ‚ÄúGeneration of anonymous chest radiographs using latent diffusion models for training thoracic abnormality classification systems,‚Äù in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì5.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
K.¬†Singhal, S.¬†Azizi, T.¬†Tu, S.¬†S. Mahdavi, J.¬†Wei, H.¬†W. Chung, N.¬†Scales, A.¬†Tanwani, H.¬†Cole-Lewis, S.¬†Pfohl <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúLarge language models encode clinical knowledge,‚Äù <em id="bib.bib80.2.2" class="ltx_emph ltx_font_italic">Nature</em>, vol. 620, no. 7972, pp. 172‚Äì180, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
A.¬†J. Thirunavukarasu, D.¬†S.¬†J. Ting, K.¬†Elangovan, L.¬†Gutierrez, T.¬†F. Tan, and D.¬†S.¬†W. Ting, ‚ÄúLarge language models in medicine,‚Äù <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Nature medicine</em>, vol.¬†29, no.¬†8, pp. 1930‚Äì1940, 2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
R.¬†Rombach, A.¬†Blattmann, D.¬†Lorenz, P.¬†Esser, and B.¬†Ommer, ‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù 2022.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
R.¬†Tang, X.¬†Han, X.¬†Jiang, and X.¬†Hu, ‚ÄúDoes synthetic data generation of llms help clinical text mining?‚Äù <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04360</em>, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
M.¬†Liu, S.¬†Li, H.¬†Yuan, M.¬†E.¬†H. Ong, Y.¬†Ning, F.¬†Xie, S.¬†E. Saffari, Y.¬†Shang, V.¬†Volovici, B.¬†Chakraborty <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúHandling missing values in healthcare data: A systematic review of deep learning-based imputation techniques,‚Äù <em id="bib.bib84.2.2" class="ltx_emph ltx_font_italic">Artificial Intelligence in Medicine</em>, p. 102587, 2023.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
F.¬†Luo, H.¬†Qian, D.¬†Wang, X.¬†Guo, Y.¬†Sun, E.¬†S. Lee, H.¬†H. Teong, R.¬†T.¬†R. Lai, and C.¬†Miao, ‚ÄúMissing value imputation for diabetes prediction,‚Äù in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2022 International Joint Conference on Neural Networks (IJCNN)</em>.¬†¬†¬†IEEE, 2022, pp. 1‚Äì8.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
M.¬†√ñzbey, O.¬†Dalmaz, S.¬†U. Dar, H.¬†A. Bedel, ≈û.¬†√ñzturk, A.¬†G√ºng√∂r, and T.¬†√áukur, ‚ÄúUnsupervised medical image translation with adversarial diffusion models,‚Äù <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>, 2023.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
P.¬†P. Liang, C.¬†Wu, L.-P. Morency, and R.¬†Salakhutdinov, ‚ÄúTowards understanding and mitigating social biases in language models,‚Äù in <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.¬†¬†¬†PMLR, 2021, pp. 6565‚Äì6576.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
H.¬†Kotek, R.¬†Dockum, and D.¬†Sun, ‚ÄúGender bias and stereotypes in large language models,‚Äù in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of The ACM Collective Intelligence Conference</em>, 2023, pp. 12‚Äì24.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z.¬†Ji, N.¬†Lee, R.¬†Frieske, T.¬†Yu, D.¬†Su, Y.¬†Xu, E.¬†Ishii, Y.¬†J. Bang, A.¬†Madotto, and P.¬†Fung, ‚ÄúSurvey of hallucination in natural language generation,‚Äù <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol.¬†55, no.¬†12, pp. 1‚Äì38, 2023.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, Y.¬†Li, L.¬†Cui, D.¬†Cai, L.¬†Liu, T.¬†Fu, X.¬†Huang, E.¬†Zhao, Y.¬†Zhang, Y.¬†Chen <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúSiren‚Äôs song in the ai ocean: A survey on hallucination in large language models,‚Äù <em id="bib.bib90.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.01219</em>, 2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
W.¬†Xu, S.¬†Agrawal, E.¬†Briakou, M.¬†J. Martindale, and M.¬†Carpuat, ‚ÄúUnderstanding and detecting hallucinations in neural machine translation via model introspection,‚Äù <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol.¬†11, pp. 546‚Äì564, 2023.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
M.¬†Fang, M.¬†Huber, and N.¬†Damer, ‚ÄúSynthaspoof: Developing face presentation attack detection based on privacy-friendly synthetic data,‚Äù in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 1061‚Äì1070.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
N.¬†Carlini, F.¬†Tramer, E.¬†Wallace, M.¬†Jagielski, A.¬†Herbert-Voss, K.¬†Lee, A.¬†Roberts, T.¬†Brown, D.¬†Song, U.¬†Erlingsson, A.¬†Oprea, and C.¬†Raffel, ‚ÄúExtracting training data from large language models,‚Äù 2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
R.¬†T. McCoy, P.¬†Smolensky, T.¬†Linzen, J.¬†Gao, and A.¬†Celikyilmaz, ‚ÄúHow much do language models copy from their training data? evaluating linguistic novelty in text generation using raven,‚Äù <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol.¬†11, pp. 652‚Äì670, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.04188" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.04190" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.04190">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04190" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.04191" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 13:35:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
