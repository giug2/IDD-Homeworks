<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2010.15561] Federated Transfer Learning: concept and applications</title><meta property="og:description" content="Development of Artificial Intelligence (AI) is inherently tied to the development of data. However, in most industries data exists in form of isolated islands, with limited scope of sharing between different organizati…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Transfer Learning: concept and applications">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Transfer Learning: concept and applications">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2010.15561">

<!--Generated on Sat Mar  2 09:18:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning; Transfer Learning; Machine Learning; Privacy-preserving.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Transfer Learning: concept and applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sudipan Saha
and Tahir Ahmad
</span><span class="ltx_author_notes">Sudipan Saha is with Technical University of Munich, Taufkirchen, Germany and Tahir Ahmad is with Fondazione Bruno
Kessler, Trento, Italy. E-mail: sudipan.saha@tum.de, ahmad@fbk.eu</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Development of Artificial Intelligence (AI) is inherently tied to the development of data. However, in most industries data exists in form of isolated islands, with limited scope of sharing between different organizations. This is an hindrance to the further development of AI. Federated learning has emerged as a possible solution to this problem in the last few years
without compromising user privacy. Among different variants of the federated learning, noteworthy is federated transfer learning (FTL) that allows knowledge to be transferred across domains that do not have many overlapping features and users. In this work we provide a comprehensive survey of the existing works on this topic. In more details, we study the background of FTL and its different existing applications. We further analyze FTL from privacy and machine learning perspective.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning; Transfer Learning; Machine Learning; Privacy-preserving.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Currently machine learning is playing an important role in many applications, including commodity recommendation, risk analysis, and image analysis. This is due to its excellent capability to extract insights from data. Machine learning and deep learning techniques strongly depend on the availability of data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Abundance of data has led to the overwhelming success of machine learning in image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, remote
sensing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and many other fields.
Traditionally, such machine learning or deep learning models are trained over a centralized corpus of data.
While it is easier to collect image data, they are also more intuitive to label without strong domain knowledge.
Moreover, image data is generally not sensitive and shared without restriction among different parties. However, training data is not easy to obtain in some industries, e.g., finance
and healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Labeling data from such industries require strong professional expertise. Moreover, data in such industries are generally protected by different privacy and security related restrictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Additionally, there exists practical risks of data abuse once the data are
shared to the third parties. In such industries, data exists in the form of isolated island <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Competition between different organizations also hinder the sharing and thus exposing one’s user data to another. Thus,
such industries own insufficient data to train reliable machine learning frameworks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) can be used to overcome the above-mentioned constraints by using data from different organizations to train machine learning model, however not violating the different data related regulations. FL system was first proposed by Google in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Their method was proposed for
mobile devices that enables users to train a centralized model while their data are stored locally.
Thus, federated learning technique can be used prevent the leakage of private information. While centralized learning needs to collect data from users and store them in centralized server, federated learning can learn a global model while the data are distributed on the users’ devices.
Many other works adopted the federated learning framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
This led to emergence of sub-groups within federated learning, e.g., horizontal federated learning and vertical federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A constraint imposed by the traditional federated learning is that training data owned by different organizations need to share same feature space. In practice, this is never the case in industries like finance or healthcare. To mitigate this shortcoming, Federated Transfer Learning (FTL) was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Different participants in FTL can have their specific feature
space, thus making it suitable for practical scenarios. FTL takes inspiration from transfer learning, a paradigm already popular in image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In this setting, machine learning models trained on a large dataset for one problem/domain is applied to a different but related problem/domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The performance of transfer learning is strongly dependent on interrelation between different domains. While talking about federated learning, stakeholders in the same
data federation are usually organizations from the same industry. Thus, it is suitable to apply transfer learning in the federated learning framework.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Federated transfer learning lies at the intersection of two different but fast-evolving fields: machine learning and information privacy. Thus it is an imperative to bridge the gap between them to fully exploit the benefits of federated transfer learning. This motivated us to investigate into the different aspects related to federated transfer learning. It is important to understand how transfer learning and federated learning intermarried to give rise to federated transfer learning. It is critical to understand about how FTL has been applied in different real-life applications. It is important to understand the different privacy and machine learning aspects related to FTL. Keeping them in mind, in this paper, we present a comprehensive survey of the federated transfer learning. Towards this we: 1) outline the definition of FTL, 2) present some case studies on FTL, 3) analyze FTL from privacy aspect, and 4) analyze FTL from machine learning aspects.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We briefly discuss about horizontal and vertical federated learning in Section <a href="#S2" title="II Related work ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Federated transfer learning is defined in
Section <a href="#S3" title="III Federated Transfer Learning ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We detail the case studies on FTL in Section <a href="#S4" title="IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We analyze FTL’s privacy aspects in Section <a href="#S5" title="V Privacy in FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>
and machine learning aspects in Section <a href="#S6" title="VI Machine learning in FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>. Datasets used in the FTL related works are briefly
presented in Section <a href="#S7" title="VII FTL datasets ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>. We conclude the work in Section <a href="#S8" title="VIII Conclusions ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">For the scenario where datasets held by different users differ mostly in samples, federated learning can be categorized into horizontal and vertical federated learning. In this section we briefly review them. We also briefly review transfer learning, considering its relation to the
federated transfer learning.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Horizontal federated learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Horizontal federated learning is a system in which all the parties share the same feature space. However, their userbase may be significantly different. Such parties can collaboratively
learn a model with help of a server. Each party locally computes training gradient and masks them with
some encryption or privacy-preservation technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. All parties send encrypted gradient to the server. The server aggregates them and sends the aggregated result to all parties. Parties update their model with the decrypted aggregated gradient and this way all parties share final model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. An example of horizontal federated learning is a set of banks located in the same city or region and thus sharing same set of features, however very few common users. Horizontal federated learning can be used to learn a common model by agglomerating models learnt in individual banks.
The horizontal federal learning can be implemented with different machine learning algorithms without
changing the main framework.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Vertical federated learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In the vertical federated learning, participating parties do not expose users that do not overlap among the parties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Overlapping users are found by using an encryption-based user ID alignment.
Since different parties have different features corresponding to the common users,
vertical federated learning aggregates different features from different parties and computes
the training loss and gradients in a privacy-preserving manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Subsequently computed gradients are
used to train the model. Vertical federated learning assumes honest participants and there is no hard requirement of a third party, as illustrated here <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. However, sometimes to
secure computations between the participants, an additional party is introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. An example of the vertical federated learning is case of cooperation between the online retailers and the insurers. They own their own feature space (and labels). However, their have significant amount of common users. In such cases, vertical federated learning exploits the situation by merging the features together to create a larger feature space for machine
learning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Transfer learning</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Most machine learning algorithms assume that the training data and the test data have same distribution and they are in the same feature space. However, this assumption does not hold in most real life scenario. In most practical cases, we have intend to analyze one domain of interest, while we have sufficient training data in another domain. As an example, we can consider the problem of classification of a
product review <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Using traditional machine learning approach, sufficient number of product review needs to be collected and annotated for training.
The distribution of review data varies from product to product and hence this training data collection and annotation process needs to
be performed separately for each product. Separate data collection for each product is time-consuming and challenging. Transfer learning emerged
as a framework to address this problem. Transfer learning provides a mechanism of training model on one product and reusing it on another product.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Transfer learning allows the tasks, domains, and distributions in the training and testing to be different. Pan and Yang <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> noted that
research on transfer learning attracted more attention since 1995 and was tackled under different names, e.g., knowledge transfer, inductive
transfer, and multi-task learning. Different approaches were adopted for transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Instance transfer</span> re-weights labeled data in the source domain to reuse it in the target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Parameter transfer</span> discovers shared parameters between the source and the target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Feature-representation transfer</span> finds a feature-representation such that it reduces the difference between source and target domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Relational-knowledge transfer</span> builds mapping of relational knowledge between source and target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</li>
</ol>
<p id="S2.SS3.p2.2" class="ltx_p">In the last decade, deep learning emerged as a very successful paradigm in the machine learning research. However, deep learning is even more
data dependent than the previous machine learning algorithms. To tackle this, researchers started adopting deep transfer learning, to utilize knowledge from other fields by
deep neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. In addition to the four approaches defined above, another approach that gained significant attention in the deep transfer learning is
adversarial-based deep transfer learning that introduces adversarial techniques based on generative adversarial network (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and its variants to find transferable representations applicable to both the source domain and the target domain.
Deep transfer learning is closely related to other topics in the deep learning, e.g., deep domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
One important line of investigation in the deep transfer learning is that which networks are more suitable for transfer and which features are transferable in the deep network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Federated Transfer Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Federated transfer learning is a special case of federated learning and different from both horizontal and vertical federated learning. In federated transfer learning, two datasets differ in the feature space. This applies to datasets collected from enterprises of different but similar nature. Due to the differences in the nature of business, such enterprises share only a small overlap in feature space. This is also applicable to the enterprises set up far in globe. Thus in such scenarios, datasets differ both in samples and in feature space.
Transfer learning techniques aim to build effective model for the target domain while
leveraging knowledge from the other (source) domains. A typical architecture of federated transfer learning is shown in
Figure <a href="#S3.F1" title="Figure 1 ‣ III Federated Transfer Learning ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Considering two parties A and B, where there is only a small overlap in feature space and sample space between A and B, a model learned on B is transferred to A by leveraging small overlapping data and features. We recall that horizontal federated learning is used when there is large overlap in the feature space between datasets and vertical federated learning is used when there is large overlap in user/sample space between datasets. In contrast to them, FTL is used when there is small overlap in both feature space and sample space (shown by dotted box in Figure <a href="#S3.F1" title="Figure 1 ‣ III Federated Transfer Learning ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). FTL ingests a model trained on source domain samples and feature space. Subsequently FTL orients the model for reuse in target space such that model is used for non-overlapping samples leveraging the knowledge acquired from source domain non-overlapping features. Thus FTL covers the region in right upper corner of the Figure <a href="#S3.F1" title="Figure 1 ‣ III Federated Transfer Learning ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> by transferring knowledge from non-overlapping features from source domain to the new samples in the target domain. The ability to use the transferred on non-overlapping data in A
makes FTL different from vertical transfer learning.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2010.15561/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Use Cases of FTL</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Wearable Healthcare</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Wearable devices are fast becoming part of everyday life for patients and healthcare providers.
Multiple features and functionalities of wearable devices include remote patient monitoring, tracking and collecting data, enhancing everyday health and lifestyle patterns, detecting chronic conditions, among others.
Healthcare data, however, are usually fragmented and private making it difficult to generate robust results across populations. A typical architecture of wearable healthcare system is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ IV-A Wearable Healthcare ‣ IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It can be seen that the users use their wearable healthcare devices to measure various health related parameters they are concerned about. The user’s data generated by healthcare devices often exist in the form of isolated islands. For further assessment and analysis of results obtained the collected data is uploaded to the remote cloud based server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2010.15561/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="369" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of the wearable healthcare system</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The architecture presents several critical challenges that hinders the development of effective analytical approaches for the generalizing of healthcare data. The adequate posture of healthcare data has several root causes including, (i) regulatory restrictions—lack of acquisition of massive user data, (ii) security and privacy—restricts sharing of data that exist in the form of isolated islands, and (iii) personalizing issue—the process of training machine learning model lacks personalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">EEG signal classification</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In addition to Section <a href="#S4.SS1" title="IV-A Wearable Healthcare ‣ IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>, another example of usage of federated transfer learning in healthcare domain is the electroencephalographic (EEG) signal classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Brain-Computer Interface (BCI) systems aim to decode participants’ brain states. The success of deep learning based BCI models for classification of EEG recordings is restricted by lack of large EEG datasets. Due to the
privacy concern and high data collection expenses, EEG-BCI
data is present in the form of multiple small datasets owned by different entities across the globe.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Towards this, Ju <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposes a method where the EEG data is represented as the spatial covariance matrix and is subsequently fed to a deep learning based federated transfer learning architecture. It is assumed that the architectures of each user’s deep classifier is same.
Federated averaging method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is adopted to aggregate models from different users. A server-client setting is used where a server acts as the model aggregator. In each round, the updated local models are sent to the server and server sends back the updated global model after aggregation. When a client receives the global model, it updates the model with its local data. This work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> clearly demonstrates that use of domain adaptation in
federated learning architecture boosts EEG classifier performance.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Autonomous Driving</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Autonomous driving normally refers to self-driving vehicles or transport systems that move without the intervention of a human driver. As seen in Figure <a href="#S4.F3" title="Figure 3 ‣ IV-C Autonomous Driving ‣ IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> autonomous driving technology is a complex integration of technologies including sensing, perception, and decision. The cloud platform (vehicular and Internet) provides data storage, simulation, high definition (HD) map generation, and deep learning model training functionalities. Autonomous vehicles are mobile systems, and autonomous driving clouds provide some basic infrastructure supports including distributed computing, distributed storage, and heterogeneous computing  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. On top of this infrastructure, essential services can be implemented in the form of applications to support autonomous vehicles.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2010.15561/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="315" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Layered architecture of autonomous driving</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The dynamic nature of the autonomous driving environment and the uncertainty of real-life scenarios makes autonomous driving as a special use-case for FTL  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Image steganalysis</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Image steganography is the technique of hiding information in the digital image without compromising its visual aesthetics. Image steganalysis is a counter technique to image steganography. It aims to detect the hidden information in
the digital images. Towards this, it extracts and analyzes the steganographic
features generated by image steganographic algorithms. There is a lack of data for training steganalysis methods due to the unwillingness of sharing data among the steganographers. Furthermore, different data owners may
have different preference for steganographic algorithms and cover images. The traditional image
steganalysis algorithms cannot account for this personalized preference.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2010.15561/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Users A, B, and C denote steganographic images owner and they do not share any information (left). For personalized model training, each user individually interacts with the cloud model (right).</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">To overcome these challenges, Yang <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed federated transfer learning framework for image steganalysis (FedSteg), as shown in
Figure <a href="#S4.F4" title="Figure 4 ‣ IV-D Image steganalysis ‣ IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In the proposed framework, different users have their own data (stego and cover images). The users do not leak the data to each other. Instead it is assumed that there is a cloud model and personalized local model for each user.
The
cloud model is trained with the data (cover and stego images) on the
cloud-side. Then the cloud model is distributed to the users. Each user
trains their local model with local data. The trained user models are sent back to the cloud side to help it train a new cloud model. This step only shares encrypted model parameters. In this fashion, each user can keep performing personalized training by consolidating the new cloud model with their previous model and its data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. However, there will be distribution discrepancy between the
cloud and user data. To mitigate the distribution discrepancy, transfer learning is performed to make the model more suitable for user. Yang <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_italic">et. al.</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> uses a CNN based deep transfer learning to train personalized model for each user.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Privacy in FTL</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Machine learning relies on the availability of vast amounts of data for training. However, in real life scenarios (as seen in Section <a href="#S4" title="IV Use Cases of FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>), data are mostly scattered
across different organizations and cannot be easily integrated due to many legal and practical constraints. Federated transfer learning (FTL) helps to improve statistical modeling under a data federation. The data federation as in case of FTL allows knowledge to be shared without compromising user privacy, and enables complimentary knowledge to be transferred in the network. Thus allowing the target domain party to develop a more flexible and powerful model by leveraging rich labels from source domain party  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The broad application of FTL is currently hindered by limited dataset availability for algorithm training and validation, due to the absence of technical and legal approaches to protect user’s privacy. FTL has strict privacy preservation requirements, therefore, to prevent user privacy compromise while promoting scientific research on large datasets, the implementation of technical solutions and development/implementation of legal frameworks to simultaneously address the demands for data protection and utilization is mandatory.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Privacy-preserving FTL typically involves multiple
parties with emphasis on security guarantees to perform machine learning. Here, we present an overview of the necessary privacy-preserving approaches and techniques in context of FTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Privacy by Design</span> FTL designed from the ground up with privacy in mind. The idea is taking into account privacy, throughout the FTL development process. The principles of privacy by design may be applied to all types of sensitive data and the strength of the implemented privacy measure must be dependent on the sensitivity of subject data. For example, processing of only necessary data, storage of data for minimal period, and limited accessibility. Optimal privacy preservation requires implementations that are secure by default and require minimal or no data transfer and provide theoretical and/or technical guarantees of privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Anonymization and pseudonymization</span> The former refers to the removal of personally identifiable information from a dataset (e.g., removing information related to age and gender), whereas, the later refers to replacement of personally identifiable information in a dataset with a synthetic entry with separate storage of the linkage record. For example, in case of health insurance companies wishing to reduce financial risk, re-identification of patient records are a lucrative target. Recently, data mining companies are adopting large-scale re-identification attacks and the sale of re-identified medical records as a business model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Keeping that into account, the use of naive anonymization or pseudonymization alone must therefore be viewed as a technically insufficient measure against identity inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Differential privacy</span> The alteration of a dataset to obfuscate individual data points while retaining the ability of interaction with a data within a certain scope (privacy budget) and of statistical analysis. The approach can also be applied to algorithms. For example, randomization of data to omit relationships between individuals and respective data entries. It provides privacy preservation against membership-inference attack in the model inference stage  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. However, during model training FTL approaches based on differential privacy are vulnerable to privacy leakage among the participants  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Homomorphic encryption</span> A cryptographic technique that preserves the ability to perform mathematical operations on data as if it was unencrypted i.e., plain text. For example, performing neural network computations on encrypted data without the need of first decrypting it. Homomorphic encryption is studied for private federated logistic regression on vertically partitioned data  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> proposed a privacy-preserving linear regression on horizontally partitioned data using homomorphic encryption and Yao’s Garbled Circuits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p"><span id="S5.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Secure multi-party computation</span> The technique is based on splitting data among collaborating entities to perform joint computation but prevents any collaborating entity from gaining knowledge of the data. For example, identifying the common patients among two hospitals without disclosing the respective hospital patient’s list. Earlier works  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> mostly focus on approaches based on multi-party computation. SecureML  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, a privacy-preserving protocol combining secret-sharing  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and Yao’s Garbled Circuit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, is considered as the state-of-the-art protocol for linear regression, logistic regression, and neural networks. SecureNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> is also proposed using a multi-party protocol for efficient neural network training.</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i6.p1" class="ltx_para">
<p id="S5.I1.i6.p1.1" class="ltx_p"><span id="S5.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Hardware security implementations</span> The approach to assure data and algorithm privacy by utilizing specialized hardware. For example, in the form of secure processors or enclaves implemented in mobile device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Due to the rising significance of hardware-level deep learning implementations, e.g., tensor processing units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>). It is likely that such system-based privacy guarantees built into edge hardware will become more common, e.g., trusted execution environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Machine learning in FTL</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Most works on FTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> have adopted variants of deep learning as the architecture for FTL.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the deep model is formed using a series of 1D convolution layers and fully connected layers. The model also consists of other auxiliary layers like pooling. Softmax layer is used for classification. A simplified schema of the deep model is shown in Figure <a href="#S6.F5" title="Figure 5 ‣ VI Machine learning in FTL ‣ Federated Transfer Learning: concept and applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. During the model from user to server, the convolution layers are kept frozen and the fully connected layers are updated to learn user and task-specific parameters.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><svg id="S6.F5.pic1" class="ltx_picture ltx_centering" height="163.72" overflow="visible" version="1.1" width="420.03"><g transform="translate(0,163.72) matrix(1 0 0 -1 0 0) translate(76.81,0) translate(0,140.16)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M -55.09 -19.39 L -40.97 19.39 C -40.19 21.55 -37.69 23.29 -35.4 23.29 L 52.35 23.29 C 54.65 23.29 55.87 21.55 55.09 19.39 L 40.97 -19.39 C 40.19 -21.55 37.69 -23.29 35.4 -23.29 L -52.35 -23.29 C -54.65 -23.29 -55.87 -21.55 -55.09 -19.39 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -34.94 -18.68)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 30.44)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.9)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Data (user)</text></g></g></g></g><path d="M 115.56 -19.39 L 129.68 19.39 C 130.46 21.55 132.96 23.29 135.25 23.29 L 236.03 23.29 C 238.33 23.29 239.55 21.55 238.77 19.39 L 224.65 -19.39 C 223.87 -21.55 221.37 -23.29 219.08 -23.29 L 118.3 -23.29 C 116 -23.29 114.78 -21.55 115.56 -19.39 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 135.71 -18.68)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 30.44)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.9)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Data (Server)</text></g></g></g></g><path d="M 46.74 -17.04 L -46.74 -17.04 C -49.03 -17.04 -50.89 -18.9 -50.89 -21.19 L -50.89 -57.94 C -50.89 -60.23 -49.03 -62.09 -46.74 -62.09 L 46.74 -62.09 C 49.03 -62.09 50.89 -60.23 50.89 -57.94 L 50.89 -21.19 C 50.89 -18.9 49.03 -17.04 46.74 -17.04 Z M -50.89 -62.09" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -46.28 -57.48)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 29.67)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">1D Conv layers</text></g></g></g></g><path d="M 249.87 -16.28 L 104.46 -16.28 C 102.17 -16.28 100.31 -18.13 100.31 -20.43 L 100.31 -58.71 C 100.31 -61 102.17 -62.86 104.46 -62.86 L 249.87 -62.86 C 252.16 -62.86 254.02 -61 254.02 -58.71 L 254.02 -20.43 C 254.02 -18.13 252.16 -16.28 249.87 -16.28 Z M 100.31 -62.86" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 104.92 -58.25)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 30.44)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.9)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">1D Conv layers (frozen)</text></g></g></g></g><path d="M 67.49 -56.61 L -67.49 -56.61 C -69.79 -56.61 -71.64 -58.47 -71.64 -60.76 L -71.64 -97.51 C -71.64 -99.8 -69.79 -101.66 -67.49 -101.66 L 67.49 -101.66 C 69.79 -101.66 71.64 -99.8 71.64 -97.51 L 71.64 -60.76 C 71.64 -58.47 69.79 -56.61 67.49 -56.61 Z M -71.64 -101.66" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -67.03 -97.05)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 29.67)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Fully connected layers</text></g></g></g></g><path d="M 244.66 -56.61 L 109.67 -56.61 C 107.38 -56.61 105.52 -58.47 105.52 -60.76 L 105.52 -97.51 C 105.52 -99.8 107.38 -101.66 109.67 -101.66 L 244.66 -101.66 C 246.95 -101.66 248.81 -99.8 248.81 -97.51 L 248.81 -60.76 C 248.81 -58.47 246.95 -56.61 244.66 -56.61 Z M 105.52 -101.66" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 110.13 -97.05)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 29.67)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Fully connected layers</text></g></g></g></g><path d="M 31.71 -97.52 L -31.71 -97.52 C -34 -97.52 -35.86 -99.38 -35.86 -101.67 L -35.86 -135.73 C -35.86 -138.02 -34 -139.88 -31.71 -139.88 L 31.71 -139.88 C 34 -139.88 35.86 -138.02 35.86 -135.73 L 35.86 -101.67 C 35.86 -99.38 34 -97.52 31.71 -97.52 Z M -35.86 -139.88" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -31.25 -135.27)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 28.325)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Prediction</text></g></g></g></g><path d="M 208.87 -97.52 L 145.46 -97.52 C 143.16 -97.52 141.3 -99.38 141.3 -101.67 L 141.3 -135.73 C 141.3 -138.02 143.16 -139.88 145.46 -139.88 L 208.87 -139.88 C 211.17 -139.88 213.03 -138.02 213.03 -135.73 L 213.03 -101.67 C 213.03 -99.38 211.17 -97.52 208.87 -97.52 Z M 141.3 -139.88" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 145.92 -135.27)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 28.325)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Prediction</text></g></g></g></g><g stroke-width="0.8pt"><path d="M 0 -23.57 L 0 -20.36" style="fill:none"></path><g transform="matrix(0.00002 1.0 -1.0 0.00002 0 -20.36)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 177.16 -23.57 L 177.17 -19.6" style="fill:none"></path><g transform="matrix(0.00008 1.0 -1.0 0.00008 177.17 -19.6)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 0 -62.37 L 0 -59.93" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 -59.93)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 177.17 -63.14 L 177.17 -59.93" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 177.17 -59.93)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 0 -101.93 L 0 -100.84" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 -100.84)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 177.17 -101.93 L 177.17 -100.84" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 177.17 -100.84)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g transform="matrix(1.0 0.0 0.0 1.0 275.57 -82.59)" fill="#000000" stroke="#000000"><foreignObject width="63.04" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.1.1.1.1.1" class="ltx_text">Alignment</span></foreignObject></g><g stroke="#0000FF" fill="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M -76.53 -106.55 h 330.23 v 54.82 h -330.23 Z" style="fill:none"></path></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The deep learning model for FTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></figcaption>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">A similar framework as above is used in FedSteg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The model consists of 9 convolution layers followed by fully connected layer. While transferring model from user to cloud/server, the convolution layers are kept frozen, while the fully connected layer is used for model transfer. For feature alignment between source and target, correlation alignment (CORAL) loss is used, which adapts the second order feature statistics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> uses both classification loss and domain loss that is implemented using maximum-mean discrepancy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In federated autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, reinforcement learning is used. Reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, different from other machine learning techniques, can be considered as a collection of observation and action spaces. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, each RL agent (user) is trained individually. Following this, knowledge of the distributed RL agents is distributed. Following that an online transfer process is performed to make alignments on the observations and actions.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">In FTL, training with heterogeneous data may present additional challenge, e.g.,
not all client data distributions may be adequately captured by the model. Furthermore, quality of a particular local data partition may be significantly different from the rest. To overcome these challenges, Dimitriadis <span id="S6.p5.1.1" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> presented a dynamic gradient aggregation (DGA) method which weights the local gradients during aggregation step.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Inspite of success of the methods discussed above, most of them do not provide an in-depth analysis of how different feature spaces can be handled in different users. Moreover, most of the above works are restricted to limited number of users. They are based on simplistic assumptions that convolution layers learn shared feature while fully connected layers learn domain-specific/ task-specific features. While correct, such assumptions do not fully exploit the tremendous progress made by deep domain adaptation community
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. Furthermore, there are very few works that investigates the complicated machine learning issues that can arise in the heterogeneous FTL setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">FTL datasets</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Most works on FTL have adopted existing machine learning datasets and modified them as per the requirement of FTL.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Fedhealth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> used human activity recognition dataset - UCI Smartphone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, consisting of 6 activities collected from 30 users within an age bracket of 19-48 years.
To adapt the dataset for FedHealth,
5 subjects were regarded as isolated
users (who cannot share data) and the remaining were used to train cloud model.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Ju <span id="S7.p3.1.1" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> used PhysioNet
EEG Motor Imagery (MI) Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. The MI dataset is recorded from 109 subjects. Their experiments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> use 5-fold cross-validation settings with 4 folds being
used for training. Fedsteg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> used two different image datasets. Gao <span id="S7.p3.1.2" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> conducted experiments from several
public datasets from UCI repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Differently from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>,
Liang <span id="S7.p4.1.1" class="ltx_text ltx_font_italic">et. al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> conducted real-life experiments on RC cars and Airsim.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Data isolation and data privacy concerns pose significant challenge for applying artificial intelligence techniques in real life applications.
Moreover, features and users generally vary from organization to
organization.
In the last few years, federated learning, especially FTL has emerged as
a potential solution to overcome the above challenges. In this work, we analyzed the concept of FTL and its applications in several practical domains. Furthermore, we did a detailed review of the machine learning techniques used in FTL. Our analysis shows that while FTL has explored different machine learning techniques, there is still potential of using more sophisticated machine learning techniques along with FTL. Adversarial techniques may provide a promising direction for further strengthen FTL. While FTL has been used in some practical applications, number of such applications are still few. Moreover, most of the datasets used in the FTL works are basic machine learning datasets and not tailored to real life scenarios. FTL holds the promise to break the barrier between different enterprises. However, there is scope of further advancement, by incorporating advanced machine learning techniques in FTL and applying FTL to more practical applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Deep
learning</em>.   MIT press Cambridge, 2016,
vol. 1.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Saha, F. Bovolo, and L. Bruzzone, “Unsupervised multiple-change detection
in vhr multisensor images via deep-learning based adaptation,” in
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing
Symposium</em>.   IEEE, 2019, pp.
5033–5036.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Saha, F. Bovolo, and L. Bruzzone, “Building change detection in vhr sar
images via unsupervised deep transcoding,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Geoscience and Remote Sensing</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Jing, W. Wang, J. Zhang, H. Tian, and K. Chen, “Quantifying the performance
of federated transfer learning,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.12795</em>,
2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P. Voigt and A. Von dem Bussche, “The eu general data protection regulation
(gdpr),” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</em>, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of
deep networks using model averaging,” 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik,
“Federated optimization: Distributed machine learning for on-device
intelligence,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Zhu and Y. Jin, “Multi-objective evolutionary federated learning,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, vol. 31,
no. 4, pp. 1310–1322, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Kang, C. Xing, T. Chen, and Q. Yang, “A secure federated transfer
learning framework,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Saha, Y. T. Solano-Correa, F. Bovolo, and L. Bruzzone, “Unsupervised deep
transfer learning-based change detection for hr multispectral images,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Geoscience and Remote Sensing Letters</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Saha and N. Sheikh, “Ultrasound image classification using acgan with small
training dataset,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.01539</em>, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Saha, F. Bovolo, and L. Bruzzone, “Unsupervised deep change vector analysis
for multiple-change detection in vhr images,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Geoscience and Remote Sensing</em>, vol. 57, no. 6, pp. 3677–3693, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Aono, T. Hayashi, L. Wang, S. Moriai <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Privacy-preserving
deep learning via additively homomorphic encryption,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">IEEE
Transactions on Information Forensics and Security</em>, vol. 13, no. 5, pp.
1333–1345, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Hardy, W. Henecka, H. Ivey-Law, R. Nock, G. Patrini, G. Smith, and
B. Thorne, “Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1711.10677</em>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Yang, B. Ren, X. Zhou, and L. Liu, “Parallel distributed logistic
regression for vertical federated learning without third-party coordinator,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.09824</em>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
G. Wang, “Interpret federated learning with shapley values,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1905.04519</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Jiang and C. Zhai, “Instance weighting for domain adaptation in nlp,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th annual meeting of the association of
computational linguistics</em>, 2007, pp. 264–271.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Gao, W. Fan, J. Jiang, and J. Han, “Knowledge transfer via multiple model
local structure mapping,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and data mining</em>, 2008, pp.
283–291.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Argyriou, M. Pontil, Y. Ying, and C. A. Micchelli, “A spectral
regularization framework for multi-task structure learning,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2008, pp. 25–32.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
L. Mihalkova, T. Huynh, and R. J. Mooney, “Mapping and revising markov logic
networks for transfer learning,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Aaai</em>, vol. 7, 2007, pp. 608–614.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep
transfer learning,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International conference on artificial neural
networks</em>.   Springer, 2018, pp.
270–279.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” in
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2014, pp.
2672–2680.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
features in deep neural networks?” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, 2014, pp. 3320–3328.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
F. Sun, W. Zang, R. Gravina, G. Fortino, and Y. Li, “Gait-based identification
for elderly users in wearable healthcare systems,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Information
Fusion</em>, vol. 53, pp. 134–144, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated transfer
learning framework for wearable healthcare,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent
Systems</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Ju, D. Gao, R. Mane, B. Tan, Y. Liu, and C. Guan, “Federated transfer
learning for eeg signal classification,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2004.12321</em>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Liu, J. Tang, C. Wang, Q. Wang, and J.-L. Gaudiot, “Implementing a cloud
platform for autonomous driving,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.02696</em>,
2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X. Liang, Y. Liu, T. Chen, M. Liu, and Q. Yang, “Federated transfer
reinforcement learning for autonomous driving,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1910.06001</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
H. Yang, H. He, W. Zhang, and X. Cao, “Fedsteg: A federated transfer learning
framework for secure image steganalysis,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network
Science and Engineering</em>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
G. A. Kaissis, M. R. Makowski, D. Rückert, and R. F. Braren, “Secure,
privacy-preserving and federated machine learning in medical imaging,”
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, pp. 1–7, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. Gao, Y. Liu, A. Huang, C. Ju, H. Yu, and Q. Yang, “Privacy-preserving
heterogeneous federated transfer learning,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International
Conference on Big Data (Big Data)</em>.   IEEE, 2019, pp. 2552–2559.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Cavoukian and M. Prosch, “Privacy by redesign: Building a better legacy,”
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Information Privacy Commissioner Ontario</em>, pp. 1–8, 2011.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Tanner, <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Our bodies, our data: how companies make billions selling our
medical records</em>.   Beacon Press, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Theory of cryptography
conference</em>.   Springer, 2006, pp.
265–284.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Y. Wang, Q. Gu, and D. Brown, “Differentially private hypothesis transfer
learning,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and
Knowledge Discovery in Databases</em>.   Springer, 2018, pp. 811–826.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Q. Yao, X. Guo, J. T. Kwok, W. Tu, Y. Chen, W. Dai, and Q. Yang, “Differential
private stack generalization with an application to diabetes prediction,”
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.09491</em>, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft,
“Privacy-preserving ridge regression on hundreds of millions of records,”
in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2013 IEEE Symposium on Security and Privacy</em>.   IEEE, 2013, pp. 334–348.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S. Yakoubov, “A gentle introduction to yao’s garbled circuits,” 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. Kantarcioglu and C. Clifton, “Privacy-preserving distributed mining of
association rules on horizontally partitioned data,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE transactions
on knowledge and data engineering</em>, vol. 16, no. 9, pp. 1026–1037, 2004.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
L. Wan, W. K. Ng, S. Han, and V. C. Lee, “Privacy-preservation for gradient
descent methods,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th ACM SIGKDD international
conference on Knowledge discovery and data mining</em>, 2007, pp. 775–783.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. C. Yao, “Protocols for secure computations,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">23rd annual
symposium on foundations of computer science (sfcs 1982)</em>.   IEEE, 1982, pp. 160–164.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-preserving
machine learning,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy
(SP)</em>.   IEEE, 2017, pp. 19–38.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. Shamir, “How to share a secret,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>,
vol. 22, no. 11, pp. 612–613, 1979.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Wagh, D. Gupta, and N. Chandran, “Securenn: Efficient and private neural
network training.” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">IACR Cryptol. ePrint Arch.</em>, vol. 2018, p. 442,
2018.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A. P. Security, “Secure enclave overview,” https://support.apple.com/
guide/security/secure-enclave-overview-sec59b0b31f/web, 2020, accessed:
24-Sep-2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Google, “Cloud TPU,” https://cloud.google.com/tpu/, 2020, accessed:
24-Sep-2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. J. Smola, “A
kernel method for the two-sample-problem,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, 2007, pp. 513–520.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Y. Li, “Deep reinforcement learning: An overview,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1701.07274</em>, 2017.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
D. Dimitriadis, K. Kumatani, R. Gmyr, Y. Gaur, and S. E. Eskimez, “Federated
transfer learning with dynamic gradient aggregation,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2008.02452</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
G. Wilson and D. J. Cook, “A survey of unsupervised deep domain adaptation,”
<em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
vol. 11, no. 5, pp. 1–46, 2020.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “Human
activity recognition on smartphones using a multiclass hardware-friendly
support vector machine,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">International workshop on ambient assisted
living</em>.   Springer, 2012, pp. 216–223.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw,
“Bci2000: a general-purpose brain-computer interface (bci) system,”
<em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on biomedical engineering</em>, vol. 51, no. 6, pp.
1034–1043, 2004.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A. Asuncion and D. Newman, “Uci machine learning repository,” 2007.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2010.15560" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2010.15561" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2010.15561">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2010.15561" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2010.15562" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 09:18:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
