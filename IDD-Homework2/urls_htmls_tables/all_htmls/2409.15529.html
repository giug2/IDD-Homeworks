<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.15529] VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion</title><meta property="og:description" content="Vehicle object detection is possible using both LiDAR and camera data. Methods using LiDAR generally outperform those using cameras only. The highest accuracy methods utilize both of these modalities through data fusio…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.15529">

<!--Generated on Sat Oct  5 23:20:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vanshika Vats<sup id="id4.4.id1" class="ltx_sup"><span id="id4.4.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>, Marzia Binta Nizam<sup id="id5.5.id2" class="ltx_sup"><span id="id5.5.id2.1" class="ltx_text ltx_font_italic">∗</span></sup> and James Davis
</span><span class="ltx_author_notes"><sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">∗</span></sup> Authors contributed equallyVanshika Vats, Marzia Binta Nizam, and James Davis are with the Department of Computer Science and Engineering, University of California Santa Cruz,
Santa Cruz, CA 95064, USA
<span id="id7.7.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(vvats, manizam, davisje)@ucsc.edu</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Vehicle object detection is possible using both LiDAR and camera data. Methods using LiDAR generally outperform those using cameras only. The highest accuracy methods utilize both of these modalities through data fusion. In our study, we propose a model-independent late fusion method, VaLID, which validates whether each predicted bounding box is acceptable or not. Our method verifies the higher-performing, yet overly optimistic LiDAR model detections using camera detections that are obtained from either specially trained, general, or open-vocabulary models. VaLID uses a simple multi-layer perceptron trained with a high recall bias to reduce the false predictions made by the LiDAR detector, while still preserving the true ones. Evaluating with multiple combinations of LiDAR and camera detectors on the KITTI dataset, we reduce false positives by an average of 63.9%, thus outperforming the individual detectors on 2D average precision (2DAP). Our approach is model-agnostic and demonstrates state-of-the-art competitive performance even when using generic camera detectors that were not trained specifically for this dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The development of autonomous vehicles relies heavily on accurate object detection to safely navigate their environment. These systems use multiple sensors, such as cameras and LiDAR, each offering unique strengths and weaknesses. Cameras provide detailed visual information, while LiDAR offers precise depth information crucial for 3D localization. While systems using only cameras or LiDAR data provide valuable information, single-sensor approaches often fail. Cameras struggle in low-light conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and LiDAR lacks rich high resolution visual data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Multi-modal fusion is often used to ensure robust and reliable detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
These methods combine LiDAR’s depth data with camera visuals to create a more reliable detection system. However, the integration of these modalities often requires advanced fusion techniques with methods designed to match the specific characteristics of the underlying data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The primary challenge is ensuring that the strengths of each modality complement one another to improve overall detection accuracy in varied and dynamic driving conditions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Sensor fusion in autonomous vehicles can be categorized into early, deep, and late fusion techniques. Early fusion integrates raw data from multiple sensors early in the processing pipeline, allowing for detailed cross-sensor interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. However, this approach can lead to increased computational costs due to the need for complex preprocessing steps, like semantic segmentation or depth completion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Deep fusion aligns features at a higher abstraction level, balancing information from both sensors but adding complexity to the model design<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Late fusion, in contrast, merges detection outputs at the bounding box level after independent processing, offering more flexibility and computational efficiency but with reduced feature interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Early and deep fusion methods leverage cross-modality information in their fundamental representations, potentially leading to higher accuracy at the cost of complexity. Late fusion methods, on the other hand, are simpler to integrate into complex industrial workflows, since they do not require a redesign when advances are made in underlying detection technologies. Pre-trained single-modality detectors can be used and replaced without modification, requiring only detection-level outputs at the time of fusion. An ideal fusion method would allow the flexibility of late-fusion while obtaining the performance of early and deep fusion.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We introduce a new late fusion method called VaLID: Verification as Late Integration of Detections. This method takes raw detections from a LiDAR model and cross-validates them against corresponding camera detections using a simple multi-layer perceptron neural network. Ensuring that only verified LiDAR boxes are kept significantly reduces false positives, improving overall performance.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate our method on the KITTI dataset with two LiDAR detectors, PV-RCNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and TED-S<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. To ensure generality, we use three distinct camera models: MonoDETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> is a special purpose detector trained specifically on KITTI, YOLO-NAS is a general-purpose model from the YOLO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> family, and Grounding DINO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is an open-vocabulary model.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Detection methods trained for a specific dataset normally outperform general purpose models. Our two LiDAR methods were selected as state-of-the-art methods designed and tested on the KITTI dataset. For camera detectors we included general purpose detectors in our evaluation. These have lower performance than a specialized model when considered as a primary detector, however our method uses camera models for the easier task of verification.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We find that the VaLID method removes an average 63.9% of false positives, consistently improving overall detection average precision.
Importantly, improvement is as effective using the general camera models as it is with the specifically trained method. In both cases results are competitive with state of the art.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The contribution of this paper is a model-independent late fusion method that combines LiDAR and camera detections.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.15529/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We conduct our study on the KITTI dataset. The figure rows show (a) official 2D ground truth (b) detections from the specialized LiDAR model PVRCNN, (c) specialized camera model MonoDETR, and (d) open Vocabulary model GroundingDino. LiDAR generally produces too many false positives, seen most obviously in Column [i]. Camera models can help verify and reject false positives despite imperfections in their own detections.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Detection Using Camera Images</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Cameras images are fundamentally a 2D data source, and 2D object detection is a well-studied area with many surveys and general purpose datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Higher accuracy is possible in specific domains such as vehicle detection by designing specialized object detection methods which are trained on vehicle specific datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Since autonomous driving requires 3D awareness, there have also been attempts to detect 3D bounding boxes from camera images. However, accurately estimating depth without dedicated sensors is challenging, especially for distant objects. Early approaches like Deep3DBox leverage 2D bounding boxes and geometric constraints to infer 3D dimensions but struggle with long-range accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Similarly, MonoDETR improves detection by incorporating depth embeddings into a transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Other methods generate depth maps to assist with detection. For instance, MonoPGC refines pixel-wise depth estimation through cross-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, and Pseudo-LiDAR simulates LiDAR input from depth maps to enhance detection performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. While these techniques improve depth accuracy, they often introduce significant computational overhead, especially for complex or occluded scenes. Multi-task approaches like Deep MANTA predict both 2D and 3D attributes simultaneously, leveraging shared features to enhance efficiency, but they still face challenges in reliable depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Unfortunately, camera-only methods, while effective in some scenarios, are limited by their inability to produce reliable depth estimation without additional sensors, and 3D bounding box accuracy remains below 2D bounding box accuracy. In our work, we use the higher accuracy 2D detections as one of the inputs to our late fusion method, which integrates data from multiple sensors.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Detection Using LiDAR Point Clouds</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">LiDAR-based 3D detection has become a fundamental component in autonomous driving due to its ability to capture detailed spatial information. One-stage models, like PointNet and PointNet++, were early efforts that applied neural networks directly to raw point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. These methods prioritized real-time processing but often struggled with long-range accuracy. Other approaches, like those using range images, attempt to enhance detection by encoding depth information at the pixel level, yet they still face limitations in handling complex, distant scenes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
To address these challenges, two-stage models were developed to refine detection for better accuracy. VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> introduced the use of 3D voxels and sparse convolutions, enhancing both feature extraction and processing efficiency. Building on this, PV-RCNN combines voxel-based CNNs with PointNet abstraction to generate high-quality 3D proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, while PointRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> refines 3D proposals in a two-stage process. Part-A2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> goes a step further by predicting part locations within objects to refine proposals, improving 3D bounding box accuracy. LiDAR-only methods offer robust spatial data but often struggle with complex environments and distant objects. In our late fusion approach, LiDAR detections are combined with camera detections to provide more accurate and reliable 3D object detection.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Detection Using LiDAR-Camera Fusion</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Multi-modal fusion techniques for 3D detection are increasingly common, particularly in autonomous driving, where camera and LiDAR sensors complement each other. These techniques are categorized into three main approaches: early, deep, and late fusion.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.15529/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="531" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Precision-Recall curve (PRC) of our baseline single modality models on the KITTI moderate difficulty data set. Notice that both LiDAR models outperform all three of the camera models, and that the specialized camera detector outperforms the two general purpose camera-based object detectors.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Early fusion methods integrate camera data with LiDAR point clouds early in the perception pipeline to enhance object detection. Techniques like PointPainting and MVP enrich LiDAR data by projecting 2D camera segmentation or generating virtual points to address LiDAR sparsity in distant regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. However, image-based segmentation can blur object boundaries, a problem mitigated by FusionPainting using 3D segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Techniques like VirConv create virtual points and apply Stochastic Voxel Discard to filter noise, though this adds computational overhead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Deep fusion methods operate at a higher level of abstraction, aligning features from both modalities. Models like PointAugmenting and DVF bridge the 2D-3D gap by projecting point clouds onto image feature maps or reweighting voxel features based on pixel confidence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Advanced models such as EPNet, EPNet++, and DeepFusion use attention mechanisms to dynamically adjust feature weights, reducing inconsistencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. SFD addresses information loss by fusing features directly in the 3D Region of Interest (RoI), while BEVFusion unifies features into a Bird’s Eye View (BEV) representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Late fusion integrates outputs from each sensor after independent processing. For example, CLOCs and Fast-CLOCs refine detection confidence by ensuring that 2D and 3D bounding boxes overlap accurately, but these methods primarily focus on raw data from each sensor<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. They use Multi-Layer Perceptrons (MLP) to process detection candidates, but insufficient feature alignment can result in suboptimal training performance. C<span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">-</span>CLOCs addresses this limitation by incorporating contrastive learning and aligning features from both 2D and 3D sensors to improve detection accuracy, particularly in challenging environments<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Çaldıran and Acarman take a different approach, integrating 2D segmentation with 3D LiDAR to reduce false positives, such as roadblocks and tunnel walls, improving detection accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. All of these late-fusion methods for vehicle detection have focused on combining specialized LiDAR and camera vehicle detectors.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Our work proposes a new late fusion method that is competitive with the state of the art. We demonstrate results with both a specialized camera detector trained on the KITTI dataset and with more general vision models that lack specific knowledge of the dataset.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.15529/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="266" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The distribution of false positives and true positives across confidence score bands for one LiDAR and two camera object detection models on the KITTI moderate set. Notice that the LiDAR method has false positives across all confidence bands, and that the camera methods have different and sometimes complementary distributions.</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2409.15529/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="249" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>In our method called VaLID, we take LiDAR (L) boxes as the primary detections and use the camera (C) modality to verify whether each L box is an acceptable detection or not. The input vector is given as the bounding box dimensions of width <math id="S2.F4.6.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.F4.6.m1.1b"><mi id="S2.F4.6.m1.1.1" xref="S2.F4.6.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.F4.6.m1.1c"><ci id="S2.F4.6.m1.1.1.cmml" xref="S2.F4.6.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.6.m1.1d">w</annotation></semantics></math>, height <math id="S2.F4.7.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.F4.7.m2.1b"><mi id="S2.F4.7.m2.1.1" xref="S2.F4.7.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.F4.7.m2.1c"><ci id="S2.F4.7.m2.1.1.cmml" xref="S2.F4.7.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.7.m2.1d">h</annotation></semantics></math>, and center <math id="S2.F4.8.m3.2" class="ltx_Math" alttext="(cx,cy)" display="inline"><semantics id="S2.F4.8.m3.2b"><mrow id="S2.F4.8.m3.2.2.2" xref="S2.F4.8.m3.2.2.3.cmml"><mo stretchy="false" id="S2.F4.8.m3.2.2.2.3" xref="S2.F4.8.m3.2.2.3.cmml">(</mo><mrow id="S2.F4.8.m3.1.1.1.1" xref="S2.F4.8.m3.1.1.1.1.cmml"><mi id="S2.F4.8.m3.1.1.1.1.2" xref="S2.F4.8.m3.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.F4.8.m3.1.1.1.1.1" xref="S2.F4.8.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.F4.8.m3.1.1.1.1.3" xref="S2.F4.8.m3.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.F4.8.m3.2.2.2.4" xref="S2.F4.8.m3.2.2.3.cmml">,</mo><mrow id="S2.F4.8.m3.2.2.2.2" xref="S2.F4.8.m3.2.2.2.2.cmml"><mi id="S2.F4.8.m3.2.2.2.2.2" xref="S2.F4.8.m3.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.F4.8.m3.2.2.2.2.1" xref="S2.F4.8.m3.2.2.2.2.1.cmml">​</mo><mi id="S2.F4.8.m3.2.2.2.2.3" xref="S2.F4.8.m3.2.2.2.2.3.cmml">y</mi></mrow><mo stretchy="false" id="S2.F4.8.m3.2.2.2.5" xref="S2.F4.8.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F4.8.m3.2c"><interval closure="open" id="S2.F4.8.m3.2.2.3.cmml" xref="S2.F4.8.m3.2.2.2"><apply id="S2.F4.8.m3.1.1.1.1.cmml" xref="S2.F4.8.m3.1.1.1.1"><times id="S2.F4.8.m3.1.1.1.1.1.cmml" xref="S2.F4.8.m3.1.1.1.1.1"></times><ci id="S2.F4.8.m3.1.1.1.1.2.cmml" xref="S2.F4.8.m3.1.1.1.1.2">𝑐</ci><ci id="S2.F4.8.m3.1.1.1.1.3.cmml" xref="S2.F4.8.m3.1.1.1.1.3">𝑥</ci></apply><apply id="S2.F4.8.m3.2.2.2.2.cmml" xref="S2.F4.8.m3.2.2.2.2"><times id="S2.F4.8.m3.2.2.2.2.1.cmml" xref="S2.F4.8.m3.2.2.2.2.1"></times><ci id="S2.F4.8.m3.2.2.2.2.2.cmml" xref="S2.F4.8.m3.2.2.2.2.2">𝑐</ci><ci id="S2.F4.8.m3.2.2.2.2.3.cmml" xref="S2.F4.8.m3.2.2.2.2.3">𝑦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.8.m3.2d">(cx,cy)</annotation></semantics></math>, confidence scores <math id="S2.F4.9.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.F4.9.m4.1b"><mi id="S2.F4.9.m4.1.1" xref="S2.F4.9.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.F4.9.m4.1c"><ci id="S2.F4.9.m4.1.1.cmml" xref="S2.F4.9.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.9.m4.1d">s</annotation></semantics></math>, and the measure of overlap <math id="S2.F4.10.m5.1" class="ltx_Math" alttext="IoU" display="inline"><semantics id="S2.F4.10.m5.1b"><mrow id="S2.F4.10.m5.1.1" xref="S2.F4.10.m5.1.1.cmml"><mi id="S2.F4.10.m5.1.1.2" xref="S2.F4.10.m5.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.F4.10.m5.1.1.1" xref="S2.F4.10.m5.1.1.1.cmml">​</mo><mi id="S2.F4.10.m5.1.1.3" xref="S2.F4.10.m5.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.F4.10.m5.1.1.1b" xref="S2.F4.10.m5.1.1.1.cmml">​</mo><mi id="S2.F4.10.m5.1.1.4" xref="S2.F4.10.m5.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F4.10.m5.1c"><apply id="S2.F4.10.m5.1.1.cmml" xref="S2.F4.10.m5.1.1"><times id="S2.F4.10.m5.1.1.1.cmml" xref="S2.F4.10.m5.1.1.1"></times><ci id="S2.F4.10.m5.1.1.2.cmml" xref="S2.F4.10.m5.1.1.2">𝐼</ci><ci id="S2.F4.10.m5.1.1.3.cmml" xref="S2.F4.10.m5.1.1.3">𝑜</ci><ci id="S2.F4.10.m5.1.1.4.cmml" xref="S2.F4.10.m5.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.10.m5.1d">IoU</annotation></semantics></math>. We pass this vector through an MLP and output an accept or reject signal as a sigmoid value. The target loss is calculated considering the overlap of the ground truth with the given L box.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">LiDAR and Camera Models for Late-Fusion</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Many urban driving object detection methods utilize LiDAR and camera modalities separately, as well as in fusion, for enhanced performance. Most methods, however, use specialized training and fine-tuning tailored to specific datasets, which is both time-consuming and computationally demanding. To reduce this dependency, our study leverages general and open-vocabulary camera models without requiring dataset-specific fine-tuning. We perform our study on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, using PVRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and TED-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> models as the primary LiDAR models. For fusion, we use MonoDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> as a specialized KITTI model, and YOLO-NAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as a general camera detector that is not trained on KITTI. GroundingDINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is used as an open-vocabulary model that can detect objects that are not even seen during the training. Although not as precise as fine-tuned models, these general models offer cross-domain versatility, providing the flexibility needed for a more robust late-fusion approach.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Model Detections Complement Each Other</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">LiDAR and camera models have their own strengths and weaknesses. Fig. <a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates how these models, while missing certain detections, often compensate for one another’s limitations. For instance, PVRCNN can identify numerous objects but tends to generate a lot of false positives. Meanwhile, MonoDETR and GroundingDINO may miss some detections covered by PVRCNN, but they also detect objects that PVRCNN overlooks. GroundingDINO, despite producing imperfect bounding boxes and some false positives, detects objects that specialized models may miss.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ II-C Detection Using LiDAR-Camera Fusion ‣ II Related Work ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall performance of our baseline LiDAR and camera methods. Notice that both LiDAR methods outperform all three camera methods, and that the specialized camera model outperforms the two general models. Since the LiDAR methods outperform the camera methods, we take them as our primary detectors, and use the camera methods to augment their detection ability.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Unfortunately, the tested LiDAR methods produce many false positives which negatively impacts the overall system performance. Reducing these false positives is crucial for enhancing precision. To better understand the distribution of false positives, Fig. <a href="#S2.F3" title="Figure 3 ‣ II-C Detection Using LiDAR-Camera Fusion ‣ II Related Work ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows false positives and true positives over different confidence bands for several methods. Notice that the overall distributions are different in different methods. The neural network in our fusion model learns to leverage these differences.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Design and Implementation</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Method Overview</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We use camera model detections and a neural network to verify whether a particular LiDAR bounding box detection is valid. Fig. <a href="#S2.F4" title="Figure 4 ‣ II-C Detection Using LiDAR-Camera Fusion ‣ II Related Work ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides an overview of our method.
For each LiDAR detection, we look for a corresponding camera detection. If a matching camera box is found, the camera bounding box, the Intersection over Union (IoU) between the LiDAR and camera boxes, and the confidence scores of both detections are fed into a neural network, along with the LiDAR box. If no matching camera box is found, we input a zero for both the bounding box and the confidence score for the camera model.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our goal is to determine whether to keep or discard each LiDAR (L) detection. We generate ground truth by calculating the IoU between the LiDAR box and the official KITTI ground truth, and follow the datasets standard by setting an IoU threshold of 0.7. Detections above this threshold are marked as valid (positive class), while those below are ignored.
The inputs given would be:
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><math id="S4.SS1.p3.1.m1.10" class="ltx_Math" alttext="I=\{\text{Bbox}_{L},\text{Bbox}_{C},S_{L},S_{C},\text{IoU}_{(L,C)},[\text{IoU}_{(L,GD)}]\}" display="inline"><semantics id="S4.SS1.p3.1.m1.10a"><mrow id="S4.SS1.p3.1.m1.10.10" xref="S4.SS1.p3.1.m1.10.10.cmml"><mi id="S4.SS1.p3.1.m1.10.10.8" xref="S4.SS1.p3.1.m1.10.10.8.cmml">I</mi><mo id="S4.SS1.p3.1.m1.10.10.7" xref="S4.SS1.p3.1.m1.10.10.7.cmml">=</mo><mrow id="S4.SS1.p3.1.m1.10.10.6.6" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.10.10.6.6.7" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">{</mo><msub id="S4.SS1.p3.1.m1.5.5.1.1.1" xref="S4.SS1.p3.1.m1.5.5.1.1.1.cmml"><mtext id="S4.SS1.p3.1.m1.5.5.1.1.1.2" xref="S4.SS1.p3.1.m1.5.5.1.1.1.2a.cmml">Bbox</mtext><mi id="S4.SS1.p3.1.m1.5.5.1.1.1.3" xref="S4.SS1.p3.1.m1.5.5.1.1.1.3.cmml">L</mi></msub><mo id="S4.SS1.p3.1.m1.10.10.6.6.8" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">,</mo><msub id="S4.SS1.p3.1.m1.6.6.2.2.2" xref="S4.SS1.p3.1.m1.6.6.2.2.2.cmml"><mtext id="S4.SS1.p3.1.m1.6.6.2.2.2.2" xref="S4.SS1.p3.1.m1.6.6.2.2.2.2a.cmml">Bbox</mtext><mi id="S4.SS1.p3.1.m1.6.6.2.2.2.3" xref="S4.SS1.p3.1.m1.6.6.2.2.2.3.cmml">C</mi></msub><mo id="S4.SS1.p3.1.m1.10.10.6.6.9" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">,</mo><msub id="S4.SS1.p3.1.m1.7.7.3.3.3" xref="S4.SS1.p3.1.m1.7.7.3.3.3.cmml"><mi id="S4.SS1.p3.1.m1.7.7.3.3.3.2" xref="S4.SS1.p3.1.m1.7.7.3.3.3.2.cmml">S</mi><mi id="S4.SS1.p3.1.m1.7.7.3.3.3.3" xref="S4.SS1.p3.1.m1.7.7.3.3.3.3.cmml">L</mi></msub><mo id="S4.SS1.p3.1.m1.10.10.6.6.10" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">,</mo><msub id="S4.SS1.p3.1.m1.8.8.4.4.4" xref="S4.SS1.p3.1.m1.8.8.4.4.4.cmml"><mi id="S4.SS1.p3.1.m1.8.8.4.4.4.2" xref="S4.SS1.p3.1.m1.8.8.4.4.4.2.cmml">S</mi><mi id="S4.SS1.p3.1.m1.8.8.4.4.4.3" xref="S4.SS1.p3.1.m1.8.8.4.4.4.3.cmml">C</mi></msub><mo id="S4.SS1.p3.1.m1.10.10.6.6.11" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">,</mo><msub id="S4.SS1.p3.1.m1.9.9.5.5.5" xref="S4.SS1.p3.1.m1.9.9.5.5.5.cmml"><mtext id="S4.SS1.p3.1.m1.9.9.5.5.5.2" xref="S4.SS1.p3.1.m1.9.9.5.5.5.2a.cmml">IoU</mtext><mrow id="S4.SS1.p3.1.m1.2.2.2.4" xref="S4.SS1.p3.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.2.2.2.4.1" xref="S4.SS1.p3.1.m1.2.2.2.3.cmml">(</mo><mi id="S4.SS1.p3.1.m1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.cmml">L</mi><mo id="S4.SS1.p3.1.m1.2.2.2.4.2" xref="S4.SS1.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS1.p3.1.m1.2.2.2.2" xref="S4.SS1.p3.1.m1.2.2.2.2.cmml">C</mi><mo stretchy="false" id="S4.SS1.p3.1.m1.2.2.2.4.3" xref="S4.SS1.p3.1.m1.2.2.2.3.cmml">)</mo></mrow></msub><mo id="S4.SS1.p3.1.m1.10.10.6.6.12" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">,</mo><mrow id="S4.SS1.p3.1.m1.10.10.6.6.6.1" xref="S4.SS1.p3.1.m1.10.10.6.6.6.2.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.10.10.6.6.6.1.2" xref="S4.SS1.p3.1.m1.10.10.6.6.6.2.1.cmml">[</mo><msub id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.cmml"><mtext id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2a.cmml">IoU</mtext><mrow id="S4.SS1.p3.1.m1.4.4.2.2" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.4.4.2.2.2" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">(</mo><mi id="S4.SS1.p3.1.m1.3.3.1.1" xref="S4.SS1.p3.1.m1.3.3.1.1.cmml">L</mi><mo id="S4.SS1.p3.1.m1.4.4.2.2.3" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">,</mo><mrow id="S4.SS1.p3.1.m1.4.4.2.2.1" xref="S4.SS1.p3.1.m1.4.4.2.2.1.cmml"><mi id="S4.SS1.p3.1.m1.4.4.2.2.1.2" xref="S4.SS1.p3.1.m1.4.4.2.2.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.4.4.2.2.1.1" xref="S4.SS1.p3.1.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S4.SS1.p3.1.m1.4.4.2.2.1.3" xref="S4.SS1.p3.1.m1.4.4.2.2.1.3.cmml">D</mi></mrow><mo stretchy="false" id="S4.SS1.p3.1.m1.4.4.2.2.4" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">)</mo></mrow></msub><mo stretchy="false" id="S4.SS1.p3.1.m1.10.10.6.6.6.1.3" xref="S4.SS1.p3.1.m1.10.10.6.6.6.2.1.cmml">]</mo></mrow><mo stretchy="false" id="S4.SS1.p3.1.m1.10.10.6.6.13" xref="S4.SS1.p3.1.m1.10.10.6.7.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.10b"><apply id="S4.SS1.p3.1.m1.10.10.cmml" xref="S4.SS1.p3.1.m1.10.10"><eq id="S4.SS1.p3.1.m1.10.10.7.cmml" xref="S4.SS1.p3.1.m1.10.10.7"></eq><ci id="S4.SS1.p3.1.m1.10.10.8.cmml" xref="S4.SS1.p3.1.m1.10.10.8">𝐼</ci><set id="S4.SS1.p3.1.m1.10.10.6.7.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6"><apply id="S4.SS1.p3.1.m1.5.5.1.1.1.cmml" xref="S4.SS1.p3.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.5.5.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.5.5.1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.5.5.1.1.1.2a.cmml" xref="S4.SS1.p3.1.m1.5.5.1.1.1.2"><mtext id="S4.SS1.p3.1.m1.5.5.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.5.5.1.1.1.2">Bbox</mtext></ci><ci id="S4.SS1.p3.1.m1.5.5.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.5.5.1.1.1.3">𝐿</ci></apply><apply id="S4.SS1.p3.1.m1.6.6.2.2.2.cmml" xref="S4.SS1.p3.1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.6.6.2.2.2.1.cmml" xref="S4.SS1.p3.1.m1.6.6.2.2.2">subscript</csymbol><ci id="S4.SS1.p3.1.m1.6.6.2.2.2.2a.cmml" xref="S4.SS1.p3.1.m1.6.6.2.2.2.2"><mtext id="S4.SS1.p3.1.m1.6.6.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.6.6.2.2.2.2">Bbox</mtext></ci><ci id="S4.SS1.p3.1.m1.6.6.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.6.6.2.2.2.3">𝐶</ci></apply><apply id="S4.SS1.p3.1.m1.7.7.3.3.3.cmml" xref="S4.SS1.p3.1.m1.7.7.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.7.7.3.3.3.1.cmml" xref="S4.SS1.p3.1.m1.7.7.3.3.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.7.7.3.3.3.2.cmml" xref="S4.SS1.p3.1.m1.7.7.3.3.3.2">𝑆</ci><ci id="S4.SS1.p3.1.m1.7.7.3.3.3.3.cmml" xref="S4.SS1.p3.1.m1.7.7.3.3.3.3">𝐿</ci></apply><apply id="S4.SS1.p3.1.m1.8.8.4.4.4.cmml" xref="S4.SS1.p3.1.m1.8.8.4.4.4"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.8.8.4.4.4.1.cmml" xref="S4.SS1.p3.1.m1.8.8.4.4.4">subscript</csymbol><ci id="S4.SS1.p3.1.m1.8.8.4.4.4.2.cmml" xref="S4.SS1.p3.1.m1.8.8.4.4.4.2">𝑆</ci><ci id="S4.SS1.p3.1.m1.8.8.4.4.4.3.cmml" xref="S4.SS1.p3.1.m1.8.8.4.4.4.3">𝐶</ci></apply><apply id="S4.SS1.p3.1.m1.9.9.5.5.5.cmml" xref="S4.SS1.p3.1.m1.9.9.5.5.5"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.9.9.5.5.5.1.cmml" xref="S4.SS1.p3.1.m1.9.9.5.5.5">subscript</csymbol><ci id="S4.SS1.p3.1.m1.9.9.5.5.5.2a.cmml" xref="S4.SS1.p3.1.m1.9.9.5.5.5.2"><mtext id="S4.SS1.p3.1.m1.9.9.5.5.5.2.cmml" xref="S4.SS1.p3.1.m1.9.9.5.5.5.2">IoU</mtext></ci><interval closure="open" id="S4.SS1.p3.1.m1.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.2.2.2.4"><ci id="S4.SS1.p3.1.m1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1">𝐿</ci><ci id="S4.SS1.p3.1.m1.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2">𝐶</ci></interval></apply><apply id="S4.SS1.p3.1.m1.10.10.6.6.6.2.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.10.10.6.6.6.2.1.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.2">delimited-[]</csymbol><apply id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.1.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2a.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2"><mtext id="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2.cmml" xref="S4.SS1.p3.1.m1.10.10.6.6.6.1.1.2">IoU</mtext></ci><interval closure="open" id="S4.SS1.p3.1.m1.4.4.2.3.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2"><ci id="S4.SS1.p3.1.m1.3.3.1.1.cmml" xref="S4.SS1.p3.1.m1.3.3.1.1">𝐿</ci><apply id="S4.SS1.p3.1.m1.4.4.2.2.1.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.1"><times id="S4.SS1.p3.1.m1.4.4.2.2.1.1.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.1.1"></times><ci id="S4.SS1.p3.1.m1.4.4.2.2.1.2.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.1.2">𝐺</ci><ci id="S4.SS1.p3.1.m1.4.4.2.2.1.3.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.1.3">𝐷</ci></apply></interval></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.10c">I=\{\text{Bbox}_{L},\text{Bbox}_{C},S_{L},S_{C},\text{IoU}_{(L,C)},[\text{IoU}_{(L,GD)}]\}</annotation></semantics></math> 
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.6" class="ltx_p">where <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="{Bbox}_{L}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1a" xref="S4.SS1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.4" xref="S4.SS1.p4.1.m1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1b" xref="S4.SS1.p4.1.m1.1.1.1.cmml">​</mo><msub id="S4.SS1.p4.1.m1.1.1.5" xref="S4.SS1.p4.1.m1.1.1.5.cmml"><mi id="S4.SS1.p4.1.m1.1.1.5.2" xref="S4.SS1.p4.1.m1.1.1.5.2.cmml">x</mi><mi id="S4.SS1.p4.1.m1.1.1.5.3" xref="S4.SS1.p4.1.m1.1.1.5.3.cmml">L</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></times><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝐵</ci><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">𝑏</ci><ci id="S4.SS1.p4.1.m1.1.1.4.cmml" xref="S4.SS1.p4.1.m1.1.1.4">𝑜</ci><apply id="S4.SS1.p4.1.m1.1.1.5.cmml" xref="S4.SS1.p4.1.m1.1.1.5"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.5.1.cmml" xref="S4.SS1.p4.1.m1.1.1.5">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.5.2.cmml" xref="S4.SS1.p4.1.m1.1.1.5.2">𝑥</ci><ci id="S4.SS1.p4.1.m1.1.1.5.3.cmml" xref="S4.SS1.p4.1.m1.1.1.5.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">{Bbox}_{L}</annotation></semantics></math> and <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="{Bbox}_{C}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1a" xref="S4.SS1.p4.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.2.m2.1.1.4" xref="S4.SS1.p4.2.m2.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1b" xref="S4.SS1.p4.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS1.p4.2.m2.1.1.5" xref="S4.SS1.p4.2.m2.1.1.5.cmml"><mi id="S4.SS1.p4.2.m2.1.1.5.2" xref="S4.SS1.p4.2.m2.1.1.5.2.cmml">x</mi><mi id="S4.SS1.p4.2.m2.1.1.5.3" xref="S4.SS1.p4.2.m2.1.1.5.3.cmml">C</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><times id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></times><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝐵</ci><ci id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">𝑏</ci><ci id="S4.SS1.p4.2.m2.1.1.4.cmml" xref="S4.SS1.p4.2.m2.1.1.4">𝑜</ci><apply id="S4.SS1.p4.2.m2.1.1.5.cmml" xref="S4.SS1.p4.2.m2.1.1.5"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.5.1.cmml" xref="S4.SS1.p4.2.m2.1.1.5">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.5.2.cmml" xref="S4.SS1.p4.2.m2.1.1.5.2">𝑥</ci><ci id="S4.SS1.p4.2.m2.1.1.5.3.cmml" xref="S4.SS1.p4.2.m2.1.1.5.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">{Bbox}_{C}</annotation></semantics></math> are the dimensions of bounding boxes of LiDAR and camera models respectively, represented by their width, height, and center coordinates, normalized by the image’s dimensions. <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="{S}_{L}" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><msub id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">S</mi><mi id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">𝑆</ci><ci id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">{S}_{L}</annotation></semantics></math> and <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="{S}_{C}" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><msub id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml">S</mi><mi id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2">𝑆</ci><ci id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">{S}_{C}</annotation></semantics></math> are their confidence scores, and <math id="S4.SS1.p4.5.m5.2" class="ltx_Math" alttext="IoU_{(L,C)}" display="inline"><semantics id="S4.SS1.p4.5.m5.2a"><mrow id="S4.SS1.p4.5.m5.2.3" xref="S4.SS1.p4.5.m5.2.3.cmml"><mi id="S4.SS1.p4.5.m5.2.3.2" xref="S4.SS1.p4.5.m5.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.3.1" xref="S4.SS1.p4.5.m5.2.3.1.cmml">​</mo><mi id="S4.SS1.p4.5.m5.2.3.3" xref="S4.SS1.p4.5.m5.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.3.1a" xref="S4.SS1.p4.5.m5.2.3.1.cmml">​</mo><msub id="S4.SS1.p4.5.m5.2.3.4" xref="S4.SS1.p4.5.m5.2.3.4.cmml"><mi id="S4.SS1.p4.5.m5.2.3.4.2" xref="S4.SS1.p4.5.m5.2.3.4.2.cmml">U</mi><mrow id="S4.SS1.p4.5.m5.2.2.2.4" xref="S4.SS1.p4.5.m5.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS1.p4.5.m5.2.2.2.4.1" xref="S4.SS1.p4.5.m5.2.2.2.3.cmml">(</mo><mi id="S4.SS1.p4.5.m5.1.1.1.1" xref="S4.SS1.p4.5.m5.1.1.1.1.cmml">L</mi><mo id="S4.SS1.p4.5.m5.2.2.2.4.2" xref="S4.SS1.p4.5.m5.2.2.2.3.cmml">,</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2" xref="S4.SS1.p4.5.m5.2.2.2.2.cmml">C</mi><mo stretchy="false" id="S4.SS1.p4.5.m5.2.2.2.4.3" xref="S4.SS1.p4.5.m5.2.2.2.3.cmml">)</mo></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.2b"><apply id="S4.SS1.p4.5.m5.2.3.cmml" xref="S4.SS1.p4.5.m5.2.3"><times id="S4.SS1.p4.5.m5.2.3.1.cmml" xref="S4.SS1.p4.5.m5.2.3.1"></times><ci id="S4.SS1.p4.5.m5.2.3.2.cmml" xref="S4.SS1.p4.5.m5.2.3.2">𝐼</ci><ci id="S4.SS1.p4.5.m5.2.3.3.cmml" xref="S4.SS1.p4.5.m5.2.3.3">𝑜</ci><apply id="S4.SS1.p4.5.m5.2.3.4.cmml" xref="S4.SS1.p4.5.m5.2.3.4"><csymbol cd="ambiguous" id="S4.SS1.p4.5.m5.2.3.4.1.cmml" xref="S4.SS1.p4.5.m5.2.3.4">subscript</csymbol><ci id="S4.SS1.p4.5.m5.2.3.4.2.cmml" xref="S4.SS1.p4.5.m5.2.3.4.2">𝑈</ci><interval closure="open" id="S4.SS1.p4.5.m5.2.2.2.3.cmml" xref="S4.SS1.p4.5.m5.2.2.2.4"><ci id="S4.SS1.p4.5.m5.1.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1">𝐿</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2">𝐶</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.2c">IoU_{(L,C)}</annotation></semantics></math> is the Intersection over Union of LiDAR and camera detection. When two camera methods are used, information about a second bounding box is provided. For example, <math id="S4.SS1.p4.6.m6.2" class="ltx_Math" alttext="IoU_{(L,GD)}" display="inline"><semantics id="S4.SS1.p4.6.m6.2a"><mrow id="S4.SS1.p4.6.m6.2.3" xref="S4.SS1.p4.6.m6.2.3.cmml"><mi id="S4.SS1.p4.6.m6.2.3.2" xref="S4.SS1.p4.6.m6.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.6.m6.2.3.1" xref="S4.SS1.p4.6.m6.2.3.1.cmml">​</mo><mi id="S4.SS1.p4.6.m6.2.3.3" xref="S4.SS1.p4.6.m6.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.6.m6.2.3.1a" xref="S4.SS1.p4.6.m6.2.3.1.cmml">​</mo><msub id="S4.SS1.p4.6.m6.2.3.4" xref="S4.SS1.p4.6.m6.2.3.4.cmml"><mi id="S4.SS1.p4.6.m6.2.3.4.2" xref="S4.SS1.p4.6.m6.2.3.4.2.cmml">U</mi><mrow id="S4.SS1.p4.6.m6.2.2.2.2" xref="S4.SS1.p4.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS1.p4.6.m6.2.2.2.2.2" xref="S4.SS1.p4.6.m6.2.2.2.3.cmml">(</mo><mi id="S4.SS1.p4.6.m6.1.1.1.1" xref="S4.SS1.p4.6.m6.1.1.1.1.cmml">L</mi><mo id="S4.SS1.p4.6.m6.2.2.2.2.3" xref="S4.SS1.p4.6.m6.2.2.2.3.cmml">,</mo><mrow id="S4.SS1.p4.6.m6.2.2.2.2.1" xref="S4.SS1.p4.6.m6.2.2.2.2.1.cmml"><mi id="S4.SS1.p4.6.m6.2.2.2.2.1.2" xref="S4.SS1.p4.6.m6.2.2.2.2.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.6.m6.2.2.2.2.1.1" xref="S4.SS1.p4.6.m6.2.2.2.2.1.1.cmml">​</mo><mi id="S4.SS1.p4.6.m6.2.2.2.2.1.3" xref="S4.SS1.p4.6.m6.2.2.2.2.1.3.cmml">D</mi></mrow><mo stretchy="false" id="S4.SS1.p4.6.m6.2.2.2.2.4" xref="S4.SS1.p4.6.m6.2.2.2.3.cmml">)</mo></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.2b"><apply id="S4.SS1.p4.6.m6.2.3.cmml" xref="S4.SS1.p4.6.m6.2.3"><times id="S4.SS1.p4.6.m6.2.3.1.cmml" xref="S4.SS1.p4.6.m6.2.3.1"></times><ci id="S4.SS1.p4.6.m6.2.3.2.cmml" xref="S4.SS1.p4.6.m6.2.3.2">𝐼</ci><ci id="S4.SS1.p4.6.m6.2.3.3.cmml" xref="S4.SS1.p4.6.m6.2.3.3">𝑜</ci><apply id="S4.SS1.p4.6.m6.2.3.4.cmml" xref="S4.SS1.p4.6.m6.2.3.4"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m6.2.3.4.1.cmml" xref="S4.SS1.p4.6.m6.2.3.4">subscript</csymbol><ci id="S4.SS1.p4.6.m6.2.3.4.2.cmml" xref="S4.SS1.p4.6.m6.2.3.4.2">𝑈</ci><interval closure="open" id="S4.SS1.p4.6.m6.2.2.2.3.cmml" xref="S4.SS1.p4.6.m6.2.2.2.2"><ci id="S4.SS1.p4.6.m6.1.1.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1.1.1">𝐿</ci><apply id="S4.SS1.p4.6.m6.2.2.2.2.1.cmml" xref="S4.SS1.p4.6.m6.2.2.2.2.1"><times id="S4.SS1.p4.6.m6.2.2.2.2.1.1.cmml" xref="S4.SS1.p4.6.m6.2.2.2.2.1.1"></times><ci id="S4.SS1.p4.6.m6.2.2.2.2.1.2.cmml" xref="S4.SS1.p4.6.m6.2.2.2.2.1.2">𝐺</ci><ci id="S4.SS1.p4.6.m6.2.2.2.2.1.3.cmml" xref="S4.SS1.p4.6.m6.2.2.2.2.1.3">𝐷</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.2c">IoU_{(L,GD)}</annotation></semantics></math> is the optionally provided IoU between LiDAR and GroundingDINO.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">The entire input vector is then fed to a two-layer multi-layer perceptron (MLP) and trained with a high recall to provide a bias towards avoiding accidentally removing true positive bounding boxes while reducing false positives. This MLP is simple and architecturally identical in all of our experiments. However the activation weights need to be trained for each pair of LiDAR and camera methods since each baseline method has its own distributions of true and false positives and the goal of the MLP is to make specific tradeoffs in these distributions.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">In general we desire over production of bounding boxes in the verification method as opposed to under production. Since the detected boxes of the open vocabulary model are not as precise as the specialized LiDAR and camera models, especially around occluded objects, we use a lower IoU threshold to allow more boxes to match.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:342.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(57.3pt,-45.2pt) scale(1.35889341815319,1.35889341815319) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="2"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Easy</span></th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="2"><span id="S4.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Hard</span></th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Modality</span></th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">FP</span></th>
<th id="S4.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="S4.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">FP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr" style="background-color:#F3F3F3;">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.1.1" class="ltx_text" style="background-color:#F3F3F3;">PVRCNN</span></th>
<th id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.2.1" class="ltx_text" style="background-color:#F3F3F3;">L</span></th>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#F3F3F3;padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.3.1" class="ltx_text" style="background-color:#F3F3F3;">1433</span></td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.4.1" class="ltx_text" style="background-color:#F3F3F3;">1287</span></td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#F3F3F3;padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.5.1" class="ltx_text" style="background-color:#F3F3F3;">5364</span></td>
<td id="S4.T1.1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.3.1.6.1" class="ltx_text" style="background-color:#F3F3F3;">3620</span></td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">PV+Mono (Ours)</th>
<th id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1428</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">393</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5280</td>
<td id="S4.T1.1.1.4.2.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1084</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">PV+YOLO (Ours)</th>
<th id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1430</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">360</td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5278</td>
<td id="S4.T1.1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">965</td>
</tr>
<tr id="S4.T1.1.1.6.4" class="ltx_tr">
<th id="S4.T1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">PV+GDino (Ours)</th>
<th id="S4.T1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.6.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1430</td>
<td id="S4.T1.1.1.6.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">366</td>
<td id="S4.T1.1.1.6.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5271</td>
<td id="S4.T1.1.1.6.4.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">937</td>
</tr>
<tr id="S4.T1.1.1.7.5" class="ltx_tr">
<th id="S4.T1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">PV+Mono+GDino (Ours)</th>
<th id="S4.T1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LCC</th>
<td id="S4.T1.1.1.7.5.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1430</td>
<td id="S4.T1.1.1.7.5.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">354</td>
<td id="S4.T1.1.1.7.5.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5261</td>
<td id="S4.T1.1.1.7.5.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">906</td>
</tr>
<tr id="S4.T1.1.1.8.6" class="ltx_tr">
<th id="S4.T1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">PV+YOLO+GDino (Ours)</th>
<th id="S4.T1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LCC</th>
<td id="S4.T1.1.1.8.6.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1430</td>
<td id="S4.T1.1.1.8.6.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">347</td>
<td id="S4.T1.1.1.8.6.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5266</td>
<td id="S4.T1.1.1.8.6.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">887</td>
</tr>
<tr id="S4.T1.1.1.9.7" class="ltx_tr" style="background-color:#F3F3F3;">
<th id="S4.T1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.1.1" class="ltx_text" style="background-color:#F3F3F3;">TED</span></th>
<th id="S4.T1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.2.1" class="ltx_text" style="background-color:#F3F3F3;">L</span></th>
<td id="S4.T1.1.1.9.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#F3F3F3;padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.3.1" class="ltx_text" style="background-color:#F3F3F3;">1434</span></td>
<td id="S4.T1.1.1.9.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.4.1" class="ltx_text" style="background-color:#F3F3F3;">864</span></td>
<td id="S4.T1.1.1.9.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#F3F3F3;padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.5.1" class="ltx_text" style="background-color:#F3F3F3;">5425</span></td>
<td id="S4.T1.1.1.9.7.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T1.1.1.9.7.6.1" class="ltx_text" style="background-color:#F3F3F3;">1848</span></td>
</tr>
<tr id="S4.T1.1.1.10.8" class="ltx_tr">
<th id="S4.T1.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">TED+Mono (Ours)</th>
<th id="S4.T1.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.10.8.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1434</td>
<td id="S4.T1.1.1.10.8.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">323</td>
<td id="S4.T1.1.1.10.8.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5210</td>
<td id="S4.T1.1.1.10.8.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">599</td>
</tr>
<tr id="S4.T1.1.1.11.9" class="ltx_tr">
<th id="S4.T1.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">TED+YOLO (Ours)</th>
<th id="S4.T1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.11.9.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1434</td>
<td id="S4.T1.1.1.11.9.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">389</td>
<td id="S4.T1.1.1.11.9.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5380</td>
<td id="S4.T1.1.1.11.9.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1028</td>
</tr>
<tr id="S4.T1.1.1.12.10" class="ltx_tr">
<th id="S4.T1.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">TED+GDino (Ours)</th>
<th id="S4.T1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LC</th>
<td id="S4.T1.1.1.12.10.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1434</td>
<td id="S4.T1.1.1.12.10.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">352</td>
<td id="S4.T1.1.1.12.10.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5359</td>
<td id="S4.T1.1.1.12.10.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">869</td>
</tr>
<tr id="S4.T1.1.1.13.11" class="ltx_tr">
<th id="S4.T1.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">TED+Mono+GDino (Ours)</th>
<th id="S4.T1.1.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LCC</th>
<td id="S4.T1.1.1.13.11.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1434</td>
<td id="S4.T1.1.1.13.11.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">334</td>
<td id="S4.T1.1.1.13.11.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5343</td>
<td id="S4.T1.1.1.13.11.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">826</td>
</tr>
<tr id="S4.T1.1.1.14.12" class="ltx_tr">
<th id="S4.T1.1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">TED+YOLO+GDino (Ours)</th>
<th id="S4.T1.1.1.14.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">LCC</th>
<td id="S4.T1.1.1.14.12.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">1434</td>
<td id="S4.T1.1.1.14.12.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">381</td>
<td id="S4.T1.1.1.14.12.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">5354</td>
<td id="S4.T1.1.1.14.12.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">914</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Since ours is a late fusion method that generates no new detections, we focus on reducing false positives (FPs) while preserving true positives (TPs). Our method is successfully able to do so, reducing the FPs by an average of 73.6% on PVRCNN and 54.2% on TED.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.15529/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="292" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Our fusion method is able to reduce the false positives significantly on (a) PVRCNN, and (b) TED model detections. Note that this improvement is achieved for all tested camera methods used for verification. </figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementation</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To implement our late-fusion method, we combine two specialized LiDAR detectors (PVRCNN and TED) with various camera models: a specialized model (MonoDETR), a general-purpose model (YOLO-NAS), and an open-vocabulary model (GroundingDINO). The experiments are conducted on the KITTI dataset. We remove dataset images for which ground truth is not available, as well as those which have already been used to train our baseline models. This leaves 3769 images available for our experiments comprising the subset that the dataset providers call validation. We split these images into training and testing subsets. For PVRCNN, these images generate a training set of 12,303 LiDAR bounding box samples and test set of 12,817 bounding boxes. For TED-S, we obtain 9,016 training and 9,406 testing bounding boxes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The final inputs to our model include details such as LiDAR bounding box dimensions, confidence scores, and IoUs from the camera methods. These inputs pass through a two-layer multi-layer perceptron (MLP) with ReLU activation function and a last sigmoid layer. This output decides whether to keep each LiDAR box or discard the detection. The training loss function compares these outputs with the ground truth labels. Since our focus is on reducing false positives without significantly disturbing the true positive count, we use class weights of 1:10 for negative and positive classes to maintain high recall. We train our models for 50 epochs with a binary cross entropy (BCE) loss and an Adam learning rate of 0.0001.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present the results of our methods in two parts: First, we show the impact of our fusion method on reducing false positives generated by LiDAR-only models. Second, we provide an overall comparison of the 2D Average Precision (AP), comparing our fusion method with single-modality models as well as state-of-the-art fusion methods.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">False Positives Reduction</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S4.T1" title="TABLE I ‣ IV-A Method Overview ‣ IV Design and Implementation ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the performance of LiDAR-only models in terms of true positive and false positive bounding box predictions. Note that both LiDAR models produce a large number of false positives. Our late fusion method is evaluated on each LiDAR model, using each of our three baseline camera detectors, as well as using combinations of camera detectors. Our method consistently and substantially reduces false positives across all combinations. This is made especially clear through the visualization in Figure <a href="#S4.F5" title="Figure 5 ‣ IV-A Method Overview ‣ IV Design and Implementation ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Our late fusion method achieves an average false positive reduction of 73.6% on PVRCNN and 54.2% on TED. Importantly, the true positive (TP) numbers have only a small reduction, indicating that the significant decrease in false positives does not come at the cost of many missed detections.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"></td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;" colspan="3"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">2D AP (%)</span></td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Models</span></td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Modality</span></td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model Type</span></td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Easy</span></td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Moderate</span></td>
<td id="S5.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Hard</span></td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<td id="S5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.3.3.1.1" class="ltx_text" style="font-size:70%;">MonoDETR</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.3.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S5.T2.1.3.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.3.3.2.1" class="ltx_text" style="font-size:70%;">C</span></td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.3.3.3.1" class="ltx_text" style="font-size:70%;">Specialized</span></td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.3.3.4.1" class="ltx_text" style="font-size:70%;">95.12</span></td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.3.3.5.1" class="ltx_text" style="font-size:70%;">87.36</span></td>
<td id="S5.T2.1.3.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.3.3.6.1" class="ltx_text" style="font-size:70%;">79.94</span></td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<td id="S5.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.4.4.1.1" class="ltx_text" style="font-size:70%;">YOLO-NAS</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.4.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S5.T2.1.4.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.4.4.2.1" class="ltx_text" style="font-size:70%;">C</span></td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.4.4.3.1" class="ltx_text" style="font-size:70%;">General</span></td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.4.4.4.1" class="ltx_text" style="font-size:70%;">75.32</span></td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.4.4.5.1" class="ltx_text" style="font-size:70%;">68.69</span></td>
<td id="S5.T2.1.4.4.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.4.4.6.1" class="ltx_text" style="font-size:70%;">55.11</span></td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<td id="S5.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.5.5.1.1" class="ltx_text" style="font-size:70%;">GroundingDino</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.5.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S5.T2.1.5.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.5.5.2.1" class="ltx_text" style="font-size:70%;">C</span></td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.5.5.3.1" class="ltx_text" style="font-size:70%;">Open-Vocab</span></td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.5.5.4.1" class="ltx_text" style="font-size:70%;">84.73</span></td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.5.5.5.1" class="ltx_text" style="font-size:70%;">78.68</span></td>
<td id="S5.T2.1.5.5.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.5.5.6.1" class="ltx_text" style="font-size:70%;">62.82</span></td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="S5.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#CCCCCC;">PVRCNN<cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.6.6.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S5.T2.1.6.6.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span></td>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#CCCCCC;">L</span></td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.3.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">Specialized</span></td>
<td id="S5.T2.1.6.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#CCCCCC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.4.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">98.30</span></td>
<td id="S5.T2.1.6.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#CCCCCC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.5.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">94.44</span></td>
<td id="S5.T2.1.6.6.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.6.6.6.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">94.08</span></td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.7.7.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(PVRCNN+MonoDETR)</em></span></td>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.7.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.4.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">98.88</span></td>
<td id="S5.T2.1.7.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#FFF2CC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.5.1" class="ltx_text" style="font-size:70%;background-color:#FFF2CC;">95.11</span></td>
<td id="S5.T2.1.7.7.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#F3F3F3;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.7.7.6.1" class="ltx_text" style="font-size:70%;background-color:#F3F3F3;">92.89</span></td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.8.8.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(PVRCNN+YOLO-NAS)</em></span></td>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.8.8.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.4.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">98.88</span></td>
<td id="S5.T2.1.8.8.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.5.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">95.14</span></td>
<td id="S5.T2.1.8.8.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#F3F3F3;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.8.8.6.1" class="ltx_text" style="font-size:70%;background-color:#F3F3F3;">92.91</span></td>
</tr>
<tr id="S5.T2.1.9.9" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.9.9.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(PVRCNN+GDino)</em></span></td>
<td id="S5.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.9.9.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.4.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">98.88</span></td>
<td id="S5.T2.1.9.9.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.5.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">95.14</span></td>
<td id="S5.T2.1.9.9.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#F3F3F3;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.9.9.6.1" class="ltx_text" style="font-size:70%;background-color:#F3F3F3;">92.92</span></td>
</tr>
<tr id="S5.T2.1.10.10" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.10.10.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(PVRCNN+MonoDETR+GDino)</em></span></td>
<td id="S5.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LCC</span></td>
<td id="S5.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.10.10.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFDB71;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.4.1" class="ltx_text" style="font-size:70%;background-color:#FFDB71;">98.90</span></td>
<td id="S5.T2.1.10.10.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFDB71;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.5.1" class="ltx_text" style="font-size:70%;background-color:#FFDB71;">95.20</span></td>
<td id="S5.T2.1.10.10.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#F3F3F3;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.10.10.6.1" class="ltx_text" style="font-size:70%;background-color:#F3F3F3;">92.96</span></td>
</tr>
<tr id="S5.T2.1.11.11" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.11.11.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(PVRCNN+YOLO-NAS+GDino)</em></span></td>
<td id="S5.T2.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LCC</span></td>
<td id="S5.T2.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.11.11.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFCB32;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.4.1" class="ltx_text" style="font-size:70%;background-color:#FFCB32;">98.92</span></td>
<td id="S5.T2.1.11.11.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFCB32;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.5.1" class="ltx_text" style="font-size:70%;background-color:#FFCB32;">95.23</span></td>
<td id="S5.T2.1.11.11.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#F3F3F3;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.11.11.6.1" class="ltx_text" style="font-size:70%;background-color:#F3F3F3;">92.95</span></td>
</tr>
<tr id="S5.T2.1.12.12" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="S5.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#CCCCCC;">TED-S<cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.12.12.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S5.T2.1.12.12.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span></td>
<td id="S5.T2.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#CCCCCC;">L</span></td>
<td id="S5.T2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.3.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">Specialized</span></td>
<td id="S5.T2.1.12.12.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#CCCCCC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.4.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">98.88</span></td>
<td id="S5.T2.1.12.12.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#CCCCCC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.5.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">96.73</span></td>
<td id="S5.T2.1.12.12.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.12.12.6.1" class="ltx_text" style="font-size:70%;background-color:#CCCCCC;">94.96</span></td>
</tr>
<tr id="S5.T2.1.13.13" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.13.13.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(TED+MonoDETR)</em></span></td>
<td id="S5.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.13.13.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#FFF2CC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.4.1" class="ltx_text" style="font-size:70%;background-color:#FFF2CC;">99.14</span></td>
<td id="S5.T2.1.13.13.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#EFEFEF;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.5.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">93.34</span></td>
<td id="S5.T2.1.13.13.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="background-color:#FFF2CC;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.13.13.6.1" class="ltx_text" style="font-size:70%;background-color:#FFF2CC;">93.35</span></td>
</tr>
<tr id="S5.T2.1.14.14" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.14.14.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(TED+YOLO-NAS)</em></span></td>
<td id="S5.T2.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.14.14.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFDB71;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.4.1" class="ltx_text" style="font-size:70%;background-color:#FFDB71;">99.15</span></td>
<td id="S5.T2.1.14.14.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#EFEFEF;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.5.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">93.35</span></td>
<td id="S5.T2.1.14.14.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.14.14.6.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">95.41</span></td>
</tr>
<tr id="S5.T2.1.15.15" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.15.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.15.15.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(TED+GDino)</em></span></td>
<td id="S5.T2.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LC</span></td>
<td id="S5.T2.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.15.15.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFF6DE;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.4.1" class="ltx_text" style="font-size:70%;background-color:#FFF6DE;">99.11</span></td>
<td id="S5.T2.1.15.15.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#EFEFEF;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.5.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">95.35</span></td>
<td id="S5.T2.1.15.15.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFDB71;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.15.15.6.1" class="ltx_text" style="font-size:70%;background-color:#FFDB71;">95.44</span></td>
</tr>
<tr id="S5.T2.1.16.16" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.16.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.16.16.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(TED+MonoDETR+GDino)</em></span></td>
<td id="S5.T2.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LCC</span></td>
<td id="S5.T2.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.16.16.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFCB32;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.4.1" class="ltx_text" style="font-size:70%;background-color:#FFCB32;">99.18</span></td>
<td id="S5.T2.1.16.16.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#EFEFEF;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.5.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">95.46</span></td>
<td id="S5.T2.1.16.16.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFD24F;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.16.16.6.1" class="ltx_text" style="font-size:70%;background-color:#FFD24F;">95.51</span></td>
</tr>
<tr id="S5.T2.1.17.17" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S5.T2.1.17.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.1.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Ours <em id="S5.T2.1.17.17.1.1.1" class="ltx_emph ltx_font_italic" style="background-color:#EFEFEF;">(TED+YOLO-NAS+GDino)</em></span></td>
<td id="S5.T2.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.2.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">LCC</span></td>
<td id="S5.T2.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.3.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">Late-Fusion</span></td>
<td id="S5.T2.1.17.17.4" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.4.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">99.13</span></td>
<td id="S5.T2.1.17.17.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#EFEFEF;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.5.1" class="ltx_text" style="font-size:70%;background-color:#EFEFEF;">95.34</span></td>
<td id="S5.T2.1.17.17.6" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#FFE9AA;padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.17.17.6.1" class="ltx_text" style="font-size:70%;background-color:#FFE9AA;">95.39</span></td>
</tr>
<tr id="S5.T2.1.18.18" class="ltx_tr">
<td id="S5.T2.1.18.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.18.18.1.1" class="ltx_text" style="font-size:70%;">CLOCs </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.18.18.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S5.T2.1.18.18.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.18.18.2.1" class="ltx_text" style="font-size:70%;">LC</span></td>
<td id="S5.T2.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.18.18.3.1" class="ltx_text" style="font-size:70%;">Late-Fusion</span></td>
<td id="S5.T2.1.18.18.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.18.18.4.1" class="ltx_text" style="font-size:70%;">99.33</span></td>
<td id="S5.T2.1.18.18.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.18.18.5.1" class="ltx_text" style="font-size:70%;">93.75</span></td>
<td id="S5.T2.1.18.18.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.18.18.6.1" class="ltx_text" style="font-size:70%;">92.89</span></td>
</tr>
<tr id="S5.T2.1.19.19" class="ltx_tr">
<td id="S5.T2.1.19.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.19.19.1.1" class="ltx_text" style="font-size:70%;">CMKD</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.19.19.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S5.T2.1.19.19.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.19.19.2.1" class="ltx_text" style="font-size:70%;">LC</span></td>
<td id="S5.T2.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.19.19.3.1" class="ltx_text" style="font-size:70%;">Knowledge Distil.</span></td>
<td id="S5.T2.1.19.19.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.19.19.4.1" class="ltx_text" style="font-size:70%;">96.08</span></td>
<td id="S5.T2.1.19.19.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.19.19.5.1" class="ltx_text" style="font-size:70%;">92.43</span></td>
<td id="S5.T2.1.19.19.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.19.19.6.1" class="ltx_text" style="font-size:70%;">87.11</span></td>
</tr>
<tr id="S5.T2.1.20.20" class="ltx_tr">
<td id="S5.T2.1.20.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.20.20.1.1" class="ltx_text" style="font-size:70%;">LogoNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.20.20.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S5.T2.1.20.20.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.20.20.2.1" class="ltx_text" style="font-size:70%;">LC</span></td>
<td id="S5.T2.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.20.20.3.1" class="ltx_text" style="font-size:70%;">Deep Fusion</span></td>
<td id="S5.T2.1.20.20.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.20.20.4.1" class="ltx_text" style="font-size:70%;">97.04</span></td>
<td id="S5.T2.1.20.20.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.20.20.5.1" class="ltx_text" style="font-size:70%;">89.44</span></td>
<td id="S5.T2.1.20.20.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.20.20.6.1" class="ltx_text" style="font-size:70%;">89.20</span></td>
</tr>
<tr id="S5.T2.1.21.21" class="ltx_tr">
<td id="S5.T2.1.21.21.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S5.T2.1.21.21.1.1" class="ltx_text" style="font-size:70%;">TED-M</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T2.1.21.21.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S5.T2.1.21.21.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S5.T2.1.21.21.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.21.21.2.1" class="ltx_text" style="font-size:70%;">LC</span></td>
<td id="S5.T2.1.21.21.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.21.21.3.1" class="ltx_text" style="font-size:70%;">Deep Fusion</span></td>
<td id="S5.T2.1.21.21.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.21.21.4.1" class="ltx_text" style="font-size:70%;">99.48</span></td>
<td id="S5.T2.1.21.21.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.21.21.5.1" class="ltx_text" style="font-size:70%;">96.25</span></td>
<td id="S5.T2.1.21.21.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;"><span id="S5.T2.1.21.21.6.1" class="ltx_text" style="font-size:70%;">93.72</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Perfomance on the Average Precision (AP) metric for our method on all difficulty sets on KITTI-val. Our late fusion method consistently outperforms the original specialized LiDAR and camera methods on the easy set, along with moderate on PVRCNN and hard on TED. Our method is also comparable to the SOTA deep fusion methods, despite being simpler and lightweight, without requiring special integrated training.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2409.15529/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="315" height="431" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>PRC curves showing the LiDAR method TED before and after adding validation with our method. Especially for the lower confidence predictions needed to produce high recall rates, the LiDAR method produces many false positives. Zoomed-in plots reveal our method’s effectiveness in reducing these false positives, increasing the average precision (AP).</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Average Precision</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T2" title="TABLE II ‣ V-A False Positives Reduction ‣ V Results ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents the 2D Average Precision (AP) across the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">Easy</span>, <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">Moderate</span>, and <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">Hard</span> KITTI sets, comparing individual LiDAR models, camera models, and their fusion combinations.
Our late fusion method shows improved performance compared to any single-modality models, outperforming PV-RCNN on the <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_italic">Easy</span> and <span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_italic">Moderate</span> sets, and TED-S on the <span id="S5.SS2.p1.1.6" class="ltx_text ltx_font_italic">Easy</span> and <span id="S5.SS2.p1.1.7" class="ltx_text ltx_font_italic">Hard</span> sets. The general and open-vocabulary camera models YOLO-NAS and GroundingDINO, perform as well as the special purpose camera model MonoDETR. These models, not specifically trained on KITTI, achieve comparable performance when fused with LiDAR models, demonstrating the adaptability of our method across different types of detectors.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">In comparison with several state-of-the-art (SOTA) fusion methods, our method shows comparable results. Some of the comparison methods show better performance for the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">Easy</span> and <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_italic">Moderate</span> sets, while our method demonstrates better results on the <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_italic">Hard</span> set. Our late-fusion method can match more complex fusion methods in terms of accuracy while offering better adaptability and lower computational overhead.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">To better understand our verification based method, Fig. <a href="#S5.F6" title="Figure 6 ‣ V-A False Positives Reduction ‣ V Results ‣ VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the Precision-Recall (PR) curves comparing TED-S with two of our fusion approaches. The PR curves show that at high recall values our method is able to increase precision by removing false positives.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Limitations</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our late fusion method is targeted at reducing false positives, but does not seek to increase true positives. We thus require that the LiDAR (or primary) detector successfully finds most true positives. Similarly, since our approach relies on validating LiDAR detections with camera inputs, it may not perform well with camera models that produce few detections, and is therefore more applicable to methods which can be tuned for a greater number.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we propose a model-independent late fusion method that combines LiDAR and camera detections to enhance object detection performance for autonomous vehicles. Our approach focuses on reducing false positives by cross-validating LiDAR detections with camera inputs. Experiments on the KITTI dataset demonstrate that our method is exceptional at reducing false positives, and performs competitively with state-of-the-art methods on average precision. Importantly, the method works equally well with several camera detectors, including non-specialized and open-vocabulary models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, Saber Fallah, David Oxtoby, and Alex Mouzakitis.

</span>
<span class="ltx_bibblock">A Survey on 3D Object Detection Methods for Autonomous Driving Applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, 20(10):3782–3795, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.09927</span>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">nuScenes: A Multimodal Dataset for Autonomous Driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 11621–11631, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bekir Eren Çaldiran and Tankut Acarman.

</span>
<span class="ltx_bibblock">A Late Asymmetric Fusion Approach To Eliminate False Positives.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</span>, pages 2080–2085. IEEE, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teuliere, and Thierry Chateau.

</span>
<span class="ltx_bibblock">Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, pages 2040–2049, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">ImageNet: A Large-scale Hierarchical Image Database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on Computer Vision and Pattern Recognition</span>, pages 248–255. IEEE, 2009.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jian Dou, Jianru Xue, and Jianwu Fang.

</span>
<span class="ltx_bibblock">SEG-VoxelNet for 3D vehicle detection from RGB and LiDAR data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Robotics and Automation (ICRA)</span>, pages 4362–4368. IEEE, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Vision meets Robotics: The KITTI dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">The International Journal of Robotics Research</span>, 32(11):1231–1237, 2013.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yu Hong, Hang Dai, and Yong Ding.

</span>
<span class="ltx_bibblock">Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 87–104. Springer, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Keli Huang, Botian Shi, Xiang Li, Xin Li, Siyuan Huang, and Yikang Li.

</span>
<span class="ltx_bibblock">Multi-modal Sensor Fusion for Auto Driving Perception: A Survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.02703</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Tengteng Huang, Zhe Liu, Xiwu Chen, and Xiang Bai.

</span>
<span class="ltx_bibblock">EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16</span>, pages 35–52. Springer, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">PointPillars: Fast Encoders for Object Detection from Point Clouds.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 12697–12705, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang, Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao, et al.

</span>
<span class="ltx_bibblock">LoGoNet: Towards Accurate 3D Object Detection With Local-to-Global Cross-Modal Fusion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 17524–17534, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al.

</span>
<span class="ltx_bibblock">DeepFusion: LiDAR-Camera Deep Fusion for Multi-Modal 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 17182–17191, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</span>, pages 740–755. Springer, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.

</span>
<span class="ltx_bibblock">Grounding DINO: Marrying DINO with grounded pre-training for open-set Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.05499</span>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.

</span>
<span class="ltx_bibblock">SSD: Single Shot Multibox Detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</span>, pages 21–37. Springer, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhe Liu, Tengteng Huang, Bingling Li, Xiwu Chen, Xi Wang, and Xiang Bai.

</span>
<span class="ltx_bibblock">EPNet++: Cascade Bi-Directional Fusion for Multi-modal 3D Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, 45(7):8324–8341, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han.

</span>
<span class="ltx_bibblock">BEVfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2023 IEEE international conference on robotics and automation (ICRA)</span>, pages 2774–2781. IEEE, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Anas Mahmoud, Jordan SK Hu, and Steven L Waslander.

</span>
<span class="ltx_bibblock">Dense Voxel Fusion for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF winter conference on applications of Computer Vision</span>, pages 663–672, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Madhusri Maity, Sriparna Banerjee, and Sheli Sinha Chaudhuri.

</span>
<span class="ltx_bibblock">Faster R-CNN and YOLO based Vehicle Detection: A Survey.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2021 5th international conference on computing methodologies and communication (ICCMC)</span>, pages 1442–1447. IEEE, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl K Wellington.

</span>
<span class="ltx_bibblock">LaserNet: An efficient probabilistic 3D object detector for Autonomous Driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 12677–12686, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka.

</span>
<span class="ltx_bibblock">3D bounding box estimation using deep learning and geometry.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, pages 7074–7082, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Su Pang, Daniel Morris, and Hayder Radha.

</span>
<span class="ltx_bibblock">Clocs: Camera-LiDAR object candidates fusion for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pages 10386–10393. IEEE, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Su Pang, Daniel Morris, and Hayder Radha.

</span>
<span class="ltx_bibblock">Fast-clocs: Fast Camera-LiDAR object candidates fusion for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span>, pages 187–196, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">PointNet: Deep learning on Point sets for 3D classification and segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, pages 652–660, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">PointNet++: Deep hierarchical feature learning on point sets in a metric space.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
LA Rakhsith, KS Anusha, BE Karthik, D Arun Nithish, and V Kishore Kumar.

</span>
<span class="ltx_bibblock">A Survey on Object Detection methods in Deep Learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. of 2021 Second Int. Conf. on Electronics and Sustainable Communication Systems (ICESC)</span>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J Redmon.

</span>
<span class="ltx_bibblock">You Only Look Once: Unified, Real-Time Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Tahira Shehzadi, Khurram Azeem Hashmi, Didier Stricker, and Muhammad Zeshan Afzal.

</span>
<span class="ltx_bibblock">2D Object Detection with Transformers: A Review.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.04670</span>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.

</span>
<span class="ltx_bibblock">PV-RCNN: Point-voxel feature set abstraction for 3DObject Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 10529–10538, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.

</span>
<span class="ltx_bibblock">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 770–779, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al.

</span>
<span class="ltx_bibblock">Scalability in perception for Autonomous Driving: Waymo open dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 2446–2454, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">PointPainting: Sequential Fusion for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 4604–4612, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">PointAugmenting: Cross-modal augmentation for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 11794–11803, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ke Wang, Tianqiang Zhou, Xingcan Li, and Fan Ren.

</span>
<span class="ltx_bibblock">Performance and challenges of 3D Object Detection methods in complex scenes for Autonomous Driving.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 8(2):1699–1716, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Li Wang, Xinyu Zhang, Ziying Song, Jiangfeng Bi, Guoxin Zhang, Haiyue Wei, Liyao Tang, Lei Yang, Jun Li, Caiyan Jia, et al.

</span>
<span class="ltx_bibblock">Multi-modal 3D Object Detection in Autonomous Driving: A Survey and taxonomy.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 8(7):3781–3798, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Xiangheng Wang, Hengyi Li, Xuebin Yue, and Lin Meng.

</span>
<span class="ltx_bibblock">A comprehensive Survey on Object Detection YOLO.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings http://ceur-ws. org ISSN</span>, 1613:0073, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger.

</span>
<span class="ltx_bibblock">Pseudo-LiDAR from visual depth estimation: Bridging the gap in 3D Object Detection for Autonomous Driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 8445–8453, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Zhangjing Wang, Yu Wu, and Qingqing Niu.

</span>
<span class="ltx_bibblock">Multi-Sensor Fusion in Automated Driving: A Survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:2847–2868, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hai Wu, Chenglu Wen, Wei Li, Xin Li, Ruigang Yang, and Cheng Wang.

</span>
<span class="ltx_bibblock">Transformation-equivariant 3D Object Detection for Autonomous Driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 37, pages 2795–2802, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Hai Wu, Chenglu Wen, Shaoshuai Shi, Xin Li, and Cheng Wang.

</span>
<span class="ltx_bibblock">Virtual Sparse Convolution for Multimodal 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 21653–21662, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng Liu, and Deng Cai.

</span>
<span class="ltx_bibblock">Sparse Fuse Dense: Towards high quality 3D Detection with Depth Completion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 5418–5427, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zizhang Wu, Yuanzhu Gan, Lei Wang, Guilian Chen, and Jian Pu.

</span>
<span class="ltx_bibblock">MonoPGC: Monocular 3D Object Detection with pixel geometry contexts.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 4842–4849. IEEE, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.

</span>
<span class="ltx_bibblock">FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</span>, pages 3047–3054. IEEE, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yan Yan, Yuxing Mao, and Bo Li.

</span>
<span class="ltx_bibblock">SECOND: Sparsely Embedded Convolutional Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 18(10):3337, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Tianwei Yin, Xingyi Zhou, and Philipp Kr”̈ahenb”̈uhl.

</span>
<span class="ltx_bibblock">Multimodal Virtual Point 3D Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34:16494–16507, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu Qiao, Hongsheng Li, and Peng Gao.

</span>
<span class="ltx_bibblock">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 9155–9166, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Tingyu Zhang, Zhigang Liang, Yanzhao Yang, Xinyu Yang, Yu Zhu, and Jian Wang.

</span>
<span class="ltx_bibblock">Contrastive Late Fusion for 3D Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2024.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Xiangmo Zhao, Pengpeng Sun, Zhigang Xu, Haigen Min, and Hongkai Yu.

</span>
<span class="ltx_bibblock">Fusion of 3D LiDAR and Camera data for Object Detection in Autonomous Vehicle Applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">IEEE Sensors Journal</span>, 20(9):4901–4913, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yin Zhou and Oncel Tuzel.

</span>
<span class="ltx_bibblock">VoxelNet: End-to-end Learning for Point Cloud based 3D Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, pages 4490–4499, 2018.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye.

</span>
<span class="ltx_bibblock">Object Detection in 20 years: A Survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 111(3):257–276, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.15528" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.15529" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.15529">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.15529" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.15530" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 23:20:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
