<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.06805] Tunable Soft Prompts are Messengers in Federated Learning</title><meta property="og:description" content="Federated learning (FL) enables multiple participants to collaboratively train machine learning models using decentralized data sources, alleviating privacy concerns that arise from directly sharing local data. Howeverâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tunable Soft Prompts are Messengers in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Tunable Soft Prompts are Messengers in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.06805">

<!--Generated on Tue Feb 27 19:21:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Tunable Soft Prompts are Messengers in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenhe Dong<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1,</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution.</span></span></span><math id="id2.2.m2.1" class="ltx_Math" alttext="{}^{\;\,,}" display="inline"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">,</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">,</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\;\,,}</annotation></semantics></math><span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Work done at Alibaba.</span></span></span> â€ƒYuexiang Xie<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">2,</span></sup><span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution.</span></span></span> â€ƒBolin Ding<sup id="id11.11.id3" class="ltx_sup">2</sup> â€ƒYing Shen<sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">1,</span></sup><span id="footnotex4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Corresponding authors.</span></span></span> â€ƒYaliang Li<sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">2,</span></sup><span id="footnotex5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Corresponding authors.</span></span></span>
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic">1</span></sup>Sun Yat-sen University â€ƒ<sup id="id15.15.id7" class="ltx_sup"><span id="id15.15.id7.1" class="ltx_text ltx_font_italic">2</span></sup>Alibaba Group 
<br class="ltx_break">dongchh@mail2.sysu.edu.cn â€ƒ{yuexiang.xyx, bolin.ding, yaliang.li}@alibaba-inc.com 
<br class="ltx_break">sheny76@mail.sysu.edu.cn 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Federated learning (FL) enables multiple participants to collaboratively train machine learning models using decentralized data sources, alleviating privacy concerns that arise from directly sharing local data. However, the lack of model privacy protection in FL becomes an unneglectable challenge, especially when people want to federally finetune models based on a proprietary large language model. In this study, we propose a novel FL training approach that accomplishes information exchange among participants via tunable soft prompts. These soft prompts, updated and transmitted between the server and clients, assume the role of the global model parameters and serve as messengers to deliver useful knowledge from the local data and global model.
As the global model itself is not required to be shared and the local training is conducted based on an auxiliary model with fewer parameters than the global model, the proposed approach provides protection for the global model while reducing communication and computation costs in FL. Extensive experiments show the effectiveness of the proposed approach compared to several baselines.
We have released the source code at <a target="_blank" href="https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large language models (LLMs)Â <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>); Brown etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>); Zhang etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>); Chowdhery etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Scao etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>); Zeng etÂ al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>); OpenAI (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>); Touvron etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> have been witnessed incredible progress in the recent years, which is tied to the support of large amounts of training corpus and computation resources. To further promote the development of LLMs and broaden their applications, how to make good use of private and decentralized data can be one of the critical steps, which brings both opportunities and challenges to federated learning (FL)Â <cite class="ltx_cite ltx_citemacro_cite">KoneÄná»³ etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>); McMahan etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2017</a>); Yang etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>); Kairouz etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The main idea of FL is that multiple participants collectively train a global model using their local data, and the updates generated by each participantâ€™s local training process are then aggregated together to optimize the global model.
On the one hand, FL offers a feasible solution for finetuning LLMs by utilizing multiple data sources without private data leakage. Participants are allowed to conduct local training processes based on their private data, and then share the learned knowledge through the exchange of model updates. In this way, privacy concerns, raised by directly sharing private data, can be alleviated.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, on the other hand, the application of FL can be constrained by concerns regarding model privacy, especially when the finetuning process relies on proprietary LLMs. In particular, during each training round, the up-to-date global model would be distributed to participants, which goes against the interests of model owners and might be deemed unacceptable in real-world scenarios.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Such conflicts between protecting data privacy and model privacy are attributed to the training paradigm of FL, which involves sharing model parameters to accomplish the knowledge exchange process for collaboratively training models. To address this challenge, we propose <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>, a novel FL training approach that leverages <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">tunable soft prompts</span> to enable the exchange of useful knowledge among participants, achieving both data privacy and model privacy protection at the same time.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Soft prompts can be regarded as some additional tunable parameters plugged into the model, which are updated to capture knowledge from downstream tasks while keeping the model parameters frozen in <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">parameter-efficient finetuning</span> (PEFT) algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">Houlsby etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>); Lester etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>); Hu etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Zaken etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>. We incorporate these tunable soft prompts in FL, serving as messengers among participants to deliver knowledge learned from local data and contained in the global model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Specifically, at the start of an FL course, a server broadcasts an auxiliary model (which typically has much fewer parameters than the global model) to participating clients. Then in each training round, the server sends up-to-date tunable soft prompts to selected clients, and these clients combine the received soft prompts with their maintained auxiliary models. After that, each client performs local training to accomplish the process consisting of (i) <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Global Model Alignment</span>, in which clients update the auxiliary models to align them with the representative capabilities of the global model, using the up-to-date soft prompts; and (ii) <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">Local Knowledge Capturing</span>, in which clients freeze their auxiliary models and finetune the soft prompts to capture useful knowledge from their local data. These updated soft prompts are sent back to the server once the local training is complete. The server is responsible for aggregating and optimizing these soft prompts to drive the next training round.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In contrast to the existing FL training paradigm that entails sharing the parameters of the global model, the proposed <span id="S1.p7.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> suggests exchanging tunable soft prompts during training rounds, which ensures privacy protection for the global model.
Meanwhile, clients are only expected to update the auxiliary models and soft prompts in the local training process, which mitigates both computation and communication overhead compared to federally finetuning of large global models like LLMs.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.2" class="ltx_p">We conduct a series of experiments with two LLMs (GPT2-XL and OPT-1.3B) on seven benchmarking datasets, showing that <span id="S1.p8.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span> achieves competitive performance compared to baseline methods while significantly reducing the model size by 14.5<math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><times id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\times</annotation></semantics></math>/8.5<math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><times id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\times</annotation></semantics></math> on GPT2-XL/OPT-1.3B.
These experimental results demonstrate the effectiveness and advantages of the proposed idea that using tunable soft prompts as knowledge messengers in FL.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminary</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Because of the huge model size (billions of parameters), large language models (LLMs)Â <cite class="ltx_cite ltx_citemacro_cite">Chowdhery etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Zeng etÂ al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>); OpenAI (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>); Touvron etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Zhao etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2023</a>); Chen etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> are usually kept at the cloud server who owns adequate resources for inferring, finetuning, and training. Nowadays, users of LLMs have to send the instructions to the cloud server and wait for the model-generated results, or upload their private data<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://platform.openai.com/docs/guides/fine-tuning</span></span></span> for finetuning the LLMs on their downstream tasks. The server can also benefit from the user instructions and human feedbacks to continually improve the model performance.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Such usage of LLMs might bring privacy issues from two different perspectives. For the users of LLMs, they might not be allowed to upload their private data to the server, especially in some scenarios with highly sensitive information such as health care, so-called <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">data privacy</span>. For the cloud servers (i.e., the model owners), they tend to keep the proprietary language model private to ensure their interests, so-called <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">model privacy</span>.
As a result, how to provide protection for both data privacy and model privacy is a practical and challenging problem when training, finetuning, and deploying LLMs in real-world applications.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Federated Learning (FL)Â <cite class="ltx_cite ltx_citemacro_cite">KoneÄná»³ etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>); McMahan etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2017</a>)</cite> is one of the considered solutions for alleviating the aforementioned data privacy issue. Different from the centralized training paradigm that needs to gather usersâ€™ local data for training a global model, FL proposes to aggregate participantsâ€™ model updates to avoid directly sharing the private data.
To be more specific, at each training round, a server broadcasts the up-to-date global model to the participating clients. Each client updates the received global model based on its local data, and then sends the model updates back to the server for performing federated aggregation. Formally, the federated aggregation at the <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">t</annotation></semantics></math>-th training round can be defined as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\boldsymbol{w}_{g}^{t}=\sum_{k=1}^{K}\frac{N_{k}}{N}\boldsymbol{w}_{k}^{t}," display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><msubsup id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.cmml">ğ’˜</mi><mi id="S2.E1.m1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.3.cmml">g</mi><mi id="S2.E1.m1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo rspace="0.111em" id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><munderover id="S2.E1.m1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.1.1.3.1.2.2" xref="S2.E1.m1.1.1.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.1.1.1.1.3.1.2.3" xref="S2.E1.m1.1.1.1.1.3.1.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.1.2.3.2" xref="S2.E1.m1.1.1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S2.E1.m1.1.1.1.1.3.1.2.3.1" xref="S2.E1.m1.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.1.1.3.1.2.3.3" xref="S2.E1.m1.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.1.1.1.1.3.1.3" xref="S2.E1.m1.1.1.1.1.3.1.3.cmml">K</mi></munderover><mrow id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml"><mfrac id="S2.E1.m1.1.1.1.1.3.2.2" xref="S2.E1.m1.1.1.1.1.3.2.2.cmml"><msub id="S2.E1.m1.1.1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.1.1.3.2.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2.2.2.2" xref="S2.E1.m1.1.1.1.1.3.2.2.2.2.cmml">N</mi><mi id="S2.E1.m1.1.1.1.1.3.2.2.2.3" xref="S2.E1.m1.1.1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S2.E1.m1.1.1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.1.1.3.2.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.3.2.1" xref="S2.E1.m1.1.1.1.1.3.2.1.cmml">â€‹</mo><msubsup id="S2.E1.m1.1.1.1.1.3.2.3" xref="S2.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2.3.2.2" xref="S2.E1.m1.1.1.1.1.3.2.3.2.2.cmml">ğ’˜</mi><mi id="S2.E1.m1.1.1.1.1.3.2.3.2.3" xref="S2.E1.m1.1.1.1.1.3.2.3.2.3.cmml">k</mi><mi id="S2.E1.m1.1.1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.1.1.3.2.3.3.cmml">t</mi></msubsup></mrow></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"></eq><apply id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2">ğ’˜</ci><ci id="S2.E1.m1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.3">ğ‘”</ci></apply><ci id="S2.E1.m1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><apply id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.1.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.3.1.2.cmml" xref="S2.E1.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.1.1.3.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.1.2.2"></sum><apply id="S2.E1.m1.1.1.1.1.3.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.1.2.3"><eq id="S2.E1.m1.1.1.1.1.3.1.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1.2.3.1"></eq><ci id="S2.E1.m1.1.1.1.1.3.1.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.1.2.3.2">ğ‘˜</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.3.1.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.1.1.1.1.3.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3.1.3">ğ¾</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2"><times id="S2.E1.m1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.1"></times><apply id="S2.E1.m1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2"><divide id="S2.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2"></divide><apply id="S2.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2.2.2">ğ‘</ci><ci id="S2.E1.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2.2.3">ğ‘˜</ci></apply><ci id="S2.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2.3">ğ‘</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.2.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.2.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3.2.2">ğ’˜</ci><ci id="S2.E1.m1.1.1.1.1.3.2.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3.2.3">ğ‘˜</ci></apply><ci id="S2.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3.3">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\boldsymbol{w}_{g}^{t}=\sum_{k=1}^{K}\frac{N_{k}}{N}\boldsymbol{w}_{k}^{t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p3.6" class="ltx_p">where <math id="S2.p3.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p3.2.m1.1a"><mi id="S2.p3.2.m1.1.1" xref="S2.p3.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m1.1b"><ci id="S2.p3.2.m1.1.1.cmml" xref="S2.p3.2.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m1.1c">K</annotation></semantics></math> is the number of clients, <math id="S2.p3.3.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p3.3.m2.1a"><mi id="S2.p3.3.m2.1.1" xref="S2.p3.3.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m2.1b"><ci id="S2.p3.3.m2.1.1.cmml" xref="S2.p3.3.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m2.1c">N</annotation></semantics></math> is the total amount of training data, <math id="S2.p3.4.m3.1" class="ltx_Math" alttext="\boldsymbol{w}_{k}" display="inline"><semantics id="S2.p3.4.m3.1a"><msub id="S2.p3.4.m3.1.1" xref="S2.p3.4.m3.1.1.cmml"><mi id="S2.p3.4.m3.1.1.2" xref="S2.p3.4.m3.1.1.2.cmml">ğ’˜</mi><mi id="S2.p3.4.m3.1.1.3" xref="S2.p3.4.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.4.m3.1b"><apply id="S2.p3.4.m3.1.1.cmml" xref="S2.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m3.1.1.1.cmml" xref="S2.p3.4.m3.1.1">subscript</csymbol><ci id="S2.p3.4.m3.1.1.2.cmml" xref="S2.p3.4.m3.1.1.2">ğ’˜</ci><ci id="S2.p3.4.m3.1.1.3.cmml" xref="S2.p3.4.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m3.1c">\boldsymbol{w}_{k}</annotation></semantics></math> is the updated model parameters of the <math id="S2.p3.5.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p3.5.m4.1a"><mi id="S2.p3.5.m4.1.1" xref="S2.p3.5.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p3.5.m4.1b"><ci id="S2.p3.5.m4.1.1.cmml" xref="S2.p3.5.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m4.1c">k</annotation></semantics></math>-th client, and <math id="S2.p3.6.m5.1" class="ltx_Math" alttext="\boldsymbol{w}_{g}" display="inline"><semantics id="S2.p3.6.m5.1a"><msub id="S2.p3.6.m5.1.1" xref="S2.p3.6.m5.1.1.cmml"><mi id="S2.p3.6.m5.1.1.2" xref="S2.p3.6.m5.1.1.2.cmml">ğ’˜</mi><mi id="S2.p3.6.m5.1.1.3" xref="S2.p3.6.m5.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.6.m5.1b"><apply id="S2.p3.6.m5.1.1.cmml" xref="S2.p3.6.m5.1.1"><csymbol cd="ambiguous" id="S2.p3.6.m5.1.1.1.cmml" xref="S2.p3.6.m5.1.1">subscript</csymbol><ci id="S2.p3.6.m5.1.1.2.cmml" xref="S2.p3.6.m5.1.1.2">ğ’˜</ci><ci id="S2.p3.6.m5.1.1.3.cmml" xref="S2.p3.6.m5.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m5.1c">\boldsymbol{w}_{g}</annotation></semantics></math> is the aggregated global model parameters.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Nevertheless, as the global model is required to be shared in the training process of FL, the model privacy issue has not been well addressed yet, which motivates us to make an improvement in providing both data privacy protection and model privacy protection in FL.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2311.06805/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="433" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overall architecture of the proposed <span id="S2.F1.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span>.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe the details of the proposed <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>.
The intuition behind <span id="S3.p1.1.2" class="ltx_text ltx_font_smallcaps">FedSP</span> is to adopt some tunable soft prompts to replace the shared global model in the training process of FL, which serves as messengers to deliver useful knowledge in the local data and the global model to the server and clients, as introduced in SectionÂ <a href="#S3.SS1" title="3.1 Tunable Soft Prompts â€£ 3 Methodology â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
With the help of these tunable soft prompts, clients are expected to perform local training for updating an auxiliary model and the soft prompts alternatively, and send the updated soft prompts to the servers for sharing the knowledge learned from their local data (more details in SectionÂ <a href="#S3.SS2" title="3.2 Training Procedure â€£ 3 Methodology â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
The overall architecture of the proposed <span id="S3.p1.1.3" class="ltx_text ltx_font_smallcaps">FedSP</span> is illustrated in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Preliminary â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tunable Soft Prompts</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The existing FL algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>); Kairouz etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> leverage the global model parameters to exchange the useful knowledge learned from participantsâ€™ local data. However, it might raise the model privacy issue when the training process is conducted on a private model, such as finetuning a proprietary large language model on downstream tasks. Inspired by the parameter-efficient finetuning (PEFT) algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>, we propose to adopt tunable soft prompts to tackle such a model privacy issue.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">These soft prompts are continuous vectors added to the LLMs, serving as instructive contexts to influence the generation process of LLMs by steering the probability distribution of the next token. By optimizing these soft prompts over the training datasets via backward propagation, the knowledge contained in the data can be captured and condensed into their parameters.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In this study, we follow the tunable soft prompts proposed by <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span>Â <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>.
The tunable soft prompts would be updated and transmitted between the server and clients for exchanging useful knowledge. Note that soft prompts should be plugged into a model first, and are updated based on the training data while other parameters of this model are kept frozen. For the server that owns the global model, it is straightforward and reasonable to add tunable soft prompts to the global model. However, adding and maintaining soft prompts for the clients becomes challenging, since clients could not access the global model due to the model privacy issue.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In order to tackle such a challenge, each client is expected to integrate the tunable soft prompts with an auxiliary model and perform an alternative local training process. Before we introduce more details of the local training, we first pay more attention to the auxiliary model. In the proposed <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>, the auxiliary model should be tied to the global model but could not cause model privacy leakage, and had better be a shadow model that has fewer parameters than the global model considering data quantity and computation resources of a client can be limited in some application scenarios.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Motivated by the aforementioned requirements, we construct the auxiliary model by substantially reducing the depth of the global model and applying the cross-layer parameter sharing, inspired by the main idea of AlbertÂ <cite class="ltx_cite ltx_citemacro_cite">Lan etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. Given that most of the LLMs adopt the transformer-based architectureÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>, such a design of the auxiliary model is suitable for plugging and tuning the soft prompts shared by the server.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.3" class="ltx_p">The example illustrated in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Preliminary â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a) shows that, the server owns a global model with total <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mi id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">L</annotation></semantics></math> layers and each layer is concatenated with some prefix prompts, while each client keeps an auxiliary model with only <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mn id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><cn type="integer" id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">1</annotation></semantics></math> layer and plugged with <math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mi id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><ci id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">L</annotation></semantics></math>-layer prefix prompts.
Using the auxiliary model, clients only need to update the tunable soft prompts based on their local data and exchange the updated soft prompts with others to accomplish the FL course. Thus the proposed <span id="S3.SS1.p6.3.1" class="ltx_text ltx_font_smallcaps">FedSP</span> provides privacy protection for the global model as the global model is only maintained by the server and wonâ€™t be shared.
Meanwhile, clients need fewer computation and communication resources for updating the auxiliary model and soft prompts in the local training process compared to those of training the global model as in previous studies.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">In the rest of this section, we describe the local training process of clients in the proposed <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>, showing how to deliver useful knowledge via updating and exchanging the tunable soft prompts.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Procedure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">At the beginning of an FL course, the server first initializes the auxiliary model according to its global model, and broadcasts the generated auxiliary model to all the participating clients.
Then the server sends the up-to-date soft prompts to the selected clients at each training round, while the clients finetune the received soft prompts and also update the auxiliary model accordingly.
The updated soft prompts are uploaded to the server for performing federated aggregation, and then the aggregated soft prompts are plugged into the global model and further optimized by the server. Finally, the up-to-date soft prompts are sent to clients to start a new training round.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Note that compared to the existing FL training paradigms that exchange the global model, in the proposed <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>, the models kept at the server (i.e., the global model) and clients (i.e., the auxiliary models) have different model sizes but are plugged with the same prompts. As a result, to narrow the optimization gap and make the federal training process meaningful, we propose to provide a good initialization of the auxiliary model and perform an alternative local training process for the clients.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Initialization</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">The different model sizes between the server and clients might lead to the misaligned representation, and further cause the updating of the soft prompts mismatch between the server and clients. Such a mismatch seriously hinders effective knowledge exchange between the server and clients in an FL course.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.2" class="ltx_p">To alleviate such a mismatch, the server leverages knowledge distillation (KD)Â <cite class="ltx_cite ltx_citemacro_cite">Hinton etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2015</a>)</cite> techniques to initialize the auxiliary models before sharing them with clients. The main idea is to align the representations of
the student models (i.e., auxiliary models) with the large teacher model (i.e., the global model), which is widely used in previous studiesÂ <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>); Xiao etÂ al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> for representation alignment.
Formally, given the last hidden states of the teacher model <math id="S3.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{H}^{T}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.1a"><msup id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">ğ‡</mi><mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.2">ğ‡</ci><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.1c">\mathbf{H}^{T}</annotation></semantics></math> and the student model <math id="S3.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{H}^{S}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.2.m2.1a"><msup id="S3.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml">ğ‡</mi><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">S</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2">ğ‡</ci><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.2.m2.1c">\mathbf{H}^{S}</annotation></semantics></math>, the loss function of KD can be defined as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mbox{MSE}(\mathbf{H}^{T},\mathbf{W}^{S}\mathbf{H}^{S})," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">â„’</mi><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mtext id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4a.cmml">MSE</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">(</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">ğ‡</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">T</mi></msup><mo id="S3.E2.m1.1.1.1.1.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.cmml"><msup id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml">ğ–</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml">S</mi></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml">â€‹</mo><msup id="S3.E2.m1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.2.cmml">ğ‡</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.3.cmml">S</mi></msup></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.5" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></eq><ci id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4">â„’</ci><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></times><ci id="S3.E2.m1.1.1.1.1.2.4a.cmml" xref="S3.E2.m1.1.1.1.1.2.4"><mtext id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4">MSE</mtext></ci><interval closure="open" id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ğ‡</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">ğ‘‡</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.1"></times><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2">ğ–</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3">ğ‘†</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.2">ğ‡</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.3">ğ‘†</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}=\mbox{MSE}(\mathbf{H}^{T},\mathbf{W}^{S}\mathbf{H}^{S}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px1.p2.4" class="ltx_p">where <span id="S3.SS2.SSS0.Px1.p2.4.1" class="ltx_text ltx_markedasmath">MSE</span> is the <span id="S3.SS2.SSS0.Px1.p2.4.2" class="ltx_text ltx_font_italic">mean square error</span> function, <math id="S3.SS2.SSS0.Px1.p2.4.m2.1" class="ltx_Math" alttext="\mathbf{W}^{S}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.4.m2.1a"><msup id="S3.SS2.SSS0.Px1.p2.4.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.cmml">ğ–</mi><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.cmml">S</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.4.m2.1b"><apply id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2">ğ–</ci><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.4.m2.1c">\mathbf{W}^{S}</annotation></semantics></math> is a learnable transformation matrix.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p3.1" class="ltx_p">Notably, the tunable soft prompts are not added to the global model and auxiliary models during the KD process. And the aforementioned KD process is only performed by the server at the beginning of an FL course, whose computation cost is affordable.
After initialization, the auxiliary models are broadcast to all participating clients and updated by clients according to their local data independently.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Alternative Local Training</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">When receiving the auxiliary models, which have fewer layer numbers compared to the global model, the clients apply the cross-layer parameter sharingÂ <cite class="ltx_cite ltx_citemacro_cite">Lan etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> to imitate the auxiliary models as the global model in the local training process. Each shared layer is concatenated with the corresponding tunable soft prompts. In this way, the tunable soft prompts in the server and clients are served in the same manner in an FL course.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The comparisons between the proposed <span id="S3.T1.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and baselines with GPT2-XL.</figcaption>
<div id="S3.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:156.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.0pt,11.6pt) scale(0.871304954373163,0.871304954373163) ;">
<table id="S3.T1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<td id="S3.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S3.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.2.1" class="ltx_text ltx_font_bold">ARC-C</span></td>
<td id="S3.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.3.1" class="ltx_text ltx_font_bold">ARC-E</span></td>
<td id="S3.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.4.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S3.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.5.1" class="ltx_text ltx_font_bold">OpenBookQA</span></td>
<td id="S3.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.6.1" class="ltx_text ltx_font_bold">PIQA</span></td>
<td id="S3.T1.3.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.7.1" class="ltx_text ltx_font_bold">RACE</span></td>
<td id="S3.T1.3.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.1.8.1" class="ltx_text ltx_font_bold">SciQ</span></td>
</tr>
<tr id="S3.T1.3.1.2" class="ltx_tr">
<td id="S3.T1.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.2.1.1" class="ltx_text ltx_font_smallcaps">Zero-Shot</span></td>
<td id="S3.T1.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">25.1</td>
<td id="S3.T1.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">58.2</td>
<td id="S3.T1.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">40.0</td>
<td id="S3.T1.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">23.0</td>
<td id="S3.T1.3.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">70.9</td>
<td id="S3.T1.3.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">33.0</td>
<td id="S3.T1.3.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">83.2</td>
</tr>
<tr id="S3.T1.3.1.3" class="ltx_tr">
<td id="S3.T1.3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.3.1.1" class="ltx_text ltx_font_smallcaps">Finetune</span></td>
<td id="S3.T1.3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">30.0</td>
<td id="S3.T1.3.1.3.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">62.9</td>
<td id="S3.T1.3.1.3.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.7</td>
<td id="S3.T1.3.1.3.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">30.0</td>
<td id="S3.T1.3.1.3.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">73.2</td>
<td id="S3.T1.3.1.3.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">43.2</td>
<td id="S3.T1.3.1.3.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">92.5</td>
</tr>
<tr id="S3.T1.3.1.4" class="ltx_tr">
<td id="S3.T1.3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.4.1.1" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span></td>
<td id="S3.T1.3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">28.2</td>
<td id="S3.T1.3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">58.9</td>
<td id="S3.T1.3.1.4.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.4</td>
<td id="S3.T1.3.1.4.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">25.6</td>
<td id="S3.T1.3.1.4.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">72.3</td>
<td id="S3.T1.3.1.4.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">38.2</td>
<td id="S3.T1.3.1.4.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">92.9</td>
</tr>
<tr id="S3.T1.3.1.5" class="ltx_tr">
<td id="S3.T1.3.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt</span></td>
<td id="S3.T1.3.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.2.1" class="ltx_text ltx_font_bold">27.5</span></td>
<td id="S3.T1.3.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">59.4</td>
<td id="S3.T1.3.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">40.7</td>
<td id="S3.T1.3.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.5.1" class="ltx_text ltx_font_bold">26.2</span></td>
<td id="S3.T1.3.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.6.1" class="ltx_text ltx_font_bold">71.9</span></td>
<td id="S3.T1.3.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.7.1" class="ltx_text ltx_font_bold">38.3</span></td>
<td id="S3.T1.3.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.5.8.1" class="ltx_text ltx_font_bold">92.8</span></td>
</tr>
<tr id="S3.T1.3.1.6" class="ltx_tr">
<td id="S3.T1.3.1.6.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.6.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt-Single</span></td>
<td id="S3.T1.3.1.6.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">20.1</td>
<td id="S3.T1.3.1.6.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">38.3</td>
<td id="S3.T1.3.1.6.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">35.0</td>
<td id="S3.T1.3.1.6.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">12.4</td>
<td id="S3.T1.3.1.6.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">63.9</td>
<td id="S3.T1.3.1.6.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">31.6</td>
<td id="S3.T1.3.1.6.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">72.3</td>
</tr>
<tr id="S3.T1.3.1.7" class="ltx_tr">
<td id="S3.T1.3.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">
<span id="S3.T1.3.1.7.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> (ours)</td>
<td id="S3.T1.3.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">26.5</td>
<td id="S3.T1.3.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.7.3.1" class="ltx_text ltx_font_bold">61.2</span></td>
<td id="S3.T1.3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.7.4.1" class="ltx_text ltx_font_bold">40.9</span></td>
<td id="S3.T1.3.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">24.2</td>
<td id="S3.T1.3.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">71.0</td>
<td id="S3.T1.3.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">35.2</td>
<td id="S3.T1.3.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T1.3.1.7.8.1" class="ltx_text ltx_font_bold">92.8</span></td>
</tr>
<tr id="S3.T1.3.1.8" class="ltx_tr">
<td id="S3.T1.3.1.8.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S3.T1.3.1.8.1.1" class="ltx_text ltx_font_italic">w/o</span> KD</td>
<td id="S3.T1.3.1.8.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">17.8</td>
<td id="S3.T1.3.1.8.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">41.4</td>
<td id="S3.T1.3.1.8.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.1</td>
<td id="S3.T1.3.1.8.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">13.0</td>
<td id="S3.T1.3.1.8.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">62.2</td>
<td id="S3.T1.3.1.8.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">36.7</td>
<td id="S3.T1.3.1.8.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">84.4</td>
</tr>
<tr id="S3.T1.3.1.9" class="ltx_tr">
<td id="S3.T1.3.1.9.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S3.T1.3.1.9.1.1" class="ltx_text ltx_font_italic">w/o</span> CS</td>
<td id="S3.T1.3.1.9.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">21.0</td>
<td id="S3.T1.3.1.9.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">44.6</td>
<td id="S3.T1.3.1.9.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.0</td>
<td id="S3.T1.3.1.9.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">10.6</td>
<td id="S3.T1.3.1.9.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">66.6</td>
<td id="S3.T1.3.1.9.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">37.2</td>
<td id="S3.T1.3.1.9.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">89.2</td>
</tr>
<tr id="S3.T1.3.1.10" class="ltx_tr">
<td id="S3.T1.3.1.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S3.T1.3.1.10.1.1" class="ltx_text ltx_font_italic">w/o</span> AT</td>
<td id="S3.T1.3.1.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">25.6</td>
<td id="S3.T1.3.1.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">56.4</td>
<td id="S3.T1.3.1.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">37.1</td>
<td id="S3.T1.3.1.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">13.4</td>
<td id="S3.T1.3.1.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">69.7</td>
<td id="S3.T1.3.1.10.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">34.3</td>
<td id="S3.T1.3.1.10.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">81.0</td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">During the local training process, clients adopt an alternative training method to achieve both <span id="S3.SS2.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">global model alignment</span> and <span id="S3.SS2.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">local knowledge capturing</span>, as shown in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Preliminary â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b).
To be more specific, firstly clients concatenate the received soft prompts with the local auxiliary models, and finetune the parameters of the auxiliary models while freezing the soft prompts. Since these soft prompts have been plugged into the global model and updated by the server before being shared, the purpose of freezing soft prompts and updating the auxiliary models is to align the representation of auxiliary models and the global model, with the help of the same soft prompts. Such an alignment is a necessary step in local training as the misaligned might cause a mismatch of the soft prompts between the server and clients.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p">After the global model alignment, clients perform prompt tunning as those used in PEFT algorithms, which implies that clients freeze the auxiliary models and only finetune the soft prompts for learning useful knowledge from local data, so-called local knowledge capturing. These updated soft prompts are sent back to the server, serving as messengers to deliver useful knowledge learned from participantsâ€™ local data.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The comparisons between the proposed <span id="S4.T2.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and baselines with OPT-1.3B.</figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:156.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.0pt,11.6pt) scale(0.871304954373163,0.871304954373163) ;">
<table id="S4.T2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<td id="S4.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">ARC-C</span></td>
<td id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">ARC-E</span></td>
<td id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.5.1" class="ltx_text ltx_font_bold">OpenBookQA</span></td>
<td id="S4.T2.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.6.1" class="ltx_text ltx_font_bold">PIQA</span></td>
<td id="S4.T2.3.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.7.1" class="ltx_text ltx_font_bold">RACE</span></td>
<td id="S4.T2.3.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.1.8.1" class="ltx_text ltx_font_bold">SciQ</span></td>
</tr>
<tr id="S4.T2.3.1.2" class="ltx_tr">
<td id="S4.T2.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.2.1.1" class="ltx_text ltx_font_smallcaps">Zero-Shot</span></td>
<td id="S4.T2.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">23.5</td>
<td id="S4.T2.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">56.9</td>
<td id="S4.T2.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">41.5</td>
<td id="S4.T2.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">23.4</td>
<td id="S4.T2.3.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">71.5</td>
<td id="S4.T2.3.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">34.2</td>
<td id="S4.T2.3.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">84.4</td>
</tr>
<tr id="S4.T2.3.1.3" class="ltx_tr">
<td id="S4.T2.3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.3.1.1" class="ltx_text ltx_font_smallcaps">Finetune</span></td>
<td id="S4.T2.3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">27.7</td>
<td id="S4.T2.3.1.3.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">61.3</td>
<td id="S4.T2.3.1.3.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">42.7</td>
<td id="S4.T2.3.1.3.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">31.4</td>
<td id="S4.T2.3.1.3.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">75.2</td>
<td id="S4.T2.3.1.3.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">37.0</td>
<td id="S4.T2.3.1.3.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">92.5</td>
</tr>
<tr id="S4.T2.3.1.4" class="ltx_tr">
<td id="S4.T2.3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.4.1.1" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span></td>
<td id="S4.T2.3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">27.4</td>
<td id="S4.T2.3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">57.7</td>
<td id="S4.T2.3.1.4.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">41.9</td>
<td id="S4.T2.3.1.4.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">27.2</td>
<td id="S4.T2.3.1.4.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">73.2</td>
<td id="S4.T2.3.1.4.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">38.5</td>
<td id="S4.T2.3.1.4.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">89.6</td>
</tr>
<tr id="S4.T2.3.1.5" class="ltx_tr">
<td id="S4.T2.3.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.5.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt</span></td>
<td id="S4.T2.3.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.5.2.1" class="ltx_text ltx_font_bold">26.8</span></td>
<td id="S4.T2.3.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">57.6</td>
<td id="S4.T2.3.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">41.8</td>
<td id="S4.T2.3.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.5.5.1" class="ltx_text ltx_font_bold">27.4</span></td>
<td id="S4.T2.3.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.5.6.1" class="ltx_text ltx_font_bold">72.9</span></td>
<td id="S4.T2.3.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.5.7.1" class="ltx_text ltx_font_bold">38.4</span></td>
<td id="S4.T2.3.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">89.8</td>
</tr>
<tr id="S4.T2.3.1.6" class="ltx_tr">
<td id="S4.T2.3.1.6.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.6.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt-Single</span></td>
<td id="S4.T2.3.1.6.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">24.6</td>
<td id="S4.T2.3.1.6.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">54.4</td>
<td id="S4.T2.3.1.6.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">38.3</td>
<td id="S4.T2.3.1.6.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">19.0</td>
<td id="S4.T2.3.1.6.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">68.8</td>
<td id="S4.T2.3.1.6.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">30.4</td>
<td id="S4.T2.3.1.6.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">80.5</td>
</tr>
<tr id="S4.T2.3.1.7" class="ltx_tr">
<td id="S4.T2.3.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">
<span id="S4.T2.3.1.7.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> (ours)</td>
<td id="S4.T2.3.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.7.2.1" class="ltx_text ltx_font_bold">26.8</span></td>
<td id="S4.T2.3.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.7.3.1" class="ltx_text ltx_font_bold">58.0</span></td>
<td id="S4.T2.3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.7.4.1" class="ltx_text ltx_font_bold">42.1</span></td>
<td id="S4.T2.3.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">26.0</td>
<td id="S4.T2.3.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">71.9</td>
<td id="S4.T2.3.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">36.6</td>
<td id="S4.T2.3.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.3.1.7.8.1" class="ltx_text ltx_font_bold">89.9</span></td>
</tr>
<tr id="S4.T2.3.1.8" class="ltx_tr">
<td id="S4.T2.3.1.8.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S4.T2.3.1.8.1.1" class="ltx_text ltx_font_italic">w/o</span> KD</td>
<td id="S4.T2.3.1.8.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">22.1</td>
<td id="S4.T2.3.1.8.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">53.5</td>
<td id="S4.T2.3.1.8.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.8</td>
<td id="S4.T2.3.1.8.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">17.6</td>
<td id="S4.T2.3.1.8.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">68.0</td>
<td id="S4.T2.3.1.8.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">32.2</td>
<td id="S4.T2.3.1.8.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">80.7</td>
</tr>
<tr id="S4.T2.3.1.9" class="ltx_tr">
<td id="S4.T2.3.1.9.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S4.T2.3.1.9.1.1" class="ltx_text ltx_font_italic">w/o</span> CS</td>
<td id="S4.T2.3.1.9.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">24.3</td>
<td id="S4.T2.3.1.9.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">53.5</td>
<td id="S4.T2.3.1.9.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.6</td>
<td id="S4.T2.3.1.9.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">18.0</td>
<td id="S4.T2.3.1.9.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">69.3</td>
<td id="S4.T2.3.1.9.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">36.0</td>
<td id="S4.T2.3.1.9.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">76.5</td>
</tr>
<tr id="S4.T2.3.1.10" class="ltx_tr">
<td id="S4.T2.3.1.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">Â Â Â Â Â <span id="S4.T2.3.1.10.1.1" class="ltx_text ltx_font_italic">w/o</span> AT</td>
<td id="S4.T2.3.1.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">26.2</td>
<td id="S4.T2.3.1.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">56.4</td>
<td id="S4.T2.3.1.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">35.1</td>
<td id="S4.T2.3.1.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">21.8</td>
<td id="S4.T2.3.1.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">71.0</td>
<td id="S4.T2.3.1.10.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">33.2</td>
<td id="S4.T2.3.1.10.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">89.8</td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models and Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conduct a series of experiments with two popular large language models, i.e., GPT2-XLÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> and OPT-1.3BÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>. Specifically, both GPT2-XL and OPT-1.3B adopt the transformer-based architectureÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>. GPT2-XL has 48 layers and 1.5 billion parameters, and OPT-1.3B has 24 layers and 1.3 billion parameters. For the benchmarking datasets, we adopt seven question-answering datasets for quantitative analysis, including ARC-C/EÂ <cite class="ltx_cite ltx_citemacro_cite">Clark etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>, HellaSwagÂ <cite class="ltx_cite ltx_citemacro_cite">Zellers etÂ al. (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>, OpenBookQAÂ <cite class="ltx_cite ltx_citemacro_cite">Mihaylov etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>, PIQAÂ <cite class="ltx_cite ltx_citemacro_cite">Bisk etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, RACEÂ <cite class="ltx_cite ltx_citemacro_cite">Lai etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, and SciQÂ <cite class="ltx_cite ltx_citemacro_cite">Welbl etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite>.
We use accuracy as the evaluation metric in the experiments.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baselines</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare the proposed <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> with five different kinds of baseline methods, including:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Zero-Shot</span>, which directly evaluates the pre-trained language models without updating based on the downstream datasets.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Finetune</span>, which finetunes the entire LLMs on the downstream datasets, respectively.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Prefix-Tuning</span>Â <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>, which adds tunable soft prompts and only finetunes them on the downstream datasets, respectively. Note that the parameters of LLMs are frozen during the finetuning process.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">FedPrompt</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>, which applies <span id="S4.I1.i4.p1.1.2" class="ltx_text ltx_font_smallcaps">Prefix-tuning</span> in the field of FL. Clients need to keep both the global model and the soft prompts and only update and exchange the soft prompts. Note that the parameters of the global model are frozen.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">FedPrompt-Single</span>, a variant of <span id="S4.I1.i5.p1.1.2" class="ltx_text ltx_font_smallcaps">FedPrompt</span>, which reduces the layer number of clientsâ€™ global model to 1 to ensure the privacy protection of the global model. This setting is similar to that of <span id="S4.I1.i5.p1.1.3" class="ltx_text ltx_font_smallcaps">FedSP</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">The experiments are conducted on eight NVIDIA GeForce RTX 3090 GPUs.
We implement the proposed approach and baseline methods based on Huggingface TransformersÂ <cite class="ltx_cite ltx_citemacro_cite">Wolf etÂ al. (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> and FederatedScopeÂ <cite class="ltx_cite ltx_citemacro_cite">Xie etÂ al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>, and conduct the evaluation with the support of <span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_typewriter">lm-eval<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_serif">https://github.com/EleutherAI/lm-evaluation-harness</span></span></span></span></span>.
Following previous studyÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>, we divide each dataset into 10 partitions, and each client owns one of them. A linear decay learning rate scheduler with a warm-up proportion of 0.1 is used in the experiments. We adopt AdamWÂ <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> as the optimizer with <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><msub id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">Î²</mi><mn id="S4.SS3.p1.1.m1.1.1.2.3" xref="S4.SS3.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><eq id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></eq><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><msub id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2.cmml">Î²</mi><mn id="S4.SS3.p1.2.m2.1.1.2.3" xref="S4.SS3.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><eq id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></eq><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.p1.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\beta_{2}=0.999</annotation></semantics></math>, the weight decay is set to 0.01, and the batch size is set to 16. The layer number of the auxiliary model used in <span id="S4.SS3.p1.2.2" class="ltx_text ltx_font_smallcaps">FedSP</span> is set to 1.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We conduct the grid search for the optimal hyperparameters. Specifically, the learning rate is tuned in {1e-4, 2e-4, 5e-4}, the training round is tuned in {20, 50, 100, 200}, and the local training step is tuned in {10, 20}. For the methods that adopt prefix-tunning, we set the dimension of the tunable soft prompts to 40. And we conduct knowledge distillation for 5000 training steps with an initial learning rate of 5e-4. Inspired by previous studyÂ <cite class="ltx_cite ltx_citemacro_citet">Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>, we adopt the reparametrization strategy with a hidden size of 512 on HellaSwag, RACE, and SciQ datasets.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Model performance w.r.t. different selected layers of the auxiliary models.</figcaption>
<div id="S4.T3.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:117.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.4pt,4.4pt) scale(0.929726868366842,0.929726868366842) ;">
<table id="S4.T3.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.6.6.7" class="ltx_tr">
<td id="S4.T3.6.6.7.1" class="ltx_td ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"></td>
<td id="S4.T3.6.6.7.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.2.1" class="ltx_text ltx_font_bold">ARC-C</span></td>
<td id="S4.T3.6.6.7.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.3.1" class="ltx_text ltx_font_bold">ARC-E</span></td>
<td id="S4.T3.6.6.7.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.4.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S4.T3.6.6.7.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.5.1" class="ltx_text ltx_font_bold">OpenBookQA</span></td>
<td id="S4.T3.6.6.7.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.6.1" class="ltx_text ltx_font_bold">PIQA</span></td>
<td id="S4.T3.6.6.7.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.7.1" class="ltx_text ltx_font_bold">RACE</span></td>
<td id="S4.T3.6.6.7.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.7.8.1" class="ltx_text ltx_font_bold">SciQ</span></td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">GPT2-XL<math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{BOT}}" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.1.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.1a.cmml">BOT</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><ci id="S4.T3.1.1.1.1.1.m1.1.1.1a.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.1">BOT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">{}_{\textsc{BOT}}</annotation></semantics></math></span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">26.5</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">61.2</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">40.9</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">24.2</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">71.0</span></td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">35.2</td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.1.1.1.8.1" class="ltx_text ltx_font_bold">92.8</span></td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">GPT2-XL<math id="S4.T3.2.2.2.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{MID}}" display="inline"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><msub id="S4.T3.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.1.m1.1.1a" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.2.2.2.1.1.m1.1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.1a.cmml">MID</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><apply id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1"><ci id="S4.T3.2.2.2.1.1.m1.1.1.1a.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.1">MID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">{}_{\textsc{MID}}</annotation></semantics></math></span></td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">22.7</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">60.9</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">39.6</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">15.0</td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">70.4</td>
<td id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">35.3</td>
<td id="S4.T3.2.2.2.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">90.9</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.3.3.3.1.1" class="ltx_text ltx_font_smallcaps">GPT2-XL<math id="S4.T3.3.3.3.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{TOP}}" display="inline"><semantics id="S4.T3.3.3.3.1.1.m1.1a"><msub id="S4.T3.3.3.3.1.1.m1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.1.1.m1.1.1a" xref="S4.T3.3.3.3.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.3.3.3.1.1.m1.1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.1a.cmml">TOP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.1.m1.1b"><apply id="S4.T3.3.3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1"><ci id="S4.T3.3.3.3.1.1.m1.1.1.1a.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.1">TOP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.1.m1.1c">{}_{\textsc{TOP}}</annotation></semantics></math></span></td>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">25.2</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">59.4</td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">39.2</td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">9.6</td>
<td id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">70.8</td>
<td id="S4.T3.3.3.3.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.3.3.3.7.1" class="ltx_text ltx_font_bold">35.6</span></td>
<td id="S4.T3.3.3.3.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">86.3</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.1.1" class="ltx_text ltx_font_smallcaps">OPT-1.3B<math id="S4.T3.4.4.4.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{BOT}}" display="inline"><semantics id="S4.T3.4.4.4.1.1.m1.1a"><msub id="S4.T3.4.4.4.1.1.m1.1.1" xref="S4.T3.4.4.4.1.1.m1.1.1.cmml"><mi id="S4.T3.4.4.4.1.1.m1.1.1a" xref="S4.T3.4.4.4.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.4.4.4.1.1.m1.1.1.1" xref="S4.T3.4.4.4.1.1.m1.1.1.1a.cmml">BOT</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.1.m1.1b"><apply id="S4.T3.4.4.4.1.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1"><ci id="S4.T3.4.4.4.1.1.m1.1.1.1a.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.4.4.4.1.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.1">BOT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.1.m1.1c">{}_{\textsc{BOT}}</annotation></semantics></math></span></td>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.2.1" class="ltx_text ltx_font_bold">26.8</span></td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.3.1" class="ltx_text ltx_font_bold">58.0</span></td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.4.1" class="ltx_text ltx_font_bold">42.1</span></td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.5.1" class="ltx_text ltx_font_bold">26.0</span></td>
<td id="S4.T3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.6.1" class="ltx_text ltx_font_bold">71.9</span></td>
<td id="S4.T3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.7.1" class="ltx_text ltx_font_bold">36.6</span></td>
<td id="S4.T3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.4.4.4.8.1" class="ltx_text ltx_font_bold">89.9</span></td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.5.5.5.1.1" class="ltx_text ltx_font_smallcaps">OPT-1.3B<math id="S4.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{MID}}" display="inline"><semantics id="S4.T3.5.5.5.1.1.m1.1a"><msub id="S4.T3.5.5.5.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml"><mi id="S4.T3.5.5.5.1.1.m1.1.1a" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.5.5.5.1.1.m1.1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.1a.cmml">MID</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.m1.1b"><apply id="S4.T3.5.5.5.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1"><ci id="S4.T3.5.5.5.1.1.m1.1.1.1a.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.5.5.5.1.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.1">MID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.m1.1c">{}_{\textsc{MID}}</annotation></semantics></math></span></td>
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">27.0</td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">56.4</td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">40.8</td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">23.2</td>
<td id="S4.T3.5.5.5.6" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">70.8</td>
<td id="S4.T3.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">35.8</td>
<td id="S4.T3.5.5.5.8" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">89.5</td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T3.6.6.6.1.1" class="ltx_text ltx_font_smallcaps">OPT-1.3B<math id="S4.T3.6.6.6.1.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{TOP}}" display="inline"><semantics id="S4.T3.6.6.6.1.1.m1.1a"><msub id="S4.T3.6.6.6.1.1.m1.1.1" xref="S4.T3.6.6.6.1.1.m1.1.1.cmml"><mi id="S4.T3.6.6.6.1.1.m1.1.1a" xref="S4.T3.6.6.6.1.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.T3.6.6.6.1.1.m1.1.1.1" xref="S4.T3.6.6.6.1.1.m1.1.1.1a.cmml">TOP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.1.m1.1b"><apply id="S4.T3.6.6.6.1.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.1.m1.1.1"><ci id="S4.T3.6.6.6.1.1.m1.1.1.1a.cmml" xref="S4.T3.6.6.6.1.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.T3.6.6.6.1.1.m1.1.1.1.cmml" xref="S4.T3.6.6.6.1.1.m1.1.1.1">TOP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.1.m1.1c">{}_{\textsc{TOP}}</annotation></semantics></math></span></td>
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">26.5</td>
<td id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">56.6</td>
<td id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">39.1</td>
<td id="S4.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">24.0</td>
<td id="S4.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">70.9</td>
<td id="S4.T3.6.6.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">35.6</td>
<td id="S4.T3.6.6.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">89.4</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparisons</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">The comparison results between <span id="S4.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and baseline methods are demonstrated in TableÂ <a href="#S3.T1" title="Table 1 â€£ Alternative Local Training â€£ 3.2 Training Procedure â€£ 3 Methodology â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
From these results we can observe that, <span id="S4.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">Finetune</span> brings a large performance boost on the downstream tasks compared to those results achieved by <span id="S4.SS4.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">Zero-Shot</span>, and costs lots of computation resources. Therefore, <span id="S4.SS4.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span>, which only tunes soft prompts, achieve comparable performance but needs much fewer resource than <span id="S4.SS4.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_smallcaps">Finetune</span>. These results are consistent with previous studiesÂ <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>); He etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite> and show the effectiveness of PEFT techniques.</p>
</div>
<div id="S4.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p2.1" class="ltx_p">When applying FL for privacy protection, we can see that <span id="S4.SS4.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt</span> achieves similar performances with <span id="S4.SS4.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span> on all the downstream datasets, demonstrating the effectiveness of FL algorithms in maintaining the performance compared to the central training algorithms.
However, there exists a noticeable gap between the performance achieved by <span id="S4.SS4.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_smallcaps">FedPrompt</span> and <span id="S4.SS4.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_smallcaps">Finetune</span>, which is attributed to the fact that the number of tunable parameters in <span id="S4.SS4.SSS0.Px1.p2.1.5" class="ltx_text ltx_font_smallcaps">FedPrompt</span> is much fewer than those in <span id="S4.SS4.SSS0.Px1.p2.1.6" class="ltx_text ltx_font_smallcaps">Finetune</span>.</p>
</div>
<div id="S4.SS4.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p3.1" class="ltx_p">Further, <span id="S4.SS4.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt-Single</span> performs significantly worse than <span id="S4.SS4.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_smallcaps">FedPrompt</span>, and even worse than <span id="S4.SS4.SSS0.Px1.p3.1.3" class="ltx_text ltx_font_smallcaps">Zero-Shot</span> on almost all the adopted datasets.
These experimental results further confirm that the misalignment between the serverâ€™s and clientsâ€™ models might bring the optimization gap and make the federal training process meaningless, as discussed in SectionÂ <a href="#S3.SS2" title="3.2 Training Procedure â€£ 3 Methodology â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S4.SS4.SSS0.Px1.p4" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p4.1" class="ltx_p">The proposed <span id="S4.SS4.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> achieves much better performance compared to <span id="S4.SS4.SSS0.Px1.p4.1.2" class="ltx_text ltx_font_smallcaps">FedPrompt-Single</span>, and is competitive with those of <span id="S4.SS4.SSS0.Px1.p4.1.3" class="ltx_text ltx_font_smallcaps">FedPrompt</span> and <span id="S4.SS4.SSS0.Px1.p4.1.4" class="ltx_text ltx_font_smallcaps">Prefix-Tuning</span>.
For example, evaluated on the ARC-C dataset with GPT2-XL/OPT-1.3B, the performance of <span id="S4.SS4.SSS0.Px1.p4.1.5" class="ltx_text ltx_font_smallcaps">FedSP</span> is significantly better than <span id="S4.SS4.SSS0.Px1.p4.1.6" class="ltx_text ltx_font_smallcaps">FedPrompt-Single</span> with an improvement of 6.4%/2.2%, and is similar to <span id="S4.SS4.SSS0.Px1.p4.1.7" class="ltx_text ltx_font_smallcaps">FedPrompt</span> within 1% performance gap.
These results show the effectiveness and advantage of <span id="S4.SS4.SSS0.Px1.p4.1.8" class="ltx_text ltx_font_smallcaps">FedSP</span> in solving the misalignment issue and using tunable soft prompts as messengers for knowledge delivery.
Meanwhile, with the help of these tunable soft prompts, it is worth pointing out that the proposed <span id="S4.SS4.SSS0.Px1.p4.1.9" class="ltx_text ltx_font_smallcaps">FedSP</span> provides both privacy protection for local data and global model, and needs fewer computation and communication costs compared to baselines.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablation Study</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">We conduct an ablation study to show the contributions of different techniques used in the proposed <span id="S4.SS4.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>. We separately remove the effect of using knowledge distillation (w/o KD), cross-layer sharing (w/o CS), and alternative training (w/o AT). The experimental results are reported at the bottom of TableÂ <a href="#S3.T1" title="Table 1 â€£ Alternative Local Training â€£ 3.2 Training Procedure â€£ 3 Methodology â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p2.1" class="ltx_p">From these results, we can observe that removing any one of the adopted techniques brings a significant performance drop on most of the adopted datasets.
For example, when removing KD/CS/AT on the ARC-C dataset, the model performance is decreased by 8.7%/5.5%/0.9% for GPT2-XL and 4.7%/2.5%/0.6% for OPT-1.3B, respectively.
These results show that the techniques are all necessary in <span id="S4.SS4.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and have positive effects for further improving overall model performance.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Further Discussions</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In this section, we provide further discussions to better understand the proposed <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>.</p>
</div>
<section id="S4.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Auxiliary Model</h4>

<div id="S4.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px1.p1.1" class="ltx_p">The auxiliary models are first produced by the server according to the global model, and then updated by the clients based on their local data independently. In this part, we compare different mechanisms for producing the auxiliary model, including varying the selected layer from the global model and the layer numbers. The specific layer of bottom/middle/top is 1/24/48 and 1/12/24 for GPT2-XL and OPT-1.3B, respectively</p>
</div>
<div id="S4.SS5.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS5.SSS0.Px1.p2.6" class="ltx_p">As shown in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Implementation Details â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the server can select one of the layers from the bottom (denoted as BOT), the middle (denoted as MID), or the top (denoted as TOP) of the global model (i.e. LLMs) to produce the auxiliary model. We can observe from the table that the BOT selection strategy has the best performances on most datasets, and the other two strategies have closed performances. For example, compared with GPT2-XL<math id="S4.SS5.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="{}_{\textsc{BOT}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.1.m1.1a"><msub id="S4.SS5.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.1.m1.1.1a" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1a.cmml">BOT</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS5.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1"><ci id="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.1.m1.1.1.1">BOT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.1.m1.1c">{}_{\textsc{BOT}}</annotation></semantics></math>/OPT-1.3B<math id="S4.SS5.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="{}_{\textsc{BOT}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.2.m2.1a"><msub id="S4.SS5.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.2.m2.1.1a" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1a.cmml">BOT</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.2.m2.1b"><apply id="S4.SS5.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1"><ci id="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.2.m2.1.1.1">BOT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.2.m2.1c">{}_{\textsc{BOT}}</annotation></semantics></math> on the ARC-E dataset, the performances of GPT2-XL<math id="S4.SS5.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="{}_{\textsc{MID}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.3.m3.1a"><msub id="S4.SS5.SSS0.Px1.p2.3.m3.1.1" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.3.m3.1.1a" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1a.cmml">MID</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.3.m3.1b"><apply id="S4.SS5.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1"><ci id="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.3.m3.1.1.1">MID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.3.m3.1c">{}_{\textsc{MID}}</annotation></semantics></math>/OPT-1.3B<math id="S4.SS5.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="{}_{\textsc{MID}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.4.m4.1a"><msub id="S4.SS5.SSS0.Px1.p2.4.m4.1.1" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.4.m4.1.1a" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1a.cmml">MID</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.4.m4.1b"><apply id="S4.SS5.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1"><ci id="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.4.m4.1.1.1">MID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.4.m4.1c">{}_{\textsc{MID}}</annotation></semantics></math> and GPT2-XL<math id="S4.SS5.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="{}_{\textsc{TOP}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.5.m5.1a"><msub id="S4.SS5.SSS0.Px1.p2.5.m5.1.1" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.5.m5.1.1a" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1a.cmml">TOP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.5.m5.1b"><apply id="S4.SS5.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1"><ci id="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.5.m5.1.1.1">TOP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.5.m5.1c">{}_{\textsc{TOP}}</annotation></semantics></math>/OPT-1.3B<math id="S4.SS5.SSS0.Px1.p2.6.m6.1" class="ltx_Math" alttext="{}_{\textsc{TOP}}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p2.6.m6.1a"><msub id="S4.SS5.SSS0.Px1.p2.6.m6.1.1" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p2.6.m6.1.1a" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1.cmml"></mi><mtext class="ltx_font_smallcaps" id="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1a.cmml">TOP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p2.6.m6.1b"><apply id="S4.SS5.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1"><ci id="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1a.cmml" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p2.6.m6.1.1.1">TOP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p2.6.m6.1c">{}_{\textsc{TOP}}</annotation></semantics></math> have a degradation of 0.3%/1.6% and 1.8%/1.4%, respectively. These results imply the fact that the bottom layer is more suitable for achieving a good model alignment, as it is directly connected with the word embedding layer for learning low-level representations, and might change slighter than other layers during the finetuning process.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2311.06805/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="433" height="289" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model performance w.r.t. different layer numbers of the auxiliary models on PIQA dataset.</figcaption>
</figure>
<div id="S4.SS5.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS5.SSS0.Px1.p3.3" class="ltx_p">Besides, we vary the layer numbers of the auxiliary models, from 1 to 4, and report the experimental results in FigureÂ <a href="#S4.F2" title="Figure 2 â€£ Auxiliary Model â€£ 4.5 Further Discussions â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To apply the cross-layer sharing technique, the parameters of auxiliary models are shared <math id="S4.SS5.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="L/N" display="inline"><semantics id="S4.SS5.SSS0.Px1.p3.1.m1.1a"><mrow id="S4.SS5.SSS0.Px1.p3.1.m1.1.1" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.2" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.2.cmml">L</mi><mo id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.1" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.1.cmml">/</mo><mi id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.3" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p3.1.m1.1b"><apply id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1"><divide id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.1"></divide><ci id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.2">ğ¿</ci><ci id="S4.SS5.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S4.SS5.SSS0.Px1.p3.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p3.1.m1.1c">L/N</annotation></semantics></math> times where <math id="S4.SS5.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS5.SSS0.Px1.p3.2.m2.1a"><mi id="S4.SS5.SSS0.Px1.p3.2.m2.1.1" xref="S4.SS5.SSS0.Px1.p3.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p3.2.m2.1b"><ci id="S4.SS5.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S4.SS5.SSS0.Px1.p3.2.m2.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p3.2.m2.1c">L</annotation></semantics></math> is the total layer number of the global model and <math id="S4.SS5.SSS0.Px1.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS5.SSS0.Px1.p3.3.m3.1a"><mi id="S4.SS5.SSS0.Px1.p3.3.m3.1.1" xref="S4.SS5.SSS0.Px1.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p3.3.m3.1b"><ci id="S4.SS5.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S4.SS5.SSS0.Px1.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p3.3.m3.1c">N</annotation></semantics></math> is the layer numbers of auxiliary models.
We conduct evaluations on the PIQA dataset, which show the proposed <span id="S4.SS5.SSS0.Px1.p3.3.1" class="ltx_text ltx_font_smallcaps">FedSP</span> is able to achieve better performances with the increase of selected layer number. For example, based on GPT2-XL/OPT-1.3B, the performance of <span id="S4.SS5.SSS0.Px1.p3.3.2" class="ltx_text ltx_font_smallcaps">FedSP</span> on PIQA improves from 71.0/71.9 to 71.4/72.9 when the number of selected layers increases from 1 to 4.
These results demonstrate that the proposed method is flexible to satisfy different requirements of computation resources from various applications, achieving a better balancing of privacy protection and model utility.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Efficiency Comparisons</h4>

<div id="S4.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px2.p1.1" class="ltx_p">We compare the model sizes and communication costs of the proposed approach and baselines, as shown in TableÂ <a href="#S4.T4" title="Table 4 â€£ Privacy Protection â€£ 4.5 Further Discussions â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S4.T5" title="Table 5 â€£ Privacy Protection â€£ 4.5 Further Discussions â€£ 4 Experiments â€£ Tunable Soft Prompts are Messengers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The model size refers to the number of model parameters loaded by the clients, and the communication cost refers to the number of parameters being exchanged between the server and clients for each training round. Here to better show such comparisons, we assume that the communication cost of <span id="S4.SS5.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">Finetune</span> in the context of FL is the same as the model size of LLMs.</p>
</div>
<div id="S4.SS5.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS5.SSS0.Px2.p2.1" class="ltx_p">From the table, we can conclude the model size and communication cost of <span id="S4.SS5.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> are significantly less than <span id="S4.SS5.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_smallcaps">Finetune</span> (with degradation of 99.5%/99.6% and 93.1%/88.2% on GPT2-XL/OPT-1.3B, respectively). Compared with <span id="S4.SS5.SSS0.Px2.p2.1.3" class="ltx_text ltx_font_smallcaps">FedPrompt</span>, <span id="S4.SS5.SSS0.Px2.p2.1.4" class="ltx_text ltx_font_smallcaps">FedSP</span> has similar communication cost while significantly reducing the model size loaded by the clients (with a decrease of 93.1%/88.2% on GPT2-XL/OPT-1.3B, respectively).
These results demonstrate the efficiency of the proposed <span id="S4.SS5.SSS0.Px2.p2.1.5" class="ltx_text ltx_font_smallcaps">FedSP</span> for knowledge exchanging in an FL course, since clients only need to update the auxiliary model and soft prompts in the local training process.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Privacy Protection</h4>

<div id="S4.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px3.p1.1" class="ltx_p">The proposed <span id="S4.SS5.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span> provides fundamental privacy protection (i.e., no sharing data directly) with the help of the federated learning framework. As the number of the model parameters of soft prompts is much smaller than that of the entire model, we believe that the level of privacy-preserving of the proposed method would be better or at least at the same level.
Further, privacy protection can be enhanced by privacy protection algorithms, such as differential privacy techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">Triastcyn and Faltings (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>); Wei etÂ al. (<a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, to satisfy flexible protection requirements from different applications.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Efficiency comparisons between <span id="S4.T4.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and baselines on ARC-C with GPT2-XL.</figcaption>
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:168.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(100.9pt,-39.2pt) scale(1.87106473705481,1.87106473705481) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.1.2.1" class="ltx_text ltx_font_bold">Model Size</span></td>
<td id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.1.3.1" class="ltx_text ltx_font_bold">Comm. Cost</span></td>
</tr>
<tr id="S4.T4.3.1.2" class="ltx_tr">
<td id="S4.T4.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.2.1.1" class="ltx_text ltx_font_smallcaps">Zero-Shot</span></td>
<td id="S4.T4.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">1.6B</td>
<td id="S4.T4.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr id="S4.T4.3.1.3" class="ltx_tr">
<td id="S4.T4.3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.3.1.1" class="ltx_text ltx_font_smallcaps">Finetune</span></td>
<td id="S4.T4.3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.6B</td>
<td id="S4.T4.3.1.3.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.6B</td>
</tr>
<tr id="S4.T4.3.1.4" class="ltx_tr">
<td id="S4.T4.3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.4.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt</span></td>
<td id="S4.T4.3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.6B</td>
<td id="S4.T4.3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">6.1M (0.4%)</td>
</tr>
<tr id="S4.T4.3.1.5" class="ltx_tr">
<td id="S4.T4.3.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T4.3.1.5.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span></td>
<td id="S4.T4.3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">111.1M (6.9%)</td>
<td id="S4.T4.3.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">7.2M (0.5%)</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Efficiency comparisons between <span id="S4.T5.2.1" class="ltx_text ltx_font_smallcaps">FedSP</span> and baselines on ARC-C with OPT-1.3B.</figcaption>
<div id="S4.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:164.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.4pt,-37.4pt) scale(1.83154906248036,1.83154906248036) ;">
<table id="S4.T5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<td id="S4.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.1.2.1" class="ltx_text ltx_font_bold">Model Size</span></td>
<td id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.1.3.1" class="ltx_text ltx_font_bold">Comm. Cost</span></td>
</tr>
<tr id="S4.T5.3.1.2" class="ltx_tr">
<td id="S4.T5.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.2.1.1" class="ltx_text ltx_font_smallcaps">Zero-Shot</span></td>
<td id="S4.T5.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td id="S4.T5.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">-</td>
</tr>
<tr id="S4.T5.3.1.3" class="ltx_tr">
<td id="S4.T5.3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.3.1.1" class="ltx_text ltx_font_smallcaps">Finetune</span></td>
<td id="S4.T5.3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td id="S4.T5.3.1.3.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
</tr>
<tr id="S4.T5.3.1.4" class="ltx_tr">
<td id="S4.T5.3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.4.1.1" class="ltx_text ltx_font_smallcaps">FedPrompt</span></td>
<td id="S4.T5.3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td id="S4.T5.3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">3.9M (0.3%)</td>
</tr>
<tr id="S4.T5.3.1.5" class="ltx_tr">
<td id="S4.T5.3.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T5.3.1.5.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span></td>
<td id="S4.T5.3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">153.8M (11.8%)</td>
<td id="S4.T5.3.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">5.4M (0.4%)</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Parameter-efficient Finetuning</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">With the increasingly larger scale of the pre-trained large language models (LLMs), finetuning the entire model becomes unaffordable for many individual researchers. As an efficient adaptation of LLMs to new downstream tasks, Parameter-efficient Finetuning (PEFT)Â <cite class="ltx_cite ltx_citemacro_cite">Houlsby etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>); Lester etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>); Hu etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Zaken etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2021</a>); He etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Khashabi etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> algorithms have emerged where only negligible proportions of parameters in the original LLMs are required to be updated.
The main idea of PEFT is to add continuous soft prompts to the original model and only the parameters of the soft prompts need to be finetuned. For example, <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Lester etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> propose to add a few continuous tunable parameters as soft prompts to the input word embeddings, and <cite class="ltx_cite ltx_citemacro_citet">Li and Liang (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> suggest to prepend separate soft prompts to every model layer.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated Learning in NLP</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">In order to protect data privacy, federated Learning (FL) has attracted a lot of attention from both academic and industrialÂ <cite class="ltx_cite ltx_citemacro_cite">KoneÄná»³ etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>); McMahan etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2017</a>); Yang etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>); Kairouz etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. With more and more language assistance products being applied in real-world applications, FL has also increasingly appeared in the community of NLP to address the problem of privacy leakage, such as machine translationÂ <cite class="ltx_cite ltx_citemacro_cite">Passban etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>); Du etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and question answeringÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>); Ait-Mlouk etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, and so onÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Lin etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>); Kuang etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Cai etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>); Liu etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">Besides data privacy, related works are also focused on the following challenges in the field of NLP: (i) Data heterogeneity. For example, <cite class="ltx_cite ltx_citemacro_citet">Chen etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> proposes a federated matching framework with a backbone-patch architecture to address the non-identical and independent distribution (non-IID) problem of the training data. <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> propose to split the model into a local part and a global part to handle the non-IID issue. (ii) Task heterogeneity. For example, <cite class="ltx_cite ltx_citemacro_citet">Dong etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2022a</a>)</cite> propose an Assign-Then-Contrast framework to collaborate heterogeneous NLP tasks. <cite class="ltx_cite ltx_citemacro_citet">Dong etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2022b</a>)</cite> propose a few-shot FL framework with an energy-based weighting algorithm to enhance the cross-task generalization ability. (iii) Efficient communication. For example, <cite class="ltx_cite ltx_citemacro_citet">Passban etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite> presents a dynamic pulling FL method to dynamically control the communication bandwidth. <cite class="ltx_cite ltx_citemacro_citet">Du etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> presents a federated nearest neighbor framework to reduce the communication overhead.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p3.1" class="ltx_p">Although FL can protect data privacy to some extent, it is incapable to cope with the situation when the server and clients need to protect their model privacy. In this paper, we make the first attempt to incorporate the idea of using tunable soft prompts as messengers to meet the requirement of protecting both data privacy and model privacy.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose a novel FL training approach, named <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>, with tunable soft prompts as messengers to accomplish the knowledge delivery among participants.
Since the proposed <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">FedSP</span> does not need to share the global model, it provides the necessary privacy protection for the global model when finetuning LLMs.
These soft prompts are broadcast to the clients at each training round, plugged into an auxiliary model, and used to capture the knowledge from local data.
Extensive experiments on various benchmarking datasets demonstrate the effectiveness of <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">FedSP</span>, showing that using tunable soft prompts as messengers in FL is able to protect both model privacy and data privacy, and achieve competitive performance with baselines and needs much less computation and communication costs.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We propose to use tunable soft prompts as messengers for useful knowledge exchange in FL. As for the limitations of this study, we conclude from the following three perspectives:
(1) The proposed approach achieves privacy protection for both local data and the global model, but at the same time brings a slight performance drop compared to finetuning the LLMs directly. It can be regarded as a trade-off between privacy protection and model utility. We hope that further research can achieve a better balance on such a trade-off.
(2) Some adopted techniques, such as cross-layer sharing, rely on the transformer-based architecture, which is the most widely used architecture in LLMs. In the long run, it could be better to choose a general technique to construct the auxiliary model used in <span id="Sx1.p1.1.1" class="ltx_text ltx_font_smallcaps">FedSP</span>.
(3) Although we conduct the first attempt to provide protection for both local data and global models, a future direction can be how to further improve and adjust the privacy protection strength. For example, how to protect the model architecture of the global models, or how to utilize differential privacy techniques to satisfy various real-world applications with different protection requirements.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work was supported in part by the National Natural Science Foundation of China under Grant 61602013.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ait-Mlouk etÂ al. (2022)</span>
<span class="ltx_bibblock">
Addi Ait-Mlouk, Sadi Alawadi, Salman Toor, and Andreas Hellander. 2022.

</span>
<span class="ltx_bibblock">Fedqas: Privacy-aware machine reading comprehension with federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.04742</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan LeÂ bras, Jianfeng Gao, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dongqi Cai, Yaozong Wu, Shangguang Wang, FelixÂ Xiaozhu Lin, and Mengwei Xu.
2023.

</span>
<span class="ltx_bibblock">Efficient federated learning for modern nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th Annual International Conference on
Mobile Computing and Networking</em>, pages 1â€“16.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, CeÂ Ge, Dawei
Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Data-juicer: A one-stop data processing system for large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.02033</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2021.

</span>
<span class="ltx_bibblock">Fedmatch: Federated learning over heterogeneous question answering
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on
Information Knowledge Management</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sijie Cheng, Jingwen Wu, Yanghua Xiao, and Yang Liu. 2021.

</span>
<span class="ltx_bibblock">Fedgems: Federated learning of larger server models via selective
knowledge fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.11027</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian
Gehrmann, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, and Yaliang Li.
2022a.

</span>
<span class="ltx_bibblock">Collaborating heterogeneous natural language processing tasks via
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.05789</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Weilong Dong, Xinwei Wu, Junzhuo Li, Shuangzhi Wu, Chao Bian, and Deyi Xiong.
2022b.

</span>
<span class="ltx_bibblock">Fewfedweight: Few-shot federated learning framework across multiple
nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08354</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yichao Du, Zhirui Zhang, Bingzhe Wu, Lemao Liu, Tong Xu, and Enhong Chen. 2023.

</span>
<span class="ltx_bibblock">Federated nearest neighbor machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2021)</span>
<span class="ltx_bibblock">
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
Neubig. 2021.

</span>
<span class="ltx_bibblock">Towards a unified view of parameter-efficient transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.04366</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton etÂ al. (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NIPS Deep Learning and Representation Learning Workshop</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby etÂ al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
DeÂ Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
2790â€“2799.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, LuÂ Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz etÂ al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, HÂ Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi
Bennis, ArjunÂ Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Foundations and TrendsÂ® in Machine Learning</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khashabi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean
Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh,
and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock">Prompt waywardness: The curious case of discretized interpretation of
continuous prompts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 3631â€“3643.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KoneÄná»³ etÂ al. (2016)</span>
<span class="ltx_bibblock">
Jakub KoneÄná»³, HÂ Brendan McMahan, Daniel Ramage, and Peter
RichtÃ¡rik. 2016.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan,
Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Federatedscope-llm: A comprehensive package for fine-tuning large
language models in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.00363</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai etÂ al. (2017)</span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.

</span>
<span class="ltx_bibblock">RACE: Large-scale ReAding comprehension dataset from
examinations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. 2020.

</span>
<span class="ltx_bibblock">Albert: A lite bert for self-supervised learning of language
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester etÂ al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2021)</span>
<span class="ltx_bibblock">
Qinbin Li, Bingsheng He, and Dawn Song. 2021.

</span>
<span class="ltx_bibblock">Model-contrastive federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 10713â€“10722.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
XiangÂ Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhengyang Li, Shijing Si, Jianzong Wang, and Jing Xiao. 2022.

</span>
<span class="ltx_bibblock">Federated split BERT for heterogeneous text classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Neural Networks</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022)</span>
<span class="ltx_bibblock">
BillÂ Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe
Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr.
2022.

</span>
<span class="ltx_bibblock">FedNLP: Benchmarking federated learning methods for natural
language processing tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
NAACL 2022</em>, pages 157â€“175.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie
Tang. 2022.

</span>
<span class="ltx_bibblock">P-tuning: Prompt tuning can be comparable to fine-tuning across
scales and tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
Jie Tang. 2021.

</span>
<span class="ltx_bibblock">Gpt understands, too.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv:2103.10385</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
YiÂ Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, and XuÂ Sun. 2023.

</span>
<span class="ltx_bibblock">Communication efficient federated learning for multilingual neural
machine translation with adapter.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 5315â€“5328.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgueraÂ y
Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov etÂ al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passban etÂ al. (2022)</span>
<span class="ltx_bibblock">
Peyman Passban, Tanya Roosta, Rahul Gupta, Ankit Chadha, and Clement Chung.
2022.

</span>
<span class="ltx_bibblock">Training mixed-domain translation models via federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao etÂ al. (2022)</span>
<span class="ltx_bibblock">
TevenÂ Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡,
Daniel Hesslow, Roman CastagnÃ©, AlexandraÂ Sasha Luccioni, FranÃ§ois
Yvon, Matthias GallÃ©, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Triastcyn and Faltings (2019)</span>
<span class="ltx_bibblock">
Aleksei Triastcyn and Boi Faltings. 2019.

</span>
<span class="ltx_bibblock">Federated learning with bayesian differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data (Big Data)</em>,
pages 2587â€“2596.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, HowardÂ H Yang, Farhad Farokhi, Shi Jin,
TonyÂ QS Quek, and HÂ Vincent Poor. 2020.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em>,
15:3454â€“3469.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl etÂ al. (2017)</span>
<span class="ltx_bibblock">
Johannes Welbl, NelsonÂ F. Liu, and Matt Gardner. 2017.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.06209</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf etÂ al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven LeÂ Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Guangxuan Xiao, JiÂ Lin, and Song Han. 2023.

</span>
<span class="ltx_bibblock">Offsite-tuning: Transfer learning without full model.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.04870</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Federatedscope: A flexible federated learning platform for
heterogeneity.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em>, 16(5):1059â€“1072.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaken etÂ al. (2021)</span>
<span class="ltx_bibblock">
EladÂ Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.

</span>
<span class="ltx_bibblock">Bitfit: Simple parameter-efficient fine-tuning for transformer-based
masked language-models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.10199</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers etÂ al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07830</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02414</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, PunitÂ Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. 2022.

</span>
<span class="ltx_bibblock">Fedprompt: Communication-efficient and privacy preserving prompt
tuning in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.12268</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, etÂ al. 2023.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.06804" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.06805" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.06805">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.06805" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.06806" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 19:21:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
