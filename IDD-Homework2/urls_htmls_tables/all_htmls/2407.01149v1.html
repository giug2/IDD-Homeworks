<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media</title>
<!--Generated on Mon Jul  1 10:14:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.01149v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S1" title="In Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S1.SS1" title="In 1 Introduction ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Related Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2" title="In Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1" title="In 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Sources</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS1" title="In 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS2" title="In 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Annotation Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS3" title="In 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Cultural Alignment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS2" title="In 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>System Development</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS2.SSS1" title="In 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Feature Engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS2.SSS2" title="In 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Model Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS3" title="In 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Communities of Interest</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3" title="In Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4" title="In Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S5" title="In Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#Sx2.SS0.SSS0.Px1" title="In Limitations ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title">Invisibility of Q+ identities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#Sx2.SS0.SSS0.Px2" title="In Limitations ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title">Sociocultural bias during data collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#Sx2.SS0.SSS0.Px3" title="In Limitations ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title">Pitfalls of large language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#Sx2.SS0.SSS0.Px4" title="In Limitations ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title">Class imbalance and distribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#Sx2.SS0.SSS0.Px5" title="In Limitations ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_title">Further work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sidney G.-J. Wong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Canterbury, New Zealand
</span>
<span class="ltx_contact ltx_role_affiliation">Geospatial Research Institute, New Zealand
</span>
<span class="ltx_contact ltx_role_affiliation">New Zealand Institute of Language, Brain and Behaviour, New Zealand
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The purpose of this paper is to ascertain the influence of sociocultural factors (i.e., social, cultural, and political) in the development of hate speech detection systems. We set out to investigate the suitability of using open-source training data to monitor levels of anti-LGBTQ+ content on social media across different national-varieties of English. Our findings suggests the social and cultural alignment of open-source hate speech data sets influences the predicted outputs. Furthermore, the keyword-search approach of anti-LGBTQ+ slurs in the development of open-source training data encourages detection models to overfit on slurs; therefore, anti-LGBTQ+ content may go undetected. We recommend combining empirical outputs with qualitative insights to ensure these systems are fit for purpose.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\usesmartdiagramlibrary</span>
<p class="ltx_p" id="p1.2">additions







</p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<div class="ltx_block ltx_align_bottom" id="p2.1">
<p class="ltx_p" id="p2.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1">Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p2.1.2.1" style="width:0.0pt;"></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_bold" id="p3.1.1">Content Warning</span>: This paper contains unobfuscated examples of slurs, hate speech, and offensive language with reference to homophobia and transphobia which may cause distress.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The proliferation of hate speech on social media platforms continues to negatively impact LGBTQ+ communities <cite class="ltx_cite ltx_citemacro_cite">Stefania and Buf (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib48" title="">2021</a>)</cite>. As a consequence of anti-LGBTQ+ hate speech, these already minoritised and marginalised communities may experience digital exclusion and barriers to access in the form of the digital divide <cite class="ltx_cite ltx_citemacro_cite">Norris (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib38" title="">2001</a>)</cite>. There have been considerable developments within the field of Natural Language Processing (NLP) in response to this social issue <cite class="ltx_cite ltx_citemacro_cite">Sánchez-Sánchez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib49" title="">2024</a>)</cite>, with most of the methodological advancements in this area being made in the last three decades <cite class="ltx_cite ltx_citemacro_cite">Tontodimamma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib51" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While much of hate speech research has focused on documentation and detection, there has been little attention on how these approaches can be applied across different social, political, or linguistic contexts <cite class="ltx_cite ltx_citemacro_cite">Locatelli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib33" title="">2023</a>)</cite>. Just as the appropriateness of swear words is highly contextually variable depending on language and culture <cite class="ltx_cite ltx_citemacro_cite">Jay and Janschewitz (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib25" title="">2008</a>)</cite>, hate speech in the form of anti-LGBTQ+ hate speech is often predicated by social, cultural, and political attitudes towards diverse gender and sexualities. With minimal literature beyond just a system development context, we set out to investigate the suitability of implementing open-source anti-LGBTQ+ hate speech system on real-world sources of social media data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper makes two contributions: firstly, we show the predicted outputs from classification models can be transformed into various time series data sets to monitor the rate and volume of anti-LGBTQ+ hate speech on social media. Secondly, we argue that social, cultural, and linguistic bias introduced during the data collection phase has an impact on the suitability of these approaches.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Work</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Hate speech detection is often treated as a text classification task, whereby existing data can be used to train machine learning models to predict the attributes of unknown data <cite class="ltx_cite ltx_citemacro_cite">Jahan and Oussalah (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib24" title="">2023</a>)</cite>. The main focus of these systems are racism, sexism and gender discrimination, and violent radicalism <cite class="ltx_cite ltx_citemacro_cite">Sánchez-Sánchez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib49" title="">2024</a>)</cite>. Both the production and deployment of hate speech detection systems are methodologically similar produced under the following pipeline <cite class="ltx_cite ltx_citemacro_cite">Kowsari et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib29" title="">2019</a>)</cite>:</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">a)</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.ix1.p1.1.1">Data Set Collection and Preparation</span>: involves collecting either real-world or synthetic instances of hate speech in a language condition (i.e., keyword search). This phase may involve or manual annotation from experts of crowd-sourced annotators.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">b)</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.ix2.p1.1.1">Feature Engineering</span>: involves manipulating and transforming instances of hate speech. This may involve anonymisation or confidentialisation depending on the privacy and data use rules for each social media platform.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">c)</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.ix3.p1.1.1">Model Training</span>: involves developing a hate speech detection system with machine learning algorithms. This may involve statistical language models or incorporating transformer-based large language models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">d)</span>
<div class="ltx_para" id="S1.I1.ix4.p1">
<p class="ltx_p" id="S1.I1.ix4.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.ix4.p1.1.1">Model Evaluation</span>: involves producing model performance metrics to determine the statistical validity of the system. This may involve making predictions on unseen or test data.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">Despite their straightforward workflow, these systems pose a number of ethical challenges and risks to the vulnerable communities <cite class="ltx_cite ltx_citemacro_cite">Vidgen and Derczynski (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib53" title="">2020</a>)</cite>. Cultural biases and harms can be introduced at each stage of the data set production process <cite class="ltx_cite ltx_citemacro_cite">Sap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib46" title="">2019</a>)</cite>. Some of this can be attributed to poorly designed systems which are not fit for purpose <cite class="ltx_cite ltx_citemacro_cite">Vidgen and Derczynski (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib53" title="">2020</a>)</cite>. For example, racial bias was identified in one open-source hate speech detection system developed by <cite class="ltx_cite ltx_citemacro_citet">Davidson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib11" title="">2017</a>)</cite> which resulted in samples of written African American English being misclassified as instances of hate speech and offensive language <cite class="ltx_cite ltx_citemacro_cite">Davidson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib10" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">The presence of racial bias can be attributed to the decisions made during the <span class="ltx_text ltx_font_italic" id="S1.SS1.p4.1.1">Data Set Collection and Preparation</span> phase during system development. <cite class="ltx_cite ltx_citemacro_citet">Davidson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib11" title="">2017</a>)</cite> took a keyword search approach (i.e., slurs and profanities) to identify instances of hate speech and offensive language. These samples were then used in the development of the detection system. Although slurs and profanities are good evidence of anti-social behaviour, the same words can also be re-appropriated or reclaimed by target communities <cite class="ltx_cite ltx_citemacro_cite">Popa-Wyatt (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib44" title="">2020</a>)</cite>. Classifications algorithms are unable to account for implicit world knowledge.</p>
</div>
<div class="ltx_para" id="S1.SS1.p5">
<p class="ltx_p" id="S1.SS1.p5.1">Similarly, simple machine learning algorithms cannot account for linguistic variation which is another form of implicit world knowledge. Of interest to our current investigation, <cite class="ltx_cite ltx_citemacro_citet">Wong (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib56" title="">2023a</a>)</cite> applied the same system developed by <cite class="ltx_cite ltx_citemacro_citet">Davidson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib11" title="">2017</a>)</cite> on samples of tweets/posts originating in New Zealand. The system erroneously classified tweets/posts with words such as <span class="ltx_text ltx_font_italic" id="S1.SS1.p5.1.1">bugger</span>, <span class="ltx_text ltx_font_italic" id="S1.SS1.p5.1.2">digger</span>, and <span class="ltx_text ltx_font_italic" id="S1.SS1.p5.1.3">stagger</span> as instances of hate speech. An unintended consequence of these misclassified tweets/posts is that rural areas exhibited higher rates of hate speech and offensive language when compared to the national mean.</p>
</div>
<div class="ltx_para" id="S1.SS1.p6">
<p class="ltx_p" id="S1.SS1.p6.1">However, not all forms of biases stem from decisions made during system development. Recent innovations in transformer-based language models, such as <span class="ltx_text ltx_font_smallcaps" id="S1.SS1.p6.1.1">bert</span> <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib12" title="">2019</a>)</cite>, have introduced new ethical challenges as the presence of gender, race, and other forms of bias have been observed in the word embeddings of large language models <cite class="ltx_cite ltx_citemacro_cite">Tan and Celis (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib50" title="">2019</a>)</cite>. This means there is potential for bias even in the later stages of system development during the <span class="ltx_text ltx_font_italic" id="S1.SS1.p6.1.2">Model Training</span> phase.</p>
</div>
<div class="ltx_para" id="S1.SS1.p7">
<p class="ltx_p" id="S1.SS1.p7.1">While we grow increasingly aware of the impacts from these limitations <cite class="ltx_cite ltx_citemacro_cite">Alonso Alemany et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib1" title="">2023</a>)</cite>, the number of hate speech detection data sets and systems continue to increase <cite class="ltx_cite ltx_citemacro_cite">Tontodimamma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib51" title="">2021</a>)</cite>. A systematic review of hate speech literature has identified over 69 training data sets to detect hate speech on online and social media for 21 different language conditions <cite class="ltx_cite ltx_citemacro_cite">Jahan and Oussalah (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib24" title="">2023</a>)</cite>. Seemingly, the solution to addressing social, cultural, and political discrepancies within hate speech detection is to develop more systems in different languages.</p>
</div>
<div class="ltx_para" id="S1.SS1.p8">
<p class="ltx_p" id="S1.SS1.p8.1">There remains little interest from NLP researchers to consider the issue of hate speech detection from a social impact lens <cite class="ltx_cite ltx_citemacro_cite">Hovy and Spruit (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib23" title="">2016</a>)</cite>. The primary concerns in this research area are largely methodological. For example, improving model performance of detection systems resulting from noisy training data <cite class="ltx_cite ltx_citemacro_citep">(Arango et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib2" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Laaksonen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib31" title="">2020</a>)</cite> critiqued the <span class="ltx_text ltx_font_italic" id="S1.SS1.p8.1.1">datafication</span> of hate speech detection which in turn has become an unnecessary distraction for NLP researchers in combating this social issue.</p>
</div>
<div class="ltx_para" id="S1.SS1.p9">
<p class="ltx_p" id="S1.SS1.p9.1">In fact, the appetite in applying NLP approaches for social good has decreased over time <cite class="ltx_cite ltx_citemacro_cite">Fortuna et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib15" title="">2021</a>)</cite>. Some researchers are beginning to question whether the efforts put towards the development and production of hate speech detection systems is the ideal solution for this social issue <cite class="ltx_cite ltx_citemacro_cite">Parker and Ruths (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib42" title="">2023</a>)</cite>. In sidelining these pressing issues in hate speech detection research, we may unintentionally perpetuate existing prejudices against marginalised and minoritised groups these systems were meant to support <cite class="ltx_cite ltx_citemacro_cite">Buhmann and Fieseler (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib4" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p10">
<p class="ltx_p" id="S1.SS1.p10.1">In light of these ethical and methodological challenges in hate speech detection <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib9" title="">2023</a>)</cite>, we are starting to see how sociolinguistic information can be used to fine tune and improve the social and cultural performance of hate speech detection (<cite class="ltx_cite ltx_citemacro_citep">Wong et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib55" title="">2023</a></cite>; <cite class="ltx_cite ltx_citemacro_citep">Wong and Durward, <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib54" title="">2024</a></cite>) using well-attested methods such as domain adaptation <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib32" title="">2019</a>)</cite>. NLP researchers may still play an invaluable role in combating online hate speech by incorporating sociocultural considerations in the development and deployment of hate speech detection systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Hostility</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Direct</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Indirect</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.1">Abusive</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2">20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3">45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">65</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.3.2.1">Disrespectful</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3">56</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.4">61</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.4.3.1">Fearful</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.3">47</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.4">52</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.5.4.1">Hateful</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.2">36</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.3">106</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.4">142</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.6.5.1">Normal</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.2">13</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.3">71</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.4">84</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.7.6.1">Offensive</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.2">65</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.3">308</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.4">373</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S2.T1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.8.7.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.8.7.2">144</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.8.7.3">633</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.8.7.4">777</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> The distribution of English posts/tweets and the level of hostility by directness targeting sexual orientation in <cite class="ltx_cite ltx_citemacro_citet">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib40" title="">2019</a>)</cite>. Note that all totals are total responses.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S1.SS1" title="1.1 Related Work ‣ 1 Introduction ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">1.1</span></a>, hate speech detection research needs to undergo a paradigmatic shift in order to truly enable positive social impact, social good, and social benefit potential. The main purpose of this paper is to ascertain the influence of sociocultural factors (i.e., social, cultural, and political) in the development of hate speech detection systems. Our research questions are as follows:</p>
</div>
<div class="ltx_para" id="S2.p2">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_smallcaps" id="S2.I1.ix1.1.1.1">rq1</span></span>
<div class="ltx_para" id="S2.I1.ix1.p1">
<p class="ltx_p" id="S2.I1.ix1.p1.1">Can we use open-source hate speech training data to monitor anti-LGBTQ+ hate speech in real world instances of social media? and;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_smallcaps" id="S2.I1.ix2.1.1.1">rq2</span></span>
<div class="ltx_para" id="S2.I1.ix2.p1">
<p class="ltx_p" id="S2.I1.ix2.p1.1">How do the social, cultural, and linguistic contexts of open-source training data impact on the suitability of anti-LGBTQ+ hate speech detection?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In order to address <span class="ltx_text ltx_font_smallcaps" id="S2.p3.1.1">rq1</span>, we compare and contrast two anti-LGBTQ+ hate speech detection systems. We provide an in depth description of the data sources in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1" title="2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.1</span></a> and our system development pipeline in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS2" title="2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.2</span></a>. Once we develop the detection systems, we apply the detection systems on real-world samples of social media data to monitor anti-LGBTQ+ hate speech across different geographic dialects.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">We opted for a mixed-methods approach to address this emergent area of enquiry. This is because <span class="ltx_text ltx_font_smallcaps" id="S2.p4.1.1">rq2</span> can only be addressed qualitatively as we consider the suitability of the detection systems and the sociocultural relevance of the predicted outputs. We will address <span class="ltx_text ltx_font_smallcaps" id="S2.p4.1.2">rq2</span> in the discussion (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4" title="4 Discussion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a>); however, we have provided relevant sociolinguistic, cultural, and political information in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS3" title="2.3 Communities of Interest ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.3</span></a> to contextualise our discussion.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Sources</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As part of our investigation, we use two open-source training data sets to develop our anti-LGBTQ+ hate speech detection systems in our investigation: <cite class="ltx_cite ltx_citemacro_citet">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib40" title="">2019</a>)</cite> (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Multilingual and Multi-Aspect Hate Speech Data Set</span>; <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.2">mlma</span>) and <cite class="ltx_cite ltx_citemacro_citet">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib7" title="">2021</a>)</cite> (<span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.3">ltedi</span>)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We refer to it as <span class="ltx_text ltx_font_smallcaps" id="footnote1.1">ltedi</span> with reference to its central role in the various shared tasks hosted as part of the <span class="ltx_text ltx_font_italic" id="footnote1.2">Language Technology for Equity, Diversity, and Inclusion</span></span></span></span>. The <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.4">mlma</span> and <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.5">ltedi</span> were chosen due to the availability of data and documentation to understand the data set collection and annotation process.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1">Class</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S2.T2.1.1.1.2.1">eng</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S2.T2.1.1.1.3.1">tam</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S2.T2.1.1.1.4.1">tam<span class="ltx_text ltx_font_upright" id="S2.T2.1.1.1.4.1.1">-</span>eng</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T2.1.2.2.1"><span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.2.2.1.1">homo</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.2.2">276</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.2.3">723</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.2.4">465</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.3.3.1"><span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.3.3.1.1">trans</span></th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.3.3.2">13</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.3.3.3">233</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.3.3.4">184</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.4.4.1"><span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.4.4.1.1">none</span></th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.4.4.2">4,657</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.4.4.3">3,205</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.4.4.4">5,385</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S2.T2.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.5.5.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.1.5.5.2">4,946</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.1.5.5.3">4,161</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.1.5.5.4">6,034</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> The class distribution of YouTube comments based on the three-class classification system (homophobic (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.9.1">homo</span>), transphobic (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.10.2">trans</span>), and non-anti-LGBTQ+ (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.11.3">none</span>) content) by language condition (English (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.12.4">eng</span>), Tamil (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.13.5">tam</span>), and Tamil-English (<span class="ltx_text ltx_font_smallcaps" id="S2.T2.14.6">tam</span>-<span class="ltx_text ltx_font_smallcaps" id="S2.T2.15.7">eng</span>)) in <cite class="ltx_cite ltx_citemacro_citet">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib7" title="">2021</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.1">mlma</span> is a multilingual hate speech data set for posts/tweets from X (Twitter) for English, French, and Arabic <cite class="ltx_cite ltx_citemacro_cite">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib40" title="">2019</a>)</cite>. The authors took a keyword search approach by retrieving posts/tweets which matched a list of common slurs, controversial topics, and discourse patterns typically found in a hate speech. This approach proved challenging due to the high-rates of code-switching in the English and French conditions and Arabic diglossia. The posts/tweets were then posted on the crowd-sourcing platform, Mechanical Turk, for public annotation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">One of the most well-documented anti-LGBTQ+ training data sets is the English, Tamil, and English-Tamil anti-LGBTQ+ hate speech data set developed by <cite class="ltx_cite ltx_citemacro_citet">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib7" title="">2021</a>)</cite>. The data set contains public comments to LGBTQ+ videos on YouTube. The comments were manually annotated based on a three-class (i.e., homophobic, transphobic, and non-anti-LGBTQ+ hate speech). The training data was tested with three language models: <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.1">muril</span> <cite class="ltx_cite ltx_citemacro_cite">Khanuja et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib28" title="">2021</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.2">mbert</span> <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib43" title="">2019</a>)</cite>, and <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.3">xlm-roberta</span> <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib8" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The results show that transformer-based models, such as <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p4.1.1">bert</span>, outperformed statistical language models with minimal fine-tuning. The best performing <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p4.1.2">bert</span>-based system for English yielded an averaged <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><msub id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mi id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">F</mi><mn id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2">𝐹</ci><cn id="S2.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score of 0.94 <cite class="ltx_cite ltx_citemacro_cite">Maimaitituoheti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib36" title="">2022</a>)</cite>. This anti-LGBTQ+ training data set has since expanded to a suite of additional language conditions such as Spanish <cite class="ltx_cite ltx_citemacro_cite">García-Díaz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib17" title="">2020</a>)</cite>, Hindi and Malayalam <cite class="ltx_cite ltx_citemacro_cite">Kumaresan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib30" title="">2023</a>)</cite>, and Telugu, Kannada, Gujarati, Marathi, and Tulu <cite class="ltx_cite ltx_citemacro_cite">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib6" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">We discuss the similarities and differences between the two data sets in relation sociocultural considerations regarding the data collection strategy in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS1" title="2.1.1 Data Collection ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>, the annotation strategy in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS2" title="2.1.2 Annotation Process ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>, and the cultural alignment in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.SS1.SSS3" title="2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2.1.3</span></a> derived from available documentation.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Data Collection</h4>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="225" id="S2.F1.g1" src="extracted/5702373/figures/sampling.png" width="276"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model comparison of anti-LGBTQ+ hate speech on ten randomised samples of 10,000 posts/tweets per month from India between June 2018 to June 2023 including grouped mean and the upper and lower confidence intervals.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">The developers of the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p1.1.1">mlma</span> took a culturally-agnostic approach with limited information on the data collection points; however, evidence of code-switching between English with Hindi, Spanish, and French posed a challenge to annotators. The <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p1.1.2">mlma</span> took a keyword search approach to filter X (Twitter) for instances hate speech. The keywords in relation to anti-LGBTQ+ hate in English included: <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p1.1.3">dyke</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p1.1.4">twat</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p1.1.5">faggot</span>. This contrasts <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p1.1.6">ltedi</span> which took a content search approach of users reacting to LGBTQ+ content from India.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">The high-level of code-switching and script-switching between English and other Indo-Aryan and Dravidian languages provides some level of social, cultural, and linguistic information of the training data. Both training data sets are comparable in size; however, <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p2.1.1">mlma</span> is 13.2% larger than <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p2.1.2">ltedi</span> by number of observations. The proportion of anti-LGBTQ+ hate speech in the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p2.1.3">mlma</span> is 9.1% while the proportion of anti-LGBTQ+ hate speech in the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS1.p2.1.4">ltedi</span> is 5.8%.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Annotation Process</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Bender and Friedman (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib3" title="">2018</a>)</cite> proposed including data statement framework in the hope to mitigate different forms of social bias by dutifully documenting the NLP production process. Neither data sets provided annotator metadata <cite class="ltx_cite ltx_citemacro_cite">Bender and Friedman (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib3" title="">2018</a>)</cite>; therefore, we can only infer some of the annotator information from available documentation. Where the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS2.p1.1.1">mlma</span> took a crowd-sourcing approach, the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS2.p1.1.2">ltedi</span> data set were annotated by members of the LGBTQ+ communities. Based on the limited details, <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS2.p1.1.3">ltedi</span> we know the annotators were English speakers based at the National University of Ireland Galway. Unsurprisingly, the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS2.p1.1.4">mlma</span> at 0.15 is lower than <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS2.p1.1.5">ltedi</span> at 0.67 based on Krippendorf’s alpha where 1 suggests perfect reliability while 0 suggests no reliability beyond chance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Cultural Alignment</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">With limited documentation to the data set collection and annotation process beyond the system description papers, we tentatively determine the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS3.p1.1.1">ltedi</span> is largely in alignment with anti-LGBTQ+ discourse from the South Asian cultural sphere and the <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.SSS3.p1.1.2">mlma</span> as culturally-undetermined anti-LGBTQ+ rhetoric. This creates a useful contrast which not only compares the efficacy of two training data sets, but also anti-LGBTQ+ behaviour in different varieties of World Englishes which are influenced by their own unique social, cultural, and linguistic contexts <cite class="ltx_cite ltx_citemacro_cite">Kachru (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib26" title="">1982</a>)</cite>. We predict the data set collection and annotation approaches will have an impact on the outputs of the automatic detection systems.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S2.F2.g1" src="extracted/5702373/figures/compare.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of anti-LGBTQ+ hate speech detected in 10,000 samples of posts/tweets from inner- and outer-circle varieties of English between June 2018 to June 2023 including grouped mean and the upper and lower confidence intervals.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>System Development</h3>
<figure class="ltx_table" id="S2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T3.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.5.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S2.T3.5.1.1.1" rowspan="2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S2.T3.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.5.1.1.2.1">Macro</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S2.T3.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T3.5.1.1.3.1">Weighted</span></th>
</tr>
<tr class="ltx_tr" id="S2.T3.5.2.2">
<td class="ltx_td ltx_align_center" id="S2.T3.5.2.2.1"><span class="ltx_text ltx_font_italic" id="S2.T3.5.2.2.1.1">Base</span></td>
<td class="ltx_td ltx_align_center" id="S2.T3.5.2.2.2"><span class="ltx_text ltx_font_italic" id="S2.T3.5.2.2.2.1">Retrain</span></td>
<td class="ltx_td ltx_align_center" id="S2.T3.5.2.2.3"><span class="ltx_text ltx_font_italic" id="S2.T3.5.2.2.3.1">Base</span></td>
<td class="ltx_td ltx_align_center" id="S2.T3.5.2.2.4"><span class="ltx_text ltx_font_italic" id="S2.T3.5.2.2.4.1">Retrain</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T3.5.3.3.1"><span class="ltx_text ltx_font_smallcaps" id="S2.T3.5.3.3.1.1">ltedi</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.5.3.3.2">0.78</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.5.3.3.3">0.81</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.5.3.3.4">0.95</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.5.3.3.5">0.96</th>
</tr>
<tr class="ltx_tr" id="S2.T3.5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S2.T3.5.4.4.1"><span class="ltx_text ltx_font_smallcaps" id="S2.T3.5.4.4.1.1">mlma</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.5.4.4.2">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.5.4.4.3">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.5.4.4.4">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.5.4.4.5">0.94</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span> Model evaluation metrics comparing the four candidate models by average macro <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.T3.3.m1.1"><semantics id="S2.T3.3.m1.1b"><msub id="S2.T3.3.m1.1.1" xref="S2.T3.3.m1.1.1.cmml"><mi id="S2.T3.3.m1.1.1.2" xref="S2.T3.3.m1.1.1.2.cmml">F</mi><mn id="S2.T3.3.m1.1.1.3" xref="S2.T3.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T3.3.m1.1c"><apply id="S2.T3.3.m1.1.1.cmml" xref="S2.T3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T3.3.m1.1.1.1.cmml" xref="S2.T3.3.m1.1.1">subscript</csymbol><ci id="S2.T3.3.m1.1.1.2.cmml" xref="S2.T3.3.m1.1.1.2">𝐹</ci><cn id="S2.T3.3.m1.1.1.3.cmml" type="integer" xref="S2.T3.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.3.m1.1d">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.3.m1.1e">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score and average weighted <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.T3.4.m2.1"><semantics id="S2.T3.4.m2.1b"><msub id="S2.T3.4.m2.1.1" xref="S2.T3.4.m2.1.1.cmml"><mi id="S2.T3.4.m2.1.1.2" xref="S2.T3.4.m2.1.1.2.cmml">F</mi><mn id="S2.T3.4.m2.1.1.3" xref="S2.T3.4.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T3.4.m2.1c"><apply id="S2.T3.4.m2.1.1.cmml" xref="S2.T3.4.m2.1.1"><csymbol cd="ambiguous" id="S2.T3.4.m2.1.1.1.cmml" xref="S2.T3.4.m2.1.1">subscript</csymbol><ci id="S2.T3.4.m2.1.1.2.cmml" xref="S2.T3.4.m2.1.1.2">𝐹</ci><cn id="S2.T3.4.m2.1.1.3.cmml" type="integer" xref="S2.T3.4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.4.m2.1d">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.4.m2.1e">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The first phase of our investigation involves developing multiclass classification models to detect anti-LGBTQ+ hate speech in English. We opted for a transformer-based language modelling approach. Even though the focus of <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.1">ltedi</span> is YouTube, we can adapt Pretrained Language Models (<span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.2">PLM</span>s) to specific domains, or register of language, through pretraining with additional samples of text <cite class="ltx_cite ltx_citemacro_cite">Gururangan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib18" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">We initially trained two classification models with minimal feature engineering in order to determine the best approaches to develop our automatic detection systems. We split the training data into training, development, and test sets with a train:development:test split of 90:5:5. We used Multi-Class Classification model from the Simple Transformers<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://simpletransformers.ai/</span></span></span> Python package to finetune and train the multi-class classification model. We trained each model for 8 iterations. We used AdamW as the optimiser <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib34" title="">2018</a>)</cite>. Our baseline <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.1">plm</span> is <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.2">xlm-roberta</span>, which is a cross-lingual transformer-based language model <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib8" title="">2020</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Feature Engineering</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Class imbalance had an effect on our detection system. Therefore, we collapsed the multiple classes from each training data set into a binary classification. We also removed the confidentialised usernames and URLs from <cite class="ltx_cite ltx_citemacro_citet">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib40" title="">2019</a>)</cite>, as we could not mask these high-frequency tokens from the classification model. We used RandomOverSampler from the Imbalanced Learn<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://imbalanced-learn.org/</span></span></span> Python package to upsample the minority classes. We address the register discrepancy in <cite class="ltx_cite ltx_citemacro_citet">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib7" title="">2021</a>)</cite>. We retrained <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS1.p1.1.1">xlm-roberta</span> with 120,000 samples of X (Twitter) language data from the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS1.p1.1.2">cglu</span> <cite class="ltx_cite ltx_citemacro_cite">Dunn (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib13" title="">2020</a>)</cite>. The composition of the language data included 10,000 samples from each language condition.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Model Evaluation</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.3">We present the model evaluation metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.T3" title="Table 3 ‣ 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">3</span></a>. In Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.T3" title="Table 3 ‣ 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the model evaluation results for the four candidate models (<span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.1">ltedi<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.1.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.1.1.1">b</span></sub></span>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.2">ltedi<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.2.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.2.1.1">r</span></sub></span>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.3">mlma<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.3.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.3.1.1">b</span></sub></span>, and <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.4">mlma<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.4.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.4.1.1">r</span></sub></span>). The model performance improved in three of the four candidate models based on both macro average and weighted average <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.1.m1.1"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><msub id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml">F</mi><mn id="S2.SS2.SSS2.p1.1.m1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><apply id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2">𝐹</ci><cn id="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score. Surprisingly, there were no differences between the two approaches for the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.5">mlma</span> models. With a focus on the anti-LGBTQ+ class, domain adaptation improved the <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.2.m2.1"><semantics id="S2.SS2.SSS2.p1.2.m2.1a"><msub id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS2.p1.2.m2.1.1.2" xref="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml">F</mi><mn id="S2.SS2.SSS2.p1.2.m2.1.1.3" xref="S2.SS2.SSS2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.1b"><apply id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.2">𝐹</ci><cn id="S2.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.2.m2.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score from 0.58 to 0.64 for the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.6">ltedi<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.6.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.6.1.1">r</span></sub></span> model. The <math alttext="F_{1}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.3.m3.1"><semantics id="S2.SS2.SSS2.p1.3.m3.1a"><msub id="S2.SS2.SSS2.p1.3.m3.1.1" xref="S2.SS2.SSS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.1.1.2" xref="S2.SS2.SSS2.p1.3.m3.1.1.2.cmml">F</mi><mn id="S2.SS2.SSS2.p1.3.m3.1.1.3" xref="S2.SS2.SSS2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.3.m3.1b"><apply id="S2.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1.2">𝐹</ci><cn id="S2.SS2.SSS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.SS2.SSS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.3.m3.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.3.m3.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-score for the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.7">mlma<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.7.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.7.1.1">r</span></sub></span> remains unchanged at 0.69. Based on the model performance metrics for the four candidate models, we advanced with the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.8">ltedi<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.8.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.8.1.1">r</span></sub></span> and <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.9">mlma<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.9.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.9.1.1">r</span></sub></span> classification models with domain adaptation and feature engineering during finetuning. We continued to apply domain adaptation in both systems despite not seeing significant improvements in the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.3.10">mlma<sub class="ltx_sub" id="S2.SS2.SSS2.p1.3.10.1"><span class="ltx_text" id="S2.SS2.SSS2.p1.3.10.1.1">r</span></sub></span> model to maintain consistency between the two classification models.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="718" id="S2.F3.g1" src="extracted/5702373/figures/growth_ltedi.png" width="608"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Quarterly growth rate of anti-LGBTQ+ hate speech detected with the <span class="ltx_text ltx_font_smallcaps" id="S2.F3.2.1">ltedi</span> model with number of posts/tweets by country between June 2018 and June 2023.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Communities of Interest</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Even though the <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.1">mlma</span> is supposedly culturally-agnostic, we have broadly identified the cultural alignment within the <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.2">ltedi</span> based on the data set collection and annotation process outlined in <cite class="ltx_cite ltx_citemacro_citet">Chakravarthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib7" title="">2021</a>)</cite>. More specifically, high-levels of code-switching and script-switching between English, Hindi, and Tamil in the <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.3">ltedi</span> suggests the presence of an Indian English substrate in the training data. Written English is often treated as homogeneous language; however, geographic-dialects represented by national-varieties of English maintain a constant-level of variation <cite class="ltx_cite ltx_citemacro_cite">Dunn and Wong (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib14" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Furthermore, the presence of Indian English on social media, or English spoken and written in India introduced as a result of British colonisation <cite class="ltx_cite ltx_citemacro_cite">Hickey (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib22" title="">2005</a>)</cite>, is uncontested <cite class="ltx_cite ltx_citemacro_cite">Rajee (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib45" title="">2024</a>)</cite>. In the three concentric circles model of World Englishes, Indian English is categorised as an outer-circle variety of English <cite class="ltx_cite ltx_citemacro_cite">Kachru et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib27" title="">1985</a>)</cite>. Outer-circle and inner-circle varieties of English are defined as national-varieties with British colonial ties. The distinguishing feature of outer-circle varieties is that English is not the primary language of social life and the government sector. These outer-circle varieties of English often co-exist alongside other indigenous languages.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In order to test for the influence of social, cultural, and linguistic factors, we retrieved samples of social media language from outer-circle and inner-circle varieties of English. Outer-circle varieties of English as written English originating from Ghana, India, Kenya, Malaysia, the Philippines, and Pakistan. Similarly, inner-circle varieties as written English originating from Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States. The data source of our social media language data comes from a subset of <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p3.1.1">cglu</span> corpus which contains georeferenced posts/tweets from X (Twitter) <cite class="ltx_cite ltx_citemacro_cite">Dunn (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib13" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">For each national-variety of English, we filtered the data for tweets in English. All posts/tweets were processed with hyperlinks, emojis, and user identifying information removed. In addition to the monthly samples for each country, we re-sampled monthly tweets from India over ten iterations to determine the impact of our sampling methodology. All posts/tweets were produced between July 2018 to June 2023. Of relevance to our analysis, the countries associated with these national-varieties all criminalised same-sex sexual activity as a legacy of the English common law legal system (with the exception of the Philippines) <cite class="ltx_cite ltx_citemacro_cite">Han and O’Mahoney (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib20" title="">2014</a>)</cite>. All but four of these countries (Kenya, Ghana, Pakistan, Malaysia) have since decriminalised same-sex sexual activity. However, LGBTQ+ rights vary significantly between countries and LGBTQ+ communities continue to face discrimination in response to increased anti-LGBTQ+ legislation in the United States disproportionately affecting transgender people <cite class="ltx_cite ltx_citemacro_cite">Canady (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib5" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We dedicate the current section to describe the results of the second phase of our investigation. This phase involved applying the candidate models to automatically detect anti-LGBTQ+ hate speech on real-world instances of social media data in English. Firstly, we applied both anti-LGBTQ+ hate speech detection models on the ten randomised monthly samples of social media language data from India using the same sampling methodology for other national-varieties of English. The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F1" title="Figure 1 ‣ 2.1.1 Data Collection ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">1</span></a>. As expected, the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">ltedi<sub class="ltx_sub" id="S3.p1.1.1.1"><span class="ltx_text" id="S3.p1.1.1.1.1">r</span></sub></span> model predicted higher rates of anti-LGBTQ+ hate speech; however, what was unexpected were the low number of predictions from the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">mlma<sub class="ltx_sub" id="S3.p1.1.2.1"><span class="ltx_text" id="S3.p1.1.2.1.1">r</span></sub></span> model. The narrow confidence intervals suggest little instability between the different samples and the predictions remained constant across samples.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">After validating our sampling methodology by visually inspecting the ten randomised monthly samples from India, we applied both models on random samples of inner- and outer-circle varieties of English. We compared the results of the detection models as visualised in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F2" title="Figure 2 ‣ 2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2</span></a>. These were consistent with our initial results. The rate of anti-LGBTQ+ hate speech remained constant according to the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">mlma<sub class="ltx_sub" id="S3.p2.1.1.1"><span class="ltx_text" id="S3.p2.1.1.1.1">r</span></sub></span> model, while anti-LGBTQ+ hate speech has increased over time based on a visual inspection of the results. Of interest to our investigation, the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.2">mlma<sub class="ltx_sub" id="S3.p2.1.2.1"><span class="ltx_text" id="S3.p2.1.2.1.1">r</span></sub></span> model identified a higher proportion of anti-LGBTQ+ hate speech in inner-circle varieties of English. We saw an inverse relationship with the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.3">ltedi<sub class="ltx_sub" id="S3.p2.1.3.1"><span class="ltx_text" id="S3.p2.1.3.1.1">r</span></sub></span> where we see a higher proportion of anti-LGBTQ+ hate speech in outer-circle varieties of English. The wide confidence intervals of the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.4">ltedi<sub class="ltx_sub" id="S3.p2.1.4.1"><span class="ltx_text" id="S3.p2.1.4.1.1">r</span></sub></span> suggests greater between-variety instability in outer-circle varieties of English.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We calculated the quarterly growth rates for each variety of English for the predictions from the <span class="ltx_text ltx_font_smallcaps" id="S3.p3.1.1">ltedi<sub class="ltx_sub" id="S3.p3.1.1.1"><span class="ltx_text" id="S3.p3.1.1.1.1">r</span></sub></span>. We included the total number of predicted posts/tweets in our visualisation as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F3" title="Figure 3 ‣ 2.2.2 Model Evaluation ‣ 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">3</span></a>. The growth rates allowed us to determine the growth rate for each variety of English independently. The results suggest the growth rate of predicted anti-LGBTQ+ hate speech has remained stable over time.</p>
</div>
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_bottom" id="S3.F4" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Training Data</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F4.sf1.g1" src="extracted/5702373/figures/wordcloud_mlma.png" width="517"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Predicted Outputs</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F4.sf2.g1" src="extracted/5702373/figures/wordcloud_mlma-output.png" width="517"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_smallcaps" id="S3.F4.2.1">mlma</span> Wordcloud.</figcaption>
</figure>
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_bottom" id="S3.F5" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F5.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Training Data</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F5.sf1.g1" src="extracted/5702373/figures/wordcloud_ltedi.png" width="517"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F5.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Predicted Outputs</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F5.sf2.g1" src="extracted/5702373/figures/wordcloud_ltedi-output.png" width="517"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_smallcaps" id="S3.F5.2.1">ltedi</span> Wordcloud.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The results from our study raises some interesting questions on the efficacy of these systems on real-world instances of social media data. With regards to the first research question, our transformer-based multiclass classification model enabled us to detect instances of anti-LGBTQ+ hate speech from samples of georeferenced posts/tweets from X (Twitter). We were able to manipulate the predicted outputs into different forms of time series as shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F2" title="Figure 2 ‣ 2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F3" title="Figure 3 ‣ 2.2.2 Model Evaluation ‣ 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">3</span></a>. The level of anti-LGBTQ+ hate speech has maintained a constant rate of growth despite decreasing usership on the social media platform since the acquisition of X (Twitter) by Elon Musk in 2022. The results suggest that anti-LGBTQ+ hate speech on X (Twitter) is indeed increasing in both rate and volume over time <cite class="ltx_cite ltx_citemacro_cite">Hattotuwa et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib21" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">When we compare the predicted results between the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.1">mlma<sub class="ltx_sub" id="S4.p2.1.1.1"><span class="ltx_text" id="S4.p2.1.1.1.1">r</span></sub></span> and the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.2">ltedi</span> models, we can see significant differences between the two models. This is particularly obvious when we compare the predicted outputs in Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F1" title="Figure 1 ‣ 2.1.1 Data Collection ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F2" title="Figure 2 ‣ 2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2</span></a>, where the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.3">ltedi<sub class="ltx_sub" id="S4.p2.1.3.1"><span class="ltx_text" id="S4.p2.1.3.1.1">r</span></sub></span> model on average predicted 50 times more instances of anti-LGBTQ+ hate speech than the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.4">mlma<sub class="ltx_sub" id="S4.p2.1.4.1"><span class="ltx_text" id="S4.p2.1.4.1.1">r</span></sub></span>. These was unexpected as the model evaluation metrics during model development suggested the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.5">mlma<sub class="ltx_sub" id="S4.p2.1.5.1"><span class="ltx_text" id="S4.p2.1.5.1.1">r</span></sub></span> model performed marginally better than the <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.6">ltedi<sub class="ltx_sub" id="S4.p2.1.6.1"><span class="ltx_text" id="S4.p2.1.6.1.1">r</span></sub></span>. Considering both the sampling methodology and the model development approaches were held constant between the models, we propose the differences we see in the predicted outputs is a result of the open-source training data.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">One challenge of applying multiclass classification models on unknown data is that there is no simple method to validate the results. This is because we do not have access to labelled training, development, and test sets to evaluate the model performance. We are therefore reliant on qualitative methods to validate the performance of our detection models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F4" title="Figure 4 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a> is a visual representation of the word-token frequencies between the open-source training data (a) and the predicted anti-LGBTQ+ hate speech (b) from the samples of posts/tweets. The most prominent word-token in the training data is <span class="ltx_text ltx_font_italic" id="S4.p3.1.1">faggot</span> followed by <span class="ltx_text ltx_font_italic" id="S4.p3.1.2">dyke</span>. This is not unexpected as these word-tokens (including <span class="ltx_text ltx_font_italic" id="S4.p3.1.3">twat</span>) were used to identify instances of anti-LGBTQ+ hate speech on X (Twitter). Counterintuitively, we did not see a similar distribution in the predicted outputs.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">With reference to Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F4" title="Figure 4 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a>, the word-tokens with the highest frequency in the predicted output were not <span class="ltx_text ltx_font_italic" id="S4.p4.1.1">faggot</span> or <span class="ltx_text ltx_font_italic" id="S4.p4.1.2">dyke</span>, but <span class="ltx_text ltx_font_italic" id="S4.p4.1.3">sleep</span> and <span class="ltx_text ltx_font_italic" id="S4.p4.1.4">gay</span>. When we filtered for the keyword search terms in the samples, we found few instances across the varieties of English as shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4.T4" title="Table 4 ‣ 4 Discussion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4.T5" title="Table 5 ‣ 4 Discussion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">5</span></a>. This is unexpected as the keyword search terms are highly prevalent in inner-circle varieties of spoken English (such as the United Kingdom and Ireland) <cite class="ltx_cite ltx_citemacro_cite">Love (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib35" title="">2021</a>)</cite>. This is supported by the higher word-token frequencies in inner-circle varieties of English as shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4.T4" title="Table 4 ‣ 4 Discussion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S4.T5" title="Table 5 ‣ 4 Discussion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">5</span></a>. We attribute the infrequent occurrence of LGBTQ+ slurs in direct response to X (Twitter) rules which discourages hateful conduct on the platform.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Our analysis of the <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.1">mlma<sub class="ltx_sub" id="S4.p5.1.1.1"><span class="ltx_text" id="S4.p5.1.1.1.1">r</span></sub></span> model suggests a relationship between the training data and the resulting detection model. Incidentally, we also observe this bias towards inner-circle varieties of English in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F2" title="Figure 2 ‣ 2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2</span></a> where the <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.2">mlma<sub class="ltx_sub" id="S4.p5.1.2.1"><span class="ltx_text" id="S4.p5.1.2.1.1">r</span></sub></span> is more inclined to identify more anti-LGBTQ+ hate speech in inner-circle than outer-circle varieties of English. This leads our discussion to the second research question where we determine how the social, cultural, and linguistic context impacts the efficacy of anti-LGBTQ+ hate speech detection. Although anti-LGBTQ+ discourse is consistent across languages <cite class="ltx_cite ltx_citemacro_cite">Locatelli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib33" title="">2023</a>)</cite>, slurs and swearwords are not <cite class="ltx_cite ltx_citemacro_cite">Jay and Janschewitz (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib25" title="">2008</a>)</cite>. This form of cultural bias toward inner-circle varieties of English (or oversight of outer-circle varieties) introduced during the data collection process, raises questions on the suitability of the <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.3">mlma<sub class="ltx_sub" id="S4.p5.1.3.1"><span class="ltx_text" id="S4.p5.1.3.1.1">r</span></sub></span> model in monitoring anti-LGBTQ+ hate speech.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Variety</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.2.1">dyke</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.3.1">faggot</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.4.1">twat</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.2.1.1.1">gh</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4">6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.3.2.1.1">in</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2">5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4">6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.4.3.1.1">ke</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.2">1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.3">4</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.4">7</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.5.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.5.4.1.1">my</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.2">3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.3">4</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.4">14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.6.5.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.6.5.1.1">ph</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.2">8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.3">4</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.4">8</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.7.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.7.6.1.1">pk</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.7.6.2">3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.7.6.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.7.6.4">6</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.2.1.1.1.1">gay</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.1.1">353</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3.2">
<td class="ltx_td ltx_align_center" id="S4.T4.2.3.2.1">226</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4.3">
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.3.1">295</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5.4">
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.4.1">500</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6.5">
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.5.1">701</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.2.7.6.1">478</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span> Frequency of LGBTQ+ related slurs for outer circle varieties of English.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Variety</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.2.1">dyke</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.3.1">faggot</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.4.1">twat</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.2.1.1.1">au</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.4">48</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.3.2.1.1">ca</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3">13</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.4">19</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.4.3.1.1">ie</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.2">15</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.3">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.4">62</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.5.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.5.4.1.1">nz</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.2">6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.3">14</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.4">53</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.6.5.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.6.5.1.1">uk</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.2">23</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.3">9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.4">148</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T5.1.7.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.7.6.1.1">us</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.7.6.2">19</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.7.6.3">11</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.7.6.4">13</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T5.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.2.1.1.1.1">gay</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.2.1.1">635</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.3.2">
<td class="ltx_td ltx_align_center" id="S4.T5.2.3.2.1">623</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.4.3">
<td class="ltx_td ltx_align_center" id="S4.T5.2.4.3.1">659</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.5.4">
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.4.1">627</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.6.5">
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.5.1">679</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.2.7.6.1">875</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span> Frequency of LGBTQ+ related slurs for inner circle varieties of English.</figcaption>
</figure>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">As we determined the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.1">ltedi<sub class="ltx_sub" id="S4.p6.1.1.1"><span class="ltx_text" id="S4.p6.1.1.1.1">r</span></sub></span> model to be more culturally aligned with the South Asian context, we initially predicted the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.2">ltedi</span> model would be more appropriate for South Asian contexts. However, the results suggest the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.3">ltedi<sub class="ltx_sub" id="S4.p6.1.3.1"><span class="ltx_text" id="S4.p6.1.3.1.1">r</span></sub></span> model as more fit for purpose in contrast to the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.4">mlma<sub class="ltx_sub" id="S4.p6.1.4.1"><span class="ltx_text" id="S4.p6.1.4.1.1">r</span></sub></span> model. Not only do we observe high-congruency between the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.5">ltedi<sub class="ltx_sub" id="S4.p6.1.5.1"><span class="ltx_text" id="S4.p6.1.5.1.1">r</span></sub></span> model output and the outer-circle varieties of English as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F2" title="Figure 2 ‣ 2.1.3 Cultural Alignment ‣ 2.1 Data Sources ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">2</span></a>, the word-token frequencies between the training data (a) and the predicted outputs (b) in the <span class="ltx_text ltx_font_smallcaps" id="S4.p6.1.6">ltedi</span> appear to have a similar distribution as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F5" title="Figure 5 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">Curiously, both the training data and predicted output lack slurs. Instead, we see word-tokens associated with community (e.g., <span class="ltx_text ltx_font_italic" id="S4.p7.1.1">people</span>) and religion (e.g., <span class="ltx_text ltx_font_italic" id="S4.p7.1.2">bible</span>, <span class="ltx_text ltx_font_italic" id="S4.p7.1.3">god</span>, and <span class="ltx_text ltx_font_italic" id="S4.p7.1.4">Adam</span> possibly in reference to the Abrahamic creation myth of <span class="ltx_text ltx_font_italic" id="S4.p7.1.5">Adam and Eve</span>). This is unsurprising as anti-LGBTQ+ legislation is often rooted in puritanical beliefs on morality <cite class="ltx_cite ltx_citemacro_cite">Han and O’Mahoney (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib20" title="">2014</a>)</cite>. With reference to Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S2.F3" title="Figure 3 ‣ 2.2.2 Model Evaluation ‣ 2.2 System Development ‣ 2 Methodology ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">3</span></a>, we observed a possible link between the increased growth rate with nationwide response to the Covid-19 pandemic. Once again this raises a question on the validity of the predicted outputs and whether the posts/tweets are anti-LGBTQ+ or religious/spiritual in nature (or indeed, both).</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The findings from this current paper raises a number challenges in applying hate speech detection in a real-world context. Even within national-varieties of English, we observed the impacts of social, cultural, and linguistic factors. For example, the <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">ltedi<sub class="ltx_sub" id="S5.p1.1.1.1"><span class="ltx_text" id="S5.p1.1.1.1.1">r</span></sub></span> which was culturally aligned with Indian English was more sensitive to outer circle varieties of English, while the <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.2">mlma<sub class="ltx_sub" id="S5.p1.1.2.1"><span class="ltx_text" id="S5.p1.1.2.1.1">r</span></sub></span> model was slightly more sensitive to inner circle varieties of English. We conclude that monitoring anti-LGBTQ+ hate speech with open-source training data is not problematic in itself; however, we must interpret these empirical outputs with qualitative insights to ensure these systems are fit for purpose.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The purpose of this paper is to investigate the suitability of using open-source training data to develop a multiclass classification model to monitor and forecast levels of anti-LGBTQ+ hate speech on social media across different geographic dialect contexts in English. This study contributes to the efforts in mitigating harmful hate speech experienced by LGBTQ+ communities. In our investigation, we combine methods from NLP, sociolinguistics, and discourse analysis to evaluate the effectiveness of anti-LGBTQ+ hate speech detection.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">We recognise the importance of advocate and activist-led research in particular by members of under-represented and minoritised communities <cite class="ltx_cite ltx_citemacro_cite">Hale (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib19" title="">2008</a>)</cite>. The lead author acknowledges their positionality as an active advocate and a member of the LGBTQ+ community <cite class="ltx_cite ltx_citemacro_cite">Wong (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib57" title="">2023b</a>)</cite>. The lead author is familiar with anti-LGBTQ+ discourse both in online and offline spaces and its harmful effects on members of the LGBTQ+ communities.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S5" title="5 Conclusion ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">5</span></a>, we support the critique of <cite class="ltx_cite ltx_citemacro_citet">Parker and Ruths (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib42" title="">2023</a>)</cite> for NLP researchers to reflect on the efficacy and suitability of hate speech detection models. The development of hate speech data sets impose a ‘diversity tax’ on already marginalised LGBTQ+ communities. Originally coined by <cite class="ltx_cite ltx_citemacro_citet">Padilla (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib41" title="">1994</a>)</cite>, this refers to the unintentional burden placed on marginalised peoples to address inequities, exclusion, and inaccessibility particularly in a research context. NLP researchers need to work alongside key-stakeholders (e.g., affected communities, advocates, and activists) as well as social media platforms, non-profit organisations, and government entities to determine the solutions of this social issue.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">The inclusion of unobfuscated examples of slurs, hate speech, and offensive language towards LGBTQ+ communities is a deliberate attempt to initiate the process of reclaiming and re-appropriating some anti-LGBTQ+ slurs in NLP research. Currently, there are limited best practice guidelines on the obfuscation of profanities in NLP research <cite class="ltx_cite ltx_citemacro_cite">Nozza and Hovy (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib39" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Worthen (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib58" title="">2020</a>)</cite> theorised that anti-LGBTQ+ slurs are used to stigmatise violations of social norms. Re-appropriating these stigmatising labels can enhance what were once devalued social identities <cite class="ltx_cite ltx_citemacro_cite">Galinsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib16" title="">2003</a>)</cite>. This process of ‘cleaning’ and ‘detoxifying’ slurs is also a process of resistance and to reclaim power and control <cite class="ltx_cite ltx_citemacro_cite">Popa-Wyatt (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib44" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">We argue that within context of social media research giving unwarranted attention to slurs ignores the root of this social issue: hate speech expresses hate <cite class="ltx_cite ltx_citemacro_cite">Marques (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib37" title="">2023</a>)</cite>. Many social media platforms have already put in place procedures to censor sensitive word-tokens; however, social media users continue to adopt innovative linguistic strategies such as <span class="ltx_text ltx_font_italic" id="Sx1.p5.1.1">voldermorting</span> <cite class="ltx_cite ltx_citemacro_cite">van der Nagel (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib52" title="">2018</a>)</cite> and <span class="ltx_text ltx_font_italic" id="Sx1.p5.1.2">Algospeak</span> <cite class="ltx_cite ltx_citemacro_cite">Steen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib47" title="">2023</a>)</cite> to contravene well-meaning moderation and censorship algorithms. Our results suggest hate speech training data sets do not identify the full breadth of hateful content on social media.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">This paper does not include human or animal participants. Furthermore, we abide by the data sharing rules of X (Twitter) and posts/tweets with identifiable personal details will not be shared publicly. The authors have no conflicts of interests to declare.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">In this section, we address some of the known limitations of our approach in addition to limitations of the open-source training data and the social media data we have used in the current study.</p>
</div>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Invisibility of Q+ identities</h5>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px1.p1.1">This paper uses the LGBTQ+ acronym to signify diverse gender and sexualities who continue to experience forms of discrimination and stigmatisation (namely Lesbian, Gay, Bisexual, and Transgender people). While the Q+ refers to those who are not straight or not cisgender (Queer+), we acknowledge the invisibility of other minorities who are often excluded from NLP research including intersex and indigenous expressions of gender, sexualities, and sex characteristics at birth.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Sociocultural bias during data collection</h5>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px2.p1.1">Despite including more training data, the <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px2.p1.1.1">mlma</span> identified significantly fewer instances of anti-LGBTQ+ hate speech than the <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px2.p1.1.2">ltedi</span> across the national-varieties of English. With reference to the wordclouds produced from the training data for <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px2.p1.1.3">mlma</span> and <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px2.p1.1.4">ltedi</span> as shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F4" title="Figure 4 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F5" title="Figure 5 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">5</span></a>, there is a high likelihood the keyword search (on <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px2.p1.1.5">dyke</span>, <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px2.p1.1.6">twat</span>, and <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px2.p1.1.7">faggot</span>) during the data collection process has caused the classification model to over-fit the training data. Similarly, the religious subtext in the <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px2.p1.1.8">ltedi</span> training data reinforces polarising beliefs that religion is anti-LGBTQ+. Furthermore, these detection systems do not account for semantic bleaching or the reclamation of slurs <cite class="ltx_cite ltx_citemacro_cite">Popa-Wyatt (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib44" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Pitfalls of large language models</h5>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px3.p1.1">We acknowledge the cultural and linguistic biases introduced through the <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px3.p1.1.1">plm</span>s used in our transformer-based approach. However, we have mitigated some of these impacts through domain adaptation <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#bib.bib32" title="">2019</a>)</cite>. With reference to Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01149v1#S3.F4" title="Figure 4 ‣ 3 Results ‣ Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social Media"><span class="ltx_text ltx_ref_tag">4</span></a>, we have reason to believe the transformer-based detection systems erroneously classified <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px3.p1.1.2">dylan</span>, <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px3.p1.1.3">mike</span> and <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px3.p1.1.4">like</span> with <span class="ltx_text ltx_font_italic" id="Sx2.SS0.SSS0.Px3.p1.1.5">dyke</span>. A breakdown of the character-trigrams (<span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px3.p1.1.6">#dy</span>, <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px3.p1.1.7">dyk</span>, <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px3.p1.1.8">yke</span>, and <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px3.p1.1.9">#ke</span>) confirms this belief.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Class imbalance and distribution</h5>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px4.p1.1">We were able to improve the performance of the detection model during model development by up-sampling the minority classes. The <span class="ltx_text ltx_font_smallcaps" id="Sx2.SS0.SSS0.Px4.p1.1.1">ltedi</span> detected a constant proportion of anti-LGBTQ+ hate speech between 5-10% for all varieties of English which is a similar proportion of anti-LGBTQ+ hate speech in the training data (or 5.8% of the training data). This raises potential questions on the efficacy of transformer-based classification models.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Further work</h5>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px5.p1.1">We welcome NLP researchers to address these limitations in their research especially on increasing the visibility of Q+ communities and the sociocultural biases shown in open-source training data sets and large language models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">The lead author wants to thank Dr. Benjamin Adams (University of Canterbury | Te Whare Wānanga o Waitaha) and Dr. Jonathan Dunn (University of Illinois Urbana-Champaign) for their feedback on the initial manuscript. The lead author wants to thank the three anonymous peer reviewers and the programme chairs for their constructive feedback. Lastly, the lead author wants to thank Fulbright New Zealand | Te Tūāpapa Mātauranga o Aotearoa me Amerika and their partnership with the Ministry of Business, Innovation, and Employment | Hīkina Whakatutuki for their support through the Fulbright New Zealand Science and Innovation Graduate Award.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alonso Alemany et al. (2023)</span>
<span class="ltx_bibblock">
Laura Alonso Alemany, Luciana Benotti, Hernán Maina, Lucía Gonzalez, Lautaro
Martínez, Beatriz Busaniche, Alexia Halvorsen, Amanda Rojo, and Mariela
Rajngewerc. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.c3nlp-1.10" title="">Bias assessment
for experts in discrimination, not in computer science</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the First Workshop on Cross-Cultural
Considerations in NLP (C3NLP)</em>, pages 91–106, Dubrovnik, Croatia.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arango et al. (2022)</span>
<span class="ltx_bibblock">
Aymé Arango, Jorge Pérez, and Barbara Poblete. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.is.2020.101584" title="">Hate speech
detection is not as easy as you may think: A closer look at model
validation (extended version)</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Information Systems</em>, 105:101584.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender and Friedman (2018)</span>
<span class="ltx_bibblock">
Emily M. Bender and Batya Friedman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00041" title="">Data Statements for
Natural Language Processing: Toward Mitigating System Bias and
Enabling Better Science</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Transactions of the Association for Computational Linguistics</em>,
6:587–604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buhmann and Fieseler (2021)</span>
<span class="ltx_bibblock">
Alexander Buhmann and Christian Fieseler. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.techsoc.2020.101475" title="">Towards a
deliberative framework for responsible innovation in artificial
intelligence</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Technology in Society</em>, 64:101475.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canady (2023)</span>
<span class="ltx_bibblock">
Valerie A. Canady. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1002/mhw.33603" title="">Mounting anti-LGBTQ+
bills impact mental health of youths</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Mental Health Weekly</em>, 33(15):1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakravarthi et al. (2024)</span>
<span class="ltx_bibblock">
Bharathi Raja Chakravarthi, Prasanna Kumaresan, Ruba Priyadharshini, Paul
Buitelaar, Asha Hegde, Hosahalli Shashirekha, Saranya Rajiakodi,
Miguel Ángel García, Salud María Jiménez-Zafra, José García-Díaz,
Rafael Valencia-García, Kishore Ponnusamy, Poorvi Shetty, and Daniel
García-Baena. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.ltedi-1.11" title="">Overview of Third
Shared Task on Homophobia and Transphobia Detection in Social
Media Comments</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Fourth Workshop on Language
Technology for Equality, Diversity, Inclusion</em>, pages 124–132, St.
Julian’s, Malta. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakravarthi et al. (2021)</span>
<span class="ltx_bibblock">
Bharathi Raja Chakravarthi, Ruba Priyadharshini, Rahul Ponnusamy,
Prasanna Kumar Kumaresan, Kayalvizhi Sampath, Durairaj Thenmozhi, Sathiyaraj
Thangasamy, Rajendran Nallathambi, and John Phillip McCrae. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2109.00227" title="">Dataset for
Identification of Homophobia and Transophobia in Multilingual
YouTube Comments</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2109.00227 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="">Unsupervised
Cross-lingual Representation Learning at Scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 8440–8451, Online.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2023)</span>
<span class="ltx_bibblock">
Dipto Das, Shion Guha, and Bryan Semaan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.c3nlp-1.8" title="">Toward Cultural
Bias Evaluation Datasets: The Case of Bengali Gender,
Religious, and National Identity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the First Workshop on Cross-Cultural
Considerations in NLP (C3NLP)</em>, pages 68–83, Dubrovnik, Croatia.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Davidson, Debasmita Bhattacharya, and Ingmar Weber. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-3504" title="">Racial Bias in Hate
Speech and Abusive Language Detection Datasets</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Third Workshop on Abusive
Language Online</em>, pages 25–35, Florence, Italy. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson et al. (2017)</span>
<span class="ltx_bibblock">
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1703.04009" title="">Automated Hate Speech
Detection and the Problem of Offensive Language</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:1703.04009 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/N19-1423.pdf?utm_medium=email&amp;utm_source=transaction" title="">BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of NAACL-HLT</em>, pages 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn (2020)</span>
<span class="ltx_bibblock">
Jonathan Dunn. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10579-020-09489-2" title="">Mapping
languages: the Corpus of Global Language Use</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Language Resources and Evaluation</em>, 54(4):999–1018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn and Wong (2022)</span>
<span class="ltx_bibblock">
Jonathan Dunn and Sidney Wong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.3" title="">Stability of
Syntactic Dialect Classification over Space and Time</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 29th International Conference on
Computational Linguistics</em>, pages 26–36, Gyeongju, Republic of Korea.
International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fortuna et al. (2021)</span>
<span class="ltx_bibblock">
Paula Fortuna, Laura Pérez-Mayos, Ahmed AbuRa’ed, Juan Soler-Company, and Leo
Wanner. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.nlp4posimpact-1.3" title="">Cartography of Natural Language Processing for Social Good
(NLP4SG): Searching for Definitions, Statistics and White Spots</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 1st Workshop on NLP for Positive
Impact</em>, pages 19–26, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galinsky et al. (2003)</span>
<span class="ltx_bibblock">
Adam D Galinsky, Kurt Hugenberg, Carla Groom, and Galen V Bodenhausen. 2003.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/S1534-0856(02)05009-0" title="">The
reappropriation of stigmatizing labels: Implications for social identity</a>.

</span>
<span class="ltx_bibblock">In Jeffrey Polzer, editor, <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Identity Issues in Groups</em>,
volume 5 of <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">Research on Managing Groups and Teams</em>, pages
221–256. Emerald Group Publishing Limited.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">García-Díaz et al. (2020)</span>
<span class="ltx_bibblock">
José Antonio García-Díaz, Ángela Almela, Gema Alcaraz-Mármol, and Rafael
Valencia-García. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6292" title="">UMUCorpusClassifier: Compilation and evaluation of linguistic corpus for
Natural Language Processing tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Procesamiento del Lenguaje Natural</em>, 65(0):139–142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2020)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
Doug Downey, and Noah A. Smith. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2004.10964" title="">Don’t Stop
Pretraining: Adapt Language Models to Domains and Tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2004.10964 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hale (2008)</span>
<span class="ltx_bibblock">
Charles R. Hale. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.degruyter.com/document/doi/10.1525/9780520916173/html" title="">Engaging
Contradictions: Theory, Politics, and Methods of Activist
Scholarship</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Engaging Contradictions</em>. University of California Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han and O’Mahoney (2014)</span>
<span class="ltx_bibblock">
Enze Han and Joseph O’Mahoney. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1080/09557571.2013.867298" title="">British
colonialism and the criminalization of homosexuality</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Cambridge Review of International Affairs</em>, 27(2):268–288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hattotuwa et al. (2023)</span>
<span class="ltx_bibblock">
Sanjana Hattotuwa, Kate Hannah, and Kayli Taylor. 2023.

</span>
<span class="ltx_bibblock">Transgressive transitions: Transphobia, community building,
bridging, and bonding within Aotearoa New Zealand’s disinformation
ecologies march-April 2023.

</span>
<span class="ltx_bibblock">Technical report, The Disinformation Project, New Zealand.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hickey (2005)</span>
<span class="ltx_bibblock">
Raymond Hickey, editor. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/CBO9780511486920" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1.1">Legacies of
Colonial English: Studies in Transported Dialects</em></a>.

</span>
<span class="ltx_bibblock">Studies in English Language. Cambridge University Press,
Cambridge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hovy and Spruit (2016)</span>
<span class="ltx_bibblock">
Dirk Hovy and Shannon L. Spruit. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-2096" title="">The Social Impact
of Natural Language Processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short
Papers)</em>, pages 591–598, Berlin, Germany. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahan and Oussalah (2023)</span>
<span class="ltx_bibblock">
Md Saroar Jahan and Mourad Oussalah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.neucom.2023.126232" title="">A systematic
review of hate speech automatic detection using natural language processing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Neurocomputing</em>, 546:126232.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jay and Janschewitz (2008)</span>
<span class="ltx_bibblock">
Timothy Jay and Kristin Janschewitz. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1515/JPLR.2008.013" title="">The pragmatics of
swearing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Journal of Politeness Research Language Behaviour Culture</em>,
4(2):267–288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kachru (1982)</span>
<span class="ltx_bibblock">
Braj B. Kachru. 1982.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">The Other tongue: English across cultures</em>.

</span>
<span class="ltx_bibblock">University of Illinois Press, Urbana-Champaign.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kachru et al. (1985)</span>
<span class="ltx_bibblock">
Braj B. Kachru, R. Quirk, and H. G. Widdowson. 1985.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://books.google.com/books?hl=en&amp;lr=&amp;id=Z3mydCcllYYC&amp;oi=fnd&amp;pg=PA241&amp;ots=3hj0jVQ1y6&amp;sig=qwdJzgkgac-1CP1LaRwv-xMlsZw" title="">Standards, codification and sociolinguistic realism</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">World Englishes. Critical Concepts in Linguistics</em>, pages
241–270.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanuja et al. (2021)</span>
<span class="ltx_bibblock">
Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey,
Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu,
Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, and
Partha Talukdar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2103.10730" title="">MuRIL:
Multilingual Representations for Indian Languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2103.10730 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kowsari et al. (2019)</span>
<span class="ltx_bibblock">
Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu,
Laura Barnes, and Donald Brown. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3390/info10040150" title="">Text Classification
Algorithms: A Survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Information</em>, 10(4):150.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumaresan et al. (2023)</span>
<span class="ltx_bibblock">
Prasanna Kumar Kumaresan, Rahul Ponnusamy, Ruba Priyadharshini, Paul Buitelaar,
and Bharathi Raja Chakravarthi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.nlp.2023.100041" title="">Homophobia and
transphobia detection for low-resourced languages in social media comments</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Natural Language Processing Journal</em>, 5:100041.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laaksonen et al. (2020)</span>
<span class="ltx_bibblock">
Salla-Maaria Laaksonen, Jesse Haapoja, Teemu Kinnunen, Matti Nelimarkka, and
Reeta Pöyhtäri. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fdata.2020.00003" title="">The Datafication
of Hate: Expectations and Challenges in Automated Hate Speech
Monitoring</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Frontiers in Big Data</em>, 3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1907.11692" title="">RoBERTa: A
Robustly Optimized BERT Pretraining Approach</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:1907.11692 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Locatelli et al. (2023)</span>
<span class="ltx_bibblock">
Davide Locatelli, Greta Damo, and Debora Nozza. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.c3nlp-1.3" title="">A
Cross-Lingual Study of Homotransphobia on Twitter</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the First Workshop on Cross-Cultural
Considerations in NLP (C3NLP)</em>, pages 16–24, Dubrovnik, Croatia.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2018)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled
Weight Decay Regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Love (2021)</span>
<span class="ltx_bibblock">
Robbie Love. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1515/text-2020-0051" title="">Swearing in informal
spoken English: 1990s–2010s</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Text &amp; Talk</em>, 41(5-6):739–762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maimaitituoheti et al. (2022)</span>
<span class="ltx_bibblock">
Abulimiti Maimaitituoheti, Yong Yang, and Xiaochao Fan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.ltedi-1.19" title="">ABLIMET
@LT-EDI-ACL2022: A Roberta based Approach for
Homophobia/Transphobia Detection in Social Media</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the Second Workshop on Language
Technology for Equality, Diversity and Inclusion</em>, pages 155–160,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marques (2023)</span>
<span class="ltx_bibblock">
Teresa Marques. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1111/japp.12608" title="">The Expression of
Hate in Hate Speech</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Journal of Applied Philosophy</em>, 40(5):769–787.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Norris (2001)</span>
<span class="ltx_bibblock">
Pippa Norris. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/CBO9781139164887" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1.1">Digital
Divide: Civic Engagement, Information Poverty, and the Internet
Worldwide</em></a>.

</span>
<span class="ltx_bibblock">Communication, Society and Politics. Cambridge University Press,
Cambridge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nozza and Hovy (2023)</span>
<span class="ltx_bibblock">
Debora Nozza and Dirk Hovy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.findings-acl.240/" title="">The State
of Profanity Obfuscation in Natural Language Processing
Scientific Publications</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Findings of the Association for Computational
Linguistics: ACL 2023</em>, pages 3897–3909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ousidhoum et al. (2019)</span>
<span class="ltx_bibblock">
Nedjma Ousidhoum, Zizheng Lin, Hongming Zhang, Yangqiu Song, and Dit-Yan Yeung.
2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1474" title="">Multilingual and
Multi-Aspect Hate Speech Analysis</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)</em>, pages 4675–4684, Hong Kong, China. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Padilla (1994)</span>
<span class="ltx_bibblock">
Amado M. Padilla. 1994.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.2307/1176259" title="">Ethnic Minority
Scholars, Research, and Mentoring: Current and Future Issues</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Educational Researcher</em>, 23(4):24–27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parker and Ruths (2023)</span>
<span class="ltx_bibblock">
Sara Parker and Derek Ruths. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1073/pnas.2209384120" title="">Is hate speech
detection the solution the world wants?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the National Academy of Sciences</em>,
120(10):e2209384120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1493" title="">How Multilingual is
Multilingual BERT?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics</em>, pages 4996–5001, Florence,
Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popa-Wyatt (2020)</span>
<span class="ltx_bibblock">
Mihaela Popa-Wyatt. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1163/18756735-09701009" title="">Reclamation:
Taking Back Control of Words</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Grazer Philosophische Studien</em>, 97(1):159–176.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajee (2024)</span>
<span class="ltx_bibblock">
Clarissa Jane Rajee. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://languageinindia.com/march2024/clarissacitizensociolinguisticsindianenglish.pdf" title="">Analyzing Social Values of Indian English in YouTube Video
Comments: A Citizen Sociolinguistic Perspective</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Strength for Today and Bright Hope for Tomorrow Volume 24: 3
March 2024 ISSN 1930-2940</em>, page 9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1163" title="">The Risk of Racial
Bias in Hate Speech Detection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics</em>, pages 1668–1678, Florence,
Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steen et al. (2023)</span>
<span class="ltx_bibblock">
Ella Steen, Kathryn Yurechko, and Daniel Klug. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1177/20563051231194586" title="">You Can (Not)
Say What You Want: Using Algospeak to Contest and Evade
Algorithmic Content Moderation on TikTok</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Social Media + Society</em>, 9(3).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stefania and Buf (2021)</span>
<span class="ltx_bibblock">
Oana Stefania and Diana-Maria Buf. 2021.

</span>
<span class="ltx_bibblock">Hate Speech in Social Media and Its Effects on the LGBT
Community: A Review of the Current Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Romanian Journal of Communication &amp; Public Relations</em>,
23(1):47–55.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sánchez-Sánchez et al. (2024)</span>
<span class="ltx_bibblock">
Ana M. Sánchez-Sánchez, David Ruiz-Muñoz, and Francisca J.
Sánchez-Sánchez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s13178-023-00879-z" title="">Mapping
Homophobia and Transphobia on Social Media</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Sexuality Research and Social Policy</em>, 21(1):210–226.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Celis (2019)</span>
<span class="ltx_bibblock">
Yi Chern Tan and L. Elisa Celis. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html" title="">Assessing Social and Intersectional Biases in Contextualized Word
Representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</em>,
volume 32. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tontodimamma et al. (2021)</span>
<span class="ltx_bibblock">
Alice Tontodimamma, Eugenia Nissi, Annalina Sarra, and Lara Fontanella. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s11192-020-03737-6" title="">Thirty years of
research into hate speech: topics of interest and their evolution</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Scientometrics</em>, 126(1):157–179.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Nagel (2018)</span>
<span class="ltx_bibblock">
Emily van der Nagel. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1177/1329878X18783002" title="">‘Networks that
work too well’: intervening in algorithmic connections</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Media International Australia</em>, 168(1):81–92.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vidgen and Derczynski (2020)</span>
<span class="ltx_bibblock">
Bertie Vidgen and Leon Derczynski. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1371/journal.pone.0243300" title="">Directions in
abusive language training data, a systematic review: Garbage in, garbage
out</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">PLOS ONE</em>, 15(12):e0243300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong and Durward (2024)</span>
<span class="ltx_bibblock">
Sidney Wong and Matthew Durward. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.ltedi-1.19" title="">cantnlp@LT-EDI-2024: Automatic Detection of Anti-LGBTQ+ Hate
Speech in Under-resourced Languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the Fourth Workshop on Language
Technology for Equality, Diversity, Inclusion</em>, pages 177–183, St.
Julian’s, Malta. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong et al. (2023)</span>
<span class="ltx_bibblock">
Sidney Wong, Matthew Durward, Benjamin Adams, and Jonathan Dunn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.ltedi-1.15" title="">cantnlp@LT-EDI-2023: Homophobia/Transphobia Detection in Social
Media Comments using Spatio-Temporally Retrained Language
Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the Third Workshop on Language
Technology for Equality, Diversity and Inclusion</em>, pages 103–108,
Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong (2023a)</span>
<span class="ltx_bibblock">
Sidney Gig-Jan Wong. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.6084/m9.figshare.24041403.v1" title="">Monitoring
Hate Speech and Offensive Language on Social Media</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Fourth Spatial Data Science Symposium</em>, University
of Canterbury.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong (2023b)</span>
<span class="ltx_bibblock">
Sidney Gig-Jan Wong. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://livedplacespublishing.com/book/isbn/9781915271488" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1.1">Queer Asian Identities in Contemporary Aotearoa New
Zealand: One Foot Out of the Closet</em></a>.

</span>
<span class="ltx_bibblock">Lived Places Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Worthen (2020)</span>
<span class="ltx_bibblock">
Meredith Worthen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.taylorfrancis.com/books/mono/10.4324/9781315280332/queers-bis-straight-lies-meredith-worthen" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1.1">Queers, bis, and straight lies: An intersectional examination of
LGBTQ stigma</em></a>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jul  1 10:14:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
