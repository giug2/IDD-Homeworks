<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Claim Verification in the Age of Large Language Models: A Survey</title>
<!--Generated on Mon Aug 26 14:41:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.14317v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx1" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx2" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Search Criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Claim Verification Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px1" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Claim Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px2" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Check-worthy Claim Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px3" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Claim Matching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px4" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Document/Evidence Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px5" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Rationale/Sentence Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px6" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Veracity Label Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx3.SS0.SSS0.Px7" title="In Claim Verification Pipeline ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Explanation/Justification Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">LLM Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx1" title="In LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Evidence Retrieval Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx2" title="In LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Prompt Creation Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx3" title="In LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Transfer Learning Strategies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx3.SSSx1" title="In Transfer Learning Strategies ‣ LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Fine-Tuning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx3.SSSx2" title="In Transfer Learning Strategies ‣ LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">In-Context Learning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx4" title="In LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">LLM Generation Strategies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx4.SSSx1" title="In LLM Generation Strategies ‣ LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Label and Evidence Generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.SSx4.SSSx2" title="In LLM Generation Strategies ‣ LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Explainable Generation.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx5" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Evaluation and Benchmarking</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx5.SSx1" title="In Evaluation and Benchmarking ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx5.SSx2" title="In Evaluation and Benchmarking ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx5.SSx3" title="In Evaluation and Benchmarking ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Shared Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx6" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Open Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx6.SSx3.SSSx2.Px1" title="In Open Challenges ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Handling Irrelevant Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx6.SSx3.SSSx2.Px2" title="In Open Challenges ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Handling Knowledge Conflicts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx6.SSx3.SSSx2.Px3" title="In Open Challenges ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Multilinguality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx7" title="In Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Claim Verification in the Age of Large Language Models: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Alphaeus Dmonte<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Roland Oruche<sup class="ltx_sup" id="id2.2.id2">2</sup>,
Marcos Zampieri<sup class="ltx_sup" id="id3.3.id3">1</sup>,
Prasad Calyam<sup class="ltx_sup" id="id4.4.id4">2</sup>,
Isabelle Augenstein<sup class="ltx_sup" id="id5.5.id5">3</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use the terms <em class="ltx_emph ltx_font_italic" id="footnote1.1">claim verification</em> and <em class="ltx_emph ltx_font_italic" id="footnote1.2">fact verification</em> interchangeably given the overlap between the two concepts.</span></span></span> Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<figure class="ltx_figure" id="Sx1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="878" id="Sx1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of claim verification systems between NLP-based (traditional) and LLM-based for claim veracity.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="Sx1.F2.g1" src="x2.png" width="678"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The archetypal claim verification pipeline and its main components.</figcaption>
</figure>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">False information is widely present on social media and on the Web motivating the development of automated fact verification systems <cite class="ltx_cite ltx_citemacro_citep">(Guo, Schlichtkrull, and Vlachos <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib31" title="">2022</a>)</cite>. The introduction of LLMs has provided malicious actors with sophisticated ways of creating and disseminating false information. Recent election cycles saw a large number of claims spread across both social media and news platforms alike <cite class="ltx_cite ltx_citemacro_citep">(Dmonte et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib24" title="">2024</a>)</cite>. Similarly, during the COVID-19 pandemic, many claims were spread across social media platforms. Many of these claims were factually inaccurate which led to the spread of misinformation <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib125" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Fact-checking potentially false claims is an essential moderation task to reduce the spread of misinformation. Several organizations such as FactCheck, PolitiFact, NewsGuard, and Full Fact perform manual fact-checking to verify claims in different domains. However, this has been regarded as a laborious task that requires domain expertise <cite class="ltx_cite ltx_citemacro_citep">(Adair et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib1" title="">2017</a>; Hanselowski <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib34" title="">2020</a>)</cite> and it has become less and less feasible due to the sheer volume of misinformation that can be generated by humans and AI models. Automated fact-checking has become an increasingly popular approach to verify the veracity of claims in a given text.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We acknowledge that claim verification models can also be applied to other modalities of data (e.g., images). In this survey, we address models applied to text only.</span></span></span> There are several steps involved in the fact-verification pipelines, but the three main components are claim detection, evidence retrieval, and veracity prediction.
Models used in these steps have followed the general methodological developments of the field and we have thus observed an increase in the use of LLMs for claim verification <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Gao <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>; Wang and Shu <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib101" title="">2023</a>; Quelle and Bovet <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib77" title="">2024</a>)</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example claim veracity scenario and compares the architectural differences between NLP-based (traditional) and LLM-based claim verification systems. Compared to traditional systems which use NLP-based models, fact verification are less prone to error propagation, and provide explainable/interpretable generations/justification when verifying claims.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Despite this, LLMs are pre-trained on very large collections of texts and are prone to hallucinations, often generating texts containing incorrect information. Furthermore, such models can be used to generate misinformation at scale <cite class="ltx_cite ltx_citemacro_citep">(Chen and Shu <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib10" title="">2023</a>; Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib125" title="">2023</a>; Dmonte et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib24" title="">2024</a>)</cite> and therefore they can be exploited by malicious actors to generate and spread wrong and factually incorrect information at an unprecedented rate <cite class="ltx_cite ltx_citemacro_citep">(Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib70" title="">2023c</a>)</cite>. Furthermore, using these pre-trained models for fact-verification may generate incorrect veracity labels, as the models may also rely on obsolete information to assess the veracity of a claim. Approaches like RAG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib28" title="">2023</a>)</cite>, which aid the models in their decision-making abilities are used in this task for the model to get access to the most recent information during the fact-verification task.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">A few general automated claim verification surveys have been published over the years <cite class="ltx_cite ltx_citemacro_citep">(Zeng, Abumansour, and Zubiaga <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib118" title="">2021</a>; Bekoulis, Papagiannopoulou, and Deligiannis <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib6" title="">2021</a>; Guo, Schlichtkrull, and Vlachos <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib31" title="">2022</a>)</cite> as well as a couple of surveys focusing on particular aspects of the task such as explainability <cite class="ltx_cite ltx_citemacro_citep">(Vallayil et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib93" title="">2023</a>)</cite> and applications to specialized domains such as scientific texts <cite class="ltx_cite ltx_citemacro_citep">(Vladika and Matthes <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib95" title="">2023</a>)</cite>. However, all past related surveys, lack consideration for LLM-based approaches or focus on specific sub-tasks of the pipeline <cite class="ltx_cite ltx_citemacro_citep">(Panchendrarajan and Zubiaga <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib71" title="">2024</a>)</cite>. In this paper, we fill this important gap by surveying LLM-based frameworks proposed in recent years. To the best of our knowledge, this is the first survey to explore claim verification with LLMs. We expect it to be a valuable resource to researchers in the field opening exciting new avenues for future research.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Search Criteria</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We search various well-known repositories<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>ACM: <span class="ltx_text ltx_font_italic" id="footnote3.1">portal.acm.org</span>. IEEE Xplore: <span class="ltx_text ltx_font_italic" id="footnote3.2">ieeexplore.ieee.org</span>. Scopus: <span class="ltx_text ltx_font_italic" id="footnote3.3">scopus.com</span>. ACL: <span class="ltx_text ltx_font_italic" id="footnote3.4">aclanthology.org</span>. Web of Science: <span class="ltx_text ltx_font_italic" id="footnote3.5">isiknowledge.com</span>. Springer: <span class="ltx_text ltx_font_italic" id="footnote3.6">link.springer.com</span>. ArXiv: <span class="ltx_text ltx_font_italic" id="footnote3.7">arxiv.org</span>. CEUR: <span class="ltx_text ltx_font_italic" id="footnote3.8">https://ceur-ws.org/</span>.</span></span></span> of scientific articles to collect the papers that serve as primary sources for this survey. We apply queries including <em class="ltx_emph ltx_font_italic" id="Sx2.p1.1.1">LLMs</em>, <em class="ltx_emph ltx_font_italic" id="Sx2.p1.1.2">claim verification</em>, <em class="ltx_emph ltx_font_italic" id="Sx2.p1.1.3">fact verification</em> and related keywords on these repositories. We focus primarily on the <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.4">ACL Anthology</span>, the <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.5">ACM Digital Library</span>, and <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.6">IEEE Xplore</span>, and proceedings of related conferences such as <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.7">AAAI</span> and <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.8">IJCAI</span>. We further search on <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.9">Scopus</span>, <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.10">Springer Link</span>, and <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.11">Science@Direct</span>, and <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.12">ArXiv</span>.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">We collected and reviewed over 100 papers based on our search terms and filter papers based on the following search criteria: (i) since the scope of our review is on text-based LLM claim verification systems, we omit papers related to multimodal approaches (e.g., text-to-images), our papers from other modalities in claim verification (e.g., images, graph-based systems). (ii) since our review focuses on LLM-based systems for claim veracity, we omit papers related to claim identification, claim detection, and/or detecting LLM-generated content. Based on our filtering criteria, we collected a total of 49 papers related to LLM-based approaches for veracity labeling. The papers on topic LLM-based veracity labeling have been published primarily in the ACL Anthology and ACM Digital Library, but also in other repositories such as the IEEE Xplore. We release this curated list to be publicly available.</p>
</div>
<figure class="ltx_figure" id="Sx2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="Sx2.F3.g1" src="x3.png" width="823"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Claim verification pipeline using large language models. Instead of traditional automatic claim verification, this pipeline creates a prompt from the retrieved evidence and the input claim as input to the LLM to generate a label, sentence evidence, and/or explanation of its response.</figcaption>
</figure>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Claim Verification Pipeline</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx1.F2" title="Figure 2 ‣ Introduction ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a> shows an archetypal claim verification pipeline consisting of the following modules: claim detection, claim matching, claim check-worthiness, document/evidence retrieval, rationale/sentence selection, veracity label prediction, and explanation/justification generation. Many proposed systems make use of only some of these modules. The following subsections describe each of these modules in detail. Finally, as described in <cite class="ltx_cite ltx_citemacro_citet">Panchendrarajan and Zubiaga (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib71" title="">2024</a>)</cite>, performance evaluation in these sub-tasks is usually carried out using well-established automatic evaluation metrics used in text classification such as Precision, Recall, and F-score.</p>
</div>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Claim Detection</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px1.p1.1">Input texts may contain one or more statements but not all statements are claims. Given the input text, a claim detection module is designed to identify all the statements containing a claim. For example, the statement <em class="ltx_emph ltx_font_italic" id="Sx3.SS0.SSS0.Px1.p1.1.1">’I loved the movie Oppenheimer.’</em> is an opinion, whereas the statement <em class="ltx_emph ltx_font_italic" id="Sx3.SS0.SSS0.Px1.p1.1.2">’The COVID-19 pandemic started in Texas.’</em> contains a claim.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Check-worthy Claim Identification</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px2.p1.1">Not all the identified claims are check-worthy. In the check-worthy claim identification sub-task, given an input claim, the task is to identify the claims that include real-world assertions and that may need to be verified <cite class="ltx_cite ltx_citemacro_citep">(Hassan, Li, and Tremayne <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib35" title="">2015</a>; Nakov et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib64" title="">2021</a>)</cite>. This is a subjective task and it relies on the factors like popularity of the claim, public interest in determining the veracity of the claim, etc. For example, the claim <em class="ltx_emph ltx_font_italic" id="Sx3.SS0.SSS0.Px2.p1.1.1">’The President met the State Governor to discuss the infrastructure deal.’</em> is less check-worthy than the claim <em class="ltx_emph ltx_font_italic" id="Sx3.SS0.SSS0.Px2.p1.1.2">’Drinking salt water cures COVID.’</em>.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Claim Matching</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px3.p1.1">The identified check-worthy claims can be matched to the previously fact-checked claims. Given an input claim and a database of previously fact-checked claims, claim matching is used to determine if the input claim is previously fact-checked and exists in the database <cite class="ltx_cite ltx_citemacro_citep">(Shaar et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib84" title="">2020</a>; Nakov et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib64" title="">2021</a>)</cite>. This can help avoid the next steps in the pipeline and a veracity label can be predicted directly.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Document/Evidence Retrieval</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px4.p1.1">If the input claim does not exist in the database of fact-checked claims, the claim needs to be verified. In the document or evidence retrieval sub-task, all the relevant documents related to the input claim are extracted, either from an external database or through an internet search/information retrieval<cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib11" title="">2017</a>)</cite>. A threshold of how many documents are to be retrieved can be predetermined.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Rationale/Sentence Selection</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px5.p1.1">All the information in the retrieved documents is not relevant to the claim. Hence in the rationale selection task, only the information or evidence most relevant to the claim are selected to be used to predict the veracity label.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Veracity Label Prediction</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px6.p1.1">Once the rationale is selected, the claim along with the rationale and additional features if any, are given as input to a machine learning model. The task of the classifier is to predict a veracity label from among the following three labels; ’SUPPORTED’, ’REFUTED’, or ’NOT ENOUGH EVIDENCE’. In some datasets, the labels can also be ’TRUE’, or ’FALSE’, depending on the specific task.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx3.SS0.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">Explanation/Justification Generation</h5>
<div class="ltx_para" id="Sx3.SS0.SSS0.Px7.p1">
<p class="ltx_p" id="Sx3.SS0.SSS0.Px7.p1.1">Recent works have focused on generating explanations for the veracity labels prediction. This specific task is focused on generating natural language justifications or explanations for the prediction considering the claim and evidence to generate these explanations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">LLM Approaches</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">In this section, we describe the recent advancements that enable LLMs to be robust in fact verification scenarios. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx2.F3" title="Figure 3 ‣ Search Criteria ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example pipeline that encapsulates multiple component modules (i.e., Evidence Retrieval, Prompt Creation, Transfer Learning, and LLM Generation) for verifying claims. Different from traditional fact verification pipelines that select evidence set for verifying claims, LLM-based claim verification conditions generated text based on the concatenated input claim and retrieved evidence. The ability of large, pre-trained LMs augmented with retrieval enables them to perform well on knowledge-intensive tasks such as text generation.</p>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Evidence Retrieval Strategies</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">RAG models, which have been developed to combat the issue of hallucination in LLMs in knowledge-intensive tasks, have shown success in the scope of fact verification <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib50" title="">2020</a>; Izacard et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib39" title="">2023</a>; Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib28" title="">2023</a>; Guan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib30" title="">2023</a>)</cite>. Early work such as in <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib50" title="">2020</a>)</cite> developed a framework that incorporates a retriever to retrieve evidence from an external database such as Wikipedia for conditionally generating veracity labels in fact verification. Authors in <cite class="ltx_cite ltx_citemacro_citet">Izacard et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib39" title="">2023</a>)</cite> demonstrate that RAGs perform well on the FEVER shared task <cite class="ltx_cite ltx_citemacro_citep">(Thorne et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib92" title="">2018b</a>)</cite>) in few-shot settings, showing approximately 5% improvement over large-scale LLMs such as Gopher <cite class="ltx_cite ltx_citemacro_citep">(Rae et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib78" title="">2021</a>)</cite> with significantly fewer parameters (i.e., 11B compared to 280B). Other works consider the optimization of either document ranking <cite class="ltx_cite ltx_citemacro_citep">(Glass et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib29" title="">2022</a>; Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib17" title="">2022c</a>; Hofstätter et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib36" title="">2023</a>)</cite> or input claims (i.e., queries) <cite class="ltx_cite ltx_citemacro_citep">(Hang, Yu, and Tan <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib33" title="">2024</a>)</cite> as a crucial step for improving evidence retrieval for veracity labeling. Authors in <cite class="ltx_cite ltx_citemacro_citet">Hofstätter et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib36" title="">2023</a>)</cite> use an autoregressive re-ranker to get the most relevant passages from the retriever. These are then passed to the generation model to generate the veracity label.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1">Despite this, RAG models can often fail when encountering long or complex input claims, causing the model to incorrectly generate veracity labels or evidence sentences. Recent works have addressed this issue by segmenting long claims into smaller sub-claims and performing multiple rounds of retrieval <cite class="ltx_cite ltx_citemacro_citep">(Khattab, Potts, and Zaharia <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib47" title="">2021</a>; Shao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib85" title="">2023</a>)</cite>. Authors in <cite class="ltx_cite ltx_citemacro_citet">Khattab, Potts, and Zaharia (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib47" title="">2021</a>)</cite> present a pipeline for multi-hop claim verification that uses an iterative retriever and neural methods for effective document retrieval and re-ranking.  <cite class="ltx_cite ltx_citemacro_citet">Shao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib85" title="">2023</a>)</cite> show that using a re-ranker to distill knowledge to a retriever helps close the semantic gaps between a query and document passage when verifying claims using iterative RAGs. Other works address the issue of complex claims by using fine-grained retrieval techniques based on claim decomposition <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib13" title="">2023a</a>; Zhang and Gao <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>; Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib69" title="">2023b</a>)</cite>.<cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib13" title="">2023a</a>)</cite> generate sub-questions based on a claim, which a document retriever uses to retrieve relevant documents. A fine-grained retriever retrieves top-k text spans as evidence based on a k-word window and BM25. Authors in <cite class="ltx_cite ltx_citemacro_citet">Zhang and Gao (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>)</cite> decompose a claim into sub-claims and generate questions to verify the sub-claims. External knowledge sources are used to retrieve relevant information to verify the sub-claims and generate a final veracity label. <cite class="ltx_cite ltx_citemacro_citet">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib69" title="">2023b</a>)</cite> follow a programming paradigm, where the claim is broken down into subtasks and the final label is the aggregation of the execution of each subtask. The fact-verification subtask uses external knowledge to retrieve relevant evidence for a claim.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p3">
<p class="ltx_p" id="Sx4.SSx1.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Hang, Yu, and Tan (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib33" title="">2024</a>)</cite> retrieve evidence based on generated knowledge graphs of evidences. They generate a knowledge graph of user query or input claim and compare it to the database of knowledge graphs to retrieve the most relevant information for claim verification. Authors in <cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib37" title="">2023</a>)</cite> propose a latent variable model that allows the retrieval of the most relevant evidence sentences from a document while removing the irrelevant sentences. This approach reduces the noisy data during the verification process. Stochastic-RAG, an approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Zamani and Bendersky (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib115" title="">2024</a>)</cite> uses a stochastic sampling without replacement process for evidence retrieval and selection. This approach overcomes the ranking and selection of the evidence hence optimizing the RAG model. Authors in <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib121" title="">2023</a>)</cite> optimize the evidence retrieval process by using feedback from the claim verifier. The divergence between the evidence from a retrieved evidence set provided to the verifier and the gold standard evidence, acts as a feedback signal used to train the retriever. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib110" title="">2024b</a>)</cite> propose Search-in-the-Chain, an approach where an LLM generates a reasoning chain and based on the answer to each node in the chain, retrieval can be used to correct an answer or provide additional knowledge. The approach improves the generation accuracy of the LLM.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Prompt Creation Strategies</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">Text prompting has been shown to be an effective technique for improving the desired output of large-scale generative models. In the context of claim verification, several works investigate prompting strategies, using both manual and automated techniques, for building more robust claim verification systems <cite class="ltx_cite ltx_citemacro_citet">Zhang and Gao (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib53" title="">2023c</a>); Zeng and Gao (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib117" title="">2023</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib15" title="">2023b</a>)</cite>. Authors in <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Gao <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>)</cite> develop a hierarchical prompting technique that enables LLMs to verify multiple sub-claims using a step-by-step approach. The work in <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib53" title="">2023c</a>)</cite> demonstrates a self-sufficient claim verification through prompting instructions on multiple language models.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1">ProToCo <cite class="ltx_cite ltx_citemacro_citep">(Zeng and Gao <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib117" title="">2023</a>)</cite> demonstrates improved claim verification performance of LLMs by leveraging a consistency mechanism to construct variants of the original claim-evidence pair prompt based on three logical relations (i.e., confirmation, negation, uncertainty). Authors in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib15" title="">2023b</a>)</cite> develop a unified retrieval framework that employs discrete, continuous, and hybrid prompt strategies for adjusting to various knowledge-intensive tasks such as claim verification. Other works such as the FactualityPrompts <cite class="ltx_cite ltx_citemacro_citep">(Lee et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>)</cite> framework test the output generations of LLMs given an input prompt and use an external database such as Wikipedia to calculate factuality and quality measures compared to the ground truth. Other works aim to improve the LLM’s reasoning abilities by appending the claims with evidence during prompting in the context of claim verification and text generation <cite class="ltx_cite ltx_citemacro_citep">(Parvez <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib73" title="">2024</a>; Dougrez-Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib25" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Transfer Learning Strategies</h3>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Fine-Tuning.</h4>
<div class="ltx_para" id="Sx4.SSx3.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx1.p1.1">Although recent studies show the success of pre-trained LLMs on zero- or few-shot tasks, they often fail to verify real-world claims given their limited internal knowledge. The success of fine-tuning has motivated recent work on claim verification <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib16" title="">2022b</a>; Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib67" title="">2021</a>; Zeng and Zubiaga <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib119" title="">2024</a>)</cite>. The work in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib16" title="">2022b</a>)</cite> leverages a language model to fine-tune over an external corpus for retrieving passage titles and evidence sentences using constrained beam search. The results showed improved performance against the traditional fact verification pipeline over the FEVER dataset. Other work demonstrates that using GPT models to generate synthetic training data improves the performance of LLMs on various tasks such as fact checking <cite class="ltx_cite ltx_citemacro_citep">(Tang, Laban, and Durrett <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib90" title="">2024</a>)</cite> and claim matching <cite class="ltx_cite ltx_citemacro_citep">(Choi and Ferrara <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib21" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.SSSx1.p2">
<p class="ltx_p" id="Sx4.SSx3.SSSx1.p2.1">Authors in <cite class="ltx_cite ltx_citemacro_citet">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib67" title="">2021</a>)</cite> develop a pipeline for creating a fact verification dataset and fine-tuning a language model by leveraging passages from Wikipedia to generate QA pairs related to claim veracity. The authors show that it can improve state-of-the-art language models in zero-shot settings. The work in <cite class="ltx_cite ltx_citemacro_citep">(Zeng and Zubiaga <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib119" title="">2024</a>)</cite> shows that using unlabelled pairwise data to increase the alignment between claim-evidence pairs shows significant improvement of LLM performance in few-shot claim verification tasks. In addition, other recent works leverage techniques such as reinforcement learning to fine-tune models for improving the veracity of claims and supporting evidence <cite class="ltx_cite ltx_citemacro_citet">Zhang and Gao (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib123" title="">2024</a>); Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib38" title="">2024</a>)</cite>. A document-level and question-level retrieval policy is proposed by <cite class="ltx_cite ltx_citemacro_citet">Zhang and Gao (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib123" title="">2024</a>)</cite>, where the top-k and top-1 documents for the document and question-level policy respectively are used as input to a scoring function for label prediction during training. This approach outperforms retrieval and prompting approaches. <cite class="ltx_cite ltx_citemacro_citet">Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib20" title="">2024</a>)</cite> fine-tune LLMs for their multi-stage fact verification approach. They fine-tune a model to generate answers based on claim-evidence pairs and a set of questions, whereas another model is fine-tuned to verify the claim based on the claim-evidence and question-answer pairs. Authors in <cite class="ltx_cite ltx_citemacro_citet">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib126" title="">2023</a>)</cite> fine-tune a generation model to generate counterfactuals to train the fact verification model’s performance on out-of-domain claims.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">In-Context Learning.</h4>
<div class="ltx_para" id="Sx4.SSx3.SSSx2.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx2.p1.1">The recent success in the performance of pre-trained LLMs in zero- and few-shot settings is largely attributed to its <span class="ltx_text ltx_font_italic" id="Sx4.SSx3.SSSx2.p1.1.1">in-context learning</span> (ICL) abilities <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib48" title="">2022</a>; Brown et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib8" title="">2020</a>)</cite>. In the scope of claim verification, popular ICL techniques with strong zero- and few-shot performance include chain-of-thought (CoT) reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib106" title="">2022</a>)</cite>. The work in <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib124" title="">2024</a>)</cite> develops a multi-stage verification pipeline for claim verification based on claim decomposition and self-reflection. An LLM-based verifier module is created using instruction prompting to generate a reasoning analysis among all sub-claims created by the decomposer module. The results suggest that using zero-shot prompting techniques provides better performance in multi-hop claim verification domains such as HOVER and FEVEROUS compared to few-shot prompting and fine-tuning methods. Authors in <cite class="ltx_cite ltx_citemacro_citet">Kanaani (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib43" title="">2024</a>)</cite> enable LLMs to generate reasons over retrieved evidence in claim verification using few-shot ICL and the STaR CoT technique inspired by the work of <cite class="ltx_cite ltx_citemacro_citet">Zelikman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib116" title="">2022</a>)</cite>. Similar work has leveraged CoT techniques for the purposes of effectively verifying complex claims using reasoning steps <cite class="ltx_cite ltx_citemacro_citep">(Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib112" title="">2023</a>; Ni et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib66" title="">2024</a>)</cite>. On the other hand, HiSS <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Gao <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib122" title="">2023</a>)</cite> demonstrates that using prompting techniques for few-shot learning and claim decomposition can substantially improve the performance of CoT models for complex news claim verification. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib52" title="">2023b</a>)</cite> leverage the ICL capability of LLMs to perform multiple tasks simultaneously. Their approach outperforms or achieves comparable task performance in a zero-shot setting on claim verification datasets.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx4.SSx4">
<h3 class="ltx_title ltx_title_subsection">LLM Generation Strategies</h3>
<section class="ltx_subsubsection" id="Sx4.SSx4.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Label and Evidence Generation.</h4>
<div class="ltx_para" id="Sx4.SSx4.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx4.SSSx1.p1.1">While the majority of claim verification systems predict veracity labels based on the concatenation of the input claim and the set of evidence sentences <cite class="ltx_cite ltx_citemacro_citep">(Pradeep et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib76" title="">2021</a>)</cite>, recent work has proposed alternate strategies for determining veracity labels as well as selecting/generating evidence pieces. The work in <cite class="ltx_cite ltx_citemacro_citet">Cao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib9" title="">2024</a>)</cite> develops, SERIf, a claim verification pipeline that features an inference module that predicts the veracity label of scientific news articles based on a two-step summarization (i.e., ‘Extractive - Abstractive‘) and evidence retrieval technique. Each summary-evidence pair is fed into the LLM and it produces a binary label, indicating whether the news article is reliable (supported) or unreliable (refuted). Authors in <cite class="ltx_cite ltx_citemacro_citet">Wadden et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib99" title="">2022b</a>)</cite> leverage the Longformer transformer model <cite class="ltx_cite ltx_citemacro_citep">(Beltagy, Peters, and Cohan <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib7" title="">2020</a>)</cite> that uses a shared encoding over the claim and document abstracts for rationale identification and claim label prediction.</p>
</div>
<div class="ltx_para" id="Sx4.SSx4.SSSx1.p2">
<p class="ltx_p" id="Sx4.SSx4.SSSx1.p2.4">The work in <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib54" title="">2024a</a>)</cite> develops an algorithm that selects the minimal evidence group (MEG) within a set of retrieved candidate documents. The algorithm aims to minimize the redundancy while also selecting the most relevant piece of evidence to prompt the language model. Authors in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib16" title="">2022b</a>)</cite> create an LLM claim verification framework in which the authors use the BART model to encode all candidate sentences from the most relevant retrieved documents. The BART decoder serves as an evidence decoder to predict the <math alttext="g" class="ltx_Math" display="inline" id="Sx4.SSx4.SSSx1.p2.1.m1.1"><semantics id="Sx4.SSx4.SSSx1.p2.1.m1.1a"><mi id="Sx4.SSx4.SSSx1.p2.1.m1.1.1" xref="Sx4.SSx4.SSSx1.p2.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx4.SSSx1.p2.1.m1.1b"><ci id="Sx4.SSx4.SSSx1.p2.1.m1.1.1.cmml" xref="Sx4.SSx4.SSSx1.p2.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx4.SSSx1.p2.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx4.SSSx1.p2.1.m1.1d">italic_g</annotation></semantics></math>-th evidence sentence via generation conditioned on the top <math alttext="k" class="ltx_Math" display="inline" id="Sx4.SSx4.SSSx1.p2.2.m2.1"><semantics id="Sx4.SSx4.SSSx1.p2.2.m2.1a"><mi id="Sx4.SSx4.SSSx1.p2.2.m2.1.1" xref="Sx4.SSx4.SSSx1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx4.SSSx1.p2.2.m2.1b"><ci id="Sx4.SSx4.SSSx1.p2.2.m2.1.1.cmml" xref="Sx4.SSx4.SSSx1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx4.SSSx1.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx4.SSSx1.p2.2.m2.1d">italic_k</annotation></semantics></math> retrieved documents and the input claim. <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>)</cite> developed a variant of nucleus sampling called, <span class="ltx_text ltx_font_italic" id="Sx4.SSx4.SSSx1.p2.4.1">factual-nucleus sampling</span>, in which the top-<math alttext="p" class="ltx_Math" display="inline" id="Sx4.SSx4.SSSx1.p2.3.m3.1"><semantics id="Sx4.SSx4.SSSx1.p2.3.m3.1a"><mi id="Sx4.SSx4.SSSx1.p2.3.m3.1.1" xref="Sx4.SSx4.SSSx1.p2.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx4.SSSx1.p2.3.m3.1b"><ci id="Sx4.SSx4.SSSx1.p2.3.m3.1.1.cmml" xref="Sx4.SSx4.SSSx1.p2.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx4.SSSx1.p2.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx4.SSSx1.p2.3.m3.1d">italic_p</annotation></semantics></math> sampling pool is selected as a set of sub-words whose cumulative probability exceeds <math alttext="p" class="ltx_Math" display="inline" id="Sx4.SSx4.SSSx1.p2.4.m4.1"><semantics id="Sx4.SSx4.SSSx1.p2.4.m4.1a"><mi id="Sx4.SSx4.SSSx1.p2.4.m4.1.1" xref="Sx4.SSx4.SSSx1.p2.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx4.SSSx1.p2.4.m4.1b"><ci id="Sx4.SSx4.SSSx1.p2.4.m4.1.1.cmml" xref="Sx4.SSx4.SSSx1.p2.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx4.SSSx1.p2.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx4.SSSx1.p2.4.m4.1d">italic_p</annotation></semantics></math>. The authors show that factual-nucleus sampling can improve evidence and label generation without claim verification datasets such as FEVER. <cite class="ltx_cite ltx_citemacro_citet">Kao and Yen (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib44" title="">2024</a>)</cite> propose a multi-stage approach, where the evidence sentences are retrieved from articles related to a claim, and arguments are generated by aggregating and reconstructing the evidence. The arguments are refined and passed to a LLM to generate a verification label. Other approaches leverage LLMs reasoning capabilities to generate veracity labels and factual evidence <cite class="ltx_cite ltx_citemacro_citep">(Cheng, Tan, and Lu <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib18" title="">2024</a>; Jafari and Allan <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib40" title="">2024</a>; Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib55" title="">2024b</a>; Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib27" title="">2024</a>; Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib68" title="">2023a</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="Sx4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T1.1" style="width:505.9pt;height:395.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.8pt,90.4pt) scale(0.685977208611721,0.685977208611721) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx4.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T1.1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T1.1.1.1.1.2.1"># Instances</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T1.1.1.1.1.3.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T1.1.1.1.1.4.1"># Labels</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx4.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T1.1.1.1.1.5.1">Reference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.1.2.1.1">AVeriTeC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T1.1.1.2.1.2">4,568</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.1.2.1.3">Google FactCheck Claim Search API</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T1.1.1.2.1.4">4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T1.1.1.2.1.5"><cite class="ltx_cite ltx_citemacro_citet">Schlichtkrull, Guo, and Vlachos (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib82" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.3.2">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.3.2.1">BINGCHECK</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.3.2.2">3,840</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.3.2.3">Microsoft Copilot</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.3.2.4">4</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.3.2.5"><cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib53" title="">2023c</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.4.3">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.4.3.1">Check-COVID</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.4.3.2">1,504</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.4.3.3">News websites, CORD-19</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.4.3.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.4.3.5"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib100" title="">2023a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.5.4">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.5.4.1">CLAIMDECOMP</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.5.4.2">1,000</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.5.4.3">PolitiFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.5.4.4">6</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.5.4.5"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib14" title="">2022a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.6.5">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.6.5.1">CLIMATE-FEVER</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.6.5.2">1,535</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.6.5.3">Google Search, Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.6.5.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.6.5.5"><cite class="ltx_cite ltx_citemacro_citet">Diggelmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib23" title="">2020</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.7.6">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.7.6.1">CoVERT</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.7.6.2">212</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.7.6.3">Manually created</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.7.6.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.7.6.5"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib56" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.8.7">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.8.7.1">COVID-Fact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.8.7.2">4,086</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.8.7.3">Reddit</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.8.7.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.8.7.5"><cite class="ltx_cite ltx_citemacro_citet">Saakyan, Chakrabarty, and Muresan (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib80" title="">2021</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.9.8">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.9.8.1">EX-FEVER</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.9.8.2">60,000</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.9.8.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.9.8.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.9.8.5"><cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib59" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.10.9">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.10.9.1">ExpertQA</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.10.9.2">2177</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.10.9.3">LLM, Google Search</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.10.9.4">5</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.10.9.5"><cite class="ltx_cite ltx_citemacro_citet">Malaviya et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib60" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.11.10">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.11.10.1">FACTIFY-5WQA</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.11.10.2">391,041</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.11.10.3">Public Fact-Verification Datasets</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.11.10.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.11.10.5"><cite class="ltx_cite ltx_citemacro_citet">Rani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib79" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.12.11">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.12.11.1">FactCheck-Bench</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.12.11.2">678</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.12.11.3">LLM</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.12.11.4">4</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.12.11.5"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib104" title="">2023b</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.13.12">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.13.12.1">FactCheckQA</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.13.12.2">20,871</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.13.12.3">PolitiFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.13.12.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.13.12.5"><cite class="ltx_cite ltx_citemacro_citet">Bashlovkina et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib5" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.14.13">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.14.13.1">FACTUALITYPROMPTS</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.14.13.2">16,000</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.14.13.3">Wikipedia, LLM</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.14.13.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.14.13.5"><cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.15.14">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.15.14.1">FAVIQ</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.15.14.2">188,000</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.15.14.3">Wikipedia, Existing Datasets</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.15.14.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.15.14.5"><cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib72" title="">2022</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.16.15">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.16.15.1">FEVER</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.16.15.2">185,445</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.16.15.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.16.15.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.16.15.5"><cite class="ltx_cite ltx_citemacro_citet">Thorne et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib91" title="">2018a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.17.16">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.17.16.1">FEVEROUS</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.17.16.2">87,026</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.17.16.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.17.16.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.17.16.5"><cite class="ltx_cite ltx_citemacro_citet">Aly et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib2" title="">2021</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.18.17">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.18.17.1">FoolMeTwice</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.18.17.2">12,968</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.18.17.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.18.17.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.18.17.5"><cite class="ltx_cite ltx_citemacro_citet">Eisenschlos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib26" title="">2021</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.19.18">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.19.18.1">HealthVer</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.19.18.2">14,330</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.19.18.3">Bing Web Search API</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.19.18.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.19.18.5"><cite class="ltx_cite ltx_citemacro_citet">Sarrouti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib81" title="">2021</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.20.19">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.20.19.1">HOVER</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.20.19.2">26,171</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.20.19.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.20.19.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.20.19.5"><cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib41" title="">2020</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.21.20">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.21.20.1">LIAR</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.21.20.2">12,500</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.21.20.3">PolitiFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.21.20.4">6</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.21.20.5"><cite class="ltx_cite ltx_citemacro_citet">Wang (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib103" title="">2017</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.22.21">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.22.21.1">MultiFC</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.22.21.2">34,918</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.22.21.3">Fact-checking Websites</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.22.21.4">2-40</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.22.21.5"><cite class="ltx_cite ltx_citemacro_citet">Augenstein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib3" title="">2019</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.23.22">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.23.22.1">PolitiFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.23.22.2">21,152</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.23.22.3">PolitiFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.23.22.4">6</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.23.22.5"><cite class="ltx_cite ltx_citemacro_citet">Misra (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib63" title="">2022</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.24.23">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.24.23.1">QuanTemp</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.24.23.2">15,514</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.24.23.3">Google Fact Check
Tool API</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.24.23.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.24.23.5"><cite class="ltx_cite ltx_citemacro_citet">Venktesh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib94" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.25.24">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.25.24.1">RAWFC</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.25.24.2">2,012</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.25.24.3">Snopes</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.25.24.4">6</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.25.24.5"><cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib111" title="">2022</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.26.25">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.26.25.1">SciFact</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.26.25.2">1,400</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.26.25.3">S2ORC</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.26.25.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.26.25.5"><cite class="ltx_cite ltx_citemacro_citet">Wadden et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib96" title="">2020</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.27.26">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.27.26.1">SciFact-Open</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.27.26.2">279</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.27.26.3">S2ORC</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.27.26.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.27.26.5"><cite class="ltx_cite ltx_citemacro_citet">Wadden et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib98" title="">2022a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.28.27">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.28.27.1">SciNews</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.28.27.2">2,400</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.28.27.3">Wiki Fake News, LLM, Existing Datasets</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.28.27.4">2</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.28.27.5"><cite class="ltx_cite ltx_citemacro_citet">Cao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib9" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.29.28">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.29.28.1">SCITAB</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.29.28.2">1,225</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.29.28.3">SciGen Dataset (arXiv)</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.29.28.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.29.28.5"><cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib58" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.30.29">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.30.29.1">VitaminC</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.30.29.2">400,000</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.30.29.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.30.29.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.30.29.5"><cite class="ltx_cite ltx_citemacro_citet">Schuster, Fisch, and Barzilay (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib83" title="">2021</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.31.30">
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.31.30.1">WiCE</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.31.30.2">1,967</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.31.30.3">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="Sx4.T1.1.1.31.30.4">3</td>
<td class="ltx_td ltx_align_left" id="Sx4.T1.1.1.31.30.5"><cite class="ltx_cite ltx_citemacro_citet">Kamoi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib42" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.1.1.32.31">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx4.T1.1.1.32.31.1">XClaimCheck</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T1.1.1.32.31.2">16,177</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx4.T1.1.1.32.31.3">PolitiFact</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T1.1.1.32.31.4">5</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx4.T1.1.1.32.31.5"><cite class="ltx_cite ltx_citemacro_citet">Kao and Yen (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib44" title="">2024</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Available datasets along with the number of instances, data sources, and references.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx4.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Explainable Generation.</h4>
<div class="ltx_para" id="Sx4.SSx4.SSSx2.p1">
<p class="ltx_p" id="Sx4.SSx4.SSSx2.p1.1">Recent studies investigate explainable approaches to improve LLM-based claim verification systems <cite class="ltx_cite ltx_citemacro_citep">(Wang and Shu <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib101" title="">2023</a>; Dammu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib22" title="">2024</a>; Si et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib87" title="">2023</a>)</cite>. Authors in <cite class="ltx_cite ltx_citemacro_citet">Wang and Shu (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib101" title="">2023</a>)</cite> present an explainable claim verification framework named FOLK, that leverages the explanation capabilities of LLMs when verifying a claim and justifies the prediction through a summary of its decision process. The work in <cite class="ltx_cite ltx_citemacro_citet">Dammu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib22" title="">2024</a>)</cite> proposes a knowledge graph (KG)-based approach for text verification and evidence attribution. An objective function is used to fine-tune LLMs on evidence attribution based on the input text and retrieved triplets from KG, inducing explanations on claim predictions. While explainable techniques can aid humans in fact-checking, LLMs are prone to incorrect explanations due to hallucinations, causing them to be unreliable in certain claim verification scenarios <cite class="ltx_cite ltx_citemacro_citep">(Si et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib87" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib59" title="">2024</a>)</cite> prompt an LLM in a few-shot setting to generate a concise summary of the gathered evidence documents and the input claim. This summary serves as an explanation for the verified claim. Other works in claim verification leverage reasoning techniques such as chain-of-thought (CoT) for enabling the LLM to be interpretable in its decision-making process when verifying claims <cite class="ltx_cite ltx_citemacro_citep">(Yao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib112" title="">2023</a>; Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib68" title="">2023a</a>; Zhao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib124" title="">2024</a>; Kanaani <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib43" title="">2024</a>; Ni et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib66" title="">2024</a>; Quelle and Bovet <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib77" title="">2024</a>; Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib27" title="">2024</a>)</cite>. The authors in <cite class="ltx_cite ltx_citemacro_citet">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib68" title="">2023a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib27" title="">2024</a>)</cite> leverage the reasoning ability of the LLMs to generate explanations by using question-guided reasoning and minimizing the inherent model biases respectively.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Evaluation and Benchmarking</h2>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Metrics</h3>
<div class="ltx_para" id="Sx5.SSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.p1.1">The F1 score is the most commonly used metric to measure the performance of automatic claim verification systems. Other metrics like Precision, Recall, and Accuracy are also used to evaluate the system performance. <cite class="ltx_cite ltx_citemacro_citet">Katranidis and Barany (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib45" title="">2024</a>)</cite> used the error rate between the human and automated fact-verification system to measure the verification accuracy. However, these metrics consider a single pipeline component to evaluate the system’s overall performance. Hence, <cite class="ltx_cite ltx_citemacro_citet">Thorne et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib91" title="">2018a</a>)</cite> introduced the FEVER score, a metric that uses both the verification accuracy as well as the evidence retrieval accuracy to compute overall system performance. While these metrics are valuable for assessing the performance of the classification tasks, they are inadequate for evaluating the performance of the non-classification components of the pipeline. Hence, metrics like Recall@k are used to measure the performance of the retrieval task <cite class="ltx_cite ltx_citemacro_citep">(Pan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib69" title="">2023b</a>; Pradeep et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib76" title="">2021</a>)</cite>, while BLEU and METEOR are used to evaluate the quality of explanations or generated questions and answers. <cite class="ltx_cite ltx_citemacro_citet">Schlichtkrull, Guo, and Vlachos (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib82" title="">2024</a>)</cite> propose a new evaluation metric AVeriTeC score that uses METEOR and accuracy metrics, for question-answer-based veracity label prediction systems. Other metrics like Mean Absolute Error (MAE), Expected Calibration Error (ECE), Area Under ROC Curve (AUC-ROC), and Pearson’s Correlation are also used. Most of these metrics are inadequate to evaluate the factual accuracy of the LLM-generated text. Several metrics like FactScore <cite class="ltx_cite ltx_citemacro_citep">(Min et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib62" title="">2023</a>)</cite>, SAFE <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib107" title="">2024</a>)</cite>, and VERISCORE <cite class="ltx_cite ltx_citemacro_citep">(Song, Kim, and Iyyer <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib89" title="">2024</a>)</cite> are introduced to evaluate the factual accuracy of the LLM-generated text. While these metrics evaluate the accuracy, other metrics and frameworks to evaluate the factual errors in generated text <cite class="ltx_cite ltx_citemacro_citep">(Lee et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>; Chern et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib19" title="">2023</a>)</cite> as well as their alignment <cite class="ltx_cite ltx_citemacro_citep">(Zha et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib120" title="">2023</a>)</cite> and entailment <cite class="ltx_cite ltx_citemacro_citep">(Lee et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>)</cite> considering the factuality have been proposed.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Datasets</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1">The fundamental resource for training and evaluating claim verification systems is datasets containing annotated texts. As most research in this area deals with English data, we collect information about all publicly available English datasets used in the papers discussed in this survey and present them in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#Sx4.T1" title="Table 1 ‣ Label and Evidence Generation. ‣ LLM Generation Strategies ‣ LLM Approaches ‣ Claim Verification in the Age of Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.p2">
<p class="ltx_p" id="Sx5.SSx2.p2.1">Several general-domain datasets have been released over the years. Different online data sources and websites are used to extract facts and claims including Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Thorne et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib91" title="">2018a</a>; Jiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib41" title="">2020</a>; Diggelmann et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib23" title="">2020</a>; Eisenschlos et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib26" title="">2021</a>; Schuster, Fisch, and Barzilay <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib83" title="">2021</a>; Kamoi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib42" title="">2023</a>)</cite>, due to the extensive amount of information available spanning various topics and domains, and online fact-checking websites like PolitiFact <cite class="ltx_cite ltx_citemacro_citep">(Wang <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib103" title="">2017</a>; Augenstein et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib3" title="">2019</a>; Kao and Yen <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib44" title="">2024</a>)</cite>. Factually incorrect claims are shared through social media channels like X, Reddit, etc. <cite class="ltx_cite ltx_citemacro_citet">Saakyan, Chakrabarty, and Muresan (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib80" title="">2021</a>)</cite> introduced the COVID-Fact dataset consisting of claims extracted from Reddit posts. Several datasets for scientific fact verification have also been introduced over the years <cite class="ltx_cite ltx_citemacro_citep">(Wadden et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib96" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib98" title="">2022a</a>; Lu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib58" title="">2023</a>)</cite>. Most fact-verification datasets rely on unstructured textual evidence. Hence, a few datasets with structured data sources as evidence have been introduced <cite class="ltx_cite ltx_citemacro_citep">(Aly et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib2" title="">2021</a>; Lu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib58" title="">2023</a>)</cite>. While a large number of datasets are focused on only veracity labels, some datasets were deloped to address explainability as well <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib14" title="">2022a</a>; Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib111" title="">2022</a>; Ma et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib59" title="">2024</a>; Rani et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib79" title="">2023</a>; Schlichtkrull, Guo, and Vlachos <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib82" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.p3">
<p class="ltx_p" id="Sx5.SSx2.p3.1">Most of the claim-verification datasets include claims extracted from available information sources. LLMs are widely used to generate content, given their superior text-generation capabilities. These models often generate factually incorrect information. Efforts to identify such non-factual text have been undertaken. However, existing claim-verification datasets can be inadequate for this task due to the linguistic variations between the human- and LLM-generated text. To overcome this issue, LLM-generated claim verification datasets have been introduced <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib53" title="">2023c</a>; Cao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib9" title="">2024</a>)</cite>. While these datasets are used to evaluate the automatic claim verification systems, there is a need to evaluate the factual accuracy of the LLM-generated text. Hence, a few datasets to evaluate the LLM generation’s factual accuracy have been introduced <cite class="ltx_cite ltx_citemacro_citep">(Lee et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib49" title="">2022</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib104" title="">2023b</a>; Malaviya et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib60" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx5.SSx3">
<h3 class="ltx_title ltx_title_subsection">Shared Tasks</h3>
<div class="ltx_para" id="Sx5.SSx3.p1">
<p class="ltx_p" id="Sx5.SSx3.p1.1">Shared tasks are competitions where participating teams develop systems to solve a task using a common benchmark dataset. There have been multiple shared tasks on the claim and fact-checking organized over the years like the Fact Extraction and Verification (FEVER) <cite class="ltx_cite ltx_citemacro_citep">(Thorne et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib92" title="">2018b</a>)</cite>, TabFact<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://competitions.codalab.org/competitions/21611</span></span></span>, CLEF 2020 CheckThat! <cite class="ltx_cite ltx_citemacro_citep">(Barrón-Cedeno et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib4" title="">2020</a>)</cite>, SCIVER <cite class="ltx_cite ltx_citemacro_citep">(Wadden and Lo <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib97" title="">2021</a>)</cite>, SEM-TAB-FACT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib102" title="">2021</a>)</cite>, FACTIFY-5WQA<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://defactify.com/factify3.html</span></span></span>, and more recently AVeriTeC<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://fever.ai/task.html</span></span></span>. While there have been various techniques like question-answer generation as a precursor task to label prediction as in the AVeriTeC shared task, all these shared tasks are centered on predicting a veracity label for a claim. Given the LLM’s tendency to hallucinate generating plausible yet factually inaccurate text, there is a need to organize shared tasks to evaluate the factual accuracy of LLM-generated content.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Open Challenges</h2>
<section class="ltx_paragraph" id="Sx6.SSx3.SSSx2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Handling Irrelevant Context</h5>
<div class="ltx_para" id="Sx6.SSx3.SSSx2.Px1.p1">
<p class="ltx_p" id="Sx6.SSx3.SSSx2.Px1.p1.1">Retrieved evidence may be irrelevant, which is a challenge for LLMs, as they may not be trained to ignore such evidence. The lack of robustness to the noise can cause the LLM to produce misinformation and incorrect verification. Recent research on open-domain question answering shows that external knowledge relevant to the task can aid the model performance, however, irrelevant context can also lead the model to make inaccurate predictions <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib74" title="">2020</a>; Shi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib86" title="">2023</a>; Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib51" title="">2023a</a>; Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib114" title="">2023</a>)</cite>. For the fact-verification task, recent works have proposed techniques to identify the most relevant context while aiding the veracity label prediction and thus improving the overall system performance <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib105" title="">2023c</a>; Yoran et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib113" title="">2024</a>; Xia et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib108" title="">2024</a>)</cite>. However, more work on the topic is required to verify the effectiveness of this design approach across multiple fact verification domains.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx6.SSx3.SSSx2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Handling Knowledge Conflicts</h5>
<div class="ltx_para" id="Sx6.SSx3.SSSx2.Px2.p1">
<p class="ltx_p" id="Sx6.SSx3.SSSx2.Px2.p1.1">Fact-verification approaches reliance on retrieved evidence can cause knowledge conflicts in LLM-based approaches where retrieved evidence as the internal parameters of the pre-trained LLM may conflict with the external knowledge. This causes the LLM to ignore the retrieved evidence and produces hallucinations. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib109" title="">2024a</a>)</cite> provides an in-depth analysis of this scenario. Works by <cite class="ltx_cite ltx_citemacro_citep">(Li et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib51" title="">2023a</a>; Neeman et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib65" title="">2023</a>; Mallen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib61" title="">2023</a>; Longpre et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib57" title="">2021</a>; Chen, Zhang, and Choi <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib12" title="">2022</a>)</cite> introduce approaches to avoid knowledge conflicts and mitigate hallucinations for question-answering. Expanding this work to fact-verification, especially to the approaches that use LLMs, is vital for an effective verification process.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx6.SSx3.SSSx2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Multilinguality</h5>
<div class="ltx_para" id="Sx6.SSx3.SSSx2.Px3.p1">
<p class="ltx_p" id="Sx6.SSx3.SSSx2.Px3.p1.1">Most automated claim verification approaches rely on English datasets. Furthermore, there are limited multilingual fact-verification datasets <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Srikumar <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib32" title="">2021</a>; Kazemi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib46" title="">2022</a>; Pikuliak et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib75" title="">2023</a>; Singh et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib88" title="">2023</a>)</cite>. This hinders the development of approaches for multilingual fact-verification, which achieve the best performance when trained on language-specific datasets <cite class="ltx_cite ltx_citemacro_citep">(Panchendrarajan and Zubiaga <a class="ltx_ref" href="https://arxiv.org/html/2408.14317v1#bib.bib71" title="">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx7">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx7.p1">
<p class="ltx_p" id="Sx7.p1.1">We presented a survey on LLM approaches to claim verification. To the best of our knowledge, this is the first claim verification survey to focus exclusively on LLM approaches thus filling an important gap in the literature. We have described each of the sub-tasks of the typical claim verification pipeline and discussed various LLM-based approaches used in this task. Finally, we have also described publicly available English datasets providing important information to new and seasoned researchers on this topic.</p>
</div>
<div class="ltx_para" id="Sx7.p2">
<p class="ltx_p" id="Sx7.p2.1">Advances in LLM development will likely continue improving the quality of claim verification systems. We hope this survey motivates future research on this topic taking advantage of recently-proposed LLMs, RAG methods, etc. Claim verification is a vibrant research topic and we see multiple open research directions as described in the next sub-section.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adair et al. (2017)</span>
<span class="ltx_bibblock">
Adair, B.; Li, C.; Yang, J.; and Yu, C. 2017.

</span>
<span class="ltx_bibblock">Progress toward “the holy grail”: The continued quest to automate fact-checking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Computation+ Journalism Symposium,(September)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aly et al. (2021)</span>
<span class="ltx_bibblock">
Aly, R.; Guo, Z.; Schlichtkrull, M. S.; Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; Cocarascu, O.; and Mittal, A. 2021.

</span>
<span class="ltx_bibblock">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Augenstein et al. (2019)</span>
<span class="ltx_bibblock">
Augenstein, I.; Lioma, C.; Wang, D.; Lima, L. C.; Hansen, C.; Hansen, C.; and Simonsen, J. G. 2019.

</span>
<span class="ltx_bibblock">MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 4685–4697.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrón-Cedeno et al. (2020)</span>
<span class="ltx_bibblock">
Barrón-Cedeno, A.; Elsayed, T.; Nakov, P.; Da San Martino, G.; Hasanain, M.; Suwaileh, R.; Haouari, F.; Babulkov, N.; Hamdan, B.; Nikolov, A.; et al. 2020.

</span>
<span class="ltx_bibblock">Overview of CheckThat! 2020: Automatic identification and verification of claims in social media.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 22–25, 2020, Proceedings 11</em>. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bashlovkina et al. (2023)</span>
<span class="ltx_bibblock">
Bashlovkina, V.; Kuang, Z.; Matthews, R.; Clifford, E.; Jun, Y.; Cohen, W. W.; and Baumgartner, S. 2023.

</span>
<span class="ltx_bibblock">Trusted source alignment in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2311.06697</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bekoulis, Papagiannopoulou, and Deligiannis (2021)</span>
<span class="ltx_bibblock">
Bekoulis, G.; Papagiannopoulou, C.; and Deligiannis, N. 2021.

</span>
<span class="ltx_bibblock">A review on fact extraction and verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Computing Surveys (CSUR)</em>, 55(1): 1–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy, Peters, and Cohan (2020)</span>
<span class="ltx_bibblock">
Beltagy, I.; Peters, M. E.; and Cohan, A. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2004.05150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in neural information processing systems</em>, 33: 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2024)</span>
<span class="ltx_bibblock">
Cao, Y.; Nair, A. M.; Eyimife, E.; Soofi, N. J.; Subbalakshmi, K.; Wullert II, J. R.; Basu, C.; and Shallcross, D. 2024.

</span>
<span class="ltx_bibblock">Can Large Language Models Detect Misinformation in Scientific News Reporting?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2402.14268</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Shu (2023)</span>
<span class="ltx_bibblock">
Chen, C.; and Shu, K. 2023.

</span>
<span class="ltx_bibblock">Combating misinformation in the age of llms: Opportunities and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2311.05656</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Chen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017.

</span>
<span class="ltx_bibblock">Reading Wikipedia to Answer Open-Domain Questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen, Zhang, and Choi (2022)</span>
<span class="ltx_bibblock">
Chen, H.-T.; Zhang, M.; and Choi, E. 2022.

</span>
<span class="ltx_bibblock">Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Chen, J.; Kim, G.; Sriram, A.; Durrett, G.; and Choi, E. 2023a.

</span>
<span class="ltx_bibblock">Complex claim verification with evidence retrieved in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2305.11859</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022a)</span>
<span class="ltx_bibblock">
Chen, J.; Sriram, A.; Choi, E.; and Durrett, G. 2022a.

</span>
<span class="ltx_bibblock">Generating Literal and Implied Subquestions to Fact-check Complex Claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Chen, J.; Zhang, R.; Guo, J.; de Rijke, M.; Liu, Y.; Fan, Y.; and Cheng, X. 2023b.

</span>
<span class="ltx_bibblock">A unified generative retriever for knowledge-intensive language tasks via prompt learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022b)</span>
<span class="ltx_bibblock">
Chen, J.; Zhang, R.; Guo, J.; Fan, Y.; and Cheng, X. 2022b.

</span>
<span class="ltx_bibblock">GERE: Generative evidence retrieval for fact verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022c)</span>
<span class="ltx_bibblock">
Chen, J.; Zhang, R.; Guo, J.; Liu, Y.; Fan, Y.; and Cheng, X. 2022c.

</span>
<span class="ltx_bibblock">Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng, Tan, and Lu (2024)</span>
<span class="ltx_bibblock">
Cheng, X.; Tan, Z.; and Lu, W. 2024.

</span>
<span class="ltx_bibblock">Information Re-Organization Improves Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2404.13985</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et al. (2023)</span>
<span class="ltx_bibblock">
Chern, I.; Chern, S.; Chen, S.; Yuan, W.; Feng, K.; Zhou, C.; He, J.; Neubig, G.; Liu, P.; et al. 2023.

</span>
<span class="ltx_bibblock">FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2307.13528</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2024)</span>
<span class="ltx_bibblock">
Chiang, S.-H.; Lo, M.-C.; Chao, L.-W.; and Peng, W.-C. 2024.

</span>
<span class="ltx_bibblock">Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2403.10281</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi and Ferrara (2024)</span>
<span class="ltx_bibblock">
Choi, E. C.; and Ferrara, E. 2024.

</span>
<span class="ltx_bibblock">Automated claim matching with large language models: empowering fact-checkers in the fight against misinformation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Companion Proceedings of the ACM on Web Conference 2024</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dammu et al. (2024)</span>
<span class="ltx_bibblock">
Dammu, P. P. S.; Naidu, H.; Dewan, M.; Kim, Y.; Roosta, T.; Chadha, A.; and Shah, C. 2024.

</span>
<span class="ltx_bibblock">ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2403.09724</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diggelmann et al. (2020)</span>
<span class="ltx_bibblock">
Diggelmann, T.; Boyd-Graber, J.; Bulian, J.; Ciaramita, M.; and Leippold, M. 2020.

</span>
<span class="ltx_bibblock">CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Tackling Climate Change with Machine Learning workshop at NeurIPS 2020</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dmonte et al. (2024)</span>
<span class="ltx_bibblock">
Dmonte, A.; Zampieri, M.; Lybarger, K.; and Albanese, M. 2024.

</span>
<span class="ltx_bibblock">Classifying Human-Generated and AI-Generated Election Claims in Social Media.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2404.16116</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dougrez-Lewis et al. (2024)</span>
<span class="ltx_bibblock">
Dougrez-Lewis, J.; Akhter, M. E.; He, Y.; and Liakata, M. 2024.

</span>
<span class="ltx_bibblock">Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2402.10735</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eisenschlos et al. (2021)</span>
<span class="ltx_bibblock">
Eisenschlos, J.; Dhingra, B.; Bulian, J.; Börschinger, B.; and Boyd-Graber, J. 2021.

</span>
<span class="ltx_bibblock">Fool Me Twice: Entailment from Wikipedia Gamification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2024)</span>
<span class="ltx_bibblock">
Fang, Y.; Li, M.; Wang, W.; Lin, H.; and Feng, F. 2024.

</span>
<span class="ltx_bibblock">Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2406.11514</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; and Wang, H. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et al. (2022)</span>
<span class="ltx_bibblock">
Glass, M.; Rossiello, G.; Chowdhury, M. F. M.; Naik, A.; Cai, P.; and Gliozzo, A. 2022.

</span>
<span class="ltx_bibblock">Re2G: Retrieve, Rerank, Generate.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan et al. (2023)</span>
<span class="ltx_bibblock">
Guan, J.; Dodge, J.; Wadden, D.; Huang, M.; and Peng, H. 2023.

</span>
<span class="ltx_bibblock">Language Models Hallucinate, but May Excel at Fact Verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2310.14564</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo, Schlichtkrull, and Vlachos (2022)</span>
<span class="ltx_bibblock">
Guo, Z.; Schlichtkrull, M.; and Vlachos, A. 2022.

</span>
<span class="ltx_bibblock">A survey on automated fact-checking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Transactions of the Association for Computational Linguistics</em>, 10: 178–206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Srikumar (2021)</span>
<span class="ltx_bibblock">
Gupta, A.; and Srikumar, V. 2021.

</span>
<span class="ltx_bibblock">X-Fact: A New Benchmark Dataset for Multilingual Fact Checking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hang, Yu, and Tan (2024)</span>
<span class="ltx_bibblock">
Hang, C. N.; Yu, P.-D.; and Tan, C. W. 2024.

</span>
<span class="ltx_bibblock">TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2024 58th Annual Conference on Information Sciences and Systems (CISS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanselowski (2020)</span>
<span class="ltx_bibblock">
Hanselowski, A. 2020.

</span>
<span class="ltx_bibblock">A machine-learning-based pipeline approach to automated fact-checking.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan, Li, and Tremayne (2015)</span>
<span class="ltx_bibblock">
Hassan, N.; Li, C.; and Tremayne, M. 2015.

</span>
<span class="ltx_bibblock">Detecting check-worthy factual claims in presidential debates.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 24th acm international on conference on information and knowledge management</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstätter et al. (2023)</span>
<span class="ltx_bibblock">
Hofstätter, S.; Chen, J.; Raman, K.; and Zamani, H. 2023.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Hu, X.; Chen, J.; Guo, Z.; and Yu, P. S. 2023.

</span>
<span class="ltx_bibblock">Give Me More Details: Improving Fact-Checking with Latent Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2305.16128</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Huang, C.; Wu, Z.; Hu, Y.; and Wang, W. 2024.

</span>
<span class="ltx_bibblock">Training Language Models to Generate Text with Citations via Fine-grained Rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2402.04315</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2023)</span>
<span class="ltx_bibblock">
Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2023.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Journal of Machine Learning Research</em>, 24(251): 1–43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jafari and Allan (2024)</span>
<span class="ltx_bibblock">
Jafari, N.; and Allan, J. 2024.

</span>
<span class="ltx_bibblock">Robust Claim Verification Through Fact Detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2407.18367</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Jiang, Y.; Bordia, S.; Zhong, Z.; Dognin, C.; Singh, M.; and Bansal, M. 2020.

</span>
<span class="ltx_bibblock">HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamoi et al. (2023)</span>
<span class="ltx_bibblock">
Kamoi, R.; Goyal, T.; Rodriguez, J. D.; and Durrett, G. 2023.

</span>
<span class="ltx_bibblock">WiCE: Real-World Entailment for Claims in Wikipedia.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 7561–7583.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanaani (2024)</span>
<span class="ltx_bibblock">
Kanaani, M. 2024.

</span>
<span class="ltx_bibblock">Triple-R: Automatic Reasoning for Fact Verification Using Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kao and Yen (2024)</span>
<span class="ltx_bibblock">
Kao, W.-Y.; and Yen, A.-Z. 2024.

</span>
<span class="ltx_bibblock">MAGIC: Multi-Argument Generation with Self-Refinement for Domain Generalization in Automatic Fact-Checking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katranidis and Barany (2024)</span>
<span class="ltx_bibblock">
Katranidis, V.; and Barany, G. 2024.

</span>
<span class="ltx_bibblock">FaaF: Facts as a Function for the evaluation of RAG systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2403.03888</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemi et al. (2022)</span>
<span class="ltx_bibblock">
Kazemi, A.; Li, Z.; Peréz-Rosas, V.; Hale, S.; and Mihalcea, R. 2022.

</span>
<span class="ltx_bibblock">Matching tweets with applicable fact-checks across languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CEUR Workshop Proceedings</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab, Potts, and Zaharia (2021)</span>
<span class="ltx_bibblock">
Khattab, O.; Potts, C.; and Zaharia, M. 2021.

</span>
<span class="ltx_bibblock">Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval.

</span>
<span class="ltx_bibblock">In Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J. W., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Advances in neural information processing systems</em>, 35: 22199–22213.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Lee, N.; Ping, W.; Xu, P.; Patwary, M.; Fung, P.; Shoeybi, M.; and Catanzaro, B. 2022.

</span>
<span class="ltx_bibblock">Factuality Enhanced Language Models for Open-Ended Text Generation.

</span>
<span class="ltx_bibblock">In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</em>, 33: 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.; Veit, A.; Yu, F.; and Kumar, S. 2023a.

</span>
<span class="ltx_bibblock">Large Language Models with Controllable Working Memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Li, J.; Zhao, R.; Yang, Y.; He, Y.; and Gui, L. 2023b.

</span>
<span class="ltx_bibblock">OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Li, M.; Peng, B.; Galley, M.; Gao, J.; and Zhang, Z. 2023c.

</span>
<span class="ltx_bibblock">Self-checker: Plug-and-play modules for fact-checking with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2305.14623</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Li, X.; Chen, S.; Kapadia, R.; Ouyang, J.; and Zhang, F. 2024a.

</span>
<span class="ltx_bibblock">Minimal Evidence Group Identification for Claim Verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2404.15588</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Li, X.; Zhao, R.; Chia, Y. K.; Ding, B.; Joty, S.; Poria, S.; and Bing, L. 2024b.

</span>
<span class="ltx_bibblock">Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Liu, H.; Soroush, A.; Nestor, J. G.; Park, E.; Idnay, B.; Fang, Y.; Pan, J.; Liao, S.; Bernard, M.; Peng, Y.; et al. 2024.

</span>
<span class="ltx_bibblock">Retrieval augmented scientific claim verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">JAMIA open</em>, 7(1): ooae021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et al. (2021)</span>
<span class="ltx_bibblock">
Longpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois, C.; and Singh, S. 2021.

</span>
<span class="ltx_bibblock">Entity-Based Knowledge Conflicts in Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023)</span>
<span class="ltx_bibblock">
Lu, X.; Pan, L.; Liu, Q.; Nakov, P.; and Kan, M.-Y. 2023.

</span>
<span class="ltx_bibblock">SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024)</span>
<span class="ltx_bibblock">
Ma, H.; Xu, W.; Wei, Y.; Chen, L.; Wang, L.; Liu, Q.; and Wu, S. 2024.

</span>
<span class="ltx_bibblock">EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Findings of the Association for Computational Linguistics: ACL 2024</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malaviya et al. (2024)</span>
<span class="ltx_bibblock">
Malaviya, C.; Lee, S.; Chen, S.; Sieber, E.; Yatskar, M.; and Roth, D. 2024.

</span>
<span class="ltx_bibblock">ExpertQA: Expert-Curated Questions and Attributed Answers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, 3025–3045.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et al. (2023)</span>
<span class="ltx_bibblock">
Mallen, A.; Asai, A.; Zhong, V.; Das, R.; Khashabi, D.; and Hajishirzi, H. 2023.

</span>
<span class="ltx_bibblock">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2023)</span>
<span class="ltx_bibblock">
Min, S.; Krishna, K.; Lyu, X.; Lewis, M.; Yih, W.-t.; Koh, P.; Iyyer, M.; Zettlemoyer, L.; and Hajishirzi, H. 2023.

</span>
<span class="ltx_bibblock">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 12076–12100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra (2022)</span>
<span class="ltx_bibblock">
Misra, R. 2022.

</span>
<span class="ltx_bibblock">Politifact Fact Check Dataset.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakov et al. (2021)</span>
<span class="ltx_bibblock">
Nakov, P.; Da San Martino, G.; Elsayed, T.; Barrón-Cedeño, A.; Míguez, R.; Shaar, S.; Alam, F.; Haouari, F.; Hasanain, M.; Babulkov, N.; et al. 2021.

</span>
<span class="ltx_bibblock">The CLEF-2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">European Conference on Information Retrieval</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neeman et al. (2023)</span>
<span class="ltx_bibblock">
Neeman, E.; Aharoni, R.; Honovich, O.; Choshen, L.; Szpektor, I.; and Abend, O. 2023.

</span>
<span class="ltx_bibblock">DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2024)</span>
<span class="ltx_bibblock">
Ni, J.; Shi, M.; Stammbach, D.; Sachan, M.; Ash, E.; and Leippold, M. 2024.

</span>
<span class="ltx_bibblock">AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2402.11073</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2021)</span>
<span class="ltx_bibblock">
Pan, L.; Chen, W.; Xiong, W.; Kan, M.-Y.; and Wang, W. Y. 2021.

</span>
<span class="ltx_bibblock">Zero-shot Fact Verification by Claim Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of ACL 2021</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023a)</span>
<span class="ltx_bibblock">
Pan, L.; Lu, X.; Kan, M.-Y.; and Nakov, P. 2023a.

</span>
<span class="ltx_bibblock">QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, 264–273.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023b)</span>
<span class="ltx_bibblock">
Pan, L.; Wu, X.; Lu, X.; Luu, A. T.; Wang, W. Y.; Kan, M.-Y.; and Nakov, P. 2023b.

</span>
<span class="ltx_bibblock">Fact-Checking Complex Claims with Program-Guided Reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023c)</span>
<span class="ltx_bibblock">
Pan, Y.; Pan, L.; Chen, W.; Nakov, P.; Kan, M.-Y.; and Wang, W. 2023c.

</span>
<span class="ltx_bibblock">On the Risk of Misinformation Pollution with Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panchendrarajan and Zubiaga (2024)</span>
<span class="ltx_bibblock">
Panchendrarajan, R.; and Zubiaga, A. 2024.

</span>
<span class="ltx_bibblock">Claim detection for automated fact-checking: A survey on monolingual, multilingual and cross-lingual research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Natural Language Processing Journal</em>, 7: 100066.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2022)</span>
<span class="ltx_bibblock">
Park, J.; Min, S.; Kang, J.; Zettlemoyer, L.; and Hajishirzi, H. 2022.

</span>
<span class="ltx_bibblock">FaVIQ: FAct Verification from Information-seeking Questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 5154–5166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parvez (2024)</span>
<span class="ltx_bibblock">
Parvez, M. R. 2024.

</span>
<span class="ltx_bibblock">Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2401.05787</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2020)</span>
<span class="ltx_bibblock">
Petroni, F.; Lewis, P.; Piktus, A.; Rocktäschel, T.; Wu, Y.; Miller, A. H.; and Riedel, S. 2020.

</span>
<span class="ltx_bibblock">How Context Affects Language Models’ Factual Predictions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Automated Knowledge Base Construction</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pikuliak et al. (2023)</span>
<span class="ltx_bibblock">
Pikuliak, M.; Srba, I.; Moro, R.; Hromadka, T.; Smoleň, T.; Melišek, M.; Vykopal, I.; Simko, J.; Podroužek, J.; and Bieliková, M. 2023.

</span>
<span class="ltx_bibblock">Multilingual Previously Fact-Checked Claim Retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep et al. (2021)</span>
<span class="ltx_bibblock">
Pradeep, R.; Ma, X.; Nogueira, R.; and Lin, J. 2021.

</span>
<span class="ltx_bibblock">Scientific Claim Verification with VerT5erini.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quelle and Bovet (2024)</span>
<span class="ltx_bibblock">
Quelle, D.; and Bovet, A. 2024.

</span>
<span class="ltx_bibblock">The perils and promises of fact-checking with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Frontiers in Artificial Intelligence</em>, 7: 1341697.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et al. (2021)</span>
<span class="ltx_bibblock">
Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; et al. 2021.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training gopher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2112.11446</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rani et al. (2023)</span>
<span class="ltx_bibblock">
Rani, A.; Tonmoy, S. T. I.; Dalal, D.; Gautam, S.; Chakraborty, M.; Chadha, A.; Sheth, A.; and Das, A. 2023.

</span>
<span class="ltx_bibblock">FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saakyan, Chakrabarty, and Muresan (2021)</span>
<span class="ltx_bibblock">
Saakyan, A.; Chakrabarty, T.; and Muresan, S. 2021.

</span>
<span class="ltx_bibblock">COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarrouti et al. (2021)</span>
<span class="ltx_bibblock">
Sarrouti, M.; Abacha, A. B.; M’rabet, Y.; and Demner-Fushman, D. 2021.

</span>
<span class="ltx_bibblock">Evidence-based fact-checking of health-related claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlichtkrull, Guo, and Vlachos (2024)</span>
<span class="ltx_bibblock">
Schlichtkrull, M.; Guo, Z.; and Vlachos, A. 2024.

</span>
<span class="ltx_bibblock">Averitec: A dataset for real-world claim verification with evidence from the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster, Fisch, and Barzilay (2021)</span>
<span class="ltx_bibblock">
Schuster, T.; Fisch, A.; and Barzilay, R. 2021.

</span>
<span class="ltx_bibblock">Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 624–643.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaar et al. (2020)</span>
<span class="ltx_bibblock">
Shaar, S.; Babulkov, N.; Da San Martino, G.; and Nakov, P. 2020.

</span>
<span class="ltx_bibblock">That is a Known Lie: Detecting Previously Fact-Checked Claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2023)</span>
<span class="ltx_bibblock">
Shao, Z.; Gong, Y.; Shen, Y.; Huang, M.; Duan, N.; and Chen, W. 2023.

</span>
<span class="ltx_bibblock">Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E. H.; Schärli, N.; and Zhou, D. 2023.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2023)</span>
<span class="ltx_bibblock">
Si, C.; Goyal, N.; Wu, S. T.; Zhao, C.; Feng, S.; Daumé III, H.; and Boyd-Graber, J. 2023.

</span>
<span class="ltx_bibblock">Large Language Models Help Humans Verify Truthfulness–Except When They Are Convincingly Wrong.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2310.12558</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2023)</span>
<span class="ltx_bibblock">
Singh, I.; Scarton, C.; Song, X.; and Bontcheva, K. 2023.

</span>
<span class="ltx_bibblock">Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2308.05680</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song, Kim, and Iyyer (2024)</span>
<span class="ltx_bibblock">
Song, Y.; Kim, Y.; and Iyyer, M. 2024.

</span>
<span class="ltx_bibblock">VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2406.19276</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang, Laban, and Durrett (2024)</span>
<span class="ltx_bibblock">
Tang, L.; Laban, P.; and Durrett, G. 2024.

</span>
<span class="ltx_bibblock">MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2404.10774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et al. (2018a)</span>
<span class="ltx_bibblock">
Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal, A. 2018a.

</span>
<span class="ltx_bibblock">FEVER: a Large-scale Dataset for Fact Extraction and VERification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et al. (2018b)</span>
<span class="ltx_bibblock">
Thorne, J.; Vlachos, A.; Cocarascu, O.; Christodoulopoulos, C.; and Mittal, A. 2018b.

</span>
<span class="ltx_bibblock">The Fact Extraction and VERification (FEVER) Shared Task.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vallayil et al. (2023)</span>
<span class="ltx_bibblock">
Vallayil, M.; Nand, P.; Yan, W. Q.; and Allende-Cid, H. 2023.

</span>
<span class="ltx_bibblock">Explainability of automated fact verification systems: A comprehensive review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Applied Sciences</em>, 13(23): 12608.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venktesh et al. (2024)</span>
<span class="ltx_bibblock">
Venktesh, V.; Anand, A.; Anand, A.; and Setty, V. 2024.

</span>
<span class="ltx_bibblock">QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arxiv:2403.17169</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vladika and Matthes (2023)</span>
<span class="ltx_bibblock">
Vladika, J.; and Matthes, F. 2023.

</span>
<span class="ltx_bibblock">Scientific Fact-Checking: A Survey of Resources and Approaches.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadden et al. (2020)</span>
<span class="ltx_bibblock">
Wadden, D.; Lin, S.; Lo, K.; Wang, L. L.; van Zuylen, M.; Cohan, A.; and Hajishirzi, H. 2020.

</span>
<span class="ltx_bibblock">Fact or Fiction: Verifying Scientific Claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadden and Lo (2021)</span>
<span class="ltx_bibblock">
Wadden, D.; and Lo, K. 2021.

</span>
<span class="ltx_bibblock">Overview and Insights from the SCIVER shared task on Scientific Claim Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Proceedings of the Second Workshop on Scholarly Document Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadden et al. (2022a)</span>
<span class="ltx_bibblock">
Wadden, D.; Lo, K.; Kuehl, B.; Cohan, A.; Beltagy, I.; Wang, L. L.; and Hajishirzi, H. 2022a.

</span>
<span class="ltx_bibblock">SciFact-Open: Towards open-domain scientific claim verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, 4719–4734.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadden et al. (2022b)</span>
<span class="ltx_bibblock">
Wadden, D.; Lo, K.; Wang, L. L.; Cohan, A.; Beltagy, I.; and Hajishirzi, H. 2022b.

</span>
<span class="ltx_bibblock">MultiVerS: Improving scientific claim verification with weak supervision and full-document context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Findings of the Association for Computational Linguistics: NAACL 2022</em>, 61–76.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Wang, G.; Harwood, K.; Chillrud, L.; Ananthram, A.; Subbiah, M.; and Mckeown, K. 2023a.

</span>
<span class="ltx_bibblock">Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, 14114–14127.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Shu (2023)</span>
<span class="ltx_bibblock">
Wang, H.; and Shu, K. 2023.

</span>
<span class="ltx_bibblock">Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, N. X.; Mahajan, D.; Danilevsky, M.; and Rosenthal, S. 2021.

</span>
<span class="ltx_bibblock">SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2017)</span>
<span class="ltx_bibblock">
Wang, W. Y. 2017.

</span>
<span class="ltx_bibblock">“Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 422–426.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Wang, Y.; Reddy, R. G.; Mujahid, Z. M.; Arora, A.; Rubashevskii, A.; Geng, J.; Afzal, O. M.; Pan, L.; Borenstein, N.; Pillai, A.; Augenstein, I.; Gurevych, I.; and Nakov, P. 2023b.

</span>
<span class="ltx_bibblock">Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023c)</span>
<span class="ltx_bibblock">
Wang, Z.; Araki, J.; Jiang, Z.; Parvez, M. R.; and Neubig, G. 2023c.

</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2311.08377</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Advances in neural information processing systems</em>, 35: 24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Wei, J.; Yang, C.; Song, X.; Lu, Y.; Hu, N.; Tran, D.; Peng, D.; Liu, R.; Huang, D.; Du, C.; et al. 2024.

</span>
<span class="ltx_bibblock">Long-form factuality in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">arXiv preprint arXiv:2403.18802</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2024)</span>
<span class="ltx_bibblock">
Xia, Y.; Zhou, J.; Shi, Z.; Chen, J.; and Huang, H. 2024.

</span>
<span class="ltx_bibblock">Improving Retrieval Augmented Language Model with Self-Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:2407.19813</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Xu, R.; Qi, Z.; Wang, C.; Wang, H.; Zhang, Y.; and Xu, W. 2024a.

</span>
<span class="ltx_bibblock">Knowledge Conflicts for LLMs: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2403.08319</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-S. 2024b.

</span>
<span class="ltx_bibblock">Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the ACM on Web Conference 2024</em>, 1362–1373.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Yang, Z.; Ma, J.; Chen, H.; Lin, H.; Luo, Z.; and Chang, Y. 2022.

</span>
<span class="ltx_bibblock">A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023)</span>
<span class="ltx_bibblock">
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K. R.; and Cao, Y. 2023.

</span>
<span class="ltx_bibblock">ReAct: Synergizing Reasoning and Acting in Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. (2024)</span>
<span class="ltx_bibblock">
Yoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2024.

</span>
<span class="ltx_bibblock">Making Retrieval-Augmented Language Models Robust to Irrelevant Context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D. 2023.

</span>
<span class="ltx_bibblock">Chain-of-note: Enhancing robustness in retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">arXiv preprint arXiv:2311.09210</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zamani and Bendersky (2024)</span>
<span class="ltx_bibblock">
Zamani, H.; and Bendersky, M. 2024.

</span>
<span class="ltx_bibblock">Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2641–2646.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al. (2022)</span>
<span class="ltx_bibblock">
Zelikman, E.; Wu, Y.; Mu, J.; and Goodman, N. 2022.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Advances in Neural Information Processing Systems</em>, 35: 15476–15488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng and Gao (2023)</span>
<span class="ltx_bibblock">
Zeng, F.; and Gao, W. 2023.

</span>
<span class="ltx_bibblock">Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng, Abumansour, and Zubiaga (2021)</span>
<span class="ltx_bibblock">
Zeng, X.; Abumansour, A. S.; and Zubiaga, A. 2021.

</span>
<span class="ltx_bibblock">Automated fact-checking: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Language and Linguistics Compass</em>, 15(10): e12438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng and Zubiaga (2024)</span>
<span class="ltx_bibblock">
Zeng, X.; and Zubiaga, A. 2024.

</span>
<span class="ltx_bibblock">MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:2401.16282</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al. (2023)</span>
<span class="ltx_bibblock">
Zha, Y.; Yang, Y.; Li, R.; and Hu, Z. 2023.

</span>
<span class="ltx_bibblock">AlignScore: Evaluating Factual Consistency with A Unified Alignment Function.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 11328–11348.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhang, H.; Zhang, R.; Guo, J.; de Rijke, M.; Fan, Y.; and Cheng, X. 2023.

</span>
<span class="ltx_bibblock">From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, 6373–6384.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Gao (2023)</span>
<span class="ltx_bibblock">
Zhang, X.; and Gao, W. 2023.

</span>
<span class="ltx_bibblock">Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Gao (2024)</span>
<span class="ltx_bibblock">
Zhang, X.; and Gao, W. 2024.

</span>
<span class="ltx_bibblock">Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, 13861–13873.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Zhao, X.; Wang, L.; Wang, Z.; Cheng, H.; Zhang, R.; and Wong, K.-F. 2024.

</span>
<span class="ltx_bibblock">Pacar: Automated fact-checking with planning and customized action reasoning using large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Zhou, J.; Zhang, Y.; Luo, Q.; Parker, A. G.; and De Choudhury, M. 2023.

</span>
<span class="ltx_bibblock">Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Zhu, Y.; Si, J.; Zhao, Y.; Zhu, H.; Zhou, D.; and He, Y. 2023.

</span>
<span class="ltx_bibblock">EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 26 14:41:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
