<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters</title>
<!--Generated on Tue Jun 11 22:35:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.01741v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S1" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S2" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S3" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Computing PVF</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S3.SS1" title="In III Computing PVF ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">PVF Definition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S3.SS2" title="In III Computing PVF ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Fault Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S3.SS3" title="In III Computing PVF ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Fault Injection (FI) Approach</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Case Study on DLRM</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.SS1" title="In IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">DLRM Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.SS2" title="In IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.SS3" title="In IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">PVF under MBF</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.SS4" title="In IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">PVF under SBF</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S5" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Case Study on CNN Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S6" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Case Study on NLP Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S7" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S8" title="In PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xun Jiao, Fred Lin, Harish Dixit, Joel Coburn, Abhinav Pandey, Han Wang 
<br class="ltx_break"/>Venkat Ramesh, Jianyu Huang, Wang Xu, Daniel Moore, Sriram Sankar 
<br class="ltx_break"/>Meta Platforms, Inc
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Reliability of AI systems is a fundamental concern for the successful deployment and widespread adoption of AI technologies. Unfortunately, the escalating complexity and heterogeneity of AI hardware systems make them increasingly susceptible to hardware faults, e.g., silent data corruptions (SDC), that can potentially corrupt model parameters. When this occurs during AI inference/servicing, it can potentially lead to incorrect or degraded model output for users, ultimately affecting the quality and reliability of AI services. In light of the escalating threat, it is crucial to address key questions: How vulnerable are AI models to parameter corruptions, and how do different parts (such as modules and layers) of the models exhibit varying vulnerability levels to parameter corruptions? To systematically address this question, we propose a novel quantitative metric, Parameter Vulnerability Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer architecture community, aiming to standardize the quantification of AI model vulnerability against parameter corruptions. We define a model parameter’s PVF as the probability that a corruption in that particular model parameter will result in an incorrect output. Similar to AVF, this statistical concept can be derived from statistically extensive and meaningful fault injection (FI) experiments.
In this paper, we present several use cases on applying PVF to three types of tasks/models during inference – recommendation (DLRM), vision classification (CNN), and text classification (BERT), while presenting an in-depth vulnerability analysis on DLRM. In DLRM, our FI results show that different parts of DLRM present different vulnerability levels: top-MLP layers are the most vulnerable parameter component, while embedding tables exhibit comparatively lower vulnerability level. We also discuss the potential use case of PVF during model training. PVF can provide pivotal insights to AI hardware designers in balancing the tradeoff between fault protection and performance/efficiency such as mapping vulnerable AI parameter components to well-protected hardware modules. PVF metric is applicable to any AI model and has a potential to help unify and standardize AI vulnerability/resilience evaluation practice.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The reliability of AI systems directly translates to the dependability, safety, and functionality of services running on top of them. For instance, in recommendation model inference, a reliable model is essential for accurate personalized recommendations, crucial for achieving positive business outcomes.
Unfortunately, as AI hardware systems become more complex and heterogeneous, and as transistor technology plunges into the deep-nanometer regime, the reliability of AI hardware systems faces a mounting challenge and a rising susceptibility to faults that can potentially corrupt model parameters. One pronounced threat that has been gaining increasing attention recently in hardware systems is data corruption, referring to errors or alterations in data that may occur during storage, transmission, or processing, leading to unintended changes in information. This can happen due to manufacturing defects, aging components, or environmental factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib4" title="">4</a>]</cite>. For AI systems, these faults can impact accuracy, integrity, and reliability of high-level AI application quality.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In particular, hardware faults that are not reported by standard fault reporting mechanisms but leading to erroneous application behavior have become increasingly prominent and harder to detect. We refer to these as <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">silent data corruption (SDC)</span>, and has been reported by Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib6" title="">6</a>]</cite>, and confirmed by Google and Alibaba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib23" title="">23</a>]</cite>.
In AI systems, Nvidia reported that “Hopper architecture GPUs may intermittently experience SDC resulting in incorrect results” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib2" title="">2</a>]</cite>, and Google reported hard to debug SDCs in their Tensor Processing Unit (TPU) systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S1.F1.g1" src="x1.png" width="1245"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Fault injection experiments flow</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing work on evaluating AI vulnerability against hardware faults used metrics such as accuracy drop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib16" title="">16</a>]</cite> or SDC rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib3" title="">3</a>]</cite>, focusing on model-level vulnerability, but there is a lack of an unified parameter-level vulnerability metric that can answer this question: <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">How likely is a parameter corruption to result in an incorrect model output?</span> The answer to this question is critical in AI hardware design, especially when mapping AI model parameters or software variables to hardware blocks which may have varying fault protection capabilities.
Thus, we propose the PVF metric, with the following features:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Parameter-level Quantitative Assessment: As a quantitative metric, PVF concentrates on parameter-level vulnerability, calculating the likelihood that a corruption in a specific model parameter will lead to an incorrect model output. This “parameter” can be defined at different scales and granularities, such as an individual parameter or a group of parameters. By applying PVF to several example models, we quantify the varying vulnerability levels of different parts of a given model.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Scalability Across AI Models/Tasks: PVF can be scalable and applicable across a wide range of AI models, tasks, and hardware fault models. While in this paper, we present case studies on three types of AI classification tasks/models (recommendation, vision, and text), PVF can be applied to other AI models with model-specific attributes such as the definition of “incorrectness”. We further discuss potential extension of PVF to training.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Guiding AI System Design: PVF can provide pivotal insights for AI system designers, guiding them in making informed decisions about balancing fault protection with performance and efficiency, e.g., map higher vulnerable parameters to better-protected hardware blocks and explore tradeoffs on latency, power and reliability by enabling a surgical approach to fault tolerance at selective locations instead of a “catch-all/none” approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Standardization of AI Vulnerability/Resilience Evaluation: We propose to use PVF as a standard metric for AI vulnerability/resilience evaluation. It has the potential to unify and standardize such practices, making it easier to compare the reliability of different AI systems/parameters and fostering open collaboration and progress in the industry/research community.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Evaluating AI vulnerability against hardware faults has been a topic of increasing focus and priority with recent SDC findings from hyperscalars such as Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib6" title="">6</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib15" title="">15</a>]</cite>), not only because of increasing susceptibility of AI systems to hardware faults in general, but also the increasing need to improve AI hardware acceleration by balancing the tradeoff between latency and fault protection. For example, Ares <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib20" title="">20</a>]</cite> and PyTorchFI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib16" title="">16</a>]</cite> are two popular tools on top of PyTorch that can inject faults to AI models and evaluate the resulting accuracy drop. In addition, another widely-used evaluation metric is SDC rate which compute the probability of output corruption by performing FI experiments. For instance, Li et al. compute the SDC rates of CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib15" title="">15</a>]</cite> and Agarwal et al. compute the SDC rates for large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">However, these evaluation metrics are focused on model-level vulnerability; there is a lack of an unified metric to quantify parameter-level vulnerability. Quantification of model parameter vulnerability has much implication in AI system software-hardware codesign, especially when mapping software parameters to hardware blocks with different fault protection capabilities. For example, given the different fault management capabilities of SRAMs, DRAMs and HBMs, there are varying degrees of vulnerabilities under consideration while mapping and allocating different model parameters.
To facilitate scenarios like this and unify evaluation across AI models and fault scenarios, we propose PVF.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Another set of relevant work is in the field of adversarial attack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib19" title="">19</a>]</cite>; these works use gradient-based methods looking for most vulnerable parameter. Our work is fundamentally different from these works because our target is different, and fault model is different (these works often assume statistical corruption such as Gaussian noise which cannot handle faults such as NaN due to bit flips).</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Computing PVF</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">PVF Definition</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">PVF is inspired by architectural vulnerability factor (AVF) in computer architecture community. AVF quantifies the vulnerability of a processor’s microarchitecture to soft errors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib17" title="">17</a>]</cite>. An architectural structure’s AVF is the probability that a fault in that particular structure will result in a program output error. Similarly, we define a model parameter’s PVF as the probability that a corruption in that particular model parameter will result in an incorrect model output, e.g., a wrong click prediction or a wrong image classification. Note that PVF can scale to different fault models and parameter granularities, which is shown later.
As a statistical concept, PVF needs to be derived through a large number of fault injection (FI) experiments that are statistically meaningful.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Fault Model</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this paper, we consider parameter corruptions using three hardware fault models. These simulate hardware faults such as memory bit flips due to soft errors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib7" title="">7</a>]</cite>. These fault models are used in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib12" title="">12</a>]</cite>. We recognize that different hardware platforms exhibit different error patterns, and the fault model used here may not fully represent realistic hardware error patterns. However, PVF is adaptable to any user-specified fault model based on users’ specific scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p2.1.1">Single-Bit Flip (SBF):</span> Under SBF model, we inject a random single bit flip to the target parameter component (e.g., embedding table) during each FI experiment. This is the most widely used fault model across different resilience studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p3.2.1">Multiple Bit Flip (MBF)/Bit Error Rate (BER):</span> Under MBF model, we inject multiple bit flips to the target parameter component during each FI experiment. For example, instead of injecting 1 bit flip, we can inject 32 bit flips to the embedding table. Multiple bit flips can be injected either using a fixed number of multiple bits, or using a bit error rate (BER), which is the portion of the bits getting flipped based on the target parameter count. These two metrics are technically interchangeable and one can be derived using the other.
In our FI experiment, we decide to use the fixed-number based multiple bit flip model. This is to ensure that target parameter with small parameter count (e.g., 48 weights in the <math alttext="8^{th}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">8</mn><mrow id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p3.1.m1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><cn id="S3.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1.2">8</cn><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><times id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">8^{th}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> embedding table of the experimented DLRM) under application of a realistic BER (e.g., <math alttext="10^{-10}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mn id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">10</mn><mrow id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml"><mo id="S3.SS2.p3.2.m2.1.1.3a" xref="S3.SS2.p3.2.m2.1.1.3.cmml">−</mo><mn id="S3.SS2.p3.2.m2.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.3.2.cmml">10</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><cn id="S3.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.2">10</cn><apply id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><minus id="S3.SS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.3"></minus><cn id="S3.SS2.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">10^{-10}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT</annotation></semantics></math>) does not yield a practically unusable number for the FI experiment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p4.1.1">Multiple Burst Bit Flip (MBBF): </span> Under MBBF model, we assume a burst error model, where for each FI experiment, two consecutive bits will get flipped.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Fault Injection (FI) Approach</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We use the following steps to perform FI experiments for any given AI model during inference (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S1.F1" title="Figure 1 ‣ I Introduction ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Step 1 Load the (pre-trained) AI Model: Since we focus on inference for now, we assume there is a pre-trained AI model already. We load the pre-trained AI model (weights) that we want to inject faults.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Step 2 Identify Target Parameters: We identify the specific parameters in the AI model that we want to compute PVF; examples include a specific embedding table, a specific convolutional kernels, or a specific fully-connected layer, or other relevant parts of the model.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Step 3 Inject Faults: For the target parameter component, we inject faults based on the given fault model. For example, if using SBF for an embedding table with 100 embedding weights where each weight has 32 bits, we flip 1 bit randomly sampled from the 3200 bits.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Step 4 Evaluate the Corrupted Model: Run the AI model inference with the injected faults on the test data, and compare the model’s output with groundtruth output. Step 3-4 is considered as one FI experiment.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.3">Step 5 Repeat for <math alttext="N" class="ltx_Math" display="inline" id="S3.I1.i5.p1.1.m1.1"><semantics id="S3.I1.i5.p1.1.m1.1a"><mi id="S3.I1.i5.p1.1.m1.1.1" xref="S3.I1.i5.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.1.m1.1b"><ci id="S3.I1.i5.p1.1.m1.1.1.cmml" xref="S3.I1.i5.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.1.m1.1d">italic_N</annotation></semantics></math> times: Repeat Step 3-4 for <math alttext="N" class="ltx_Math" display="inline" id="S3.I1.i5.p1.2.m2.1"><semantics id="S3.I1.i5.p1.2.m2.1a"><mi id="S3.I1.i5.p1.2.m2.1.1" xref="S3.I1.i5.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.2.m2.1b"><ci id="S3.I1.i5.p1.2.m2.1.1.cmml" xref="S3.I1.i5.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.2.m2.1d">italic_N</annotation></semantics></math> times (e.g., 1 million) wherein each FI we use different random faults and inputs, and record the number of incorrect output (<math alttext="D" class="ltx_Math" display="inline" id="S3.I1.i5.p1.3.m3.1"><semantics id="S3.I1.i5.p1.3.m3.1a"><mi id="S3.I1.i5.p1.3.m3.1.1" xref="S3.I1.i5.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.3.m3.1b"><ci id="S3.I1.i5.p1.3.m3.1.1.cmml" xref="S3.I1.i5.p1.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.3.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.p1.3.m3.1d">italic_D</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">Step 6 Calculate PVF: Compute PVF as the <math alttext="D/N" class="ltx_Math" display="inline" id="S3.I1.i6.p1.1.m1.1"><semantics id="S3.I1.i6.p1.1.m1.1a"><mrow id="S3.I1.i6.p1.1.m1.1.1" xref="S3.I1.i6.p1.1.m1.1.1.cmml"><mi id="S3.I1.i6.p1.1.m1.1.1.2" xref="S3.I1.i6.p1.1.m1.1.1.2.cmml">D</mi><mo id="S3.I1.i6.p1.1.m1.1.1.1" xref="S3.I1.i6.p1.1.m1.1.1.1.cmml">/</mo><mi id="S3.I1.i6.p1.1.m1.1.1.3" xref="S3.I1.i6.p1.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.1.m1.1b"><apply id="S3.I1.i6.p1.1.m1.1.1.cmml" xref="S3.I1.i6.p1.1.m1.1.1"><divide id="S3.I1.i6.p1.1.m1.1.1.1.cmml" xref="S3.I1.i6.p1.1.m1.1.1.1"></divide><ci id="S3.I1.i6.p1.1.m1.1.1.2.cmml" xref="S3.I1.i6.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i6.p1.1.m1.1.1.3.cmml" xref="S3.I1.i6.p1.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.1.m1.1c">D/N</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i6.p1.1.m1.1d">italic_D / italic_N</annotation></semantics></math>. Note, because the model can have wrong predictions without any hardware errors, we only focus on cases where correct predictions become incorrect.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Case Study on DLRM</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">DLRM Architecture</span>
</h3>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S4.F2.g1" src="x2.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>DLRM architecture overview</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We present the first case study on DLRM inference. DLRM was developed by Meta for personalized content recommendation, and has constituted 79% of the overall AI inference cycles at the Meta data center <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib11" title="">11</a>]</cite>. The fundamental architecture of a DLRM is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F2" title="Figure 2 ‣ IV-A DLRM Architecture ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">2</span></a>, composed of three key parameter components: embedding tables, bottom MLP (bot-MLP), and top MLP (top-MLP). Embedding tables transform sparse categorical features into dense, continuous representations. These embeddings capture latent features and relationships, allowing the model to discern intricate patterns and correlations in the data.
The MLP layers in DLRM contains a bot-MLP and a top-MLP. Firstly, the dense features undergo transformation through the bot-MLP. This segment comprises a sequence of linear layers accompanied by Rectified Linear Unit (ReLU) activations. Subsequently, the outcome from the bot-MLP and the embedding vectors are combined in a feature interaction such as feature concatenation. The result of this interaction is then fed into the top-MLP. The top-MLP generates the final model output, e.g., click prediction.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We focus on fault injection to model parameters such as MLP layers and embedding tables because they make most of the total storage needed for a model. Further, they are essentially read-only and static so bit flips are likely to have persisting impact, whereas for intermediate model data such as activations, they have short lifetime before they are updated again. Other data structures critical to model execution will typically be protected, similar to protecting stack/heap, etc., so we do not consider these.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We train the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">DLRM</span> using Criteo DAC dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib1" title="">1</a>]</cite>, as used by the original DLRM paper, and achieve the baseline accuracy (78.83%), aligned with the original DLRM paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib18" title="">18</a>]</cite>. DLRM uses 32-bit floating point number (FP32) as the default data type for its parameters.
The Criteo-DAC dataset contains the click records of 45 million users over a 7-day span with 13 dense features and 26 categorical features.
The number of parameters of embedding tables ranges between 48 to 162 million, bot-MLP and top-MLP components have 155984 and 320001 parameters, respectively. To be statistically meaningful, each PVF is derived by performing 1 million independent FI experiments.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">PVF under MBF</span>
</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="S4.F3.g1" src="extracted/5660439/figures/pvf_multi_bit_flip_single_bit.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>PVF of DLRM Parameters under MBF</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F3" title="Figure 3 ‣ IV-C PVF under MBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the PVF of three DLRM parameter components, embedding table, bot-MLP, and top-MLP, under MBF fault models with 1, 2, 4, 8, 16, 32, 64, and 128 bit flips during each inference. We can observe several important facts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS3.p2.1.1">Observation 1: Different parts of DLRM present different vulnerability levels.</span>
We observe that different parts of DLRM present different vulnerability level, e.g., under a single bit flip, most embedding tables have low PVF, e.g., less than 0.0001%; however, top-MLP can have 0.4% under even a single bit flip. This is significant – for every 1000 inferences, 4 inferences will be incorrect. This highlights the importance of protecting specific vulnerable parameters for a given model based on the PVF measurement.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We observe that even with 128 bit flips during each inference, the PVF of embedding table is still low (<math alttext="&lt;0.0001\%" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">&lt;</mo><mrow id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml"><mn id="S4.SS3.p3.1.m1.1.1.3.2" xref="S4.SS3.p3.1.m1.1.1.3.2.cmml">0.0001</mn><mo id="S4.SS3.p3.1.m1.1.1.3.1" xref="S4.SS3.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><lt id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.p3.1.m1.1.1.3.1">percent</csymbol><cn id="S4.SS3.p3.1.m1.1.1.3.2.cmml" type="float" xref="S4.SS3.p3.1.m1.1.1.3.2">0.0001</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">&lt;0.0001\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">&lt; 0.0001 %</annotation></semantics></math>), exhibiting notable resilience against corruptions; for MLP components, PVF has increased to 40% or 10% for top-MLP and bot-MLP components respectively, while observing multiple NaN values.
This is attributed to embedding tables being highly sparse, and parameter corruptions are only activated when the particular corrupted parameter is “hit” by the corresponding sparse feature.
Even if activated, it may get masked by the subsequent neural processing. Consequently, a bit flip in the embedding table is likely to not get exposed or reflected in the DLRM output in most cases. This further lowers the probability of error exposure for the architecture under study.
Note that, because different parameter components has different parameter count, the same bit flip count is essentially equivalent to different BERs for these parameter components. For example, 1000 bit flips for embedding table is 0.000185%, and for top-MLP and bot-MLP are 0.641% and 0.3124%. Thus, when we compare the PVF across different parameter components, we need to explicitly state the underlying fault model (which can be specified by users based on the assumption regarding hardware faults) to make a fair and clear comparison.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S4.F4.g1" src="extracted/5660439/figures/pvf_multi_bit-burst-2-bit.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>PVF of DLRM Parameters under MBBF</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F4" title="Figure 4 ‣ IV-C PVF under MBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">4</span></a> presents the PVF under burst errors, where we sweep the burst error counts from 1 to 128. In this case, each burst error will flip two consecutive bits. Results show that, even at 128 burst errors, the PVF of embedding table is still low (<math alttext="&lt;0.0001\%" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mi id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">&lt;</mo><mrow id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3.cmml"><mn id="S4.SS3.p4.1.m1.1.1.3.2" xref="S4.SS3.p4.1.m1.1.1.3.2.cmml">0.0001</mn><mo id="S4.SS3.p4.1.m1.1.1.3.1" xref="S4.SS3.p4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><lt id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.p4.1.m1.1.1.3.cmml" xref="S4.SS3.p4.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.3.1.cmml" xref="S4.SS3.p4.1.m1.1.1.3.1">percent</csymbol><cn id="S4.SS3.p4.1.m1.1.1.3.2.cmml" type="float" xref="S4.SS3.p4.1.m1.1.1.3.2">0.0001</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">&lt;0.0001\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">&lt; 0.0001 %</annotation></semantics></math>), while for top mlp and bottom mlp, the PVF increased to 50% and 30% respectively under 128 burst errors, higher than the PVF under MBF which is intuitively reasonable.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">PVF under SBF</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To further break down the granularity of target parameters, we measure the PVF of individual embedding tables <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">under the SBF fault model</span>. The results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F5" title="Figure 5 ‣ IV-D PVF under SBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="396" id="S4.F5.g1" src="extracted/5660439/figures/pvf_individual_emb_single_bit.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>PVF of DLRM Parameters under SBF</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S4.F6.g1" src="extracted/5660439/figures/param_count.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Inverse of Number of DLRM Parameters (1/count)</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS4.p2.1.1">Observation 2: Top-MLP has higher vulnerability than bot-MLP and embedding tables, while different individual embedding tables show different vulnerabilities. </span>
Based on Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F5" title="Figure 5 ‣ IV-D PVF under SBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">5</span></a>, we observe that MLP components have higher PVF than embedding tables. The top-MLP component has highest PVF among all target parameter components, while bot-MLP component has higher PVF than most embedding tables. The reason for this is that embedding tables are highly sparse (stated in Observation 2 and 3 earlier).
Consequently, a bit flip in the embedding table is likely not to get exposed or reflected in the DLRM processing in most cases. This further lowers the probability of error reflection.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">We also observe that top MLP component has higher PVF than bottom MLP. This is attributed to the top-MLP being closer to the final model, and hence has less of a chance to be mitigated by inherent error masking probability of DLRM models. Note that in this case, top-MLP has more parameters than bot-MLP – meaning that same bit flip count suggests smaller BER for top-MLP.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Under SBF fault model, we observe that the PVF of embedding tables varies by orders of magnitude, and is highly (inversely) correlated with the parameter count (as seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F6" title="Figure 6 ‣ IV-D PVF under SBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">6</span></a>), which shows the inverse of the parameter count. We draw a correlation that, the smaller an embedding table is, the higher its PVF (under SBF model). This is intuitively reasonable because under SBF fault model, a single bit flip will always occur in the target parameter regardless of the size of the target parameter. Thus, for an embedding table with smaller parameter count, it is more likely that the bit flip will be activated by the sparse feature.
However, it is worth to note that, for a smaller embedding table, the probability of a bit flip occurrence is also smaller than a larger embedding table.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S4.F7.g1" src="extracted/5660439/figures/pvf_bit_position.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>PVF of different bit positions</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS4.p5.1.1">Observation 3: Different bits have different vulnerability levels.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S4.F7" title="Figure 7 ‣ IV-D PVF under SBF ‣ IV Case Study on DLRM ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">7</span></a> presents PVF of different bit positions (we omit some least significant bits because they consistently have 0 PVF) for embedding, top-MLP, and bot-MLP. In this scenario, to obtain the PVF of a given bit position, we only inject bit flips to that specific bit position. A key observation is that the sign bit (bit 31) is not the most vulnerable bit. Bit 30 is the most vulnerable bit because compared to other bits, it is more likely to result in large values as well as abnormal data such as NaNs; in FP32 data, NaN could be caused by a “0” to “1” flip at bit position 30.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Case Study on CNN Models</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We present the second case study on convolutional neural networks (CNNs) for vision classification tasks. For the sake of simplicity, we pick a small CNN, LeNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib14" title="">14</a>]</cite>, for MNIST dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib13" title="">13</a>]</cite>, under the SBF model. We show the CNN layers and the corresponding weights count in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S5.T1" title="TABLE I ‣ V Case Study on CNN Models ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>CNN layers w/ weight counts</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1">Layer Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.2">Weight Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.3">Weight Count</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.1">conv1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2">[6, 1, 5, 5]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.3">150</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.1">conv2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2">[16, 6, 5, 5]</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.3">2400</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.1">fc1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.2">[120, 256]</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.3">30720</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.1">fc2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.2">[84, 120]</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.3">10080</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.6.5.1">fc3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.6.5.2">[10, 84]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.6.5.3">840</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="367" id="S5.F8.g1" src="extracted/5660439/figures/lenet_SBF.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>PVF of LeNet under SBF</figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We perform 100K FI experiments for each target parameter, and present the obtained PVF for each layer of LeNet in Fig <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S5.F8" title="Figure 8 ‣ V Case Study on CNN Models ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">8</span></a>, where we can see that, conv1 has the highest PVF among all the conv (convolutional) and fc (fully-connected) layers. That is, a single bit flip in conv1 is more likely to result in an incorrect output. However, it is worth to note that, similar to previous discussion, the weight count of conv1 is only 0.06% of conv2 and 0.488% of fc1, thus the probability of having a bit flip in conv1 is less than conv2 and fc1 under same hardware condition. A fairer comparison would be using BER for all parameters; however, for a small BER such as <math alttext="10^{-7}" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><msup id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mn id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml"><mo id="S5.p2.1.m1.1.1.3a" xref="S5.p2.1.m1.1.1.3.cmml">−</mo><mn id="S5.p2.1.m1.1.1.3.2" xref="S5.p2.1.m1.1.1.3.2.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1">superscript</csymbol><cn id="S5.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.p2.1.m1.1.1.2">10</cn><apply id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3"><minus id="S5.p2.1.m1.1.1.3.1.cmml" xref="S5.p2.1.m1.1.1.3"></minus><cn id="S5.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S5.p2.1.m1.1.1.3.2">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">10^{-7}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT</annotation></semantics></math>, conv1 layer will not even produce a single bit flip for most FI experiments.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Case Study on NLP Models</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We also present a case study on applying PVF to NLP models. For the sake of simplicity, we fine-tuned a smaller BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib5" title="">5</a>]</cite> with four encoder layers, for ag-news dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#bib.bib24" title="">24</a>]</cite>, under the SBF model. For the experimental setup, we directly use the torchtext.datasets() utility function to load the ag-news dataset. We target 12 BERT parameter components: for each encoder layer from layer 0 to layer 3, we target query.weight, key.weight, and value.weight, which are the weights that perform linear transformations to the original input token. Each weight component has same shape ([256, 256]) and 65536 parameter counts.</p>
</div>
<figure class="ltx_figure" id="S6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="410" id="S6.F9.g1" src="extracted/5660439/figures/bert_sbf.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>PVF of Tiny BERT under SBF. (L.0.A.q.w means layer.0.attention.query.weight)</figcaption>
</figure>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.4">We perform 100K FI experiments for each target parameter, and present the obtained PVF for each target component in Fig <a class="ltx_ref" href="https://arxiv.org/html/2405.01741v3#S6.F9" title="Figure 9 ‣ VI Case Study on NLP Models ‣ PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters"><span class="ltx_text ltx_ref_tag">9</span></a>, where we can see that, value.weight parameter components always have the highest PVF among all the components, regardless of its specific layer. This is intuitively reasonable because in the original self-attention equation, both <math alttext="Q" class="ltx_Math" display="inline" id="S6.p2.1.m1.1"><semantics id="S6.p2.1.m1.1a"><mi id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.p2.1.m1.1d">italic_Q</annotation></semantics></math> (query) and <math alttext="K" class="ltx_Math" display="inline" id="S6.p2.2.m2.1"><semantics id="S6.p2.2.m2.1a"><mi id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><ci id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S6.p2.2.m2.1d">italic_K</annotation></semantics></math> (key) would go through more transformations such as downscaling (divide by <math alttext="\sqrt{d_{k}}" class="ltx_Math" display="inline" id="S6.p2.3.m3.1"><semantics id="S6.p2.3.m3.1a"><msqrt id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml"><msub id="S6.p2.3.m3.1.1.2" xref="S6.p2.3.m3.1.1.2.cmml"><mi id="S6.p2.3.m3.1.1.2.2" xref="S6.p2.3.m3.1.1.2.2.cmml">d</mi><mi id="S6.p2.3.m3.1.1.2.3" xref="S6.p2.3.m3.1.1.2.3.cmml">k</mi></msub></msqrt><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><apply id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1"><root id="S6.p2.3.m3.1.1a.cmml" xref="S6.p2.3.m3.1.1"></root><apply id="S6.p2.3.m3.1.1.2.cmml" xref="S6.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S6.p2.3.m3.1.1.2.1.cmml" xref="S6.p2.3.m3.1.1.2">subscript</csymbol><ci id="S6.p2.3.m3.1.1.2.2.cmml" xref="S6.p2.3.m3.1.1.2.2">𝑑</ci><ci id="S6.p2.3.m3.1.1.2.3.cmml" xref="S6.p2.3.m3.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">\sqrt{d_{k}}</annotation><annotation encoding="application/x-llamapun" id="S6.p2.3.m3.1d">square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>) and softmax which may mitigate certain bit flips, while <math alttext="V" class="ltx_Math" display="inline" id="S6.p2.4.m4.1"><semantics id="S6.p2.4.m4.1a"><mi id="S6.p2.4.m4.1.1" xref="S6.p2.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.1b"><ci id="S6.p2.4.m4.1.1.cmml" xref="S6.p2.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S6.p2.4.m4.1d">italic_V</annotation></semantics></math> almost directly determines the self-attention output.</p>
</div>
<div class="ltx_para" id="S6.p3">
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Attention(Q,K,V)=\text{softmax}\frac{Q\cdot K^{T}}{\sqrt{d_{k}}}\cdot V" class="ltx_Math" display="block" id="S6.E1.m1.3"><semantics id="S6.E1.m1.3a"><mrow id="S6.E1.m1.3.4" xref="S6.E1.m1.3.4.cmml"><mrow id="S6.E1.m1.3.4.2" xref="S6.E1.m1.3.4.2.cmml"><mi id="S6.E1.m1.3.4.2.2" xref="S6.E1.m1.3.4.2.2.cmml">A</mi><mo id="S6.E1.m1.3.4.2.1" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.3" xref="S6.E1.m1.3.4.2.3.cmml">t</mi><mo id="S6.E1.m1.3.4.2.1a" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.4" xref="S6.E1.m1.3.4.2.4.cmml">t</mi><mo id="S6.E1.m1.3.4.2.1b" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.5" xref="S6.E1.m1.3.4.2.5.cmml">e</mi><mo id="S6.E1.m1.3.4.2.1c" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.6" xref="S6.E1.m1.3.4.2.6.cmml">n</mi><mo id="S6.E1.m1.3.4.2.1d" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.7" xref="S6.E1.m1.3.4.2.7.cmml">t</mi><mo id="S6.E1.m1.3.4.2.1e" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.8" xref="S6.E1.m1.3.4.2.8.cmml">i</mi><mo id="S6.E1.m1.3.4.2.1f" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.9" xref="S6.E1.m1.3.4.2.9.cmml">o</mi><mo id="S6.E1.m1.3.4.2.1g" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S6.E1.m1.3.4.2.10" xref="S6.E1.m1.3.4.2.10.cmml">n</mi><mo id="S6.E1.m1.3.4.2.1h" xref="S6.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S6.E1.m1.3.4.2.11.2" xref="S6.E1.m1.3.4.2.11.1.cmml"><mo id="S6.E1.m1.3.4.2.11.2.1" stretchy="false" xref="S6.E1.m1.3.4.2.11.1.cmml">(</mo><mi id="S6.E1.m1.1.1" xref="S6.E1.m1.1.1.cmml">Q</mi><mo id="S6.E1.m1.3.4.2.11.2.2" xref="S6.E1.m1.3.4.2.11.1.cmml">,</mo><mi id="S6.E1.m1.2.2" xref="S6.E1.m1.2.2.cmml">K</mi><mo id="S6.E1.m1.3.4.2.11.2.3" xref="S6.E1.m1.3.4.2.11.1.cmml">,</mo><mi id="S6.E1.m1.3.3" xref="S6.E1.m1.3.3.cmml">V</mi><mo id="S6.E1.m1.3.4.2.11.2.4" stretchy="false" xref="S6.E1.m1.3.4.2.11.1.cmml">)</mo></mrow></mrow><mo id="S6.E1.m1.3.4.1" xref="S6.E1.m1.3.4.1.cmml">=</mo><mrow id="S6.E1.m1.3.4.3" xref="S6.E1.m1.3.4.3.cmml"><mrow id="S6.E1.m1.3.4.3.2" xref="S6.E1.m1.3.4.3.2.cmml"><mtext id="S6.E1.m1.3.4.3.2.2" xref="S6.E1.m1.3.4.3.2.2a.cmml">softmax</mtext><mo id="S6.E1.m1.3.4.3.2.1" xref="S6.E1.m1.3.4.3.2.1.cmml">⁢</mo><mfrac id="S6.E1.m1.3.4.3.2.3" xref="S6.E1.m1.3.4.3.2.3.cmml"><mrow id="S6.E1.m1.3.4.3.2.3.2" xref="S6.E1.m1.3.4.3.2.3.2.cmml"><mi id="S6.E1.m1.3.4.3.2.3.2.2" xref="S6.E1.m1.3.4.3.2.3.2.2.cmml">Q</mi><mo id="S6.E1.m1.3.4.3.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S6.E1.m1.3.4.3.2.3.2.1.cmml">⋅</mo><msup id="S6.E1.m1.3.4.3.2.3.2.3" xref="S6.E1.m1.3.4.3.2.3.2.3.cmml"><mi id="S6.E1.m1.3.4.3.2.3.2.3.2" xref="S6.E1.m1.3.4.3.2.3.2.3.2.cmml">K</mi><mi id="S6.E1.m1.3.4.3.2.3.2.3.3" xref="S6.E1.m1.3.4.3.2.3.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S6.E1.m1.3.4.3.2.3.3" xref="S6.E1.m1.3.4.3.2.3.3.cmml"><msub id="S6.E1.m1.3.4.3.2.3.3.2" xref="S6.E1.m1.3.4.3.2.3.3.2.cmml"><mi id="S6.E1.m1.3.4.3.2.3.3.2.2" xref="S6.E1.m1.3.4.3.2.3.3.2.2.cmml">d</mi><mi id="S6.E1.m1.3.4.3.2.3.3.2.3" xref="S6.E1.m1.3.4.3.2.3.3.2.3.cmml">k</mi></msub></msqrt></mfrac></mrow><mo id="S6.E1.m1.3.4.3.1" lspace="0.222em" rspace="0.222em" xref="S6.E1.m1.3.4.3.1.cmml">⋅</mo><mi id="S6.E1.m1.3.4.3.3" xref="S6.E1.m1.3.4.3.3.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.3b"><apply id="S6.E1.m1.3.4.cmml" xref="S6.E1.m1.3.4"><eq id="S6.E1.m1.3.4.1.cmml" xref="S6.E1.m1.3.4.1"></eq><apply id="S6.E1.m1.3.4.2.cmml" xref="S6.E1.m1.3.4.2"><times id="S6.E1.m1.3.4.2.1.cmml" xref="S6.E1.m1.3.4.2.1"></times><ci id="S6.E1.m1.3.4.2.2.cmml" xref="S6.E1.m1.3.4.2.2">𝐴</ci><ci id="S6.E1.m1.3.4.2.3.cmml" xref="S6.E1.m1.3.4.2.3">𝑡</ci><ci id="S6.E1.m1.3.4.2.4.cmml" xref="S6.E1.m1.3.4.2.4">𝑡</ci><ci id="S6.E1.m1.3.4.2.5.cmml" xref="S6.E1.m1.3.4.2.5">𝑒</ci><ci id="S6.E1.m1.3.4.2.6.cmml" xref="S6.E1.m1.3.4.2.6">𝑛</ci><ci id="S6.E1.m1.3.4.2.7.cmml" xref="S6.E1.m1.3.4.2.7">𝑡</ci><ci id="S6.E1.m1.3.4.2.8.cmml" xref="S6.E1.m1.3.4.2.8">𝑖</ci><ci id="S6.E1.m1.3.4.2.9.cmml" xref="S6.E1.m1.3.4.2.9">𝑜</ci><ci id="S6.E1.m1.3.4.2.10.cmml" xref="S6.E1.m1.3.4.2.10">𝑛</ci><vector id="S6.E1.m1.3.4.2.11.1.cmml" xref="S6.E1.m1.3.4.2.11.2"><ci id="S6.E1.m1.1.1.cmml" xref="S6.E1.m1.1.1">𝑄</ci><ci id="S6.E1.m1.2.2.cmml" xref="S6.E1.m1.2.2">𝐾</ci><ci id="S6.E1.m1.3.3.cmml" xref="S6.E1.m1.3.3">𝑉</ci></vector></apply><apply id="S6.E1.m1.3.4.3.cmml" xref="S6.E1.m1.3.4.3"><ci id="S6.E1.m1.3.4.3.1.cmml" xref="S6.E1.m1.3.4.3.1">⋅</ci><apply id="S6.E1.m1.3.4.3.2.cmml" xref="S6.E1.m1.3.4.3.2"><times id="S6.E1.m1.3.4.3.2.1.cmml" xref="S6.E1.m1.3.4.3.2.1"></times><ci id="S6.E1.m1.3.4.3.2.2a.cmml" xref="S6.E1.m1.3.4.3.2.2"><mtext id="S6.E1.m1.3.4.3.2.2.cmml" xref="S6.E1.m1.3.4.3.2.2">softmax</mtext></ci><apply id="S6.E1.m1.3.4.3.2.3.cmml" xref="S6.E1.m1.3.4.3.2.3"><divide id="S6.E1.m1.3.4.3.2.3.1.cmml" xref="S6.E1.m1.3.4.3.2.3"></divide><apply id="S6.E1.m1.3.4.3.2.3.2.cmml" xref="S6.E1.m1.3.4.3.2.3.2"><ci id="S6.E1.m1.3.4.3.2.3.2.1.cmml" xref="S6.E1.m1.3.4.3.2.3.2.1">⋅</ci><ci id="S6.E1.m1.3.4.3.2.3.2.2.cmml" xref="S6.E1.m1.3.4.3.2.3.2.2">𝑄</ci><apply id="S6.E1.m1.3.4.3.2.3.2.3.cmml" xref="S6.E1.m1.3.4.3.2.3.2.3"><csymbol cd="ambiguous" id="S6.E1.m1.3.4.3.2.3.2.3.1.cmml" xref="S6.E1.m1.3.4.3.2.3.2.3">superscript</csymbol><ci id="S6.E1.m1.3.4.3.2.3.2.3.2.cmml" xref="S6.E1.m1.3.4.3.2.3.2.3.2">𝐾</ci><ci id="S6.E1.m1.3.4.3.2.3.2.3.3.cmml" xref="S6.E1.m1.3.4.3.2.3.2.3.3">𝑇</ci></apply></apply><apply id="S6.E1.m1.3.4.3.2.3.3.cmml" xref="S6.E1.m1.3.4.3.2.3.3"><root id="S6.E1.m1.3.4.3.2.3.3a.cmml" xref="S6.E1.m1.3.4.3.2.3.3"></root><apply id="S6.E1.m1.3.4.3.2.3.3.2.cmml" xref="S6.E1.m1.3.4.3.2.3.3.2"><csymbol cd="ambiguous" id="S6.E1.m1.3.4.3.2.3.3.2.1.cmml" xref="S6.E1.m1.3.4.3.2.3.3.2">subscript</csymbol><ci id="S6.E1.m1.3.4.3.2.3.3.2.2.cmml" xref="S6.E1.m1.3.4.3.2.3.3.2.2">𝑑</ci><ci id="S6.E1.m1.3.4.3.2.3.3.2.3.cmml" xref="S6.E1.m1.3.4.3.2.3.3.2.3">𝑘</ci></apply></apply></apply></apply><ci id="S6.E1.m1.3.4.3.3.cmml" xref="S6.E1.m1.3.4.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.3c">Attention(Q,K,V)=\text{softmax}\frac{Q\cdot K^{T}}{\sqrt{d_{k}}}\cdot V</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.3d">italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n ( italic_Q , italic_K , italic_V ) = softmax divide start_ARG italic_Q ⋅ italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ⋅ italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">PVF is a versatile metric that can be tailored to various AI models/tasks. The definition of an “incorrect output” will vary based on the model/task and can be adapted to suit user requirements. For instance, in a large language model (LLM) used for question-answering tasks, an incorrect output would be an incorrect answer. For a GenAI model/task, users might have a specific metric to define “incorrectness” for the generated content. We anticipate that the introduction of PVF will stimulate diverse use cases in both research and production settings.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">PVF is also adaptable to various hardware fault models. While our study presents PVF under three fault models, we acknowledge that different hardware platforms under varying physical conditions may exhibit different fault models, such as bit flips, statistical variations, etc. The principle of PVF remains applicable, and the method to calculate PVF remains consistent. The only modification required is the manner in which the fault is injected, based on the assumed fault models.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Furthermore, PVF can be extended to the training phase to evaluate the effects of parameter corruptions on the model’s convergence capability. During training, the model’s parameters are iteratively updated to minimize a loss function. A corruption in a parameter could potentially disrupt this learning process, preventing the model from converging to an optimal solution. By applying the PVF concept during training, we could quantify the probability that a corruption in each parameter would result in such a convergence failure. A key distinction between training and inference is that during training, the parameters are dynamically updated, making the PVF potentially a function of time as well. We reserve this aspect for future exploration and investigation.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this research, we propose a novel quantitative metric, the Parameter Vulnerability Factor (PVF), designed to quantify the vulnerability of AI models to parameter corruptions. Through fault injection, PVF can be calculated for any target parameter component of a given AI model. In this paper, we present three case studies where we apply PVF to recommendation (DLRM), vision classification (CNN), and text classification (BERT). The PVF measurements allow us to examine the vulnerability levels of different parameter components within a specific model. The introduction of the PVF metric provides crucial insights for AI hardware designers, assisting them in balancing fault protection with performance and efficiency. For instance, it can guide the assignment of vulnerable AI parameter components to highly protected hardware modules. The PVF metric is versatile and can be applied to any AI model. It has the potential to serve as a unifying and standardizing tool for evaluating AI vulnerability and resilience.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Criteo kaggle display advertising dataset: https://ailab.criteo.com/ressources.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
tesla-release-notes: https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-535-129-03/index.html.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Udit Kumar Agarwal, Abraham Chan, and Karthik Pattabiraman.

</span>
<span class="ltx_bibblock">Resilience assessment of large language models under transient hardware faults.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE)</span>, pages 659–670. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chun-Kai Chang, Sangkug Lym, Nicholas Kelly, Michael B Sullivan, and Mattan Erez.

</span>
<span class="ltx_bibblock">Evaluating and accelerating high-fidelity error injection for hpc.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">SC18: International Conference for High Performance Computing, Networking, Storage and Analysis</span>, pages 577–589. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Harish Dattatraya Dixit et al.

</span>
<span class="ltx_bibblock">Detecting silent data corruptions in the wild.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2203.08989</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Peter Hazucha et al.

</span>
<span class="ltx_bibblock">Neutron soft error rate measurements in a 90-nm cmos process and scaling trends in sram from 0.25-/spl mu/m to 90-nm generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">IEDM</span>, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yi He, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, and Yanjing Li.

</span>
<span class="ltx_bibblock">Understanding and mitigating hardware failures in deep learning training systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 50th Annual International Symposium on Computer Architecture</span>, pages 1–16, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Zhezhi He, Adnan Siraj Rakin, and Deliang Fan.

</span>
<span class="ltx_bibblock">Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 588–597, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Peter H Hochschild, Paul Turner, Jeffrey C Mogul, Rama Govindaraju, Parthasarathy Ranganathan, David E Culler, and Amin Vahdat.

</span>
<span class="ltx_bibblock">Cores that don’t count.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Workshop on Hot Topics in Operating Systems</span>, pages 9–16, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Samuel Hsia et al.

</span>
<span class="ltx_bibblock">Mp-rec: Hardware-software co-design to enable multi-path recommendation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">ASPLOS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Sung Kim et al.

</span>
<span class="ltx_bibblock">Matic: Learning around errors for efficient low-voltage neural network accelerators.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">DATE</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel.

</span>
<span class="ltx_bibblock">Handwritten digit recognition with a back-propagation network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Advances in neural information processing systems</span>, 2, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE</span>, 86(11):2278–2324, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Guanpeng Li et al.

</span>
<span class="ltx_bibblock">Understanding error propagation in deep learning neural network (dnn) accelerators and applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">SC</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Abdulrahman Mahmoud et al.

</span>
<span class="ltx_bibblock">Pytorchfi: A runtime perturbation tool for dnns.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">DSN-W</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shubhendu S Mukherjee, Christopher Weaver, Joel Emer, Steven K Reinhardt, and Todd Austin.

</span>
<span class="ltx_bibblock">A systematic methodology to compute the architectural vulnerability factors for a high-performance microprocessor.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings. 36th Annual IEEE/ACM International Symposium on Microarchitecture, 2003. MICRO-36.</span>, pages 29–40. IEEE, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for personalization and recommendation systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1906.00091</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Adnan Siraj Rakin, Zhezhi He, and Deliang Fan.

</span>
<span class="ltx_bibblock">Bit-flip attack: Crushing neural network with progressive bit search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 1211–1220, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Brandon Reagen et al.

</span>
<span class="ltx_bibblock">Ares: A framework for quantifying the resilience of deep neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">DAC</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Behrooz Sangchoolie, Karthik Pattabiraman, and Johan Karlsson.

</span>
<span class="ltx_bibblock">One bit is (not) enough: An empirical study of the impact of single and multiple bit-flip errors.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">2017 47th annual IEEE/IFIP international conference on dependable systems and networks (DSN)</span>, pages 97–108. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li.

</span>
<span class="ltx_bibblock">Exploring the vulnerability of deep neural networks: A study of parameter corruption.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 35, pages 11648–11656, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Shaobu Wang, Guangyan Zhang, Junyu Wei, Yang Wang, Jiesheng Wu, and Qingchao Luo.

</span>
<span class="ltx_bibblock">Understanding silent data corruptions in a large production cpu population.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</span>, pages 216–230, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Zhao, and Yann LeCun.

</span>
<span class="ltx_bibblock">Character-level convolutional networks for text classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 11 22:35:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
