<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1809.07857] In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning</title><meta property="og:description" content="Recently, along with the rapid development of mobile communication technology,
edge computing theory and techniques have been attracting more and more
attentions from global researchers and engineers,
which can signifiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1809.07857">

<!--Generated on Sat Mar 16 08:03:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Mobile Edge Computing,  Artificial Intelligence,  Deep Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiaofei Wang<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup>,
Yiwen Han<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">1</span></sup>,
Chenyang Wang<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">1</span></sup>,
Qiyang Zhao<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">2</span></sup>,
Xu Chen<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">3</span></sup>,
Min Chen<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">4âˆ—</span></sup>

<br class="ltx_break"><sup id="id17.17.id7" class="ltx_sup"><span id="id17.17.id7.1" class="ltx_text" style="font-size:70%;">1</span></sup><span id="id10.10.3" class="ltx_text" style="font-size:70%;">Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology,
Tianjin University, Tianjin, China.
<br class="ltx_break"><sup id="id10.10.3.1" class="ltx_sup">2</sup>Huawei Technology, Shenzhen, P. R. China.
<br class="ltx_break"><sup id="id10.10.3.2" class="ltx_sup">3</sup>School of Data and Computer Science, Sun Yat-sen University, Guangdong, Guangzhou, P. R. China.
<br class="ltx_break"><sup id="id10.10.3.3" class="ltx_sup">4</sup>School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, P. R. China.
<br class="ltx_break">*Corresponding author.

</span>
</span><span class="ltx_author_notes"><span id="id18.18.id1" class="ltx_text" style="font-size:70%;">@ 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">Recently, along with the rapid development of mobile communication technology,
edge computing theory and techniques have been attracting more and more
attentions from global researchers and engineers,
which can significantly bridge the capacity of cloud and requirement of devices by the network edges,
and thus can accelerate the content deliveries and improve the quality of mobile services.
In order to bring more intelligence to the edge systems, compared to traditional optimization methodology,
and driven by the current deep learning techniques,
we propose to integrate the Deep Reinforcement Learning techniques
and Federated Learning framework with the mobile edge systems,
for optimizing the mobile edge computing, caching and communication.
And thus, we design the â€œIn-Edge AIâ€ framework in order to intelligently utilize
the collaboration among devices and edge nodes
to exchange the learning parameters for a better training and inference of the models,
and thus to carry out dynamic system-level optimization and application-level enhancement
while reducing the unnecessary system communication load.
â€œIn-Edge AIâ€ is evaluated and proved to have near-optimal performance
but relatively low overhead of learning,
while the system is cognitive and adaptive to the mobile communication systems.
Finally, we discuss several related challenges and opportunities
for unveiling a promising upcoming future of â€œIn-Edge AIâ€.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Mobile Edge Computing, Artificial Intelligence, Deep Learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the increasing quantity and quality of rich multimedia services over mobile networks, there has been a huge increase in the traffic and computation for mobile users and devices
over recent years, which imposes a huge amount of workload
on todayâ€™s already-congested backbone networks and the mobile networks.
</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Naturally, the emerging idea of Mobile Edge Computing (MEC) is proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
as a novel paradigm for easing the burden of backbone networks by pushing the computation/storage resources
to the proximity of the User Equipments (UEs). On the other hand,
MEC circumvents the long propagation delays introduced by transmitting
data from mobile devices to remote cloud computing infrastructures,
and hence is able to support latency-critical mobile and Internet of Things (IoT) applications.
Specifically, edge nodes, i.e., base stations equipped with computation/storage capability,
could deal with the computation and content requests of UEs,
and consequently this scheme improves the Quality-of-Service (QoS) of Mobile Network Operators (MNOs) and the Quality-of-Experience (QoE) of UEs and relieves the load of backbone networks,
and pressure of clouds (data centers) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Fulfilling the requirement of QoE of UEs is not a trivial even by virtue of MEC.
The key and difficult point lies in that the computation offloading requires wireless data transmission
and might bring about the congestion of wireless channels,
and hence raises the decision making or optimization problem on the whole communication
and computation integrated system, i.e., how to jointly allocate communication resources
and computation resources of edge nodes.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Several pioneer works have been proposed and realize quite good results in their assuming settings
based on convex optimization, game theory and so on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> .
Nevertheless, considering the particular use cases in MEC,
these optimization methods may suffer from the following issues:
<em id="S1.p4.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">1) Uncertain Inputs:</em> they assume the that some key information factors are given as inputs,
but actually some of them are difficult to obtain due to variant wireless channels and privacy policies;
<em id="S1.p4.1.2" class="ltx_emph ltx_font_bold ltx_font_italic">2) Dynamic Conditions:</em> dynamics of the integrated communication
and computation system are not well addressed;
<em id="S1.p4.1.3" class="ltx_emph ltx_font_bold ltx_font_italic">3) Temporal Isolation:</em> most of them do not consider the long-term effect of
current decision on resource allocation except for Lyapunov optimization,
viz., in a highly time-varying MEC system, most of proposed optimization algorithms is
optimal or close-to-optimal only for a snapshot of the system.
In a word, the problem existed in the resource allocation optimization of the MEC system
is â€œ<span id="S1.p4.1.4" class="ltx_text ltx_font_bold">lack of intelligence</span>â€.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In view of the increasing complexity of mobile networks, e.g., a typical 5G node is expected to have 2000 or more configurable parameters, a recent new trend is to optimize wireless communication by Artificial Intelligence (AI) techniques
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
include but not limited to the application of AI for Physical Layer (PHY),
Data Link Layer, and traffic control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Particularly, related studies on edge computing and caching,
such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>,
have shown that reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
(include Deep Q-Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in Deep Reinforcement Learning)
has the potential to be effective in joint resource management.
But <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is based on Q-Learning, which is not feasible for the practical MEC system
where the state-action space is tremendous and the focus of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is on vehicular networks.
Besides, none of these works has thought over
1) in what form the training data shall be gathered
(whether in a distributed or a centralized way),
2) where the reinforcement learning agent should be placed and trained
(whether in UEs, edge nodes or remote cloud infrastructures),
3) how the update process of reinforcement learning agents should be proceeded and collaborated,
and
4) the privacy protection of training data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Therefore, in this article, we use Deep Reinforcement Learning (DRL) to jointly manage the communication and computation resources. And both cases of computation offloading and edge caching among the MEC system (illustrated in Fig. <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is discussed.
In addition, Federated Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is introduced as a framework for
training DRL agents in a distributed manner while
1) largely reducing the amount of data which should be uploaded via the wireless uplink channel,
2) reacting cognitively to the mobile communication environment and conditions of cellular networks,
3) adapting well with heterogeneous UEs in a practical cellular network,
and 4) preserving the personal data privacy,</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1809.07857/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="545" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework of AI-supported mobile edge system with cognitive ability</figcaption>
</figure>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To the best of our knowledge, we are the first group to
study the application of DRL coupled
with Federated Learning for intelligent joint resource management
of communication and computation in MEC systems.
Our contribution can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">1) we discuss the methodology of utilizing DRL (specifically, Deep Q-Learning), and Distributed DRL for optimizing the Edge Caching and Computation;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">2) we propose â€œIn-Edge AIâ€ framework to further
utilize the â€œFederated Learningâ€ for a better deployment
of intelligent resource management in the MEC system;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">3) we provide proof-of-concept evaluation and verify that
the proposed scheme has advantages on the balance of performance and cost.</p>
</div>
</li>
</ul>
<p id="S1.p7.2" class="ltx_p">In addition, we also discuss opportunities and challenges
to hopefully unveil the upcoming future of edge architecture supporting various AI-based applications.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Optimizing the Edge by DRL</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We intend to use AI techniques (particularly DRL) as the method of cognitive computing for building an intelligentizing mobile edge computing, caching and communication system. The cognitive process among protocol stacks of wireless communication is given as Fig. <a href="#S2.F2" title="Figure 2 â€£ II-A DRL over the MEC System for Caching â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where we partition the whole process into three main parts.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Information Collecting:</span> Sense and collect the indispensable observing data for cognitive computing among the MEC system, including but not limit on the usage of communication and computation resources, wireless environments and intensities of UEsâ€™ requests;</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Cognitive Computing:</span> By making use of the observing data of the system, cognitive computing is performed to fuse the massive observed data and further give the decision of scheduling;</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Request Handling:</span> The MEC system deals with the request of UEs on the basis of the scheduling decision given by cognitive computing.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this section, two representative use cases in the MEC system are investigated.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">DRL over the MEC System for Caching</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recently, we have observed the emergence of promising mobile content caching and delivery techniques, by which popular contents are cached in the intermediate servers (or middleboxes, gateways or routers) so that demands from users for the same content can be accommodated easily without duplicate transmissions from remote cloud servers, and hence significantly reducing redundant traffic.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.3" class="ltx_p">Thereinafter, we focus on the scenario of caching contents in edge nodes. In the MEC system depicted by Fig. <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, there is a library of <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">F</annotation></semantics></math> popular content files, denoted as <math id="S2.SS1.p2.2.m2.3" class="ltx_Math" alttext="\mathcal{F}=\{1,...,F\}" display="inline"><semantics id="S2.SS1.p2.2.m2.3a"><mrow id="S2.SS1.p2.2.m2.3.4" xref="S2.SS1.p2.2.m2.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.2.m2.3.4.2" xref="S2.SS1.p2.2.m2.3.4.2.cmml">â„±</mi><mo id="S2.SS1.p2.2.m2.3.4.1" xref="S2.SS1.p2.2.m2.3.4.1.cmml">=</mo><mrow id="S2.SS1.p2.2.m2.3.4.3.2" xref="S2.SS1.p2.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.3.4.3.2.1" xref="S2.SS1.p2.2.m2.3.4.3.1.cmml">{</mo><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S2.SS1.p2.2.m2.3.4.3.2.2" xref="S2.SS1.p2.2.m2.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p2.2.m2.2.2" xref="S2.SS1.p2.2.m2.2.2.cmml">â€¦</mi><mo id="S2.SS1.p2.2.m2.3.4.3.2.3" xref="S2.SS1.p2.2.m2.3.4.3.1.cmml">,</mo><mi id="S2.SS1.p2.2.m2.3.3" xref="S2.SS1.p2.2.m2.3.3.cmml">F</mi><mo stretchy="false" id="S2.SS1.p2.2.m2.3.4.3.2.4" xref="S2.SS1.p2.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.3b"><apply id="S2.SS1.p2.2.m2.3.4.cmml" xref="S2.SS1.p2.2.m2.3.4"><eq id="S2.SS1.p2.2.m2.3.4.1.cmml" xref="S2.SS1.p2.2.m2.3.4.1"></eq><ci id="S2.SS1.p2.2.m2.3.4.2.cmml" xref="S2.SS1.p2.2.m2.3.4.2">â„±</ci><set id="S2.SS1.p2.2.m2.3.4.3.1.cmml" xref="S2.SS1.p2.2.m2.3.4.3.2"><cn type="integer" id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">1</cn><ci id="S2.SS1.p2.2.m2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2">â€¦</ci><ci id="S2.SS1.p2.2.m2.3.3.cmml" xref="S2.SS1.p2.2.m2.3.3">ğ¹</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.3c">\mathcal{F}=\{1,...,F\}</annotation></semantics></math>, that all mobile users may request in the system. The content popularity is defined as <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="(P_{\mathrm{f}})_{F\times 1}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mrow id="S2.SS1.p2.3.m3.1.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.1.2" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p2.3.m3.1.1.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.1.1.1.2" xref="S2.SS1.p2.3.m3.1.1.1.1.1.2.cmml">P</mi><mi mathvariant="normal" id="S2.SS1.p2.3.m3.1.1.1.1.1.3" xref="S2.SS1.p2.3.m3.1.1.1.1.1.3.cmml">f</mi></msub><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.1.3" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3.2" xref="S2.SS1.p2.3.m3.1.1.3.2.cmml">F</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.3.m3.1.1.3.1" xref="S2.SS1.p2.3.m3.1.1.3.1.cmml">Ã—</mo><mn id="S2.SS1.p2.3.m3.1.1.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1">subscript</csymbol><apply id="S2.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S2.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1.1.3">f</ci></apply><apply id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><times id="S2.SS1.p2.3.m3.1.1.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3.1"></times><ci id="S2.SS1.p2.3.m3.1.1.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.2">ğ¹</ci><cn type="integer" id="S2.SS1.p2.3.m3.1.1.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">(P_{\mathrm{f}})_{F\times 1}</annotation></semantics></math>, which is the probability distribution of content requests from all users. Content popularity indicates the common interests of all users in the network. In related works, the content popularity is always described by MZipf distribution. Moreover, for simply asserting the efficiency of DRL in edge caching, we assume that the content popularity changes slowly and all contents have the same size. For each request, the DRL agent in the edge node can make a decision to cache or not cache, and if yes, the agent determines which local content shall be replaced. We assume that all content popularity, user preference and average arrival rate of requests are static during a relatively long period. We model the cache replacement problem in all edge nodes as a Markov Decision Process (MDP) and use DRL to solve it, and the results will be shown in Section <a href="#S4" title="IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/1809.07857/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="454" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Procedure of utilizing cognitive computing in mobile edge system among protocol stacks</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">DRL over the MEC System for Computation Offloading</span>
</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Communication Model</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.8" class="ltx_p">As illustrated in Fig. <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, considering an environment where a set of UEs <math id="S2.SS2.SSS1.p1.1.m1.3" class="ltx_Math" alttext="\mathcal{N}=\{1,...,N\}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.3a"><mrow id="S2.SS2.SSS1.p1.1.m1.3.4" xref="S2.SS2.SSS1.p1.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.1.m1.3.4.2" xref="S2.SS2.SSS1.p1.1.m1.3.4.2.cmml">ğ’©</mi><mo id="S2.SS2.SSS1.p1.1.m1.3.4.1" xref="S2.SS2.SSS1.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.1.m1.3.4.3.2" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.1.m1.3.4.3.2.1" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml">{</mo><mn id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">1</mn><mo id="S2.SS2.SSS1.p1.1.m1.3.4.3.2.2" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.1.m1.2.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.1.m1.3.4.3.2.3" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml">,</mo><mi id="S2.SS2.SSS1.p1.1.m1.3.3" xref="S2.SS2.SSS1.p1.1.m1.3.3.cmml">N</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.1.m1.3.4.3.2.4" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.3b"><apply id="S2.SS2.SSS1.p1.1.m1.3.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.4"><eq id="S2.SS2.SSS1.p1.1.m1.3.4.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.4.1"></eq><ci id="S2.SS2.SSS1.p1.1.m1.3.4.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.4.2">ğ’©</ci><set id="S2.SS2.SSS1.p1.1.m1.3.4.3.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.4.3.2"><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">1</cn><ci id="S2.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2">â€¦</ci><ci id="S2.SS2.SSS1.p1.1.m1.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3">ğ‘</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.3c">\mathcal{N}=\{1,...,N\}</annotation></semantics></math> is covered by a set of base stations <math id="S2.SS2.SSS1.p1.2.m2.3" class="ltx_Math" alttext="\mathcal{B}=\{1,...,B\}" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.3a"><mrow id="S2.SS2.SSS1.p1.2.m2.3.4" xref="S2.SS2.SSS1.p1.2.m2.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.2.m2.3.4.2" xref="S2.SS2.SSS1.p1.2.m2.3.4.2.cmml">â„¬</mi><mo id="S2.SS2.SSS1.p1.2.m2.3.4.1" xref="S2.SS2.SSS1.p1.2.m2.3.4.1.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.2.m2.3.4.3.2" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.2.m2.3.4.3.2.1" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">1</mn><mo id="S2.SS2.SSS1.p1.2.m2.3.4.3.2.2" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.2.m2.2.2" xref="S2.SS2.SSS1.p1.2.m2.2.2.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.2.m2.3.4.3.2.3" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S2.SS2.SSS1.p1.2.m2.3.3" xref="S2.SS2.SSS1.p1.2.m2.3.3.cmml">B</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.2.m2.3.4.3.2.4" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.3b"><apply id="S2.SS2.SSS1.p1.2.m2.3.4.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.4"><eq id="S2.SS2.SSS1.p1.2.m2.3.4.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.4.1"></eq><ci id="S2.SS2.SSS1.p1.2.m2.3.4.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.4.2">â„¬</ci><set id="S2.SS2.SSS1.p1.2.m2.3.4.3.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.4.3.2"><cn type="integer" id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">1</cn><ci id="S2.SS2.SSS1.p1.2.m2.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.2.2">â€¦</ci><ci id="S2.SS2.SSS1.p1.2.m2.3.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.3">ğµ</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.3c">\mathcal{B}=\{1,...,B\}</annotation></semantics></math>, UEs could choose to offload their intensive computation tasks to an edge node via the wireless channel or execute these tasks locally. There are <math id="S2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">M</annotation></semantics></math> wireless channels and the set <math id="S2.SS2.SSS1.p1.4.m4.3" class="ltx_Math" alttext="\mathcal{M}=\{1,...,M\}" display="inline"><semantics id="S2.SS2.SSS1.p1.4.m4.3a"><mrow id="S2.SS2.SSS1.p1.4.m4.3.4" xref="S2.SS2.SSS1.p1.4.m4.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.4.m4.3.4.2" xref="S2.SS2.SSS1.p1.4.m4.3.4.2.cmml">â„³</mi><mo id="S2.SS2.SSS1.p1.4.m4.3.4.1" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.4.m4.3.4.3.2" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.3.4.3.2.1" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml">{</mo><mn id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">1</mn><mo id="S2.SS2.SSS1.p1.4.m4.3.4.3.2.2" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.4.m4.2.2" xref="S2.SS2.SSS1.p1.4.m4.2.2.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.4.m4.3.4.3.2.3" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.SS2.SSS1.p1.4.m4.3.3" xref="S2.SS2.SSS1.p1.4.m4.3.3.cmml">M</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.3.4.3.2.4" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.3b"><apply id="S2.SS2.SSS1.p1.4.m4.3.4.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.4"><eq id="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.4.1"></eq><ci id="S2.SS2.SSS1.p1.4.m4.3.4.2.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.4.2">â„³</ci><set id="S2.SS2.SSS1.p1.4.m4.3.4.3.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.4.3.2"><cn type="integer" id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.1">1</cn><ci id="S2.SS2.SSS1.p1.4.m4.2.2.cmml" xref="S2.SS2.SSS1.p1.4.m4.2.2">â€¦</ci><ci id="S2.SS2.SSS1.p1.4.m4.3.3.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.3">ğ‘€</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.3c">\mathcal{M}=\{1,...,M\}</annotation></semantics></math> denotes channels of one base station. Specifically, among the decision choice <math id="S2.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="a_{n}\in\{0\}\bigcup\mathcal{M}" display="inline"><semantics id="S2.SS2.SSS1.p1.5.m5.1a"><mrow id="S2.SS2.SSS1.p1.5.m5.1.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.cmml"><msub id="S2.SS2.SSS1.p1.5.m5.1.2.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.1.2.2.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.2.cmml">a</mi><mi id="S2.SS2.SSS1.p1.5.m5.1.2.2.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.3.cmml">n</mi></msub><mo id="S2.SS2.SSS1.p1.5.m5.1.2.1" xref="S2.SS2.SSS1.p1.5.m5.1.2.1.cmml">âˆˆ</mo><mrow id="S2.SS2.SSS1.p1.5.m5.1.2.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.cmml"><mrow id="S2.SS2.SSS1.p1.5.m5.1.2.3.2.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.1.2.3.2.2.1" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2.1.cmml">{</mo><mn id="S2.SS2.SSS1.p1.5.m5.1.1" xref="S2.SS2.SSS1.p1.5.m5.1.1.cmml">0</mn><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.1.2.3.2.2.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2.1.cmml">}</mo></mrow><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.5.m5.1.2.3.1" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS1.p1.5.m5.1.2.3.3" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.cmml"><mo id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.1" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.1.cmml">â‹ƒ</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.2" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.2.cmml">â„³</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.5.m5.1b"><apply id="S2.SS2.SSS1.p1.5.m5.1.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2"><in id="S2.SS2.SSS1.p1.5.m5.1.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.1"></in><apply id="S2.SS2.SSS1.p1.5.m5.1.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.2">ğ‘</ci><ci id="S2.SS2.SSS1.p1.5.m5.1.2.2.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.2.3">ğ‘›</ci></apply><apply id="S2.SS2.SSS1.p1.5.m5.1.2.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3"><times id="S2.SS2.SSS1.p1.5.m5.1.2.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.1"></times><set id="S2.SS2.SSS1.p1.5.m5.1.2.3.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.2.2"><cn type="integer" id="S2.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.1">0</cn></set><apply id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3"><union id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.1"></union><ci id="S2.SS2.SSS1.p1.5.m5.1.2.3.3.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.2.3.3.2">â„³</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.5.m5.1c">a_{n}\in\{0\}\bigcup\mathcal{M}</annotation></semantics></math>, UE <math id="S2.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.SSS1.p1.6.m6.1a"><mi id="S2.SS2.SSS1.p1.6.m6.1.1" xref="S2.SS2.SSS1.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.6.m6.1b"><ci id="S2.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.6.m6.1c">n</annotation></semantics></math> could choose to offload the computation to the edge via a wireless channel <math id="S2.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="a_{n}" display="inline"><semantics id="S2.SS2.SSS1.p1.7.m7.1a"><msub id="S2.SS2.SSS1.p1.7.m7.1.1" xref="S2.SS2.SSS1.p1.7.m7.1.1.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.1.1.2" xref="S2.SS2.SSS1.p1.7.m7.1.1.2.cmml">a</mi><mi id="S2.SS2.SSS1.p1.7.m7.1.1.3" xref="S2.SS2.SSS1.p1.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.7.m7.1b"><apply id="S2.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.1.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.1.1.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1.2">ğ‘</ci><ci id="S2.SS2.SSS1.p1.7.m7.1.1.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.7.m7.1c">a_{n}</annotation></semantics></math> or compute its tasks locally as a decision <math id="S2.SS2.SSS1.p1.8.m8.1" class="ltx_Math" alttext="a_{n}=0" display="inline"><semantics id="S2.SS2.SSS1.p1.8.m8.1a"><mrow id="S2.SS2.SSS1.p1.8.m8.1.1" xref="S2.SS2.SSS1.p1.8.m8.1.1.cmml"><msub id="S2.SS2.SSS1.p1.8.m8.1.1.2" xref="S2.SS2.SSS1.p1.8.m8.1.1.2.cmml"><mi id="S2.SS2.SSS1.p1.8.m8.1.1.2.2" xref="S2.SS2.SSS1.p1.8.m8.1.1.2.2.cmml">a</mi><mi id="S2.SS2.SSS1.p1.8.m8.1.1.2.3" xref="S2.SS2.SSS1.p1.8.m8.1.1.2.3.cmml">n</mi></msub><mo id="S2.SS2.SSS1.p1.8.m8.1.1.1" xref="S2.SS2.SSS1.p1.8.m8.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS1.p1.8.m8.1.1.3" xref="S2.SS2.SSS1.p1.8.m8.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.8.m8.1b"><apply id="S2.SS2.SSS1.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1"><eq id="S2.SS2.SSS1.p1.8.m8.1.1.1.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.1"></eq><apply id="S2.SS2.SSS1.p1.8.m8.1.1.2.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.8.m8.1.1.2.1.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.8.m8.1.1.2.2.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.2.2">ğ‘</ci><ci id="S2.SS2.SSS1.p1.8.m8.1.1.2.3.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.2.3">ğ‘›</ci></apply><cn type="integer" id="S2.SS2.SSS1.p1.8.m8.1.1.3.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.8.m8.1c">a_{n}=0</annotation></semantics></math>. For the purpose of simulating the variation of wireless channels, the channel gain state between a UE and a base station (belongs to an edge node) is independently picked from a finite state space, by which the channel state transitions are modelled as a finite-state discrete-time Markov chain. In this wireless scenario, the achievable data rate could be evaluated by Shannon-Hartley theorem.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Computation Model</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.8" class="ltx_p">In order to portrait the long-term effects within the MEC system, computation tasks are generated according to Bernoulli distribution across the time horizon. A computation task is represented by <math id="S2.SS2.SSS2.p1.1.m1.2" class="ltx_Math" alttext="(\mu,\nu)" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.2a"><mrow id="S2.SS2.SSS2.p1.1.m1.2.3.2" xref="S2.SS2.SSS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.3.2.1" xref="S2.SS2.SSS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">Î¼</mi><mo id="S2.SS2.SSS2.p1.1.m1.2.3.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS2.SSS2.p1.1.m1.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.cmml">Î½</mi><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.3.2.3" xref="S2.SS2.SSS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.2b"><interval closure="open" id="S2.SS2.SSS2.p1.1.m1.2.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.3.2"><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">ğœ‡</ci><ci id="S2.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2">ğœˆ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.2c">(\mu,\nu)</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S2.SS2.SSS2.p1.2.m2.1a"><mi id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.1b"><ci id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.1c">\mu</annotation></semantics></math>, <math id="S2.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\nu" display="inline"><semantics id="S2.SS2.SSS2.p1.3.m3.1a"><mi id="S2.SS2.SSS2.p1.3.m3.1.1" xref="S2.SS2.SSS2.p1.3.m3.1.1.cmml">Î½</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.3.m3.1b"><ci id="S2.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1">ğœˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.3.m3.1c">\nu</annotation></semantics></math> denote the size of computation input data (in bits) and the total number of CPU cycles needed to complete the computation task, respectively. All these generated tasks are stored in a task queue and executed sequentially on the UE or edge node according to a FIFO (First-in First-out) principle. When the task is executed locally, the computation execution time of it is given as <math id="S2.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="d_{L}=\nu/f_{L}" display="inline"><semantics id="S2.SS2.SSS2.p1.4.m4.1a"><mrow id="S2.SS2.SSS2.p1.4.m4.1.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.cmml"><msub id="S2.SS2.SSS2.p1.4.m4.1.1.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.cmml"><mi id="S2.SS2.SSS2.p1.4.m4.1.1.2.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.2.cmml">d</mi><mi id="S2.SS2.SSS2.p1.4.m4.1.1.2.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.3.cmml">L</mi></msub><mo id="S2.SS2.SSS2.p1.4.m4.1.1.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S2.SS2.SSS2.p1.4.m4.1.1.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.4.m4.1.1.3.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.2.cmml">Î½</mi><mo id="S2.SS2.SSS2.p1.4.m4.1.1.3.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.1.cmml">/</mo><msub id="S2.SS2.SSS2.p1.4.m4.1.1.3.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3.cmml"><mi id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3.3.cmml">L</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.4.m4.1b"><apply id="S2.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1"><eq id="S2.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1"></eq><apply id="S2.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.4.m4.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.4.m4.1.1.2.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.2">ğ‘‘</ci><ci id="S2.SS2.SSS2.p1.4.m4.1.1.2.3.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.3">ğ¿</ci></apply><apply id="S2.SS2.SSS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3"><divide id="S2.SS2.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.1"></divide><ci id="S2.SS2.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.2">ğœˆ</ci><apply id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.4.m4.1.1.3.3.3.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.3.3.3">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.4.m4.1c">d_{L}=\nu/f_{L}</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="f_{L}" display="inline"><semantics id="S2.SS2.SSS2.p1.5.m5.1a"><msub id="S2.SS2.SSS2.p1.5.m5.1.1" xref="S2.SS2.SSS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.SSS2.p1.5.m5.1.1.2" xref="S2.SS2.SSS2.p1.5.m5.1.1.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.5.m5.1.1.3" xref="S2.SS2.SSS2.p1.5.m5.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.5.m5.1b"><apply id="S2.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.5.m5.1c">f_{L}</annotation></semantics></math> is the the computation capability (i.e., CPU cycles per second) of the UE and determined by the amount of energy the UE decides to allocate. And when the task is scheduled to be executed on the edge node, the execution time of this offloaded task can be calculated as <math id="S2.SS2.SSS2.p1.6.m6.1" class="ltx_Math" alttext="d_{E}=\nu/f_{E}" display="inline"><semantics id="S2.SS2.SSS2.p1.6.m6.1a"><mrow id="S2.SS2.SSS2.p1.6.m6.1.1" xref="S2.SS2.SSS2.p1.6.m6.1.1.cmml"><msub id="S2.SS2.SSS2.p1.6.m6.1.1.2" xref="S2.SS2.SSS2.p1.6.m6.1.1.2.cmml"><mi id="S2.SS2.SSS2.p1.6.m6.1.1.2.2" xref="S2.SS2.SSS2.p1.6.m6.1.1.2.2.cmml">d</mi><mi id="S2.SS2.SSS2.p1.6.m6.1.1.2.3" xref="S2.SS2.SSS2.p1.6.m6.1.1.2.3.cmml">E</mi></msub><mo id="S2.SS2.SSS2.p1.6.m6.1.1.1" xref="S2.SS2.SSS2.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S2.SS2.SSS2.p1.6.m6.1.1.3" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.6.m6.1.1.3.2" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.2.cmml">Î½</mi><mo id="S2.SS2.SSS2.p1.6.m6.1.1.3.1" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.1.cmml">/</mo><msub id="S2.SS2.SSS2.p1.6.m6.1.1.3.3" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3.cmml"><mi id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.2" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.3" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3.3.cmml">E</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.6.m6.1b"><apply id="S2.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1"><eq id="S2.SS2.SSS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.1"></eq><apply id="S2.SS2.SSS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.6.m6.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.6.m6.1.1.2.2.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.2.2">ğ‘‘</ci><ci id="S2.SS2.SSS2.p1.6.m6.1.1.2.3.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.2.3">ğ¸</ci></apply><apply id="S2.SS2.SSS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3"><divide id="S2.SS2.SSS2.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.1"></divide><ci id="S2.SS2.SSS2.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.2">ğœˆ</ci><apply id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.1.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.2.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.6.m6.1.1.3.3.3.cmml" xref="S2.SS2.SSS2.p1.6.m6.1.1.3.3.3">ğ¸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.6.m6.1c">d_{E}=\nu/f_{E}</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p1.7.m7.1" class="ltx_Math" alttext="f_{E}" display="inline"><semantics id="S2.SS2.SSS2.p1.7.m7.1a"><msub id="S2.SS2.SSS2.p1.7.m7.1.1" xref="S2.SS2.SSS2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.SSS2.p1.7.m7.1.1.2" xref="S2.SS2.SSS2.p1.7.m7.1.1.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.7.m7.1.1.3" xref="S2.SS2.SSS2.p1.7.m7.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.7.m7.1b"><apply id="S2.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.SSS2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.SSS2.p1.7.m7.1.1.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.SSS2.p1.7.m7.1.1.3">ğ¸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.7.m7.1c">f_{E}</annotation></semantics></math> is the computation capability the edge node allocated to the UE and the formula <math id="S2.SS2.SSS2.p1.8.m8.1" class="ltx_Math" alttext="f_{E}\gg f_{L}" display="inline"><semantics id="S2.SS2.SSS2.p1.8.m8.1a"><mrow id="S2.SS2.SSS2.p1.8.m8.1.1" xref="S2.SS2.SSS2.p1.8.m8.1.1.cmml"><msub id="S2.SS2.SSS2.p1.8.m8.1.1.2" xref="S2.SS2.SSS2.p1.8.m8.1.1.2.cmml"><mi id="S2.SS2.SSS2.p1.8.m8.1.1.2.2" xref="S2.SS2.SSS2.p1.8.m8.1.1.2.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.8.m8.1.1.2.3" xref="S2.SS2.SSS2.p1.8.m8.1.1.2.3.cmml">E</mi></msub><mo id="S2.SS2.SSS2.p1.8.m8.1.1.1" xref="S2.SS2.SSS2.p1.8.m8.1.1.1.cmml">â‰«</mo><msub id="S2.SS2.SSS2.p1.8.m8.1.1.3" xref="S2.SS2.SSS2.p1.8.m8.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.8.m8.1.1.3.2" xref="S2.SS2.SSS2.p1.8.m8.1.1.3.2.cmml">f</mi><mi id="S2.SS2.SSS2.p1.8.m8.1.1.3.3" xref="S2.SS2.SSS2.p1.8.m8.1.1.3.3.cmml">L</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.8.m8.1b"><apply id="S2.SS2.SSS2.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1"><csymbol cd="latexml" id="S2.SS2.SSS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.1">much-greater-than</csymbol><apply id="S2.SS2.SSS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.8.m8.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.8.m8.1.1.2.2.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.2.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.8.m8.1.1.2.3.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.2.3">ğ¸</ci></apply><apply id="S2.SS2.SSS2.p1.8.m8.1.1.3.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.8.m8.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.8.m8.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.3.2">ğ‘“</ci><ci id="S2.SS2.SSS2.p1.8.m8.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.8.m8.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.8.m8.1c">f_{E}\gg f_{L}</annotation></semantics></math> holds (the computing performance of edge nodes is much stronger than the UEâ€™s).</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Problem Formulation for Computation Offloading</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.4" class="ltx_p">For efficiently performing computation offloading over the MEC system, the UE shall make a joint communication and computation resource allocation decision in terms of a control action <math id="S2.SS2.SSS3.p1.1.m1.2" class="ltx_Math" alttext="(c,e)" display="inline"><semantics id="S2.SS2.SSS3.p1.1.m1.2a"><mrow id="S2.SS2.SSS3.p1.1.m1.2.3.2" xref="S2.SS2.SSS3.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p1.1.m1.2.3.2.1" xref="S2.SS2.SSS3.p1.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml">c</mi><mo id="S2.SS2.SSS3.p1.1.m1.2.3.2.2" xref="S2.SS2.SSS3.p1.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS2.SSS3.p1.1.m1.2.2" xref="S2.SS2.SSS3.p1.1.m1.2.2.cmml">e</mi><mo stretchy="false" id="S2.SS2.SSS3.p1.1.m1.2.3.2.3" xref="S2.SS2.SSS3.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.2b"><interval closure="open" id="S2.SS2.SSS3.p1.1.m1.2.3.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.2.3.2"><ci id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1">ğ‘</ci><ci id="S2.SS2.SSS3.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS3.p1.1.m1.2.2">ğ‘’</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.2c">(c,e)</annotation></semantics></math>, where <math id="S2.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="c\in\{0\}\bigcup\mathcal{M}" display="inline"><semantics id="S2.SS2.SSS3.p1.2.m2.1a"><mrow id="S2.SS2.SSS3.p1.2.m2.1.2" xref="S2.SS2.SSS3.p1.2.m2.1.2.cmml"><mi id="S2.SS2.SSS3.p1.2.m2.1.2.2" xref="S2.SS2.SSS3.p1.2.m2.1.2.2.cmml">c</mi><mo id="S2.SS2.SSS3.p1.2.m2.1.2.1" xref="S2.SS2.SSS3.p1.2.m2.1.2.1.cmml">âˆˆ</mo><mrow id="S2.SS2.SSS3.p1.2.m2.1.2.3" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.cmml"><mrow id="S2.SS2.SSS3.p1.2.m2.1.2.3.2.2" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p1.2.m2.1.2.3.2.2.1" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.2.1.cmml">{</mo><mn id="S2.SS2.SSS3.p1.2.m2.1.1" xref="S2.SS2.SSS3.p1.2.m2.1.1.cmml">0</mn><mo stretchy="false" id="S2.SS2.SSS3.p1.2.m2.1.2.3.2.2.2" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.2.1.cmml">}</mo></mrow><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p1.2.m2.1.2.3.1" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p1.2.m2.1.2.3.3" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3.cmml"><mo id="S2.SS2.SSS3.p1.2.m2.1.2.3.3.1" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3.1.cmml">â‹ƒ</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS3.p1.2.m2.1.2.3.3.2" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3.2.cmml">â„³</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.2.m2.1b"><apply id="S2.SS2.SSS3.p1.2.m2.1.2.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2"><in id="S2.SS2.SSS3.p1.2.m2.1.2.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.1"></in><ci id="S2.SS2.SSS3.p1.2.m2.1.2.2.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.2">ğ‘</ci><apply id="S2.SS2.SSS3.p1.2.m2.1.2.3.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3"><times id="S2.SS2.SSS3.p1.2.m2.1.2.3.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.1"></times><set id="S2.SS2.SSS3.p1.2.m2.1.2.3.2.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.2.2"><cn type="integer" id="S2.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1">0</cn></set><apply id="S2.SS2.SSS3.p1.2.m2.1.2.3.3.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3"><union id="S2.SS2.SSS3.p1.2.m2.1.2.3.3.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3.1"></union><ci id="S2.SS2.SSS3.p1.2.m2.1.2.3.3.2.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.2.3.3.2">â„³</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.2.m2.1c">c\in\{0\}\bigcup\mathcal{M}</annotation></semantics></math> is the computation offloading decision denoting the UE chooses to execute the task locally (<math id="S2.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="c=0" display="inline"><semantics id="S2.SS2.SSS3.p1.3.m3.1a"><mrow id="S2.SS2.SSS3.p1.3.m3.1.1" xref="S2.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS3.p1.3.m3.1.1.2" xref="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml">c</mi><mo id="S2.SS2.SSS3.p1.3.m3.1.1.1" xref="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS3.p1.3.m3.1.1.3" xref="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.3.m3.1b"><apply id="S2.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1"><eq id="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.1"></eq><ci id="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.2">ğ‘</ci><cn type="integer" id="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.3.m3.1c">c=0</annotation></semantics></math>) or to offload the task via which wireless, and <math id="S2.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S2.SS2.SSS3.p1.4.m4.1a"><mi id="S2.SS2.SSS3.p1.4.m4.1.1" xref="S2.SS2.SSS3.p1.4.m4.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.4.m4.1b"><ci id="S2.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.4.m4.1c">e</annotation></semantics></math> denotes the amount of allocated energy for wireless communication and locally computation. In the MEC system, our attention in this section is focus on how to improve the task executing experience (viz., QoE) of UEs.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.4" class="ltx_p">High complexities of the MEC system will be introduced especially when dealing with task offloading problems and the difficulty of acquiring the global necessary information when involving massive UEs. Hence, DRL algorithm such as Deep Q-Leaning is utilized here as the agent, which handles the joint control action of the UE.
The whole problem could be summarized as that the UE decides a joint wireless channel selection and energy allocation decision according to a stationary control policy <math id="S2.SS2.SSS3.p2.1.m1.4" class="ltx_Math" alttext="\mathbf{\Phi}=(\Phi_{\mathrm{c}}(\mathbf{\Upsilon}),\Phi_{\mathrm{e}}(\mathbf{\Upsilon}))" display="inline"><semantics id="S2.SS2.SSS3.p2.1.m1.4a"><mrow id="S2.SS2.SSS3.p2.1.m1.4.4" xref="S2.SS2.SSS3.p2.1.m1.4.4.cmml"><mi id="S2.SS2.SSS3.p2.1.m1.4.4.4" xref="S2.SS2.SSS3.p2.1.m1.4.4.4.cmml">ğš½</mi><mo id="S2.SS2.SSS3.p2.1.m1.4.4.3" xref="S2.SS2.SSS3.p2.1.m1.4.4.3.cmml">=</mo><mrow id="S2.SS2.SSS3.p2.1.m1.4.4.2.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.3.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.3" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.3.cmml">(</mo><mrow id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.cmml"><msub id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.2" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.2.cmml">Î¦</mi><mi mathvariant="normal" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.3" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.1" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.3.2" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.3.2.1" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.cmml">(</mo><mi id="S2.SS2.SSS3.p2.1.m1.1.1" xref="S2.SS2.SSS3.p2.1.m1.1.1.cmml">ğš¼</mi><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.3.2.2" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.4" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.3.cmml">,</mo><mrow id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.cmml"><msub id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.2.cmml">Î¦</mi><mi mathvariant="normal" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.3" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.3.cmml">e</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.1" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.3.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.3.2.1" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.cmml">(</mo><mi id="S2.SS2.SSS3.p2.1.m1.2.2" xref="S2.SS2.SSS3.p2.1.m1.2.2.cmml">ğš¼</mi><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.3.2.2" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.5" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.1.m1.4b"><apply id="S2.SS2.SSS3.p2.1.m1.4.4.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4"><eq id="S2.SS2.SSS3.p2.1.m1.4.4.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.3"></eq><ci id="S2.SS2.SSS3.p2.1.m1.4.4.4.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.4">ğš½</ci><interval closure="open" id="S2.SS2.SSS3.p2.1.m1.4.4.2.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2"><apply id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1"><times id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.1"></times><apply id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.2">Î¦</ci><ci id="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.3.3.1.1.1.2.3">c</ci></apply><ci id="S2.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1">ğš¼</ci></apply><apply id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2"><times id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.1"></times><apply id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.2">Î¦</ci><ci id="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.4.4.2.2.2.2.3">e</ci></apply><ci id="S2.SS2.SSS3.p2.1.m1.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.2.2">ğš¼</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.1.m1.4c">\mathbf{\Phi}=(\Phi_{\mathrm{c}}(\mathbf{\Upsilon}),\Phi_{\mathrm{e}}(\mathbf{\Upsilon}))</annotation></semantics></math>, while it keeps observing the network state <math id="S2.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{\Upsilon}" display="inline"><semantics id="S2.SS2.SSS3.p2.2.m2.1a"><mi id="S2.SS2.SSS3.p2.2.m2.1.1" xref="S2.SS2.SSS3.p2.2.m2.1.1.cmml">ğš¼</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.2.m2.1b"><ci id="S2.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1">ğš¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.2.m2.1c">\mathbf{\Upsilon}</annotation></semantics></math> which involves the task queuing state, the cumulative energy consumption of the UE, the occupying wireless channel of the UE and qualities of all wireless channels. In addition, we define an immediate utility function <math id="S2.SS2.SSS3.p2.3.m3.4" class="ltx_Math" alttext="u(\mathbf{\Upsilon},(c,e))" display="inline"><semantics id="S2.SS2.SSS3.p2.3.m3.4a"><mrow id="S2.SS2.SSS3.p2.3.m3.4.4" xref="S2.SS2.SSS3.p2.3.m3.4.4.cmml"><mi id="S2.SS2.SSS3.p2.3.m3.4.4.3" xref="S2.SS2.SSS3.p2.3.m3.4.4.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.3.m3.4.4.2" xref="S2.SS2.SSS3.p2.3.m3.4.4.2.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p2.3.m3.4.4.1.1" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.2" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.2.cmml">(</mo><mi id="S2.SS2.SSS3.p2.3.m3.3.3" xref="S2.SS2.SSS3.p2.3.m3.3.3.cmml">ğš¼</mi><mo id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.3" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.2.cmml">,</mo><mrow id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.2" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.2.1" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.1.cmml">(</mo><mi id="S2.SS2.SSS3.p2.3.m3.1.1" xref="S2.SS2.SSS3.p2.3.m3.1.1.cmml">c</mi><mo id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.2.2" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.1.cmml">,</mo><mi id="S2.SS2.SSS3.p2.3.m3.2.2" xref="S2.SS2.SSS3.p2.3.m3.2.2.cmml">e</mi><mo stretchy="false" id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.2.3" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.4" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.3.m3.4b"><apply id="S2.SS2.SSS3.p2.3.m3.4.4.cmml" xref="S2.SS2.SSS3.p2.3.m3.4.4"><times id="S2.SS2.SSS3.p2.3.m3.4.4.2.cmml" xref="S2.SS2.SSS3.p2.3.m3.4.4.2"></times><ci id="S2.SS2.SSS3.p2.3.m3.4.4.3.cmml" xref="S2.SS2.SSS3.p2.3.m3.4.4.3">ğ‘¢</ci><interval closure="open" id="S2.SS2.SSS3.p2.3.m3.4.4.1.2.cmml" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1"><ci id="S2.SS2.SSS3.p2.3.m3.3.3.cmml" xref="S2.SS2.SSS3.p2.3.m3.3.3">ğš¼</ci><interval closure="open" id="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.4.4.1.1.1.2"><ci id="S2.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1">ğ‘</ci><ci id="S2.SS2.SSS3.p2.3.m3.2.2.cmml" xref="S2.SS2.SSS3.p2.3.m3.2.2">ğ‘’</ci></interval></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.3.m3.4c">u(\mathbf{\Upsilon},(c,e))</annotation></semantics></math> to evaluate the QoE of UEs, which is inversely proportional to the execution delay of tasks (include the wireless transmission delay and the computation delay), the task queuing delay, the energy consumption of the UE and the count of task dropping and failing. By taking advantage of Deep Q-Learning and its improved version, e.g., Double DQN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the control policy <math id="S2.SS2.SSS3.p2.4.m4.4" class="ltx_Math" alttext="\mathbf{\Phi}=(\Phi_{\mathrm{c}}(\mathbf{\Upsilon}),\Phi_{\mathrm{e}}(\mathbf{\Upsilon}))" display="inline"><semantics id="S2.SS2.SSS3.p2.4.m4.4a"><mrow id="S2.SS2.SSS3.p2.4.m4.4.4" xref="S2.SS2.SSS3.p2.4.m4.4.4.cmml"><mi id="S2.SS2.SSS3.p2.4.m4.4.4.4" xref="S2.SS2.SSS3.p2.4.m4.4.4.4.cmml">ğš½</mi><mo id="S2.SS2.SSS3.p2.4.m4.4.4.3" xref="S2.SS2.SSS3.p2.4.m4.4.4.3.cmml">=</mo><mrow id="S2.SS2.SSS3.p2.4.m4.4.4.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.3.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.3" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.3.cmml">(</mo><mrow id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.cmml"><msub id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.2" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.2.cmml">Î¦</mi><mi mathvariant="normal" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.3" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.1" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.3.2" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.3.2.1" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.cmml">(</mo><mi id="S2.SS2.SSS3.p2.4.m4.1.1" xref="S2.SS2.SSS3.p2.4.m4.1.1.cmml">ğš¼</mi><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.3.2.2" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.4" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.3.cmml">,</mo><mrow id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.cmml"><msub id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.2.cmml">Î¦</mi><mi mathvariant="normal" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.3" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.3.cmml">e</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.1" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.3.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.3.2.1" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.cmml">(</mo><mi id="S2.SS2.SSS3.p2.4.m4.2.2" xref="S2.SS2.SSS3.p2.4.m4.2.2.cmml">ğš¼</mi><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.3.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.5" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.4.m4.4b"><apply id="S2.SS2.SSS3.p2.4.m4.4.4.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4"><eq id="S2.SS2.SSS3.p2.4.m4.4.4.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.3"></eq><ci id="S2.SS2.SSS3.p2.4.m4.4.4.4.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.4">ğš½</ci><interval closure="open" id="S2.SS2.SSS3.p2.4.m4.4.4.2.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2"><apply id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1"><times id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.1"></times><apply id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.2">Î¦</ci><ci id="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3.1.1.1.2.3">c</ci></apply><ci id="S2.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1">ğš¼</ci></apply><apply id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2"><times id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.1"></times><apply id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.2">Î¦</ci><ci id="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4.2.2.2.2.3">e</ci></apply><ci id="S2.SS2.SSS3.p2.4.m4.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.2.2">ğš¼</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.4.m4.4c">\mathbf{\Phi}=(\Phi_{\mathrm{c}}(\mathbf{\Upsilon}),\Phi_{\mathrm{e}}(\mathbf{\Upsilon}))</annotation></semantics></math> could be trained and achieves increasing the utility of UEs for the long-term performance optimization.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">In-Edge AI with Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In Section <a href="#S2" title="II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, two use cases of DRL in the MEC system are put forward. However, one key challenge is still pending in practical, namely the deployment of DRL agent. Taking the computation offloading use case in Section <a href="#S2.SS2" title="II-B DRL over the MEC System for Computation Offloading â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> as example, <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">if the DRL agent is trained on edge nodes or remote cloud servers just as depicted in Fig. <a href="#S3.F4" title="Figure 4 â€£ III-A2 From UEs to Edge â€£ III-A Integration of Federated Learning within Edge for In-Edge AI â€£ III In-Edge AI with Federated Learning â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a)</em>, due to the wireless communication characters of the MEC system, three deficiencies existed: 1) the training data is large in quantity when considering massive UEs, and it will increase the burden of uplink wireless channels; 2) the training data which should be uploaded to edge nodes or cloud is privacy-sensitive, and it might cause potential privacy accidents; 3) if training data is transformed for privacy consideration, server-side proxy data is less relevant than on-device data. And <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">if the DRL agent is trained on the UE individually</em>, there are still another two defects: 1) the computation capability of a UE is relatively weak, and it will consume long time or is even impossible to train the DRL agent on large-scale data; 2) the training process of a DRL agent may introduce extra energy consumption of a UE.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1809.07857/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="423" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Taxonomy of applying Deep Reinforcement Learning in mobile edge system</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">DRL techniques require intensive computation capacity
for finding the optimal solutions. Particularly, if there are huge
amount of data factors, parameters, and criteria for optimizing the resource
over large-scale MEC systems (an operatorâ€™s network in cities),
advanced distributed Deep Learning (DL) approaches should be utilized. Hence, thinking about training DRL agents in a distributed fashion efficiently is natural. As the Fig. <a href="#S3.F3" title="Figure 3 â€£ III In-Edge AI with Federated Learning â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates, though keeping and training a DRL agent in every UE and edge node can achieve the best performance, it is only practical to apply distributed DRL in the MEC system, because there shall not have enough time and data for training. However, most distributed DRL architectures, described by Fig. <a href="#S3.F4" title="Figure 4 â€£ III-A2 From UEs to Edge â€£ III-A Integration of Federated Learning within Edge for In-Edge AI â€£ III In-Edge AI with Federated Learning â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(b), could not handle unbalanced and non-IID data and cope with the privacy issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In addition, they usually reduce the performance of DRL agents when UEs and network states are heterogeneous. Therefore, Federated Learning (FL) is introduced in this paper for training DRL agents in the MEC system on account of the fact it could deal with several key challenges below, which differentiates it from other distributed DL approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Non-Independent and Identically Distributed (Non-IDD):</span> The training data (transition memories in DRL) on the UE is based on the wireless environment it experienced and its own computation capability and energy consumption. Hence, any individual training data of a UE will not be able to represent the training data of all UEs. In FL, this challenge could be met by merging the updates of models with FedAvg in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Limited communication:</span> UEs are often unpredictably off-line or allocated with poor communication resources. Nevertheless, using additional computation could decrease the consumption of communication rounds needed to train a model. In addition, FL only asks a part of clients, in one round, to upload their updates, which handles the circumstance where clients are often unpredictably off-line.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Unbalanced:</span> Some UEs may have more computation tasks to be handled and some may experience more states of mobile networks, resulting in varying amounts of training data among UEs. Also, this challenge could be coped with the FedAvg algorithm.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Privacy and Security:</span> The information needs to be uploaded for FL is the minimal update necessary to improve the DRL agent. Further, techniques of secure aggregation and differential privacy could be applied naturally, which could avoid that privacy-sensitive data are contained in local updates. Nonetheless, the privacy and security is not our focus in this work, more information about these issues could be found in references of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></p>
</div>
</li>
</ul>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Integration of Federated Learning within Edge for In-Edge AI</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Between AI in cloud and AI in UEs, there is an â€œedgeâ€. With the aid of FL framework, edge nodes equipped with AI computation in â€œedgeâ€ could combine the cloud and massive UEs together and form a powerful AI entity with strong cognitive ability provided by massive UEs and edge nodes. Throughout this envisioned architecture, each edge node can support AI tasks on system level dynamically, not only for its own but for the global optimization and balancing of the whole MEC system.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Use cases of edge caching in Section <a href="#S2.SS1" title="II-A DRL over the MEC System for Caching â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a> and computation offloading in Section <a href="#S2.SS2" title="II-B DRL over the MEC System for Computation Offloading â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> are taken to represent In-Edge AI of integrating FL.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>From Edge to Cloud</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Considering edge caching in the MEC system, the DRL agent employed in an edge node makes decisions on caching appropriate content according to requested contents of UEs dynamically. However, space-time popularity dynamics of requesting contents put forward the requirement for collaborated edge caching. Thus, the cloud server belongs to MNOs could be the central server for coordinating the edge nodes. Among all edge nodes, each of them keeps a DRL agent and updates it by its own local training data.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>From UEs to Edge</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In the scenario of computation offloading, each UE shall decide whether one computation task should be offloaded to edge nodes, offloading the task via which wireless channel and the energy consumption according to the inference result of the DRL agent in it. Though UEs such as mobile phones, industrial IoT devices, and smart vehicles are able to perform some AI computation, the computation capability and the energy consumption still limit their abilities in AI computing (DRL training on large-scale data). Therefore, we proposed to use all edge nodes in â€œedgeâ€ as an integral server for coordinating massive UEs covered by them. By virtue of this scheme, UEs with relative weak computation capability could be able to hold a complex DRL agent.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The methodology of tackling two aforementioned scenarios is similar. For training a general DL model, as depicted in Fig. <a href="#S3.F4" title="Figure 4 â€£ III-A2 From UEs to Edge â€£ III-A Integration of Federated Learning within Edge for In-Edge AI â€£ III In-Edge AI with Federated Learning â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c), FL iteratively solicits a random set of clients (distributed devices which train DL model) to 1) download parameters of the DL model from a certain server, 2) perform the training process on the downloaded model with their own data, and 3) upload only the new model parameters to the server, which aggregates uploaded updates of the client to further improve the model. The process inside an individual client is also illustrated in Fig. <a href="#S3.F4" title="Figure 4 â€£ III-A2 From UEs to Edge â€£ III-A Integration of Federated Learning within Edge for In-Edge AI â€£ III In-Edge AI with Federated Learning â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(c), where symbols are taken as the same version in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> for accessible reading. Specifically, the client trains its model, which is download from the central server before, based on local training data and upload the updated weight of MainNet backward when it is available.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">To summarize, FL enables resource-constrained edge to compute devices (include UEs and edge nodes) to learn a shared model, while keeping the training data local.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x4.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="185" height="200" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F4.sf1.3.2" class="ltx_text" style="font-size:80%;">Centralized DRL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x5.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="238" height="196" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F4.sf2.3.2" class="ltx_text" style="font-size:80%;">Distributed DRL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x6.png" id="S3.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="538" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F4.sf3.3.2" class="ltx_text" style="font-size:80%;">DRL with Federated Learning</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Centralized DRL v.s. Distributed DRL v.s. DRL with Federated Learning over mobile edge system</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Pros and Cons of Federated Learning for In-Edge AI</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The core benefit of FL lies in distributing the quality of knowledge across a large number of devices without necessarily centralizing the training data. Extended by this core benefit, several particular benefits are brought further in the MEC system.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Cognitive:</span> Server-side proxy data is less relevant than on-device data. In the MEC system, massive UEs and edge nodes could act as perceptrons and acquire various, abundant and personalized data for updating the global DL model. On the perspective of UEs, these data could include the quality of the wireless channel, the remanent battery life and the energy consumption, the immediate computation capability and so on. Concerning edge nodes, the cognitive data could include the computation load, the storage occupation, the number of wireless communication links, the task queue states waiting for handling, and etc. Thus, using FL based on these raw data instead of centralized DL renders the MEC system more cognitive.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Robust:</span> FL inherently could address key issues about the availability of UEs and edge nodes and the unbalanced and non-IID data. Consequently, the performance of In-Edge AI shall not be easily affected by the unbalanced data and sometimes the poor communication environment. Further, its ability of handling non-IDD data allows massive UEs in different wireless environments to train their own DL model without concerning about the negative effect.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Flexible:</span> In FL, additional computation could be used to decrease the number of communication rounds to train a model. An effective way to add computation is increasing computation per UE, i.e., adding more local SGD updates per round. Hence, a trade-off between computation and communication existed. Specifically, powerful (both in computation and energy) UEs could decide to perform more mini-batches in training for further decreasing the communication cost and vice versa.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Certainly, due to the limitation of federated optimization, FL could not outperform the centralized DL trained model, but could still achieve the near best performance, which will demonstrated in Section <a href="#S4" title="IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Practicability Discussion</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To some extent, our proposed In-Edge AI is a future-oriented concept. We envision a near future where most of the mobile devices, particularly smartphones, are endowed with the ability of not only inferring but also training the Deep Learning model. As is well known, even the state-of-the-art edge chips, such as Edge TPU (brought by Google and powered by TensorFlow Lite), could only support elementary training processes in DL. Therefore, the practicability should be discussed with considering both practical deployment and delay requirement.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.4.1.1" class="ltx_text">III-C</span>1 </span>Deploy Challenges</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Learning takes long time of training as well as inferring according to the required accuracy level. Obviously, the DRL model should not be deployed directly while setting weights of neural networks at random. Otherwise, the MEC system will be paralyzed because the DRL model could only make random decisions at preliminary exploration.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">Nonetheless, this may be solved if the DL model is not trained from scratch. We are now working on using transfer learning to boost the training process in MEC systems. The basic idea is to simulate the wireless environment and requests of UEs. Just as evaluating and adjusting the antenna settings in a simulated testbed, the simulated environment is used to train an off-line Deep Reinforcement Learning agent. Then, the established DRL model could be distributed to mobile UEs.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.4.1.1" class="ltx_text">III-C</span>2 </span>Delay Requirement</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Learning always takes long time of training as well as inferring according to the required accuracy level. However, if using transfer learning, there shall be less time or computation consumption on the side of training.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">Based on the pre-established DRL, the requirement of training (not inferring) is alleviated, i.e., the DRL agent could reach the satisfied accuracy level after a small number of mini-batches or even directly inferring. And when the wireless environment or the pattern of UEsâ€™ requests is changed, the UE could also take advantage of the established DRL to adjust neural networks in its DRL model.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p">Unlike using an enormous number of hidden layers and neurons in CNN or RNN, the neural network in DRL is merely a Multi-Layer Perceptron (MLP, shown in Fig. 4(c)). And in our simulated scenarios, the MLP has only one hidden layer with 200 neurons. Therefore, once the model has been trained, the inferring process could be performed quickly due to the low complexity of the MainNet in DRL agents.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p">Besides, our work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> aims to meet the challenge of running DL on resource-constrained mobile devices. By incorporating this work, the requirement on computation capacity of UEs could be relaxed along with decreasing the inferring delay.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experiment Settings</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For investigating the performance of In-Edge AI with FL, two simulations on edge caching and computation offloading as in Section <a href="#S2.SS1" title="II-A DRL over the MEC System for Caching â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a> and Section <a href="#S2.SS2" title="II-B DRL over the MEC System for Computation Offloading â€£ II Optimizing the Edge by DRL â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> are presented, respectively. Among all simulations, the time horizon is discretized into time epochs.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">We capture Xenderâ€™s trace for one month (from 01/09/2016 to 30/09/2016), including 9,514 active mobile users, conveying 188,447 content files, and 2,107,100 content requests <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, of which the content popularity distribution can be well fitted by a Zipf distribution with <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\partial=1.58" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mo rspace="0.1389em" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">âˆ‚</mo><mo lspace="0.1389em" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">1.58</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><partialdiff id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"></partialdiff><cn type="float" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">1.58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\partial=1.58</annotation></semantics></math>. In edge caching simulations, we use this mined Zipf distribution as the content popularity distribution to generate the content request of UEs, and consider the cooperation of <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">6</annotation></semantics></math> edge nodes. Once an edge node receives a request from UE, the local DRL agent in it will make a decision to cache which content or not cache, and then obtain the reward of this action for its own training. In edge caching, the cloud server belongs to MNOs is the central server for coordinating the edge nodes.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p">On investigating the capabilities of DRL coupled with FL over the MEC system for computation offloading, we suppose the whole bandwidth <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\omega=5" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">Ï‰</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ğœ”</ci><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\omega=5</annotation></semantics></math> MHz of an edge node is divided into <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn type="integer" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">10</annotation></semantics></math> wireless channels, and take <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><cn type="integer" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">10</annotation></semantics></math> UEs as the clients in FL framework to individually train their DRL agents and merge them among edge nodes. The channel gain states between the UE and the edge node are from a common finite set, which quantifies the quality of the wireless channel into <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mn id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><cn type="integer" id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">6</annotation></semantics></math> levels. Throughout the simulation, the number of tasks generated on each UE follows Bernoulli distribution with average arrival rate <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="\lambda_{\mathrm{T}}" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><msub id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mi id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">Î»</mi><mi mathvariant="normal" id="S4.SS1.p3.5.m5.1.1.3" xref="S4.SS1.p3.5.m5.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2">ğœ†</ci><ci id="S4.SS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3">T</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">\lambda_{\mathrm{T}}</annotation></semantics></math> per time epoch.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.7" class="ltx_p">As for the DRL settings in UEs, edge nodes, and cloud, we choose the Double DQN algorithm and select tanh as the activation function and Adam optimizer. We used a single-layer fully-connected feedforward neural network, which includes <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mn id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><cn type="integer" id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">200</annotation></semantics></math> neurons, to serve as the target (TargetNet) and the eval (MainNet) Q network. Other parameter values in Double DQN are set as follows: replay memory capacity <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="M=5000" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">M</mi><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">5000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><eq id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></eq><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">ğ‘€</ci><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">5000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">M=5000</annotation></semantics></math>, mini-batch size <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="B=200" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">B</mi><mo id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><eq id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1"></eq><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">ğµ</ci><cn type="integer" id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">B=200</annotation></semantics></math>, discount factor <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="\gamma=0.9" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mrow id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml">Î³</mi><mo id="S4.SS1.p4.4.m4.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><eq id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1.1"></eq><ci id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2">ğ›¾</ci><cn type="float" id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">\gamma=0.9</annotation></semantics></math>, exploration probability <math id="S4.SS1.p4.5.m5.1" class="ltx_Math" alttext="\epsilon=0.001" display="inline"><semantics id="S4.SS1.p4.5.m5.1a"><mrow id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml"><mi id="S4.SS1.p4.5.m5.1.1.2" xref="S4.SS1.p4.5.m5.1.1.2.cmml">Ïµ</mi><mo id="S4.SS1.p4.5.m5.1.1.1" xref="S4.SS1.p4.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.5.m5.1.1.3" xref="S4.SS1.p4.5.m5.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><apply id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1"><eq id="S4.SS1.p4.5.m5.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1.1"></eq><ci id="S4.SS1.p4.5.m5.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.2">italic-Ïµ</ci><cn type="float" id="S4.SS1.p4.5.m5.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">\epsilon=0.001</annotation></semantics></math>, learning rate <math id="S4.SS1.p4.6.m6.1" class="ltx_Math" alttext="\zeta=0.005" display="inline"><semantics id="S4.SS1.p4.6.m6.1a"><mrow id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml"><mi id="S4.SS1.p4.6.m6.1.1.2" xref="S4.SS1.p4.6.m6.1.1.2.cmml">Î¶</mi><mo id="S4.SS1.p4.6.m6.1.1.1" xref="S4.SS1.p4.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.6.m6.1.1.3" xref="S4.SS1.p4.6.m6.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.1b"><apply id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1"><eq id="S4.SS1.p4.6.m6.1.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1.1"></eq><ci id="S4.SS1.p4.6.m6.1.1.2.cmml" xref="S4.SS1.p4.6.m6.1.1.2">ğœ</ci><cn type="float" id="S4.SS1.p4.6.m6.1.1.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.1c">\zeta=0.005</annotation></semantics></math> and the period of replacing the target Q network is <math id="S4.SS1.p4.7.m7.1" class="ltx_Math" alttext="\phi=250" display="inline"><semantics id="S4.SS1.p4.7.m7.1a"><mrow id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml"><mi id="S4.SS1.p4.7.m7.1.1.2" xref="S4.SS1.p4.7.m7.1.1.2.cmml">Ï•</mi><mo id="S4.SS1.p4.7.m7.1.1.1" xref="S4.SS1.p4.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.7.m7.1.1.3" xref="S4.SS1.p4.7.m7.1.1.3.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.1b"><apply id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1"><eq id="S4.SS1.p4.7.m7.1.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1.1"></eq><ci id="S4.SS1.p4.7.m7.1.1.2.cmml" xref="S4.SS1.p4.7.m7.1.1.2">italic-Ï•</ci><cn type="integer" id="S4.SS1.p4.7.m7.1.1.3.cmml" xref="S4.SS1.p4.7.m7.1.1.3">250</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.1c">\phi=250</annotation></semantics></math>. In addition, to build a baseline for the performance of DRL agent with FL, we construct a centralized DRL agent for comparison and it is assumed to be able to receive all data used for reinforcement learning.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation Results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To elucidate the performance of our In-Edge AI framework, experiments on edge caching and computation offloading are carried out under various settings.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.12" class="ltx_p">Fig. <a href="#S4.F5" title="Figure 5 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> depicts the performance of DDQN with FL both on edge caching and computation offloading. Three edge nodes (<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{E}_{1}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\mathrm{E}_{1}</annotation></semantics></math>, <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{E}_{2}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><msub id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathrm{E}_{2}</annotation></semantics></math> and <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathrm{E}_{3}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\mathrm{E}_{3}</annotation></semantics></math>) and three UEs (<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathrm{m}_{1}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><msub id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\mathrm{m}_{1}</annotation></semantics></math>, <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathrm{m}_{2}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><msub id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\mathrm{m}_{2}</annotation></semantics></math> and <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathrm{m}_{3}" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><msub id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">\mathrm{m}_{3}</annotation></semantics></math>) are chosen for exhibiting the capabilities of their own DRL agents. On the perspective of edge caching, <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathrm{E}_{1}" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><msub id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">\mathrm{E}_{1}</annotation></semantics></math>, <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="\mathrm{E}_{2}" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><msub id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">\mathrm{E}_{2}</annotation></semantics></math> and <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="\mathrm{E}_{3}" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><msub id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml">E</mi><mn id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2">E</ci><cn type="integer" id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">\mathrm{E}_{3}</annotation></semantics></math> are chosen to cache contents, and hit rates of them are improving and finally maintain within a certain range along with the process of training DRL agents.
In the simulation of computation offloading, the average utilities of three UEs <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="\mathrm{m}_{1}" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><msub id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.10.m10.1.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">\mathrm{m}_{1}</annotation></semantics></math>, <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="\mathrm{m}_{2}" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><msub id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">\mathrm{m}_{2}</annotation></semantics></math> and <math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="\mathrm{m}_{3}" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><msub id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p2.12.m12.1.1.2" xref="S4.SS2.p2.12.m12.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.12.m12.1.1.3" xref="S4.SS2.p2.12.m12.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><apply id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.12.m12.1.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1">subscript</csymbol><ci id="S4.SS2.p2.12.m12.1.1.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2">m</ci><cn type="integer" id="S4.SS2.p2.12.m12.1.1.3.cmml" xref="S4.SS2.p2.12.m12.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">\mathrm{m}_{3}</annotation></semantics></math> are increasing with the decreasing of training loss, and also maintain within a certain range at last. This means that the efficiency of handling offloaded tasks in the MEC system is improved.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1809.07857/assets/x7.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="515" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Capability of Federated Learning in edge caching and computation offloading</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Further, Fig. <a href="#S4.F6" title="Figure 6 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> gives the details of performance comparison as follows.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">In Fig. <a href="#S4.F6.sf1" title="In Figure 6 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, the edge caching performance of the DDQN with FL is compared to the centralized DDQN, and some other caching policies, such as known LRU (Least Recently Used), LFU (Least Frequently Used) and FIFO (First in, First out). It can be easily seen that the specific performance of FL is near close to the results of centralized DDQN in terms of achieved hit rates of <math id="S4.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.I1.ix1.p1.1.m1.1a"><mn id="S4.I1.ix1.p1.1.m1.1.1" xref="S4.I1.ix1.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p1.1.m1.1b"><cn type="integer" id="S4.I1.ix1.p1.1.m1.1.1.cmml" xref="S4.I1.ix1.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p1.1.m1.1c">3</annotation></semantics></math> clients (edge nodes), and outperform LRU, LFU, and FIFO.</p>
</div>
</li>
<li id="S4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">In the experiment on computation offloading, the performance of the DDQN with FL is compared to the centralized DDQN, and another three baseline computation offloading policies, i.e., mobile execution, edge node execution, and greedy execution. Here, mobile execution, edge node execution and greedy execution mean that the UE processes all computation tasks locally, all computation tasks are offloaded from the UE to edge nodes, and the UE makes decision on executing a computation task locally or offloading it to edge nodes with the aim of maximizing the immediate utility, respectively. In Fig. <a href="#S4.F6.sf2" title="In Figure 6 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, it can be observed that the DDQN with FL achieves the near performance of centralized DDQN and is superior to another three baseline policies;</p>
</div>
</li>
<li id="S4.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S4.I1.ix3.p1" class="ltx_para">
<p id="S4.I1.ix3.p1.1" class="ltx_p">On investigating the detailed performance in training process, we assume that the capability of wireless communication is not the hinder, i.e., both massive training data for centralized DDQN and lightweight model updates for DDQN with FL can be uploaded to the target position within a discretized time epoch. As illustrated in Fig. <a href="#S4.F6.sf3" title="In Figure 6 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, the performance of centralized DDQN is better than that of DDQN with FL at the beginning of training. However, once the model merging of FL has been processed several times, the performance of DDQN with FL becomes near to that of centralized DDQN. Certainly, if the client wishes to obtain the expected performance using DDQN with FL, it must take time to wait for the model merging, i.e., exploiting training results of other clients. Nonetheless, this experiment assumes an ideal wireless environment. In practice, massive training data are actually unable to be uploaded without any delay. Therefore, performing DDQN with FL is more practical in MEC systems, at least for now when the wireless resource is also the major consideration.</p>
</div>
</li>
<li id="S4.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S4.I1.ix4.p1" class="ltx_para">
<p id="S4.I1.ix4.p1.1" class="ltx_p">We also gather statistics of the total wireless transmission until the DRL agent is well trained both in the scenario of edge caching and computation offloading. In the framework of FL, every client only needs to upload the update of its model. Without FL framework, viz., using centralized DRL, UEs must upload the whole training data via wireless channels and thus consume more communication resources, which is demonstrated by Fig. <a href="#S4.F6.sf4" title="In Figure 6 â€£ IV-B Evaluation Results â€£ IV Data-Driven Evaluation of Proof-of-Concept In-Edge AI Framework â€£ In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a>.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">With no doubt, FL must trade something for its advantages. Specifically, due to the fact that the coordinating server in FL only executes tasks of merging updates instead of taking over the whole training, the computation load of clients is inevitably heavier on account of the local training process. This will cause more energy cost on UEs or burden the computation of both UEs and edge nodes, and these issues are still open questions.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x8.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_square" width="145" height="143" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F6.sf1.3.2" class="ltx_text" style="font-size:80%;">Performance comparison in edge caching</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x9.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_square" width="145" height="145" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:80%;">Performance comparison in computation offloading</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x10.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_square" width="145" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F6.sf3.3.2" class="ltx_text" style="font-size:80%;">Detailed performance comparison in computation offloading</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.07857/assets/x11.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_img_square" width="145" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S4.F6.sf4.3.2" class="ltx_text" style="font-size:80%;">Comparison of transmission cost</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance Evaluation of Double DDQN with and without Federated Learning</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Opportunities and Challenges</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We hereby discuss a few essential promising research directions of In-Edge AI,
and highlight several pending problems from the perspective of elaborating and widening the usage of AI and Edge Computing.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Accelerating AI Tasks by Edge Communication and Computing Systems</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Aforementioned sections is to optimize the edge of mobile communication systems by AI techniques,
but it is also important to exploit specialized methods of optimizing learning computation tasks
by the support of edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
DRL can be treated as a type of specialized edge computing task at the service-level.
How to find correct edge nodes to collaborate, and how to allocate appropriate resources,
for the large amount of AI tasks with various priorities, deadlines, scales, different requirement
on CPU, memory and so on are also indispensable.
Interestingly, this is the reverse direction of the aforementioned Federated Learning,
but also a kind of application of the â€œIn-Edge AIâ€.
Potential research topics may include
game theoretic algorithms for the competition of edge nodes and mobile users
over the multi-dimensional resource for AI acceleration by the edge,
and the dynamic and adaptive splitting of AI tasks becomes quite challenging.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Efficiency of In-edge AI for Real-time Mobile Communication System towards 5G</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In 5G, the defined Ultra-Reliable Low-Latency Communications (URLLC) scenarios strongly
desire very small delays and high reliability of the mobile system.
However, general deep learning-based optimization and prediction schemes take quite long running time
of recursions for converging to the results, which is inappropriate for mobile edge systems;
particularly the system-level edge computing tasks desiring rapid responses in the scale of millisecond.
In order to manage the guaranteed QoS of delays and bandwidth for caching, communication and computation,
In-Edge AI should be able to provide differentiated support for various types of services,
and fine-grained collaborative scheduling of the AI tasks (or split ones) over the edge nodes and mobile devices
should be accelerated in nearly real-time, which is critical but remain unsolved in current literature.
Furthermore, the integration of Federated Learning framework with AI chipset hardware should be explored as well.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Incentive and Business Model of In-Edge AI</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In-Edge AI is a tight confederation among mobile operators, SPs/CPs, and mobile users,
while capable entities may contribute more to the overall optimization of the edge caching and computing tasks,
but a large number of UEs may rely on the AI of edge nodes and other UEs.
Also, AI computation requirements from SPs and CPs should be satisfied across edge nodes of different mobile operators.
And hence, more reliable and accurate design of the incentive framework of In-Edge AI become interesting,
which should further provide a loop of the digital copyrights of the content and services.
Blockchain techniques may be integrated into In-Edge AI framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>,
but how to distribute the huge computation load of the proof of work over the edge system
and how to evaluate the contribution of In-Edge AI computation on heterogenous scenarios
are still unexplored.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this article, we discussed the potential of integrating the Deep Reinforcement Learning techniques and Federated Learning framework with the mobile edge system and optimizing the mobile edge computing, caching and communication with it.
We perform experiments on investigating the scenarios of edge caching and computation offloading in mobile edge system, and
the â€œIn-Edge AIâ€ is evaluated and proved to have the abilities to achieve near-optimal performance. For our future work, we concentrate on not only optimizing learning computation tasks by the support of edge
but also scheduling the AI tasks over the edge nodes and mobile devices in a fine-grained and collaborative manner.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
ETSI, â€œMobile Edge Computing - Introductory Technical White Paperâ€, Sep. 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X. Wang, M. Chen, T. Taleb, A. Ksentini, V. C. M. Leung,
â€œCache in the Air: Exploiting Content Caching and Delivery Techniques for 5G Systems,â€
in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Communications</em>, vol. 52, no. 2, pp. 131-139, Feb. 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Chen and Y. Hao, â€œTask Offloading for Mobile Edge Computing in
Software Defined Ultra-Dense Network,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>,
vol. 36, no. 3, pp. 587-597, Mar. 2018

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Mao, C. You, J. Zhang, K. Huang, K. B. Letaief,
â€œA Survey on Mobile Edge Computing: The Communication Perspectiveâ€,
in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surv. Tutorials</em>, vol. 19, no.4, pp. 2322-2358, Aug. 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Mao, F. Hu, and Q. Hao, â€œDeep Learning for Intelligent Wireless Networks: A Comprehensive Survey,â€
in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surv. Tutorials</em>, Early Access, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F. Tang, B. Mao, Z. M. Fadlullah, N. Kato, O. Akashi, T. Inoue, and K. Mizutani, â€œOn Removing Routing Protocol from Future Wireless Networks: A Real-Time Deep Learning Approach for Intelligent Traffic Control,â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Wireless Communications</em>, vol. 25, no. 1, pp. 154-160, Feb. 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Sadeghi, F. Sheikholeslami, and G. B. Giannakis, â€œOptimal and Scalable Caching for 5G Using Reinforcement Learning of Space-Time Popularities,â€
in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Signal Process.</em>, vol. 12, no. 1, pp. 180-190, Feb. 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. He, N. Zhao, and H. Yin, â€œIntegrated Networking, Caching, and Computing for Connected Vehicles: A Deep Reinforcement Learning Approach,â€
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Veh. Technol.</em>, vol. 67, no. 1, pp. 44-55, Jan. 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. S. Sutton and A. G. Barto, <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">â€œReinforcement Learning: An Introductionâ€</em>. Cambridge, MA, USA: MIT Press, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
V. Mnih et al., â€œHuman-Level Control through Deep Reinforcement Learning,â€
in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 518, no. 7540, pp. 529-533, Feb. 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
â€œCommunication-Efficient Learning of Deep Networks from Decentralized Data,â€
in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, Apr. 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Van Hasselt, A. Guez, and D. Silver, â€œDeep Reinforcement Learning
with Double Q-Learning,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th AAAI Conference
on Artificial Intelligence</em>, vol. 16, 2016, pp. 2094-2100.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Li, X. Wang, P.-J. Wan, Z. Han, V. C.M. Leung, â€œHierarchical
Edge Caching in Device-to-Device Aided Mobile Networks: Modeling,
Optimization, and Designâ€, <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>,
Special Issue on Caching for Communication Systems and
Networks, Early Access, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. Li, Z. Zhou, X. Chen,
â€œEdge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy,â€
in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACM SIGCOMM, MECOMM Workshop</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Z. Xiong, Y. Zhang, D. Niyato, P. Wang, and Z. Han, â€œWhen mobile
blockchain meets edge computing: Challenges and applications,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1711.05938</em>, 2017.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td"></td>
<td id="tab1.1.1.2" class="ltx_td">
<span id="tab1.1.1.2.1" class="ltx_inline-block">
<span id="tab1.1.1.2.1.1" class="ltx_p"><span id="tab1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">X</span> iaofei Wang (xiaofeiwang@tju.edu.cn)
is currently a professor in Tianjin University, China.
He received M.S. and Ph.D degrees from the
School of Computer Science and Engineering, Seoul National
University in 2008 and 2013 respectively. He received the
B.S. degree in the Department of Computer Science and Technology, Huazhong University
of Science and Technology in 2005. He is the winner of the
IEEE ComSoc Fred W. Ellersick Prize in 2017.
His current research interests are social-
aware multimedia service in cloud computing, cooperative
backhaul caching and traffic offloading in mobile content-
centric networks.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td"></td>
<td id="tab2.1.1.2" class="ltx_td">
<span id="tab2.1.1.2.1" class="ltx_inline-block">
<span id="tab2.1.1.2.1.1" class="ltx_p"><span id="tab2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Y</span> iwen Han [Sâ€™18] (hanyiwen@tju.edu.cn)
is a Ph.D student in Tianjin University, China.
He received the B.S. and M.S. degrees from Nanchang University and Tianjin University, China, in 2015 and 2017, respectively.
His current research interests include edge computing, wireless communication, and machine learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td"></td>
<td id="tab3.1.1.2" class="ltx_td">
<span id="tab3.1.1.2.1" class="ltx_inline-block">
<span id="tab3.1.1.2.1.1" class="ltx_p"><span id="tab3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">C</span> henyang Wang (chenyangwang@tju.edu.cn)
received his B.S. and M.S. degrees from Henan Normal University, Xinxiang, Henan province, China. He is now a Ph.D student in the School of Computer Science and Technology at Tianjin University. His research interests include Mobile Edge Computing, Caching, Offloading and D2D wireless networks.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td"></td>
<td id="tab4.1.1.2" class="ltx_td">
<span id="tab4.1.1.2.1" class="ltx_inline-block">
<span id="tab4.1.1.2.1.1" class="ltx_p"><span id="tab4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Q</span> iyang Zhao
is currently a senior research engineer at Huawei Technologies, China.
He received B.S. and Ph.D degrees from Xidian University and University of York in 2005 and 2013 respectively.
He is specialized in end-to-end network slicing, radio resource management, control and user planes, network orchestration, optimization, reinforcement learning, transfer learning, neural network and data analytics, plus comprehensive skills in mathematical modeling, system level simulation and large-scale prototype development.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td"></td>
<td id="tab5.1.1.2" class="ltx_td">
<span id="tab5.1.1.2.1" class="ltx_inline-block">
<span id="tab5.1.1.2.1.1" class="ltx_p"><span id="tab5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">X</span> u Chen [Mâ€™12] (chenxu35@mail.sysu.edu.cn)
received his Ph.D. degree in information engineering
from the Chinese University of Hong Kong in 2012. He was a
postdoctoral research associate with Arizona State University,
Tempe, from 2012 to 2014, and a Humboldt Fellow with the
University of Goettingen, Germany, from 2014 to 2016. He is
currently a professor with the School of Data and Computer
Science, Sun Yat-sen University, Guangzhou, China. He received
the 2017 IEEE ICC Best Paper Award, the 2014 IEEE INFOCOM
Best Paper Runner-Up Award, and the 2014 Hong Kong Young
Scientist Runner-Up Award.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1809.07856" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1809.07857" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1809.07857">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1809.07857" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1809.07859" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 08:03:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
