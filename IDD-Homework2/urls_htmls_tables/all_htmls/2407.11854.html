<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.11854] Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection</title><meta property="og:description" content="Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora.
However, these annotations are unavailable in many low-resource languages.
In this paper, we investigate GED in this context. Levâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.11854">

<!--Generated on Mon Aug  5 14:05:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Zero-shot Cross-Lingual Transfer 
<br class="ltx_break">for Synthetic Data Generation in Grammatical Error Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gaetan Lopez Latouche
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marc-AndrÃ© Carbonneau
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Swanson 
<br class="ltx_break">Ubisoft La Forge 
<br class="ltx_break">{gaetan.lopez-latouche,marc-andre.carbonneau2,ben.swanson2}@ubisoft.com
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora.
However, these annotations are unavailable in many low-resource languages.
In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages.
This approach outperforms current state-of-the-art annotation-free GED methods.
We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Grammatical Error Detection (GED) refers to the automated process of detecting errors in text. It is often framed as a binary sequence labeling task where each token is classified as either correct or erroneous <cite class="ltx_cite ltx_citemacro_cite">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>. GED is widely used in language learning applications and contributes to the performance of grammatical error correction (GEC) systems <cite class="ltx_cite ltx_citemacro_cite">Yuan etÂ al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>); Zhou etÂ al. (<a href="#bib.bib68" title="" class="ltx_ref">2023</a>); Sutter PessurnoÂ de Carvalho (<a href="#bib.bib53" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Prior research in multilingual GED has primarily operated in supervised settings <cite class="ltx_cite ltx_citemacro_cite">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>); Yuan etÂ al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>, relying on human annotated data for training.
Despite recent efforts to obtain annotated corpora <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>); Alhafni etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> many languages still lack these resources, motivating research on methods operating without GED annotations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome the absence of human annotations, researchers have explored two primary approaches. The first involves language-agnostic artificial error generation (AEG). This is achieved using rules <cite class="ltx_cite ltx_citemacro_cite">Rothe etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>); Grundkiewicz and Junczys-Dowmunt (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, non-autoregressive translation <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, or round-trip translation <cite class="ltx_cite ltx_citemacro_cite">Lichtarge etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. These methods are not trained to replicate human errors and compare unfavorably to supervised techniques like back-translation <cite class="ltx_cite ltx_citemacro_cite">Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>); Stahlberg and Kumar (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>); Kiyono etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Luhtaru etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2024b</a>)</cite> which train models to learn to generate human errors.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The second approach leverages the cross-lingual transfer (CLT) capabilities of BERT-like <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> multilingual pre-trained language models (mPLMs).
This involves fine-tuning a GED model on languages with abundant human annotations (termed as source languages) and evaluating their performance on languages devoid of human annotations (referred to as target languages). While certain languages exhibit unique error types, most adhere to shared linguistic rules, which mPLMs can exploit to detect errors across languages.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we hypothesize that error generation also share linguistic similarities across languages. We propose a novel approach to zero-shot CLT in GED by combining back-translation with the CLT capabilities of mPLMs to perform AEG in various target languages. Our methodology involves a two-stage fine-tuning pipeline: first, a GED model is fine-tuned on multilingual synthetic data produced by our language-agnostic back-translation approach; second, the model undergoes further fine-tuning on human-annotated GED corpora from the source languages.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We experiment on 6 source and 5 target languages and show that our technique surpasses previous state-of-the-art annotation-free GED methods. In addition, we provide a detailed error analysis comparing several AEG methods to ours.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel state-of-the-art method for GED on languages without annotations.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that we can leverage the CLT capabilities of mPLMs for synthetic data generation to improve performance on a different downstream task, in our case GED.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide the first evaluation of GEC annotation-free synthetic data generation methods applied to multilingual GED.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We release a synthetic GED corpus comprising over 5 million samples in 11 languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">GED</span> Originally addressed through statistical <cite class="ltx_cite ltx_citemacro_cite">Gamon (<a href="#bib.bib24" title="" class="ltx_ref">2011</a>)</cite> and neural models <cite class="ltx_cite ltx_citemacro_cite">Rei and Yannakoudakis (<a href="#bib.bib44" title="" class="ltx_ref">2016</a>)</cite>, GED is now tackled using pre-trained language models <cite class="ltx_cite ltx_citemacro_cite">Kaneko and Komachi (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>); Bell etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>); Yuan etÂ al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>); Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>); Le-Hong etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Historically, most research in GED has been concentrated on the English language. However, recently, <cite class="ltx_cite ltx_citemacro_citet">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite> organised the first shared task on multilingual GED in which <cite class="ltx_cite ltx_citemacro_citet">Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> set state-of-the-art in all non-English datasets by fine-tuning a XLM-RoBERTa large model on human annotated data in a monolingual setting. While we follow their methodology to train our GED model, we complement prior research by exploring GED for languages lacking annotations.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Artificial Error Generation</span> Current methods for AEG can be broadly categorized into language-agnostic and language-specific approaches. Language-specific methods focus on replicating the error patterns found in a specific GEC corpora. This can involve heuristic approaches tailored to mimic the linguistic errors identified in GEC corpora <cite class="ltx_cite ltx_citemacro_cite">Awasthi etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>); Cao etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2023a</a>); NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>, or employing techniques such as back-translation <cite class="ltx_cite ltx_citemacro_cite">Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>); Stahlberg and Kumar (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>); Kiyono etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Luhtaru etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2024b</a>)</cite>. While effective for languages with annotated corpora, these methods are not suitable for languages lacking such resources.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In contrast, there are few language-agnostic methods for generating artificial errors. <cite class="ltx_cite ltx_citemacro_citet">Grundkiewicz and Junczys-Dowmunt (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> introduce errors in a corpus by deleting, swapping, inserting and replacing words and characters. Replacements rely on confusion sets obtained from an inverted spellchecker. <cite class="ltx_cite ltx_citemacro_citet">Lichtarge etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> introduce noise via round-trip translation using a bridge language. Finally, <cite class="ltx_cite ltx_citemacro_citet">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> corrupt sentences by performing non-autoregressive translation using a pre-trained cross-lingual language model. All these error generation techniques have primarily been applied to GEC, and to the best of our knowledge, their performance has not been evaluated on GED.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Our work advances existing synthetic data generation methods by exploring a language-agnostic variant of back-translation.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Unsupervised GEC</span> Unlike GED, GEC without human annotations has been explored in several studies <cite class="ltx_cite ltx_citemacro_cite">Alikaniotis and Raheja (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>); Yasunaga etÂ al. (<a href="#bib.bib64" title="" class="ltx_ref">2021</a>); Cao etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2023b</a>)</cite>. State-of-the-art unsupervised GEC systems <cite class="ltx_cite ltx_citemacro_cite">Yasunaga etÂ al. (<a href="#bib.bib64" title="" class="ltx_ref">2021</a>); Cao etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2023b</a>)</cite> typically begin with the development of a GED model trained on erroneous sentences generated through rule-based methods <cite class="ltx_cite ltx_citemacro_cite">Awasthi etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> or masked language models <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2023b</a>)</cite>. This GED model is subsequently used with the Break-It-Fix-It (BIFI) method to create an unsupervised GEC system.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">However, the methods used by <cite class="ltx_cite ltx_citemacro_citet">Yasunaga etÂ al. (<a href="#bib.bib64" title="" class="ltx_ref">2021</a>); Cao etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2023b</a>)</cite> for creating the GED model are not language-agnostic, as they rely on a thorough analysis of language-specific error patterns, making them difficult to apply to languages lacking such annotations.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.11854/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="73" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our proposed method.</figcaption>
</figure>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.1" class="ltx_p"><span id="S2.p8.1.1" class="ltx_text ltx_font_bold">Cross-lingual transfer</span> Previous studies have shown the capacity of mPLMs to generalize to languages unseen during fine-tuning for both NLU <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>); Chi etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>); Lopez Latouche etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> and generative tasks <cite class="ltx_cite ltx_citemacro_cite">Xue etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2021</a>); Chirkova and Nikoulina (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>); Shaham etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2024</a>)</cite>. Close to our work, <cite class="ltx_cite ltx_citemacro_citet">Yamashita etÂ al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>)</cite> explored cross-lingual transfer in GEC, a closely related topic. Their findings indicate that pre-training with Masked Language Modeling and Translation Language Modeling enhances cross-lingual transfer. Additionally, they show that fine-tuning on a combination of a high and a low-resource language improves the performance of GEC models on the low-resource language.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">In contrast to <cite class="ltx_cite ltx_citemacro_citet">Yamashita etÂ al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>)</cite> our research focuses on zero-shot cross-lingual transfer, specifically for GED and AEG, without relying on target language annotations. Additionally, we advance previous work on zero-shot cross-lingual transfer by demonstrating its effectiveness in improving downstream task performance. Investigating zero-shot CLT in GED is particularly significant because the "translate-train" baseline <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>); Wu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2024</a>)</cite>, which involves training a GED model on a translated dataset, is infeasible. This arises because machine translation systems tend to correct the errors that the GED model is intended to detect.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our proposed GED method is developed through a four-step process, as illustrated in Figure <a href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Initially, we train a multilingual AEG model using GEC datasets from the source languages. This AEG model is subsequently employed to produce a GED dataset encompassing both target and source languages. In the third step, we fine-tune a GED model on this multilingual artificially generated dataset. Finally, we perform an additional fine-tuning of the GED model using human-annotated GED data from the source languages. The resultant GED model is capable of detecting errors across any target language.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.3" class="ltx_p"><span id="S3.p2.3.1" class="ltx_text ltx_font_bold">Data</span> Our method necessitates three types of corpora. First, the AEG model is trained using GEC datasets in a collection of source languages, <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">D</mi><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ğ·</ci><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">D_{s}</annotation></semantics></math>, which include pairs of ungrammatical sentences and their corrected versions. Additionally, monolingual corpora in the source languages <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\tilde{D}_{s}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mover accent="true" id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml"><mi id="S3.p2.2.m2.1.1.2.2" xref="S3.p2.2.m2.1.1.2.2.cmml">D</mi><mo id="S3.p2.2.m2.1.1.2.1" xref="S3.p2.2.m2.1.1.2.1.cmml">~</mo></mover><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><apply id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2"><ci id="S3.p2.2.m2.1.1.2.1.cmml" xref="S3.p2.2.m2.1.1.2.1">~</ci><ci id="S3.p2.2.m2.1.1.2.2.cmml" xref="S3.p2.2.m2.1.1.2.2">ğ·</ci></apply><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\tilde{D}_{s}</annotation></semantics></math> and in the target low-resource languages <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\tilde{D}_{t}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml"><mi id="S3.p2.3.m3.1.1.2.2" xref="S3.p2.3.m3.1.1.2.2.cmml">D</mi><mo id="S3.p2.3.m3.1.1.2.1" xref="S3.p2.3.m3.1.1.2.1.cmml">~</mo></mover><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><apply id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2"><ci id="S3.p2.3.m3.1.1.2.1.cmml" xref="S3.p2.3.m3.1.1.2.1">~</ci><ci id="S3.p2.3.m3.1.1.2.2.cmml" xref="S3.p2.3.m3.1.1.2.2">ğ·</ci></apply><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\tilde{D}_{t}</annotation></semantics></math>, consisting of raw sentences, are required.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">AEG Training</span> The AEG is a generative mPLM trained on a dataset <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">D</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ğ·</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">D_{s}</annotation></semantics></math> combining all source languages, using the corrected text as input and the ungrammatical one as output. Post-training, the AEG can introduce errors in any language supported by the mPLM, leveraging the inherent zero-shot cross-lingual transfer capabilities of generative mPLMs.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.4" class="ltx_p"><span id="S3.p4.4.1" class="ltx_text ltx_font_bold">GED Artificial Data Creation</span> Using our AEG system we obtain a multilingual dataset <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="D_{synth}" display="inline"><semantics id="S3.p4.1.m1.1a"><msub id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">D</mi><mrow id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml"><mi id="S3.p4.1.m1.1.1.3.2" xref="S3.p4.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.3.1" xref="S3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.1.m1.1.1.3.3" xref="S3.p4.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.3.1a" xref="S3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.1.m1.1.1.3.4" xref="S3.p4.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.3.1b" xref="S3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.1.m1.1.1.3.5" xref="S3.p4.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.3.1c" xref="S3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.1.m1.1.1.3.6" xref="S3.p4.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">ğ·</ci><apply id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3"><times id="S3.p4.1.m1.1.1.3.1.cmml" xref="S3.p4.1.m1.1.1.3.1"></times><ci id="S3.p4.1.m1.1.1.3.2.cmml" xref="S3.p4.1.m1.1.1.3.2">ğ‘ </ci><ci id="S3.p4.1.m1.1.1.3.3.cmml" xref="S3.p4.1.m1.1.1.3.3">ğ‘¦</ci><ci id="S3.p4.1.m1.1.1.3.4.cmml" xref="S3.p4.1.m1.1.1.3.4">ğ‘›</ci><ci id="S3.p4.1.m1.1.1.3.5.cmml" xref="S3.p4.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S3.p4.1.m1.1.1.3.6.cmml" xref="S3.p4.1.m1.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">D_{synth}</annotation></semantics></math> of raw sentences and their corresponding synthetically generated ungrammatical versions by corrupting sentences from <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\tilde{D}_{s}" display="inline"><semantics id="S3.p4.2.m2.1a"><msub id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mover accent="true" id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml"><mi id="S3.p4.2.m2.1.1.2.2" xref="S3.p4.2.m2.1.1.2.2.cmml">D</mi><mo id="S3.p4.2.m2.1.1.2.1" xref="S3.p4.2.m2.1.1.2.1.cmml">~</mo></mover><mi id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">subscript</csymbol><apply id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2"><ci id="S3.p4.2.m2.1.1.2.1.cmml" xref="S3.p4.2.m2.1.1.2.1">~</ci><ci id="S3.p4.2.m2.1.1.2.2.cmml" xref="S3.p4.2.m2.1.1.2.2">ğ·</ci></apply><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\tilde{D}_{s}</annotation></semantics></math> and <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="\tilde{D}_{t}" display="inline"><semantics id="S3.p4.3.m3.1a"><msub id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mover accent="true" id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml"><mi id="S3.p4.3.m3.1.1.2.2" xref="S3.p4.3.m3.1.1.2.2.cmml">D</mi><mo id="S3.p4.3.m3.1.1.2.1" xref="S3.p4.3.m3.1.1.2.1.cmml">~</mo></mover><mi id="S3.p4.3.m3.1.1.3" xref="S3.p4.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1">subscript</csymbol><apply id="S3.p4.3.m3.1.1.2.cmml" xref="S3.p4.3.m3.1.1.2"><ci id="S3.p4.3.m3.1.1.2.1.cmml" xref="S3.p4.3.m3.1.1.2.1">~</ci><ci id="S3.p4.3.m3.1.1.2.2.cmml" xref="S3.p4.3.m3.1.1.2.2">ğ·</ci></apply><ci id="S3.p4.3.m3.1.1.3.cmml" xref="S3.p4.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">\tilde{D}_{t}</annotation></semantics></math>. We obtain GED token-level annotation from <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="D_{synth}" display="inline"><semantics id="S3.p4.4.m4.1a"><msub id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml"><mi id="S3.p4.4.m4.1.1.2" xref="S3.p4.4.m4.1.1.2.cmml">D</mi><mrow id="S3.p4.4.m4.1.1.3" xref="S3.p4.4.m4.1.1.3.cmml"><mi id="S3.p4.4.m4.1.1.3.2" xref="S3.p4.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.1.1.3.1" xref="S3.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.4.m4.1.1.3.3" xref="S3.p4.4.m4.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.1.1.3.1a" xref="S3.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.4.m4.1.1.3.4" xref="S3.p4.4.m4.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.1.1.3.1b" xref="S3.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.4.m4.1.1.3.5" xref="S3.p4.4.m4.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.1.1.3.1c" xref="S3.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.p4.4.m4.1.1.3.6" xref="S3.p4.4.m4.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><apply id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m4.1.1.1.cmml" xref="S3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.p4.4.m4.1.1.2.cmml" xref="S3.p4.4.m4.1.1.2">ğ·</ci><apply id="S3.p4.4.m4.1.1.3.cmml" xref="S3.p4.4.m4.1.1.3"><times id="S3.p4.4.m4.1.1.3.1.cmml" xref="S3.p4.4.m4.1.1.3.1"></times><ci id="S3.p4.4.m4.1.1.3.2.cmml" xref="S3.p4.4.m4.1.1.3.2">ğ‘ </ci><ci id="S3.p4.4.m4.1.1.3.3.cmml" xref="S3.p4.4.m4.1.1.3.3">ğ‘¦</ci><ci id="S3.p4.4.m4.1.1.3.4.cmml" xref="S3.p4.4.m4.1.1.3.4">ğ‘›</ci><ci id="S3.p4.4.m4.1.1.3.5.cmml" xref="S3.p4.4.m4.1.1.3.5">ğ‘¡</ci><ci id="S3.p4.4.m4.1.1.3.6.cmml" xref="S3.p4.4.m4.1.1.3.6">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">D_{synth}</annotation></semantics></math> by tokenizing using language-specific tokenizers, and aligning both sentence versions using Levenshtein distance with minimal alignment following <cite class="ltx_cite ltx_citemacro_citet">Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>. We follow the labeling methodology of <cite class="ltx_cite ltx_citemacro_citet">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>. We designate tokens that are not aligned with themselves or tokens following a gap as incorrect, while remaining tokens are labeled as correct.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">GED model fine-tuning</span> We propose a two-stage methodology for our multilingual GED model akin to supervised GEC <cite class="ltx_cite ltx_citemacro_cite">Grundkiewicz etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>); Rothe etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>); Luhtaru etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2024a</a>)</cite>. Models are initially fine-tuned on synthetic data and later refined with human-annotated data. Our approach begins with the fine-tuning of an mPLM such as XLM-R <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> on our synthetically generated multilingual GED datasets. Then, we fine-tune this model using human-annotated GED data from all our source languages, <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.p5.1.m1.1a"><msub id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">D</mi><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">ğ·</ci><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">D_{s}</annotation></semantics></math>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">
<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><msub id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">F</mi><mn id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">F_{0.5}</annotation></semantics></math>(%)</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.2.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_left"><span id="S3.T1.1.2.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Swedish</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Italian</td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">Czech</td>
<td id="S3.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">Arabic</td>
<td id="S3.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">Chinese</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.3.1.1" class="ltx_text">
<span id="S3.T1.1.3.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S3.T1.1.3.1.1.1.1" class="ltx_p">Supervised</span>
</span></span></td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_left ltx_border_t"><cite class="ltx_cite ltx_citemacro_citet">Colla etÂ al. <span id="S3.T1.1.3.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">(</span><a href="#bib.bib14" title="" class="ltx_ref">2023</a><span id="S3.T1.1.3.2.2.2.2.1" class="ltx_text ltx_font_smallcaps">)</span></cite></td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">78.2</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">82.2</td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">73.4</td>
<td id="S3.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Alhafni etÂ al. <span id="S3.T1.1.4.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(</span><a href="#bib.bib1" title="" class="ltx_ref">2023</a><span id="S3.T1.1.4.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">)</span></cite></td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.4.5" class="ltx_td ltx_align_center">86.6</td>
<td id="S3.T1.1.4.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. <span id="S3.T1.1.5.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(</span><a href="#bib.bib34" title="" class="ltx_ref">2023</a><span id="S3.T1.1.5.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">)</span></cite></td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.5.6" class="ltx_td ltx_align_center">59.7</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.6.1.1" class="ltx_text">Synthetic data</span></td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.6.2.1" class="ltx_text ltx_font_smallcaps">Rules</span></td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">60.0</td>
<td id="S3.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">56.1</td>
<td id="S3.T1.1.6.6" class="ltx_td ltx_align_center ltx_border_t">51.9</td>
<td id="S3.T1.1.6.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.7.1.1" class="ltx_text ltx_font_smallcaps">RT translation</span></td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_center">57.0</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_align_center">43.0</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_align_center">45.9</td>
<td id="S3.T1.1.7.5" class="ltx_td ltx_align_center">38.3</td>
<td id="S3.T1.1.7.6" class="ltx_td ltx_align_center">20.1</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.8.1.1" class="ltx_text ltx_font_smallcaps">NAT</span></td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_align_center">65.9</td>
<td id="S3.T1.1.8.3" class="ltx_td ltx_align_center">58.6</td>
<td id="S3.T1.1.8.4" class="ltx_td ltx_align_center">61.1</td>
<td id="S3.T1.1.8.5" class="ltx_td ltx_align_center">52.5</td>
<td id="S3.T1.1.8.6" class="ltx_td ltx_align_center">30.4</td>
</tr>
<tr id="S3.T1.1.9" class="ltx_tr">
<td id="S3.T1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S3.T1.1.9.1.1" class="ltx_text">Zero-shot</span></td>
<td id="S3.T1.1.9.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.9.2.1" class="ltx_text ltx_font_smallcaps">DirectCLT</span></td>
<td id="S3.T1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">71.5</td>
<td id="S3.T1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">63.8</td>
<td id="S3.T1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">62.1</td>
<td id="S3.T1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">57.3</td>
<td id="S3.T1.1.9.7" class="ltx_td ltx_align_center ltx_border_t">36.2</td>
</tr>
<tr id="S3.T1.1.10" class="ltx_tr">
<td id="S3.T1.1.10.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T1.1.10.1.1" class="ltx_text ltx_font_smallcaps">Ours</span></td>
<td id="S3.T1.1.10.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.10.2.1" class="ltx_text ltx_font_bold">74.7</span></td>
<td id="S3.T1.1.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.10.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S3.T1.1.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.10.4.1" class="ltx_text ltx_font_bold">66.6</span></td>
<td id="S3.T1.1.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.10.5.1" class="ltx_text ltx_font_bold">62.8</span></td>
<td id="S3.T1.1.10.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.10.6.1" class="ltx_text ltx_font_bold">42.9</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of <math id="S3.T1.3.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S3.T1.3.m1.1b"><msub id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml"><mi id="S3.T1.3.m1.1.1.2" xref="S3.T1.3.m1.1.1.2.cmml">F</mi><mn id="S3.T1.3.m1.1.1.3" xref="S3.T1.3.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><apply id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.m1.1.1.1.cmml" xref="S3.T1.3.m1.1.1">subscript</csymbol><ci id="S3.T1.3.m1.1.1.2.cmml" xref="S3.T1.3.m1.1.1.2">ğ¹</ci><cn type="float" id="S3.T1.3.m1.1.1.3.cmml" xref="S3.T1.3.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">F_{0.5}</annotation></semantics></math> between our proposed method, previous synthetic data generation techniques, and the zero-shot cross-lingual transfer baseline on L2 corpora.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets &amp; Evaluation Metric</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We use English, German, Estonian, Russian, Icelandic, and Spanish as our source languages and Swedish, Italian, Czech, Arabic, and Chinese as our target languages. For each dataset, when multiple subsets are available we use the L2 learnersâ€™ corpora and the annotations for minimal corrections for grammaticality.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Training set</span> The English, German, Estonian, Russian, Icelandic, and Spanish datasets are taken from the FCE corpus <cite class="ltx_cite ltx_citemacro_cite">Yannakoudakis etÂ al. (<a href="#bib.bib63" title="" class="ltx_ref">2011</a>)</cite>, the Falko-MERLIN GEC corpus <cite class="ltx_cite ltx_citemacro_cite">Boyd (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>, UT-L2 GEC <cite class="ltx_cite ltx_citemacro_cite">Rummo and Praakli (<a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>, RULEC-GEC <cite class="ltx_cite ltx_citemacro_cite">Rozovskaya and Roth (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>, the Icelandic language learners section of the Icelandic Error Corpus <cite class="ltx_cite ltx_citemacro_cite">ArnardÃ³ttir etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>, and COWS-L2H <cite class="ltx_cite ltx_citemacro_cite">Davidson etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, respectively. We use the training set of each of these GEC datasets to train our generative mPLM. Additionally, for the second stage of our multilingual two-stage fine-tuning pipeline, we use the GED version of each GEC training dataset. For English and German, we use the GED dataset of <cite class="ltx_cite ltx_citemacro_citet">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>. For Russian, we convert the <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="M^{2}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msup id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">M</mi><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ‘€</ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">M^{2}</annotation></semantics></math> files <cite class="ltx_cite ltx_citemacro_cite">Dahlmeier and Ng (<a href="#bib.bib17" title="" class="ltx_ref">2012</a>)</cite> to a GED dataset following the approach used by <cite class="ltx_cite ltx_citemacro_citet">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>; for the remaining languages, we obtain GED annotations from GEC corpora as detailed in <a href="#S3" title="3 Method â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation set</span> The Swedish, Italian and Czech datasets originate from the Swell corpus <cite class="ltx_cite ltx_citemacro_cite">Volodina etÂ al. (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, MERLIN <cite class="ltx_cite ltx_citemacro_cite">Boyd etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2014</a>)</cite> and GECCC <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> respectively. We employ the processed version of those datasets provided in the Multi-GED Shared task 2023 <cite class="ltx_cite ltx_citemacro_cite">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>. For Arabic, we use both development and test data of the QALB-2015 shared tasks <cite class="ltx_cite ltx_citemacro_cite">Rozovskaya etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite> provided by <cite class="ltx_cite ltx_citemacro_citet">Alhafni etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>. Finally, the Chinese GED data is derived from two GEC corpora: MuCGEC-Dev <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite> as development set and NLPCC18-Test <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2018</a>)</cite> as test set. We apply the post-processing method described in <a href="#S3" title="3 Method â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to produce the GED versions.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Monolingual corpora</span> Our monolingual text data comes from the CC100 dataset <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> in which we sample 200 thousand error-free instances for each language.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Evaluation Metric</span> Following previous work in GED, we report the token-based <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><msub id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mi id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">F_{0.5}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Kaneko and Komachi (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>); Yuan etÂ al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>); Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>. For finer-grained analysis we also report the precision-recall curves of our main experiments.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baselines</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate the proposed artificial error generation method against strong baselines that do not require human-annotated datasets in the target language. We chose methods representative of different family of artificial error generation in GEC: Rules <cite class="ltx_cite ltx_citemacro_cite">Grundkiewicz and Junczys-Dowmunt (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, Round-trip translation (RT translation) <cite class="ltx_cite ltx_citemacro_cite">Lichtarge etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>, Non auto-regressive translation (NAT) <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>. Additionally, we compare our approach with a zero-shot CLT baseline, which involves directly fine-tuning the GED model on GED datasets from all source languages. We refer to this technique as DirectCLT to distinguish it from our method, which uses the cross-lingual transfer capabilities of generative mPLMs to generate errors in any target language. More information on the implementations of our baselines in Appendix <a href="#A1.SS1" title="A.1 Baselines â€£ Appendix A Appendix â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">
<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">F</mi><mn id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">F_{0.5}</annotation></semantics></math>(%)</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.2.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Swedish</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Italian</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Czech</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">Arabic</td>
<td id="S4.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_t">Chinese</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.3.1.1" class="ltx_text ltx_font_smallcaps">DirectCLT</span></td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">71.5</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">63.8</td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">62.1</td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">57.3</td>
<td id="S4.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_t">36.2</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.4.1.1" class="ltx_text ltx_font_smallcaps">Rules</span></td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_t">60.0</td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center ltx_border_t">56.1</td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_align_center ltx_border_t">51.9</td>
<td id="S4.T2.1.4.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.5.1.1" class="ltx_text ltx_font_smallcaps">RT translation</span></td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center">57.0</td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center">43.0</td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_align_center">45.9</td>
<td id="S4.T2.1.5.5" class="ltx_td ltx_align_center">38.3</td>
<td id="S4.T2.1.5.6" class="ltx_td ltx_align_center">20.1</td>
</tr>
<tr id="S4.T2.1.6" class="ltx_tr">
<td id="S4.T2.1.6.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.6.1.1" class="ltx_text ltx_font_smallcaps">NAT</span></td>
<td id="S4.T2.1.6.2" class="ltx_td ltx_align_center">65.9</td>
<td id="S4.T2.1.6.3" class="ltx_td ltx_align_center">58.6</td>
<td id="S4.T2.1.6.4" class="ltx_td ltx_align_center">61.1</td>
<td id="S4.T2.1.6.5" class="ltx_td ltx_align_center">52.5</td>
<td id="S4.T2.1.6.6" class="ltx_td ltx_align_center">30.4</td>
</tr>
<tr id="S4.T2.1.7" class="ltx_tr">
<td id="S4.T2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.1.7.1.1" class="ltx_text ltx_font_smallcaps">Ours monolingual</span></td>
<td id="S4.T2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.7.2.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.7.3.1" class="ltx_text ltx_font_bold">70.3</span></td>
<td id="S4.T2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.7.4.1" class="ltx_text ltx_font_bold">63.0</span></td>
<td id="S4.T2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.7.5.1" class="ltx_text ltx_font_bold">62.3</span></td>
<td id="S4.T2.1.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.7.6.1" class="ltx_text ltx_font_bold">39.8</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of <math id="S4.T2.3.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S4.T2.3.m1.1b"><msub id="S4.T2.3.m1.1.1" xref="S4.T2.3.m1.1.1.cmml"><mi id="S4.T2.3.m1.1.1.2" xref="S4.T2.3.m1.1.1.2.cmml">F</mi><mn id="S4.T2.3.m1.1.1.3" xref="S4.T2.3.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.m1.1c"><apply id="S4.T2.3.m1.1.1.cmml" xref="S4.T2.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.m1.1.1.1.cmml" xref="S4.T2.3.m1.1.1">subscript</csymbol><ci id="S4.T2.3.m1.1.1.2.cmml" xref="S4.T2.3.m1.1.1.2">ğ¹</ci><cn type="float" id="S4.T2.3.m1.1.1.3.cmml" xref="S4.T2.3.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.m1.1d">F_{0.5}</annotation></semantics></math> between the monolingual version of our method and previous synthetic data generation techniques on L2 corpora.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Models and Fine-tuning setups</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data Generation</span> We use the No Language Left Behind (NLLB-200) model <cite class="ltx_cite ltx_citemacro_cite">Team etÂ al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite> which supports 202 languages as our generative mPLM. Specifically, we use NLLB 1.3B-distilled for all our experiments. Following <cite class="ltx_cite ltx_citemacro_citet">Luhtaru etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2024b</a>)</cite>, we train the model on non-tokenized text or detokenized if the non tokenized format is not available. Details regarding our hyperparameters can be found in Appendix <a href="#A1.SS2" title="A.2 Implementation details â€£ Appendix A Appendix â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Grammatical Error Detection</span> In line with <cite class="ltx_cite ltx_citemacro_cite">Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, we use XLM-RoBERTa-large, a multilingual pre-trained encoder with strong cross-lingual abilities <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> as our GED model. We evaluate two versions of our method: (1) A Monolingual version, where the GED model is exclusively trained on synthetic data from the target language, enabling direct comparison with existing synthetic data generation techniques. (2) A Multilingual version using our two-stage fine-tuning procedure to compare against DirectCLT.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Postprocessing</span> The postprocessing steps outlined in <a href="#S3" title="3 Method â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which transform synthetic corpora into GED corpora, necessitate tokenized text. To achieve this, we use Stanza <cite class="ltx_cite ltx_citemacro_cite">Qi etÂ al. (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> for Czech and Spacy <cite class="ltx_cite ltx_citemacro_cite">Honnibal etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> for Swedish and Italian. Following previous works on Arabic GEC <cite class="ltx_cite ltx_citemacro_cite">Belkebir and Habash (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>); Alhafni etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, we use CAMeL Tools <cite class="ltx_cite ltx_citemacro_cite">Obeid etÂ al. (<a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>. Lastly, for Chinese, we use the PKUNLP word segmentation tool provided in the NLPCC 2018 shared task <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><object type="image/svg+xml" data="/html/2407.11854/assets/images/pr_curves_flat.svg" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="173"></object>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Precision-Recall curves comparing our method in different data configurations to our baselines.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">F</mi><mn id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">F_{0.5}</annotation></semantics></math>(%)</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_left"><span id="S4.T3.1.2.1.1" class="ltx_text ltx_font_bold">Configuration</span></td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Swedish</td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Italian</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_left ltx_border_t">Czech</td>
<td id="S4.T3.1.2.5" class="ltx_td ltx_align_left ltx_border_t">Arabic</td>
<td id="S4.T3.1.2.6" class="ltx_td ltx_align_left ltx_border_t">Chinese</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.3.1.1" class="ltx_text ltx_font_smallcaps">DirectCLT</span></td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_left ltx_border_t">71.5</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_left ltx_border_t">63.8</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_left ltx_border_t">62.1</td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_align_left ltx_border_t">57.3</td>
<td id="S4.T3.1.3.6" class="ltx_td ltx_align_left ltx_border_t">36.2</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T3.1.4.1.1" class="ltx_text ltx_font_smallcaps">Ours</span></td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_left"><span id="S4.T3.1.4.2.1" class="ltx_text ltx_font_bold">74.7</span></td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_left">70.4</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_left">66.6</td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_align_left">62.8</td>
<td id="S4.T3.1.4.6" class="ltx_td ltx_align_left">42.9</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_left"><span id="S4.T3.1.5.1.1" class="ltx_text ltx_font_smallcaps">Ours from source</span></td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_left">72.5</td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_left">64.1</td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_left">62.9</td>
<td id="S4.T3.1.5.5" class="ltx_td ltx_align_left">58.4</td>
<td id="S4.T3.1.5.6" class="ltx_td ltx_align_left">36.5</td>
</tr>
<tr id="S4.T3.1.6" class="ltx_tr">
<td id="S4.T3.1.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.1.1" class="ltx_text ltx_font_smallcaps">Ours from target</span></td>
<td id="S4.T3.1.6.2" class="ltx_td ltx_align_left ltx_border_bb">74.2</td>
<td id="S4.T3.1.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.3.1" class="ltx_text ltx_font_bold">71.3</span></td>
<td id="S4.T3.1.6.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.4.1" class="ltx_text ltx_font_bold">67.3</span></td>
<td id="S4.T3.1.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.5.1" class="ltx_text ltx_font_bold">71.6</span></td>
<td id="S4.T3.1.6.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.6.1" class="ltx_text ltx_font_bold">47.9</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of <math id="S4.T3.3.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S4.T3.3.m1.1b"><msub id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml"><mi id="S4.T3.3.m1.1.1.2" xref="S4.T3.3.m1.1.1.2.cmml">F</mi><mn id="S4.T3.3.m1.1.1.3" xref="S4.T3.3.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><apply id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.m1.1.1.1.cmml" xref="S4.T3.3.m1.1.1">subscript</csymbol><ci id="S4.T3.3.m1.1.1.2.cmml" xref="S4.T3.3.m1.1.1.2">ğ¹</ci><cn type="float" id="S4.T3.3.m1.1.1.3.cmml" xref="S4.T3.3.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">F_{0.5}</annotation></semantics></math> of our method where first-stage fine-tuning is performed on various data configurations.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Proposed Method Evaluation</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison to State-of-the-Art</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 â€£ 3 Method â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the performance of our method compared to previous state-of-the-art. Our method establishes a new standard in GED without human annotations across all target languages, outperforming both synthetic data generation techniques and DirectCLT by a significant margin.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We posit that our superior performance can be attributed to the capability of our AEG method to produce a diverse set of errors including language-specific errors. This hypothesis is further examined in Section <a href="#S6" title="6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">It is worth mentioning that while our results represent a significant advancement, they still fall short of the state-of-the-art supervised settings. This result is expected and aligns with the existing literature in GED, which highlights notable discrepancies when evaluating supervised models with out-of-domain data, even if it originates from the same language as the training data <cite class="ltx_cite ltx_citemacro_cite">Volodina etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation of AEG</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As all previous work using AEG for GED has been in monolingual settings, we introduce a monolingual variant of our approach. Here, the GED model is exclusively fine-tuned on synthetic data from the target language.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p">Table <a href="#S4.T2" title="Table 2 â€£ 4.2 Baselines â€£ 4 Experimental Setup â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that our synthetic data generation technique achieves the best performance among annotation-free synthetic data generation methods applied to GED. Given that rule-based methods apply a set of transformations without considering the sentence context, the average improvement of 9.2 points of <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">F</mi><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">F_{0.5}</annotation></semantics></math> over these methods highlights the significance of generating context-dependent errors in synthetic data generation. Additionally, given that NAT is not trained to generate errors but to produce translations, outperforming this method by 8.3 points of <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><msub id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">F</mi><mn id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">ğ¹</ci><cn type="float" id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">F_{0.5}</annotation></semantics></math> highlights the advantage of learning to generate errors from authentic instances, even when these instances originate from different languages.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We hypothesize that the ability to synthesize context-dependent errors combined with the acquisition of error-generation insights from authentic instances empower our method to yield errors more akin to human errors, thus leading to better performance. We further analyze this hypothesis in <a href="#S6.SS1" title="6.1 Czech Case Study â€£ 6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Additionally, our monolingual setup outperforms DirectCLT in four out of five languages. This is a notable achievement given other synthetic data generation methodsâ€™ inability to meet this benchmark. Both approaches leverage the CLT of mPLMs, albeit differently: ours uses it for artificial error generation in target languages with a generative mPLM, while DirectCLT leverages it directly to perform error detection across target languages. This comparison suggests that our method creates tailored error patterns in target languages that a GED model trained only on source language annotations cannot detect, indicating that our approach to CLT in GED could generalize to other NLU tasks, which is a promising avenue for future research.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Language Ablation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We study the effect of changing the language configuration of the synthetic data. We compare fine-tuning the GED model using synthetic data comprising different language sets: exclusively source languages, exclusively target languages, and a combination of both source and target languages.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Results in Table <a href="#S4.T3" title="Table 3 â€£ 4.3 Models and Fine-tuning setups â€£ 4 Experimental Setup â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show that any first stage fine-tuning language configuration improves the GED performance of our method over the DirectCLT baseline, highlighting the robustness of our two-stage fine-tuning pipeline. Notably, including synthetic data from the target language results in a more significant improvement which emphasize the importance of using a language-agnostic artificial error generation method capable of generating errors in any target language.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.3" class="ltx_p">Furthermore, results from Table <a href="#S4.T3" title="Table 3 â€£ 4.3 Models and Fine-tuning setups â€£ 4 Experimental Setup â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> suggest that first-stage fine-tuning exclusively on synthetic data from target languages outperforms fine-tuning on a combination of source and target languages. However, comparing <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><msub id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">F</mi><mn id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">ğ¹</ci><cn type="float" id="S5.SS3.p3.1.m1.1.1.3.cmml" xref="S5.SS3.p3.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">F_{0.5}</annotation></semantics></math> scores does not reveal the big picture and can lead to false conclusion. The <math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><msub id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">F</mi><mn id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">ğ¹</ci><cn type="float" id="S5.SS3.p3.2.m2.1.1.3.cmml" xref="S5.SS3.p3.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">F_{0.5}</annotation></semantics></math> score is computed at an operation point that is usually arbitrarily set to 0.5 in the literature <cite class="ltx_cite ltx_citemacro_cite">Kasewa etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>); Colla etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>); Le-Hong etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. For a more comprehensive comparison of performance, Figure <a href="#S4.F2" title="Figure 2 â€£ 4.3 Models and Fine-tuning setups â€£ 4 Experimental Setup â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the Precision-Recall curves for each method. It shows that fine-tuning on either synthetic data from source and target languages or target languages alone yields similar results. We can conclude that the determining factor is the inclusion of synthetic data in the target language. We can also see that our method outperforms other baseline in the curves too. We encourage practitioners to use such figures to compare GED models for more meaningful conclusions than threshold dependant metrics such as <math id="S5.SS3.p3.3.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS3.p3.3.m3.1a"><mi id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><ci id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">F</annotation></semantics></math> scores.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">We experimented with reversing our fine-tuning pipeline by initially training on human annotations from our source languages followed by fine-tuning on synthetic data. However, this approach empirically yielded inferior performance. The fact that ending the fine-tuning process with human-annotated data, even in source languages, is more effective than using target language synthetic data indicates that artificial errors still do not reach the quality of authentic corpora. Otherwise it would make sense to end the training with errors specific to the target language. We hypothesize that improved synthetic error generation techniques would lead to opposite conclusions regarding the fine-tuning order.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><object type="image/svg+xml" data="/html/2407.11854/assets/images/nb_lang.svg" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="209"></object>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Relative improvement in terms of <math id="S5.F3.2.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.F3.2.m1.1b"><msub id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml"><mi id="S5.F3.2.m1.1.1.2" xref="S5.F3.2.m1.1.1.2.cmml">F</mi><mn id="S5.F3.2.m1.1.1.3" xref="S5.F3.2.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><apply id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1"><csymbol cd="ambiguous" id="S5.F3.2.m1.1.1.1.cmml" xref="S5.F3.2.m1.1.1">subscript</csymbol><ci id="S5.F3.2.m1.1.1.2.cmml" xref="S5.F3.2.m1.1.1.2">ğ¹</ci><cn type="float" id="S5.F3.2.m1.1.1.3.cmml" xref="S5.F3.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">F_{0.5}</annotation></semantics></math> score compared to English-only fine-tuning as additional source languages are incorporated.</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Scalability</h3>

<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T4.2.2" class="ltx_tr">
<td id="S5.T4.2.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Czech <math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="L1" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml"><mi id="S5.T4.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S5.T4.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"><times id="S5.T4.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1.1"></times><ci id="S5.T4.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="S5.T4.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">L1</annotation></semantics></math>
</td>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_tt">Arabic <math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="L1" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mrow id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml"><mi id="S5.T4.2.2.2.m1.1.1.2" xref="S5.T4.2.2.2.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.T4.2.2.2.m1.1.1.1" xref="S5.T4.2.2.2.m1.1.1.1.cmml">â€‹</mo><mn id="S5.T4.2.2.2.m1.1.1.3" xref="S5.T4.2.2.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><apply id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"><times id="S5.T4.2.2.2.m1.1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1.1"></times><ci id="S5.T4.2.2.2.m1.1.1.2.cmml" xref="S5.T4.2.2.2.m1.1.1.2">ğ¿</ci><cn type="integer" id="S5.T4.2.2.2.m1.1.1.3.cmml" xref="S5.T4.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">L1</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.2.3" class="ltx_tr">
<td id="S5.T4.2.3.1" class="ltx_td ltx_align_left ltx_border_t">RT translation</td>
<td id="S5.T4.2.3.2" class="ltx_td ltx_align_left ltx_border_t">20.2</td>
<td id="S5.T4.2.3.3" class="ltx_td ltx_align_left ltx_border_t">38.7</td>
</tr>
<tr id="S5.T4.2.4" class="ltx_tr">
<td id="S5.T4.2.4.1" class="ltx_td ltx_align_left">Rules</td>
<td id="S5.T4.2.4.2" class="ltx_td ltx_align_left">26.5</td>
<td id="S5.T4.2.4.3" class="ltx_td ltx_align_left">32.9</td>
</tr>
<tr id="S5.T4.2.5" class="ltx_tr">
<td id="S5.T4.2.5.1" class="ltx_td ltx_align_left">NAT</td>
<td id="S5.T4.2.5.2" class="ltx_td ltx_align_left">38.0</td>
<td id="S5.T4.2.5.3" class="ltx_td ltx_align_left">48.9</td>
</tr>
<tr id="S5.T4.2.6" class="ltx_tr">
<td id="S5.T4.2.6.1" class="ltx_td ltx_align_left">DirectCLT</td>
<td id="S5.T4.2.6.2" class="ltx_td ltx_align_left">41.7</td>
<td id="S5.T4.2.6.3" class="ltx_td ltx_align_left">45.5</td>
</tr>
<tr id="S5.T4.2.7" class="ltx_tr">
<td id="S5.T4.2.7.1" class="ltx_td ltx_align_left ltx_border_bb">Ours</td>
<td id="S5.T4.2.7.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T4.2.7.2.1" class="ltx_text ltx_font_bold">41.8</span></td>
<td id="S5.T4.2.7.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T4.2.7.3.1" class="ltx_text ltx_font_bold">63.2</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><math id="S5.T4.4.m1.1" class="ltx_Math" alttext="F_{0.5}" display="inline"><semantics id="S5.T4.4.m1.1b"><msub id="S5.T4.4.m1.1.1" xref="S5.T4.4.m1.1.1.cmml"><mi id="S5.T4.4.m1.1.1.2" xref="S5.T4.4.m1.1.1.2.cmml">F</mi><mn id="S5.T4.4.m1.1.1.3" xref="S5.T4.4.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T4.4.m1.1c"><apply id="S5.T4.4.m1.1.1.cmml" xref="S5.T4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.4.m1.1.1.1.cmml" xref="S5.T4.4.m1.1.1">subscript</csymbol><ci id="S5.T4.4.m1.1.1.2.cmml" xref="S5.T4.4.m1.1.1.2">ğ¹</ci><cn type="float" id="S5.T4.4.m1.1.1.3.cmml" xref="S5.T4.4.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.m1.1d">F_{0.5}</annotation></semantics></math> (%) on out-of-domain L1 corpora.</figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Here we investigate how our synthetic data generation method scales as new languages corpora become available. We fine-tune the AEG model by progressively incorporating new languages in different orders to an English-only fine-tuned baseline. We follow the protocol of <cite class="ltx_cite ltx_citemacro_citet">Shaham etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2024</a>)</cite>. We report average scores per target language of a GED model fine-tuned on monolingual synthetic data.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 â€£ 5.3 Language Ablation â€£ 5 Proposed Method Evaluation â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that on average, performance increases with the number of source languages. This suggests that our synthetic data generation method applied to GED might continue to improve as new GED corpora become available.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Generalization to out-of-domain errors</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Errors vary between different populations. For instance native speakers (L1) do not commit the same type of errors than second language learners (L2). We investigate the robustness of our method to different error distributions. Our method is trained on L2 learner corpora and we evaluate it on L1 data. We found available GED annotated data of L1 speakers for Arabic and Czech: QALB 2014 <cite class="ltx_cite ltx_citemacro_cite">Mohit etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2014</a>)</cite> and the Native Formal section of GECCC <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 â€£ 5.4 Scalability â€£ 5 Proposed Method Evaluation â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results. Our method surpasses all other baselines, demonstrating its continued suitability for out-of-domain corpora in the target language. Unlike the other baselines, our method achieves approximately similar performance on both L1 and L2 Arabic corpora. However, for Czech, all methods show a significant decrease in performance.
We hypothesize that this is due to the unique stringent rules regarding the use of commas in Czech. This results in the predominance of "Punctuation" errors in the L1 Czech corpora, which are less common in many other languages, and therefore amplify the difference between domains.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis of synthetic errors</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We compare the errors produced by the AEG methods. We first study Czech using a Czech extension <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> of the ERRANT <cite class="ltx_cite ltx_citemacro_cite">Bryant etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> error annotation tool and an artificial vs human error discriminator. We then extend our analysis to many languages using GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2024</a>)</cite> to classify error types.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T5.1.1" class="ltx_tr">
<td id="S6.T5.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S6.T5.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Precision</td>
<td id="S6.T5.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">Recall</td>
<td id="S6.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><math id="S6.T5.1.1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S6.T5.1.1.1.m1.1a"><msub id="S6.T5.1.1.1.m1.1.1" xref="S6.T5.1.1.1.m1.1.1.cmml"><mi id="S6.T5.1.1.1.m1.1.1.2" xref="S6.T5.1.1.1.m1.1.1.2.cmml">F</mi><mn id="S6.T5.1.1.1.m1.1.1.3" xref="S6.T5.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.m1.1b"><apply id="S6.T5.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.1.1.1.m1.1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T5.1.1.1.m1.1.1.2.cmml" xref="S6.T5.1.1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S6.T5.1.1.1.m1.1.1.3.cmml" xref="S6.T5.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.m1.1c">F_{1}</annotation></semantics></math></td>
</tr>
<tr id="S6.T5.1.2" class="ltx_tr">
<td id="S6.T5.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Rules</td>
<td id="S6.T5.1.2.2" class="ltx_td ltx_align_left ltx_border_t">96.5</td>
<td id="S6.T5.1.2.3" class="ltx_td ltx_align_left ltx_border_t">95.2</td>
<td id="S6.T5.1.2.4" class="ltx_td ltx_align_left ltx_border_t">96.6</td>
</tr>
<tr id="S6.T5.1.3" class="ltx_tr">
<td id="S6.T5.1.3.1" class="ltx_td ltx_align_left">NAT</td>
<td id="S6.T5.1.3.2" class="ltx_td ltx_align_left">94.3</td>
<td id="S6.T5.1.3.3" class="ltx_td ltx_align_left">97.2</td>
<td id="S6.T5.1.3.4" class="ltx_td ltx_align_left">95.2</td>
</tr>
<tr id="S6.T5.1.4" class="ltx_tr">
<td id="S6.T5.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">Ours</td>
<td id="S6.T5.1.4.2" class="ltx_td ltx_align_left ltx_border_bb">79.1</td>
<td id="S6.T5.1.4.3" class="ltx_td ltx_align_left ltx_border_bb">88.3</td>
<td id="S6.T5.1.4.4" class="ltx_td ltx_align_left ltx_border_bb">83.4</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of a binary classifier trained to distinguish between human errors and errors produced by a synthetic data generation technique. We report the Precision, Recall and <math id="S6.T5.3.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S6.T5.3.m1.1b"><msub id="S6.T5.3.m1.1.1" xref="S6.T5.3.m1.1.1.cmml"><mi id="S6.T5.3.m1.1.1.2" xref="S6.T5.3.m1.1.1.2.cmml">F</mi><mn id="S6.T5.3.m1.1.1.3" xref="S6.T5.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T5.3.m1.1c"><apply id="S6.T5.3.m1.1.1.cmml" xref="S6.T5.3.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.3.m1.1.1.1.cmml" xref="S6.T5.3.m1.1.1">subscript</csymbol><ci id="S6.T5.3.m1.1.1.2.cmml" xref="S6.T5.3.m1.1.1.2">ğ¹</ci><cn type="integer" id="S6.T5.3.m1.1.1.3.cmml" xref="S6.T5.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.m1.1d">F_{1}</annotation></semantics></math> score.</figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Czech Case Study</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Similarity Analysis with Human Errors</span> To assess if the synthetic instances are realistic and human-like, we train a binary classifier (one per synthetic data generation technique) to distinguish between errors generated by a particular synthetic data generation method and human errors. We constructed a development set comprising approximately equal numbers of authentic and synthetic data and assessed performance using the <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><msub id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">F</mi><mn id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S6.SS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">F_{1}</annotation></semantics></math> score. More information on how we train the classifier can be found in <a href="#A1.SS3" title="A.3 Similarity Analysis details â€£ Appendix A Appendix â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>. Results are presented in Table <a href="#S6.T5" title="Table 5 â€£ 6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.2" class="ltx_p">Our classifier achieves an <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><msub id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">F</mi><mn id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">F_{1}</annotation></semantics></math> score of 83.4% for the proposed method, indicating a moderate ability to differentiate between synthetic and human errors. This supports our hypothesis that our synthetic data generation method does not fully replicate the quality of authentic sentences. In contrast, the classifier achieves an <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><msub id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mi id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">F</mi><mn id="S6.SS1.p2.2.m2.1.1.3" xref="S6.SS1.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S6.SS1.p2.2.m2.1.1.3.cmml" xref="S6.SS1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">F_{1}</annotation></semantics></math> score exceeding 95% for other synthetic data generation methods, suggesting a higher degree of differentiation. Overall, this suggests that our method produces errors that are more human-like, translating into better downstream performance.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><object type="image/svg+xml" data="/html/2407.11854/assets/images/czech_errant.svg" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="174"></object>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top 10 error types distribution of different annotation-free synthetic data generation methods.</figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Error Distribution</span> We use the Czech extension <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> of ERRANT to categorize the errors made by different systems. Figure <a href="#S6.F4" title="Figure 4 â€£ 6.1 Czech Case Study â€£ 6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the distribution of the top 10 error types for the various synthetic data generation methods studied. Our method produces a more diverse set of errors compared to NAT <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> and rule-based approaches <cite class="ltx_cite ltx_citemacro_cite">Grundkiewicz and Junczys-Dowmunt (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>. Notably, while other methods predominantly yield â€™Otherâ€™ and â€™Spellâ€™ error types, our method features a more balanced distribution of error types, indicating that our method is more effective in mimicking the complexity and range of human language errors.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Additionally, our method generates a higher percentage of â€™DIACRâ€™ errors compared to other techniques. Since â€™DIACRâ€™ errors are the most common among L2 learners of Czech, this could explain the performance improvements of our method.
Given that â€™DIACRâ€™ errors are specific to Czech <cite class="ltx_cite ltx_citemacro_cite">NÃ¡plava etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> in the set of languages we study, this indicates that our method can produce error types not encountered during the fine-tuning on source languages of our generative mPLM.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><object type="image/svg+xml" data="/html/2407.11854/assets/images/gpt_errant.svg" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="244"></object>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Normalized Entropy comparison of authentic and synthetic errors aggregated over different datasets.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Multilingual Extension</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We want to extend our previous findings by assessing if our synthetic data generation method effectively captures a variety of error types across all languages. For this, we need a language-agnostic classifier.
We use GPT-4 to classify errors from various sources across all the languages under investigation.
Prior studies have shown that GPT-4â€™s judgments align closely with human evaluations <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>); Fu etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> and exhibit promising error correction capabilities <cite class="ltx_cite ltx_citemacro_cite">Fang etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>); Davis etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>); Wu etÂ al. (<a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite>. Although a thorough assessment of GPT-4 for error classification is beyond the scope of the study, we performed a limited qualitative analysis of GPT-4â€™s accuracy in Italian, Swedish, Spanish, and English with native speakers. We found that it is suitable for our application. For each type of error classified by GPT-4 we compute its frequency distribution across data and compute the entropy of this distribution. Further details on our evaluation methodology are provided in Appendix <a href="#A1.SS4" title="A.4 GPT-4 analysis details â€£ Appendix A Appendix â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Figure <a href="#S6.F5" title="Figure 5 â€£ 6.1 Czech Case Study â€£ 6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> validates our previous findings that our method generates a more diverse set of errors compared to NAT. However, the range of error types generated by our method is narrower than that produced by humans. Moreover, the variability in the diversity of error types is significantly higher with our method than with human errors across different languages. This suggests that our method does not consistently perform across languages.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We introduced a novel zero-shot approach for GED with low-resource languages. Our method combines back-translation with the CLT capabilities of mPLMs to perform AEG across various target languages. Then, we fine-tune the GED model in two steps: first on multilingual synthetic data from source and target languages, then on human-annotated source language corpora. This method achieves state-of-the-art performance in annotation-free GED. Our error analysis shows that we produce errors that are more diverse and human-like than the baselines.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In future work, we intend to explore the potential of our GED models to enhance unsupervised GEC methods.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Our approach relies on the CLT capabilities obtained during the multilingual unsupervised pre-training of mPLMs. Consequently, the applicability of our method is restricted to the languages supported by the mPLM. Furthermore, its performance on each language may vary depending on the amount of pre-training data available for that language. This limitation is inherent to all studies leveraging mPLMs.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Additionally, our study primarily focuses on the errors made by second language learners. While we have analyzed the performance of our method on native language corpora, it would be valuable to evaluate its generalizability to other domains within a language. For instance, this includes errors made in casual text messaging or by machine translation systems.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Compared to the direct application of CLT in GED, our method involves additional steps such as training a generative mPLM and generating a substantial amount of synthetic data. These requirements may pose challenges for researchers with limited computational resources and could limit the practicality of developing this approach in resource-constrained environments. To address this constraint, we have made available a synthetic GED corpus encompassing more than 5 million samples across 11 languages.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ethics Statement</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Our research is driven by a commitment to supporting and preserving linguistic diversity. Low-resource languages often face marginalization in the realm of technological advancements. By developing GED models for these languages, we aim to enhance their digital presence and usability, thus promoting linguistic equity.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">However, it is important to acknowledge potential ethical concerns. The use of CLT to generate synthetic data, while beneficial for training GED models, carries the risk of misuse. Such systems could potentially be exploited to create false information or propaganda in low-resource languages. Additionally, while GED systems are crucial for regions with a shortage of language teachers, there is a risk that their widespread use could lead to an over-reliance on these tools. This dependency might result in a decline in the linguistic and grammatical skills of native speakers, as they become more reliant on technology for language correction and validation.</p>
</div>
<div id="S9.p3" class="ltx_para">
<p id="S9.p3.1" class="ltx_p">It is essential for future users to use these technologies judiciously. Balancing the use of GED tools with a genuine effort to improve oneâ€™s linguistic abilities is crucial. Building on the research by <cite class="ltx_cite ltx_citemacro_citet">Fei etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> could provide a valuable advancement by incorporating explainability into our GED systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alhafni etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bashar Alhafni, GoÂ Inoue, Christian Khairallah, and Nizar Habash. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.396" title="" class="ltx_ref ltx_href">Advancements in Arabic grammatical error detection and correction: An empirical investigation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 6430â€“6448, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alikaniotis and Raheja (2019)</span>
<span class="ltx_bibblock">
Dimitris Alikaniotis and Vipul Raheja. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-4412" title="" class="ltx_ref ltx_href">The unreasonable effectiveness of transformer language models in grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 127â€“133, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ArnardÃ³ttir etÂ al. (2021)</span>
<span class="ltx_bibblock">
ÃÃ³runn ArnardÃ³ttir, Xindan Xu, DagbjÃ¶rt GuÃ°mundsdÃ³ttir, LiljaÂ BjÃ¶rk StefÃ¡nsdÃ³ttir, and AntonÂ Karl Ingason. 2021.

</span>
<span class="ltx_bibblock">Creating an error corpus: Annotation and applicability.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of CLARIN Annual Conference</em>, pages 59â€“63.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awasthi etÂ al. (2019)</span>
<span class="ltx_bibblock">
Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, and Vihari Piratla. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1435" title="" class="ltx_ref ltx_href">Parallel iterative edit models for local sequence transduction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 4260â€“4270, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belkebir and Habash (2021)</span>
<span class="ltx_bibblock">
Riadh Belkebir and Nizar Habash. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.conll-1.47" title="" class="ltx_ref ltx_href">Automatic error type annotation for Arabic</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th Conference on Computational Natural Language Learning</em>, pages 596â€“606, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bell etÂ al. (2019)</span>
<span class="ltx_bibblock">
Samuel Bell, Helen Yannakoudakis, and Marek Rei. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-4410" title="" class="ltx_ref ltx_href">Context is key: Grammatical error detection with contextual word representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 103â€“115, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boyd (2018)</span>
<span class="ltx_bibblock">
Adriane Boyd. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W18-6111" title="" class="ltx_ref ltx_href">Using Wikipedia edits in low resource grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</em>, pages 79â€“84, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boyd etÂ al. (2014)</span>
<span class="ltx_bibblock">
Adriane Boyd, Jirka Hana, Lionel Nicolas, Detmar Meurers, Katrin Wisniewski, Andrea Abel, Karin SchÃ¶ne, Barbora Å tindlovÃ¡, and Chiara Vettori. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/606_Paper.pdf" title="" class="ltx_ref ltx_href">The MERLIN corpus: Learner language and the CEFR</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LRECâ€™14)</em>, pages 1281â€“1288, Reykjavik, Iceland. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bryant etÂ al. (2017)</span>
<span class="ltx_bibblock">
Christopher Bryant, Mariano Felice, and Ted Briscoe. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1074" title="" class="ltx_ref ltx_href">Automatic annotation and evaluation of error types for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 793â€“805, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hannan Cao, Wenmian Yang, and HweeÂ Tou Ng. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.eacl-main.155" title="" class="ltx_ref ltx_href">Mitigating exposure bias in grammatical error correction with data augmentation and reweighting</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 2123â€“2135, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hannan Cao, Liping Yuan, Yuchen Zhang, and HweeÂ Tou Ng. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.185" title="" class="ltx_ref ltx_href">Unsupervised grammatical error correction rivaling supervised methods</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 3072â€“3088, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zewen Chi, LiÂ Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.280" title="" class="ltx_ref ltx_href">InfoXLM: An information-theoretic framework for cross-lingual language model pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3576â€“3588, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirkova and Nikoulina (2024)</span>
<span class="ltx_bibblock">
Nadezhda Chirkova and Vassilina Nikoulina. 2024.

</span>
<span class="ltx_bibblock">Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.12279</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Colla etÂ al. (2023)</span>
<span class="ltx_bibblock">
Davide Colla, Matteo Delsanto, and Elisa DiÂ Nuovo. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.nlp4call-1.3" title="" class="ltx_ref ltx_href">EliCoDe at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning</em>, pages 24â€“34, TÃ³rshavn, Faroe Islands. LiU Electronic Press.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="" class="ltx_ref ltx_href">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440â€“8451, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1269" title="" class="ltx_ref ltx_href">XNLI: Evaluating cross-lingual sentence representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2475â€“2485, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahlmeier and Ng (2012)</span>
<span class="ltx_bibblock">
Daniel Dahlmeier and HweeÂ Tou Ng. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/N12-1067" title="" class="ltx_ref ltx_href">Better evaluation for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 568â€“572, MontrÃ©al, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson etÂ al. (2020)</span>
<span class="ltx_bibblock">
Sam Davidson, Aaron Yamada, Paloma FernandezÂ Mira, Agustina Carando, ClaudiaÂ H. SanchezÂ Gutierrez, and Kenji Sagae. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.lrec-1.894" title="" class="ltx_ref ltx_href">Developing NLP tools with a new corpus of learner Spanish</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pages 7238â€“7243, Marseille, France. European Language Resources Association.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis etÂ al. (2024)</span>
<span class="ltx_bibblock">
Christopher Davis, Andrew Caines, Ã˜istein Andersen, Shiva Taslimipoor, Helen Yannakoudakis, Zheng Yuan, Christopher Bryant, Marek Rei, and Paula Buttery. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2401.07702" title="" class="ltx_ref ltx_href">Prompting open-source and commercial language models for grammatical error correction of english learner text</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171â€“4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tao Fang, Shu Yang, Kaixin Lan, DerekÂ F. Wong, Jinpeng Hu, LidiaÂ S. Chao, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.01746" title="" class="ltx_ref ltx_href">Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuejiao Fei, Leyang Cui, Sen Yang, Wai Lam, Zhenzhong Lan, and Shuming Shi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.413" title="" class="ltx_ref ltx_href">Enhancing grammatical error correction systems with explanations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7489â€“7501, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2302.04166" title="" class="ltx_ref ltx_href">Gptscore: Evaluate as you desire</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gamon (2011)</span>
<span class="ltx_bibblock">
Michael Gamon. 2011.

</span>
<span class="ltx_bibblock">High-order sequence modeling for language learner error detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 180â€“189.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grundkiewicz and Junczys-Dowmunt (2019)</span>
<span class="ltx_bibblock">
Roman Grundkiewicz and Marcin Junczys-Dowmunt. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-5546" title="" class="ltx_ref ltx_href">Minimally-augmented grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</em>, pages 357â€“363, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grundkiewicz etÂ al. (2019)</span>
<span class="ltx_bibblock">
Roman Grundkiewicz, Marcin Junczys-Dowmunt, and Kenneth Heafield. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-4427" title="" class="ltx_ref ltx_href">Neural grammatical error correction systems with unsupervised pre-training on synthetic data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 252â€“263, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.09543" title="" class="ltx_ref ltx_href">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honnibal etÂ al. (2020)</span>
<span class="ltx_bibblock">
Matthew Honnibal, Ines Montani, Sofie VanÂ Landeghem, and Adriane Boyd. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.1212303" title="" class="ltx_ref ltx_href">spaCy: Industrial-strength Natural Language Processing in Python</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaneko and Komachi (2019)</span>
<span class="ltx_bibblock">
Masahiro Kaneko and Mamoru Komachi. 2019.

</span>
<span class="ltx_bibblock">Multi-head multi-layer attention to deep language representations for grammatical error detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ComputaciÃ³n y Sistemas</em>, 23(3):883â€“891.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasewa etÂ al. (2018)</span>
<span class="ltx_bibblock">
Sudhanshu Kasewa, Pontus Stenetorp, and Sebastian Riedel. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1541" title="" class="ltx_ref ltx_href">Wronging a right: Generating better errors to improve grammatical error detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 4977â€“4983, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiyono etÂ al. (2019)</span>
<span class="ltx_bibblock">
Shun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizumoto, and Kentaro Inui. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1119" title="" class="ltx_ref ltx_href">An empirical study of incorporating pseudo data into grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1236â€“1242, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2005.mtsummit-papers.11" title="" class="ltx_ref ltx_href">Europarl: A parallel corpus for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit X: Papers</em>, pages 79â€“86, Phuket, Thailand.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le-Hong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Phuong Le-Hong, TheÂ Quyen Ngo, and Thi MinhÂ Huyen Nguyen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.nlp4call-1.5" title="" class="ltx_ref ltx_href">Two neural models for multilingual grammatical error detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning</em>, pages 40â€“44, TÃ³rshavn, Faroe Islands. LiU Electronic Press.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yinghao Li, Xuebo Liu, Shuo Wang, Peiyuan Gong, DerekÂ F. Wong, Yang Gao, Heyan Huang, and Min Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.380" title="" class="ltx_ref ltx_href">TemplateGEC: Improving grammatical error correction with detection template</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 6878â€“6892, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lichtarge etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki Parmar, and Simon Tong. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1333" title="" class="ltx_ref ltx_href">Corpora generation for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 3291â€“3301, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez Latouche etÂ al. (2024)</span>
<span class="ltx_bibblock">
GaÃ«tan Lopez Latouche, Marc-AndrÃ© Carbonneau, and Ben Swanson. 2024.

</span>
<span class="ltx_bibblock">Binaryalign: Word alignment as binary sequence labeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luhtaru etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Agnes Luhtaru, Elizaveta Korotkova, and Mark Fishel. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.eacl-long.73" title="" class="ltx_ref ltx_href">No error left behind: Multilingual grammatical error correction with pre-trained translation models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1209â€“1222, St. Julianâ€™s, Malta. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luhtaru etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, and Mark Fishel. 2024b.

</span>
<span class="ltx_bibblock">To err is human, but llamas can learn it too.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05493</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohit etÂ al. (2014)</span>
<span class="ltx_bibblock">
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/W14-3605" title="" class="ltx_ref ltx_href">The first QALB shared task on automatic text correction for Arabic</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP)</em>, pages 39â€“47, Doha, Qatar. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NÃ¡plava etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jakub NÃ¡plava, Milan Straka, Jana StrakovÃ¡, and Alexandr Rosen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00470" title="" class="ltx_ref ltx_href">Czech grammar error correction with a large and diverse corpus</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:452â€“467.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Obeid etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ossama Obeid, Nasser Zalmout, Salam Khalifa, Dima Taji, Mai Oudah, Bashar Alhafni, GoÂ Inoue, Fadhl Eryani, Alexander Erdmann, and Nizar Habash. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.lrec-1.868" title="" class="ltx_ref ltx_href">CAMeL tools: An open source python toolkit for Arabic natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pages 7022â€“7032, Marseille, France. European Language Resources Association.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI etÂ al. (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, HyungÂ Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³nÂ Posada Fishman, Juston Forte, Isabella Fulford, Leo
Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, ShixiangÂ Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, NitishÂ Shirish Keskar, Tabarak Khan, Logan Kilpatrick, JongÂ Wook Kim, Christina Kim, Yongjik Kim, JanÂ Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Åukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, ChakÂ Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan
Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, ScottÂ Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen Oâ€™Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe deÂ Avila BelbuteÂ Peres, Michael Petrov, HenriqueÂ Ponde deÂ OliveiraÂ Pinto, Michael, Pokorny, Michelle Pokrass, VitchyrÂ H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, FelipeÂ Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, MadeleineÂ B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan FelipeÂ CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, JustinÂ Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJÂ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and ChristopherÂ D. Manning. 2020.

</span>
<span class="ltx_bibblock">Stanza: A Python natural language processing toolkit for many human languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei and Yannakoudakis (2016)</span>
<span class="ltx_bibblock">
Marek Rei and Helen Yannakoudakis. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1112" title="" class="ltx_ref ltx_href">Compositional sequence labeling models for error detection in learner writing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1181â€“1191, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1908.10084" title="" class="ltx_ref ltx_href">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rothe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-short.89" title="" class="ltx_ref ltx_href">A simple recipe for multilingual grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 702â€“707, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozovskaya etÂ al. (2015)</span>
<span class="ltx_bibblock">
Alla Rozovskaya, Houda Bouamor, Nizar Habash, Wajdi Zaghouani, Ossama Obeid, and Behrang Mohit. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W15-3204" title="" class="ltx_ref ltx_href">The second QALB shared task on automatic text correction for Arabic</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Workshop on Arabic Natural Language Processing</em>, pages 26â€“35, Beijing, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozovskaya and Roth (2019)</span>
<span class="ltx_bibblock">
Alla Rozovskaya and Dan Roth. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00251" title="" class="ltx_ref ltx_href">Grammar error correction in morphologically rich languages: The case of Russian</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:1â€“17.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rummo and Praakli (2017)</span>
<span class="ltx_bibblock">
Ingrid Rummo and Kristiina Praakli. 2017.

</span>
<span class="ltx_bibblock">Tu eesti keele (voorkeelena) osakonna oppijakeele tekstikorpus [the language learners corpus of the department of estonian language of the university of tartu].

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proc EAAL</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaham etÂ al. (2024)</span>
<span class="ltx_bibblock">
Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024.

</span>
<span class="ltx_bibblock">Multilingual instruction tuning with just a pinch of multilinguality.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.01854</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahlberg and Kumar (2021)</span>
<span class="ltx_bibblock">
Felix Stahlberg and Shankar Kumar. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2021.bea-1.4" title="" class="ltx_ref ltx_href">Synthetic data generation for grammatical error correction with tagged corruption models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 37â€“47, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei, and Houfeng Wang. 2022.

</span>
<span class="ltx_bibblock">A unified strategy for multilingual grammatical error correction with pre-trained cross-lingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.10707</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutter PessurnoÂ de Carvalho (2024)</span>
<span class="ltx_bibblock">
Gustavo Sutter PessurnoÂ de Carvalho. 2024.

</span>
<span class="ltx_bibblock">Multilingual grammatical error detection and its applications to prompt-based correction.

</span>
<span class="ltx_bibblock">Masterâ€™s thesis, University of Waterloo.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team etÂ al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, MartaÂ R Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, etÂ al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation (2022).

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2207.04672</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020)</span>
<span class="ltx_bibblock">
JÃ¶rg Tiedemann and Santhosh Thottingal. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.eamt-1.61" title="" class="ltx_ref ltx_href">OPUS-MT â€“ building open translation services for the world</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 479â€“480, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Volodina etÂ al. (2023)</span>
<span class="ltx_bibblock">
Elena Volodina, Christopher Bryant, Andrew Caines, OrphÃ©e DeÂ Clercq, Jennifer-Carmen Frey, Elizaveta Ershova, Alexandr Rosen, and Olga Vinogradova. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.nlp4call-1.1" title="" class="ltx_ref ltx_href">MultiGED-2023 shared task at NLP4CALL: Multilingual grammatical error detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning</em>, pages 1â€“16, TÃ³rshavn, Faroe Islands. LiU Electronic Press.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Volodina etÂ al. (2019)</span>
<span class="ltx_bibblock">
Elena Volodina, Lena Granstedt, Arild Matsson, BeÃ¡ta Megyesi, IldikÃ³ PilÃ¡n, Julia Prentice, Dan RosÃ©n, Lisa Rudebeck, Carl-Johan SchenstrÃ¶m, GunlÃ¶g Sundberg, etÂ al. 2019.

</span>
<span class="ltx_bibblock">The swell language learner corpus: From design to annotation.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Northern European Journal of Language Technology (NEJLT)</em>, 6:67â€“104.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.newsum-1.1" title="" class="ltx_ref ltx_href">Is ChatGPT a good NLG evaluator? a preliminary study</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th New Frontiers in Summarization Workshop</em>, pages 1â€“11, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.13648" title="" class="ltx_ref ltx_href">Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, and Ahmad Beirami. 2024.

</span>
<span class="ltx_bibblock">Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.12318</em>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.41" title="" class="ltx_ref ltx_href">mT5: A massively multilingual pre-trained text-to-text transformer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 483â€“498, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamashita etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ikumi Yamashita, Satoru Katsumata, Masahiro Kaneko, Aizhan Imankulova, and Mamoru Komachi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.coling-main.415" title="" class="ltx_ref ltx_href">Cross-lingual transfer learning for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 4704â€“4715, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yannakoudakis etÂ al. (2011)</span>
<span class="ltx_bibblock">
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/P11-1019" title="" class="ltx_ref ltx_href">A new dataset and method for automatically grading ESOL texts</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, pages 180â€“189, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga etÂ al. (2021)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Jure Leskovec, and Percy Liang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.611" title="" class="ltx_ref ltx_href">LM-critic: Language models for unsupervised grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 7752â€“7763, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zheng Yuan, Shiva Taslimipoor, Christopher Davis, and Christopher Bryant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.687" title="" class="ltx_ref ltx_href">Multi-class grammatical error detection for correction: A tale of two systems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 8722â€“8736, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li, BoÂ Zhang, Chen Li, Fei Huang, and Min Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.227" title="" class="ltx_ref ltx_href">MuCGEC: a multi-reference multi-source evaluation dataset for Chinese grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3118â€“3130, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun Wan. 2018.

</span>
<span class="ltx_bibblock">Overview of the nlpcc 2018 shared task: Grammatical error correction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Natural Language Processing and Chinese Computing: 7th CCF International Conference, NLPCC 2018, Hohhot, China, August 26â€“30, 2018, Proceedings, Part II 7</em>, pages 439â€“445. Springer.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2023)</span>
<span class="ltx_bibblock">
Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang, BoÂ Zhang, Chen Li, JiÂ Zhang, and Fei Huang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.495" title="" class="ltx_ref ltx_href">Improving Seq2Seq grammatical error correction via decoding interventions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 7393â€“7405, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziemski etÂ al. (2016)</span>
<span class="ltx_bibblock">
Micha
<br class="ltx_break">l Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/L16-1561" title="" class="ltx_ref ltx_href">The United Nations parallel corpus v1.0</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LRECâ€™16)</em>, pages 3530â€“3534, PortoroÅ¾, Slovenia. European Language Resources Association (ELRA).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Baselines</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p"><span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_bold">Rules</span> We re-implemented <cite class="ltx_cite ltx_citemacro_citet">Grundkiewicz and Junczys-Dowmunt (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> using Aspell dictionaries<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://aspell.net/</span></span></span> for the replacement operation.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para ltx_noindent">
<p id="A1.SS1.p2.1" class="ltx_p"><span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_bold">NAT</span> We replicated the NAT model using InfoXLM <cite class="ltx_cite ltx_citemacro_cite">Chi etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> and English as source language, following <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> methodology. For non-autoregressive translation generation, we used Europarl <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib32" title="" class="ltx_ref">2005</a>)</cite> for Italian, Swedish and Czech and the UN Parallel Corpus v1.0 <cite class="ltx_cite ltx_citemacro_cite">Ziemski etÂ al. (<a href="#bib.bib69" title="" class="ltx_ref">2016</a>)</cite> for Arabic and Chinese. We conducted hyper-parameter tuning for the NAT-based data construction by exploring the parameter set specified in <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> and selected the optimal parameters for each language based on performance on the development set.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para ltx_noindent">
<p id="A1.SS1.p3.1" class="ltx_p"><span id="A1.SS1.p3.1.1" class="ltx_text ltx_font_bold">RT translation</span> We use OPUS-MT <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a href="#bib.bib55" title="" class="ltx_ref">2020</a>)</cite> as our translation model and English as the bridge language.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation details</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p"><span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_bold">Artificial error generation</span> We use two distinct AEG models to generate errors in target and source languages, both based on NLL 1.3B-distilled but trained with different hyper-parameters.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">For synthetic data generation in target languages, we conduct preliminary grid searches on the Swedish development set to determine the optimal hyperparameters. We select the learning rate from {1e-4, 5e-4, 1e-5, 5e-5} and the number of epochs from {3, 5, 10, 15, 20}. Ultimately, we set the learning rate to 1e-5 and fine-tune for 3 epochs with a batch size of 24 and a linear scheduler.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p">For synthetic data generation in source languages, we use a different set of hyper-parameters based on grid searches on the English development set. The learning rate is set to 1e-4, and we fine-tune for 10 epochs with a batch size of 24 and a linear scheduler.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para ltx_noindent">
<p id="A1.SS2.p4.1" class="ltx_p"><span id="A1.SS2.p4.1.1" class="ltx_text ltx_font_bold">Grammatical error detection</span> Based on initial experiments with the Swedish development set, we use a learning rate of 1e-5, a batch size of 24, and train for 5 epochs with a linear scheduler. In our second-stage experiments, we maintain the same setup but fine-tune for only 1 epoch.</p>
</div>
<div id="A1.SS2.p5" class="ltx_para ltx_noindent">
<p id="A1.SS2.p5.1" class="ltx_p"><span id="A1.SS2.p5.1.1" class="ltx_text ltx_font_bold">Monolingual corpora:</span> As mentioned in Section <a href="#S4.SS1" title="4.1 Datasets &amp; Evaluation Metric â€£ 4 Experimental Setup â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, our monolingual text data is sourced from the CC100 dataset <cite class="ltx_cite ltx_citemacro_cite">Conneau etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, from which we sample 200,000 error-free instances for each language. To ensure the text is error-free, we use the DirectCLT baseline for error detection, including only sentences verified to be error-free.</p>
</div>
<div id="A1.SS2.p6" class="ltx_para">
<p id="A1.SS2.p6.1" class="ltx_p">For all our trainings, we use 3*A6000 GPUs with 48 GB of VRAM.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Similarity Analysis details</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">To distinguish between authentic and synthetic instances, we train a binary classifier. The classifier processes a pair of sentences: a grammatical sentence and its corresponding ungrammatical version separated by a separator token. Its task is to identify whether the ungrammatical sentence is synthetic or authentic. We train separate binary classifiers for each synthetic data generation method, using mdeberta-v3-base <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> as our backbone.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>GPT-4 analysis details</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">To evaluate the linguistic diversity of errors across different languages, we employed GPT-4 as an error classifier. Specifically, we used GPT-4 to describe the nature of the errors in sentences. Without constraining GPT-4 to a predetermined set of error types, it generated a diverse range of error descriptions for similar errors.</p>
</div>
<div id="A1.SS4.p2" class="ltx_para">
<p id="A1.SS4.p2.1" class="ltx_p">We then categorized these errors into distinct clusters using a clustering method based on the sentence embeddings generated using sentence-transformers <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib45" title="" class="ltx_ref">2019</a>)</cite>. In particular, we applied KMeans clustering with four different values of K (16, 32, 64, 128). This approach produced multiple sets of clusters, each representing distinct error patterns within the dataset.</p>
</div>
<div id="A1.SS4.p3" class="ltx_para">
<p id="A1.SS4.p3.1" class="ltx_p">For each value of K, we computed the frequency distribution of errors across the clusters and subsequently calculated the entropy of these distributions. To enable comparison across different values of K, we normalized the entropy values, ensuring comparability and eliminating bias from the number of clusters chosen.</p>
</div>
<div id="A1.SS4.p4" class="ltx_para">
<p id="A1.SS4.p4.1" class="ltx_p">Finally, to derive a comprehensive measure of normalized entropy for each language under study, we averaged the normalized entropy values obtained across all K settings. The resulting normalized entropy metric provides a robust indicator of the diversity of error patterns observed across different languages, as illustrated in Figure <a href="#S6.F5" title="Figure 5 â€£ 6.1 Czech Case Study â€£ 6 Analysis of synthetic errors â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.T6.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="A1.T6.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">1</td>
<td id="A1.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">2</td>
<td id="A1.T6.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">3</td>
<td id="A1.T6.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">4</td>
<td id="A1.T6.1.1.5" class="ltx_td ltx_align_left ltx_border_tt">5</td>
<td id="A1.T6.1.1.6" class="ltx_td ltx_align_left ltx_border_tt">6</td>
</tr>
<tr id="A1.T6.1.2" class="ltx_tr">
<td id="A1.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_t">en</td>
<td id="A1.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_t">en,de</td>
<td id="A1.T6.1.2.3" class="ltx_td ltx_align_left ltx_border_t">en,de,is</td>
<td id="A1.T6.1.2.4" class="ltx_td ltx_align_left ltx_border_t">en,de,is,et</td>
<td id="A1.T6.1.2.5" class="ltx_td ltx_align_left ltx_border_t">en,de,is,et,ru</td>
<td id="A1.T6.1.2.6" class="ltx_td ltx_align_left ltx_border_t">all</td>
</tr>
<tr id="A1.T6.1.3" class="ltx_tr">
<td id="A1.T6.1.3.1" class="ltx_td ltx_align_left">en</td>
<td id="A1.T6.1.3.2" class="ltx_td ltx_align_left">en,es</td>
<td id="A1.T6.1.3.3" class="ltx_td ltx_align_left">en,es,de</td>
<td id="A1.T6.1.3.4" class="ltx_td ltx_align_left">en,es,de,et</td>
<td id="A1.T6.1.3.5" class="ltx_td ltx_align_left">en,es,de,et,is</td>
<td id="A1.T6.1.3.6" class="ltx_td ltx_align_left">all</td>
</tr>
<tr id="A1.T6.1.4" class="ltx_tr">
<td id="A1.T6.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">en</td>
<td id="A1.T6.1.4.2" class="ltx_td ltx_align_left ltx_border_bb">en,is</td>
<td id="A1.T6.1.4.3" class="ltx_td ltx_align_left ltx_border_bb">en,is,es</td>
<td id="A1.T6.1.4.4" class="ltx_td ltx_align_left ltx_border_bb">en,is,es,ru</td>
<td id="A1.T6.1.4.5" class="ltx_td ltx_align_left ltx_border_bb">en,is,es,ru,de</td>
<td id="A1.T6.1.4.6" class="ltx_td ltx_align_left ltx_border_bb">all</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Subsets of source languages used to fine-tune our AEG model for
our scalability experiments in <a href="#S5.SS4" title="5.4 Scalability â€£ 5 Proposed Method Evaluation â€£ Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A1.T6.2" class="ltx_p ltx_figure_panel ltx_align_center">.</p>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.11853" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.11854" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.11854">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.11854" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.11855" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:05:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
