<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2010.14742] ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications</title><meta property="og:description" content="To train deep learning models for vision-based action recognition of elders’ daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2010.14742">

<!--Generated on Thu Mar  7 04:20:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hochul Hwang
<br class="ltx_break">Artificial Intelligence &amp; Robotics Institute
<br class="ltx_break">Korea Institute of Science and Technology (KIST)
<br class="ltx_break">Seoul 02792, Republic of Korea
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">hchlhwang@kist.re.kr</span> 
<br class="ltx_break">&amp;Cheongjae Jang 
<br class="ltx_break">Artificial Intelligence &amp; Robotics Institute
<br class="ltx_break">Korea Institute of Science and Technology (KIST)
<br class="ltx_break">Seoul 02792, Republic of Korea
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">jchastro@gmail.com</span> 
<br class="ltx_break">&amp;Geonwoo Park 
<br class="ltx_break">Artificial Intelligence &amp; Robotics Institute
<br class="ltx_break">Korea Institute of Science and Technology (KIST)
<br class="ltx_break">Seoul 02792, Republic of Korea
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">gunwop@naver.com</span> 
<br class="ltx_break">&amp;Junghyun Cho
<br class="ltx_break">Artificial Intelligence &amp; Robotics Institute
<br class="ltx_break">Korea Institute of Science and Technology (KIST)
<br class="ltx_break">Seoul 02792, Republic of Korea
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">jhcho@kist.re.kr</span> 
<br class="ltx_break">&amp;Ig-Jae Kim 
<br class="ltx_break">Artificial Intelligence &amp; Robotics Institute
<br class="ltx_break">Korea Institute of Science and Technology (KIST)
<br class="ltx_break">Seoul 02792, Republic of Korea
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">drjay@kist.re.kr</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">To train deep learning models for vision-based action recognition of elders’ daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders’ activities in many aspects, making it challenging to recognize elders’ daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders’ daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options, and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders’ activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of-the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders’ daily activities.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.4" class="ltx_p"><em id="p1.4.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.4.2" class="ltx_text ltx_font_bold">eywords</span> Classification algorithms  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
computer graphics  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
computer simulation  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
computer vision  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
supervised learning</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The need and importance of vision-based human action recognition (HAR) are growing in a wide range of eldercare services [1], including care robots [2], [3], smart surveillance [4], and health monitoring [5], [6]. Recently, the performance of vision-based HAR has been dramatically improved by deep learning methods [7]–[17], which require large-scale training datasets for accurate action recognition [7], [19]–[24] as mentioned in [18]. Accordingly, to train deep learning models to recognize elders’ activities of daily living (ADL), we need large-scale datasets which contain activities acquired under various environments and conditions that we encounter in daily life.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">However, most public datasets used in HAR, including the NTU RGB+D dataset [21], which is frequently used as a benchmark, either differ from or have limited coverage of elders’ daily activities in many aspects. Even if they have a large number of samples with various action classes, only a few action classes of such datasets match elders’ ADL. Moreover, they are usually acquired from laboratory environments that deviate from the places of daily living. The way or speed of actions may also differ from the elders’ since they mostly consist of relatively young subjects’ actions. These differences can induce inaccurate action recognition results when a model trained on such datasets is tested on data of elders’ ADL [24].</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Recently, some datasets of elders’ ADL have been publicly available [24], [25]. However, due to the limited data acquisition conditions, they often lack diversity in aspects such as background, camera viewpoint, and lighting condition. The low variations in a dataset can cause overfitting of deep learning models, especially for RGB-based HAR methods that are sensitive to the conditions above. An overfitted model will not generalize well and result in low recognition accuracy when applied to data obtained under conditions significantly different from the training datasets.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">A naive approach to this problem is to build a dataset that reflects all the conditions that arise in diverse real-world household environments. However, it is expensive and laborious to acquire such a dataset because of the combinatorial explosion [26], i.e., the number of data becomes exponentially large. More difficulties exist when acquiring labeled real data. Along with the expense of camera hardware, viewpoints are often restricted due to spatial limitations such as small-sized bathrooms or complex indoor environments. Personal privacy issues and physical limitations of the elders also make it more challenging to obtain a large-scale training dataset of good quality.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">To compensate for the limitations of available datasets and the difficulties of acquiring real data, recent studies endeavor to generate automatically-labeled synthetic data from virtual environments [27]–[29], [51]–[53] and further use those data to train deep learning models and enhance action recognition performance [29], [52]. In such virtual environments, we can freely adjust aspects such as backgrounds, subjects (or synthetic characters), camera viewpoints, and lighting conditions. Therefore, it becomes possible to customize the dataset that contains a large number of realistic data as needed. If such synthetic data are appropriately utilized for training deep learning models, we can expect those data to help fill the holes that reside in real-world datasets, e.g., the limited coverage in camera viewpoints and lighting conditions or the severe gap from the target data in subjects’ ages and backgrounds.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In this paper, based on the above ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders’ daily activities. We visualize the daily living environment and the characters of ElderSim to be as close as possible to those of the real-world using a recent three-dimensional rendering and modeling software. Considering the actual application to eldercare services, we model movements for 55 kinds of frequent daily activities of the elders, and offer variabilities in data acquiring options such as camera viewpoints and lighting conditions that change over time, to name a few. To summarize, ElderSim generates realistic daily living activities of synthetic characters with several adjustable data-generating options and provides different output modalities including RGB videos, two-dimensional (2D) and three-dimensional (3D) skeleton trajectories to further increase applicability. As an illustrating dataset generated from ElderSim, we release KIST SynADL, a large-scale simulated synthetic dataset of elders’ activities. ElderSim and the KIST SynADL dataset is publicly available in <a target="_blank" href="https://ai4robot.github.io/ElderSim" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai4robot.github.io/ElderSim</a>.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">We use KIST SynADL in addition to real datasets to train state-of-the-art HAR models, and validate the effectiveness of augmenting synthetic data. Unlike previous data augmentation studies focusing primarily on some limited benchmark datasets and experimental scenarios, we propose several new scenarios to examine various aspects which arise from recognizing elders’ ADL. Specifically, in addition to cross-view and cross-subject train/test splits, widely considered in the literature, we newly introduce cross-age and cross-dataset splits that assume the real and synthetic training datasets of different configurations. Here, the two settings are focused more on the application to the elders. We also examine synthetic data augmentation for each of the three data modalities provided, namely RGB video, 2D and 3D skeleton. From the extensive experiments held with three action recognition architectures on two different real-world datasets, we show that augmenting our synthetic data for training increases recognition performance for most of the considered methods in various settings. We also offer some guidance with insights on utilizing synthetic data to help recognize elders’ daily activities effectively. These points are of great importance since they can be easily combined with additional improvements in both deep learning models and objective functions for training to gain enhanced action recognition performance. The main pipeline of our work is described in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p">The paper is organized as follows. Section <a href="#S2" title="2 Related Work ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents related works, and we elaborate on ElderSim along with the synthetic dataset generated from ElderSim in Section <a href="#S3" title="3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Section <a href="#S4" title="4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents action recognition experiments augmenting our synthetic data. We conclude in Section <a href="#S5" title="5 Conclusion ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2010.14742/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="220" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic data generation process of ElderSim and the main pipeline of our proposed work. ElderSim generates synthetic RGB video, 2D, and 3D skeleton data based on the data-generating options that are customized by the user. Here, we experimentally augment synthetic data on real ones and train three different action recognition methods (Method I: Glimpse [11], Method II: ST-GCN [12], Method III: VA-CNN [13]) to scrutinize the effects of our generated data.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce several HAR methods with various human activity datasets utilized in the literature. We also mention previous studies that exploit synthetic data to improve action recognition.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Human Action Recognition</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Considering the temporal dimension along with the spatial dimension is essential for video understanding. Conventionally, these features were extracted by hand-crafted descriptors, including Histogram of Oriented Gradients (HOG) [30], Motion Boundary Histogram (MBH) [31], and Histograms of Optical Flow (HOF) [32], which is followed by a classifier such as Support Vector Machines (SVMs) for classification. Other methods used dense trajectories that track densely-sampled feature points and extract appearance and motion information with the previously mentioned descriptors along the trajectories [33], [34].</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">With the development of powerful computation hardware and large-scale activity datasets, deep learning methods achieved profound performance in action recognition. As being one of the convolutional neural network (CNN)-based methods [7]–[9], [35], [36], [35] computes spatial information from a still frame and samples temporal motion from multiple-frame dense optical flow. The information is then inserted into a two-stream network that consists of a spatial and temporal CNN for better training. [36] first introduced 3D convolution for action recognition, which extracts features from both the spatial and temporal dimensions with a 3D kernel. [7] tested varying architectures on their own dataset, Sports-1M, showing that the Slow Fusion model outperforms other structures with different connectivity in time and also proposed an architectural method using lowered-resolution inputs to speed up training without any performance loss. [8] built a C3D (Convolutional 3D) network with 3D kernels and empirically showed the optimal kernel size and network architecture for improved action recognition performance in large-scale video datasets. Unlike [8], [9] inflated the filters and pooling kernels of deep 2D image classification models (e.g., Inception-v1 with batch normalization [37]) into 3D to gain from the advantages of ImageNet pre-training. They marked up performance with an additional optical-flow stream which is trained separately and tested with averaged-predictions. Without the support of multimodal inputs, [10] computes a single modality of raw RGB to a two-stream design by passing each pathway with a different frame rate. Spatial meanings are captured through one stream with a higher frame rate while the temporal information is learned through the other stream with a lower frame rate. The architecture in [11] learns to predict the attention windows in the feature space; extracted from a global model. A set of recurrent architectures are used to track the unstructured windows and classify actions from RGB video inputs.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">Skeleton-based HAR methods [12]–[16] have been studied to avoid various interferences of RGB appearance while using simpler data that are coordinates of several joints and their derived forms. These methods mainly obtain the human skeletal structure utilizing depth sensors [38] or human pose estimation algorithms [39], [40]. [16] uses the main LSTM network with a spatial attention module and a temporal attention module that holds different attention levels to select discriminative joint inputs and frame outputs, respectively. The three networks are jointly trained for optimization as an end-to-end training method. More recently, initiated by [12], graph convolutional network-based action recognition studies tried to understand the skeletal information as a graph and extract features using CNNs. [14] represented the tree-based natural human body structure as a directed acyclic graph for a better interpretation. They also adopt the two-stream method by feeding a graph that contains information of joints and bones to one network and another graph that contains the motion of joints and deformation of bones to the other network. [15] used temporally different-sized kernels instead of fixed ones as in [12] and added an additional spatial graph convolution layer branch to form a parallel structure. They further improved performance by using six modalities of input features including relative positions of joints and bones. Other multimodal fusion methods enhance performance by handling data of different domains, such as RGB and 3D skeleton [17].</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Real-World Activity Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Diverse real-world human activity datasets have been publicly available with the emerging importance of robust human action recognition. Initial human activity datasets were relatively small-scale, having a small number of subjects and activity categories, until the early 2010s [41]–[44]. KTH [41], being one of the earliest databases, contains a single RGB modality of six action categories with simple motions such as <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">walk</span>, <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">run</span>, and <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">clap</span>. Depth map information was firstly provided with RGB in MSR Action3D [42], which focused on game console interaction-based motions, including <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">draw circle</span>, <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">forward kick</span>, <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">tennis swing</span>. As an extension of [42], MSR Daily Activity3D [43] covers living room daily activities, most containing human-object interaction captured by a Kinect sensor. RGBD HuDaAct [44] also deals with 12 daily activities of 30 students with RGB and depth modalities maintaining under 1,200 samples. Most of the early datasets contain only a few thousands of video samples and under 20 class categories, which allowed studies for hand-crafted methods without the support of deep learning.
Large-scale activity datasets emerged along with the advance of data-hungry action recognition methods [7], [19]–[24]. The initial version of Kinetics [19], obtained from YouTube, included 400 activity classes having more than 300K in total. The dataset is now extended to contain 700 classes with approximately 650K video clips. PKU-MMD [20] provides untrimmed daily activity video sequences in four modalities of RGB, depth, infrared (IR), and skeleton for the research field of action detection. A more recent multimodal dataset, MMAct [23], was released with seven modalities: RGB, skeleton, acceleration, and other sensor signals. NTU RGB+D [21] and its updated version [22] are extensively used as a benchmark dataset in recent human action recognition literature. They respectively captured 60 and 120 action categories, including daily actions, medical situations, and human-human interactions. Both versions are provided with RGB, depth, 3D skeletons, and IR data, having almost 115K samples for the updated version.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Some datasets are acquired under more varied settings to better reflect the conditions that are likely to occur in real-world applications. Multi-view human activity datasets were obtained from various camera viewpoints by simultaneously using several cameras or changing the camera viewpoints in a different trial [21], [22], [45], [46]. UESTC [46] considered human-robot interaction (HRI) applications for action recognition from arbitrary viewpoints. The dataset includes eight fixed viewpoints with arbitrary viewpoints sampled from the entire 360° horizontal directions. There also exist other datasets that target action recognition for specific applications, such as eldercare [24], [25]. ETRI-Activity3D [24] captured elders’ ADL from several viewpoints considering mobile robots’ heights for care robotic services. Toyota Smarthome [25] is another dataset on elders’ ADL that possesses severe class imbalance and intraclass variation by capturing unscripted daily activity videos of the elderly.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Synthetic Data Exploitation</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">To provide abundant training data for deep learning methods to avoid overfitting, some studies focused on utilizing synthetic data. Synthetic data generation is considered cost-effective and customizable since users can manipulate data reflecting one’s needs without any additional data capturing middleware or subjects. Some studies generate synthetic data using generative adversarial networks (GANs) [47], [48] or composite methods based on existing real data [49], [50]. Another group of studies uses computer graphics and game engine techniques to simulate data and exploit them for deep learning tasks [27]–[29], [51], [52].</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">[48] uses two adversarial generative networks to train instance-level pairwise cross-view connection knowledge and performs robust action recognition with additional training data generated for deficient views. [49] composites realistic images to overcome the laborious manual labeling process for the 3D skeleton, depth, and motion. The human motion is extracted based on motion capture (MoCap) recordings, randomized textures, viewpoints, and lighting conditions are added on top of a static real-world background image to generate data; such data are applied to human depth estimation and human part segmentation tasks. The subsequent study [50] extracts 3D human dynamics using a 3D human shape estimation method and synthesizes other randomized components to render complementary training data to improve the action recognition from unseen viewpoints.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">Here, we focus on game engine techniques to synthesize data without any reference RGB data and generate realistic videos by considering various factors, including the context of the background, physics, and object interaction. Various game engine-based data generation studies were conducted in fields where data acquisition in various environments is highly expensive, such as autonomous systems [51], [52] and robotics [27], [28]. For human action recognition, Souza et al. [29] initially generated abundant synthetic training data under a variety of conditions with the Unity® game engine and enhanced action recognition performance by training with a mixture of real-world and generated data. However, most activity categories are not indoor activities of daily living hence not applicable to train models for eldercare applications. [53] introduced a simulation platform, developed in Unreal Engine 4® (UE4), to procedurally produce photorealistic synthetic videos of household activities in various modalities, but fails to provide details of the synthetic data augmentation effect in action recognition. [52] developed a simulation framework to automatically generate annotated training data from a game engine. They show outstanding action recognition accuracy in classifying five activities by training a shallow skeleton-based action recognition algorithm with their generated data. In this paper, we further explore the benefits of training synthetic data based on three state-of-the-art deep action recognition algorithms (fed with different data modalities containing RGB, 2D, and 3D skeleton) to classify 55 action classes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ElderSim Development</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We now elaborate on how our elders’ activity simulation platform, denoted as ElderSim, has been developed in detail. In the development, we focus on the following two aspects: 1) to visualize the virtual environment as close as possible to the real-world and 2) to reflect various situations that could be captured in real applications. To fulfill our first aim, we utilize a real-time photorealistic rendering platform Unreal Engine 4® (UE4) and a three-dimensional (3D) computer animation and modeling software called Autodesk Maya® (Maya). Using the two software, we construct the simulation environment of elders’ daily living that resembles the real household backgrounds. We then model appearances and movements of synthetic characters based on the motion capture (MoCap) data obtained from the elders. To achieve the second objective, we consider 55 activity classes that sufficiently include the most frequent ADL of the elders. We also make it available to customize various camera viewpoints and lighting conditions, regarding care robot and smart surveillance applications. The following sections explain further development details and distinctive features of ElderSim. We then introduce KIST SynADL, a large-scale synthetic dataset generated from ElderSim.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To provide realistic simulation backgrounds for elders’ daily living in ElderSim, we have modeled four residential houses based on their indoor measurements and photographs. House models can be added if necessary. When implementing the house models in ElderSim, the household background has become visually more realistic by using physics-based materials and the Post-Process Volume function in UE4. Each of the four house models contains four areas (living room, bedroom, kitchen, and bathroom) as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Background ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In each area, we only simulate activities that are plausible to be performed (e.g., <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">wash face</span> is simulated only in the bathroom while <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">play with a mobile phone</span> is simulated in all four areas).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2010.14742/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The top view of four different residential household backgrounds implemented in ElderSim. Each household consists of four areas where daily activities are frequently occurred.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Character</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">We have modeled synthetic characters that imitate thirteen elder subjects (seven females and six males with average age and standard deviation as 69.92 and 3.36, respectively) and two relatively young subjects (a female and a male) in ElderSim. These subjects have been recruited to sufficiently represent a variety of body shapes and appearances. Their body shapes have been captured from Kinect depth sensors and utilized to design the body shape of synthetic characters in Maya. The faces of characters have been randomly created due to legal issues on portrait rights. In addition, different age-appropriate clothes have been applied to each character to enhance their appearance diversity. As a result, ElderSim can generate action data from fifteen synthetic characters possessing individual face, body shape, and appearance (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Character ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The number of synthetic characters can be increased by implementing body shape transformation techniques in computer graphics, which are left for future work.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2010.14742/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="75" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Body shapes are captured from a depth sensor in the real-world (a). Depth information is used to model the synthetic character (b) and appropriate textures are applied (c) to reflect the various appearances (d).</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Motion</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Following [24], we provide motions for 55 activity classes considered to be the most frequent ADL of the elders in ElderSim. To generate realistic motions for these activities, we utilize MoCap data obtained from the subjects recruited in Section <a href="#S3.SS2" title="3.2 Character ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Sixteen digital MoCap cameras have captured the subject motions using 40 markers attached to the subjects’ body. When acquiring data, there have not been any specific instructions for subjects to perform ADL to increase realism and diversity of motions. The obtained MoCap data have been rigged in Maya, i.e., skeletal templates and their movements that best fit the data are constructed. From the rigged data, the motions for synthetic characters are generated by adjusting the template’s kinematic parameters to those of each character and playing the constructed movements. To provide motion data in 3D skeleton modality, we define skeleton joints by attaching the sockets of UE4 to each character’s 25 joints following two types of joint format labels used in OpenPose [40] and Kinect v2. Two-dimensional (2D) joint motions are obtained by projecting those in 3D to the image plane of each camera viewpoint using a transformation function in UE4.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Viewpoint</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.8" class="ltx_p">Camera viewpoints in ElderSim contain robot- and surveillance-viewpoints, considering eldercare applications. Robot-viewpoints simulate video acquisition from care robots, and corresponding cameras are located on a circle to surround a target character with the circle radius appropriately defined from the range of the character’s motion. The cameras have equal spacing on the circle with an angle interval <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\phi</annotation></semantics></math> and located at heights specified by a set of <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">h</annotation></semantics></math> height values <math id="S3.SS4.p1.3.m3.4" class="ltx_Math" alttext="\delta=\{\delta_{1},\delta_{2},...,\delta_{h}\}" display="inline"><semantics id="S3.SS4.p1.3.m3.4a"><mrow id="S3.SS4.p1.3.m3.4.4" xref="S3.SS4.p1.3.m3.4.4.cmml"><mi id="S3.SS4.p1.3.m3.4.4.5" xref="S3.SS4.p1.3.m3.4.4.5.cmml">δ</mi><mo id="S3.SS4.p1.3.m3.4.4.4" xref="S3.SS4.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS4.p1.3.m3.4.4.3.3" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS4.p1.3.m3.4.4.3.3.4" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S3.SS4.p1.3.m3.2.2.1.1.1" xref="S3.SS4.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS4.p1.3.m3.2.2.1.1.1.2" xref="S3.SS4.p1.3.m3.2.2.1.1.1.2.cmml">δ</mi><mn id="S3.SS4.p1.3.m3.2.2.1.1.1.3" xref="S3.SS4.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.p1.3.m3.4.4.3.3.5" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p1.3.m3.3.3.2.2.2" xref="S3.SS4.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.SS4.p1.3.m3.3.3.2.2.2.2" xref="S3.SS4.p1.3.m3.3.3.2.2.2.2.cmml">δ</mi><mn id="S3.SS4.p1.3.m3.3.3.2.2.2.3" xref="S3.SS4.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.p1.3.m3.4.4.3.3.6" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS4.p1.3.m3.4.4.3.3.7" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p1.3.m3.4.4.3.3.3" xref="S3.SS4.p1.3.m3.4.4.3.3.3.cmml"><mi id="S3.SS4.p1.3.m3.4.4.3.3.3.2" xref="S3.SS4.p1.3.m3.4.4.3.3.3.2.cmml">δ</mi><mi id="S3.SS4.p1.3.m3.4.4.3.3.3.3" xref="S3.SS4.p1.3.m3.4.4.3.3.3.3.cmml">h</mi></msub><mo stretchy="false" id="S3.SS4.p1.3.m3.4.4.3.3.8" xref="S3.SS4.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.4b"><apply id="S3.SS4.p1.3.m3.4.4.cmml" xref="S3.SS4.p1.3.m3.4.4"><eq id="S3.SS4.p1.3.m3.4.4.4.cmml" xref="S3.SS4.p1.3.m3.4.4.4"></eq><ci id="S3.SS4.p1.3.m3.4.4.5.cmml" xref="S3.SS4.p1.3.m3.4.4.5">𝛿</ci><set id="S3.SS4.p1.3.m3.4.4.3.4.cmml" xref="S3.SS4.p1.3.m3.4.4.3.3"><apply id="S3.SS4.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS4.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS4.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS4.p1.3.m3.2.2.1.1.1.2">𝛿</ci><cn type="integer" id="S3.SS4.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS4.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS4.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS4.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS4.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS4.p1.3.m3.3.3.2.2.2.2">𝛿</ci><cn type="integer" id="S3.SS4.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS4.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">…</ci><apply id="S3.SS4.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS4.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.4.4.3.3.3.1.cmml" xref="S3.SS4.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.3.m3.4.4.3.3.3.2.cmml" xref="S3.SS4.p1.3.m3.4.4.3.3.3.2">𝛿</ci><ci id="S3.SS4.p1.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS4.p1.3.m3.4.4.3.3.3.3">ℎ</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.4c">\delta=\{\delta_{1},\delta_{2},...,\delta_{h}\}</annotation></semantics></math>, where <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="\delta_{i}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">δ</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝛿</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\delta_{i}</annotation></semantics></math> denotes the <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mi id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">i</annotation></semantics></math>-th height value. The angle interval <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><mi id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\phi</annotation></semantics></math> and the set of height values <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><mi id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">\delta</annotation></semantics></math> are set to be user-adjustable parameters. Given these parameter values, the number of viewpoints (<math id="S3.SS4.p1.8.m8.1" class="ltx_Math" alttext="V_{circle}" display="inline"><semantics id="S3.SS4.p1.8.m8.1a"><msub id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml"><mi id="S3.SS4.p1.8.m8.1.1.2" xref="S3.SS4.p1.8.m8.1.1.2.cmml">V</mi><mrow id="S3.SS4.p1.8.m8.1.1.3" xref="S3.SS4.p1.8.m8.1.1.3.cmml"><mi id="S3.SS4.p1.8.m8.1.1.3.2" xref="S3.SS4.p1.8.m8.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.1" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.8.m8.1.1.3.3" xref="S3.SS4.p1.8.m8.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.1a" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.8.m8.1.1.3.4" xref="S3.SS4.p1.8.m8.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.1b" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.8.m8.1.1.3.5" xref="S3.SS4.p1.8.m8.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.1c" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.8.m8.1.1.3.6" xref="S3.SS4.p1.8.m8.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.1d" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.8.m8.1.1.3.7" xref="S3.SS4.p1.8.m8.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><apply id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.8.m8.1.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p1.8.m8.1.1.2.cmml" xref="S3.SS4.p1.8.m8.1.1.2">𝑉</ci><apply id="S3.SS4.p1.8.m8.1.1.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3"><times id="S3.SS4.p1.8.m8.1.1.3.1.cmml" xref="S3.SS4.p1.8.m8.1.1.3.1"></times><ci id="S3.SS4.p1.8.m8.1.1.3.2.cmml" xref="S3.SS4.p1.8.m8.1.1.3.2">𝑐</ci><ci id="S3.SS4.p1.8.m8.1.1.3.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3">𝑖</ci><ci id="S3.SS4.p1.8.m8.1.1.3.4.cmml" xref="S3.SS4.p1.8.m8.1.1.3.4">𝑟</ci><ci id="S3.SS4.p1.8.m8.1.1.3.5.cmml" xref="S3.SS4.p1.8.m8.1.1.3.5">𝑐</ci><ci id="S3.SS4.p1.8.m8.1.1.3.6.cmml" xref="S3.SS4.p1.8.m8.1.1.3.6">𝑙</ci><ci id="S3.SS4.p1.8.m8.1.1.3.7.cmml" xref="S3.SS4.p1.8.m8.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">V_{circle}</annotation></semantics></math>) can be expressed as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="V_{circle}=h\times floor(360^{\circ}/\phi)." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1a" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.4" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1b" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.5" xref="S3.E1.m1.1.1.1.1.3.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1c" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.6" xref="S3.E1.m1.1.1.1.1.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1d" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.7" xref="S3.E1.m1.1.1.1.1.3.3.7.cmml">e</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.3.1.cmml">×</mo><mi id="S3.E1.m1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.3.3.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2d" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">360</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">∘</mo></msup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">ϕ</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.3.4">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.1.1.3.3.5">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.3.3.6.cmml" xref="S3.E1.m1.1.1.1.1.3.3.6">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.3.3.7.cmml" xref="S3.E1.m1.1.1.1.1.3.3.7">𝑒</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2">ℎ</ci><ci id="S3.E1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.3">𝑓</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.4">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.5">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.1.6">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.1.7">𝑟</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></divide><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2">360</cn><compose id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3"></compose></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">italic-ϕ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">V_{circle}=h\times floor(360^{\circ}/\phi).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.11" class="ltx_p">Such a circular camera layout may not be available occasionally due to obstacles in some backgrounds (e.g., when the character is sitting on a sofa, a wall behind the sofa hinders the rear robot-viewpoints); we then form viewpoints to cover a semicircle instead of a circle (see Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.4 Viewpoint ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In this semicircular camera layout, the number of viewpoints is given as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="V_{semicircle}=h\times(floor(180^{\circ}/\phi)+1)." display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">V</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1a" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.4" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1b" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.5" xref="S3.E2.m1.1.1.1.1.3.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1c" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.6" xref="S3.E2.m1.1.1.1.1.3.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1d" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.7" xref="S3.E2.m1.1.1.1.1.3.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1e" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.8" xref="S3.E2.m1.1.1.1.1.3.3.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1f" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.9" xref="S3.E2.m1.1.1.1.1.3.3.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1g" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.10" xref="S3.E2.m1.1.1.1.1.3.3.10.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1h" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.11" xref="S3.E2.m1.1.1.1.1.3.3.11.cmml">e</mi></mrow></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">×</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2c" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.7" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2d" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">180</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">∘</mo></msup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ϕ</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">+</mo><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝑉</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝑠</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">𝑒</ci><ci id="S3.E2.m1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.3.4">𝑚</ci><ci id="S3.E2.m1.1.1.1.1.3.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.3.5">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.3.3.6.cmml" xref="S3.E2.m1.1.1.1.1.3.3.6">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.3.3.7.cmml" xref="S3.E2.m1.1.1.1.1.3.3.7">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.3.3.8.cmml" xref="S3.E2.m1.1.1.1.1.3.3.8">𝑟</ci><ci id="S3.E2.m1.1.1.1.1.3.3.9.cmml" xref="S3.E2.m1.1.1.1.1.3.3.9">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.3.3.10.cmml" xref="S3.E2.m1.1.1.1.1.3.3.10">𝑙</ci><ci id="S3.E2.m1.1.1.1.1.3.3.11.cmml" xref="S3.E2.m1.1.1.1.1.3.3.11">𝑒</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">ℎ</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">𝑓</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4">𝑙</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.6">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.7">𝑟</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><divide id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></divide><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">180</cn><compose id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"></compose></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">italic-ϕ</ci></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">V_{semicircle}=h\times(floor(180^{\circ}/\phi)+1).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.10" class="ltx_p">To implement these camera layouts for robot-viewpoints in ElderSim, we define UE4 splines that contain multiple cameras vertically and position these splines according to the parameter settings. Meanwhile, surveillance-viewpoints simulate video acquisition from surveillance cameras such as closed-circuit televisions (CCTV). They are located at the height of <math id="S3.SS4.p1.9.m1.3" class="ltx_Math" alttext="1.5\text{\,}\mathrm{m}" display="inline"><semantics id="S3.SS4.p1.9.m1.3a"><mrow id="S3.SS4.p1.9.m1.3.3" xref="S3.SS4.p1.9.m1.3.3.cmml"><mn id="S3.SS4.p1.9.m1.1.1.1.1.1.1" xref="S3.SS4.p1.9.m1.1.1.1.1.1.1.cmml">1.5</mn><mtext id="S3.SS4.p1.9.m1.2.2.2.2.2.2" xref="S3.SS4.p1.9.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.SS4.p1.9.m1.3.3.3.3.3.3" xref="S3.SS4.p1.9.m1.3.3.3.3.3.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m1.3b"><apply id="S3.SS4.p1.9.m1.3.3.cmml" xref="S3.SS4.p1.9.m1.3.3"><csymbol cd="latexml" id="S3.SS4.p1.9.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.9.m1.2.2.2.2.2.2">times</csymbol><cn type="float" id="S3.SS4.p1.9.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.9.m1.1.1.1.1.1.1">1.5</cn><csymbol cd="latexml" id="S3.SS4.p1.9.m1.3.3.3.3.3.3.cmml" xref="S3.SS4.p1.9.m1.3.3.3.3.3.3">meter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m1.3c">1.5\text{\,}\mathrm{m}</annotation></semantics></math> and <math id="S3.SS4.p1.10.m2.3" class="ltx_Math" alttext="2.2\text{\,}\mathrm{m}" display="inline"><semantics id="S3.SS4.p1.10.m2.3a"><mrow id="S3.SS4.p1.10.m2.3.3" xref="S3.SS4.p1.10.m2.3.3.cmml"><mn id="S3.SS4.p1.10.m2.1.1.1.1.1.1" xref="S3.SS4.p1.10.m2.1.1.1.1.1.1.cmml">2.2</mn><mtext id="S3.SS4.p1.10.m2.2.2.2.2.2.2" xref="S3.SS4.p1.10.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.SS4.p1.10.m2.3.3.3.3.3.3" xref="S3.SS4.p1.10.m2.3.3.3.3.3.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.10.m2.3b"><apply id="S3.SS4.p1.10.m2.3.3.cmml" xref="S3.SS4.p1.10.m2.3.3"><csymbol cd="latexml" id="S3.SS4.p1.10.m2.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.10.m2.2.2.2.2.2.2">times</csymbol><cn type="float" id="S3.SS4.p1.10.m2.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.10.m2.1.1.1.1.1.1">2.2</cn><csymbol cd="latexml" id="S3.SS4.p1.10.m2.3.3.3.3.3.3.cmml" xref="S3.SS4.p1.10.m2.3.3.3.3.3.3">meter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.10.m2.3c">2.2\text{\,}\mathrm{m}</annotation></semantics></math> in four corners of each area to reflect the realistic camera installation, hence resulting in eight surveillance-viewpoints.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<table id="S3.F4.4" class="ltx_tabular ltx_align_middle">
<tr id="S3.F4.2.2" class="ltx_tr">
<td id="S3.F4.2.2.3" class="ltx_td ltx_align_left"><span id="S3.F4.2.2.3.1" class="ltx_text ltx_font_bold">360°</span></td>
<td id="S3.F4.1.1.1" class="ltx_td ltx_align_justify">
<span id="S3.F4.1.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2010.14742/assets/figures/view_top_360.png" id="S3.F4.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="389" height="389" alt="Refer to caption">
</span>
</td>
<td id="S3.F4.2.2.2" class="ltx_td ltx_align_justify">
<span id="S3.F4.2.2.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2010.14742/assets/figures/view_iso_360.png" id="S3.F4.2.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="389" alt="Refer to caption">
</span>
</td>
</tr>
<tr id="S3.F4.4.4" class="ltx_tr">
<td id="S3.F4.4.4.3" class="ltx_td ltx_align_left"><span id="S3.F4.4.4.3.1" class="ltx_text ltx_font_bold">180°</span></td>
<td id="S3.F4.3.3.1" class="ltx_td ltx_align_justify">
<span id="S3.F4.3.3.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2010.14742/assets/figures/view_top_180.png" id="S3.F4.3.3.1.1.g1" class="ltx_graphics ltx_img_square" width="389" height="389" alt="Refer to caption">
</span>
</td>
<td id="S3.F4.4.4.2" class="ltx_td ltx_align_justify">
<span id="S3.F4.4.4.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2010.14742/assets/figures/view_iso_180.png" id="S3.F4.4.4.2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="389" alt="Refer to caption">
</span>
</td>
</tr>
<tr id="S3.F4.4.5" class="ltx_tr">
<td id="S3.F4.4.5.1" class="ltx_td"></td>
<td id="S3.F4.4.5.2" class="ltx_td ltx_align_justify">
<span id="S3.F4.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F4.4.5.2.1.1" class="ltx_p"><span id="S3.F4.4.5.2.1.1.1" class="ltx_text ltx_font_bold">Top view</span></span>
</span>
</td>
<td id="S3.F4.4.5.3" class="ltx_td ltx_align_justify">
<span id="S3.F4.4.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F4.4.5.3.1.1" class="ltx_p"><span id="S3.F4.4.5.3.1.1.1" class="ltx_text ltx_font_bold">Front view</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An example of the virtual camera setups based on the available viewpoints. Top and front views are chosen for a better interpretation. Here, 20 cameras represent robot-viewpoints and eight cameras on each corner of the room represent surveillance-viewpoints. The user can easily manage the number of viewpoints by adjusting the angle interval and a set of heights.</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Lighting</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">Lighting conditions in ElderSim are affected by both sunlight and indoor light sources modeled in UE4. To simulate the effect of sunlight over time, we utilize the SkySphere Blueprint function of UE4 and provide an adjustable time parameter in 100 levels to vary sunlight. Indoor light sources are placed according to lighting layouts of actual houses considered in Section <a href="#S3.SS1" title="3.1 Background ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and controlled to resemble our daily life better, e.g., turned off during the daytime and turned on during the evening as shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.5 Lighting ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Rendering effects, which are significantly affected by lighting conditions, become finer by applying the Post-Process Volume effect of UE4.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2010.14742/assets/figures/lighting.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Representative lighting conditions modeled in ElderSim (clockwise from top left: <span id="S3.F5.5.1" class="ltx_text ltx_font_italic">dawn</span>, <span id="S3.F5.6.2" class="ltx_text ltx_font_italic">noon</span>, <span id="S3.F5.7.3" class="ltx_text ltx_font_italic">night</span>, and <span id="S3.F5.8.4" class="ltx_text ltx_font_italic">sunset</span>). Controllable lighting conditions appropriately reflect the real-world with the usage of indoor light sources and the Post-Process Volume effect.</figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Object</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p">Among the 55 activity classes considered in ElderSim, 35 activities contain human-object interaction. We model objects that are required to simulate these activities in UE4. The types of objects range from a single rigid body (for 28 classes) such as a cup to articulated objects such as a vacuum cleaner (for the <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_italic">vacuum the floor</span> class) or even deformable objects such as a jacket (for the <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_italic">take off jacket</span> class). All the objects are modeled in three different ways to increase diversity. When objects are used in ElderSim, they are attached to the contacting body parts’ mesh and move along with the body parts to look natural.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>User Interface</h3>

<div id="S3.SS7.p1" class="ltx_para ltx_noindent">
<p id="S3.SS7.p1.3" class="ltx_p">An intuitive graphical user interface (GUI) is provided in ElderSim to select data-generating options as needed. The user can easily choose the desired subset of activities, characters, and backgrounds from the provided sets in order (see Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.7 User Interface ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The camera viewpoints can then be selected among the robot- and surveillance-viewpoints, while preferable robot-viewpoints are adjusted by an angle interval <math id="S3.SS7.p1.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS7.p1.1.m1.1a"><mi id="S3.SS7.p1.1.m1.1.1" xref="S3.SS7.p1.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.1.m1.1b"><ci id="S3.SS7.p1.1.m1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.1.m1.1c">\phi</annotation></semantics></math> and a set of heights <math id="S3.SS7.p1.2.m2.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS7.p1.2.m2.1a"><mi id="S3.SS7.p1.2.m2.1.1" xref="S3.SS7.p1.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.2.m2.1b"><ci id="S3.SS7.p1.2.m2.1.1.cmml" xref="S3.SS7.p1.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.2.m2.1c">\delta</annotation></semantics></math> as mentioned in Section <a href="#S3.SS4" title="3.4 Viewpoint ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>. The lighting conditions are determined by choosing a subset of the hundred time-levels to vary sunlight, from 0 to 1. For the activities containing object interactions, the user can choose whether to use an object and which object model to use. In addition, for the activities containing repetitive motion, we provide three different types of motion duration to include one iteration (succinct), multiple iterations (iterative), and sequential movements (combined); the average motion duration for each activity class in ElderSim is illustrated in Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.8 KIST SynADL ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
Once the data-generating options are determined, the data are automatically generated and recorded according to all possible combinations of options in ElderSim. ElderSim provides adjustable video resolutions and frame rates of up to <math id="S3.SS7.p1.3.m3.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.SS7.p1.3.m3.1a"><mrow id="S3.SS7.p1.3.m3.1.1" xref="S3.SS7.p1.3.m3.1.1.cmml"><mn id="S3.SS7.p1.3.m3.1.1.2" xref="S3.SS7.p1.3.m3.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS7.p1.3.m3.1.1.1" xref="S3.SS7.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS7.p1.3.m3.1.1.3" xref="S3.SS7.p1.3.m3.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.3.m3.1b"><apply id="S3.SS7.p1.3.m3.1.1.cmml" xref="S3.SS7.p1.3.m3.1.1"><times id="S3.SS7.p1.3.m3.1.1.1.cmml" xref="S3.SS7.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS7.p1.3.m3.1.1.2.cmml" xref="S3.SS7.p1.3.m3.1.1.2">1920</cn><cn type="integer" id="S3.SS7.p1.3.m3.1.1.3.cmml" xref="S3.SS7.p1.3.m3.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.3.m3.1c">1920\times 1080</annotation></semantics></math> and 60 frames per second (FPS), respectively. Furthermore, three kinds of output data modalities are provided: RGB video, 2D, and 3D skeleton. For skeleton data, we provide both OpenPose- and Kinect v2-based skeletal formats. Parallel processing allows faster data generation.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2010.14742/assets/figures/eldersim_ui.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A snapshot of ElderSim showing the user interface to generate action data (a <span id="S3.F6.3.1" class="ltx_text ltx_font_italic">male elder</span> character performing <span id="S3.F6.4.2" class="ltx_text ltx_font_italic">hang out laundry</span>). The intuitive user interface allows the user to generate customized data by providing several data-generating options.</figcaption>
</figure>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>KIST SynADL</h3>

<div id="S3.SS8.p1" class="ltx_para ltx_noindent">
<p id="S3.SS8.p1.3" class="ltx_p">Based on the developmental features of ElderSim, we generate KIST SynADL, a large-scale synthetic dataset of elders’ daily activities considering care robot and smart surveillance applications. All 55 activities, 15 characters, and four backgrounds modeled in ElderSim are utilized to generate KIST SynADL. We further customize parameters for camera viewpoints as follows. To provide robot-viewpoints, we set the angle interval and height parameters (introduced in Section <a href="#S3.SS4" title="3.4 Viewpoint ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) to <math id="S3.SS8.p1.1.m1.1" class="ltx_math_unparsed" alttext="{\{\phi=36^{\circ}}" display="inline"><semantics id="S3.SS8.p1.1.m1.1a"><mrow id="S3.SS8.p1.1.m1.1b"><mo stretchy="false" id="S3.SS8.p1.1.m1.1.1">{</mo><mi id="S3.SS8.p1.1.m1.1.2">ϕ</mi><mo id="S3.SS8.p1.1.m1.1.3">=</mo><msup id="S3.SS8.p1.1.m1.1.4"><mn id="S3.SS8.p1.1.m1.1.4.2">36</mn><mo id="S3.SS8.p1.1.m1.1.4.3">∘</mo></msup></mrow><annotation encoding="application/x-tex" id="S3.SS8.p1.1.m1.1c">{\{\phi=36^{\circ}}</annotation></semantics></math>, <math id="S3.SS8.p1.2.m2.2" class="ltx_math_unparsed" alttext="{\delta=($0.7\text{\,}\mathrm{m}$,$1.2\text{\,}\mathrm{m}$)\}}" display="inline"><semantics id="S3.SS8.p1.2.m2.2a"><mrow id="S3.SS8.p1.2.m2.2b"><mi id="S3.SS8.p1.2.m2.2.3">δ</mi><mo id="S3.SS8.p1.2.m2.2.4">=</mo><mrow id="S3.SS8.p1.2.m2.2.5"><mo stretchy="false" id="S3.SS8.p1.2.m2.2.5.1">(</mo><mrow id="S3.SS8.p1.2.m2.1.1.m1.3.3"><mn id="S3.SS8.p1.2.m2.1.1.m1.1.1.1.1.1.1">0.7</mn><mtext id="S3.SS8.p1.2.m2.1.1.m1.2.2.2.2.2.2"> </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.SS8.p1.2.m2.1.1.m1.3.3.3.3.3.3">m</mi></mrow><mo id="S3.SS8.p1.2.m2.2.5.2">,</mo><mrow id="S3.SS8.p1.2.m2.2.2.m1.3.3"><mn id="S3.SS8.p1.2.m2.2.2.m1.1.1.1.1.1.1">1.2</mn><mtext id="S3.SS8.p1.2.m2.2.2.m1.2.2.2.2.2.2"> </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.SS8.p1.2.m2.2.2.m1.3.3.3.3.3.3">m</mi></mrow><mo stretchy="false" id="S3.SS8.p1.2.m2.2.5.3">)</mo></mrow><mo stretchy="false" id="S3.SS8.p1.2.m2.2.6">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS8.p1.2.m2.2c">{\delta=($0.7\text{\,}\mathrm{m}$,$1.2\text{\,}\mathrm{m}$)\}}</annotation></semantics></math>
for semicircular camera layout respectively, where the height parameters are set based on several real-world care robots. These parameter settings result in ten horizontal camera locations, each having two different heights, thus providing 20 robot-viewpoints. Including eight more surveillance-viewpoints introduced in Section <a href="#S3.SS4" title="3.4 Viewpoint ‣ 3 ElderSim Development ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, KIST SynADL contains 28 camera viewpoints. For the lighting conditions, we divide a day into five parts by setting the time parameter to 0 (<span id="S3.SS8.p1.3.1" class="ltx_text ltx_font_italic">dawn</span>), 0.25 (<span id="S3.SS8.p1.3.2" class="ltx_text ltx_font_italic">noon</span>), 0.5 (<span id="S3.SS8.p1.3.3" class="ltx_text ltx_font_italic">evening</span>), 0.75 (<span id="S3.SS8.p1.3.4" class="ltx_text ltx_font_italic">sunset</span>), and 1 (<span id="S3.SS8.p1.3.5" class="ltx_text ltx_font_italic">night</span>), with <span id="S3.SS8.p1.3.6" class="ltx_text ltx_font_italic">noon</span> being the default lighting condition. In the case of activities involving human-object interactions, we utilize only one kind of object model for each activity to generate data. RGB videos in the KIST SynADL dataset are recorded with a <math id="S3.SS8.p1.3.m3.1" class="ltx_Math" alttext="640\times 360" display="inline"><semantics id="S3.SS8.p1.3.m3.1a"><mrow id="S3.SS8.p1.3.m3.1.1" xref="S3.SS8.p1.3.m3.1.1.cmml"><mn id="S3.SS8.p1.3.m3.1.1.2" xref="S3.SS8.p1.3.m3.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS8.p1.3.m3.1.1.1" xref="S3.SS8.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS8.p1.3.m3.1.1.3" xref="S3.SS8.p1.3.m3.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.3.m3.1b"><apply id="S3.SS8.p1.3.m3.1.1.cmml" xref="S3.SS8.p1.3.m3.1.1"><times id="S3.SS8.p1.3.m3.1.1.1.cmml" xref="S3.SS8.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS8.p1.3.m3.1.1.2.cmml" xref="S3.SS8.p1.3.m3.1.1.2">640</cn><cn type="integer" id="S3.SS8.p1.3.m3.1.1.3.cmml" xref="S3.SS8.p1.3.m3.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.3.m3.1c">640\times 360</annotation></semantics></math> resolution at 20 FPS, and corresponding 2D and 3D skeleton data are saved in both OpenPose- and Kinect v2-based formats. As a result, KIST SynADL provides 462k RGB videos, 2D, and 3D skeleton data, covering 55 action classes, 28 camera viewpoints, 15 characters, five lighting conditions, and four backgrounds.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2010.14742/assets/x4.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>We provide three duration types in ElderSim to enhance applicability. The average video duration for each activity class provided in ElderSim is represented in different colors based on the duration types. Actions with a <span id="S3.F7.5.1" class="ltx_text ltx_font_italic">Succinct</span> duration type contain a single trial of an action. <span id="S3.F7.6.2" class="ltx_text ltx_font_italic">Iterative</span> duration-typed actions are performed repeatedly, but in a different way. Trivial motions are added to the <span id="S3.F7.7.3" class="ltx_text ltx_font_italic">Iterative</span> duration type to form a <span id="S3.F7.8.4" class="ltx_text ltx_font_italic">Combined</span> duration.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we experimentally validate and discuss the effect of augmenting our synthetic data, KIST SynADL (KIST) for training algorithms to recognize elders’ ADL. We begin by introducing two real-world datasets for the experiments and address how insufficient the existing public dataset (NTU RGB+D 120) is to cover the elders’ ADL. We then describe three state-of-the-art HAR methods used in the experiments as well as several experimental scenarios to examine the various aspects arising from the recognition of the elders’ ADL. Within each experimental scenario, we investigate how our synthetic data can help recognize elders’ daily activities and offer some guidance and insights for effective utilization of synthetic data.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We now introduce real datasets used in the experiments and explain how their activity classes are selected to match the elders’ ADL. Samples of the datasets are visualized in Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.1 Datasets ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<table id="S4.F8.18" class="ltx_tabular ltx_align_middle">
<tr id="S4.F8.4.4" class="ltx_tr">
<td id="S4.F8.4.4.5" class="ltx_td ltx_align_justify">
<span id="S4.F8.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.4.4.5.1.1" class="ltx_p">1. eat</span>
</span>
</td>
<td id="S4.F8.1.1.1" class="ltx_td ltx_align_justify">
<span id="S4.F8.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.1.1.1.1.1" class="ltx_p"><span id="S4.F8.1.1.1.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#800080;border-color: #800080;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/kist_1.png" id="S4.F8.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.2.2.2" class="ltx_td ltx_align_justify">
<span id="S4.F8.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.2.2.2.1.1" class="ltx_p"><span id="S4.F8.2.2.2.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#FFBFBF;border-color: #FFBFBF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_e_1.png" id="S4.F8.2.2.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="595" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.3.3.3" class="ltx_td ltx_align_justify">
<span id="S4.F8.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.3.3.3.1.1" class="ltx_p"><span id="S4.F8.3.3.3.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_y_1.png" id="S4.F8.3.3.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.4.4.4" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S4.F8.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.4.4.4.1.1" class="ltx_p"><span id="S4.F8.4.4.4.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/ntu_1.png" id="S4.F8.4.4.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="595" alt="Refer to caption"></span></span>
</span>
</td>
</tr>
<tr id="S4.F8.8.8" class="ltx_tr">
<td id="S4.F8.8.8.5" class="ltx_td ltx_align_justify">
<span id="S4.F8.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.8.8.5.1.1" class="ltx_p">2. drink</span>
</span>
</td>
<td id="S4.F8.5.5.1" class="ltx_td ltx_align_justify">
<span id="S4.F8.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.5.5.1.1.1" class="ltx_p"><span id="S4.F8.5.5.1.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#800080;border-color: #800080;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/kist_2.png" id="S4.F8.5.5.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.6.6.2" class="ltx_td ltx_align_justify">
<span id="S4.F8.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.6.6.2.1.1" class="ltx_p"><span id="S4.F8.6.6.2.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#FFBFBF;border-color: #FFBFBF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_e_2.png" id="S4.F8.6.6.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.7.7.3" class="ltx_td ltx_align_justify">
<span id="S4.F8.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.7.7.3.1.1" class="ltx_p"><span id="S4.F8.7.7.3.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_y_2.png" id="S4.F8.7.7.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.8.8.4" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S4.F8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.8.8.4.1.1" class="ltx_p"><span id="S4.F8.8.8.4.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/ntu_2.png" id="S4.F8.8.8.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption"></span></span>
</span>
</td>
</tr>
<tr id="S4.F8.12.12" class="ltx_tr">
<td id="S4.F8.12.12.5" class="ltx_td ltx_align_justify">
<span id="S4.F8.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.12.12.5.1.1" class="ltx_p">13. phone call</span>
</span>
</td>
<td id="S4.F8.9.9.1" class="ltx_td ltx_align_justify">
<span id="S4.F8.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.9.9.1.1.1" class="ltx_p"><span id="S4.F8.9.9.1.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#800080;border-color: #800080;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/kist_3.png" id="S4.F8.9.9.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.10.10.2" class="ltx_td ltx_align_justify">
<span id="S4.F8.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.10.10.2.1.1" class="ltx_p"><span id="S4.F8.10.10.2.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#FFBFBF;border-color: #FFBFBF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_e_3.png" id="S4.F8.10.10.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.11.11.3" class="ltx_td ltx_align_justify">
<span id="S4.F8.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.11.11.3.1.1" class="ltx_p"><span id="S4.F8.11.11.3.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_y_3.png" id="S4.F8.11.11.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.12.12.4" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S4.F8.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.12.12.4.1.1" class="ltx_p"><span id="S4.F8.12.12.4.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/ntu_3.png" id="S4.F8.12.12.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
</tr>
<tr id="S4.F8.16.16" class="ltx_tr">
<td id="S4.F8.16.16.5" class="ltx_td ltx_align_justify">
<span id="S4.F8.16.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.16.16.5.1.1" class="ltx_p">14. use computer</span>
</span>
</td>
<td id="S4.F8.13.13.1" class="ltx_td ltx_align_justify">
<span id="S4.F8.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.13.13.1.1.1" class="ltx_p"><span id="S4.F8.13.13.1.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#800080;border-color: #800080;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/kist_4.png" id="S4.F8.13.13.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.14.14.2" class="ltx_td ltx_align_justify">
<span id="S4.F8.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.14.14.2.1.1" class="ltx_p"><span id="S4.F8.14.14.2.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#FFBFBF;border-color: #FFBFBF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_e_4.png" id="S4.F8.14.14.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.15.15.3" class="ltx_td ltx_align_justify">
<span id="S4.F8.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.15.15.3.1.1" class="ltx_p"><span id="S4.F8.15.15.3.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/etri_y_4.png" id="S4.F8.15.15.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
<td id="S4.F8.16.16.4" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S4.F8.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.16.16.4.1.1" class="ltx_p"><span id="S4.F8.16.16.4.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="color:#00FFFF;border-color: #00FFFF;padding:0.0pt;"><img src="/html/2010.14742/assets/figures/dataset_figure/ntu_4.png" id="S4.F8.16.16.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="597" alt="Refer to caption"></span></span>
</span>
</td>
</tr>
<tr id="S4.F8.18.18" class="ltx_tr">
<td id="S4.F8.18.18.3" class="ltx_td"></td>
<td id="S4.F8.18.18.4" class="ltx_td ltx_align_justify">
<span id="S4.F8.18.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.18.18.4.1.1" class="ltx_p"><span id="S4.F8.18.18.4.1.1.1" class="ltx_text ltx_font_bold">KIST (Ours)</span></span>
</span>
</td>
<td id="S4.F8.17.17.1" class="ltx_td ltx_align_justify">
<span id="S4.F8.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.17.17.1.1.1" class="ltx_p"><span id="S4.F8.17.17.1.1.1.1" class="ltx_text ltx_font_bold">ETRI<sub id="S4.F8.17.17.1.1.1.1.1" class="ltx_sub"><span id="S4.F8.17.17.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">E</span></sub> [24]</span></span>
</span>
</td>
<td id="S4.F8.18.18.2" class="ltx_td ltx_align_justify">
<span id="S4.F8.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.18.18.2.1.1" class="ltx_p"><span id="S4.F8.18.18.2.1.1.1" class="ltx_text ltx_font_bold">ETRI<sub id="S4.F8.18.18.2.1.1.1.1" class="ltx_sub"><span id="S4.F8.18.18.2.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Y</span></sub> [24]</span></span>
</span>
</td>
<td id="S4.F8.18.18.5" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S4.F8.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F8.18.18.5.1.1" class="ltx_p"><span id="S4.F8.18.18.5.1.1.1" class="ltx_text ltx_font_bold">NTU [22]</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Sample RGB snapshots and 2D skeleton coordinates of the datasets used in the experiments. The border color of each sample indicates both data type and age group (purple: synthetic-elderly data, pink: real-elderly data, cyan: real-young data).</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>ETRI-Activity3D Dataset</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">As introduced in Section <a href="#S2" title="2 Related Work ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the ETRI-Activity3D (ETRI) [24] dataset has been recently released for care robot applications to recognize the ADL of the elders’. This dataset contains 55 activity classes performed by a hundred subjects (composed of 50 elder and 50 young subjects) and captured from eight robot-viewpoints (installed in four locations each having two different heights) under constant lighting conditions. Since the dataset contains a large number of data with 112,564 samples, we utilize the ETRI dataset as our primary real-world dataset to train and evaluate three HAR models on elders’ ADL.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>NTU RGB+D 120 Dataset</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The NTU RGB+D 120 [22] dataset consists of 120 kinds of activities performed by relatively young subjects in a laboratory environment. This dataset includes multiple trials of actions captured from five camera viewpoints installed at a constant height and under a constant lighting condition. Among 120 activities of [22], we construct the NTU dataset by selecting only 25 activities with 23,436 samples that match the 55 frequent ADL considered in the ETRI dataset. Such activities of the NTU dataset are mapped to the 23 activities of the ETRI dataset as shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1.2 NTU RGB+D 120 Dataset ‣ 4.1 Datasets ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Even though [22] is one of the most widely investigated datasets in the HAR literature due to its large scale and diverse activity classes, there is still a severe gap in the subjects’ ages, backgrounds, and activity classes compared to the elders’ ADL held in households. Therefore models trained on this dataset may not generalize well to the elders’ ADL; we discuss such a point in the cross-dataset split with further details in Section <a href="#S4.SS2" title="4.2 Experimental Scenarios ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Combined activity categories of the KIST, ETRI, and NTU datasets for the cross-dataset split. In some cases, more than two classes from NTU are merged to match a single class label in other datasets.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3">Dataset</td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.1.1" class="ltx_p">Combined Label</span>
</span>
</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.2.1.1" class="ltx_p"><span id="S4.T1.1.2.2.1.1.1" class="ltx_text ltx_font_bold">KIST (Ours)</span></span>
</span>
</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.3.1.1" class="ltx_p"><span id="S4.T1.1.2.3.1.1.1" class="ltx_text ltx_font_bold">ETRI [24]</span></span>
</span>
</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.4.1.1" class="ltx_p"><span id="S4.T1.1.2.4.1.1.1" class="ltx_text ltx_font_bold">NTU [22]</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.1.1.1" class="ltx_p">1. eat</span>
</span>
</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.1.1" class="ltx_p">eat food with a fork</span>
</span>
</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.3.1.1" class="ltx_p">eat food with a fork</span>
</span>
</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.4.1.1" class="ltx_p">eat meal</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.1.1.1" class="ltx_p">2. drink</span>
</span>
</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.2.1.1" class="ltx_p">drink water</span>
</span>
</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.1.1" class="ltx_p">drink water</span>
</span>
</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.4.1.1" class="ltx_p">drink water</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.1.1.1" class="ltx_p">3. brush teeth</span>
</span>
</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.2.1.1" class="ltx_p">brush teeth</span>
</span>
</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.3.1.1" class="ltx_p">brush teeth</span>
</span>
</td>
<td id="S4.T1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.1.1" class="ltx_p">brush teeth</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.1.1.1" class="ltx_p">4. wash hands</span>
</span>
</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.2.1.1" class="ltx_p">wash hands</span>
</span>
</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.3.1.1" class="ltx_p">wash hands</span>
</span>
</td>
<td id="S4.T1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.4.1.1" class="ltx_p">rub two hands</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.7" class="ltx_tr">
<td id="S4.T1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.1.1.1" class="ltx_p">5. brush hair</span>
</span>
</td>
<td id="S4.T1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.2.1.1" class="ltx_p">brush hair</span>
</span>
</td>
<td id="S4.T1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.3.1.1" class="ltx_p">brush hair</span>
</span>
</td>
<td id="S4.T1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.4.1.1" class="ltx_p">brush hair</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.8" class="ltx_tr">
<td id="S4.T1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.1.1.1" class="ltx_p">6. wear clothes</span>
</span>
</td>
<td id="S4.T1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.2.1.1" class="ltx_p">put on jacket</span>
</span>
</td>
<td id="S4.T1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.3.1.1" class="ltx_p">put on jacket</span>
</span>
</td>
<td id="S4.T1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.4.1.1" class="ltx_p">put on jacket</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.9" class="ltx_tr">
<td id="S4.T1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.1.1.1" class="ltx_p">7. take off clothes</span>
</span>
</td>
<td id="S4.T1.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.2.1.1" class="ltx_p">take off jacket</span>
</span>
</td>
<td id="S4.T1.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.3.1.1" class="ltx_p">take off jacket</span>
</span>
</td>
<td id="S4.T1.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.4.1.1" class="ltx_p">take off jacket</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.10" class="ltx_tr">
<td id="S4.T1.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.1.1.1" class="ltx_p">8. put on/take off shoes</span>
</span>
</td>
<td id="S4.T1.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.2.1.1" class="ltx_p">put on/take off shoes</span>
</span>
</td>
<td id="S4.T1.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.3.1.1" class="ltx_p">put on/take off shoes</span>
</span>
</td>
<td id="S4.T1.1.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.4.1.1" class="ltx_p">put on+take off a shoe</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.11" class="ltx_tr">
<td id="S4.T1.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.1.1.1" class="ltx_p">9. put on/take off glasses</span>
</span>
</td>
<td id="S4.T1.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.2.1.1" class="ltx_p">put on/take off glasses</span>
</span>
</td>
<td id="S4.T1.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.3.1.1" class="ltx_p">put on/take off glasses</span>
</span>
</td>
<td id="S4.T1.1.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.4.1.1" class="ltx_p">put on+take off glasses</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.12" class="ltx_tr">
<td id="S4.T1.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.1.1.1" class="ltx_p">10. read</span>
</span>
</td>
<td id="S4.T1.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.2.1.1" class="ltx_p">read a book</span>
</span>
</td>
<td id="S4.T1.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.3.1.1" class="ltx_p">read a book</span>
</span>
</td>
<td id="S4.T1.1.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.4.1.1" class="ltx_p">read</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.13" class="ltx_tr">
<td id="S4.T1.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.1.1.1" class="ltx_p">11. write</span>
</span>
</td>
<td id="S4.T1.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.2.1.1" class="ltx_p">handwrite</span>
</span>
</td>
<td id="S4.T1.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.3.1.1" class="ltx_p">handwrite</span>
</span>
</td>
<td id="S4.T1.1.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.4.1.1" class="ltx_p">write</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.14" class="ltx_tr">
<td id="S4.T1.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.1.1.1" class="ltx_p">12. phone call</span>
</span>
</td>
<td id="S4.T1.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.2.1.1" class="ltx_p">talk on the phone</span>
</span>
</td>
<td id="S4.T1.1.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.3.1.1" class="ltx_p">talk on the phone</span>
</span>
</td>
<td id="S4.T1.1.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.4.1.1" class="ltx_p">phone call</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.15" class="ltx_tr">
<td id="S4.T1.1.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.1.1.1" class="ltx_p">13. play with phone</span>
</span>
</td>
<td id="S4.T1.1.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.2.1.1" class="ltx_p">play with a mobile phone</span>
</span>
</td>
<td id="S4.T1.1.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.3.1.1" class="ltx_p">play with a mobile phone</span>
</span>
</td>
<td id="S4.T1.1.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.4.1.1" class="ltx_p">play with phone/tablet</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.16" class="ltx_tr">
<td id="S4.T1.1.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.1.1.1" class="ltx_p">14. use computer</span>
</span>
</td>
<td id="S4.T1.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.2.1.1" class="ltx_p">use a computer</span>
</span>
</td>
<td id="S4.T1.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.3.1.1" class="ltx_p">use a computer</span>
</span>
</td>
<td id="S4.T1.1.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.4.1.1" class="ltx_p">type on a keyboard</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.17" class="ltx_tr">
<td id="S4.T1.1.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.1.1.1" class="ltx_p">15. clap</span>
</span>
</td>
<td id="S4.T1.1.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.2.1.1" class="ltx_p">clap</span>
</span>
</td>
<td id="S4.T1.1.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.3.1.1" class="ltx_p">clap</span>
</span>
</td>
<td id="S4.T1.1.17.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.4.1.1" class="ltx_p">clap</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.18" class="ltx_tr">
<td id="S4.T1.1.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.1.1.1" class="ltx_p">16. rub face</span>
</span>
</td>
<td id="S4.T1.1.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.2.1.1" class="ltx_p">rub face with hands</span>
</span>
</td>
<td id="S4.T1.1.18.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.3.1.1" class="ltx_p">rub face with hands</span>
</span>
</td>
<td id="S4.T1.1.18.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.4.1.1" class="ltx_p">wipe face</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.19" class="ltx_tr">
<td id="S4.T1.1.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.1.1.1" class="ltx_p">17. bow</span>
</span>
</td>
<td id="S4.T1.1.19.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.2.1.1" class="ltx_p">take a bow</span>
</span>
</td>
<td id="S4.T1.1.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.3.1.1" class="ltx_p">take a bow</span>
</span>
</td>
<td id="S4.T1.1.19.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.4.1.1" class="ltx_p">nod head/bow</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.20" class="ltx_tr">
<td id="S4.T1.1.20.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.1.1.1" class="ltx_p">18. handshake</span>
</span>
</td>
<td id="S4.T1.1.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.2.1.1" class="ltx_p">handshake</span>
</span>
</td>
<td id="S4.T1.1.20.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.3.1.1" class="ltx_p">handshake</span>
</span>
</td>
<td id="S4.T1.1.20.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.4.1.1" class="ltx_p">shake hands</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.21" class="ltx_tr">
<td id="S4.T1.1.21.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.1.1.1" class="ltx_p">19. hug</span>
</span>
</td>
<td id="S4.T1.1.21.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.2.1.1" class="ltx_p">hug each other</span>
</span>
</td>
<td id="S4.T1.1.21.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.3.1.1" class="ltx_p">hug each other</span>
</span>
</td>
<td id="S4.T1.1.21.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.4.1.1" class="ltx_p">hug</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.22" class="ltx_tr">
<td id="S4.T1.1.22.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.1.1.1" class="ltx_p">20. fight</span>
</span>
</td>
<td id="S4.T1.1.22.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.2.1.1" class="ltx_p">fight each other</span>
</span>
</td>
<td id="S4.T1.1.22.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.3.1.1" class="ltx_p">fight each other</span>
</span>
</td>
<td id="S4.T1.1.22.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.4.1.1" class="ltx_p">punch/slap</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.23" class="ltx_tr">
<td id="S4.T1.1.23.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.1.1.1" class="ltx_p">21. hand wave</span>
</span>
</td>
<td id="S4.T1.1.23.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.2.1.1" class="ltx_p">wave a hand</span>
</span>
</td>
<td id="S4.T1.1.23.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.3.1.1" class="ltx_p">wave a hand</span>
</span>
</td>
<td id="S4.T1.1.23.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.4.1.1" class="ltx_p">hand wave</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.24" class="ltx_tr">
<td id="S4.T1.1.24.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.24.1.1.1" class="ltx_p">22. point finger</span>
</span>
</td>
<td id="S4.T1.1.24.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.24.2.1.1" class="ltx_p">point with a finger</span>
</span>
</td>
<td id="S4.T1.1.24.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.24.3.1.1" class="ltx_p">point with a finger</span>
</span>
</td>
<td id="S4.T1.1.24.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.24.4.1.1" class="ltx_p">point to something</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.25" class="ltx_tr">
<td id="S4.T1.1.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.25.1.1.1" class="ltx_p">23. fall down</span>
</span>
</td>
<td id="S4.T1.1.25.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.25.2.1.1" class="ltx_p">fall down</span>
</span>
</td>
<td id="S4.T1.1.25.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.25.3.1.1" class="ltx_p">fallen on the floor</span>
</span>
</td>
<td id="S4.T1.1.25.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S4.T1.1.25.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.25.4.1.1" class="ltx_p">fall down</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Scenarios</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We now introduce several experimental scenarios considered in the experiments. We begin by explaining <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">cross-subject</span> and cross-view splits which are widely considered in the literature, and then introduce newly suggested cross-age and cross-dataset splits that assume the real and synthetic training datasets of different configurations.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">In the cross-subject or <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">cross-view</span> splits, it is assumed that the real-world training dataset has limitations on available camera subjects or viewpoints. We train models using data acquired from only a part of the available subjects or viewpoints of a dataset and leave the remaining data as the test set. In the cross-subject split, we train the models on data for 24 subjects of the ETRI dataset and test on the other 76 subjects. In our cross-view split, we train on data for the two viewpoints (the seventh and eighth viewpoints of [24]) of the ETRI dataset and test on the other six viewpoints.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.4" class="ltx_p">As an extension of the cross-subject split, we assume the situation in which the training data are only limited to one age group while having the other age group for evaluation. In such <span id="S4.SS2.p3.4.1" class="ltx_text ltx_font_bold">cross-age</span> splits, we divide the ETRI dataset into two subject groups of different ages, 50 younger subjects with the average age of 23.6 (ETRI<sub id="S4.SS2.p3.4.2" class="ltx_sub"><span id="S4.SS2.p3.4.2.1" class="ltx_text ltx_font_italic">Y</span></sub>) and 50 elder subjects with the average age of 77.1 (ETRI<sub id="S4.SS2.p3.4.3" class="ltx_sub"><span id="S4.SS2.p3.4.3.1" class="ltx_text ltx_font_italic">E</span></sub>). We then train the models on ETRI<sub id="S4.SS2.p3.4.4" class="ltx_sub"><span id="S4.SS2.p3.4.4.1" class="ltx_text ltx_font_italic">Y</span></sub> and test on ETRI<sub id="S4.SS2.p3.4.5" class="ltx_sub"><span id="S4.SS2.p3.4.5.1" class="ltx_text ltx_font_italic">E</span></sub> and vice versa. From such a split, we investigate if there are some differences according to the age groups in the recognition performance as well as the effect of utilizing our synthetic data.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p">We further assume an extreme scenario of training a model to recognize ADL of the elders, while available data are far from those in several aspects. For example, data may be obtained only from young subjects in laboratory environments (e.g., the NTU dataset). In the <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">cross-dataset</span> split assuming this scenario, we train models on the NTU dataset and test on the ETRI dataset, which corresponds to the dataset on elder’s ADL. We then examine whether the models trained on the NTU dataset can be generalized well to the ETRI dataset and see if augmenting our synthetic data during training can help the generalization. Since the class composition of the datasets does not completely match each other, we define 23 combined classes based on the ETRI dataset and consider only 25 activity classes out of 120 for the NTU dataset to match the classes as explained in Section <a href="#S4.SS1" title="4.1 Datasets ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> (see Table <a href="#S4.T1" title="Table 1 ‣ 4.1.2 NTU RGB+D 120 Dataset ‣ 4.1 Datasets ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p">We then discuss the effect of training our synthetic KIST SynADL dataset for each scenario. From the baseline models trained without the KIST SynADL dataset, we investigate how action recognition performance varies when our synthetic dataset is augmented in the model training process. Furthermore, we vary the composition of the data used among the KIST SynADL dataset according to the scenarios. For example, since the ETRI dataset does not contain surveillance-viewpoints and diverse lighting conditions, we may use only robot-viewpoints and a default lighting condition of the KIST SynADL dataset when it is known to be tested on the ETRI dataset. For the later experiments, we use the abbreviation KIST for the KIST SynADL dataset containing only the default lighting condition. The experimental scenarios performed in this work is listed in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Experimental Scenarios ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> with the variation factors that differ between training and test datasets.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The experimental scenarios and the factors that differ between training and test datasets for each split.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">Variation Factor</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">Dataset</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">Subject</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">View</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">Age</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Cross-Subject</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">Cross-View</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">Cross-Age</td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">Cross-Dataset</td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T2.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training Details of HAR Methods</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">This section provides training details for the HAR methods utilized in the experiments, namely Glimpse Clouds (Glimpse) [11], Spatial Temporal Graph Convolution Network (ST-GCN) [12], and View Adaptive Convolutional Neural Network (VA-CNN) [13].</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">Glimpse [11] is a RGB-based model that uses a visual attention module over the spatio-temporal cube to generate a cloud of glimpse windows. These windows are then soft-assigned to a set of gated recurrent units (GRUs) [54] that track the windows and process classification. A loss function to appropriately locate the windows is added to the original cross-entropy loss. Here, we follow [11] and utilize the 2D skeleton data corresponding to the RGB data to encourage the training process with another loss term that helps the model to perform pose regression. The Adam optimizer is used in training with an initial learning rate of 1e-4. Training the whole architecture took 13 hours for ten epochs with a minibatch size of 32 using a single NVIDIA Tesla V100 PCIe GPU. During test time, only RGB data resized to a <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">256\times 256</annotation></semantics></math> resolution is used as an input. We sample eight frames from a video sequence as in [50] and extract three windows per frame as inputs for the recurrent units.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p">ST-GCN [12] represents 2D or 3D skeleton joint trajectories as a graph that connects nearby joints in a single frame and identical joints between consecutive frames. It then applies spatial temporal graph convolution on the constructed graph and captures the interaction between nearby joint groups and the temporal motions to facilitate action recognition. In this experiment, we apply ST-GCN on the 2D skeleton data. The 2D skeleton data of the real datasets are estimated from RGB videos using OpenPose, and pixel coordinates with the estimation confidence values of each joint are used as an input. We use the stochastic gradient descent to train ST-GCN models with batch size 64 for 50 epochs. The learning rates start at 0.1 and are reduced by 10 in epochs 20, 30, and 40. Moreover, when synthetic data is augmented in training, we split the last fully connected layer of the model so that the real and synthetic data can pass through different classifiers (except for the cross-dataset split). In this way the model empirically shows slightly better performance.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p">VA-CNN [14] represents 3D skeleton joint trajectories as a planar image by mapping joint index and time axes to height and width axes respectively, and recognizes the image using a convolutional neural network (CNN). The most distinctive feature of the method is that it adapts the input data view to enhance the recognition performance with a view adaptation subnetwork. The subnetwork used to adapt the view is also modeled using a CNN, and the whole model is trained in an end-to-end fashion. We utilize Adam optimizer to train VA-CNN with batch size 64 for 30 epochs. The learning rate starts at 1e-4 and reduces by 10 for every ten epochs. We use the Kinect v2-based format for the KIST SynADL dataset to match real datasets.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p">For the experiments augmenting the KIST SynADL dataset to train ST-GCN and VA-CNN, we balance mini-batches to contain an equal amount of real and synthetic data. Since the sizes of datasets differ, we randomly upsample the dataset of a smaller amount (usually the real-world data) to match the size. Twenty viewpoints of the KIST SynADL dataset were utilized for both methods, while only eight viewpoints were used for the Glimpse method to ensure reasonable training time.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">We now report the results of the experiments performed according to the above settings. In the experiments, we trained three recognition algorithms for the proposed experimental splits and report the average video sequence-level top-1 classification accuracy for the five test trials as the action recognition score. For the results obtained from augmenting the KIST SynADL dataset, we designate the change in the recognition score from that obtained without augmentation in the parenthesis next to the score.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Cross-Subject</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In the cross-subject split, 24 subjects (26,612 samples) from the ETRI dataset is sampled for the training set and evaluated by the remaining 76 subjects (85,912 samples) as explained in Section <a href="#S4.SS2" title="4.2 Experimental Scenarios ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. By augmenting synthetic data (26,400 samples for Glimpse and 66,000 samples for ST-GCN and VA-CNN) in training, each method’s performance increases by 3.31, 0.68, and 0.22 percent points as described in Table <a href="#S4.T3" title="Table 3 ‣ 4.4.1 Cross-Subject ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. While Glimpse shows the largest improvement, the absolute classification accuracy score is still lower than ST-GCN, showing confusion within some classes (e.g., data from <span id="S4.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">wash a towel by hands</span> class was frequently misclassified as <span id="S4.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_italic">wash hands</span> class) as illustrated in Fig. <a href="#S4.F9" title="Figure 9 ‣ 4.4.1 Cross-Subject ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. One should note that ST-GCN outperforms other methods in this cross-subject split of the ETRI dataset, while it performs worse than other considered methods on the NTU RGB+D [21] cross-subject split in the corresponding literature [11]–[13].</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy comparison for the cross-subject split.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Setting</td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3">Top-1 Accuracy (%)</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Train</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Test</td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Glimpse [11]</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ST-GCN [12]</td>
<td id="S4.T3.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA-CNN [13]</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">80.22</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">83.36</td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">81.98</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI+KIST</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T3.1.4.3.1" class="ltx_text ltx_font_bold">83.53 (+3.31)</span></td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T3.1.4.4.1" class="ltx_text ltx_font_bold">84.04 (+0.68)</span></td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T3.1.4.5.1" class="ltx_text ltx_font_bold">82.20 (+0.22)</span></td>
</tr>
</table>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2010.14742/assets/x5.png" id="S4.F9.g1" class="ltx_graphics ltx_img_landscape" width="461" height="352" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Normalized confusion matrix of the Glimpse method trained by augmenting synthetic data in the cross-subject split. Only the values over 0.1 are displayed for better visualization.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Cross-View</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">For the cross-view split, the data obtained from two camera viewpoints are used for training (26,757 samples) while the remaining six viewpoints (85,807 samples) are used for evaluation of the trained models. The cross-view results also demonstrate the benefit of training additional synthetic data for better performance as shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.4.2 Cross-View ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. While ST-GCN shows the largest increase, Glimpse outperforms other methods which also differ from the results in the baseline studies [11]–[13]. Based on observation, this implies that the accuracies based on benchmark datasets do not perfectly correspond with the performance on another dataset (ETRI).</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy comparison for the cross-view split.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Setting</td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3">Top-1 Accuracy (%)</td>
</tr>
<tr id="S4.T4.1.2" class="ltx_tr">
<td id="S4.T4.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Train</td>
<td id="S4.T4.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Test</td>
<td id="S4.T4.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Glimpse [11]</td>
<td id="S4.T4.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ST-GCN [12]</td>
<td id="S4.T4.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA-CNN [13]</td>
</tr>
<tr id="S4.T4.1.3" class="ltx_tr">
<td id="S4.T4.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T4.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T4.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">79.97</td>
<td id="S4.T4.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">77.88</td>
<td id="S4.T4.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">79.72</td>
</tr>
<tr id="S4.T4.1.4" class="ltx_tr">
<td id="S4.T4.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI+KIST</td>
<td id="S4.T4.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T4.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T4.1.4.3.1" class="ltx_text ltx_font_bold">81.59 (+1.62)</span></td>
<td id="S4.T4.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T4.1.4.4.1" class="ltx_text ltx_font_bold">80.84 (+2.96)</span></td>
<td id="S4.T4.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T4.1.4.5.1" class="ltx_text ltx_font_bold">80.00 (+0.28)</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Cross-Age</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS3.p1.8" class="ltx_p">In the cross-age split, we construct the training and test data from the ETRI dataset by splitting the data according to the subjects’ age as explained in Section <a href="#S4.SS2" title="4.2 Experimental Scenarios ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, and examine if augmenting our KIST SynADL dataset in training affects the recognition performance differently according to the age group. The action recognition performances for the cross-age split experiments are shown in Table <a href="#S4.T5" title="Table 5 ‣ 4.4.3 Cross-Age ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Similarly to the previous results, synthetic data augmentation enhances recognition performance in most of the cases. By focusing on the performance change (the values placed in parentheses) induced from augmenting synthetic data, we observe that our synthetic data affect the recognition performance in a somewhat age-specific way, i.e., the augmentation seems more beneficial for the models trained on ETRI<sub id="S4.SS4.SSS3.p1.8.1" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.1.1" class="ltx_text ltx_font_italic">Y</span></sub> (and tested on ETRI<sub id="S4.SS4.SSS3.p1.8.2" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.2.1" class="ltx_text ltx_font_italic">E</span></sub>) rather than those trained on ETRI<sub id="S4.SS4.SSS3.p1.8.3" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.3.1" class="ltx_text ltx_font_italic">E</span></sub> (and tested on ETRI<sub id="S4.SS4.SSS3.p1.8.4" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.4.1" class="ltx_text ltx_font_italic">Y</span></sub>). This effect is the most evident for the Glimpse method, which has the highest performance gain among the considered methods for the models trained on ETRI<sub id="S4.SS4.SSS3.p1.8.5" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.5.1" class="ltx_text ltx_font_italic">Y</span></sub> and even shows a performance decrease for the models trained on ETRI<sub id="S4.SS4.SSS3.p1.8.6" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.6.1" class="ltx_text ltx_font_italic">E</span></sub>. Another interesting point to note is that, as can be observed in Table <a href="#S4.T5" title="Table 5 ‣ 4.4.3 Cross-Age ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the actions in ETRI<sub id="S4.SS4.SSS3.p1.8.7" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.7.1" class="ltx_text ltx_font_italic">Y</span></sub> seem to be more challenging to classify than the actions in ETRI<sub id="S4.SS4.SSS3.p1.8.8" class="ltx_sub"><span id="S4.SS4.SSS3.p1.8.8.1" class="ltx_text ltx_font_italic">E</span></sub> when the models are trained on the other data. This tendency agrees with the observation that the actions of the young subjects usually have larger motion differentials and shorter durations than the motions performed by the elders hence contain a wider variety [24]. Comparing the three HAR methods, ST-GCN outperforms other methods in the cross-age split.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy comparison for the cross-age split.</figcaption>
<table id="S4.T5.8" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.8.9" class="ltx_tr">
<td id="S4.T5.8.9.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Setting</td>
<td id="S4.T5.8.9.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3">Top-1 Accuracy (%)</td>
</tr>
<tr id="S4.T5.8.10" class="ltx_tr">
<td id="S4.T5.8.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Train</td>
<td id="S4.T5.8.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Test</td>
<td id="S4.T5.8.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Glimpse [11]</td>
<td id="S4.T5.8.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ST-GCN [12]</td>
<td id="S4.T5.8.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA-CNN [13]</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.1.1.1.1" class="ltx_sub"><span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.2.2.2.1" class="ltx_sub"><span id="S4.T5.2.2.2.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T5.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.2.2.3.1" class="ltx_text ltx_font_bold">74.96</span></td>
<td id="S4.T5.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">77.52</td>
<td id="S4.T5.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">77.52</td>
</tr>
<tr id="S4.T5.4.4" class="ltx_tr">
<td id="S4.T5.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.3.3.1.1" class="ltx_sub"><span id="S4.T5.3.3.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>+KIST</td>
<td id="S4.T5.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.4.4.2.1" class="ltx_sub"><span id="S4.T5.4.4.2.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T5.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">73.90 (-1.06)</td>
<td id="S4.T5.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.4.4.4.1" class="ltx_text ltx_font_bold">78.12 (+0.60)</span></td>
<td id="S4.T5.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.4.4.5.1" class="ltx_text ltx_font_bold">78.00 (+0.48)</span></td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<td id="S4.T5.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.5.5.1.1" class="ltx_sub"><span id="S4.T5.5.5.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T5.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.6.6.2.1" class="ltx_sub"><span id="S4.T5.6.6.2.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T5.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">75.35</td>
<td id="S4.T5.6.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">79.32</td>
<td id="S4.T5.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">78.06</td>
</tr>
<tr id="S4.T5.8.8" class="ltx_tr">
<td id="S4.T5.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.7.7.1.1" class="ltx_sub"><span id="S4.T5.7.7.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>+KIST</td>
<td id="S4.T5.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T5.8.8.2.1" class="ltx_sub"><span id="S4.T5.8.8.2.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T5.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.8.8.3.1" class="ltx_text ltx_font_bold">77.74 (+2.41)</span></td>
<td id="S4.T5.8.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.8.8.4.1" class="ltx_text ltx_font_bold">80.38 (+1.06)</span></td>
<td id="S4.T5.8.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T5.8.8.5.1" class="ltx_text ltx_font_bold">78.18 (+0.12)</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Cross-Dataset</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS4.p1.2" class="ltx_p">From the cross-dataset split, as explained in Section <a href="#S4.SS2" title="4.2 Experimental Scenarios ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we examine whether a model trained on the NTU dataset (data from the young subjects in a laboratory background) can be generalized well to another (the ETRI dataset obtained from the elder subjects in daily-living environments) as well as the effect of augmenting our synthetic data during training. Table <a href="#S4.T6" title="Table 6 ‣ 4.4.4 Cross-Dataset ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the result from the cross-dataset split tested on the ETRI dataset. The recognition performances for the cross-dataset split are lower than the results obtained from the former splits, in which the (real) training and test data come from a common dataset. These results imply that, for the eldercare services, it may not be sufficient to utilize deep models trained only on the NTU dataset, despite its large-scale.
When synthetic data are augmented in training, we observe a firm performance increase for all the considered HAR methods for the cross-data split. The improvement gap is in general larger than the previous splits, with a remarkable boost for the Glimpse method (even over 13 percent point when tested on the ETRI dataset). Such a considerable increase in the Glimpse method may be partially because meaningful background information contained in RGB videos, which might be helpful to distinguish which activities are performed, is provided to the model from our synthetic data. In contrast, the NTU dataset alone might not provide much information on the backgrounds due to its limited laboratory setting. In Table <a href="#S4.T6" title="Table 6 ‣ 4.4.4 Cross-Dataset ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, it is also interesting to observe that the recognition performance tested on ETRI<sub id="S4.SS4.SSS4.p1.2.1" class="ltx_sub"><span id="S4.SS4.SSS4.p1.2.1.1" class="ltx_text ltx_font_italic">Y</span></sub> is higher than that on ETRI<sub id="S4.SS4.SSS4.p1.2.2" class="ltx_sub"><span id="S4.SS4.SSS4.p1.2.2.1" class="ltx_text ltx_font_italic">E</span></sub>; this may result from the fact that the NTU dataset contains actions of relatively young subjects.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Accuracy comparison for the cross-dataset split tested on the ETRI dataset.</figcaption>
<table id="S4.T6.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.4.5" class="ltx_tr">
<td id="S4.T6.4.5.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Setting</td>
<td id="S4.T6.4.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3">Top-1 Accuracy (%)</td>
</tr>
<tr id="S4.T6.4.6" class="ltx_tr">
<td id="S4.T6.4.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Train</td>
<td id="S4.T6.4.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Test</td>
<td id="S4.T6.4.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Glimpse [11]</td>
<td id="S4.T6.4.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ST-GCN [12]</td>
<td id="S4.T6.4.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA-CNN [13]</td>
</tr>
<tr id="S4.T6.4.7" class="ltx_tr">
<td id="S4.T6.4.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">NTU</td>
<td id="S4.T6.4.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T6.4.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">39.99</td>
<td id="S4.T6.4.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">46.92</td>
<td id="S4.T6.4.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">43.00</td>
</tr>
<tr id="S4.T6.4.8" class="ltx_tr">
<td id="S4.T6.4.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">NTU+KIST</td>
<td id="S4.T6.4.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T6.4.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.8.3.1" class="ltx_text ltx_font_bold">54.79 (+14.80)</span></td>
<td id="S4.T6.4.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.8.4.1" class="ltx_text ltx_font_bold">49.76 (+2.84)</span></td>
<td id="S4.T6.4.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.8.5.1" class="ltx_text ltx_font_bold">46.32 (+3.32)</span></td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">NTU</td>
<td id="S4.T6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T6.1.1.1.1" class="ltx_sub"><span id="S4.T6.1.1.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">38.61</td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">45.66</td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">41.30</td>
</tr>
<tr id="S4.T6.2.2" class="ltx_tr">
<td id="S4.T6.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">NTU+KIST</td>
<td id="S4.T6.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T6.2.2.1.1" class="ltx_sub"><span id="S4.T6.2.2.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T6.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.2.2.3.1" class="ltx_text ltx_font_bold">55.00 (+16.39)</span></td>
<td id="S4.T6.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.2.2.4.1" class="ltx_text ltx_font_bold">48.46 (+2.80)</span></td>
<td id="S4.T6.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.2.2.5.1" class="ltx_text ltx_font_bold">45.00 (+3.70)</span></td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<td id="S4.T6.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">NTU</td>
<td id="S4.T6.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T6.3.3.1.1" class="ltx_sub"><span id="S4.T6.3.3.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">41.18</td>
<td id="S4.T6.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">48.08</td>
<td id="S4.T6.3.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">44.58</td>
</tr>
<tr id="S4.T6.4.4" class="ltx_tr">
<td id="S4.T6.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">NTU+KIST</td>
<td id="S4.T6.4.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T6.4.4.1.1" class="ltx_sub"><span id="S4.T6.4.4.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T6.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.4.3.1" class="ltx_text ltx_font_bold">54.62 (+13.44)</span></td>
<td id="S4.T6.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.4.4.1" class="ltx_text ltx_font_bold">50.92 (+2.84)</span></td>
<td id="S4.T6.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T6.4.4.5.1" class="ltx_text ltx_font_bold">47.48 (+2.90)</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>Comparing VA-CNN and a Simpler Model Trained by Augmenting Synthetic Data</h4>

<div id="S4.SS4.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS5.p1.1" class="ltx_p">Using the dataset splits considered so far, we now propose a simple experiment to compare the effect of synthetic data augmentation to that of improving HAR neural network models. For ease of comparison, we adopt the VA-CNN model, in which the improvement of the neural network model is represented by implementing the view adaptation subnetwork (or VA module) [13]. Specifically, we compare the recognition performance of the baseline model, i.e., the VA-CNN without the VA module, trained by augmenting synthetic data and the VA-CNN method (the improved model from the baseline). According to the results in Table <a href="#S4.T7" title="Table 7 ‣ 4.4.5 Comparing VA-CNN and a Simpler Model Trained by Augmenting Synthetic Data ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the recognition performances from both settings are comparable to each other. In the cross-dataset split, augmenting synthetic data is superior to the model improvement. These results indicate that effective utilization of synthetic data during training can be a viable option for better HAR performance, as increasing the complexity of a neural network architecture.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Accuracy comparison between the VA-CNN method and the baseline model trained by augmenting synthetic data (odd rows for the former and even rows for the latter).</figcaption>
<table id="S4.T7.12" class="ltx_tabular ltx_align_middle">
<tr id="S4.T7.12.13" class="ltx_tr">
<td id="S4.T7.12.13.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="4">Setting</td>
<td id="S4.T7.12.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">Top-1 Accuracy (%)</td>
</tr>
<tr id="S4.T7.12.14" class="ltx_tr">
<td id="S4.T7.12.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Split</td>
<td id="S4.T7.12.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Train</td>
<td id="S4.T7.12.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Test</td>
<td id="S4.T7.12.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA Module</td>
<td id="S4.T7.12.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">VA-CNN [13]</td>
</tr>
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S4.T7.1.1.2.1" class="ltx_text">Cross-Subject</span></td>
<td id="S4.T7.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S4.T7.1.1.1.1" class="ltx_text"><math id="S4.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\left\{\begin{array}[]{c}\text{ETRI}\\
\text{ETRI+}\text{KIST}\end{array}\right." display="inline"><semantics id="S4.T7.1.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.1.m1.1.2.2" xref="S4.T7.1.1.1.1.m1.1.2.1.cmml"><mo id="S4.T7.1.1.1.1.m1.1.2.2.1" xref="S4.T7.1.1.1.1.m1.1.2.1.1.cmml">{</mo><mtable rowspacing="0pt" id="S4.T7.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mtr id="S4.T7.1.1.1.1.m1.1.1a" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mtd id="S4.T7.1.1.1.1.m1.1.1b" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T7.1.1.1.1.m1.1.1.1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.1.1.1a.cmml">ETRI</mtext></mtd></mtr><mtr id="S4.T7.1.1.1.1.m1.1.1c" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mtd id="S4.T7.1.1.1.1.m1.1.1d" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mrow id="S4.T7.1.1.1.1.m1.1.1.2.1.1" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1c.cmml"><mtext id="S4.T7.1.1.1.1.m1.1.1.2.1.1a" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1c.cmml">ETRI+</mtext><mtext id="S4.T7.1.1.1.1.m1.1.1.2.1.1b" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1c.cmml">KIST</mtext></mrow></mtd></mtr></mtable><mi id="S4.T7.1.1.1.1.m1.1.2.2.2" xref="S4.T7.1.1.1.1.m1.1.2.1.1.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.m1.1.2.1.cmml" xref="S4.T7.1.1.1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.T7.1.1.1.1.m1.1.2.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.2.2.1">cases</csymbol><matrix id="S4.T7.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><matrixrow id="S4.T7.1.1.1.1.m1.1.1a.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><ci id="S4.T7.1.1.1.1.m1.1.1.1.1.1a.cmml" xref="S4.T7.1.1.1.1.m1.1.1.1.1.1"><mtext id="S4.T7.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.1.1.1">ETRI</mtext></ci></matrixrow><matrixrow id="S4.T7.1.1.1.1.m1.1.1b.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><ci id="S4.T7.1.1.1.1.m1.1.1.2.1.1c.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1"><mrow id="S4.T7.1.1.1.1.m1.1.1.2.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1"><mtext id="S4.T7.1.1.1.1.m1.1.1.2.1.1a.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1">ETRI+</mtext><mtext id="S4.T7.1.1.1.1.m1.1.1.2.1.1b.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2.1.1">KIST</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.m1.1c">\left\{\begin{array}[]{c}\text{ETRI}\\
\text{ETRI+}\text{KIST}\end{array}\right.</annotation></semantics></math></span></td>
<td id="S4.T7.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">81.98</td>
</tr>
<tr id="S4.T7.12.15" class="ltx_tr">
<td id="S4.T7.12.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.12.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.12.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.12.15.3.1" class="ltx_text ltx_font_bold">82.26 (+0.28)</span></td>
</tr>
<tr id="S4.T7.2.2" class="ltx_tr">
<td id="S4.T7.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S4.T7.2.2.2.1" class="ltx_text">Cross-View</span></td>
<td id="S4.T7.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S4.T7.2.2.1.1" class="ltx_text"><math id="S4.T7.2.2.1.1.m1.1" class="ltx_Math" alttext="\left\{\begin{array}[]{c}\text{ETRI}\\
\text{ETRI+}\text{KIST}\end{array}\right." display="inline"><semantics id="S4.T7.2.2.1.1.m1.1a"><mrow id="S4.T7.2.2.1.1.m1.1.2.2" xref="S4.T7.2.2.1.1.m1.1.2.1.cmml"><mo id="S4.T7.2.2.1.1.m1.1.2.2.1" xref="S4.T7.2.2.1.1.m1.1.2.1.1.cmml">{</mo><mtable rowspacing="0pt" id="S4.T7.2.2.1.1.m1.1.1" xref="S4.T7.2.2.1.1.m1.1.1.cmml"><mtr id="S4.T7.2.2.1.1.m1.1.1a" xref="S4.T7.2.2.1.1.m1.1.1.cmml"><mtd id="S4.T7.2.2.1.1.m1.1.1b" xref="S4.T7.2.2.1.1.m1.1.1.cmml"><mtext id="S4.T7.2.2.1.1.m1.1.1.1.1.1" xref="S4.T7.2.2.1.1.m1.1.1.1.1.1a.cmml">ETRI</mtext></mtd></mtr><mtr id="S4.T7.2.2.1.1.m1.1.1c" xref="S4.T7.2.2.1.1.m1.1.1.cmml"><mtd id="S4.T7.2.2.1.1.m1.1.1d" xref="S4.T7.2.2.1.1.m1.1.1.cmml"><mrow id="S4.T7.2.2.1.1.m1.1.1.2.1.1" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1c.cmml"><mtext id="S4.T7.2.2.1.1.m1.1.1.2.1.1a" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1c.cmml">ETRI+</mtext><mtext id="S4.T7.2.2.1.1.m1.1.1.2.1.1b" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1c.cmml">KIST</mtext></mrow></mtd></mtr></mtable><mi id="S4.T7.2.2.1.1.m1.1.2.2.2" xref="S4.T7.2.2.1.1.m1.1.2.1.1.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.1.1.m1.1b"><apply id="S4.T7.2.2.1.1.m1.1.2.1.cmml" xref="S4.T7.2.2.1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.T7.2.2.1.1.m1.1.2.1.1.cmml" xref="S4.T7.2.2.1.1.m1.1.2.2.1">cases</csymbol><matrix id="S4.T7.2.2.1.1.m1.1.1.cmml" xref="S4.T7.2.2.1.1.m1.1.1"><matrixrow id="S4.T7.2.2.1.1.m1.1.1a.cmml" xref="S4.T7.2.2.1.1.m1.1.1"><ci id="S4.T7.2.2.1.1.m1.1.1.1.1.1a.cmml" xref="S4.T7.2.2.1.1.m1.1.1.1.1.1"><mtext id="S4.T7.2.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.T7.2.2.1.1.m1.1.1.1.1.1">ETRI</mtext></ci></matrixrow><matrixrow id="S4.T7.2.2.1.1.m1.1.1b.cmml" xref="S4.T7.2.2.1.1.m1.1.1"><ci id="S4.T7.2.2.1.1.m1.1.1.2.1.1c.cmml" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1"><mrow id="S4.T7.2.2.1.1.m1.1.1.2.1.1.cmml" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1"><mtext id="S4.T7.2.2.1.1.m1.1.1.2.1.1a.cmml" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1">ETRI+</mtext><mtext id="S4.T7.2.2.1.1.m1.1.1.2.1.1b.cmml" xref="S4.T7.2.2.1.1.m1.1.1.2.1.1">KIST</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.1.1.m1.1c">\left\{\begin{array}[]{c}\text{ETRI}\\
\text{ETRI+}\text{KIST}\end{array}\right.</annotation></semantics></math></span></td>
<td id="S4.T7.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.2.2.5.1" class="ltx_text ltx_font_bold">79.72 (+0.04)</span></td>
</tr>
<tr id="S4.T7.12.16" class="ltx_tr">
<td id="S4.T7.12.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.12.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.12.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">79.68</td>
</tr>
<tr id="S4.T7.4.4" class="ltx_tr">
<td id="S4.T7.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="4"><span id="S4.T7.4.4.3.1" class="ltx_text">Cross-Age</span></td>
<td id="S4.T7.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="4"><span id="S4.T7.3.3.1.1" class="ltx_text"><math id="S4.T7.3.3.1.1.m1.4" class="ltx_Math" alttext="\left\{\begin{array}[]{c}\text{ETRI${}_{E}$}\\
\text{ETRI${}_{E}$+KIST}\\
\text{ETRI${}_{Y}$}\\
\text{ETRI${}_{Y}$+KIST}\\
\end{array}\right." display="inline"><semantics id="S4.T7.3.3.1.1.m1.4a"><mrow id="S4.T7.3.3.1.1.m1.4.5.2" xref="S4.T7.3.3.1.1.m1.4.5.1.cmml"><mo id="S4.T7.3.3.1.1.m1.4.5.2.1" xref="S4.T7.3.3.1.1.m1.4.5.1.1.cmml">{</mo><mtable rowspacing="0pt" id="S4.T7.3.3.1.1.m1.4.4" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mtr id="S4.T7.3.3.1.1.m1.4.4a" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mtd id="S4.T7.3.3.1.1.m1.4.4b" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mrow id="S4.T7.3.3.1.1.m1.1.1.1.1.1" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1e.cmml"><mtext id="S4.T7.3.3.1.1.m1.1.1.1.1.1a" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1e.cmml">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.1.1.1.1.1b" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1e.cmml"><sub id="S4.T7.3.3.1.1.m1.1.1.1.1.1.2nest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.1.1.1.1.1.2.1nest" class="ltx_text ltx_font_italic">E</span></sub></mtext></mrow></mtd></mtr><mtr id="S4.T7.3.3.1.1.m1.4.4c" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mtd id="S4.T7.3.3.1.1.m1.4.4d" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mrow id="S4.T7.3.3.1.1.m1.2.2.2.1.1" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1f.cmml"><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1a" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1f.cmml">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1b" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1f.cmml"><sub id="S4.T7.3.3.1.1.m1.2.2.2.1.1.2nest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.2.2.2.1.1.2.1nest" class="ltx_text ltx_font_italic">E</span></sub></mtext><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1e" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1f.cmml">+KIST</mtext></mrow></mtd></mtr><mtr id="S4.T7.3.3.1.1.m1.4.4e" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mtd id="S4.T7.3.3.1.1.m1.4.4f" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mrow id="S4.T7.3.3.1.1.m1.3.3.3.1.1" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1e.cmml"><mtext id="S4.T7.3.3.1.1.m1.3.3.3.1.1a" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1e.cmml">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.3.3.3.1.1b" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1e.cmml"><sub id="S4.T7.3.3.1.1.m1.3.3.3.1.1.2nest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.3.3.3.1.1.2.1nest" class="ltx_text ltx_font_italic">Y</span></sub></mtext></mrow></mtd></mtr><mtr id="S4.T7.3.3.1.1.m1.4.4g" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mtd id="S4.T7.3.3.1.1.m1.4.4h" xref="S4.T7.3.3.1.1.m1.4.4.cmml"><mrow id="S4.T7.3.3.1.1.m1.4.4.4.1.1" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1f.cmml"><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1a" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1f.cmml">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1b" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1f.cmml"><sub id="S4.T7.3.3.1.1.m1.4.4.4.1.1.2nest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.4.4.4.1.1.2.1nest" class="ltx_text ltx_font_italic">Y</span></sub></mtext><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1e" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1f.cmml">+KIST</mtext></mrow></mtd></mtr></mtable><mi id="S4.T7.3.3.1.1.m1.4.5.2.2" xref="S4.T7.3.3.1.1.m1.4.5.1.1.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.1.1.m1.4b"><apply id="S4.T7.3.3.1.1.m1.4.5.1.cmml" xref="S4.T7.3.3.1.1.m1.4.5.2"><csymbol cd="latexml" id="S4.T7.3.3.1.1.m1.4.5.1.1.cmml" xref="S4.T7.3.3.1.1.m1.4.5.2.1">cases</csymbol><matrix id="S4.T7.3.3.1.1.m1.4.4.cmml" xref="S4.T7.3.3.1.1.m1.4.4"><matrixrow id="S4.T7.3.3.1.1.m1.4.4a.cmml" xref="S4.T7.3.3.1.1.m1.4.4"><ci id="S4.T7.3.3.1.1.m1.1.1.1.1.1e.cmml" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1"><mrow id="S4.T7.3.3.1.1.m1.1.1.1.1.1.cmml" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1"><mtext id="S4.T7.3.3.1.1.m1.1.1.1.1.1a.cmml" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.1.1.1.1.1b.cmml" xref="S4.T7.3.3.1.1.m1.1.1.1.1.1"><sub id="S4.T7.3.3.1.1.m1.1.1.1.1.1.2anest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.1.1.1.1.1.2.1anest" class="ltx_text ltx_font_italic">E</span></sub></mtext></mrow></ci></matrixrow><matrixrow id="S4.T7.3.3.1.1.m1.4.4b.cmml" xref="S4.T7.3.3.1.1.m1.4.4"><ci id="S4.T7.3.3.1.1.m1.2.2.2.1.1f.cmml" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1"><mrow id="S4.T7.3.3.1.1.m1.2.2.2.1.1.cmml" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1"><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1a.cmml" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1b.cmml" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1"><sub id="S4.T7.3.3.1.1.m1.2.2.2.1.1.2anest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.2.2.2.1.1.2.1anest" class="ltx_text ltx_font_italic">E</span></sub></mtext><mtext id="S4.T7.3.3.1.1.m1.2.2.2.1.1e.cmml" xref="S4.T7.3.3.1.1.m1.2.2.2.1.1">+KIST</mtext></mrow></ci></matrixrow><matrixrow id="S4.T7.3.3.1.1.m1.4.4c.cmml" xref="S4.T7.3.3.1.1.m1.4.4"><ci id="S4.T7.3.3.1.1.m1.3.3.3.1.1e.cmml" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1"><mrow id="S4.T7.3.3.1.1.m1.3.3.3.1.1.cmml" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1"><mtext id="S4.T7.3.3.1.1.m1.3.3.3.1.1a.cmml" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.3.3.3.1.1b.cmml" xref="S4.T7.3.3.1.1.m1.3.3.3.1.1"><sub id="S4.T7.3.3.1.1.m1.3.3.3.1.1.2anest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.3.3.3.1.1.2.1anest" class="ltx_text ltx_font_italic">Y</span></sub></mtext></mrow></ci></matrixrow><matrixrow id="S4.T7.3.3.1.1.m1.4.4d.cmml" xref="S4.T7.3.3.1.1.m1.4.4"><ci id="S4.T7.3.3.1.1.m1.4.4.4.1.1f.cmml" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1"><mrow id="S4.T7.3.3.1.1.m1.4.4.4.1.1.cmml" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1"><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1a.cmml" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1">ETRI</mtext><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1b.cmml" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1"><sub id="S4.T7.3.3.1.1.m1.4.4.4.1.1.2anest" class="ltx_sub"><span id="S4.T7.3.3.1.1.m1.4.4.4.1.1.2.1anest" class="ltx_text ltx_font_italic">Y</span></sub></mtext><mtext id="S4.T7.3.3.1.1.m1.4.4.4.1.1e.cmml" xref="S4.T7.3.3.1.1.m1.4.4.4.1.1">+KIST</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.1.1.m1.4c">\left\{\begin{array}[]{c}\text{ETRI${}_{E}$}\\
\text{ETRI${}_{E}$+KIST}\\
\text{ETRI${}_{Y}$}\\
\text{ETRI${}_{Y}$+KIST}\\
\end{array}\right.</annotation></semantics></math></span></td>
<td id="S4.T7.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.4.4.2.1" class="ltx_sub"><span id="S4.T7.4.4.2.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T7.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.4.4.5.1" class="ltx_text ltx_font_bold">77.52 (+0.10)</span></td>
</tr>
<tr id="S4.T7.5.5" class="ltx_tr">
<td id="S4.T7.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.5.5.1.1" class="ltx_sub"><span id="S4.T7.5.5.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T7.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.5.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">77.42</td>
</tr>
<tr id="S4.T7.6.6" class="ltx_tr">
<td id="S4.T7.6.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.6.6.1.1" class="ltx_sub"><span id="S4.T7.6.6.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T7.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.6.6.3.1" class="ltx_text ltx_font_bold">78.06 (+0.18)</span></td>
</tr>
<tr id="S4.T7.7.7" class="ltx_tr">
<td id="S4.T7.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.7.7.1.1" class="ltx_sub"><span id="S4.T7.7.7.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T7.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.7.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">77.88</td>
</tr>
<tr id="S4.T7.8.8" class="ltx_tr">
<td id="S4.T7.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="6"><span id="S4.T7.8.8.2.1" class="ltx_text">Cross-Dataset</span></td>
<td id="S4.T7.8.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="6"><span id="S4.T7.8.8.1.1" class="ltx_text"><math id="S4.T7.8.8.1.1.m1.1" class="ltx_Math" alttext="\left\{\begin{array}[]{c}\text{NTU}\\
\text{NTU+}\text{KIST}\\
\text{NTU}\\
\text{NTU+}\text{KIST}\\
\text{NTU}\\
\text{NTU+}\text{KIST}\end{array}\right." display="inline"><semantics id="S4.T7.8.8.1.1.m1.1a"><mrow id="S4.T7.8.8.1.1.m1.1.2.2" xref="S4.T7.8.8.1.1.m1.1.2.1.cmml"><mo id="S4.T7.8.8.1.1.m1.1.2.2.1" xref="S4.T7.8.8.1.1.m1.1.2.1.1.cmml">{</mo><mtable rowspacing="0pt" id="S4.T7.8.8.1.1.m1.1.1" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtr id="S4.T7.8.8.1.1.m1.1.1a" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1b" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.1.1.1" xref="S4.T7.8.8.1.1.m1.1.1.1.1.1a.cmml">NTU</mtext></mtd></mtr><mtr id="S4.T7.8.8.1.1.m1.1.1c" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1d" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mrow id="S4.T7.8.8.1.1.m1.1.1.2.1.1" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1c.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.2.1.1a" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1c.cmml">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.2.1.1b" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1c.cmml">KIST</mtext></mrow></mtd></mtr><mtr id="S4.T7.8.8.1.1.m1.1.1e" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1f" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.3.1.1" xref="S4.T7.8.8.1.1.m1.1.1.3.1.1a.cmml">NTU</mtext></mtd></mtr><mtr id="S4.T7.8.8.1.1.m1.1.1g" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1h" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mrow id="S4.T7.8.8.1.1.m1.1.1.4.1.1" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1c.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.4.1.1a" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1c.cmml">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.4.1.1b" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1c.cmml">KIST</mtext></mrow></mtd></mtr><mtr id="S4.T7.8.8.1.1.m1.1.1i" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1j" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.5.1.1" xref="S4.T7.8.8.1.1.m1.1.1.5.1.1a.cmml">NTU</mtext></mtd></mtr><mtr id="S4.T7.8.8.1.1.m1.1.1k" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mtd id="S4.T7.8.8.1.1.m1.1.1l" xref="S4.T7.8.8.1.1.m1.1.1.cmml"><mrow id="S4.T7.8.8.1.1.m1.1.1.6.1.1" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1c.cmml"><mtext id="S4.T7.8.8.1.1.m1.1.1.6.1.1a" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1c.cmml">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.6.1.1b" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1c.cmml">KIST</mtext></mrow></mtd></mtr></mtable><mi id="S4.T7.8.8.1.1.m1.1.2.2.2" xref="S4.T7.8.8.1.1.m1.1.2.1.1.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.8.8.1.1.m1.1b"><apply id="S4.T7.8.8.1.1.m1.1.2.1.cmml" xref="S4.T7.8.8.1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.T7.8.8.1.1.m1.1.2.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.2.2.1">cases</csymbol><matrix id="S4.T7.8.8.1.1.m1.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><matrixrow id="S4.T7.8.8.1.1.m1.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.1.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.1.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.1.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.1.1.1">NTU</mtext></ci></matrixrow><matrixrow id="S4.T7.8.8.1.1.m1.1.1b.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.2.1.1c.cmml" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1"><mrow id="S4.T7.8.8.1.1.m1.1.1.2.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.2.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.2.1.1b.cmml" xref="S4.T7.8.8.1.1.m1.1.1.2.1.1">KIST</mtext></mrow></ci></matrixrow><matrixrow id="S4.T7.8.8.1.1.m1.1.1c.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.3.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.3.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.3.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.3.1.1">NTU</mtext></ci></matrixrow><matrixrow id="S4.T7.8.8.1.1.m1.1.1d.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.4.1.1c.cmml" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1"><mrow id="S4.T7.8.8.1.1.m1.1.1.4.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.4.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.4.1.1b.cmml" xref="S4.T7.8.8.1.1.m1.1.1.4.1.1">KIST</mtext></mrow></ci></matrixrow><matrixrow id="S4.T7.8.8.1.1.m1.1.1e.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.5.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.5.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.5.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.5.1.1">NTU</mtext></ci></matrixrow><matrixrow id="S4.T7.8.8.1.1.m1.1.1f.cmml" xref="S4.T7.8.8.1.1.m1.1.1"><ci id="S4.T7.8.8.1.1.m1.1.1.6.1.1c.cmml" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1"><mrow id="S4.T7.8.8.1.1.m1.1.1.6.1.1.cmml" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1"><mtext id="S4.T7.8.8.1.1.m1.1.1.6.1.1a.cmml" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1">NTU+</mtext><mtext id="S4.T7.8.8.1.1.m1.1.1.6.1.1b.cmml" xref="S4.T7.8.8.1.1.m1.1.1.6.1.1">KIST</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.8.8.1.1.m1.1c">\left\{\begin{array}[]{c}\text{NTU}\\
\text{NTU+}\text{KIST}\\
\text{NTU}\\
\text{NTU+}\text{KIST}\\
\text{NTU}\\
\text{NTU+}\text{KIST}\end{array}\right.</annotation></semantics></math></span></td>
<td id="S4.T7.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.8.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.8.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">43.00</td>
</tr>
<tr id="S4.T7.12.17" class="ltx_tr">
<td id="S4.T7.12.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI</td>
<td id="S4.T7.12.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.12.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.12.17.3.1" class="ltx_text ltx_font_bold">44.94 (+1.94)</span></td>
</tr>
<tr id="S4.T7.9.9" class="ltx_tr">
<td id="S4.T7.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.9.9.1.1" class="ltx_sub"><span id="S4.T7.9.9.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T7.9.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">41.30</td>
</tr>
<tr id="S4.T7.10.10" class="ltx_tr">
<td id="S4.T7.10.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.10.10.1.1" class="ltx_sub"><span id="S4.T7.10.10.1.1.1" class="ltx_text ltx_font_italic">E</span></sub>
</td>
<td id="S4.T7.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.10.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.10.10.3.1" class="ltx_text ltx_font_bold">43.70 (+2.40)</span></td>
</tr>
<tr id="S4.T7.11.11" class="ltx_tr">
<td id="S4.T7.11.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.11.11.1.1" class="ltx_sub"><span id="S4.T7.11.11.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T7.11.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td id="S4.T7.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">44.58</td>
</tr>
<tr id="S4.T7.12.12" class="ltx_tr">
<td id="S4.T7.12.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">ETRI<sub id="S4.T7.12.12.1.1" class="ltx_sub"><span id="S4.T7.12.12.1.1.1" class="ltx_text ltx_font_italic">Y</span></sub>
</td>
<td id="S4.T7.12.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td id="S4.T7.12.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.T7.12.12.3.1" class="ltx_text ltx_font_bold">46.10 (+1.52)</span></td>
</tr>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Considering eldercare applications, obtaining data of elders’ activities of daily living is necessary, but challenging. We take advantage of modern realistic rendering and visualization techniques to develop a platform named ElderSim and simulate a variety of daily activities of the elderly. Based on ElderSim, we generate a large-scale synthetic dataset of elders’ activities, KIST SynADL dataset, considering possible applications for care robots and smart surveillance systems. We then demonstrate the effectiveness of augmenting the KIST SynADL dataset in training from extensive experiments involving three state-of-the-art HAR methods as well as two different real-world human activity datasets. We show noticeable improvements of action recognition performance by augmenting our synthetic data. We also offer guidance and insights for the effective utilization of our synthetic data in human action recognition.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">In the future, we plan to enlarge the subject diversity by changing the body shape of the elderly and applying the corresponding motion styles to actions. Furthermore, we will extend our ElderSim platform by employing additional features of UnrealCV [55] to apply to more various problems in computer vision and robotics. Designing a domain-adaptive learning framework for HAR to further utilize our synthetic data would be another intriguing area of future research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">The authors thank Jaewang Lee for his supportive work with realistic 3D modeling of implemented features in ElderSim.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> A. Abou Allaban, M.Wang, and T. Padır, “A systematic review of robotics research in support of in-home care for older adults,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Information</span>, vol. 11, no. 2, p. 75, Feb. 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> R. Khosla and M. T. Chu, “Embodying care in Matilda: an affective communication robot for emotional wellbeing of older people in Australian residential care facilities,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Management Information Systems (TMIS)</span>, vol. 4, no. 4, pp. 1-33, Dec. 2013.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> A.K. Pandey and R. Gelin. “A mass-produced sociable humanoid robot: Pepper: The first machine of its kind.” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Robotics &amp; Automation Magazine 25</span>, no. 3, pp. 40-48, Jul. 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> M. Yu <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">et al.</span>, “A posture recognition-based fall detection system for monitoring an elderly person in a smart home environment.” <span id="bib.bib4.2.2" class="ltx_text ltx_font_italic">IEEE transactions on information technology in biomedicine 16</span>, no. 6, pp. 1274-1286, Aug. 2012.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> M. Al-Khafajiy <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Remote health monitoring of elderly through wearable sensors.” <span id="bib.bib5.2.2" class="ltx_text ltx_font_italic">Multimedia Tools and Applications 78</span>, no. 17, pp. 24681-24706, Sep. 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> L. Yu <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Personalized health monitoring system of elderly wellness at the community level in Hong Kong.” <span id="bib.bib6.2.2" class="ltx_text ltx_font_italic">IEEE Access 6</span>, pp. 35558-35567, Jun. 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> A. Karpathy <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Large-scale video classification with convolutional neural networks,” In <span id="bib.bib7.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 1725-1732, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> D. Tran <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Learning spatiotemporal features with 3d convolutional networks,” In <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision (ICCV)</span>, pp. 4489-4497, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model and the kinetics dataset,” In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 6299-6308, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> C. Feichtenhofer <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Slowfast networks for video recognition,” In <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">proceedings of the IEEE Conference on Computer Vision (ICCV)</span>, pp. 6202-6211, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> F. Baradel <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Glimpse clouds: Human activity recognition from unstructured feature points,” In <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 469-478, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks for skeleton-based action recognition,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1801.07455,</span> Jan. 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> P. Zhang <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">et al.</span>, “View adaptive neural networks for high performance skeleton-based human action recognition,” <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, vol. 41, no. 8, pp. 1963-1978, Jan. 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> L. Shi <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Skeleton-based action recognition with directed graph neural networks,” In <span id="bib.bib14.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 7912-7921, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> F. Li <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Multi-stream and Enhanced Spatial-temporal Graph Convolution Network for Skeleton-based Action Recognition,” <span id="bib.bib15.2.2" class="ltx_text ltx_font_italic">IEEE Access</span>, May. 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> S. Song <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">et al.</span>, “An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data,” <span id="bib.bib16.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.06067</span>, Nov. 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> G. Liu <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Action Recognition Based on 3D Skeleton and RGB Frame Fusion,” In <span id="bib.bib17.2.2" class="ltx_text ltx_font_italic">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pp. 258-264, Nov. 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?,” In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 6546-6555, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> W. Kay <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">et al.</span>, “The kinetics human action video dataset,” <span id="bib.bib19.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1705.06950</span>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> C. Liu <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding,” <span id="bib.bib20.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.07475</span>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> A. Shahroudy <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Ntu rgb+ d: A large scale dataset for 3d human activity analysis,” In <span id="bib.bib21.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</span>, pp. 1010-1019, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> J. Liu <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding,” <span id="bib.bib22.2.2" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, May. 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> Q. Kong <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">et al.</span>, “MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding,” In <span id="bib.bib23.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</span>, pp. 8658-8667, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> J. Jang <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">et al.</span>, “ETRIActivity3D: a large-scale rgb-d dataset for robots to recognize daily activities of the elderly,” <span id="bib.bib24.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.01920</span>, Mar. 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> S. Das <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Toyota smarthome: Real-world activities of daily living,” In <span id="bib.bib25.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</span>, pp. 833-842, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> A. L. Yuille and C. Liu, “Deep Nets: What have they ever done for Vision?,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1805.04025</span>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> X. Puig <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Virtualhome: Simulating household activities via programs,” In <span id="bib.bib27.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 8494-8502, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> P. Martinez-Gonzalez <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">et al.</span>, “UnrealROX: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation,” <span id="bib.bib28.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.06936</span>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> C. R. de Souza, <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Procedural generation of videos to train deep action recognition networks,” In <span id="bib.bib29.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 4757-4767, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05)</span>, vol. 1, pp. 886-893, Jun. 2005.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> N. Dalal, B. Triggs, and C. Schmid, “Human detection using oriented histograms of flow and appearance,” In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">European conference on computer vision (ECCV)</span>, pp. 428-441, May. 2006.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> I. Laptev <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Learning realistic human actions from movies,” In <span id="bib.bib32.2.2" class="ltx_text ltx_font_italic">2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 1-8, Jun. 2008.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> H. Wang <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Action recognition by dense trajectories,” In <span id="bib.bib33.2.2" class="ltx_text ltx_font_italic">CVPR</span>, pp. 3169-3176, Jun. 2011.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> H. Wang and C. Schmid, “Action recognition with improved trajectories,” In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pp. 3551-3558, 2013.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action recognition in videos,” In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pp. 568-576, 2014.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"> S. Ji <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">et al.</span>, “3D convolutional neural networks for human action recognition,” <span id="bib.bib36.2.2" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, vol. 35, no. 1, pp. 221-231, 2012.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"> S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1502.03167</span>, 2015.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"> Zhang, Z, “Microsoft kinect sensor and its effect,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">IEEE multimedia</span>, vo. 19, no. 2, pp. 4-10, 2012.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"> J. Shotton <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Real-time human pose recognition in parts from single depth images,” In <span id="bib.bib39.2.2" class="ltx_text ltx_font_italic">CVPR 2011</span>, pp. 1297-1304, Jun. 2011.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"> Z. Cao <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Realtime multi-person 2d pose estimation using part affinity fields,” In <span id="bib.bib40.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</span>, pp. 7291-7299, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"> C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a local SVM approach,” In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of the 17th International Conference on Pattern Recognition (ICPR)</span>, vol. 3, pp. 32-36, Aug. 2004.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"> W. Li, Z. Zhang, and Z. Liu, “Action recognition based on a bag of 3d points,” In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</span>, pp. 9-14, Jun. 2010.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"> J. Wang <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Mining actionlet ensemble for action recognition with depth cameras,” In <span id="bib.bib43.2.2" class="ltx_text ltx_font_italic">2012 IEEE Conference on Computer Vision and Pattern Recognition</span>, pp. 1290-1297, Jun. 2012.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"> B. Ni, G. Wang, and P. Moulin, “Rgbd-hudaact: A color-depth video database for human daily activity recognition,” In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</span>, pp. 1147-1153, Nov. 2011.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"> J. Wang <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Cross-view action modeling, learning and recognition,” In <span id="bib.bib45.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 2649-2656, 2014.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"> Y. Ji <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">et al.</span>, “A large-scale RGB-D database for Arbitrary-view Human Action Recognition,” In <span id="bib.bib46.2.2" class="ltx_text ltx_font_italic">Proceedings of the 26th ACM international Conference on Multimedia</span>, pp. 1510-1518, Oct. 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"> M. Khodabandeh <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Diy human action dataset generation,” In <span id="bib.bib47.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</span>, pp. 1448-1458, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"> L. Wang <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Generative multi-view human action recognition,” In <span id="bib.bib48.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</span>, pp. 6212-6221, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"> G. Varol <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Learning from synthetic humans,” In <span id="bib.bib49.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 109-117, 2017.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"> G. Varol <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Synthetic Humans for Action Recognition from Unseen Viewpoints,” <span id="bib.bib50.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04070</span>, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"> G. Ros <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">et al.</span>, “The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes,” In <span id="bib.bib51.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</span>, pp. 3234-3243, 2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"> D. Ludl, T. Gulde, and C. Curio, “Enhancing Data-Driven Algorithms for Human Pose Estimation and Action Recognition Through Simulation,” <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"> W. Deneke <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Towards a Simulation Platform for Generation of Synthetic Videos for Human Activity Recognition,” In <span id="bib.bib53.2.2" class="ltx_text ltx_font_italic">2018 International Conference on Computational Science and Computational Intelligence (CSCI)</span>, pp. 1234-1237, Dec. 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"> K. Cho <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">et al.</span>, “On the properties of neural machine translation: Encoder-decoder approaches,” <span id="bib.bib54.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1259</span>, 2014.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"> W. Qiu <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Unrealcv: Virtual worlds for computer vision,” In <span id="bib.bib55.2.2" class="ltx_text ltx_font_italic">Proceedings of the 25th ACM international conference on Multimedia</span>, pp. 1221-1224, 2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2010.14741" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2010.14742" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2010.14742">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2010.14742" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2010.14743" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 04:20:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
