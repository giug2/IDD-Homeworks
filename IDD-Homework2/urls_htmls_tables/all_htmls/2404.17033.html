<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.17033] Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation</title><meta property="og:description" content="The high cost of creating pixel-by-pixel gold-standard labels, limited expert availability, and presence of diverse tasks make it challenging to generate segmentation labels to train deep learning models for medical im…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.17033">

<!--Generated on Sun May  5 14:57:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.8" class="ltx_ERROR undefined">\jmlrvolume</span>
<p id="p1.7" class="ltx_p">– Accepted
<span id="p1.7.1" class="ltx_ERROR undefined">\jmlryear</span>2024
<span id="p1.7.2" class="ltx_ERROR undefined">\jmlrworkshop</span>Full Paper – MIDL 2024


<span id="p1.7.3" class="ltx_ERROR undefined">\midlauthor</span><span id="p1.7.4" class="ltx_ERROR undefined">\Name</span>Tanvi Deshpande<span id="p1.7.5" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.6" class="ltx_sup"><span id="p1.7.6.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.7" class="ltx_ERROR undefined">\Email</span>tanvimd@stanford.edu
<br class="ltx_break"><span id="p1.7.8" class="ltx_ERROR undefined">\addr</span><sup id="p1.7.9" class="ltx_sup"><span id="p1.7.9.1" class="ltx_text ltx_font_italic">1</span></sup> Stanford University  and <span id="p1.7.10" class="ltx_ERROR undefined">\Name</span>Eva Prakash<span id="p1.7.11" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.12" class="ltx_sup"><span id="p1.7.12.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.13" class="ltx_ERROR undefined">\Email</span>eprakash@stanford.edu
<br class="ltx_break"><span id="p1.7.14" class="ltx_ERROR undefined">\Name</span>Elsie Gyang Ross<span id="p1.7.15" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.16" class="ltx_sup"><span id="p1.7.16.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.17" class="ltx_ERROR undefined">\Email</span>elsie.ross@stanford.edu
<br class="ltx_break"><span id="p1.7.18" class="ltx_ERROR undefined">\Name</span>Curtis Langlotz<span id="p1.7.19" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.20" class="ltx_sup"><span id="p1.7.20.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.21" class="ltx_ERROR undefined">\Email</span>langlotz@stanford.edu
<br class="ltx_break"><span id="p1.7.22" class="ltx_ERROR undefined">\Name</span>Andrew Ng<span id="p1.7.23" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.24" class="ltx_sup"><span id="p1.7.24.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.25" class="ltx_ERROR undefined">\Email</span>ang@stanford.edu
<br class="ltx_break"><span id="p1.7.26" class="ltx_ERROR undefined">\Name</span>Jeya Maria Jose Valanarasu<span id="p1.7.27" class="ltx_ERROR undefined">\nametag</span><sup id="p1.7.28" class="ltx_sup"><span id="p1.7.28.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.7.29" class="ltx_ERROR undefined">\Email</span>jmjose@stanford.edu
<br class="ltx_break">
<span id="p1.7.30" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Auto-Generating Weak Labels for Real <math id="id1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="id1.m1.1b"><mo id="id1.m1.1.1" xref="id1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="id1.m1.1c"><and id="id1.m1.1.1.cmml" xref="id1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="id1.m1.1d">\&amp;</annotation></semantics></math> Synthetic Data to Improve Label-Scarce Medical Image Segmentation</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text" lang="en">The high cost of creating pixel-by-pixel gold-standard labels, limited expert availability, and presence of diverse tasks make it challenging to generate segmentation labels to train deep learning models for medical imaging tasks. In this work, we present a new approach to overcome the hurdle of costly medical image labeling by leveraging foundation models like Segment Anything Model (SAM) and its medical alternate MedSAM. Our pipeline has the ability to generate <em id="id2.id1.1.1" class="ltx_emph ltx_font_italic">weak labels</em> for any unlabeled medical image and subsequently use it to augment label-scarce datasets. We perform this by leveraging a model trained on a few gold-standard labels and using it to intelligently prompt MedSAM for weak label generation. This automation eliminates the manual prompting step in MedSAM, creating a streamlined process for generating labels for both real and synthetic images, regardless of quantity. We conduct experiments on label-scarce settings for multiple tasks pertaining to modalities ranging from ultrasound, dermatology, and X-rays to demonstrate the usefulness of our pipeline. The code is available at <a target="_blank" href="https://github.com/stanfordmlgroup/Auto-Generate-WLs/" title="" class="ltx_ref ltx_href">github.com/stanfordmlgroup/Auto-Generate-WLs/</a>.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
<span id="id3.id1" class="ltx_text" lang="en">
Label Scarcity, Weak Labeling, Segmentation, Synthetic Data</span>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_editors"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">editors: </span>Accepted at MIDL 2024</span></span></span>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The process of automatically identifying and delineating specific structures within medical images, i.e. segmentation, holds immense use-cases in various tasks, including diagnosis, treatment planning, and surgical procedures. Deep learning-based methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">Ronneberger et al. (2015)</a>, <a href="#bib.bib37" title="" class="ltx_ref">Zhou et al. (2018)</a>, <a href="#bib.bib22" title="" class="ltx_ref">Milletari et al. (2016)</a>, <a href="#bib.bib30" title="" class="ltx_ref">Valanarasu et al. (2020)</a>, <a href="#bib.bib31" title="" class="ltx_ref">Valanarasu et al. (2021)</a>, <a href="#bib.bib5" title="" class="ltx_ref">Chen et al. (2021)</a>, <a href="#bib.bib29" title="" class="ltx_ref">Tang et al. (2022)</a>, <a href="#bib.bib20" title="" class="ltx_ref">Ma et al. (2024)</a>]</cite> have recently constituted advancements in medical image segmentation. Most of these methods are fully supervised networks and need a good amount of data and labels for them to perform well.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While it is typical in computer vision tasks to obtain many segmentation labels through crowd workers, medical imaging tasks require experts to annotate the images. Due to the high cost of obtaining gold-standard labels from qualified medical professionals, practitioners often encounter the issue of <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">label scarcity</em> in many medical imaging tasks. Annotating medical images for segmentation is also particularly difficult due to the fine-grained nature of the label. In contrast, however, unlabeled data is more freely available. In addition, generative models like GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">Goodfellow et al. (2020)</a>]</cite> and diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">Kazerouni et al. (2023)</a>]</cite> can help generate synthetic medical images, further boosting the data size. However, this does not help augment datasets in a fully supervised pipeline, as we also need paired labels with the images to train the model.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_citet">Kirillov et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> released Segment Anything Model (SAM), a vision foundation model trained on over 11 million images and 1 billion masks. It has the unique ability to segment any image out-of-the-box in a zero-shot setting. Segmentation using SAM can be done automatically without any inputs. However, this typically does not lead to optimal performance, and so segmentation is usually done with the help of prompting techniques on the image like points or boxes <cite class="ltx_cite ltx_citemacro_cite">Kirillov et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. As SAM was trained on natural images, it is often found to be suboptimal when applied to medical imaging tasks <cite class="ltx_cite ltx_citemacro_cite">de Oliveira et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. To address this gap, <cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> introduced MedSAM, a fine-tuned version of SAM created specifically for the medical imaging domain, trained on over 1 million images across 15 modalities. MedSAM was found to be superior in terms of performance on a variety of medical image segmentation tasks when compared to SAM.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.17033/assets/weak_2_2.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Labels obtained from different configurations of SAM and MedSAM. Our method auto-generates effective input prompts (bounding boxes) using only very few annotations to generate high quality weak labels while SAM and MedSAM fail in auto options and are sensitive to input prompts in manual option.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Intuitively, MedSAM should help us annotate the unlabeled images out-of-the-box and obtain labels to augment our training dataset. However, one major limitation of SAM and MedSAM is their sensitivity to input prompts. Both models are only as good as the point or box inputs they receive on a given image. In specific, for medical images, the area of interest to be segmented may be particularly subtle within the image with thin fine-grained boundaries. Also, inputs that include too much of the target or background result in over- or undersegmentation, requiring precise input prompts to get a good segmentation. This can be seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where we show labels generated from SAM and MedSAM. MedSAM clearly gives us better labels than SAM, but it is very sensitive to the bounding box prompt we use and is still constrained by the manual prompting part. We also note that auto-prompting MedSAM performs poorly and often leads to blank segmentation predictions. Therefore, selecting an appropriate input prompt is key to ensuring success. Also, eliminating the need for manual prompting could allow practitioners to auto-generate labels for any number of unlabeled real or synthetic data, which would be very useful in label-scarce scenarios.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To this end, we present a new pipeline that tackles the challenge of limited labeled data by harnessing the power of foundation models like MedSAM. Our approach leverages coarse labels, generated by training segmentation models on few labels (ranging from 25 to 50), to guide the selection of inputs for unlabeled data fed into the SAM model. This process effectively creates a richer dataset, enabling the training of a significantly more accurate model while using only a few gold-standard labels and eliminating the need for time-consuming, expensive manual labeling. This enriched dataset fuels the training of deep learning models with improvements in the dice accuracy ranging from 6.6% to 72.3% for medical image segmentation datasets like BUSI, ISIC, and CANDID-PTX.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a pipeline that automatically generates efficient weak labels for any unlabeled data using MedSAM, eliminating the need for manual prompts.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that when using these weak labels to augment training datasets, we observe notable performance gains in deep learning models. This is especially impactful in scenarios with limited labeled data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct thorough validation of the method and ablation studies across ultrasound (BUSI), dermoscopy (ISIC), and X-ray (CANDID-PTX) datasets.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">After the introduction of SAM <cite class="ltx_cite ltx_citemacro_cite">Kirillov et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> and MedSAM <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, these vision foundation models have been directly applied to a myriad of medical imaging tasks, such as brain tumor segmentation <cite class="ltx_cite ltx_citemacro_cite">Peivandi et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, eye feature segmentation <cite class="ltx_cite ltx_citemacro_cite">Maquiling et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, liver tumor segmentation, and lung nodule segmentation <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, in both fine-tuning and zero-shot settings.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Several approaches have also been introduced tailoring SAM for specific medical imaging tasks. For example, <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> tailors SAM specifically for ultrasound segmentation by injecting features into SAM’s encoder through a parallel CNN branch. <cite class="ltx_cite ltx_citemacro_citet">Shin et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> jointly trains a network on heterogeneous ultrasound datasets using condition embedding blocks along with SAM, to allow SAM to adapt to each dataset separately. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> adapts SAM for 3D medical image segmentation, by incorporating 3D adapters into SAM’s encoder.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">There has also been work involving self-prompting SAM for medical image segmentation. One branch of such work involves separately learning prompts for SAM and combining them with SAM’s existing architecture. One such approach includes learning a pixel-wise classifier from SAM’s own embedding space and encoder to prompt SAM in few-shot settings <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib34" title="" class="ltx_ref">2024</a>)</cite>. In addition, <cite class="ltx_cite ltx_citemacro_citet">Lei et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> use extreme points from 3D medical images to generate 2D bounding-box prompts for SAM. <cite class="ltx_cite ltx_citemacro_citet">Pandey et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> first train YOLOv8 on several image-mask pairs to detect bounding-boxes for the regions to segment, which are then fed to SAM. Lastly, <cite class="ltx_cite ltx_citemacro_citet">Anand et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite> use localization to a template image to prompt SAM in one-shot settings. The other branch of such work involves altering SAM’s architecture to incorporate learning prompts. <cite class="ltx_cite ltx_citemacro_citet">Shaharabany et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> learn prompts for SAM by training a separate prompt encoder to automatically generate prompts, rather than conditioning on manual prompts, as SAM does. <cite class="ltx_cite ltx_citemacro_citet">Cui et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> use SAM to generate weak segmentation labels, which are then used to fine-tune SAM.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Annotations generated from SAM can also be used to augment existing segmentation pipelines, such as U-Nets <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>.
In addition, there has been work regarding sampling prompts from SAM <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, and SAM has been used for a variety of non-medical tasks, such as inpainting <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> and captioning <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a pipeline that automatically generates weak labels for both unlabeled real and synthetic data using MedSAM, eliminating the need for manual prompting.
Our pipeline includes training a small model on the available gold-standard labels, using predictions from this model to generate prompts for MedSAM, and retraining using a larger dataset consisting of the gold-standard and weak-labeled data. Our method is illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.17033/assets/weak_1_1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustration of our pipeline to auto-generate weak labels for unlabeled data. We use the limited annotations to train an initial model that generates low-quality coarse labels on unlabeled data. We then select inputs from these coarse labels as prompts to MedSAM to create higher-quality weak labels. These weak labels are used to train a stronger segmentation model.</figcaption>
</figure>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.3" class="ltx_p">A label-scarce scenario in a medical imaging setting can be defined as a task for which there exists a small amount of labeled (gold-standard) data, and a larger pool of unlabeled data. Let the small labeled dataset be denoted by <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{labeled}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1e" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.8" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.8.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2">𝑙</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4">𝑏</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6">𝑙</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.8">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">\mathcal{D}_{labeled}</annotation></semantics></math>, where we assume less than 50 gold-standard labels from medical practitioners. We chose this number after considering some real clinical scenarios, in which we encountered situations where it was difficult to obtain more than 50 gold standard labels. We acknowledge that this number may vary depending on the specific task, and therefore, we conduct ablation studies using different label quantities in Section 5. Notably, acquiring 50 high-quality labels is typically more feasible than the hundreds to thousands often required for traditional segmentation models. We define the larger pool of unlabeled data as <math id="S3.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{unlabeled}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.6.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1e" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.8" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1f" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.9" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1g" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.10" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.10.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.2">𝑢</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.3">𝑛</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.4">𝑙</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.5">𝑎</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.6">𝑏</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.7">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.8">𝑙</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.9.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.9">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.10.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.10">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.1c">\mathcal{D}_{unlabeled}</annotation></semantics></math>, which may comprise real medical scans or synthetic images generated by models like diffusion models. The evaluation set, denoted as <math id="S3.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{test}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.4">𝑠</ci><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">\mathcal{D}_{test}</annotation></semantics></math>, remains separate for performance assessment.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">We also define <span id="S3.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">weak labels</span> as labels that are not gold-standard (as they are not created by experts), but are still useful in improving the model performance.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Coarse Label Generation</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.3" class="ltx_p">To make use of the full potential of limited annotations, we first train an initial model <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\theta</annotation></semantics></math> on the labeled dataset <math id="S3.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{labeled}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a"><msub id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1a" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.4" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1b" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.5" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1c" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.6" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1d" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.7" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1e" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.8" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.8.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3"><times id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2">𝑙</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.4">𝑏</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.5">𝑒</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.6">𝑙</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.7">𝑒</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.8">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">\mathcal{D}_{labeled}</annotation></semantics></math>. This model is used to generate coarse labels for the unlabeled dataset <math id="S3.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{unlabeled}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1a" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.4" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1b" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.5" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1c" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.6" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.6.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1d" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.7" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1e" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.8" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1f" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.9" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1g" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.10" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.10.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3"><times id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2">𝑢</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.4">𝑙</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.5">𝑎</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.6">𝑏</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.7">𝑒</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.8">𝑙</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.9.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.9">𝑒</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.10.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.10">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.3.m3.1c">\mathcal{D}_{unlabeled}</annotation></semantics></math>. These coarse labels, despite potentially limited accuracy due to the model’s training on a smaller number of labels, serve as valuable inputs for automatically prompting MedSAM to produce higher-quality weak labels.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Input Selection</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">The prompts to MedSAM can be either points or bounding-boxes. Both “positive points” (corresponding to portions of the image to segment) and “negative points” (corresponding to portions of the image <em id="S3.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">not</em> to segment) can be given as input point prompts. For bounding-boxes, one needs to draw a box around their region of interest and feed that as the prompt input to MedSAM. Although these prompts are an easier alternative to pixel-by-pixel annotations, they still require manual effort and a good knowledge about the segmentation task.</p>
</div>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p2.1" class="ltx_p">To auto-generate labels using MedSAM without any manual intervention, we first pick up the coarse label prediction from the initial model <math id="S3.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.1.m1.1a"><mi id="S3.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.1.m1.1c">\theta</annotation></semantics></math>. If the coarse label is empty, we filter out the sample, since there is no basis for a prompt to MedSAM. If not, we pick the largest contiguous blocks from the coarse label and filter out small blocks, as those are likely to be noise. For point inputs, we use the middle of the largest contiguous region(s) of the coarse label as the prompt to MedSAM; in addition, low-probability points from the original prediction mask are used as “negative points” for the MedSAM model. For bounding-box inputs, we compute the minimum and maximum indices of the largest contiguous region(s) of the coarse label as the prompt to MedSAM. An example of the input-selection process is shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ Input Selection ‣ 3 Method ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We fixed these techniques after thorough experimentation on various input selection methods (App. <a href="#A3" title="Appendix C Input Selection ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>). While the input selection technique we describe works well for multiple tasks, we do acknowledge that for some tasks, there could be more intelligent prompting techniques that could perform better. However, we would like to point out that our input selection technique is much more generic and can be helpful in obtaining useful weak labels. It should also be noted that these weak labels are far better than the ones generated by SAM and MedSAM through automatic option (App <a href="#A2" title="Appendix B Comparison to SAM and MedSAM Automatic Segmentation ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.17033/assets/wl-examples.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="389" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of prompting and labeling from coarse label.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Labeling and Filtering</h4>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p">To ensure the quality of weak labels generated from unlabeled images, we implement a filtering process that eliminates extreme cases. Specifically, masks containing a disproportionately high percentage of pixels (<math id="S3.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="&gt;97\%" display="inline"><semantics id="S3.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mn id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">97</mn><mo id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.1" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1"><gt id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.2">absent</csymbol><apply id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.3.2">97</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px4.p1.1.m1.1c">&gt;97\%</annotation></semantics></math>) belonging to either the background or the segmented class are excluded. This filtering step safeguards against misleading or uninformative masks that could potentially hinder model performance.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Augmenting the training data</h4>

<div id="S3.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px5.p1.2" class="ltx_p">Finally, the weak labels generated for unlabeled images are combined with <math id="S3.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{labeled}" display="inline"><semantics id="S3.SS0.SSS0.Px5.p1.1.m1.1a"><msub id="S3.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1a" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.4" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1b" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.5" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1c" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.6" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1d" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.7" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1e" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.8" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.8.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3"><times id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2">𝑙</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.4">𝑏</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.5">𝑒</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.6">𝑙</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.7">𝑒</ci><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.8">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px5.p1.1.m1.1c">\mathcal{D}_{labeled}</annotation></semantics></math> to form the final <em id="S3.SS0.SSS0.Px5.p1.2.1" class="ltx_emph ltx_font_italic">augmented</em> dataset <math id="S3.SS0.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{augmented}" display="inline"><semantics id="S3.SS0.SSS0.Px5.p1.2.m2.1a"><msub id="S3.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.2.cmml">𝒟</mi><mrow id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1a" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.4" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1b" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.5" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1c" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.6" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1d" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.7" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1e" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.8" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1f" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.9" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1g" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.10" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.10.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px5.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.2">𝒟</ci><apply id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3"><times id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.2">𝑎</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.3">𝑢</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.4">𝑔</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.5">𝑚</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.6">𝑒</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.7">𝑛</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.8.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.8">𝑡</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.9.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.9">𝑒</ci><ci id="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.10.cmml" xref="S3.SS0.SSS0.Px5.p1.2.m2.1.1.3.10">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px5.p1.2.m2.1c">\mathcal{D}_{augmented}</annotation></semantics></math>, consisting of images, where a small number of labels (25-50) are gold-standard, and the rest are weak labels. This augmented dataset will be used in the final training of the model.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Data</h4>

<div id="S3.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px6.p1.1" class="ltx_p">In cases where data scarcity might further exacerbate label scarcity, we explore the potential of synthetic image generation as a means to potentially increase dataset size. Thus, we additionally generate synthetic images from each dataset in order to evaluate the performance of our pipeline on such data. We train a denoising diffusion probabilistic model (DDPM) with a UNet backbone following <cite class="ltx_cite ltx_citemacro_cite">Ho et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>. We set the hyperparameters following <cite class="ltx_cite ltx_citemacro_cite">Wolleb et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, notably with a learning rate of <math id="S3.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S3.SS0.SSS0.Px6.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.cmml"><mn id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.cmml"><mo id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3a" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px6.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1"><times id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2">1</cn><apply id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.2">𝑒</ci><apply id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3"><minus id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px6.p1.1.m1.1c">1e^{-4}</annotation></semantics></math>, image size of 256, and batch size of 4.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we give details about the datasets we use, experimental setup, and the results that validate the usefulness of our method.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">We utilize four datasets featuring 2D binary medical image segmentation tasks. Within each dataset, we randomly select <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">N</annotation></semantics></math> random samples from the training set to be the “gold-standard” labels. Note that we experiment on different values of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">N</annotation></semantics></math> to mimic different label-scarce settings. The rest of the training set is considered unlabeled data (i.e. we do not use the ground-truth labels in our experiments, but generate weak labels for these images). This approach enables us to train and evaluate our model without relying on the full set of ground-truth annotations.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">BUSI:</span>
The Breast Ultrasound Images Dataset (BUSI) consists of ultrasound images in three classes: benign, malignant, and normal <cite class="ltx_cite ltx_citemacro_cite">Al-Dhabyani et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>. We focus on segmentation of benign and malignant lesions from the ultrasound images. The number of samples with segmentation masks is 650. We use a randomized train-test split of 80–20, resulting in 130 samples in the test set.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">ISIC:</span>
The International Skin Imaging Collaboration (ISIC) dataset consists of dermoscopic lesion segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">Gutman et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>. It comprises 1279 dermoscopic lesion images, divided into a training set of 900 images and a test set of 379 images.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">CANDID:</span> We use a subset of the CANDID-PTX dataset <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>, consisting of 500 images, which consists of binary segmentation of lungs from chest X-rays. We use a randomized 80–20 train-test split.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">We use UNet++ <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite> as our base segmentation network. Please note that our method is agnostic to the choice of network; we picked UNet++ as it is stable in training on label-scarce conditions. We use a combination of DICE loss and binary cross-entropy loss with a scaling ratio of 1 and 0.5 respectively to train our models. We use the SGD optimizer with a learning rate of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msup id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mo id="S4.SS2.p1.1.m1.1.1.3a" xref="S4.SS2.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">10</cn><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><minus id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">10^{-3}</annotation></semantics></math>, and a cosine annealing learning rate scheduler with minimum learning rate <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msup id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mo id="S4.SS2.p1.2.m2.1.1.3a" xref="S4.SS2.p1.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">10</cn><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><minus id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">10^{-5}</annotation></semantics></math>. All images were also reshaped to a resolution of <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">256</cn><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">256\times 256</annotation></semantics></math>. We use a batch size of 4 and train for 100 epochs on NVIDIA A4000 GPUs.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">We provide the quantitative results for 3 datasets in Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Results ‣ 4 Experiments and Results ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in label-scarce settings. The performance of models trained for datasets augmented with weak labels is compared against the performance of a model trained on just the base dataset <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{labeled}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">𝒟</mi><mrow id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1a" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.4" xref="S4.SS3.p1.1.m1.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1b" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.5" xref="S4.SS3.p1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1c" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.6" xref="S4.SS3.p1.1.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1d" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.7" xref="S4.SS3.p1.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.3.1e" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.3.8" xref="S4.SS3.p1.1.m1.1.1.3.8.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝒟</ci><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><times id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.1"></times><ci id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">𝑙</ci><ci id="S4.SS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S4.SS3.p1.1.m1.1.1.3.4.cmml" xref="S4.SS3.p1.1.m1.1.1.3.4">𝑏</ci><ci id="S4.SS3.p1.1.m1.1.1.3.5.cmml" xref="S4.SS3.p1.1.m1.1.1.3.5">𝑒</ci><ci id="S4.SS3.p1.1.m1.1.1.3.6.cmml" xref="S4.SS3.p1.1.m1.1.1.3.6">𝑙</ci><ci id="S4.SS3.p1.1.m1.1.1.3.7.cmml" xref="S4.SS3.p1.1.m1.1.1.3.7">𝑒</ci><ci id="S4.SS3.p1.1.m1.1.1.3.8.cmml" xref="S4.SS3.p1.1.m1.1.1.3.8">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\mathcal{D}_{labeled}</annotation></semantics></math>, which consists of gold-standard (GS) labels. In addition, we compare against SAM’s automatic option as well as UniverSeg <cite class="ltx_cite ltx_citemacro_cite">Butoi et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>, using <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{labeled}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">𝒟</mi><mrow id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS3.p1.2.m2.1.1.3.2" xref="S4.SS3.p1.2.m2.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.3" xref="S4.SS3.p1.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1a" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.4" xref="S4.SS3.p1.2.m2.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1b" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.5" xref="S4.SS3.p1.2.m2.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1c" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.6" xref="S4.SS3.p1.2.m2.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1d" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.7" xref="S4.SS3.p1.2.m2.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1e" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.8" xref="S4.SS3.p1.2.m2.1.1.3.8.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝒟</ci><apply id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><times id="S4.SS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.p1.2.m2.1.1.3.1"></times><ci id="S4.SS3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS3.p1.2.m2.1.1.3.2">𝑙</ci><ci id="S4.SS3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3.3">𝑎</ci><ci id="S4.SS3.p1.2.m2.1.1.3.4.cmml" xref="S4.SS3.p1.2.m2.1.1.3.4">𝑏</ci><ci id="S4.SS3.p1.2.m2.1.1.3.5.cmml" xref="S4.SS3.p1.2.m2.1.1.3.5">𝑒</ci><ci id="S4.SS3.p1.2.m2.1.1.3.6.cmml" xref="S4.SS3.p1.2.m2.1.1.3.6">𝑙</ci><ci id="S4.SS3.p1.2.m2.1.1.3.7.cmml" xref="S4.SS3.p1.2.m2.1.1.3.7">𝑒</ci><ci id="S4.SS3.p1.2.m2.1.1.3.8.cmml" xref="S4.SS3.p1.2.m2.1.1.3.8">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\mathcal{D}_{labeled}</annotation></semantics></math> as the support dataset. We provide results for both bounding-box and point prompts (automatically generated by our pipeline), observing that different prompts are more suited to different datasets. We achieve improvements of up to 73.3%, with more dramatic improvements where the initial DICE score was lower; though the weak labels may not be an exact match with the gold-standard labels, they are accurate enough to provide a boost in performance. More qualitative results can be seen in the appendix (App. <a href="#A1" title="Appendix A Qualitative Results ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results using our method in label-scarce settings. GS = Gold Standard.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:123.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.9pt,10.7pt) scale(0.851230451693065,0.851230451693065) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"># GS Labels</th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"># Weak Labels</th>
<td id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="9">DICE</td>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">BUSI</td>
<td id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" colspan="3">ISIC</td>
<td id="S4.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3">CANDID-PTX</td>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="2">UniverSeg</th>
<td id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3">0.3681</td>
<td id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">0.5257</td>
<td id="S4.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" colspan="3">0.7700</td>
</tr>
<tr id="S4.T1.1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.1.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.1.1.4.4.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">Auto</td>
<td id="S4.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">Box</td>
<td id="S4.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">Points</td>
<td id="S4.T1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">Auto</td>
<td id="S4.T1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">Box</td>
<td id="S4.T1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">Points</td>
<td id="S4.T1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">Auto</td>
<td id="S4.T1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">Box</td>
<td id="S4.T1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t">Points</td>
</tr>
<tr id="S4.T1.1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">0</th>
<td id="S4.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.3059</td>
<td id="S4.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.3059</td>
<td id="S4.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.3059</td>
<td id="S4.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">0.6123</td>
<td id="S4.T1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.6123</td>
<td id="S4.T1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">0.6123</td>
<td id="S4.T1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">0.8182</td>
<td id="S4.T1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">0.8182</td>
<td id="S4.T1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">0.8182</td>
</tr>
<tr id="S4.T1.1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S4.T1.1.1.6.6.3" class="ltx_td ltx_align_center">0.2629</td>
<td id="S4.T1.1.1.6.6.4" class="ltx_td ltx_align_center">0.3613</td>
<td id="S4.T1.1.1.6.6.5" class="ltx_td ltx_align_center">0.3777</td>
<td id="S4.T1.1.1.6.6.6" class="ltx_td ltx_align_center">0.5810</td>
<td id="S4.T1.1.1.6.6.7" class="ltx_td ltx_align_center">0.7367</td>
<td id="S4.T1.1.1.6.6.8" class="ltx_td ltx_align_center">0.7884</td>
<td id="S4.T1.1.1.6.6.9" class="ltx_td ltx_align_center">0.6396</td>
<td id="S4.T1.1.1.6.6.10" class="ltx_td ltx_align_center">0.8879</td>
<td id="S4.T1.1.1.6.6.11" class="ltx_td ltx_align_center">0.8726</td>
</tr>
<tr id="S4.T1.1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S4.T1.1.1.7.7.3" class="ltx_td ltx_align_center">0.2401</td>
<td id="S4.T1.1.1.7.7.4" class="ltx_td ltx_align_center">0.4326</td>
<td id="S4.T1.1.1.7.7.5" class="ltx_td ltx_align_center">0.4661</td>
<td id="S4.T1.1.1.7.7.6" class="ltx_td ltx_align_center">0.5907</td>
<td id="S4.T1.1.1.7.7.7" class="ltx_td ltx_align_center">0.7587</td>
<td id="S4.T1.1.1.7.7.8" class="ltx_td ltx_align_center">0.8087</td>
<td id="S4.T1.1.1.7.7.9" class="ltx_td ltx_align_center">0.5519</td>
<td id="S4.T1.1.1.7.7.10" class="ltx_td ltx_align_center">0.9044</td>
<td id="S4.T1.1.1.7.7.11" class="ltx_td ltx_align_center">0.8872</td>
</tr>
<tr id="S4.T1.1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">25</th>
<th id="S4.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">100</th>
<td id="S4.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">0.2124</td>
<td id="S4.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">0.4661</td>
<td id="S4.T1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.8.8.5.1" class="ltx_text ltx_font_bold">0.5302</span></td>
<td id="S4.T1.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">0.5893</td>
<td id="S4.T1.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_bb">0.7424</td>
<td id="S4.T1.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.8.8.8.1" class="ltx_text ltx_font_bold">0.8483</span></td>
<td id="S4.T1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_bb">0.4115</td>
<td id="S4.T1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.8.8.10.1" class="ltx_text ltx_font_bold">0.9096</span></td>
<td id="S4.T1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_bb">0.8443</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Data</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">In addition to experiments using real data, we conduct experiments using data from a diffusion model trained on unlabeled samples from each of the datasets (Table <a href="#S4.T2" title="Table 2 ‣ Synthetic Data ‣ 4.3 Results ‣ 4 Experiments and Results ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). We train the diffusion model on the train splits of BUSI, ISIC, and CXR-COVID <cite class="ltx_cite ltx_citemacro_cite">Fraiwan et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> datasets respectively and sample it multiple times to generate the required number of synthetic images. As diffusion models can produce large amounts of synthetic data after being trained on just a few samples, the number of possible weak labels is higher.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Synthetic Data Experiments</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.0pt,9.0pt) scale(0.833187064191662,0.833187064191662) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"># GS Labels</th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"># Weak Labels</th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">DICE</th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T2.1.1.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t"></th>
<th id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BUSI</th>
<th id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ISIC</th>
<th id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CANDID-PTX</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.3.1" class="ltx_tr">
<th id="S4.T2.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">0</th>
<td id="S4.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.3059</td>
<td id="S4.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.6123</td>
<td id="S4.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.8182</td>
</tr>
<tr id="S4.T2.1.1.4.2" class="ltx_tr">
<th id="S4.T2.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T2.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S4.T2.1.1.4.2.3" class="ltx_td ltx_align_center">0.4177</td>
<td id="S4.T2.1.1.4.2.4" class="ltx_td ltx_align_center">0.7762</td>
<td id="S4.T2.1.1.4.2.5" class="ltx_td ltx_align_center">0.8997</td>
</tr>
<tr id="S4.T2.1.1.5.3" class="ltx_tr">
<th id="S4.T2.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T2.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">250</th>
<td id="S4.T2.1.1.5.3.3" class="ltx_td ltx_align_center">0.4312</td>
<td id="S4.T2.1.1.5.3.4" class="ltx_td ltx_align_center">0.7228</td>
<td id="S4.T2.1.1.5.3.5" class="ltx_td ltx_align_center">0.9073</td>
</tr>
<tr id="S4.T2.1.1.6.4" class="ltx_tr">
<th id="S4.T2.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">25</th>
<th id="S4.T2.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">500</th>
<td id="S4.T2.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.4301</td>
<td id="S4.T2.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">0.7247</td>
<td id="S4.T2.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb">0.9022</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In addition to our main experiments, we perform ablation studies examining various elements of our pipeline and also discuss some limitations in this section.</p>
</div>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Label scarce settings</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">We vary the number of images in the base dataset to observe the effect on the performance boost from weak labeling and present the results in Table <a href="#S4.T3" title="Table 3 ‣ Label scarce settings ‣ 4.4 Ablation Studies ‣ 4 Experiments and Results ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Though increasing gold-standard labels is expected to reduce performance gains from weak labels, when starting with only 10 base images, the quality of coarse labels plummets, rendering MedSAM prompting ineffective in such extreme label scarcity. However, it can be noted that we always gain performance while using our pipeline.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation on label-scarce settings. Experiments are conducted on BUSI dataset.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:316.5pt;height:78.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.9pt,5.9pt) scale(0.868588663285958,0.868588663285958) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"># GS Labels</th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"># Weak Labels (Syn. Data)</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DICE (GS)</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DICE (GS + WL)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10</th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">100</th>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.2327</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.2637</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<th id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">100</th>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center">0.3059</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center">0.4661</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<th id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">100</th>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center">0.4379</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center">0.5416</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">100</th>
<th id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">100</th>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.5575</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">0.6155</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SAM vs. MedSAM</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">We ablate on the use of the base SAM model compared to the fine-tuned MedSAM model (Table <a href="#S4.T4" title="Table 4 ‣ SAM vs. MedSAM ‣ 4.4 Ablation Studies ‣ 4 Experiments and Results ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and observe that MedSAM outperforms SAM.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>SAM vs. MedSAM. Experiments are conducted on BUSI dataset.</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:63pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.5pt,4.5pt) scale(0.874575341447204,0.874575341447204) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># GS Labels</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Weak Labels</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DICE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<td id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">—</td>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">25</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.3059</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<td id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_center">SAM</td>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center">25</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center">100</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center">0.3886</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<td id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">MedSAM</td>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">25</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">100</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.4661</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">Our work focuses on 2D ultrasound, X-ray,
and dermoscopic data. We recognize that tasks involving highly intricate structures may require different input selection approaches due to potential sensitivities in our method (App. <a href="#A4" title="Appendix D Limitations: Results on CHASE Dataset ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>). We also did not touch on 3D segmentation tasks; future work could investigate extensions to address such scenarios.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce a new method for addressing label-scarce scenarios in medical image segmentation using recent advancements in vision foundation models. By selecting inputs to MedSAM from coarse labels trained on a small gold-standard dataset, we create augmented datasets with weak labels that can be auto-generated for any number of unlabeled data. Using these augmented datasets, we train models that obtain significant boosts in performance on label-scarce settings. Weak labels generated through our method can also be used to improve human-in-the-loop annotation processes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Dhabyani et al. (2020)</span>
<span class="ltx_bibblock">
Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy.

</span>
<span class="ltx_bibblock">Dataset of breast ultrasound images.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Data in Brief</em>, 28:104863, 2020.

</span>
<span class="ltx_bibblock">ISSN 2352-3409.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.dib.2019.104863" title="" class="ltx_ref">https://doi.org/10.1016/j.dib.2019.104863</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S2352340919312181" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S2352340919312181</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anand et al. (2023)</span>
<span class="ltx_bibblock">
Deepa Anand, Gurunath Reddy M, Vanika Singhal, Dattesh D. Shanbhag, Shriram KS, Uday Patil, Chitresh Bhushan, Kavitha Manickam, Dawei Gui, Rakesh Mullick, Avinash Gopal, Parminder Bhatia, and Taha Kass-Hout.

</span>
<span class="ltx_bibblock">One-shot localization and segmentation of medical images with foundation models, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Butoi et al. (2023)</span>
<span class="ltx_bibblock">
Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R. Sabuncu, John Guttag, and Adrian V. Dalca.

</span>
<span class="ltx_bibblock">Universeg: Universal medical image segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Cheng Chen, Juzheng Miao, Dufan Wu, Zhiling Yan, Sekeun Kim, Jiang Hu, Aoxiao Zhong, Zhengliang Liu, Lichao Sun, Xiang Li, Tianming Liu, Pheng-Ann Heng, and Quanzheng Li.

</span>
<span class="ltx_bibblock">Ma-sam: Modality-agnostic sam adaptation for 3d medical image segmentation, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, and Yuyin Zhou.

</span>
<span class="ltx_bibblock">Transunet: Transformers make strong encoders for medical image segmentation, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. (2023)</span>
<span class="ltx_bibblock">
Can Cui, Ruining Deng, Quan Liu, Tianyuan Yao, Shunxing Bao, Lucas W. Remedios, Yucheng Tang, and Yuankai Huo.

</span>
<span class="ltx_bibblock">All-in-sam: from weak annotation to pixel-wise nuclei segmentation with prompt-based finetuning, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Oliveira et al. (2023)</span>
<span class="ltx_bibblock">
Christian Mattjie de Oliveira, Luis Vinícius de Moura, Rafaela Cappelari Ravazio, Lucas Silveira Kupssinskü, Otávio Parraga, Marcelo Mussi Delucis, and Rodrigo C. Barros.

</span>
<span class="ltx_bibblock">Zero-shot performance of the segment anything model (sam) in 2d medical imaging: A comprehensive evaluation and practical guidelines.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.00109, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:258426532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:258426532</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2021)</span>
<span class="ltx_bibblock">
Si-Yang Feng, Damian Azzollini, Ji Soo Kim, Cheng-Kai Jin, Simon Gordon, Jason Yeoh, Eve Kim, Mina Han, Andrew Lee, Aakash Patel, Joy Wu, Martin Urschler, Amy Tsoi Lai Fong, Cameron Simmers, Gregory Patrick Tarr, Stuart Barnard, and Ben Wilson.

</span>
<span class="ltx_bibblock">Curation of the candid-ptx dataset with free-text reports.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Radiology. Artificial intelligence</em>, 3 6:e210136, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:243370709" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:243370709</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraiwan et al. (2023)</span>
<span class="ltx_bibblock">
Mohammad Fraiwan, Natheer Khasawneh, Basheer Khassawneh, and Ali Ibnian.

</span>
<span class="ltx_bibblock">A dataset of covid-19 x-ray chest images.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Data in Brief</em>, 47:109000, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraz et al. (2012)</span>
<span class="ltx_bibblock">
Muhammad Moazam Fraz, Paolo Remagnino, Andreas Hoppe, Bunyarit Uyyanonvara, Alicja R. Rudnicka, Christopher G. Owen, and Sarah A. Barman.

</span>
<span class="ltx_bibblock">An ensemble classification-based approach applied to retinal blood vessel segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical Engineering</em>, 59(9):2538–2548, 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/TBME.2012.2205687" title="" class="ltx_ref">10.1109/TBME.2012.2205687</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2020)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutman et al. (2016)</span>
<span class="ltx_bibblock">
David Gutman, Noel C. F. Codella, Emre M. Celebi, Brian Helba, Michael Marchetti, Nabin K. Mishra, and Allan Halpern.

</span>
<span class="ltx_bibblock">Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic).

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1605.01397, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:260439738" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:260439738</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Sheng He, Rina Bao, Jingpeng Li, Jeffrey Stout, Atle Bjornerud, P. Ellen Grant, and Yangming Ou.

</span>
<span class="ltx_bibblock">Computer-vision benchmark segment-anything model (sam) in medical images: Accuracy in 12 datasets, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazerouni et al. (2023)</span>
<span class="ltx_bibblock">
Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof.

</span>
<span class="ltx_bibblock">Diffusion models in medical imaging: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 88:102846, 2023.

</span>
<span class="ltx_bibblock">ISSN 1361-8415.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.media.2023.102846" title="" class="ltx_ref">https://doi.org/10.1016/j.media.2023.102846</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1361841523001068" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S1361841523001068</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.02643</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2023)</span>
<span class="ltx_bibblock">
Wenhui Lei, Xu Wei, Xiaofan Zhang, Kang Li, and Shaoting Zhang.

</span>
<span class="ltx_bibblock">Medlsam: Localize and segment anything model for 3d ct images, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Xian Lin, Yangyang Xiang, Li Zhang, Xin Yang, Zengqiang Yan, and Li Yu.

</span>
<span class="ltx_bibblock">Samus: Adapting segment anything model for clinically-friendly and generalizable ultrasound image segmentation, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang.

</span>
<span class="ltx_bibblock">Segment anything in medical images.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.12306</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024)</span>
<span class="ltx_bibblock">
Jun Ma, Feifei Li, and Bo Wang.

</span>
<span class="ltx_bibblock">U-mamba: Enhancing long-range dependency for biomedical image segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04722</em>, 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maquiling et al. (2023)</span>
<span class="ltx_bibblock">
Virmarie Maquiling, Sean Anthony Byrne, Diederick Christian Niehorster, Marcus Nyström, and Enkelejda Kasneci.

</span>
<span class="ltx_bibblock">Zero-shot segmentation of eye features using the segment anything model (sam).

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2311.08077, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:265158106" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:265158106</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milletari et al. (2016)</span>
<span class="ltx_bibblock">
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.

</span>
<span class="ltx_bibblock">V-net: Fully convolutional neural networks for volumetric medical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2016 Fourth International Conference on 3D Vision (3DV)</em>, pages 565–571, 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/3DV.2016.79" title="" class="ltx_ref">10.1109/3DV.2016.79</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandey et al. (2023)</span>
<span class="ltx_bibblock">
S. Pandey, K. Chen, and E. B. Dam.

</span>
<span class="ltx_bibblock">Comprehensive multimodal segmentation in medical imaging: Combining yolov8 with sam and hq-sam models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</em>, pages 2584–2590, Los Alamitos, CA, USA, oct 2023. IEEE Computer Society.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICCVW60793.2023.00273" title="" class="ltx_ref">10.1109/ICCVW60793.2023.00273</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.ieeecomputersociety.org/10.1109/ICCVW60793.2023.00273" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.ieeecomputersociety.org/10.1109/ICCVW60793.2023.00273</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peivandi et al. (2023)</span>
<span class="ltx_bibblock">
Mohammad Peivandi, Jason Bing Zhang, Michael Lu, Dongxiao Zhu, and Zhifeng Kou.

</span>
<span class="ltx_bibblock">Empirical evaluation of the segment anything model (sam) for brain tumor segmentation.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:263829153" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:263829153</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2023)</span>
<span class="ltx_bibblock">
Xiyu Qi, Yifan Wu, Yongqiang Mao, Wenhui Zhang, and Yidan Zhang.

</span>
<span class="ltx_bibblock">Self-guided few-shot semantic segmentation for remote sensing imagery based on large vision models, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi, editors, <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</em>, pages 234–241, Cham, 2015. Springer International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-319-24574-4.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaharabany et al. (2023)</span>
<span class="ltx_bibblock">
Tal Shaharabany, Aviad Dahan, Raja Giryes, and Lior Wolf.

</span>
<span class="ltx_bibblock">Autosam: Adapting sam to medical images by overloading the prompt encoder, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2023)</span>
<span class="ltx_bibblock">
Dongik Shin, Beomsuk Kim, M.D., and Seungjun Baek.

</span>
<span class="ltx_bibblock">Cemb-sam: Segment anything model with condition embedding for joint learning from heterogeneous datasets.

</span>
<span class="ltx_bibblock">In M. Emre Celebi, Md Sirajus Salekin, Hyunwoo Kim, Shadi Albarqouni, Catarina Barata, Allan Halpern, Philipp Tschandl, Marc Combalia, Yuan Liu, Ghada Zamzmi, Joshua Levy, Huzefa Rangwala, Annika Reinke, Diya Wynn, Bennett Landman, Won-Ki Jeong, Yiqing Shen, Zhongying Deng, Spyridon Bakas, Xiaoxiao Li, Chen Qin, Nicola Rieke, Holger Roth, and Daguang Xu, editors, <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention – MICCAI 2023 Workshops</em>, pages 275–284, Cham, 2023. Springer Nature Switzerland.

</span>
<span class="ltx_bibblock">ISBN 978-3-031-47401-9.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh.

</span>
<span class="ltx_bibblock">Self-supervised pre-training of swin transformers for 3d medical image analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 20730–20740, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valanarasu et al. (2020)</span>
<span class="ltx_bibblock">
Jeya Maria Jose Valanarasu, Vishwanath Sindagi, Ilker Hacihaliloglu, and Vishal Patel.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">KiU-Net: Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations</em>, pages 363–373.

</span>
<span class="ltx_bibblock">09 2020.

</span>
<span class="ltx_bibblock">ISBN 978-3-030-59718-4.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/978-3-030-59719-1_36" title="" class="ltx_ref">10.1007/978-3-030-59719-1_36</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valanarasu et al. (2021)</span>
<span class="ltx_bibblock">
Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M. Patel.

</span>
<span class="ltx_bibblock">Medical transformer: Gated axial-attention for medical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I</em>, page 36–46, Berlin, Heidelberg, 2021. Springer-Verlag.

</span>
<span class="ltx_bibblock">ISBN 978-3-030-87192-5.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/978-3-030-87193-2_4" title="" class="ltx_ref">10.1007/978-3-030-87193-2_4</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/978-3-030-87193-2_4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-87193-2_4</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, and Shanshan Zhao.

</span>
<span class="ltx_bibblock">Caption anything: Interactive image description with diverse multimodal controls, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolleb et al. (2021)</span>
<span class="ltx_bibblock">
Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe Valmaggia, and Philippe C. Cattin.

</span>
<span class="ltx_bibblock">Diffusion models for implicit image segmentation ensembles, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Qi Wu, Yuyao Zhang, and Marawan Elbatel.

</span>
<span class="ltx_bibblock">Self-prompting large vision models for few-shot medical image segmentation.

</span>
<span class="ltx_bibblock">In Lisa Koch, M. Jorge Cardoso, Enzo Ferrante, Konstantinos Kamnitsas, Mobarakol Islam, Meirui Jiang, Nicola Rieke, Sotirios A. Tsaftaris, and Dong Yang, editors, <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Domain Adaptation and Representation Transfer</em>, pages 156–167, Cham, 2024. Springer Nature Switzerland.

</span>
<span class="ltx_bibblock">ISBN 978-3-031-45857-6.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen.

</span>
<span class="ltx_bibblock">Inpaint anything: Segment anything meets image inpainting, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Tao Zhou, Shuo Wang, Peixian Liang, Yejia Zhang, and Danny Z Chen.

</span>
<span class="ltx_bibblock">Input augmentation with sam: Boosting medical image segmentation with segmentation foundation model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, pages 129–139. Springer, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang.

</span>
<span class="ltx_bibblock">Unet++: A nested u-net architecture for medical image segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support : 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, S…</em>, 11045:3–11, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:50786304" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:50786304</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Qualitative Results</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we provide some examples comparing predictions from the base model trained on 25 gold-standard labels, compared to the final model trained on augmented datasets consisting of 25 gold-standard and 100 weak labels. We observe that the final prediction is far more accurate than the base model predictions.</p>
</div>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2404.17033/assets/outputs.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example outputs from each dataset</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Comparison to SAM and MedSAM Automatic Segmentation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this section, we compare our weak labels to the outputs from SAM and MedSAM in their automatic options (Fig. <a href="#A2.F5" title="Figure 5 ‣ Appendix B Comparison to SAM and MedSAM Automatic Segmentation ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). SAM tends to segment many regions, in which it is not clear which is the target. In contrast, MedSAM tends to generate blank masks in its auto option. The weak labels generated from our method is better and is closer to GT while also auto-generated.</p>
</div>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2404.17033/assets/autoseg-comparisons.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison to SAM and MedSAM automatic segmentation for each dataset</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Input Selection</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We provide comparisons of various input selection methods to our method in Fig. <a href="#A3.F6" title="Figure 6 ‣ Appendix C Input Selection ‣ Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In the third column, for example, we choose inputs based on the darkest pixels present in the image. In the fourth column, we use a bounding box based solely on the image size rather than the image content. In the fifth and sixth columns, we include outputs from our pipeline, using point and box prompts that were automatically generated from the base model.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">Our pipeline outperforms both of these alternative methods. In addition, we note that our pipeline is dataset-agnostic; it may not be the case that the target region contains the darkest pixels in an image, for example, or that it can be captured with a bounding box encompassing most of the image.</p>
</div>
<figure id="A3.F6" class="ltx_figure"><img src="/html/2404.17033/assets/selection-comparisons.png" id="A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of input selection methods</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Limitations: Results on CHASE Dataset</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We provide results on the CHASE dataset <cite class="ltx_cite ltx_citemacro_cite">Fraz et al. (<a href="#bib.bib10" title="" class="ltx_ref">2012</a>)</cite>, which consists of 28 images for the task of retinal vessel segmentation, using a train-test split of 20 and 8 images, respectively.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">We note that our pipeline does not improve performance on the CHASE dataset; segmentation of particularly fine-grained vessels may require alternate input selection methods due to the increased sensitivity MedSAM may have to inputs on them. Furthermore, we note that due to the extremely small size of the CHASE dataset (28 images), the label-scarcity of the gold-standard dataset may be too extreme to provide informative coarse labels to help us generate high-quality weak labels.</p>
</div>
<figure id="A4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>CHASE</figcaption>
<table id="A4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T5.1.1.1" class="ltx_tr">
<th id="A4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># GS Labels</th>
<th id="A4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Weak Labels</th>
<th id="A4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DICE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T5.1.2.1" class="ltx_tr">
<td id="A4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="A4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="A4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.2406</td>
</tr>
<tr id="A4.T5.1.3.2" class="ltx_tr">
<td id="A4.T5.1.3.2.1" class="ltx_td ltx_align_center">5</td>
<td id="A4.T5.1.3.2.2" class="ltx_td ltx_align_center">5</td>
<td id="A4.T5.1.3.2.3" class="ltx_td ltx_align_center">0.2557</td>
</tr>
<tr id="A4.T5.1.4.3" class="ltx_tr">
<td id="A4.T5.1.4.3.1" class="ltx_td ltx_align_center">5</td>
<td id="A4.T5.1.4.3.2" class="ltx_td ltx_align_center">10</td>
<td id="A4.T5.1.4.3.3" class="ltx_td ltx_align_center">0.218</td>
</tr>
<tr id="A4.T5.1.5.4" class="ltx_tr">
<td id="A4.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">5</td>
<td id="A4.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">15</td>
<td id="A4.T5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.2081</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.17031" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.17033" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.17033">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.17033" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.17034" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 14:57:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
