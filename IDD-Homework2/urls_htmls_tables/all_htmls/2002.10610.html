<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2002.10610] Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art</title><meta property="og:description" content="Nowadays, devices are equipped with advanced sensors with higher processing/computing capabilities. Further, widespread Internet availability enables communication among sensing devices. As a result, vast amounts of da…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2002.10610">

<!--Generated on Sat Mar 16 12:14:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning for Resource-Constrained IoT Devices: 
<br class="ltx_break">Panoramas and State-of-the-art</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ahmed Imteaj<sup id="id11.2.id1" class="ltx_sup"><span id="id11.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Urmish Thakker<sup id="id12.2.id1" class="ltx_sup"><span id="id12.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shiqiang Wang<sup id="id13.2.id1" class="ltx_sup"><span id="id13.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Li<sup id="id14.8.id1" class="ltx_sup"><span id="id14.8.id1.1" class="ltx_text ltx_font_italic">5</span></sup>&amp;M. Hadi Amini<sup id="id15.9.id2" class="ltx_sup"><span id="id15.9.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Contact Author: hadi.amini@ieee.org, www.solidlab.network</span></span></span>
<br class="ltx_break"><sup id="id16.10.id3" class="ltx_sup">1</sup>School of Computing and Information Sciences, Florida International University
<br class="ltx_break"><sup id="id17.11.id4" class="ltx_sup">2</sup>Sustainability, Optimization, and Learning for InterDependent networks laboratory (solid lab)
<br class="ltx_break"><sup id="id18.12.id5" class="ltx_sup">3</sup>Arm ML Research
<br class="ltx_break"><sup id="id19.13.id6" class="ltx_sup">4</sup>IBM T. J. Watson Research Center
<br class="ltx_break"><sup id="id20.14.id7" class="ltx_sup">5</sup>Binghamton University, the State University of New York
<span id="id21.15.id8" class="ltx_text" style="font-size:90%;">aimte001@fiu.edu,
urmish.thakker@arm.com,
wangshiq@us.ibm.com,
lij@binghamton.edu, moamini@fiu.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">Nowadays, devices are equipped with advanced sensors with higher processing/computing capabilities. Further, widespread Internet availability enables communication among sensing devices. As a result, vast amounts of data are generated on edge devices to drive Internet-of-Things (IoT), crowdsourcing, and other emerging technologies. The extensive amount of collected data can be pre-processed, scaled, classified, and finally, used for predicting future events with machine learning (ML) methods. In traditional ML approaches, data is sent to and processed in a central server, which encounters communication overhead, processing delay, privacy leakage, and security issues. To overcome these challenges, each client can be trained locally based on its available data and by learning from the global model. This decentralized learning approach is referred to as federated learning (FL). However, in large-scale networks, there may be clients with varying computational resource capabilities. This may lead to implementation and scalability challenges for FL techniques. In this paper, we first introduce some recently implemented real-life applications of FL. We then emphasize on the core challenges of implementing the FL algorithms from the perspective of resource limitations (e.g., memory, bandwidth, and energy budget) of client devices. We finally discuss open issues associated with FL and highlight future directions in the FL area concerning resource-constrained devices.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The Internet-of-Things (IoT) penetration rate has recently expanded prodigiously due to the integration of billions of connected IoT devices. Such IoT edge devices include different kinds of robots, drones, and smartphones, which have limited computation and storage capabilities and can communicate with remote entities via wide-area network (WAN). The data generated from these devices at the network edge is increasing exponentially. Due to bandwidth and privacy concerns, it is infeasible to send all locally collected data to the server. However, many IoT applications require the prediction and classification of data, for which machine learning (ML) models need to be trained using data collected by multiple devices. The question is: <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">how to train ML models from the decentralized data at resource-constrained IoT devices?</em></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address the above problem, we need to devise an approach through which the learning process can be accomplished without exchanging raw data between client devices. <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">Federated learning (FL)</em> is a technique that fulfills this purpose.
With FL, each device can attain a global view and predict a situation that is seen in another device. For example, we can consider a scenario where multiple drones are placed at different locations, and each drone observes vehicles that are passing through their respective ends. If a drone observes a vehicle and gets trained by this, other drones can learn without observing that vehicle through the FL approach. Applications of FL covers a wide spectrum of domains, e.g., word prediction by training models on local edge devices and not sharing sensitive information to the central server <cite class="ltx_cite ltx_citemacro_cite">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, adaptive keyword spotting using a locally-trained voice assistant <cite class="ltx_cite ltx_citemacro_cite">Leroy and others (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Challenges in implementing FL in the presence of heterogeneous hardware in a system are discussed in <cite class="ltx_cite ltx_citemacro_cite">Lim <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>); Park <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>. Due to various resource constraints (i.e., limited capacity of communication, computation, storage, etc.) at different types of client devices, the clients cannot be treated uniformly, and additional care needs to be taken to handle such heterogeneity. <span id="S1.p3.1.1" class="ltx_text" style="color:#000000;">A feasibility study to implement FL on the low-processing unit (e.g., Raspberry Pi)</span> is given by <cite class="ltx_cite ltx_citemacro_citet">Das and Brunschwiler (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, where they also discussed the potential challenges to detect emotion from sound. Besides, FL on battery-powered devices is studied in <cite class="ltx_cite ltx_citemacro_cite">Xu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib29" title="" class="ltx_ref">2019a</a>)</cite>, where a two-layered strategy to train the model of battery-powered devices is used. In the first layer, the candidates who have sufficient power to carry out the training process are selected, and in the second layer, FL is applied on those selected battery-powered devices considering local energy optimization. This approach eliminates the problems related to straggler clients and helps improve the overall performance of the training process.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A blog on training models on edge devices <cite class="ltx_cite ltx_citemacro_cite">Pete Warden (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> focuses on approaches for model training and crucial aspects that matter while infusing deep learning models on-device. <cite class="ltx_cite ltx_citemacro_citet">Bonawitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite> designed a scalable FL system that highlighted significant components of the system, including a high-level view of device scheduling. This study focused on approaches that we can take while training models and the crucial aspects of training FL models at scale.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="color:#000000;">A resource-aware FL framework considering heterogeneous edge clients is proposed by <cite class="ltx_cite ltx_citemacro_citet">Xu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2019b</a>)</cite>. In this work, they eliminate the straggler clients using an optimization technique named ‘soft-training’ that dynamically masks different neurons based on model updates, and their proposed aggregation scheme speeds up the collaborative convergence.
Moreover, a control algorithm that adapts the communication and computation trade-off is presented by <cite class="ltx_cite ltx_citemacro_citet">Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>. They minimize the loss function for a predefined resource budget based on analyzing the impact of different configurations on the convergence rate.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="color:#000000;">However, there is no comprehensive survey on FL challenges and issues from the perspective of resource-constrained IoT clients. The main contribution of this paper is that we analyze the core challenges, potential solutions, open questions, and future pathways that need to be considered and addressed in the implementation of FL on resource-constrained IoT devices in the network.</span></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text" style="color:#000000;">The rest of this paper is organized as follows. Section 2 introduces the background of FL. In Section 3, we discuss some applications of FL. In Section 4, we introduce the core challenges to implement FL particularly for resource-constrained devices, and outline some potential solutions to address these challenges. Further, we highlight the open issues of FL algorithms and hardware developments in Section 5. In Section 6, we list some future directions, which is followed by Section 7 that concludes the paper.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background of Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.12" class="ltx_p">The FL algorithm aims to learn a single, global model from local data collected by millions of distributed devices. It learns a model under various resource-constraints of devices, where the model is trained locally and intermediate model updates are shared with the cloud (server) periodically. The overall goal is to minimize a training objective (loss) function which can be written as follows <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>)</cite>:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\min_{z}F(z):=\sum_{i=1}^{n}P_{i}F_{i}(z)," display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><mrow id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml"><munder id="S2.E1.m1.3.3.1.1.2.2.1" xref="S2.E1.m1.3.3.1.1.2.2.1.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.1.2" xref="S2.E1.m1.3.3.1.1.2.2.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.2.2.1.3" xref="S2.E1.m1.3.3.1.1.2.2.1.3.cmml">z</mi></munder><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.2.2a" xref="S2.E1.m1.3.3.1.1.2.2.cmml">⁡</mo><mi id="S2.E1.m1.3.3.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.2.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.2.1" xref="S2.E1.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.2.3.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.3.2.1" xref="S2.E1.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">z</mi><mo rspace="0.278em" stretchy="false" id="S2.E1.m1.3.3.1.1.2.3.2.2" xref="S2.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml">:=</mo><mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><munderover id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.3.1.2.2" xref="S2.E1.m1.3.3.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.3.3.1.1.3.1.2.3" xref="S2.E1.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.1.2.3.2" xref="S2.E1.m1.3.3.1.1.3.1.2.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.1.1.3.1.2.3.1" xref="S2.E1.m1.3.3.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.3.3.1.1.3.1.2.3.3" xref="S2.E1.m1.3.3.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.3.3.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.3.1.3.cmml">n</mi></munderover><mrow id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><msub id="S2.E1.m1.3.3.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.2.cmml">P</mi><mi id="S2.E1.m1.3.3.1.1.3.2.2.3" xref="S2.E1.m1.3.3.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.3.2.1" xref="S2.E1.m1.3.3.1.1.3.2.1.cmml">​</mo><msub id="S2.E1.m1.3.3.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.3.2" xref="S2.E1.m1.3.3.1.1.3.2.3.2.cmml">F</mi><mi id="S2.E1.m1.3.3.1.1.3.2.3.3" xref="S2.E1.m1.3.3.1.1.3.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.3.2.1a" xref="S2.E1.m1.3.3.1.1.3.2.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.3.2.4.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.3.2.4.2.1" xref="S2.E1.m1.3.3.1.1.3.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.3.2.4.2.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1">assign</csymbol><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><times id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.1"></times><apply id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"><apply id="S2.E1.m1.3.3.1.1.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.2.1.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.2.2.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.2"></min><ci id="S2.E1.m1.3.3.1.1.2.2.1.3.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.3">𝑧</ci></apply><ci id="S2.E1.m1.3.3.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.2">𝐹</ci></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑧</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><apply id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.3.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.2"></sum><apply id="S2.E1.m1.3.3.1.1.3.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3"><eq id="S2.E1.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.1"></eq><ci id="S2.E1.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.3.1.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.3">𝑛</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><times id="S2.E1.m1.3.3.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.1"></times><apply id="S2.E1.m1.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2">𝑃</ci><ci id="S2.E1.m1.3.3.1.1.3.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3.2">𝐹</ci><ci id="S2.E1.m1.3.3.1.1.3.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3.3">𝑖</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑧</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\min_{z}F(z):=\sum_{i=1}^{n}P_{i}F_{i}(z),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.11" class="ltx_p">where <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">n</annotation></semantics></math> is the number of devices, <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="P_{i}\geq 0" display="inline"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><msub id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml"><mi id="S2.p1.2.m2.1.1.2.2" xref="S2.p1.2.m2.1.1.2.2.cmml">P</mi><mi id="S2.p1.2.m2.1.1.2.3" xref="S2.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">≥</mo><mn id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><geq id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1"></geq><apply id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.2.1.cmml" xref="S2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.2.cmml" xref="S2.p1.2.m2.1.1.2.2">𝑃</ci><ci id="S2.p1.2.m2.1.1.2.3.cmml" xref="S2.p1.2.m2.1.1.2.3">𝑖</ci></apply><cn type="integer" id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">P_{i}\geq 0</annotation></semantics></math>
defines the relative impact of each local device, satisfying <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\sum_{i}P_{i}=1" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mrow id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml"><msub id="S2.p1.3.m3.1.1.2.1" xref="S2.p1.3.m3.1.1.2.1.cmml"><mo id="S2.p1.3.m3.1.1.2.1.2" xref="S2.p1.3.m3.1.1.2.1.2.cmml">∑</mo><mi id="S2.p1.3.m3.1.1.2.1.3" xref="S2.p1.3.m3.1.1.2.1.3.cmml">i</mi></msub><msub id="S2.p1.3.m3.1.1.2.2" xref="S2.p1.3.m3.1.1.2.2.cmml"><mi id="S2.p1.3.m3.1.1.2.2.2" xref="S2.p1.3.m3.1.1.2.2.2.cmml">P</mi><mi id="S2.p1.3.m3.1.1.2.2.3" xref="S2.p1.3.m3.1.1.2.2.3.cmml">i</mi></msub></mrow><mo id="S2.p1.3.m3.1.1.1" xref="S2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><eq id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1"></eq><apply id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2"><apply id="S2.p1.3.m3.1.1.2.1.cmml" xref="S2.p1.3.m3.1.1.2.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.2.1.1.cmml" xref="S2.p1.3.m3.1.1.2.1">subscript</csymbol><sum id="S2.p1.3.m3.1.1.2.1.2.cmml" xref="S2.p1.3.m3.1.1.2.1.2"></sum><ci id="S2.p1.3.m3.1.1.2.1.3.cmml" xref="S2.p1.3.m3.1.1.2.1.3">𝑖</ci></apply><apply id="S2.p1.3.m3.1.1.2.2.cmml" xref="S2.p1.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.2.2.1.cmml" xref="S2.p1.3.m3.1.1.2.2">subscript</csymbol><ci id="S2.p1.3.m3.1.1.2.2.2.cmml" xref="S2.p1.3.m3.1.1.2.2.2">𝑃</ci><ci id="S2.p1.3.m3.1.1.2.2.3.cmml" xref="S2.p1.3.m3.1.1.2.2.3">𝑖</ci></apply></apply><cn type="integer" id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\sum_{i}P_{i}=1</annotation></semantics></math>. <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="F_{i}" display="inline"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">F</mi><mi id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝐹</ci><ci id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">F_{i}</annotation></semantics></math> is the objective function of the <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">i</annotation></semantics></math>-th local device,
which can be defined as
<math id="S2.p1.6.m6.1" class="ltx_Math" alttext="F_{i}(x)" display="inline"><semantics id="S2.p1.6.m6.1a"><mrow id="S2.p1.6.m6.1.2" xref="S2.p1.6.m6.1.2.cmml"><msub id="S2.p1.6.m6.1.2.2" xref="S2.p1.6.m6.1.2.2.cmml"><mi id="S2.p1.6.m6.1.2.2.2" xref="S2.p1.6.m6.1.2.2.2.cmml">F</mi><mi id="S2.p1.6.m6.1.2.2.3" xref="S2.p1.6.m6.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.2.1" xref="S2.p1.6.m6.1.2.1.cmml">​</mo><mrow id="S2.p1.6.m6.1.2.3.2" xref="S2.p1.6.m6.1.2.cmml"><mo stretchy="false" id="S2.p1.6.m6.1.2.3.2.1" xref="S2.p1.6.m6.1.2.cmml">(</mo><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">x</mi><mo stretchy="false" id="S2.p1.6.m6.1.2.3.2.2" xref="S2.p1.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.2.cmml" xref="S2.p1.6.m6.1.2"><times id="S2.p1.6.m6.1.2.1.cmml" xref="S2.p1.6.m6.1.2.1"></times><apply id="S2.p1.6.m6.1.2.2.cmml" xref="S2.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.2.2.1.cmml" xref="S2.p1.6.m6.1.2.2">subscript</csymbol><ci id="S2.p1.6.m6.1.2.2.2.cmml" xref="S2.p1.6.m6.1.2.2.2">𝐹</ci><ci id="S2.p1.6.m6.1.2.2.3.cmml" xref="S2.p1.6.m6.1.2.2.3">𝑖</ci></apply><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">F_{i}(x)</annotation></semantics></math> = <math id="S2.p1.7.m7.3" class="ltx_Math" alttext="\frac{1}{s_{i}}\sum_{j_{i}=1}^{s_{i}}f_{ji}(z;x_{ji},y_{ji})" display="inline"><semantics id="S2.p1.7.m7.3a"><mrow id="S2.p1.7.m7.3.3" xref="S2.p1.7.m7.3.3.cmml"><mfrac id="S2.p1.7.m7.3.3.4" xref="S2.p1.7.m7.3.3.4.cmml"><mn id="S2.p1.7.m7.3.3.4.2" xref="S2.p1.7.m7.3.3.4.2.cmml">1</mn><msub id="S2.p1.7.m7.3.3.4.3" xref="S2.p1.7.m7.3.3.4.3.cmml"><mi id="S2.p1.7.m7.3.3.4.3.2" xref="S2.p1.7.m7.3.3.4.3.2.cmml">s</mi><mi id="S2.p1.7.m7.3.3.4.3.3" xref="S2.p1.7.m7.3.3.4.3.3.cmml">i</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.3.3.3" xref="S2.p1.7.m7.3.3.3.cmml">​</mo><mrow id="S2.p1.7.m7.3.3.2" xref="S2.p1.7.m7.3.3.2.cmml"><msubsup id="S2.p1.7.m7.3.3.2.3" xref="S2.p1.7.m7.3.3.2.3.cmml"><mo id="S2.p1.7.m7.3.3.2.3.2.2" xref="S2.p1.7.m7.3.3.2.3.2.2.cmml">∑</mo><mrow id="S2.p1.7.m7.3.3.2.3.2.3" xref="S2.p1.7.m7.3.3.2.3.2.3.cmml"><msub id="S2.p1.7.m7.3.3.2.3.2.3.2" xref="S2.p1.7.m7.3.3.2.3.2.3.2.cmml"><mi id="S2.p1.7.m7.3.3.2.3.2.3.2.2" xref="S2.p1.7.m7.3.3.2.3.2.3.2.2.cmml">j</mi><mi id="S2.p1.7.m7.3.3.2.3.2.3.2.3" xref="S2.p1.7.m7.3.3.2.3.2.3.2.3.cmml">i</mi></msub><mo id="S2.p1.7.m7.3.3.2.3.2.3.1" xref="S2.p1.7.m7.3.3.2.3.2.3.1.cmml">=</mo><mn id="S2.p1.7.m7.3.3.2.3.2.3.3" xref="S2.p1.7.m7.3.3.2.3.2.3.3.cmml">1</mn></mrow><msub id="S2.p1.7.m7.3.3.2.3.3" xref="S2.p1.7.m7.3.3.2.3.3.cmml"><mi id="S2.p1.7.m7.3.3.2.3.3.2" xref="S2.p1.7.m7.3.3.2.3.3.2.cmml">s</mi><mi id="S2.p1.7.m7.3.3.2.3.3.3" xref="S2.p1.7.m7.3.3.2.3.3.3.cmml">i</mi></msub></msubsup><mrow id="S2.p1.7.m7.3.3.2.2" xref="S2.p1.7.m7.3.3.2.2.cmml"><msub id="S2.p1.7.m7.3.3.2.2.4" xref="S2.p1.7.m7.3.3.2.2.4.cmml"><mi id="S2.p1.7.m7.3.3.2.2.4.2" xref="S2.p1.7.m7.3.3.2.2.4.2.cmml">f</mi><mrow id="S2.p1.7.m7.3.3.2.2.4.3" xref="S2.p1.7.m7.3.3.2.2.4.3.cmml"><mi id="S2.p1.7.m7.3.3.2.2.4.3.2" xref="S2.p1.7.m7.3.3.2.2.4.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.3.3.2.2.4.3.1" xref="S2.p1.7.m7.3.3.2.2.4.3.1.cmml">​</mo><mi id="S2.p1.7.m7.3.3.2.2.4.3.3" xref="S2.p1.7.m7.3.3.2.2.4.3.3.cmml">i</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.3.3.2.2.3" xref="S2.p1.7.m7.3.3.2.2.3.cmml">​</mo><mrow id="S2.p1.7.m7.3.3.2.2.2.2" xref="S2.p1.7.m7.3.3.2.2.2.3.cmml"><mo stretchy="false" id="S2.p1.7.m7.3.3.2.2.2.2.3" xref="S2.p1.7.m7.3.3.2.2.2.3.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">z</mi><mo id="S2.p1.7.m7.3.3.2.2.2.2.4" xref="S2.p1.7.m7.3.3.2.2.2.3.cmml">;</mo><msub id="S2.p1.7.m7.2.2.1.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.cmml"><mi id="S2.p1.7.m7.2.2.1.1.1.1.1.2" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.p1.7.m7.2.2.1.1.1.1.1.3" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.cmml"><mi id="S2.p1.7.m7.2.2.1.1.1.1.1.3.2" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.2.2.1.1.1.1.1.3.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.p1.7.m7.2.2.1.1.1.1.1.3.3" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.p1.7.m7.3.3.2.2.2.2.5" xref="S2.p1.7.m7.3.3.2.2.2.3.cmml">,</mo><msub id="S2.p1.7.m7.3.3.2.2.2.2.2" xref="S2.p1.7.m7.3.3.2.2.2.2.2.cmml"><mi id="S2.p1.7.m7.3.3.2.2.2.2.2.2" xref="S2.p1.7.m7.3.3.2.2.2.2.2.2.cmml">y</mi><mrow id="S2.p1.7.m7.3.3.2.2.2.2.2.3" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.cmml"><mi id="S2.p1.7.m7.3.3.2.2.2.2.2.3.2" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.3.3.2.2.2.2.2.3.1" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S2.p1.7.m7.3.3.2.2.2.2.2.3.3" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.3.cmml">i</mi></mrow></msub><mo stretchy="false" id="S2.p1.7.m7.3.3.2.2.2.2.6" xref="S2.p1.7.m7.3.3.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.3b"><apply id="S2.p1.7.m7.3.3.cmml" xref="S2.p1.7.m7.3.3"><times id="S2.p1.7.m7.3.3.3.cmml" xref="S2.p1.7.m7.3.3.3"></times><apply id="S2.p1.7.m7.3.3.4.cmml" xref="S2.p1.7.m7.3.3.4"><divide id="S2.p1.7.m7.3.3.4.1.cmml" xref="S2.p1.7.m7.3.3.4"></divide><cn type="integer" id="S2.p1.7.m7.3.3.4.2.cmml" xref="S2.p1.7.m7.3.3.4.2">1</cn><apply id="S2.p1.7.m7.3.3.4.3.cmml" xref="S2.p1.7.m7.3.3.4.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.4.3.1.cmml" xref="S2.p1.7.m7.3.3.4.3">subscript</csymbol><ci id="S2.p1.7.m7.3.3.4.3.2.cmml" xref="S2.p1.7.m7.3.3.4.3.2">𝑠</ci><ci id="S2.p1.7.m7.3.3.4.3.3.cmml" xref="S2.p1.7.m7.3.3.4.3.3">𝑖</ci></apply></apply><apply id="S2.p1.7.m7.3.3.2.cmml" xref="S2.p1.7.m7.3.3.2"><apply id="S2.p1.7.m7.3.3.2.3.cmml" xref="S2.p1.7.m7.3.3.2.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.3.1.cmml" xref="S2.p1.7.m7.3.3.2.3">superscript</csymbol><apply id="S2.p1.7.m7.3.3.2.3.2.cmml" xref="S2.p1.7.m7.3.3.2.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.3.2.1.cmml" xref="S2.p1.7.m7.3.3.2.3">subscript</csymbol><sum id="S2.p1.7.m7.3.3.2.3.2.2.cmml" xref="S2.p1.7.m7.3.3.2.3.2.2"></sum><apply id="S2.p1.7.m7.3.3.2.3.2.3.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3"><eq id="S2.p1.7.m7.3.3.2.3.2.3.1.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.1"></eq><apply id="S2.p1.7.m7.3.3.2.3.2.3.2.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.3.2.3.2.1.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.2">subscript</csymbol><ci id="S2.p1.7.m7.3.3.2.3.2.3.2.2.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.2.2">𝑗</ci><ci id="S2.p1.7.m7.3.3.2.3.2.3.2.3.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.2.3">𝑖</ci></apply><cn type="integer" id="S2.p1.7.m7.3.3.2.3.2.3.3.cmml" xref="S2.p1.7.m7.3.3.2.3.2.3.3">1</cn></apply></apply><apply id="S2.p1.7.m7.3.3.2.3.3.cmml" xref="S2.p1.7.m7.3.3.2.3.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.3.3.1.cmml" xref="S2.p1.7.m7.3.3.2.3.3">subscript</csymbol><ci id="S2.p1.7.m7.3.3.2.3.3.2.cmml" xref="S2.p1.7.m7.3.3.2.3.3.2">𝑠</ci><ci id="S2.p1.7.m7.3.3.2.3.3.3.cmml" xref="S2.p1.7.m7.3.3.2.3.3.3">𝑖</ci></apply></apply><apply id="S2.p1.7.m7.3.3.2.2.cmml" xref="S2.p1.7.m7.3.3.2.2"><times id="S2.p1.7.m7.3.3.2.2.3.cmml" xref="S2.p1.7.m7.3.3.2.2.3"></times><apply id="S2.p1.7.m7.3.3.2.2.4.cmml" xref="S2.p1.7.m7.3.3.2.2.4"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.2.4.1.cmml" xref="S2.p1.7.m7.3.3.2.2.4">subscript</csymbol><ci id="S2.p1.7.m7.3.3.2.2.4.2.cmml" xref="S2.p1.7.m7.3.3.2.2.4.2">𝑓</ci><apply id="S2.p1.7.m7.3.3.2.2.4.3.cmml" xref="S2.p1.7.m7.3.3.2.2.4.3"><times id="S2.p1.7.m7.3.3.2.2.4.3.1.cmml" xref="S2.p1.7.m7.3.3.2.2.4.3.1"></times><ci id="S2.p1.7.m7.3.3.2.2.4.3.2.cmml" xref="S2.p1.7.m7.3.3.2.2.4.3.2">𝑗</ci><ci id="S2.p1.7.m7.3.3.2.2.4.3.3.cmml" xref="S2.p1.7.m7.3.3.2.2.4.3.3">𝑖</ci></apply></apply><list id="S2.p1.7.m7.3.3.2.2.2.3.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">𝑧</ci><apply id="S2.p1.7.m7.2.2.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2">𝑥</ci><apply id="S2.p1.7.m7.2.2.1.1.1.1.1.3.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3"><times id="S2.p1.7.m7.2.2.1.1.1.1.1.3.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.1"></times><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.3.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.2">𝑗</ci><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.3.3.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S2.p1.7.m7.3.3.2.2.2.2.2.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.3.3.2.2.2.2.2.1.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2">subscript</csymbol><ci id="S2.p1.7.m7.3.3.2.2.2.2.2.2.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2.2">𝑦</ci><apply id="S2.p1.7.m7.3.3.2.2.2.2.2.3.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3"><times id="S2.p1.7.m7.3.3.2.2.2.2.2.3.1.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.1"></times><ci id="S2.p1.7.m7.3.3.2.2.2.2.2.3.2.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.2">𝑗</ci><ci id="S2.p1.7.m7.3.3.2.2.2.2.2.3.3.cmml" xref="S2.p1.7.m7.3.3.2.2.2.2.2.3.3">𝑖</ci></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.3c">\frac{1}{s_{i}}\sum_{j_{i}=1}^{s_{i}}f_{ji}(z;x_{ji},y_{ji})</annotation></semantics></math>, where <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S2.p1.8.m8.1a"><msub id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mi id="S2.p1.8.m8.1.1.2" xref="S2.p1.8.m8.1.1.2.cmml">s</mi><mi id="S2.p1.8.m8.1.1.3" xref="S2.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.p1.8.m8.1.1.2.cmml" xref="S2.p1.8.m8.1.1.2">𝑠</ci><ci id="S2.p1.8.m8.1.1.3.cmml" xref="S2.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">s_{i}</annotation></semantics></math> is the number of locally available samples, <math id="S2.p1.9.m9.1" class="ltx_Math" alttext="P_{k}={1}/{s}" display="inline"><semantics id="S2.p1.9.m9.1a"><mrow id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml"><msub id="S2.p1.9.m9.1.1.2" xref="S2.p1.9.m9.1.1.2.cmml"><mi id="S2.p1.9.m9.1.1.2.2" xref="S2.p1.9.m9.1.1.2.2.cmml">P</mi><mi id="S2.p1.9.m9.1.1.2.3" xref="S2.p1.9.m9.1.1.2.3.cmml">k</mi></msub><mo id="S2.p1.9.m9.1.1.1" xref="S2.p1.9.m9.1.1.1.cmml">=</mo><mrow id="S2.p1.9.m9.1.1.3" xref="S2.p1.9.m9.1.1.3.cmml"><mn id="S2.p1.9.m9.1.1.3.2" xref="S2.p1.9.m9.1.1.3.2.cmml">1</mn><mo id="S2.p1.9.m9.1.1.3.1" xref="S2.p1.9.m9.1.1.3.1.cmml">/</mo><mi id="S2.p1.9.m9.1.1.3.3" xref="S2.p1.9.m9.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><apply id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1"><eq id="S2.p1.9.m9.1.1.1.cmml" xref="S2.p1.9.m9.1.1.1"></eq><apply id="S2.p1.9.m9.1.1.2.cmml" xref="S2.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.2.1.cmml" xref="S2.p1.9.m9.1.1.2">subscript</csymbol><ci id="S2.p1.9.m9.1.1.2.2.cmml" xref="S2.p1.9.m9.1.1.2.2">𝑃</ci><ci id="S2.p1.9.m9.1.1.2.3.cmml" xref="S2.p1.9.m9.1.1.2.3">𝑘</ci></apply><apply id="S2.p1.9.m9.1.1.3.cmml" xref="S2.p1.9.m9.1.1.3"><divide id="S2.p1.9.m9.1.1.3.1.cmml" xref="S2.p1.9.m9.1.1.3.1"></divide><cn type="integer" id="S2.p1.9.m9.1.1.3.2.cmml" xref="S2.p1.9.m9.1.1.3.2">1</cn><ci id="S2.p1.9.m9.1.1.3.3.cmml" xref="S2.p1.9.m9.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">P_{k}={1}/{s}</annotation></semantics></math> or <math id="S2.p1.10.m10.1" class="ltx_Math" alttext="P_{k}={s_{i}}/{s}" display="inline"><semantics id="S2.p1.10.m10.1a"><mrow id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml"><msub id="S2.p1.10.m10.1.1.2" xref="S2.p1.10.m10.1.1.2.cmml"><mi id="S2.p1.10.m10.1.1.2.2" xref="S2.p1.10.m10.1.1.2.2.cmml">P</mi><mi id="S2.p1.10.m10.1.1.2.3" xref="S2.p1.10.m10.1.1.2.3.cmml">k</mi></msub><mo id="S2.p1.10.m10.1.1.1" xref="S2.p1.10.m10.1.1.1.cmml">=</mo><mrow id="S2.p1.10.m10.1.1.3" xref="S2.p1.10.m10.1.1.3.cmml"><msub id="S2.p1.10.m10.1.1.3.2" xref="S2.p1.10.m10.1.1.3.2.cmml"><mi id="S2.p1.10.m10.1.1.3.2.2" xref="S2.p1.10.m10.1.1.3.2.2.cmml">s</mi><mi id="S2.p1.10.m10.1.1.3.2.3" xref="S2.p1.10.m10.1.1.3.2.3.cmml">i</mi></msub><mo id="S2.p1.10.m10.1.1.3.1" xref="S2.p1.10.m10.1.1.3.1.cmml">/</mo><mi id="S2.p1.10.m10.1.1.3.3" xref="S2.p1.10.m10.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><apply id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1"><eq id="S2.p1.10.m10.1.1.1.cmml" xref="S2.p1.10.m10.1.1.1"></eq><apply id="S2.p1.10.m10.1.1.2.cmml" xref="S2.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.2.1.cmml" xref="S2.p1.10.m10.1.1.2">subscript</csymbol><ci id="S2.p1.10.m10.1.1.2.2.cmml" xref="S2.p1.10.m10.1.1.2.2">𝑃</ci><ci id="S2.p1.10.m10.1.1.2.3.cmml" xref="S2.p1.10.m10.1.1.2.3">𝑘</ci></apply><apply id="S2.p1.10.m10.1.1.3.cmml" xref="S2.p1.10.m10.1.1.3"><divide id="S2.p1.10.m10.1.1.3.1.cmml" xref="S2.p1.10.m10.1.1.3.1"></divide><apply id="S2.p1.10.m10.1.1.3.2.cmml" xref="S2.p1.10.m10.1.1.3.2"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.3.2.1.cmml" xref="S2.p1.10.m10.1.1.3.2">subscript</csymbol><ci id="S2.p1.10.m10.1.1.3.2.2.cmml" xref="S2.p1.10.m10.1.1.3.2.2">𝑠</ci><ci id="S2.p1.10.m10.1.1.3.2.3.cmml" xref="S2.p1.10.m10.1.1.3.2.3">𝑖</ci></apply><ci id="S2.p1.10.m10.1.1.3.3.cmml" xref="S2.p1.10.m10.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">P_{k}={s_{i}}/{s}</annotation></semantics></math>, and total samples <math id="S2.p1.11.m11.1" class="ltx_Math" alttext="s=\sum_{i}s_{i}" display="inline"><semantics id="S2.p1.11.m11.1a"><mrow id="S2.p1.11.m11.1.1" xref="S2.p1.11.m11.1.1.cmml"><mi id="S2.p1.11.m11.1.1.2" xref="S2.p1.11.m11.1.1.2.cmml">s</mi><mo rspace="0.111em" id="S2.p1.11.m11.1.1.1" xref="S2.p1.11.m11.1.1.1.cmml">=</mo><mrow id="S2.p1.11.m11.1.1.3" xref="S2.p1.11.m11.1.1.3.cmml"><msub id="S2.p1.11.m11.1.1.3.1" xref="S2.p1.11.m11.1.1.3.1.cmml"><mo id="S2.p1.11.m11.1.1.3.1.2" xref="S2.p1.11.m11.1.1.3.1.2.cmml">∑</mo><mi id="S2.p1.11.m11.1.1.3.1.3" xref="S2.p1.11.m11.1.1.3.1.3.cmml">i</mi></msub><msub id="S2.p1.11.m11.1.1.3.2" xref="S2.p1.11.m11.1.1.3.2.cmml"><mi id="S2.p1.11.m11.1.1.3.2.2" xref="S2.p1.11.m11.1.1.3.2.2.cmml">s</mi><mi id="S2.p1.11.m11.1.1.3.2.3" xref="S2.p1.11.m11.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.11.m11.1b"><apply id="S2.p1.11.m11.1.1.cmml" xref="S2.p1.11.m11.1.1"><eq id="S2.p1.11.m11.1.1.1.cmml" xref="S2.p1.11.m11.1.1.1"></eq><ci id="S2.p1.11.m11.1.1.2.cmml" xref="S2.p1.11.m11.1.1.2">𝑠</ci><apply id="S2.p1.11.m11.1.1.3.cmml" xref="S2.p1.11.m11.1.1.3"><apply id="S2.p1.11.m11.1.1.3.1.cmml" xref="S2.p1.11.m11.1.1.3.1"><csymbol cd="ambiguous" id="S2.p1.11.m11.1.1.3.1.1.cmml" xref="S2.p1.11.m11.1.1.3.1">subscript</csymbol><sum id="S2.p1.11.m11.1.1.3.1.2.cmml" xref="S2.p1.11.m11.1.1.3.1.2"></sum><ci id="S2.p1.11.m11.1.1.3.1.3.cmml" xref="S2.p1.11.m11.1.1.3.1.3">𝑖</ci></apply><apply id="S2.p1.11.m11.1.1.3.2.cmml" xref="S2.p1.11.m11.1.1.3.2"><csymbol cd="ambiguous" id="S2.p1.11.m11.1.1.3.2.1.cmml" xref="S2.p1.11.m11.1.1.3.2">subscript</csymbol><ci id="S2.p1.11.m11.1.1.3.2.2.cmml" xref="S2.p1.11.m11.1.1.3.2.2">𝑠</ci><ci id="S2.p1.11.m11.1.1.3.2.3.cmml" xref="S2.p1.11.m11.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.11.m11.1c">s=\sum_{i}s_{i}</annotation></semantics></math>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The FL terminology was first introduced by <cite class="ltx_cite ltx_citemacro_citet">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>)</cite> with an algorithm called federated averaging (FedAvg). FedAvg includes multiple rounds of communication between clients and the server which are interleaved by multiple local model update steps at each client.
The clients do not share raw data due to security and privacy considerations. In this way, FL preserves data privacy since only the statistical summary or model weights are shared with the server.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2002.10610/assets/figure1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>FL procedure considering <math id="S2.F1.3.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.F1.3.2.m1.1b"><mi id="S2.F1.3.2.m1.1.1" xref="S2.F1.3.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.F1.3.2.m1.1c"><ci id="S2.F1.3.2.m1.1.1.cmml" xref="S2.F1.3.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.2.m1.1d">N</annotation></semantics></math> participants.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">FL generally includes three steps as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Background of Federated Learning ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><em id="S2.p4.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Step 1 (Initialization of training task and global model)</em>: In the initial phase, the central server decides the task requirement and target application. An initial global model is generated by using hyper-parameters and maintaining a training procedure (e.g., learning rate) specified by the server. Then the server broadcasts the initialized global model <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="W_{G}^{0}" display="inline"><semantics id="S2.p4.1.m1.1a"><msubsup id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mi id="S2.p4.1.m1.1.1.2.2" xref="S2.p4.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.p4.1.m1.1.1.2.3" xref="S2.p4.1.m1.1.1.2.3.cmml">G</mi><mn id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1">superscript</csymbol><apply id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.2.1.cmml" xref="S2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.2.cmml" xref="S2.p4.1.m1.1.1.2.2">𝑊</ci><ci id="S2.p4.1.m1.1.1.2.3.cmml" xref="S2.p4.1.m1.1.1.2.3">𝐺</ci></apply><cn type="integer" id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">W_{G}^{0}</annotation></semantics></math> to the selected local participants.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.7" class="ltx_p"><em id="S2.p5.7.1" class="ltx_emph ltx_font_bold ltx_font_italic">Step 2 (Local model update)</em>:
Each participating client in the network has a collection of data from where it performs local model update. Upon receiving the global model <math id="S2.p5.1.m1.1" class="ltx_Math" alttext="W_{G}^{t}" display="inline"><semantics id="S2.p5.1.m1.1a"><msubsup id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml"><mi id="S2.p5.1.m1.1.1.2.2" xref="S2.p5.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.p5.1.m1.1.1.2.3" xref="S2.p5.1.m1.1.1.2.3.cmml">G</mi><mi id="S2.p5.1.m1.1.1.3" xref="S2.p5.1.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><apply id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.1.m1.1.1.1.cmml" xref="S2.p5.1.m1.1.1">superscript</csymbol><apply id="S2.p5.1.m1.1.1.2.cmml" xref="S2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.1.m1.1.1.2.1.cmml" xref="S2.p5.1.m1.1.1">subscript</csymbol><ci id="S2.p5.1.m1.1.1.2.2.cmml" xref="S2.p5.1.m1.1.1.2.2">𝑊</ci><ci id="S2.p5.1.m1.1.1.2.3.cmml" xref="S2.p5.1.m1.1.1.2.3">𝐺</ci></apply><ci id="S2.p5.1.m1.1.1.3.cmml" xref="S2.p5.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">W_{G}^{t}</annotation></semantics></math>, where <math id="S2.p5.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p5.2.m2.1a"><mi id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><ci id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">t</annotation></semantics></math> denotes the <math id="S2.p5.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p5.3.m3.1a"><mi id="S2.p5.3.m3.1.1" xref="S2.p5.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p5.3.m3.1b"><ci id="S2.p5.3.m3.1.1.cmml" xref="S2.p5.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.3.m3.1c">t</annotation></semantics></math>-th iteration, each client <math id="S2.p5.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p5.4.m4.1a"><mi id="S2.p5.4.m4.1.1" xref="S2.p5.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p5.4.m4.1b"><ci id="S2.p5.4.m4.1.1.cmml" xref="S2.p5.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.4.m4.1c">i</annotation></semantics></math> updates its model parameters <math id="S2.p5.5.m5.1" class="ltx_Math" alttext="W_{i}^{t}" display="inline"><semantics id="S2.p5.5.m5.1a"><msubsup id="S2.p5.5.m5.1.1" xref="S2.p5.5.m5.1.1.cmml"><mi id="S2.p5.5.m5.1.1.2.2" xref="S2.p5.5.m5.1.1.2.2.cmml">W</mi><mi id="S2.p5.5.m5.1.1.2.3" xref="S2.p5.5.m5.1.1.2.3.cmml">i</mi><mi id="S2.p5.5.m5.1.1.3" xref="S2.p5.5.m5.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p5.5.m5.1b"><apply id="S2.p5.5.m5.1.1.cmml" xref="S2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p5.5.m5.1.1.1.cmml" xref="S2.p5.5.m5.1.1">superscript</csymbol><apply id="S2.p5.5.m5.1.1.2.cmml" xref="S2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p5.5.m5.1.1.2.1.cmml" xref="S2.p5.5.m5.1.1">subscript</csymbol><ci id="S2.p5.5.m5.1.1.2.2.cmml" xref="S2.p5.5.m5.1.1.2.2">𝑊</ci><ci id="S2.p5.5.m5.1.1.2.3.cmml" xref="S2.p5.5.m5.1.1.2.3">𝑖</ci></apply><ci id="S2.p5.5.m5.1.1.3.cmml" xref="S2.p5.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.5.m5.1c">W_{i}^{t}</annotation></semantics></math> with the goal of
finding optimal parameters <math id="S2.p5.6.m6.1" class="ltx_Math" alttext="W_{i}^{t}" display="inline"><semantics id="S2.p5.6.m6.1a"><msubsup id="S2.p5.6.m6.1.1" xref="S2.p5.6.m6.1.1.cmml"><mi id="S2.p5.6.m6.1.1.2.2" xref="S2.p5.6.m6.1.1.2.2.cmml">W</mi><mi id="S2.p5.6.m6.1.1.2.3" xref="S2.p5.6.m6.1.1.2.3.cmml">i</mi><mi id="S2.p5.6.m6.1.1.3" xref="S2.p5.6.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p5.6.m6.1b"><apply id="S2.p5.6.m6.1.1.cmml" xref="S2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p5.6.m6.1.1.1.cmml" xref="S2.p5.6.m6.1.1">superscript</csymbol><apply id="S2.p5.6.m6.1.1.2.cmml" xref="S2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p5.6.m6.1.1.2.1.cmml" xref="S2.p5.6.m6.1.1">subscript</csymbol><ci id="S2.p5.6.m6.1.1.2.2.cmml" xref="S2.p5.6.m6.1.1.2.2">𝑊</ci><ci id="S2.p5.6.m6.1.1.2.3.cmml" xref="S2.p5.6.m6.1.1.2.3">𝑖</ci></apply><ci id="S2.p5.6.m6.1.1.3.cmml" xref="S2.p5.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.6.m6.1c">W_{i}^{t}</annotation></semantics></math> that minimizes the local loss function <math id="S2.p5.7.m7.1" class="ltx_Math" alttext="F_{i}(W_{i}^{t})" display="inline"><semantics id="S2.p5.7.m7.1a"><mrow id="S2.p5.7.m7.1.1" xref="S2.p5.7.m7.1.1.cmml"><msub id="S2.p5.7.m7.1.1.3" xref="S2.p5.7.m7.1.1.3.cmml"><mi id="S2.p5.7.m7.1.1.3.2" xref="S2.p5.7.m7.1.1.3.2.cmml">F</mi><mi id="S2.p5.7.m7.1.1.3.3" xref="S2.p5.7.m7.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.p5.7.m7.1.1.2" xref="S2.p5.7.m7.1.1.2.cmml">​</mo><mrow id="S2.p5.7.m7.1.1.1.1" xref="S2.p5.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p5.7.m7.1.1.1.1.2" xref="S2.p5.7.m7.1.1.1.1.1.cmml">(</mo><msubsup id="S2.p5.7.m7.1.1.1.1.1" xref="S2.p5.7.m7.1.1.1.1.1.cmml"><mi id="S2.p5.7.m7.1.1.1.1.1.2.2" xref="S2.p5.7.m7.1.1.1.1.1.2.2.cmml">W</mi><mi id="S2.p5.7.m7.1.1.1.1.1.2.3" xref="S2.p5.7.m7.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.p5.7.m7.1.1.1.1.1.3" xref="S2.p5.7.m7.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.p5.7.m7.1.1.1.1.3" xref="S2.p5.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p5.7.m7.1b"><apply id="S2.p5.7.m7.1.1.cmml" xref="S2.p5.7.m7.1.1"><times id="S2.p5.7.m7.1.1.2.cmml" xref="S2.p5.7.m7.1.1.2"></times><apply id="S2.p5.7.m7.1.1.3.cmml" xref="S2.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.p5.7.m7.1.1.3.1.cmml" xref="S2.p5.7.m7.1.1.3">subscript</csymbol><ci id="S2.p5.7.m7.1.1.3.2.cmml" xref="S2.p5.7.m7.1.1.3.2">𝐹</ci><ci id="S2.p5.7.m7.1.1.3.3.cmml" xref="S2.p5.7.m7.1.1.3.3">𝑖</ci></apply><apply id="S2.p5.7.m7.1.1.1.1.1.cmml" xref="S2.p5.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S2.p5.7.m7.1.1.1.1.1.1.cmml" xref="S2.p5.7.m7.1.1.1.1">superscript</csymbol><apply id="S2.p5.7.m7.1.1.1.1.1.2.cmml" xref="S2.p5.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S2.p5.7.m7.1.1.1.1.1.2.1.cmml" xref="S2.p5.7.m7.1.1.1.1">subscript</csymbol><ci id="S2.p5.7.m7.1.1.1.1.1.2.2.cmml" xref="S2.p5.7.m7.1.1.1.1.1.2.2">𝑊</ci><ci id="S2.p5.7.m7.1.1.1.1.1.2.3.cmml" xref="S2.p5.7.m7.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.p5.7.m7.1.1.1.1.1.3.cmml" xref="S2.p5.7.m7.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.7.m7.1c">F_{i}(W_{i}^{t})</annotation></semantics></math>.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.2" class="ltx_p"><em id="S2.p6.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Step 3 (Global aggregation)</em>:
The central server aggregates the local updates received from all clients and generates an aggregated updated global model <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="W_{G}^{t+1}" display="inline"><semantics id="S2.p6.1.m1.1a"><msubsup id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml"><mi id="S2.p6.1.m1.1.1.2.2" xref="S2.p6.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.p6.1.m1.1.1.2.3" xref="S2.p6.1.m1.1.1.2.3.cmml">G</mi><mrow id="S2.p6.1.m1.1.1.3" xref="S2.p6.1.m1.1.1.3.cmml"><mi id="S2.p6.1.m1.1.1.3.2" xref="S2.p6.1.m1.1.1.3.2.cmml">t</mi><mo id="S2.p6.1.m1.1.1.3.1" xref="S2.p6.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.p6.1.m1.1.1.3.3" xref="S2.p6.1.m1.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><apply id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.1.1.1.cmml" xref="S2.p6.1.m1.1.1">superscript</csymbol><apply id="S2.p6.1.m1.1.1.2.cmml" xref="S2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.1.1.2.1.cmml" xref="S2.p6.1.m1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.1.1.2.2.cmml" xref="S2.p6.1.m1.1.1.2.2">𝑊</ci><ci id="S2.p6.1.m1.1.1.2.3.cmml" xref="S2.p6.1.m1.1.1.2.3">𝐺</ci></apply><apply id="S2.p6.1.m1.1.1.3.cmml" xref="S2.p6.1.m1.1.1.3"><plus id="S2.p6.1.m1.1.1.3.1.cmml" xref="S2.p6.1.m1.1.1.3.1"></plus><ci id="S2.p6.1.m1.1.1.3.2.cmml" xref="S2.p6.1.m1.1.1.3.2">𝑡</ci><cn type="integer" id="S2.p6.1.m1.1.1.3.3.cmml" xref="S2.p6.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">W_{G}^{t+1}</annotation></semantics></math>. This latest global model is then sent back to clients who contributed to generating this new model. The goal of the central server is to minimize the global loss function
<math id="S2.p6.2.m2.1" class="ltx_Math" alttext="F(W_{G}^{t})" display="inline"><semantics id="S2.p6.2.m2.1a"><mrow id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml"><mi id="S2.p6.2.m2.1.1.3" xref="S2.p6.2.m2.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.p6.2.m2.1.1.2" xref="S2.p6.2.m2.1.1.2.cmml">​</mo><mrow id="S2.p6.2.m2.1.1.1.1" xref="S2.p6.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p6.2.m2.1.1.1.1.2" xref="S2.p6.2.m2.1.1.1.1.1.cmml">(</mo><msubsup id="S2.p6.2.m2.1.1.1.1.1" xref="S2.p6.2.m2.1.1.1.1.1.cmml"><mi id="S2.p6.2.m2.1.1.1.1.1.2.2" xref="S2.p6.2.m2.1.1.1.1.1.2.2.cmml">W</mi><mi id="S2.p6.2.m2.1.1.1.1.1.2.3" xref="S2.p6.2.m2.1.1.1.1.1.2.3.cmml">G</mi><mi id="S2.p6.2.m2.1.1.1.1.1.3" xref="S2.p6.2.m2.1.1.1.1.1.3.cmml">t</mi></msubsup><mo stretchy="false" id="S2.p6.2.m2.1.1.1.1.3" xref="S2.p6.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><apply id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1"><times id="S2.p6.2.m2.1.1.2.cmml" xref="S2.p6.2.m2.1.1.2"></times><ci id="S2.p6.2.m2.1.1.3.cmml" xref="S2.p6.2.m2.1.1.3">𝐹</ci><apply id="S2.p6.2.m2.1.1.1.1.1.cmml" xref="S2.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m2.1.1.1.1.1.1.cmml" xref="S2.p6.2.m2.1.1.1.1">superscript</csymbol><apply id="S2.p6.2.m2.1.1.1.1.1.2.cmml" xref="S2.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m2.1.1.1.1.1.2.1.cmml" xref="S2.p6.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.p6.2.m2.1.1.1.1.1.2.2.cmml" xref="S2.p6.2.m2.1.1.1.1.1.2.2">𝑊</ci><ci id="S2.p6.2.m2.1.1.1.1.1.2.3.cmml" xref="S2.p6.2.m2.1.1.1.1.1.2.3">𝐺</ci></apply><ci id="S2.p6.2.m2.1.1.1.1.1.3.cmml" xref="S2.p6.2.m2.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">F(W_{G}^{t})</annotation></semantics></math> as defined in (<a href="#S2.E1" title="In 2 Background of Federated Learning ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Steps 2 and 3 are repeated until the central server attains target training accuracy or reaches a convergence.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated Learning Applications</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">FL suits best in applications where data within the device is more significant than data located in the server. Current applications related to FL are mostly based on supervised learning, typically utilizing labels retrieved from user activities (e.g., button click, keyboard type, etc.). In this section, we briefly discuss some applications based on FL.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p2.1.m1.1a"><mo id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Smart Healthcare:</span> Smart healthcare involves sensitive data, and it needs to train models on-device. For instance, heart attack situations can be predicted locally for end-users with wearable devices <cite class="ltx_cite ltx_citemacro_cite">Huang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite> suggested that if all medical centers cooperate to form a large dataset by sharing their data with proper labeling, the performance of ML model would be remarkably improved. The combination of FL and transfer learning is a preeminent way to achieve this goal.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p3.1.m1.1a"><mo id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Recommendation System:</span> It is a widely used method that depends on information sharing among users, which may cause privacy leakage. To handle this, <cite class="ltx_cite ltx_citemacro_citet">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> proposed a federated meta-learning based approach, where local devices share their algorithm instead of data or model with the central server. It eliminates the risk of privacy leakage and leads to proper training of the model on the local devices.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p4.1.m1.1a"><mo id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Next-word Prediction:</span> An on-device, distributed framework for next-word prediction for smartphones is proposed by <cite class="ltx_cite ltx_citemacro_citet">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>. They trained the local client devices using the FedAvg algorithm and observed higher prediction recall compared with other approaches that transferred some sensitive information to the server for learning.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><math id="S3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p5.1.m1.1a"><mo id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Keyword Spotting:</span> An embedded speech model, i.e., wake word detector, is proposed in <cite class="ltx_cite ltx_citemacro_cite">Leroy and others (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, where an experiment using ‘Hey Snips’ keyword is conducted based on a crowd-sourced dataset. For keeping user’s speech private, they applied the FL strategy.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><math id="S3.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p6.1.m1.1a"><mo id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p6.1.1" class="ltx_text ltx_font_bold">On-device Ranking:</span> An on-device ranking of search results is implemented in <cite class="ltx_cite ltx_citemacro_cite">Bonawitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite> without conducting expensive calls to the server. This avoids issues related to constrained resources, and sensitive information remains on the device. The system can label the user interaction with the ranking feature by observing the user’s preferred selected item from the ranked list during the interaction period.</p>
</div>
<div id="S3.p7" class="ltx_para ltx_noindent">
<p id="S3.p7.1" class="ltx_p"><math id="S3.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.p7.1.m1.1a"><mo id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><ci id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.p7.1.1" class="ltx_text ltx_font_bold">Relevant Content Suggestions for On-Device Keyboard:</span>
A content suggestion approach for on-device smartphone keyboard was presented
in <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite>,
where value is added to the users by suggesting related contents. For instance, while typing on a keyboard, it can suggest related contents by getting triggered based on the assigned value learned through
on-device training.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Core Challenges to Implement Federated Learning on Resource-Constrained Devices</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In real-world IoT environments, available clients may have heterogeneous hardware (e.g., memory, computational ability) and variant resource-assets (e.g., energy budget). As a result, we cannot consider all available clients uniformly as the client’s behavior depends on resource-availability. Besides, heterogeneity in hardware can preclude weak clients from participating in FL.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Extensive surveys were conducted on the architecture and training process of IoT edge networks and FL <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>); Cui <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>); Park <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>); Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite>, but challenges of FL systems due to resource limitation were not comprehensively discussed. In this section, we discuss the core challenges (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Core Challenges to Implement Federated Learning on Resource-Constrained Devices ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) associated with the implementation of FL for resource-constrained IoT devices.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2002.10610/assets/fig3_updated.png" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="544" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Core challenges associated with resource-constrained IoT clients in the FL system.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Limited Memory and Energy Budget</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The agents participating in FL may have limited memory capacity, constrained computational ability, and bounded energy budget. While reduced computational capabilities imply that it takes more time to process data, limited memory capacity makes the device prone to over-flooding. These situations can lead to more expensive communication (see Section <a href="#S4.SS2" title="4.2 Expensive Communication ‣ 4 Core Challenges to Implement Federated Learning on Resource-Constrained Devices ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> for more details) and curtail the overall performance of the system. <cite class="ltx_cite ltx_citemacro_citet">Das and Brunschwiler (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> analyzed hardware limitation challenges in the implementation of FL by considering Raspberry Pi clients. They studied the feasibility of implementing FL on resource-constrained edge devices. Furthermore, necessary hardware requirements are highlighted in <cite class="ltx_cite ltx_citemacro_cite">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> during the implementation of next word prediction on keyboard. In terms of hardware requirement, the devices must have at least 2 gigabytes of memory availability, whereas some microcontrollers have very limited memory. Such memory limitation of clients can be managed by storing limited sizes of data and this will help the resource-bounded clients to process those data locally. After a certain period, data within a client can be aggregated and backed-up to avoid unexpected overflow of memory. In this regard, a novel FL-based approach can be adopted through which shards of data are distributed to the clients to obtain the target model quickly <cite class="ltx_cite ltx_citemacro_cite">Haddadpour <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. According to resource availability, we can also choose proficient clients that have higher bandwidth, better processing ability, greater memory size, and higher energy budget to participate in FL, and clients with resource shortage will not participate.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Expensive Communication</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The necessity of training local model can be motivated by insufficient communication bandwidth to broadcast local data to a server for central computing. In FL, the server interacts with clients for getting updates and based on local model training, after which the server disseminates an updated global model. When we consider resource-constrained clients with limited bandwidth and transmission power, it is challenging to utilize those resources prudently for reaching a convergence. Furthermore, FL systems may comprise millions of devices with various degrees of resources, and local computation within the devices can be faster than the network communication <cite class="ltx_cite ltx_citemacro_cite">Huang and others (<a href="#bib.bib11" title="" class="ltx_ref">2013</a>)</cite>. Although frequent interaction between the server and the clients can help us to attain a target model swiftly, it is costly to perform communication repeatedly. We need to consider this trade-off while designing optimization algorithms to make proper use of limited resources. <cite class="ltx_cite ltx_citemacro_citet">Ma <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> discussed the trade-offs between communication expenses and optimal resource utilization, but they did not study the complexity of the local problem’s solution. We need to devise a way to achieve the target model by sending a compressed size of message <cite class="ltx_cite ltx_citemacro_cite">Konečnỳ <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> and by carrying out a minimal number of communication round between the server and local clients.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Heterogeneous Hardware</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In FL, training can run on multiple devices, each coming from different vendors or belonging to a different generation of products. It creates a network of devices with varying computing and memory capabilities and different battery lives. Therefore, training efficiency may vary significantly across client devices, and considering all clients with the same scale does not provide us an optimal solution. <cite class="ltx_cite ltx_citemacro_citet">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> discussed why FL should be aware of heterogeneous hardware configurations. We need to select clients for training purposes based on system requirements. However, due to strict cost and energy requirements, only a few clients might end up meeting the required criterion. It is possible that most of the proficient clients go out of the network, and existing clients do not fulfill system requirements. Hence dealing with resource-constrained heterogeneous devices is a challenge.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Energy Efficient Training of DNNs</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Deep neural networks (DNNs) are widely used, especially in implementing artificial intelligence-based applications. It is difficult to perform DNNs on resource-constrained clients as we need to ensure the required processing capability and energy availability. <cite class="ltx_cite ltx_citemacro_citet">Wu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite> discussed enabling training on local devices. They demonstrated a way to carry out both inference and training with comparative low-bitwidth integers to ensure that back-propagation requires integer numbers, which reduces the hardware requirement of training. <cite class="ltx_cite ltx_citemacro_citet">Park <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> showed one way to generate a high-quality machine learning model based on on-device model parameters, output, and data aggregation, while <cite class="ltx_cite ltx_citemacro_citet">Jiang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> proposed a method for efficient learning using pruned models. Another exciting approach presented in <cite class="ltx_cite ltx_citemacro_cite">Leroy and others (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite> discussed training higher capacity models with fewer parameters. Particularly, energy-efficient learning on resource-constrained devices is essential if the size and number of features of the training dataset are large. In such a case, generating a higher-quality model by considering fewer parameters and performing on-device training could be a challenge.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Scheduling</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In synchronous FL, all clients interact with server at the same time, while in asynchronous FL, the training period can be different. Hence, it is essential to determine the training period for all local participants, which we call <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">scheduling</span>. If we consider resource-constrained clients, then it is not an ideal solution to carry out scheduling frequently. Rather, an optimized scheduling period would cost minimal energy consumption and less bandwidth. To achieve this, parallel training sessions can be avoided due to high resource consumption, and a worker queue can be maintained for on-device multi-tenant systems. Moreover, clients should not perform scheduling tasks when they possess old data. It may be possible that older data are repeatedly used for training, while newer data are omitted <cite class="ltx_cite ltx_citemacro_cite">Bonawitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. Old data will not give us much variation to the model parameter, and resources will be wasted without model improvement. In addition, any client may frequently use a particular app that provides malicious data, and identification of such app usage is also a challenge. Thus, it is necessary to execute scheduling after filtering out which data should be used for training.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Stragglers</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">In FL, one main reason for performance bottleneck is the presence of straggler clients. While each client is responsible for generating a model with their data and sharing that with the server after a certain period, a straggler client may fail to share its model with the server at a proper time convenience. Due to this, the central server needs to wait until all the straggler clients share their model. Hence, the overall training procedure of the clients is delayed. One solution to avoid straggler clients is to select competent clients based on their resource-availability. <cite class="ltx_cite ltx_citemacro_citet">Das and Brunschwiler (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> proposed a solution to acknowledge the computational power, i.e., overall resource utilization of the clients after each local update. By observing each client’s resource utilization, a predictive model can be formed for adjusting the local computation of the clients. Another option is to use asynchronous training, which, however, is challenging in the presence of non-independent/identically distributed (non-IID) data among clients (see Section <a href="#S5.SS2" title="5.2 Guarantee Convergence in Asynchronous Federated Learning ‣ 5 Open Issues of Federated Learning Algorithms and Hardware Developments ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Labelling Unlabelled Data</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">In the FL system, most existing techniques considered that data are labeled. However, data collected by local devices can be unlabeled due to connection or communication issues and can be mislabeled. <cite class="ltx_cite ltx_citemacro_citet">Gu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> designed a framework to identify mislabeled data, while <cite class="ltx_cite ltx_citemacro_citet">Lim <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> proposed a solution to put a label on unlabeled data using collaborative learning with neighboring devices. However, it is challenging to label data in real-time for resource-constrained devices, as a client may have power limitations or other issues.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Open Issues of Federated Learning Algorithms and Hardware Developments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the previous section, we discussed the core challenges for resource-constrained IoT clients while deploying FL. There still exist some aspects that need to be addressed (see Figure <a href="#S5.F3" title="Figure 3 ‣ 5 Open Issues of Federated Learning Algorithms and Hardware Developments ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and can be considered as open research problems. In this section, we highlight promising future trends.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2002.10610/assets/fig4_updated3.png" id="S5.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="389" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Open issues and future directions of the federating learning theory and applications.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Deploying Existing Algorithm to Reduce Communication Overhead</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Existing methods in the literature that are proposed to reduce communication overhead can be categorized into decentralized training, compression, and local updating. Integration of these methods can generate an optimized FL platform with fast convergence time. For instance, infusing redundancy amongst the client dataset was proposed in <cite class="ltx_cite ltx_citemacro_cite">Haddadpour <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> for bringing diversity and reaching convergence in a shorter time. <cite class="ltx_cite ltx_citemacro_citet">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> presented a joint learning framework by quantifying the impact of wireless factors on clients in FL environment. Still, further researches are needed to attain minimal communication overhead at scale.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Guarantee Convergence in Asynchronous Federated Learning</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Most existing studies and implementations consider synchronous FL, where the progress of the iteration round’s training period depends on the slowest device within the network. This means synchronous FL has a direct effect on the overall performance of the system, although it guarantees convergence. In asynchronous FL, the participant can join the training round even in the middle of the training progress. It also ensures scalability although it does not guarantee convergence <cite class="ltx_cite ltx_citemacro_cite">Sprague <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>. Considering the effectiveness of asynchronous FL is one of the core research issues to formulate a method for ensuring convergence during the asynchronous training of clients.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Quantification of Statistical Heterogeneity</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In an IoT environment, the data collected by local devices are inherently non-IID; thus, they may have a discrepancy in terms of the number of samples and dataset structure. It is challenging to quantify the level of heterogeneity in the system before the training begins. A local dissimilarity based strategy was designed in <cite class="ltx_cite ltx_citemacro_cite">Eliazar and
Sokolov (<a href="#bib.bib6" title="" class="ltx_ref">2010</a>)</cite> to quantify heterogeneity, but it cannot quantify heterogeneity before training starts. If we have a mechanism of quantifying heterogeneity at initialization, the system can be configured accordingly to allow more efficient training.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Handling False Data Injection</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In a distributed system, the local devices are responsible for generating their model using the raw data they extracted. But, somehow, if the client constructs its model using false data that are injected during data extraction, then the generated model will cause an erroneous update to the global model. This opens up a new research direction to identify false data efficiently, particularly using resource-constrained devices with the limitations discussed earlier.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>On-device Training</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text" style="color:#000000;">As discussed previously, devices in the IoT domain can have extremely limited capabilities. As a result, it becomes extremely important to do training and inference on the device and, thus, limit the interaction with servers via energy-inefficient communication methods. However, training on a device leads to two problems. First, finding a model with enough capacity that can run on devices having small size of memory, while still capturing the complexity of the data can be hard. </span><cite class="ltx_cite ltx_citemacro_citet">Kumar <span class="ltx_text ltx_font_italic">et al.</span> <span id="S5.SS5.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib16" title="" class="ltx_ref">2017</a><span id="S5.SS5.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.SS5.p1.1.4" class="ltx_text" style="color:#000000;"> and </span><cite class="ltx_cite ltx_citemacro_citet">Thakker <span class="ltx_text ltx_font_italic">et al.</span> <span id="S5.SS5.p1.1.5.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib26" title="" class="ltx_ref">2019</a><span id="S5.SS5.p1.1.6.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.SS5.p1.1.7" class="ltx_text" style="color:#000000;">
solved these problems for inference, but did not discuss training the models on the device. Second, training can require a much larger computational and memory capacity than these simple clients can provide. Section </span><a href="#S4.SS4" title="4.4 Energy Efficient Training of DNNs ‣ 4 Core Challenges to Implement Federated Learning on Resource-Constrained Devices ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4.4</span></a><span id="S5.SS5.p1.1.8" class="ltx_text" style="color:#000000;"> discussed some techniques which either work on specialized neuromorphic or FPGA hardware or do not meet the aggressive constraints found in the IoT domain. Solving this dual problem is paramount. </span><cite class="ltx_cite ltx_citemacro_citet">Thakker <span class="ltx_text ltx_font_italic">et al.</span> <span id="S5.SS5.p1.1.9.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib26" title="" class="ltx_ref">2019</a><span id="S5.SS5.p1.1.10.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.SS5.p1.1.11" class="ltx_text" style="color:#000000;"> provided a potential direction in this regard. They suggested a new network architecture for IoT application to create high capacity models with 15-38x fewer parameters than a traditional model used for those applications. Exploring such new architectures and enabling them to learn over the severely resource-constrained FL environment is an unexplored domain.</span><span id="S5.SS5.p1.1.12" class="ltx_text"></span></p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Managing Dropped Participants</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">In the resource-constrained FL system, clients might have heterogeneous resources, i.e. variable bandwidth, limited battery life and variant transmission power. As a result, any client within a network can be disconnected during the communication with the server. Recent studies widely assumed that all the participants are available and connected with the server throughout the process. But, in real-life, any client can go offline due to the non-availability of resources. Eventually, the disconnection of a significant number of clients can degrade the convergence speed and model accuracy.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="color:#000000;">FL is a recently invented technique and an active ongoing research area. After analyzing the potential challenges of implementing FL in resource-constrained clients in Section <a href="#S4" title="4 Core Challenges to Implement Federated Learning on Resource-Constrained Devices ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and discussing some open issues along with potential solutions in Section <a href="#S5" title="5 Open Issues of Federated Learning Algorithms and Hardware Developments ‣ Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we figure-out some future directions that need to be highlighted. In this section, we point out those future directions.
</span>
<span id="S6.p1.1.2" class="ltx_text" style="color:#000000;"></span></p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><math id="S6.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p2.1.m1.1a"><mo mathcolor="#000000" id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p2.1.1" class="ltx_text" style="color:#000000;"> In the FL environment, some clients may generate more data (i.e., heavier use of an application by a particular user) than other clients within the underlying network of decision-making entities. This may lead to varying amounts of local data during the training period, and we can not represent any client dataset as population distribution. Handing such discrepancy in </span><span id="S6.p2.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">local training dataset</span><span id="S6.p2.1.3" class="ltx_text" style="color:#000000;"> requires further research.</span></p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><math id="S6.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p3.1.m1.1a"><mo mathcolor="#000000" id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><ci id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p3.1.1" class="ltx_text" style="color:#000000;"> To ensure convergence in a non-IID scenario, particularly for asynchronous learning, loss functions of the </span><span id="S6.p3.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">non-convex problem</span><span id="S6.p3.1.3" class="ltx_text" style="color:#000000;"> need to be considered, and supportive algorithms should be proposed.</span></p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p"><math id="S6.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p4.1.m1.1a"><mo mathcolor="#000000" id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><ci id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p4.1.1" class="ltx_text" style="color:#000000;"> In the FL scenario, selection of suitable cluster heads and maintaining coordination within the overall system need more investigation to ensure </span><span id="S6.p4.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">energy efficiency</span><span id="S6.p4.1.3" class="ltx_text" style="color:#000000;">. It may be possible that a cluster of clients agrees to generate an aggregated model using their local data and disseminate it to the central server. The aggregated model can be passed by the leader client, and hence, it can reduce energy consumption. In order to reduce energy consumption, we can consider a group of clients as a cluster in a distributed structure, and select a proficient client that will act as a leader client. That client will be responsible for interaction with the central server in an asynchronous fashion, while the interaction inside each cluster’s agents is conducted in a synchronous fashion. This reduces the energy consumption of the overall system by avoiding unnecessary communication among all agents and the central server.</span></p>
</div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<p id="S6.p5.1" class="ltx_p"><math id="S6.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p5.1.m1.1a"><mo mathcolor="#000000" id="S6.p5.1.m1.1.1" xref="S6.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p5.1.m1.1b"><ci id="S6.p5.1.m1.1.1.cmml" xref="S6.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p5.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p5.1.1" class="ltx_text" style="color:#000000;"> A </span><span id="S6.p5.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">device-centric wake-up mechanism</span><span id="S6.p5.1.3" class="ltx_text" style="color:#000000;"> can be built through which the clients can automatically understand the period to interact with the server. This functionality will help resource-constrained clients, particularly in asynchronous FL, where a client needs to wait for other neighbor clients to send their models to the server. By building such a mechanism, the energy consumption rate of the clients can be reduced by a significant margin.</span></p>
</div>
<div id="S6.p6" class="ltx_para ltx_noindent">
<p id="S6.p6.1" class="ltx_p"><math id="S6.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p6.1.m1.1a"><mo mathcolor="#000000" id="S6.p6.1.m1.1.1" xref="S6.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p6.1.m1.1b"><ci id="S6.p6.1.m1.1.1.cmml" xref="S6.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p6.1.1" class="ltx_text" style="color:#000000;"> The resource-constrained IoT clients may need to perform more interaction with the server due to </span><span id="S6.p6.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">statistical heterogeneity</span><span id="S6.p6.1.3" class="ltx_text" style="color:#000000;"> in the system. So, it is necessary to evaluate an efficient method for identifying the statistical heterogeneity even before the training phase and to avoid idiosyncratic situations due to data sample variation.</span></p>
</div>
<div id="S6.p7" class="ltx_para ltx_noindent">
<p id="S6.p7.1" class="ltx_p"><math id="S6.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p7.1.m1.1a"><mo mathcolor="#000000" id="S6.p7.1.m1.1.1" xref="S6.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p7.1.m1.1b"><ci id="S6.p7.1.m1.1.1.cmml" xref="S6.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p7.1.1" class="ltx_text" style="color:#000000;"> In terms of scalability, frequent drop-out of the participant is a significant bottleneck. A new approach should be adapted to make the FL system more </span><span id="S6.p7.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">robust to frequent drop-outs</span><span id="S6.p7.1.3" class="ltx_text" style="color:#000000;">. One solution could be predicting or identifying the probability of a participant disconnection. Further, a dedicated connection (e.g., cellular connection) can be provided as an incentive to avoid connection drop-out </span><cite class="ltx_cite ltx_citemacro_cite">Lim <span class="ltx_text ltx_font_italic">et al.</span> <span id="S6.p7.1.4.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2019</a><span id="S6.p7.1.5.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S6.p7.1.6" class="ltx_text" style="color:#000000;">. Moreover, a structure can be maintained while designing a protocol so that drop-out participants can try to make a connection multiple times during the long-running model-aggregation. The issue regarding intermittent client availability at scale is not addressed in prior works.</span></p>
</div>
<div id="S6.p8" class="ltx_para ltx_noindent">
<p id="S6.p8.1" class="ltx_p"><math id="S6.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p8.1.m1.1a"><mo mathcolor="#000000" id="S6.p8.1.m1.1.1" xref="S6.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p8.1.m1.1b"><ci id="S6.p8.1.m1.1.1.cmml" xref="S6.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p8.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p8.1.1" class="ltx_text" style="color:#000000;"> Due to </span><span id="S6.p8.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">mobility</span><span id="S6.p8.1.3" class="ltx_text" style="color:#000000;"> of clients, new clients may join a network that is more competent than the existing clients, and any client may leave the network during communication that can hamper the model training. Besides, because of mobility, there may be a large number of clients in some areas while other areas may not have enough clients to generate a feasible model. Handling such situations by considering both mobility and bandwidth ability of clients can be a research direction.</span></p>
</div>
<div id="S6.p9" class="ltx_para ltx_noindent">
<p id="S6.p9.1" class="ltx_p"><math id="S6.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p9.1.m1.1a"><mo mathcolor="#000000" id="S6.p9.1.m1.1.1" xref="S6.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p9.1.m1.1b"><ci id="S6.p9.1.m1.1.1.cmml" xref="S6.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p9.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p9.1.1" class="ltx_text" style="color:#000000;"> The optimal communication degree in the FL implementation is still ongoing research, and there is not yet a deterministic algorithm to identify the </span><span id="S6.p9.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">globally-optimum communication topology</span><span id="S6.p9.1.3" class="ltx_text" style="color:#000000;">. Although divide-and-conquer and one-shot communication methods are discussed in </span><cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> <span id="S6.p9.1.4.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib34" title="" class="ltx_ref">2015</a><span id="S6.p9.1.5.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S6.p9.1.6" class="ltx_text" style="color:#000000;">, these schemes are not well-suited for heterogeneous resource-constrained clients. This specifically leads to challenges related to FL implementation in heterogeneous IoT systems with time-varying communication topology, such as distributed mobile sensor networks. Although one-shot and few-shot FL approaches are proposed in </span><cite class="ltx_cite ltx_citemacro_cite">Guha <span class="ltx_text ltx_font_italic">et al.</span> <span id="S6.p9.1.7.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib8" title="" class="ltx_ref">2019</a><span id="S6.p9.1.8.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S6.p9.1.9" class="ltx_text" style="color:#000000;">, extensive practical evaluation is needed to identify a solution to the optimal communication topology problem. In the context of wireless networks, the distribution of </span><span id="S6.p9.1.10" class="ltx_text ltx_font_bold" style="color:#000000;">fair resources</span><span id="S6.p9.1.11" class="ltx_text" style="color:#000000;"> has been studied extensively. While optimizing the utilities may give us higher throughput, unfair resource allocations may cause inadequate service facilities. The global model can be considered as a resource for providing service to clients. If we use asynchronous FL, then any client may receive a pre-assigned </span><span id="S6.p9.1.12" class="ltx_text ltx_font_bold" style="color:#000000;">fairness</span><span id="S6.p9.1.13" class="ltx_text" style="color:#000000;"> to modify its objective function during the training period. We can handle trade-offs between fairness and other metrics (e.g., average accuracy) by tuning the parameter. Still, more theoretical and practical analysis needs to be conducted to optimize resource distribution, particularly for resource-constrained devices.</span></p>
</div>
<div id="S6.p10" class="ltx_para ltx_noindent">
<p id="S6.p10.1" class="ltx_p"><math id="S6.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p10.1.m1.1a"><mo mathcolor="#000000" id="S6.p10.1.m1.1.1" xref="S6.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p10.1.m1.1b"><ci id="S6.p10.1.m1.1.1.cmml" xref="S6.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p10.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p10.1.1" class="ltx_text" style="color:#000000;"> A </span><span id="S6.p10.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">blockchain</span><span id="S6.p10.1.3" class="ltx_text" style="color:#000000;"> paradigm can be constructed to make the FL clients’ communication more robust and secure. The model update and exchange of resource-constrained IoT clients can be verified by blockchain. </span><cite class="ltx_cite ltx_citemacro_citet">Kim <span class="ltx_text ltx_font_italic">et al.</span> <span id="S6.p10.1.4.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib14" title="" class="ltx_ref">2019</a><span id="S6.p10.1.5.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S6.p10.1.6" class="ltx_text" style="color:#000000;"> and </span><cite class="ltx_cite ltx_citemacro_citet">Xu <span class="ltx_text ltx_font_italic">et al.</span> <span id="S6.p10.1.7.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib31" title="" class="ltx_ref">2020</a><span id="S6.p10.1.8.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S6.p10.1.9" class="ltx_text" style="color:#000000;"> proposed a blockchain-based on-device FL, but they did not consider resource-constrained clients. How the resource-constrained IoT clients perform block traversing, select miners as well as leader-client, ensure atomicity, and reach a consensus for FL scenario is a future direction.</span></p>
</div>
<div id="S6.p11" class="ltx_para ltx_noindent">
<p id="S6.p11.1" class="ltx_p"><math id="S6.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p11.1.m1.1a"><mo mathcolor="#000000" id="S6.p11.1.m1.1.1" xref="S6.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p11.1.m1.1b"><ci id="S6.p11.1.m1.1.1.cmml" xref="S6.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p11.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p11.1.1" class="ltx_text" style="color:#000000;"> Designing an </span><span id="S6.p11.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">incentive mechanism for transparent participation</span><span id="S6.p11.1.3" class="ltx_text" style="color:#000000;"> is required in FL. As participants may be resource-bounded or business competitors, it is essential to develop an approach that effectively divides the earnings in order to enhance the long-term participation of the clients. Furthermore, how to defend against adversarial client data owners and optimize the non-adversarial owner participation for ensuring security needs to be explored.</span></p>
</div>
<div id="S6.p12" class="ltx_para ltx_noindent">
<p id="S6.p12.1" class="ltx_p"><math id="S6.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.p12.1.m1.1a"><mo mathcolor="#000000" id="S6.p12.1.m1.1.1" xref="S6.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.p12.1.m1.1b"><ci id="S6.p12.1.m1.1.1.cmml" xref="S6.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p12.1.m1.1c">\bullet</annotation></semantics></math><span id="S6.p12.1.1" class="ltx_text" style="color:#000000;"> The solitary computational model of FL can lead us to build a more refined trust-based model. As it is challenging in terms of </span><span id="S6.p12.1.2" class="ltx_text ltx_font_bold" style="color:#000000;">security</span><span id="S6.p12.1.3" class="ltx_text" style="color:#000000;"> to select a participant for the training phase, a </span><span id="S6.p12.1.4" class="ltx_text ltx_font_bold" style="color:#000000;">trust-based model</span><span id="S6.p12.1.5" class="ltx_text" style="color:#000000;"> can reduce extra communication overhead. Typically, we assume that the server is operated by a non-adversarial entity. This server can analyze the behavior of participants and leverage a trust model. According to the trust score, an incentive mechanism can be designed, and that trust model can assist us when the number of participants is significant. This opens up a new research direction to explore.</span></p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we conducted a comprehensive survey on FL, particularly for resource-constrained IoT devices, and highlighted the ongoing research related to this area. We began by discussing the importance of leveraging FL for resource-constrained IoT clients and discussed some previous works that considered resource scarcity during the implementation of FL system. We highlighted the background, including the working procedure of FL, and explored existing FL applications by discussing their importance in conducting model training locally. Further, we focused on core challenges of implementing FL, particularly for resource-constrained devices by considering hardware limitation, communication expense, client behavior, statistical data variation and labeling, and energy-efficient training. Finally, we emphasized the need for future directions for contriving new FL algorithms in terms of currently open issues and designing the latest hardware considering resource-constrained challenges, especially in an FL scenario.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Eichner, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.01046</span>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Fei Chen, Zhenhua Dong, et al.

</span>
<span class="ltx_bibblock">Federated meta-learning for recommendation.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">arXiv:1802.07876</span>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Mingzhe Chen, Zhaohui Yang, et al.

</span>
<span class="ltx_bibblock">A joint learning and communications framework for federated learning
over wireless networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">arXiv:1909.07972</span>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui <span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Laizhong Cui, Shu Yang, et al.

</span>
<span class="ltx_bibblock">A survey on application of machine learning for internet of things.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">J. M. L. Cybernetics</span>, 9(8):1399–1417, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das and Brunschwiler [2019]</span>
<span class="ltx_bibblock">
Anirban Das and Thomas Brunschwiler.

</span>
<span class="ltx_bibblock">Privacy is what we care about: Experimental investigation of
federated learning on edge devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">AIChallengeIoT</span>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eliazar and
Sokolov [2010]</span>
<span class="ltx_bibblock">
Iddo I Eliazar and Igor M Sokolov.

</span>
<span class="ltx_bibblock">Measuring statistical heterogeneity: The pietra index.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Physica A: Stat. Mech. App.</span>, 389(1):117–125, 2010.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu <span id="bib.bib7.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Zhongshu Gu, Hani Jamjoom, et al.

</span>
<span class="ltx_bibblock">Reaching data confidentiality and model accountability on the
caltrain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.3.1" class="ltx_text ltx_font_italic">IEEE DSN</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Neel Guha, Ameet Talwlkar, et al.

</span>
<span class="ltx_bibblock">One-shot federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">arXiv:1902.11175</span>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddadpour <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Farzin Haddadpour, Mohammad Mahdi Kamani, et al.

</span>
<span class="ltx_bibblock">Trading redundancy for communication: Speeding up distributed sgd for
non-convex optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">ICML</span>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Ramaswamy, et al.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">arXiv:1811.03604</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and others [2013]</span>
<span class="ltx_bibblock">
Junxian Huang et al.

</span>
<span class="ltx_bibblock">An in-depth study of lte: effect of network protocol and application
behavior on performance.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">ACM SIGCOMM CCR</span>, 43(4):363–374, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Li Huang, Yifeng Yin, et al.

</span>
<span class="ltx_bibblock">Loadaboost: Loss-based adaboost federated machine learning on medical
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">arXiv:1811.12629</span>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Yuang Jiang, Shiqiang Wang, et al.

</span>
<span class="ltx_bibblock">Model pruning enables efficient federated learning on edge devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">arXiv:1909.12326</span>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, et al.

</span>
<span class="ltx_bibblock">Blockchained on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">IEEE Communications Letters</span>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Ashish Kumar, Saurabh Goyal, et al.

</span>
<span class="ltx_bibblock">Resource-efficient machine learning in 2 kb ram for the internet of
things.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">ICML</span>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy and others [2019]</span>
<span class="ltx_bibblock">
David Leroy et al.

</span>
<span class="ltx_bibblock">Federated learning for keyword spotting.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE ICASSP</span>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib18.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
He Li, Kaoru Ota, et al.

</span>
<span class="ltx_bibblock">Learning iot in edge: Deep learning for the internet of things with
edge computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">IEEE network</span>, 32(1):96–101, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib19.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, et al.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">arXiv:1908.07873</span>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim <span id="bib.bib20.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, et al.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">arXiv:1909.11875</span>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Chenxin Ma, Jakub Konečnỳ, et al.

</span>
<span class="ltx_bibblock">Distributed optimization with arbitrary local solvers.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">Optimization Methods and Software</span>, 32(4):813–848, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider Moore, et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">arXiv:1602.05629</span>, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jihong Park, Shiqiang Wang, et al.

</span>
<span class="ltx_bibblock">Distilling on-device intelligence at the network edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">arXiv:1908.05895</span>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pete Warden [2018]</span>
<span class="ltx_bibblock">
What does it take to train deep learning models on-device?, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sprague <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Michael R Sprague, Amir Jalalirad, et al.

</span>
<span class="ltx_bibblock">Asynchronous federated learning for geospatial applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">ECML-PKDD</span>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakker <span id="bib.bib26.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Urmish Thakker, Jesse Beu, et al.

</span>
<span class="ltx_bibblock">Compressing rnns for iot devices by 15-38x using kronecker products.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.02876</span>, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Shiqiang Wang, Tiffany Tuor, et al.

</span>
<span class="ltx_bibblock">Adaptive federated learning in resource constrained edge computing
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">IEEE JSAC</span>, 37(6):1205–1221, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu <span id="bib.bib28.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Shuang Wu, Guoqi Li, et al.

</span>
<span class="ltx_bibblock">Training and inference with integers in deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">arXiv:1802.04680</span>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib29.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019a]</span>
<span class="ltx_bibblock">
Zichen Xu, Li Li, et al.

</span>
<span class="ltx_bibblock">Exploring federated learning on battery-powered devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">ACM TURC</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib30.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019b]</span>
<span class="ltx_bibblock">
Zirui Xu, Zhao Yang, et al.

</span>
<span class="ltx_bibblock">Elfish: Resource-aware federated learning on heterogeneous edge
devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">arXiv:1912.01684</span>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib31.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Ronghua Xu, Yu Chen, and Jian Li.

</span>
<span class="ltx_bibblock">MicroFL: A lightweight, secure-by-design edge network fabric for
decentralized IoT systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">NDSS</span>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib32.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, et al.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic">arXiv:1812.02903</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib33.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, et al.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic">ACM Trans. on TIST</span>, 10(2):12, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span id="bib.bib34.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Yuchen Zhang, John Duchi, et al.

</span>
<span class="ltx_bibblock">Divide and conquer kernel ridge regression: A distributed algorithm
with minimax optimal rates.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic">JMLR</span>, 16(1):3299–3340, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2002.10609" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2002.10610" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2002.10610">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2002.10610" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2002.10611" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 12:14:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
