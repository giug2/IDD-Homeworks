<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.00334] MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows</title><meta property="og:description" content="A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.00334">

<!--Generated on Tue Feb 27 20:12:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MetisFL: An Embarrassingly Parallelized Controller for 
<br class="ltx_break">Scalable &amp; Efficient Federated Learning Workflows</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Dimitris Stripelis  </span><span id="id2.2.id2" class="ltx_text ltx_font_italic" style="font-size:90%;">stripeli@isi.edu 
<br class="ltx_break">Information Science Institute
<br class="ltx_break">University of Southern California
</span><span id="id3.3.id3" class="ltx_text ltx_font_bold">Chrysovalantis Anastasiou  </span><span id="id4.4.id4" class="ltx_text ltx_font_italic" style="font-size:90%;">canastas@usc.edu 
<br class="ltx_break">Viterbi School of Engineering
<br class="ltx_break">University of Southern California
</span><span id="id5.5.id5" class="ltx_text ltx_font_bold">Patrick Toral  </span><span id="id6.6.id6" class="ltx_text ltx_font_italic" style="font-size:90%;">pjtoral@isi.edu
<br class="ltx_break">Information Science Institute
<br class="ltx_break">University of Southern California
</span><span id="id7.7.id7" class="ltx_text ltx_font_bold">Armaghan Asghar  </span><span id="id8.8.id8" class="ltx_text ltx_font_italic" style="font-size:90%;">asghar@usc.edu
<br class="ltx_break">Viterbi School of Engineering
<br class="ltx_break">University of Southern California
</span><span id="id9.9.id9" class="ltx_text ltx_font_bold">José Luis Ambite  </span><span id="id10.10.id10" class="ltx_text ltx_font_italic" style="font-size:90%;">ambite@isi.edu
<br class="ltx_break">Information Science Institute
<br class="ltx_break">University of Southern California
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations conducted by the federation controller to accelerate the training of large-scale FL workflows. By quantitatively comparing MetisFL against other state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a 10-fold wall-clock time execution boost across a wide range of challenging FL workflows with increasing model sizes and federation sites.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.00334/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="48" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A typical Federated Learning workflow. The red color represents operations executed by the controller. The green color represents operations executed by the learner(s).</figcaption>
</figure>
<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) has emerged as a standard distributed learning approach for training machine and deep learning models across dispersed data sources that cannot share data due to regulatory or privacy concerns. During the execution of an FL workflow, data always remain at the source, and sources only share their locally trained model parameters. Even though this approach mitigates the problem of direct data leakage, it creates many new system challenges related to the design, development, and deployment of FL workflows. Recently, several open-source FL frameworks have become available to meet this need, including but not limited to Nvidia FLARE <cite class="ltx_cite ltx_citemacro_cite">Roth et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, Flower <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, IBM FL <cite class="ltx_cite ltx_citemacro_cite">Ludwig et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, OpenFL <cite class="ltx_cite ltx_citemacro_cite">Reina et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, LEAF <cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, and TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Google </a></cite> (TFF). Some systems (e.g., Flower, TFF) make it easier to adapt the execution of centralized ML models into federated settings and implement new FL algorithms, while others (e.g., NVFlare, OpenFL, IBM FL) facilitate the transition of existing ML/DL model training pipelines into production environments.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Even though each system offers unique solutions and optimizations, one processing entity is often overlooked: the federation controller/aggregator. Typically, a centralized FL environment <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In our work, we only focus on centralized environments, even though other topologies exist as well <cite class="ltx_cite ltx_citemacro_cite">Rieke et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, such as decentralized and hierarchical.</span></span></span> consists of a single controller and a set of participating clients/learners. Even though the controller has no ownership over the computational resources or the data of the participating learners in the federation, it is still responsible for managing the execution of the FL workflows across learners, scheduling and dispatching the local training tasks, and receiving, storing, and aggregating learners’ model parameters. When conducting large-scale FL workflows with hundreds of learners and very large models, the controller must efficiently manage all available resources and perform all required operations with the minimum cost. On these grounds, we consider the controller the primary bottleneck of any FL system when handling the orchestration of large-scale workflows.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we present the sequence of these operations for a typical FL workflow. At Timestamp T1, the controller receives an update request from a participating learner(s). Timestamps T2-T4 represent learner(s)’ local model training and local model parameter transfer, and timestamps T4-T7 represent the reception, storing, selection, and aggregation of learner(s)’ locally trained model parameters by the controller. Finally, timestamps T7-T9 represent the scheduling and dispatching of the newly computed federated model to the learners for evaluation. All operations occurring between timestamps T1-T9 represent the federation round or community update request for synchronous and asynchronous settings <cite class="ltx_cite ltx_citemacro_cite">Stripelis et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022b</a>)</cite>, respectively. Timestamps T1-T4 represent the training round for a single (asynchronous) or multiple learners (synchronous), timestamps T4-T7 local model consolidation and aggregation, and timestamps T7-T9 the evaluation round for a single (asynchronous) or multiple learners (synchronous).</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">To this end, we designed and developed from scratch a novel FL system, called <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">MetisFL</span>, where the federation controller is the first-class citizen of the system. To boost the performance of the controller’s operations, we re-engineered the controller in C++ and optimized weight tensor processing and network transmission. MetisFL achieves a 10-fold to 100-fold wall-clock time execution improvement against other leading FL systems. MetisFL is the first FL system that accelerates the training of FL workflow by optimizing the controller’s operations. Our study is the first to stress-test these operations across different FL systems.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning Systems</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Various FL systems have been recently introduced <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>); Caldas et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>); <a href="#bib.bib7" title="" class="ltx_ref">Google </a>; Roth et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>); Reina et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Ludwig et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, with each one aiming to optimize different aspects of a particular FL workload.
FedML <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> is an FL framework that supports standalone simulation, distributed training, cross-silo, and cross-device FL applications. FedML uses PyTorch as its core training engine and supports communication protocols like NCCL, MPI, and MQTT. Flower <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> is an open-source FL framework designed to be an extensible and scalable federated ML/DL model agnostic framework targeted for heterogeneous client environments for simulation and real-world experiments. It supports a wide range of machine and deep learning computing engines (e.g., TensorFlow, PyTorch, JAX) and uses gRPC for communication between participating clients. NVIDIA Flare (NVFlare) <cite class="ltx_cite ltx_citemacro_cite">Roth et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, OpenFL <cite class="ltx_cite ltx_citemacro_cite">Reina et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, and IBM FL <cite class="ltx_cite ltx_citemacro_cite">Ludwig et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> were designed primarily for production-oriented environments. NVFlare supports a high-availability controller with fail-over capabilities and provides a domain-agnostic FL SDK for implementing a wide range of FL applications. OpenFL <cite class="ltx_cite ltx_citemacro_cite">Reina et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> originated from the collaboration of Intel Labs and the University of Pennsylvania as part of a research project for FL applications in healthcare. The framework has now become domain-agnostic, supporting various use cases. At its core, OpenFL defines an FL plan with all the configurations necessary to execute an FL workload across learners. IBM FL’s key design is its fast start-up time for enterprise applications. IBM FL aims to minimize the learning curve for a data scientist to migrate existing centralized machine learning workloads into federated settings by enabling the design of custom fusion (aggregation) algorithms and providing easy deployment of workloads across computing environments.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">When it comes to systematically comparing the various FL systems, previous studies have focused on the qualitative aspects of existing FL systems and have taxonomized them based on functional capabilities and architectural designs <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021b</a>); Liu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022a</a>); Kairouz et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. Other studies <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022b</a>); Hu et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> have focused on quantitatively benchmarking these systems by executing FL workloads that include challenging ML/DL model architectures and non-IID dataset distributions but whose aim was to measure the communication and computational overhead required to reach particular FL model learning performances. In our work, we take inspiration from these studies and perform a qualitative comparison between MetisFL and other leading frameworks, as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 MetisFL: Design &amp; Architecture ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. However, our quantitative analysis does not focus on traditional measures of the FL model’s performance (e.g., model accuracy, communication, and computation cost). Instead, we conduct an end-to-end stress test analysis across frameworks to quantify the system scalability occurring from the operations performed at the controller, which is often a major scalability bottleneck in FL systems.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MetisFL: Design &amp; Architecture</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">MetisFL <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/bioint/MetisFL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/bioint/MetisFL</a></span></span></span> is designed from the bottom up to comply with the architectural principles of modularity (implementation of micro-services), extensibility (expanding system functionalities), and configurability (ease of use) <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>. MetisFL is domain agnostic and can be used to conduct various FL workflows across organizations (a.k.a. cross-silo) or devices (a.k.a. cross-device) <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>. Figure <a href="#S3.F2" title="Figure 2 ‣ 3 MetisFL: Design &amp; Architecture ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a detailed overview of the MetisFL internal architecture. The framework consists of three core components, the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Federation Driver</span>, the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">Federation Controller</span>, and the <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">Federation Learner</span>, similar to the programming model of the Apache Spark framework <cite class="ltx_cite ltx_citemacro_cite">Zaharia et al. (<a href="#bib.bib27" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.00334/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>MetisFL Internal Components.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 MetisFL: Design &amp; Architecture ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, when a system user needs to run an FL workload, he/she needs to wrap the original Keras/PyTorch model architecture around the MetisFL model abstraction and materializing specific functions that the system can use to run the model operations (e.g., fit and evaluate functions for PyTorch models). Subsequently, the user needs to specify how the data will be loaded in the model (data recipe) and the federated environment (yaml file) that contains all the model (e.g., optimizer) and running environment (e.g., host machines) configurations. Once all this information is defined, the Federation Driver parses the FL workflow and creates the MetisFL Context. The MetisFL Context is responsible for initializing and monitoring the federation cluster, initializing the original model state, shipping the model state to each learner, defining the data loading recipe for each learner, and generating the security keys where needed, e.g., SSL certificates (see also Figure <a href="#A2.F11" title="Figure 11 ‣ Secure Sockets Layer ‣ Appendix B MetisFL Internal Procedures ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> in Appendix) and/or FHE key pair <cite class="ltx_cite ltx_citemacro_cite">Stripelis et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>; <a href="#bib.bib24" title="" class="ltx_ref">2022a</a>)</cite>. When all required services are alive, the Federation Controller manages the federation cluster by selecting and delegating training and evaluating tasks that need to be performed by the Federation Learners (cluster nodes) over their private datasets and storing and aggregating the learners’ local models (w/ or w/out encryption) when local training is complete.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2311.00334/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>MetisFL Workflow.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The MetisFL prototype was initially proposed in the work of <cite class="ltx_cite ltx_citemacro_cite">Stripelis et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022b</a>)</cite>. However, since then, substantial improvements have been made in code execution and parallelism. In its original implementation, the MetisFL controller was developed in Python. However, due to Python’s limited memory management capabilities, the Python version led to a high latency when aggregating large-sized models and/or aggregating models from a large pool of participating clients (e.g., <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.p3.1.m1.1a"><mo id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><gt id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">&gt;</annotation></semantics></math> 100). Moreover, in its original implementation, as more clients were considered in the federation and the size of the ML/DL models became larger, the training and evaluation tasks scheduling and dispatching became extremely slow. Given that Python relies on the Global Interpreter Lock (GIL) for proper thread management, the concurrent execution of tasks was not possible, dramatically slowing down the overall execution time of an FL workflow. For these reasons, the MetisFL architecture was redesigned and the controller was refactored in a native C++ implementation.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2311.00334/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>MetisFL Parallelized Model Aggregation.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">In terms of redesign, compared to other systems (e.g., Flower, FedML), in MetisFL, the ML/DL model is transferred over the network as a sequence of tensors with each tensor being represented in a byte protobuf data type. This allows different gRPC services (i.e., controller-learners) to communicate with each other with very low overhead. This functionality is accomplished by first flattening each tensor/matrix of the ML/DL model, then dumping the tensor (as bytes), and finally constructing a proto message that represents the structure of the original tensor to enable reconstruction after proto tensor reception e.g., tensor’s byte order and data type.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.2" class="ltx_p">With respect to the controller’s process refactoring, the controller can now leverage the underlying system’s hardware capabilities and parallelize most of its computationally heavy operations, i.e., task scheduling and dispatching and model aggregation. Especially in the case of model aggregation, by taking advantage of the tensor-based model representation, the controller parallelizes the aggregation of multiple model tensors over all available cores to speed up the operation. As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3 MetisFL: Design &amp; Architecture ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, for a federation of N learners and a model with k tensors, MetisFL uses one thread per model tensor to compute the aggregated tensor. For instance, to calculate the aggregated tensor for the first model tensor (<math id="S3.p5.1.m1.1" class="ltx_Math" alttext="T_{1}^{C}" display="inline"><semantics id="S3.p5.1.m1.1a"><msubsup id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2.2" xref="S3.p5.1.m1.1.1.2.2.cmml">T</mi><mn id="S3.p5.1.m1.1.1.2.3" xref="S3.p5.1.m1.1.1.2.3.cmml">1</mn><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">C</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1">superscript</csymbol><apply id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.2.1.cmml" xref="S3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.2.2.cmml" xref="S3.p5.1.m1.1.1.2.2">𝑇</ci><cn type="integer" id="S3.p5.1.m1.1.1.2.3.cmml" xref="S3.p5.1.m1.1.1.2.3">1</cn></apply><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">T_{1}^{C}</annotation></semantics></math>), a single thread is used to aggregate the N learners’ tensors (i.e., tensor <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="T_{1}^{1}...T_{1}^{N}" display="inline"><semantics id="S3.p5.2.m2.1a"><mrow id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><msubsup id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml"><mi id="S3.p5.2.m2.1.1.2.2.2" xref="S3.p5.2.m2.1.1.2.2.2.cmml">T</mi><mn id="S3.p5.2.m2.1.1.2.2.3" xref="S3.p5.2.m2.1.1.2.2.3.cmml">1</mn><mn id="S3.p5.2.m2.1.1.2.3" xref="S3.p5.2.m2.1.1.2.3.cmml">1</mn></msubsup><mo lspace="0em" rspace="0em" id="S3.p5.2.m2.1.1.1" xref="S3.p5.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.p5.2.m2.1.1.1a" xref="S3.p5.2.m2.1.1.1.cmml">​</mo><msubsup id="S3.p5.2.m2.1.1.4" xref="S3.p5.2.m2.1.1.4.cmml"><mi id="S3.p5.2.m2.1.1.4.2.2" xref="S3.p5.2.m2.1.1.4.2.2.cmml">T</mi><mn id="S3.p5.2.m2.1.1.4.2.3" xref="S3.p5.2.m2.1.1.4.2.3.cmml">1</mn><mi id="S3.p5.2.m2.1.1.4.3" xref="S3.p5.2.m2.1.1.4.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><times id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1.1"></times><apply id="S3.p5.2.m2.1.1.2.cmml" xref="S3.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.2.1.cmml" xref="S3.p5.2.m2.1.1.2">superscript</csymbol><apply id="S3.p5.2.m2.1.1.2.2.cmml" xref="S3.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.2.2.1.cmml" xref="S3.p5.2.m2.1.1.2">subscript</csymbol><ci id="S3.p5.2.m2.1.1.2.2.2.cmml" xref="S3.p5.2.m2.1.1.2.2.2">𝑇</ci><cn type="integer" id="S3.p5.2.m2.1.1.2.2.3.cmml" xref="S3.p5.2.m2.1.1.2.2.3">1</cn></apply><cn type="integer" id="S3.p5.2.m2.1.1.2.3.cmml" xref="S3.p5.2.m2.1.1.2.3">1</cn></apply><ci id="S3.p5.2.m2.1.1.3.cmml" xref="S3.p5.2.m2.1.1.3">…</ci><apply id="S3.p5.2.m2.1.1.4.cmml" xref="S3.p5.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.4.1.cmml" xref="S3.p5.2.m2.1.1.4">superscript</csymbol><apply id="S3.p5.2.m2.1.1.4.2.cmml" xref="S3.p5.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.4.2.1.cmml" xref="S3.p5.2.m2.1.1.4">subscript</csymbol><ci id="S3.p5.2.m2.1.1.4.2.2.cmml" xref="S3.p5.2.m2.1.1.4.2.2">𝑇</ci><cn type="integer" id="S3.p5.2.m2.1.1.4.2.3.cmml" xref="S3.p5.2.m2.1.1.4.2.3">1</cn></apply><ci id="S3.p5.2.m2.1.1.4.3.cmml" xref="S3.p5.2.m2.1.1.4.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">T_{1}^{1}...T_{1}^{N}</annotation></semantics></math>); thread parallelism is enabled using OpenMP <cite class="ltx_cite ltx_citemacro_cite">Dagum &amp; Menon (<a href="#bib.bib6" title="" class="ltx_ref">1998</a>)</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.46" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.46.47.1" class="ltx_tr">
<td id="S3.T1.46.47.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.46.47.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dimension</span></td>
<td id="S3.T1.46.47.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.2.1" class="ltx_text" style="font-size:70%;">Nvidia FLARE</span></td>
<td id="S3.T1.46.47.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.3.1" class="ltx_text" style="font-size:70%;">Flower</span></td>
<td id="S3.T1.46.47.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.4.1" class="ltx_text" style="font-size:70%;">FedML</span></td>
<td id="S3.T1.46.47.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.5.1" class="ltx_text" style="font-size:70%;">IBM FL</span></td>
<td id="S3.T1.46.47.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.6.1" class="ltx_text" style="font-size:70%;">OpenFL</span></td>
<td id="S3.T1.46.47.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S3.T1.46.47.1.7.1" class="ltx_text" style="font-size:70%;">MetisFL*</span></td>
</tr>
<tr id="S3.T1.46.48.2" class="ltx_tr">
<td id="S3.T1.46.48.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.46.48.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Deployment</span></td>
<td id="S3.T1.46.48.2.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.46.48.2.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.46.48.2.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.46.48.2.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.46.48.2.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.46.48.2.7" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T1.46.49.3" class="ltx_tr">
<td id="S3.T1.46.49.3.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.49.3.1.1" class="ltx_text" style="font-size:70%;">Standalone</span></td>
<td id="S3.T1.46.49.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.49.3.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.49.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.49.3.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.49.3.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.49.3.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.49.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.49.3.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.49.3.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.49.3.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.49.3.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.49.3.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.50.4" class="ltx_tr">
<td id="S3.T1.46.50.4.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.50.4.1.1" class="ltx_text" style="font-size:70%;">Distributed</span></td>
<td id="S3.T1.46.50.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.50.4.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.50.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.50.4.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.50.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.50.4.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.50.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.50.4.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.50.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.50.4.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.50.4.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.50.4.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.51.5" class="ltx_tr">
<td id="S3.T1.46.51.5.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.51.5.1.1" class="ltx_text" style="font-size:70%;">Cross-Silo</span></td>
<td id="S3.T1.46.51.5.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.51.5.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.51.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.51.5.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.51.5.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.51.5.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.51.5.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.51.5.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.51.5.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.51.5.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.51.5.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.51.5.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.4.1" class="ltx_text" style="font-size:70%;">Cross-Device</span></td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center"><math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo mathsize="70%" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><times id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.3.3.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.3.3.6" class="ltx_td ltx_align_center"><span id="S3.T1.3.3.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center"><math id="S3.T1.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.2.2.2.m1.1a"><mo mathsize="70%" id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><times id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center"><math id="S3.T1.3.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.3.3.3.m1.1a"><mo mathsize="70%" id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><times id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.3.3.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_left"><span id="S3.T1.4.4.2.1" class="ltx_text" style="font-size:70%;">Containerized</span></td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.4.4.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.4.4.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.4.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.4.4.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.4.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.4.4.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_center"><math id="S3.T1.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.4.4.1.m1.1a"><mo mathsize="70%" id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><times id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.4.4.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.52.6" class="ltx_tr">
<td id="S3.T1.46.52.6.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.52.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">ML Environment</span></td>
<td id="S3.T1.46.52.6.2" class="ltx_td"></td>
<td id="S3.T1.46.52.6.3" class="ltx_td"></td>
<td id="S3.T1.46.52.6.4" class="ltx_td"></td>
<td id="S3.T1.46.52.6.5" class="ltx_td"></td>
<td id="S3.T1.46.52.6.6" class="ltx_td"></td>
<td id="S3.T1.46.52.6.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<td id="S3.T1.10.10.7" class="ltx_td ltx_align_left"><span id="S3.T1.10.10.7.1" class="ltx_text" style="font-size:70%;">Model Types</span></td>
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_center">
<span id="S3.T1.5.5.1.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.5.5.1.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.5.5.1.m1.1a"><mo mathsize="70%" id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><ci id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.5.5.1.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
<td id="S3.T1.6.6.2" class="ltx_td ltx_align_center">
<span id="S3.T1.6.6.2.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.6.6.2.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.6.6.2.m1.1a"><mo mathsize="70%" id="S3.T1.6.6.2.m1.1.1" xref="S3.T1.6.6.2.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.m1.1b"><ci id="S3.T1.6.6.2.m1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.6.6.2.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
<td id="S3.T1.7.7.3" class="ltx_td ltx_align_center">
<span id="S3.T1.7.7.3.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.7.7.3.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.7.7.3.m1.1a"><mo mathsize="70%" id="S3.T1.7.7.3.m1.1.1" xref="S3.T1.7.7.3.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.3.m1.1b"><ci id="S3.T1.7.7.3.m1.1.1.cmml" xref="S3.T1.7.7.3.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.3.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.7.7.3.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
<td id="S3.T1.8.8.4" class="ltx_td ltx_align_center">
<span id="S3.T1.8.8.4.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.8.8.4.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.8.8.4.m1.1a"><mo mathsize="70%" id="S3.T1.8.8.4.m1.1.1" xref="S3.T1.8.8.4.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.4.m1.1b"><ci id="S3.T1.8.8.4.m1.1.1.cmml" xref="S3.T1.8.8.4.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.4.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.8.8.4.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
<td id="S3.T1.9.9.5" class="ltx_td ltx_align_center">
<span id="S3.T1.9.9.5.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.9.9.5.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.9.9.5.m1.1a"><mo mathsize="70%" id="S3.T1.9.9.5.m1.1.1" xref="S3.T1.9.9.5.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.5.m1.1b"><ci id="S3.T1.9.9.5.m1.1.1.cmml" xref="S3.T1.9.9.5.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.5.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.9.9.5.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
<td id="S3.T1.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S3.T1.10.10.6.1" class="ltx_text" style="font-size:70%;">ML</span><math id="S3.T1.10.10.6.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.10.10.6.m1.1a"><mo mathsize="70%" id="S3.T1.10.10.6.m1.1.1" xref="S3.T1.10.10.6.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.6.m1.1b"><ci id="S3.T1.10.10.6.m1.1.1.cmml" xref="S3.T1.10.10.6.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.6.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.10.10.6.2" class="ltx_text" style="font-size:70%;">DL</span>
</td>
</tr>
<tr id="S3.T1.21.21" class="ltx_tr">
<td id="S3.T1.21.21.12" class="ltx_td ltx_align_left"><span id="S3.T1.21.21.12.1" class="ltx_text" style="font-size:70%;">Backend</span></td>
<td id="S3.T1.12.12.2" class="ltx_td ltx_align_center">
<span id="S3.T1.12.12.2.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.11.11.1.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.11.11.1.m1.1a"><mo mathsize="70%" id="S3.T1.11.11.1.m1.1.1" xref="S3.T1.11.11.1.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.1.m1.1b"><ci id="S3.T1.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.1.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.12.12.2.2" class="ltx_text" style="font-size:70%;">TF</span><math id="S3.T1.12.12.2.m2.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.12.12.2.m2.1a"><mo mathsize="70%" id="S3.T1.12.12.2.m2.1.1" xref="S3.T1.12.12.2.m2.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.2.m2.1b"><ci id="S3.T1.12.12.2.m2.1.1.cmml" xref="S3.T1.12.12.2.m2.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.2.m2.1c">\mid</annotation></semantics></math><span id="S3.T1.12.12.2.3" class="ltx_text" style="font-size:70%;">MONAI</span>
</td>
<td id="S3.T1.15.15.5" class="ltx_td ltx_align_center">
<span id="S3.T1.15.15.5.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.13.13.3.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.13.13.3.m1.1a"><mo mathsize="70%" id="S3.T1.13.13.3.m1.1.1" xref="S3.T1.13.13.3.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.3.m1.1b"><ci id="S3.T1.13.13.3.m1.1.1.cmml" xref="S3.T1.13.13.3.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.3.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.15.15.5.2" class="ltx_text" style="font-size:70%;">TF</span><math id="S3.T1.14.14.4.m2.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.14.14.4.m2.1a"><mo mathsize="70%" id="S3.T1.14.14.4.m2.1.1" xref="S3.T1.14.14.4.m2.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.4.m2.1b"><ci id="S3.T1.14.14.4.m2.1.1.cmml" xref="S3.T1.14.14.4.m2.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.4.m2.1c">\mid</annotation></semantics></math><span id="S3.T1.15.15.5.3" class="ltx_text" style="font-size:70%;">MX</span><math id="S3.T1.15.15.5.m3.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.15.15.5.m3.1a"><mo mathsize="70%" id="S3.T1.15.15.5.m3.1.1" xref="S3.T1.15.15.5.m3.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.5.m3.1b"><ci id="S3.T1.15.15.5.m3.1.1.cmml" xref="S3.T1.15.15.5.m3.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.5.m3.1c">\mid</annotation></semantics></math><span id="S3.T1.15.15.5.4" class="ltx_text" style="font-size:70%;">JAX</span>
</td>
<td id="S3.T1.18.18.8" class="ltx_td ltx_align_center">
<span id="S3.T1.18.18.8.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.16.16.6.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.16.16.6.m1.1a"><mo mathsize="70%" id="S3.T1.16.16.6.m1.1.1" xref="S3.T1.16.16.6.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.16.16.6.m1.1b"><ci id="S3.T1.16.16.6.m1.1.1.cmml" xref="S3.T1.16.16.6.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.16.6.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.18.18.8.2" class="ltx_text" style="font-size:70%;">TF</span><math id="S3.T1.17.17.7.m2.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.17.17.7.m2.1a"><mo mathsize="70%" id="S3.T1.17.17.7.m2.1.1" xref="S3.T1.17.17.7.m2.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.17.17.7.m2.1b"><ci id="S3.T1.17.17.7.m2.1.1.cmml" xref="S3.T1.17.17.7.m2.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.17.7.m2.1c">\mid</annotation></semantics></math><span id="S3.T1.18.18.8.3" class="ltx_text" style="font-size:70%;">MX</span><math id="S3.T1.18.18.8.m3.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.18.18.8.m3.1a"><mo mathsize="70%" id="S3.T1.18.18.8.m3.1.1" xref="S3.T1.18.18.8.m3.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.18.18.8.m3.1b"><ci id="S3.T1.18.18.8.m3.1.1.cmml" xref="S3.T1.18.18.8.m3.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.18.8.m3.1c">\mid</annotation></semantics></math><span id="S3.T1.18.18.8.4" class="ltx_text" style="font-size:70%;">JAX</span>
</td>
<td id="S3.T1.19.19.9" class="ltx_td ltx_align_center">
<span id="S3.T1.19.19.9.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.19.19.9.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.19.19.9.m1.1a"><mo mathsize="70%" id="S3.T1.19.19.9.m1.1.1" xref="S3.T1.19.19.9.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.19.19.9.m1.1b"><ci id="S3.T1.19.19.9.m1.1.1.cmml" xref="S3.T1.19.19.9.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.19.9.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.19.19.9.2" class="ltx_text" style="font-size:70%;">TF</span>
</td>
<td id="S3.T1.20.20.10" class="ltx_td ltx_align_center">
<span id="S3.T1.20.20.10.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.20.20.10.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.20.20.10.m1.1a"><mo mathsize="70%" id="S3.T1.20.20.10.m1.1.1" xref="S3.T1.20.20.10.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.20.20.10.m1.1b"><ci id="S3.T1.20.20.10.m1.1.1.cmml" xref="S3.T1.20.20.10.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.20.10.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.20.20.10.2" class="ltx_text" style="font-size:70%;">TF</span>
</td>
<td id="S3.T1.21.21.11" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S3.T1.21.21.11.1" class="ltx_text" style="font-size:70%;">Torch</span><math id="S3.T1.21.21.11.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.21.21.11.m1.1a"><mo mathsize="70%" id="S3.T1.21.21.11.m1.1.1" xref="S3.T1.21.21.11.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.21.21.11.m1.1b"><ci id="S3.T1.21.21.11.m1.1.1.cmml" xref="S3.T1.21.21.11.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.21.21.11.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.21.21.11.2" class="ltx_text" style="font-size:70%;">TF</span>
</td>
</tr>
<tr id="S3.T1.46.53.7" class="ltx_tr">
<td id="S3.T1.46.53.7.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.53.7.1.1" class="ltx_text" style="font-size:70%;">LocalOpt</span></td>
<td id="S3.T1.46.53.7.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.53.7.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.53.7.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.53.7.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.53.7.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.53.7.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.53.7.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.53.7.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.53.7.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.53.7.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.53.7.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.53.7.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.54.8" class="ltx_tr">
<td id="S3.T1.46.54.8.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.54.8.1.1" class="ltx_text" style="font-size:70%;">GlobalOpt</span></td>
<td id="S3.T1.46.54.8.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.54.8.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.54.8.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.54.8.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.54.8.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.54.8.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.54.8.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.54.8.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.54.8.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.54.8.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.54.8.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.54.8.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.55.9" class="ltx_tr">
<td id="S3.T1.46.55.9.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.55.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Data Partitioning</span></td>
<td id="S3.T1.46.55.9.2" class="ltx_td"></td>
<td id="S3.T1.46.55.9.3" class="ltx_td"></td>
<td id="S3.T1.46.55.9.4" class="ltx_td"></td>
<td id="S3.T1.46.55.9.5" class="ltx_td"></td>
<td id="S3.T1.46.55.9.6" class="ltx_td"></td>
<td id="S3.T1.46.55.9.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.46.56.10" class="ltx_tr">
<td id="S3.T1.46.56.10.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.56.10.1.1" class="ltx_text" style="font-size:70%;">Horizontal</span></td>
<td id="S3.T1.46.56.10.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.56.10.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.56.10.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.56.10.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.56.10.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.56.10.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.56.10.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.56.10.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.56.10.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.56.10.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.56.10.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.56.10.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.27.27" class="ltx_tr">
<td id="S3.T1.27.27.7" class="ltx_td ltx_align_left"><span id="S3.T1.27.27.7.1" class="ltx_text" style="font-size:70%;">Vertical</span></td>
<td id="S3.T1.22.22.1" class="ltx_td ltx_align_center"><math id="S3.T1.22.22.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.22.22.1.m1.1a"><mo mathsize="70%" id="S3.T1.22.22.1.m1.1.1" xref="S3.T1.22.22.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.22.22.1.m1.1b"><times id="S3.T1.22.22.1.m1.1.1.cmml" xref="S3.T1.22.22.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.22.22.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.23.23.2" class="ltx_td ltx_align_center"><math id="S3.T1.23.23.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.23.23.2.m1.1a"><mo mathsize="70%" id="S3.T1.23.23.2.m1.1.1" xref="S3.T1.23.23.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.23.23.2.m1.1b"><times id="S3.T1.23.23.2.m1.1.1.cmml" xref="S3.T1.23.23.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.23.23.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.24.24.3" class="ltx_td ltx_align_center"><math id="S3.T1.24.24.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.24.24.3.m1.1a"><mo mathsize="70%" id="S3.T1.24.24.3.m1.1.1" xref="S3.T1.24.24.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.24.24.3.m1.1b"><times id="S3.T1.24.24.3.m1.1.1.cmml" xref="S3.T1.24.24.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.24.24.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.25.25.4" class="ltx_td ltx_align_center"><math id="S3.T1.25.25.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.25.25.4.m1.1a"><mo mathsize="70%" id="S3.T1.25.25.4.m1.1.1" xref="S3.T1.25.25.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.25.25.4.m1.1b"><times id="S3.T1.25.25.4.m1.1.1.cmml" xref="S3.T1.25.25.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.25.25.4.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.26.26.5" class="ltx_td ltx_align_center"><math id="S3.T1.26.26.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.26.26.5.m1.1a"><mo mathsize="70%" id="S3.T1.26.26.5.m1.1.1" xref="S3.T1.26.26.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.26.26.5.m1.1b"><times id="S3.T1.26.26.5.m1.1.1.cmml" xref="S3.T1.26.26.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.26.26.5.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.27.27.6" class="ltx_td ltx_nopad_r ltx_align_center"><math id="S3.T1.27.27.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.27.27.6.m1.1a"><mo mathsize="70%" id="S3.T1.27.27.6.m1.1.1" xref="S3.T1.27.27.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.27.27.6.m1.1b"><times id="S3.T1.27.27.6.m1.1.1.cmml" xref="S3.T1.27.27.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.27.27.6.m1.1c">\times</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.46.57.11" class="ltx_tr">
<td id="S3.T1.46.57.11.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.57.11.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Privacy &amp; Security</span></td>
<td id="S3.T1.46.57.11.2" class="ltx_td"></td>
<td id="S3.T1.46.57.11.3" class="ltx_td"></td>
<td id="S3.T1.46.57.11.4" class="ltx_td"></td>
<td id="S3.T1.46.57.11.5" class="ltx_td"></td>
<td id="S3.T1.46.57.11.6" class="ltx_td"></td>
<td id="S3.T1.46.57.11.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.46.58.12" class="ltx_tr">
<td id="S3.T1.46.58.12.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.58.12.1.1" class="ltx_text" style="font-size:70%;">Private Training</span></td>
<td id="S3.T1.46.58.12.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.58.12.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.58.12.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.58.12.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.58.12.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.58.12.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.58.12.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.58.12.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.58.12.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.58.12.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.58.12.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.58.12.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.29.29" class="ltx_tr">
<td id="S3.T1.29.29.3" class="ltx_td ltx_align_left"><span id="S3.T1.29.29.3.1" class="ltx_text" style="font-size:70%;">Secure Aggregation</span></td>
<td id="S3.T1.29.29.4" class="ltx_td ltx_align_center"><span id="S3.T1.29.29.4.1" class="ltx_text" style="font-size:70%;">FHE</span></td>
<td id="S3.T1.28.28.1" class="ltx_td ltx_align_center">
<span id="S3.T1.28.28.1.1" class="ltx_text" style="font-size:70%;">Masking</span><math id="S3.T1.28.28.1.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.28.28.1.m1.1a"><mo mathsize="70%" id="S3.T1.28.28.1.m1.1.1" xref="S3.T1.28.28.1.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.28.28.1.m1.1b"><ci id="S3.T1.28.28.1.m1.1.1.cmml" xref="S3.T1.28.28.1.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.28.28.1.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.28.28.1.2" class="ltx_text" style="font-size:70%;">FHE</span>
</td>
<td id="S3.T1.29.29.2" class="ltx_td ltx_align_center">
<span id="S3.T1.29.29.2.1" class="ltx_text" style="font-size:70%;">Masking</span><math id="S3.T1.29.29.2.m1.1" class="ltx_Math" alttext="\mid" display="inline"><semantics id="S3.T1.29.29.2.m1.1a"><mo mathsize="70%" id="S3.T1.29.29.2.m1.1.1" xref="S3.T1.29.29.2.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="S3.T1.29.29.2.m1.1b"><ci id="S3.T1.29.29.2.m1.1.1.cmml" xref="S3.T1.29.29.2.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.29.29.2.m1.1c">\mid</annotation></semantics></math><span id="S3.T1.29.29.2.2" class="ltx_text" style="font-size:70%;">FHE</span>
</td>
<td id="S3.T1.29.29.5" class="ltx_td ltx_align_center"><span id="S3.T1.29.29.5.1" class="ltx_text" style="font-size:70%;">FHE</span></td>
<td id="S3.T1.29.29.6" class="ltx_td ltx_align_center"><span id="S3.T1.29.29.6.1" class="ltx_text" style="font-size:70%;">TEE</span></td>
<td id="S3.T1.29.29.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.29.29.7.1" class="ltx_text" style="font-size:70%;">FHE</span></td>
</tr>
<tr id="S3.T1.46.59.13" class="ltx_tr">
<td id="S3.T1.46.59.13.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.59.13.1.1" class="ltx_text" style="font-size:70%;">Crypto Library</span></td>
<td id="S3.T1.46.59.13.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.59.13.2.1" class="ltx_text" style="font-size:70%;">TenSeal</span></td>
<td id="S3.T1.46.59.13.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.59.13.3.1" class="ltx_text" style="font-size:70%;">native</span></td>
<td id="S3.T1.46.59.13.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.59.13.4.1" class="ltx_text" style="font-size:70%;">native</span></td>
<td id="S3.T1.46.59.13.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.59.13.5.1" class="ltx_text" style="font-size:70%;">HElayers</span></td>
<td id="S3.T1.46.59.13.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.59.13.6.1" class="ltx_text" style="font-size:70%;">Graphene</span></td>
<td id="S3.T1.46.59.13.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.59.13.7.1" class="ltx_text" style="font-size:70%;">PALISADE</span></td>
</tr>
<tr id="S3.T1.46.60.14" class="ltx_tr">
<td id="S3.T1.46.60.14.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.60.14.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Communication</span></td>
<td id="S3.T1.46.60.14.2" class="ltx_td"></td>
<td id="S3.T1.46.60.14.3" class="ltx_td"></td>
<td id="S3.T1.46.60.14.4" class="ltx_td"></td>
<td id="S3.T1.46.60.14.5" class="ltx_td"></td>
<td id="S3.T1.46.60.14.6" class="ltx_td"></td>
<td id="S3.T1.46.60.14.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.46.61.15" class="ltx_tr">
<td id="S3.T1.46.61.15.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.61.15.1.1" class="ltx_text" style="font-size:70%;">Centralized</span></td>
<td id="S3.T1.46.61.15.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.61.15.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.61.15.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.61.15.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.61.15.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.61.15.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.61.15.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.61.15.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.61.15.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.61.15.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.61.15.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.61.15.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.35.35" class="ltx_tr">
<td id="S3.T1.35.35.7" class="ltx_td ltx_align_left"><span id="S3.T1.35.35.7.1" class="ltx_text" style="font-size:70%;">Decentralized</span></td>
<td id="S3.T1.30.30.1" class="ltx_td ltx_align_center"><math id="S3.T1.30.30.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.30.30.1.m1.1a"><mo mathsize="70%" id="S3.T1.30.30.1.m1.1.1" xref="S3.T1.30.30.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.30.30.1.m1.1b"><times id="S3.T1.30.30.1.m1.1.1.cmml" xref="S3.T1.30.30.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.30.30.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.31.31.2" class="ltx_td ltx_align_center"><math id="S3.T1.31.31.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.31.31.2.m1.1a"><mo mathsize="70%" id="S3.T1.31.31.2.m1.1.1" xref="S3.T1.31.31.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.31.31.2.m1.1b"><times id="S3.T1.31.31.2.m1.1.1.cmml" xref="S3.T1.31.31.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.31.31.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.32.32.3" class="ltx_td ltx_align_center"><math id="S3.T1.32.32.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T1.32.32.3.m1.1a"><mi mathsize="70%" mathvariant="normal" id="S3.T1.32.32.3.m1.1.1" xref="S3.T1.32.32.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T1.32.32.3.m1.1b"><ci id="S3.T1.32.32.3.m1.1.1.cmml" xref="S3.T1.32.32.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.32.32.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T1.33.33.4" class="ltx_td ltx_align_center"><math id="S3.T1.33.33.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.33.33.4.m1.1a"><mo mathsize="70%" id="S3.T1.33.33.4.m1.1.1" xref="S3.T1.33.33.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.33.33.4.m1.1b"><times id="S3.T1.33.33.4.m1.1.1.cmml" xref="S3.T1.33.33.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.33.33.4.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.34.34.5" class="ltx_td ltx_align_center"><math id="S3.T1.34.34.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.34.34.5.m1.1a"><mo mathsize="70%" id="S3.T1.34.34.5.m1.1.1" xref="S3.T1.34.34.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.34.34.5.m1.1b"><times id="S3.T1.34.34.5.m1.1.1.cmml" xref="S3.T1.34.34.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.34.34.5.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.35.35.6" class="ltx_td ltx_nopad_r ltx_align_center"><math id="S3.T1.35.35.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.35.35.6.m1.1a"><mo mathsize="70%" id="S3.T1.35.35.6.m1.1.1" xref="S3.T1.35.35.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.35.35.6.m1.1b"><times id="S3.T1.35.35.6.m1.1.1.cmml" xref="S3.T1.35.35.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.35.35.6.m1.1c">\times</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.41.41" class="ltx_tr">
<td id="S3.T1.41.41.7" class="ltx_td ltx_align_left"><span id="S3.T1.41.41.7.1" class="ltx_text" style="font-size:70%;">Hierarchical</span></td>
<td id="S3.T1.36.36.1" class="ltx_td ltx_align_center"><math id="S3.T1.36.36.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.36.36.1.m1.1a"><mo mathsize="70%" id="S3.T1.36.36.1.m1.1.1" xref="S3.T1.36.36.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.36.36.1.m1.1b"><times id="S3.T1.36.36.1.m1.1.1.cmml" xref="S3.T1.36.36.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.36.36.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.37.37.2" class="ltx_td ltx_align_center"><math id="S3.T1.37.37.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.37.37.2.m1.1a"><mo mathsize="70%" id="S3.T1.37.37.2.m1.1.1" xref="S3.T1.37.37.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.37.37.2.m1.1b"><times id="S3.T1.37.37.2.m1.1.1.cmml" xref="S3.T1.37.37.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.37.37.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.38.38.3" class="ltx_td ltx_align_center"><math id="S3.T1.38.38.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.38.38.3.m1.1a"><mo mathsize="70%" id="S3.T1.38.38.3.m1.1.1" xref="S3.T1.38.38.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.38.38.3.m1.1b"><times id="S3.T1.38.38.3.m1.1.1.cmml" xref="S3.T1.38.38.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.38.38.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.39.39.4" class="ltx_td ltx_align_center"><math id="S3.T1.39.39.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.39.39.4.m1.1a"><mo mathsize="70%" id="S3.T1.39.39.4.m1.1.1" xref="S3.T1.39.39.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.39.39.4.m1.1b"><times id="S3.T1.39.39.4.m1.1.1.cmml" xref="S3.T1.39.39.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.39.39.4.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.40.40.5" class="ltx_td ltx_align_center"><math id="S3.T1.40.40.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.40.40.5.m1.1a"><mo mathsize="70%" id="S3.T1.40.40.5.m1.1.1" xref="S3.T1.40.40.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.40.40.5.m1.1b"><times id="S3.T1.40.40.5.m1.1.1.cmml" xref="S3.T1.40.40.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.40.40.5.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.41.41.6" class="ltx_td ltx_nopad_r ltx_align_center"><math id="S3.T1.41.41.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.41.41.6.m1.1a"><mo mathsize="70%" id="S3.T1.41.41.6.m1.1.1" xref="S3.T1.41.41.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.41.41.6.m1.1b"><times id="S3.T1.41.41.6.m1.1.1.cmml" xref="S3.T1.41.41.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.41.41.6.m1.1c">\times</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.46.62.16" class="ltx_tr">
<td id="S3.T1.46.62.16.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.62.16.1.1" class="ltx_text" style="font-size:70%;">TLS</span></td>
<td id="S3.T1.46.62.16.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.62.16.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.62.16.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.62.16.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.62.16.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.62.16.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.62.16.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.62.16.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.62.16.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.62.16.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.62.16.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.62.16.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.63.17" class="ltx_tr">
<td id="S3.T1.46.63.17.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.63.17.1.1" class="ltx_text" style="font-size:70%;">Network</span></td>
<td id="S3.T1.46.63.17.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.63.17.2.1" class="ltx_text" style="font-size:70%;">gRPC</span></td>
<td id="S3.T1.46.63.17.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.63.17.3.1" class="ltx_text" style="font-size:70%;">gRPC</span></td>
<td id="S3.T1.46.63.17.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.63.17.4.1" class="ltx_text" style="font-size:70%;">MPI</span></td>
<td id="S3.T1.46.63.17.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.63.17.5.1" class="ltx_text" style="font-size:70%;">AMQP</span></td>
<td id="S3.T1.46.63.17.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.63.17.6.1" class="ltx_text" style="font-size:70%;">gRPC</span></td>
<td id="S3.T1.46.63.17.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.63.17.7.1" class="ltx_text" style="font-size:70%;">gRPC</span></td>
</tr>
<tr id="S3.T1.46.64.18" class="ltx_tr">
<td id="S3.T1.46.64.18.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.64.18.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Communication Protocol</span></td>
<td id="S3.T1.46.64.18.2" class="ltx_td"></td>
<td id="S3.T1.46.64.18.3" class="ltx_td"></td>
<td id="S3.T1.46.64.18.4" class="ltx_td"></td>
<td id="S3.T1.46.64.18.5" class="ltx_td"></td>
<td id="S3.T1.46.64.18.6" class="ltx_td"></td>
<td id="S3.T1.46.64.18.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.46.65.19" class="ltx_tr">
<td id="S3.T1.46.65.19.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.65.19.1.1" class="ltx_text" style="font-size:70%;">Synchronous</span></td>
<td id="S3.T1.46.65.19.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.65.19.2.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.65.19.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.65.19.3.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.65.19.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.65.19.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.65.19.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.65.19.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.65.19.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.65.19.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T1.46.65.19.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.65.19.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.46" class="ltx_tr">
<td id="S3.T1.46.46.6" class="ltx_td ltx_align_left"><span id="S3.T1.46.46.6.1" class="ltx_text" style="font-size:70%;">Asynchronous</span></td>
<td id="S3.T1.42.42.1" class="ltx_td ltx_align_center"><math id="S3.T1.42.42.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.42.42.1.m1.1a"><mo mathsize="70%" id="S3.T1.42.42.1.m1.1.1" xref="S3.T1.42.42.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.42.42.1.m1.1b"><times id="S3.T1.42.42.1.m1.1.1.cmml" xref="S3.T1.42.42.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.42.42.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.43.43.2" class="ltx_td ltx_align_center"><math id="S3.T1.43.43.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.43.43.2.m1.1a"><mo mathsize="70%" id="S3.T1.43.43.2.m1.1.1" xref="S3.T1.43.43.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.43.43.2.m1.1b"><times id="S3.T1.43.43.2.m1.1.1.cmml" xref="S3.T1.43.43.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.43.43.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.44.44.3" class="ltx_td ltx_align_center"><math id="S3.T1.44.44.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.44.44.3.m1.1a"><mo mathsize="70%" id="S3.T1.44.44.3.m1.1.1" xref="S3.T1.44.44.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.44.44.3.m1.1b"><times id="S3.T1.44.44.3.m1.1.1.cmml" xref="S3.T1.44.44.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.44.44.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.45.45.4" class="ltx_td ltx_align_center"><math id="S3.T1.45.45.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.45.45.4.m1.1a"><mo mathsize="70%" id="S3.T1.45.45.4.m1.1.1" xref="S3.T1.45.45.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.45.45.4.m1.1b"><times id="S3.T1.45.45.4.m1.1.1.cmml" xref="S3.T1.45.45.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.45.45.4.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.46.46.5" class="ltx_td ltx_align_center"><math id="S3.T1.46.46.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.46.46.5.m1.1a"><mo mathsize="70%" id="S3.T1.46.46.5.m1.1.1" xref="S3.T1.46.46.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.46.46.5.m1.1b"><times id="S3.T1.46.46.5.m1.1.1.cmml" xref="S3.T1.46.46.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.46.46.5.m1.1c">\times</annotation></semantics></math></td>
<td id="S3.T1.46.46.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.46.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T1.46.66.20" class="ltx_tr">
<td id="S3.T1.46.66.20.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.66.20.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Software</span></td>
<td id="S3.T1.46.66.20.2" class="ltx_td"></td>
<td id="S3.T1.46.66.20.3" class="ltx_td"></td>
<td id="S3.T1.46.66.20.4" class="ltx_td"></td>
<td id="S3.T1.46.66.20.5" class="ltx_td"></td>
<td id="S3.T1.46.66.20.6" class="ltx_td"></td>
<td id="S3.T1.46.66.20.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S3.T1.46.67.21" class="ltx_tr">
<td id="S3.T1.46.67.21.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.67.21.1.1" class="ltx_text" style="font-size:70%;">End-user</span></td>
<td id="S3.T1.46.67.21.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.67.21.2.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.67.21.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.67.21.3.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.67.21.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.67.21.4.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.67.21.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.67.21.5.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.67.21.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.67.21.6.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.67.21.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.67.21.7.1" class="ltx_text" style="font-size:70%;">Python</span></td>
</tr>
<tr id="S3.T1.46.68.22" class="ltx_tr">
<td id="S3.T1.46.68.22.1" class="ltx_td ltx_align_left"><span id="S3.T1.46.68.22.1.1" class="ltx_text" style="font-size:70%;">Learner</span></td>
<td id="S3.T1.46.68.22.2" class="ltx_td ltx_align_center"><span id="S3.T1.46.68.22.2.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.68.22.3" class="ltx_td ltx_align_center"><span id="S3.T1.46.68.22.3.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.68.22.4" class="ltx_td ltx_align_center"><span id="S3.T1.46.68.22.4.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.68.22.5" class="ltx_td ltx_align_center"><span id="S3.T1.46.68.22.5.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.68.22.6" class="ltx_td ltx_align_center"><span id="S3.T1.46.68.22.6.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.68.22.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.46.68.22.7.1" class="ltx_text" style="font-size:70%;">Python</span></td>
</tr>
<tr id="S3.T1.46.69.23" class="ltx_tr">
<td id="S3.T1.46.69.23.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T1.46.69.23.1.1" class="ltx_text" style="font-size:70%;">Aggregator</span></td>
<td id="S3.T1.46.69.23.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.2.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.69.23.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.3.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.69.23.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.4.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.69.23.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.5.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.69.23.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.6.1" class="ltx_text" style="font-size:70%;">Python</span></td>
<td id="S3.T1.46.69.23.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T1.46.69.23.7.1" class="ltx_text" style="font-size:70%;">C++</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>A qualitative comparison of different Federated Learning Systems.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We perform a qualitative and quantitative analysis to compare the trade-offs across leading FL frameworks <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We do not compare against LEAF <cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> and TFF <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Google </a></cite> frameworks since both frameworks are used primarily to experiment with new FL optimization algorithms <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021b</a>)</cite>.</span></span></span>. The aim of our evaluation is two-fold. Through the qualitative comparison, we aim to understand the out-of-the-box support provided by the different FL frameworks, and through the quantitative evaluation, we aim to gain insights into how different implementations of the federation controller can affect the overall execution of an FL workflow. Following the taxonomy proposed in the works of <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021b</a>); Kairouz et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022b</a>)</cite>, we report our qualitative comparison in Table <a href="#S3.T1" title="Table 1 ‣ 3 MetisFL: Design &amp; Architecture ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, while Figures <a href="#S4.F5" title="Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S4.F6" title="Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#S4.F7" title="Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> present the execution time of the various operations, in isolation, occurring during a typical FL workflow (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). It is critical to note that in the current evaluation, we assume that all local models fit in the controller’s in-memory store (e.g., hash map). Therefore, we consider model insertion and selection operations to take constant time across all frameworks (see section <a href="#S5" title="5 Discussion ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Qualitative Evaluation</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We compare leading FL systems across several categories.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">The <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">Deployment</span> category refers to the case where a federation can be run in a simulated environment as parallel processes within a single server or distributed across multiple nodes (servers). All systems support standalone and distributed execution and deployment for cross-silo settings. However, OpenFL, Nvidia FLARE, and IBM FL do not support execution in cross-device settings, with OpenFL having no support for containerized execution.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">In the <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">ML Environment</span> category, model types describe the machine learning models each system can support, backend the machine/deep learning engine used to perform model training, evaluation, and inference, and LocalOpt and GlobalOpt whether the system allows the development of customized local (learner) and global (controller) function optimization algorithms. In this category, all frameworks support the execution of various model architectures and both the PyTorch and Tensorflow (TF) backend engines. Our system (MetisFL) currently only supports PyTorch and Tensorflow; however, it can be easily extended to support other training engines, e.g., MONAI, MXNet(MX), JAX.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p">In the case of <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">Data Partitioning</span>, we evaluate whether each system can support learning over different partitioning schemes. All systems readily support horizontally partitioned learning environments. In their documentation, FedML also states that their platform can be extended to support vertical learning scenarios; however, support is not provided out of the box.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p">For the <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_italic">Privacy &amp; Security</span> category, we assess whether a system can support private learning (differential privacy), the cryptographic method type used to perform secure aggregation operations, and which library is used for the cryptography operations. All systems support differential private learning. Nvidia FLARE, IBM FL, and MetisFL support homomorphic operations through the CKKS <cite class="ltx_cite ltx_citemacro_cite">Cheon et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> scheme. For FHE operations, NVFlare uses the TenSeal library <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">Microsoft </a></cite>, IBM FL the HElayers library <cite class="ltx_cite ltx_citemacro_cite">Aharoni et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, and MetisFL the PALISADE library <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">PALISADE </a></cite>. OpenFL operates on a hardware-integrated trusted execution environment (TEE <cite class="ltx_cite ltx_citemacro_cite">Sabt et al. (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>), while Flower (Salvia <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021a</a>)</cite>) and FedML (LightSecAgg <cite class="ltx_cite ltx_citemacro_cite">So et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>) both utilize a mask-based encryption approach; both frameworks recently introduced the support for FHE protocols. Concerning the cryptography library, all systems (including ours) depend on an external library, while Flower and FedML provide native implementations for the required masking operations.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p">In the <span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_italic">Communication</span> category, we compare the federated learning topologies under which each system can operate, whether the communication across all participating parties is performed within an encrypted channel (TLS), and what network protocol is used to exchange messages. All systems can operate in a centralized federated learning environment (one aggregator, multiple clients). FedML provides support for decentralized settings (peer-to-peer), with FedML stating in their documentation support for hierarchical federated settings. Finally, every system uses gRPC to establish communication across all federation parties, except for FedML and IBM FL, which use the MPI and AMQP protocols.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.p7.1" class="ltx_p">Another category in which we observe limited implementation capabilities across existing systems is the <span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_italic">Communication Protocol</span>. Even though systems provide support for synchronous communication and aggregation, they lack support for asynchronous protocols. In contrast, MetisFL provides both synchronous (including semi-synchronous <cite class="ltx_cite ltx_citemacro_cite">Stripelis et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022b</a>)</cite>) and asynchronous execution. Based on other systems’ documentation, Flower and FedML are the only systems planning to support asynchronous execution <cite class="ltx_cite ltx_citemacro_cite">Stripelis et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022a</a>)</cite>.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para ltx_noindent">
<p id="S4.SS1.p8.1" class="ltx_p">Finally, compared to previously proposed metrics <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021b</a>); Kairouz et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022b</a>)</cite>, we also consider the programming language used to develop each component in the federated learning system: aggregator, learner and end-user API. All three components for all presented systems are developed in Python. However, in our framework, the aggregator is developed in C++.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">For the quantitative evaluation, presented in Figures <a href="#S4.F5" title="Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S4.F6" title="Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#S4.F7" title="Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we conduct end-to-end system stress tests across all frameworks over an increasing number of participating learners: {10, 25, 50, 100, 200}, and model sizes: {100k, 1M, 10M} parameters. We test all frameworks in a synchronous FL setting, using FedAvg as the global aggregation rule and with all learners participating at every federation round. To generate the different model sizes, we define an MLP architecture with 100 densely connected (hidden) layers and a constant number of parameters per layer <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>100k: 32 params/layer, 1M: 100params/layer, 10M: 320params/layer</span></span></span>. For training and evaluating the model, we use the HousingMLP dataset <cite class="ltx_cite ltx_citemacro_cite">Breiman (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> to generate the train and test datasets. Given that our analysis aims to stress test the framework’s efficiency and not the FL model’s learning performance, we randomly pick (with replacement) and assign 100 samples per learner for model training and testing. As local model optimizer, we use Vanilla SGD, and the batch size is set to 100 (both for training and testing).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">All experiments were conducted in a simulated federated environment on the same host machine equipped with 32cores Intel(R) Xeon(R) Gold 5217 CPU @ 3.00GHz, 251GB memory, and 8TB disk space. All conducted experiments were CPU-intensive, with each learner’s workload being CPU-bound, and no GPU was employed during federated training. With regard to frameworks’ versioning, we used NVFlare 2.3.1, Flower 1.2.0, FedML 0.7.6, and IBM FL 1.1.0. Our evaluation does not report OpenFL results because of the framework’s complexity in incorporating new models and logging features <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We will try to include them in a future version of our work.</span></span></span>. For all frameworks, we used gRPC as the services’ communication layer, except for IBM FL, where we used the FLASK API. For Flower, IBM FL, and MetisFL, we used Keras as the backend NN engine, and for NVFlare and FedML, we used PyTorch.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x5.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Train Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x6.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Train Round</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x7.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Aggregation Time</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x8.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Eval Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x9.png" id="S4.F5.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Eval Round</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x10.png" id="S4.F5.sf6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Federation Round</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>FL frameworks operations comparison for 100k parameters (the y-axis is in logscale).</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x11.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Train Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x12.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Train Round</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x13.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Aggregation Time</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x14.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Eval Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x15.png" id="S4.F6.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Eval Round</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x16.png" id="S4.F6.sf6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Federation Round</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>FL frameworks operations comparison for 1M parameters (the y-axis is in logscale).</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">With respect to the frameworks’ measurement reporting, in the Figures <a href="#S4.F5" title="Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S4.F6" title="Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#S4.F7" title="Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, for all other frameworks, except for FedML, we were able to capture all required metrics. For FedML, in particular, we only report the aggregation and federation time wall-clock time because it was hard to navigate into the codebase and find the required code segments where the logging functionality for the rest of the operations must be placed. Moreover, concerning execution failures, NVFlare and IBM FL did not run in the federated environment of 10M parameters for 100 and 200 learners and 200 learners, respectively.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p">In the case of MetisFL, we ran each MetisFL for each environment twice, one with the OpenMP enabled (MetisFL gRPC + OpenMP) and one without (MetisFL gRPC). As expected, OpenMP model aggregation is 10 times faster than no parallelization (cf. MetisFL w/ and w/out parallelization in Figures <a href="#S4.F5.sf3" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5c</span></a>, <a href="#S4.F6.sf3" title="In Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6c</span></a>, <a href="#S4.F7.sf3" title="In Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7c</span></a>) and almost 100 times faster than other frameworks, especially in the case of federation environments with large models (cf. MetisFL w/ OpenMP to other FL frameworks in Figure <a href="#S4.F7.sf3" title="In Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7c</span></a>). Since parallelization is only used during model aggregation, all other operations performed by MetisFL with and without OpenMP have similar and almost identical performances across all environments.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p">When comparing each federated operation in isolation, we observe that most systems’ overhead is attributed to the dispatch time of training and evaluation tasks (cf. Figures <a href="#S4.F5.sf1" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5a</span></a>, <a href="#S4.F5.sf4" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5d</span></a>, <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6a</span></a>, <a href="#S4.F6.sf4" title="In Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6d</span></a>, <a href="#S4.F7.sf1" title="In Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7a</span></a>, <a href="#S4.F5.sf4" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5d</span></a>). MetiFL has a relatively smaller overhead for these operations than other systems because the MetisFL controller submits the tasks to the learners through asynchronous callbacks, and all model tensors are transmitted as a protobuf bytes data type.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x17.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Train Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x18.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Train Round</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x19.png" id="S4.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Aggregation Time</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x20.png" id="S4.F7.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Eval Task Dispatch</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x21.png" id="S4.F7.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Eval Round</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F7.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00334/assets/x22.png" id="S4.F7.sf6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="115" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Federation Round</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>FL frameworks operations comparison for 10M parameters (the y-axis is in logscale).</figcaption>
</figure>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p">From our evaluation, we also observe that all NVFlare’s operations, especially its training and evaluation dispatch tasks times, underperform the corresponding operations from all other frameworks. When it comes to IBM FL, even though IBM FL does great work on performing extremely fast evaluation task dispatching and round execution, it underperforms other systems when executing the corresponding training operations. However, across all operations and frameworks, Flower has the most consistent behavior. As the federation size is doubled or the federation model becomes larger, Flower’s execution latency is linearly increasing.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.6" class="ltx_p">Finally, when comparing all frameworks holistically with respect to the federation round execution time (cf. Figures <a href="#S4.F5.sf6" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5f</span></a>, <a href="#S4.F6.sf6" title="In Figure 6 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6f</span></a>, <a href="#S4.F7.sf6" title="In Figure 7 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7f</span></a>), we observe that as the model sizes become larger and the federation size increases, the impact becomes much more significant. For instance, even though FedML is outperforming all frameworks in the federation environments with model sizes of 100k and 1M parameters, its performance starts to deteriorate when the size of the federated model becomes larger, i.e., 10M parameters. However, when comparing MetisFL to FedML in these challenging environments, we can observe that MetisFL leads to a 10-fold improvement. In general, all frameworks seem to underperform in these computationally demanding environments, with the overall federation execution time increasing almost non-linearly as the size of the federation doubles (cf. Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Quantitative Evaluation ‣ 4 Evaluation ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), e.g., 13.85s <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mo stretchy="false" id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><ci id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">\rightarrow</annotation></semantics></math> 25.31s <math id="S4.SS2.p7.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.2.m2.1a"><mo stretchy="false" id="S4.SS2.p7.2.m2.1.1" xref="S4.SS2.p7.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.2.m2.1b"><ci id="S4.SS2.p7.2.m2.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.2.m2.1c">\rightarrow</annotation></semantics></math> 243.54 for FedML and 448.57s <math id="S4.SS2.p7.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.3.m3.1a"><mo stretchy="false" id="S4.SS2.p7.3.m3.1.1" xref="S4.SS2.p7.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.3.m3.1b"><ci id="S4.SS2.p7.3.m3.1.1.cmml" xref="S4.SS2.p7.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.3.m3.1c">\rightarrow</annotation></semantics></math> 934.73s <math id="S4.SS2.p7.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.4.m4.1a"><mo stretchy="false" id="S4.SS2.p7.4.m4.1.1" xref="S4.SS2.p7.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.4.m4.1b"><ci id="S4.SS2.p7.4.m4.1.1.cmml" xref="S4.SS2.p7.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.4.m4.1c">\rightarrow</annotation></semantics></math> 1,914.92s for IBM FL, while Flower’s overall federation execution time is doubled every time the number of participating learners is doubled, i.e., 60.44 <math id="S4.SS2.p7.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.5.m5.1a"><mo stretchy="false" id="S4.SS2.p7.5.m5.1.1" xref="S4.SS2.p7.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.5.m5.1b"><ci id="S4.SS2.p7.5.m5.1.1.cmml" xref="S4.SS2.p7.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.5.m5.1c">\rightarrow</annotation></semantics></math> 116.05 <math id="S4.SS2.p7.6.m6.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p7.6.m6.1a"><mo stretchy="false" id="S4.SS2.p7.6.m6.1.1" xref="S4.SS2.p7.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.6.m6.1b"><ci id="S4.SS2.p7.6.m6.1.1.cmml" xref="S4.SS2.p7.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.6.m6.1c">\rightarrow</annotation></semantics></math> 216.38.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">#Learners</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NVFlare</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Flower</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FedML</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IBM FL</th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">MetisFL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">62.32</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">21.51</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">6.21</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">175.19</td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4.58</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">180.63</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center">30.71</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center">13.85</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center">448.57</td>
<td id="S4.T2.1.3.2.6" class="ltx_td ltx_nopad_r ltx_align_center">6.10</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">907.92</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center">60.44</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center">25.31</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center">934.73</td>
<td id="S4.T2.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_center">14.13</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">100</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center">116.05</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center">243.54</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center">1,914.92</td>
<td id="S4.T2.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_center">21.28</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">200</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">216.38</td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">683.21</td>
<td id="S4.T2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td id="S4.T2.1.6.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">45.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Federation Round Time (secs) for 10M Parameters.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We presented the Metis Federated Learning framework, a novel system for accelerating the training of large-scale FL workflows. By re-engineering the federation controller and treating it as the system’s first-class citizen, MetisFL leads to a powerful 10-fold and 100-fold wall-clock time reduction compared to existing leading FL frameworks. In our immediate future work, we plan to further enhance the computational capabilities of the controller by studying large-scale FL workflows in environments consisting of thousands and millions of learners and extremely large models (+100M parameters) in both synchronous and asynchronous settings. Given the amount of computational resources required in these environments, the controller may not be able to store all required local models in memory. In such settings, we plan to incorporate different model stores (e.g., distributed key-value or on-disk model stores) and understand their trade-offs in FL workflows’ convergence. We also plan to perform a more systematic performance analysis with respect to the cryptographic schemes employed by each framework and realize how the various FL workflows are affected.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) under contract HR00112090104, and in part by the National Institutes of Health (NIH) under grants U01AG068057 and RF1AG051710. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, NIH, or the U.S. Government.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2020)</span>
<span class="ltx_bibblock">
Ehud Aharoni, Allon Adir, Moran Baruch, Nir Drucker, Gilad Ezov, Ariel Farkash, Lev Greenberg, Ramy Masalha, Guy Moshkowich, Dov Murik, et al.

</span>
<span class="ltx_bibblock">Helayers: A tile tensors framework for large neural networks on encrypted data.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.01805</em>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. (2022)</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão, et al.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.14390</em>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Breiman (2017)</span>
<span class="ltx_bibblock">
Leo Breiman.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Classification and regression trees</em>.

</span>
<span class="ltx_bibblock">Routledge, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheon et al. (2017)</span>
<span class="ltx_bibblock">
Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song.

</span>
<span class="ltx_bibblock">Homomorphic encryption for arithmetic of approximate numbers.

</span>
<span class="ltx_bibblock">In Tsuyoshi Takagi and Thomas Peyrin (eds.), <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Cryptology – ASIACRYPT 2017</em>, pp.  409–437, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dagum &amp; Menon (1998)</span>
<span class="ltx_bibblock">
Leonardo Dagum and Ramesh Menon.

</span>
<span class="ltx_bibblock">Openmp: an industry standard api for shared-memory programming.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE computational science and engineering</em>, 5(1):46–55, 1998.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Tensorflow federated: Machine learning on decentralized data.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Sixu Hu, Yuan Li, Xu Liu, Qinbin Li, Zhaomin Wu, and Bingsheng He.

</span>
<span class="ltx_bibblock">The oarf benchmark suite: Characterization and implications for federated learning systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>, 13(4):1–32, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, 14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021a)</span>
<span class="ltx_bibblock">
Kwing Hei Li, Pedro Porto Buarque de Gusmão, Daniel J Beutel, and Nicholas D Lane.

</span>
<span class="ltx_bibblock">Secure aggregation for federated learning in flower.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning</em>, pp.  8–14, 2021a.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021b)</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for data privacy and protection.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, 2021b.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong, and Dejing Dou.

</span>
<span class="ltx_bibblock">From distributed machine learning to federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em>, 64(4):885–917, 2022a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Xiaoyuan Liu, Tianneng Shi, Chulin Xie, Qinbin Li, Kangping Hu, Haoyu Kim, Xiaojun Xu, Bo Li, and Dawn Song.

</span>
<span class="ltx_bibblock">Unifed: A benchmark for federated learning frameworks.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.10308</em>, 2022b.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ludwig et al. (2020)</span>
<span class="ltx_bibblock">
Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn, et al.

</span>
<span class="ltx_bibblock">Ibm federated learning: an enterprise framework white paper v0. 1.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.10987</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Microsoft seal.

</span>
<span class="ltx_bibblock">https://github.com/Microsoft/SEAL.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
PALISADE.

</span>
<span class="ltx_bibblock">Palisade library.

</span>
<span class="ltx_bibblock">https://palisade-crypto.org/.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reina et al. (2021)</span>
<span class="ltx_bibblock">
G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina, Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr Mokrov, Dmitry Agapov, et al.

</span>
<span class="ltx_bibblock">Openfl: An open-source framework for federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.06413</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke et al. (2020)</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al.

</span>
<span class="ltx_bibblock">The future of digital health with federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 3(1):1–7, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al. (2022)</span>
<span class="ltx_bibblock">
Holger R Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue Xu, Yuan-Ting Hsieh, Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, et al.

</span>
<span class="ltx_bibblock">Nvidia flare: Federated learning from simulation to real-world.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13291</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabt et al. (2015)</span>
<span class="ltx_bibblock">
Mohamed Sabt, Mohammed Achemlal, and Abdelmadjid Bouabdallah.

</span>
<span class="ltx_bibblock">Trusted execution environment: what it is, and what it is not.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Trustcom/BigDataSE/ISPA</em>, volume 1, pp.  57–64. IEEE, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al. (2022)</span>
<span class="ltx_bibblock">
Jinhyun So, Chaoyang He, Chien-Sheng Yang, Songze Li, Qian Yu, Ramy E Ali, Basak Guler, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 4:694–720, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stripelis et al. (2021)</span>
<span class="ltx_bibblock">
Dimitris Stripelis, Hamza Saleem, Tanmay Ghai, Nikhil Dhinagar, Umang Gupta, Chrysovalantis Anastasiou, Greg Ver Steeg, Srivatsan Ravi, Muhammad Naveed, Paul M Thompson, et al.

</span>
<span class="ltx_bibblock">Secure neuroimaging analysis using federated learning with homomorphic encryption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">17th International Symposium on Medical Information Processing and Analysis</em>, volume 12088, pp.  351–359. SPIE, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stripelis et al. (2022a)</span>
<span class="ltx_bibblock">
Dimitris Stripelis, Umang Gupta, Hamza Saleem, Nikhil Dhinagar, Tanmay Ghai, Rafael Sanchez, Chrysovalantis Anastasiou, Armaghan Asghar, Greg Ver Steeg, Srivatsan Ravi, et al.

</span>
<span class="ltx_bibblock">Secure federated learning for neuroimaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.05249</em>, 2022a.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stripelis et al. (2022b)</span>
<span class="ltx_bibblock">
Dimitris Stripelis, Paul M Thompson, and José Luis Ambite.

</span>
<span class="ltx_bibblock">Semi-synchronous federated learning for energy-efficient training and accelerated convergence in cross-silo settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>, 13(5):1–29, 2022b.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>, 10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaharia et al. (2010)</span>
<span class="ltx_bibblock">
Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, Ion Stoica, et al.

</span>
<span class="ltx_bibblock">Spark: Cluster computing with working sets.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">HotCloud</em>, 10(10-10):95, 2010.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix ltx_pruned_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>FL Frameworks Configuration</h2>

<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Flower</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">Flower is a widely used open-source federated learning framework. Flower’s architecture addresses critical distributed computing challenges such as scalability, client and communication heterogeneity, and system flexibility by intelligently organizing workload infrastructure between the server and client. To perform the Flower experiments, we leveraged the system’s federated averaging strategy abstraction to represent the coordinating server to the distributed client set. To set up all experimental environments, we created a Client Learner class, which defined each learner’s training and evaluation behavior in the federation. Then, we defined the client-factory function, which creates the required number of learners for the simulation and initializes each client with the model training and test data. We used system-native logging to record timestamp differences between specific code segments to benchmark performance successfully. We added timestamps wherever appropriate to record each experiment’s aggregation, dispatch, and overall wall-clock time.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">FedML</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">To configure the environments for our experiments, we installed FedML using pip inside a docker container. We changed the code in the Python source files installed using pip to record timestamps for the model aggregation time, federation, training, and evaluation round times, along with the associated dispatch time for the training and test tasks. To define the simulation for our use cases, we defined a data loader class for reading the input training data to our model. We registered the model by making code changes to the function Model Hub. To stress-test the system with different communication protocols, we run the system with two different back-end communication services: gRPC and MQTT.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">IBM FL</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p">We spawn the IBM FL framework in a containerized environment. Specific terminology within the framework designates the Aggregator as the coordinating server and the distributed clients as Parties. Arrangement of both the aggregator and parties is handled by IBM FL native python scripts that process experiment parameters to create configuration files for the aggregator and parties. The federated learning training algorithm is assigned to the aggregator defined as a fusion handler. In our experiments, we selected the native FedAvgFusionHandler. Each party is assigned our custom Housing MLP model specified with parameter size at configuration time. To execute our experiments, we spawned one process solely responsible for the aggregator and a variable number of party processes set to the number of parties. We added timestamps in the code logic surrounding the global model aggregation function, the functions that execute the dispatch and evaluate logic. Moreover, to enable easier initialization of the federated environments, we had to tailor the initialization logic of the Party class code so that it could execute model training as soon as the aggregator directed it. By doing so, we did not have to initialize the Party class through command line instructions, which was extremely cumbersome when considering multiple learners for each experiment.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Nvidia Flare</h4>

<div id="A1.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="A1.SS0.SSS0.Px4.p1.1" class="ltx_p">NVFlare operates as a generic open-source software development kit (SDK) to facilitate distributed collaborative computing in multithreaded environments. Central to this framework’s architecture is a flexible implementation of communication protocols carried out by two distinct entities: Controllers and Workers. In our FL experiments, we utilized the functionality of these communication protocols to represent the principal roles of the solitary server (Controller) and the set of distributed clients (Workers). An example of such a communication protocol used in tandem with Federated Averaging is the “Broadcast and Wait” protocol. In this protocol, the server broadcasts a task and global information to the clients. Each client then locally executes the task and sends the client’s local updates to the orchestrating server for aggregation. The framework’s infrastructure places a strong emphasis on workflows. Namely, we used the Scatter and Gather as well as Global Model Evaluation workflows. To execute the experiments, NVFLARE provides a robust simulator command line interface, which we employ to configure various experiment details, including the client count, number of threads, and machine learning tasks. We utilized developer-friendly built-in logging methods to record training and evaluation times and capture system-related metrics. By harnessing the robust foundational system, effective communication protocol, user-friendly simulator, and reliable logging, we successfully benchmarked the approach with metrics pertinent to this paper.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>MetisFL Internal Procedures</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">In this section, we describe in more detail some of the internal mechanisms of MetisFL execution flow. Specifically, we consider a synchronous FL setting, where federated training and evaluation occur over a series of federation rounds, with every federation round consisting of a federated training round followed by a federated evaluation round.</p>
</div>
<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph ltx_pruned_first">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Federation Round Flow</h4>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px1.p1.1" class="ltx_p">The Federation Round lifecycle (see Figure <a href="#A2.F8" title="Figure 8 ‣ Federation Round Flow ‣ Appendix B MetisFL Internal Procedures ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) consists of three core steps: <span id="A2.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Initialization, Monitoring, Shutdown</span>. The driver initializes the controller process at the remote host (or localhost) and receives an acknowledgment when the remote process is alive. After that, the driver sends the initial state of the model (just the model tensors, not the actual architecture) to the controller and proceeds with learners’ initialization. Once all learners are initialized, the driver sends the model (model tensors &amp; architecture) to every other learner. The driver ships the actual model architecture to the learners because they are required to perform training and evaluation on their local private datasets. In contrast, the controller is only responsible for orchestrating the federation workflow and aggregating learners’ weights.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px1.p2.1" class="ltx_p">Once learners register with the controller (join the federation), the training and evaluation of the federated model occur for multiple federation rounds. Within every federation round, the federated model is sent for training and, subsequently, for evaluation to all participating learners. At this point, the driver monitors the lifecycle of the federation and periodically pings (heartbeat) remote processes. Once any of the federated training termination criteria is met, such as the execution wall-clock time or a number of federation rounds, then the driver sends a shutdown signal to all processes, first to the learners and then to the controller.</p>
</div>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2311.00334/assets/figures/MetisFL_Round_Federation_v1.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="186" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>MetisFL Federation Round Flow.</figcaption>
</figure>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Training Round Flow</h4>

<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px2.p1.1" class="ltx_p">Figure <a href="#A2.F9" title="Figure 9 ‣ Training Round Flow ‣ Appendix B MetisFL Internal Procedures ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> presents how the federated training round is executed within MetisFL. Before the training round starts, the controller creates/defines the model training task and selects the learners participating in model training. Once the learners have been selected the train task scheduler sends the training task to every participating learner (RunTask request).</p>
</div>
<div id="A2.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px2.p2.1" class="ltx_p">The Learner(s) entity receives the task through the Learner Servicer process and submits the training task to the training task pool executor running in the background. Upon task submission, the executor replies with an Acknowledgment (Ack) message that the servicer relays to the controller. Note that the status of the Ack message is false when the training task is not submitted or received or any unexpected failure occurs.</p>
</div>
<figure id="A2.F9" class="ltx_figure"><img src="/html/2311.00334/assets/figures/MetisFL_Round_Training_v1.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="192" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>MetisFL Training Round Flow.</figcaption>
</figure>
<div id="A2.SS0.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px2.p3.1" class="ltx_p">The submitted training task is registered with a callback function that will handle the completed training task when it is completed. Once this occurs, the servicer sends a MarkTaskCompleted request to the controller containing the learner’s local model and other execution metadata related to model training (e.g., training time per batch, number of completed steps, and epochs). Finally, the controller stores and aggregates all received local models.</p>
</div>
<div id="A2.SS0.SSS0.Px2.p4" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px2.p4.1" class="ltx_p">To improve the scheduling of training tasks, the controller submits the tasks to the learners as asynchronous calls, meaning that the controller does not wait for the training task to complete. In other words, the controller submits the task, but the learner needs to inform the controller when its local training is complete.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px3" class="ltx_paragraph ltx_pruned_first">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Evaluation Round Flow</h4>

<div id="A2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px3.p1.1" class="ltx_p">In Figure <a href="#A2.F10" title="Figure 10 ‣ Evaluation Round Flow ‣ Appendix B MetisFL Internal Procedures ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> we present the execution of the federated evaluation round. Similar to the training round, the evaluation round starts with the controller constructing the evaluation task and selecting the learners participating in the evaluation of the global model. Once these steps are defined, the evaluation task scheduler sends an EvaluateModel request to all participating learners and receives the respective model evaluations. Compared to the training tasks, the evaluation tasks are synchronous calls, meaning that the controller keeps the connection alive till the evaluation of the model is complete.</p>
</div>
<figure id="A2.F10" class="ltx_figure"><img src="/html/2311.00334/assets/figures/MetisFL_Round_Evaluation_v1.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="196" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>MetisFL Evaluation Round Flow.</figcaption>
</figure>
</section>
<section id="A2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Secure Sockets Layer</h4>

<div id="A2.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p1.1" class="ltx_p">Figure <a href="#A2.F11" title="Figure 11 ‣ Secure Sockets Layer ‣ Appendix B MetisFL Internal Procedures ‣ MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows how SSL is enabled within MetisFL. From the perspective of SSL connectivity, in a FL setting every participating entity (controller, learner) acts both as a client and as a server at different points of the federated execution workflow. Therefore, when establishing SSL-enabled connections we need first to generate the pair of files (private key, public certificate) that will be used by the server process running at a given host (e.g., learner/controller). This will allow the gRPC server to receive secure connections from requesting clients. When the controller receives model update requests from the learners, the controller acts as a server and the learners as the server’s clients. Similarly, when a learner receives local model training requests from the controller, the learner acts as a server and the controller as a client.</p>
</div>
<div id="A2.SS0.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p2.1" class="ltx_p">In a MetisFL simulated environment, the internal mechanism for enabling SSL connectivity starts with the federation driver. The driver uses the defined SSL certificates (self-signed or trusted authority) to start the federation controller and spawn its server process with the provided key pair. Analogously, the driver initializes the learner processes using the defined pair. Finally, during the learner-controller registration, the learners share their public certificate with the controller as part of the exchanged message.</p>
</div>
<div id="A2.SS0.SSS0.Px4.p3" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p3.1" class="ltx_p">In a production environment, though, it is not required for the SSL certificates to be generated by the driver. The certificates can be developed independently by each process (controller, learner), and then the public certificates can be shared with the driver to establish secure connections wherever needed.</p>
</div>
<figure id="A2.F11" class="ltx_figure"><img src="/html/2311.00334/assets/x23.png" id="A2.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>MetisFL with SSL Execution.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.00333" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.00334" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.00334">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.00334" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.00335" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 20:12:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
