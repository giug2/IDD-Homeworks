<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.00342] Survey of Privacy Threats and Countermeasures in Federated Learning</title><meta property="og:description" content="Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and pri…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Survey of Privacy Threats and Countermeasures in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Survey of Privacy Threats and Countermeasures in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.00342">

<!--Generated on Tue Mar  5 19:09:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
horizontal federated learning,  vertical federated learning,  transfer federated learning,  threat to privacy,  countermeasure against privacy threat.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Survey of Privacy Threats and 
<br class="ltx_break">Countermeasures in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Masahiro Hayashitani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Junki Mori
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
and Isamu Teranishi
</span><span class="ltx_author_notes">M. Hayashitani, J. Mori, and I. Teranishi are with NEC Secure System Platform Research Laboratories. E-mail: hayashitani@nec.com, junki.mori@nec.com, teranisi@nec.com.
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
horizontal federated learning, vertical federated learning, transfer federated learning, threat to privacy, countermeasure against privacy threat.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As computing devices become more ubiquitous, people generate vast amounts of data in their daily lives. Collecting this data in centralized storage facilities is costly and time-consuming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Another important concern is user privacy and confidentiality, as usage data typically contains sensitive information. Sensitive data such as biometrics and healthcare can be used for targeted social advertising and recommendations, posing immediate or potential privacy risks. Therefore, private data should not be shared directly without any privacy considerations. As societies become more privacy-conscious, legal restrictions such as the General Data Protection Regulation (GDPR) and the EU AI ACT are emerging, making data aggregation practices less feasible. In this case, federated learning has emerged as a promising machine learning technique where each client learns and sends the information to a server.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning has attracted attention as a privacy-preserving machine learning technique because it can learn a global model without exchanging private raw data between clients. However, federated learning still poses a threat to privacy. Recent works have shown that federated learning may not always provide sufficient privacy guarantees, since the communication of model updates throughout the training process may still reveal sensitive information, even to a third party or to the central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Typical examples of federated learning include horizontal federated learning where features are common, vertical federated learning where IDs are common, and federated transfer learning where some features or IDs are common. However, we note that common and unique privacy threats among each type of federated learning have not been categorized and described in a comprehensive and specific way.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For example, in the case of horizontal federated learning, semi-honest server can infer client’s data by inference attacks on a model sent by the client. If the client is an attacker, the attacker can infer the data of other clients by inference attacks on a global model received from the server. Such an attack is possible because the global model is design to reflect the data of all clients. If the attacker is a third party that is neither a server nor a client, it can eavesdrop on models passing through the communication channel and infer client data through inference attacks. In vertical federated learning, the main threat to privacy is the identify leakage through identity matching between clients. In addition, since the intermediate outputs of a model are sent to the server, there is a possibility that client data can be inferred through an inference attack. Also, as in horizontal federated learning, client data can be inferred by an inference attack on the server. Finally, in federated transfer learning, member and attribute guessing attacks are possible by exploiting a prediction network. If IDs are common, gradient information is exchanged when features are made similar. Therefore member and attribute guessing attacks are possible by using gradient information. When there are common features among clients, attribute guessing attacks are possible by exploiting networks that complement the missing features from the common features.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we discuss the above threats to privacy in detail and countermeasures against privacy threats in three types of federated learning; horizontal federated learning, vertical federated learning, and federated transfer learning. The paper is organized as follows: Section 2 presents learning methods for horizontal federated learning, vertical federated learning, and federated transfer learning; Section 3 discusses threats to privacy in each federated learning; Section 4 discusses countermeasures against privacy threats in each federated learning; and Section 5 concludes.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Categorization of Federated Learning</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="color:#000000;">Based on the data structures among clients, federated learning is categorized into three types as first introduced by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>: horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL). Figure <a href="#S2.F1" title="Figure 1 ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the data structure among clients for each type of federated learning. HFL assumes that each client has the same features and labels but different samples (Figure <a href="#S2.F1.sf1" title="In Figure 1 ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>). On the other hand, VFL assumes that each client has the same samples but disjoint features (Figure <a href="#S2.F1.sf1" title="In Figure 1 ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>). Finally, FTL applies to the scenario where each of the two clients has data that differ in not only samples but also features (Figure <a href="#S2.F1.sf3" title="In Figure 1 ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>).</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="color:#000000;">In the following subsections, we describe the learning and prediction methods for each type of federated learning.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.00342/assets/fig/HFL_structure.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="186" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S2.F1.sf1.3.2" class="ltx_text" style="font-size:80%;">Horizontal federated learning.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.00342/assets/fig/VFL_structure.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="186" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S2.F1.sf2.3.2" class="ltx_text" style="font-size:80%;">Vertical federated learning.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.00342/assets/fig/FTL_structure.png" id="S2.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="186" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S2.F1.sf3.3.2" class="ltx_text" style="font-size:80%;">Federated transfer learning.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.2.1" class="ltx_text" style="color:#000000;">Categorization of federated learning based on data structure owned by clients.</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Horizontal Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text" style="color:#000000;">HFL is the most common federated learning category which was first introduced by Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The goal of HFL is for each client holding different samples to collaboratively improve the accuracy of a model with a common structure.</span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text" style="color:#000000;">Figure <a href="#S2.F2" title="Figure 2 ‣ II-A Horizontal Federated Learning ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of the HFL learning protocol. Two types of entities participate in learning of HFL:</span></p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Server</span><span id="S2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;"> - Coordinator. Server exchanges model parameters with the clients and aggregates model parameters received from the clients.</span></p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Clients</span><span id="S2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> - Data owners. Each client locally trains a model using their own private data and exchanges model parameters with the server.</span></p>
</div>
</li>
</ol>
<p id="S2.SS1.p2.2" class="ltx_p"><span id="S2.SS1.p2.2.1" class="ltx_text" style="color:#000000;">Each clients first trains a local model for a few steps and sends the model parameters to the server. Next, the server updates a global model by aggregating (in standard methods such as FedAvg, simply averaging) the local models and sends it to all clients. This process is repeated until the convergence. During inference time, each client separately predicts the label using a global model and its own features.</span></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text" style="color:#000000;">The protocol described above is called centralized HFL because it requires a trusted third party, a central server. On the other hand, decentralized HFL, which eliminates the need for a central server, has emerged in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In decentralized HFL, clients directly communicates with each other, resulting in communication resource savings. There are various possible methods of communication between clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For example, the most common method for HFL of gradient boosting decision trees is for each client to add trees to the global model by sequence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
</span></p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.00342/assets/fig/LM_HFL.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="354" height="219" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the HFL learning protocol.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Vertical Federated Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">VFL enables clients holding the different features of the same samples to collaboratively train a model which takes all of the various features each client has as input. There are VFL studies to deal with various models including linear/logistic regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, decision trees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and other non-linear models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</span></p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text" style="color:#000000;">Figure <a href="#S2.F3" title="Figure 3 ‣ II-B Vertical Federated Learning ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an overview of the standard VFL learning protocol. In VFL, only one client holds labels and it plays the role of a server. Therefore, two types of entities participate in learning of VFL:</span></p>
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Active client</span><span id="S2.I2.i1.p1.1.2" class="ltx_text" style="color:#000000;"> - Features and labels owner. Active client coordinates the learning procedure. It calculates the loss and exchanges intermediate results with the passive clients.</span></p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Passive clients</span><span id="S2.I2.i2.p1.1.2" class="ltx_text" style="color:#000000;"> - Features owners. Each passive client keeps both its features and model local but exchanges intermediate results with the active client.</span></p>
</div>
</li>
</ol>
<p id="S2.SS2.p2.2" class="ltx_p"><span id="S2.SS2.p2.2.1" class="ltx_text" style="color:#000000;">VFL consists of two phases: IDs matching and learning phases. In IDs matching phases, all clients shares the common sample IDs. In learning phase, each client has a separate model with its own features as input, and the passive clients send the computed intermediate outputs to the active client. The active client calculates the loss based on the aggregated intermediate outputs and sends the gradients to all passive clients. Then, the passive clients updates its own model parameters. This process is repeated until the convergence. During inference time, all clients need to cooperate to predict the label of a sample.
</span></p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2402.00342/assets/fig/LM_VFL.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of the standard VFL learning protocol.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Federated Transfer Learning</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text" style="color:#000000;">FTL assumes two clients that shares only a small portion of samples or features. The goal of FTL is to create a model that can predict labels on the client that does not possess labels (target client), by transferring the knowledge of the other client that does possess labels (source client) to the target client.</span></p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text" style="color:#000000;">Figure <a href="#S2.F4" title="Figure 4 ‣ II-C3 Labels of target client ‣ II-C Federated Transfer Learning ‣ II Categorization of Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows an overall of the FTL learning protocol. As noted above, two types of entities participate in FTL:</span></p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<ol id="S2.I3" class="ltx_enumerate">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Source client</span><span id="S2.I3.i1.p1.1.2" class="ltx_text" style="color:#000000;"> - Features and labels owner. Source client exchanges intermediate results such as outputs and gradients with the target client and calculates the loss.</span></p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Target client</span><span id="S2.I3.i2.p1.1.2" class="ltx_text" style="color:#000000;"> - Features owners. Target client exchanges intermediate results with the source client.</span></p>
</div>
</li>
</ol>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text" style="color:#000000;">In FTL, two clients exchange intermediate outputs to learn a common representation. The source client uses the labeled data to compute the loss and sends the gradient to the target client, which updates the target client’s representation. This process is repeated until the convergence. During inference time, the target client predicts the label of a sample using its own model and features.</span></p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text" style="color:#000000;">The detail of the learning protocol varies depending on the specific method. Although only a limited number of FTL methods have been proposed, we introduce three major types of methods. FTL requires some supplementary information to bridge two clients, such as common IDs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, common features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and labels of target client <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
</span></p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS1.4.1.1" class="ltx_text">II-C</span>1 </span><span id="S2.SS3.SSS1.5.2" class="ltx_text" style="color:#000000;">Common IDs</span>
</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p"><span id="S2.SS3.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">Most FTL methods assumes the existence of the common ID’s samples between two clients. This type of FTL requires ID matching before the learning phase as with VFL. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposed the first FTL protocol, which learns feature transformation functions so that the different features of the common samples are mapped into the same features. The following work by Sharma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> improved communication overhead of the first FTL using multi-party computation and enhanced the security by incorporating malicious clients. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed a dual learning framework in which two clients impute each other’s missing features by exchanging the outputs of the imputation models for the common samples.
</span></p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS2.4.1.1" class="ltx_text">II-C</span>2 </span><span id="S2.SS3.SSS2.5.2" class="ltx_text" style="color:#000000;">Common features</span>
</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p"><span id="S2.SS3.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">In real-world applications, it is difficult to share samples with the same IDs. Therefore, Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed a method to realize FTL by assuming common features instead of common samples. In that method, two clients mutually reconstruct the missing features by using exchanged feature mapping models. Then,using all features, the clients conduct HFL to obtain a label prediction model. In the original paper, the authors assumes that all clients posses labels, but this method is applicable to the target client that does not posses labels because the source client can learn the label prediction model only by itself. Mori et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposed a method for neural networks in which each client incorporates its own unique features in addition to common features into HFL training. However, their method is based on HFL and cannot be applied to the target clients that does not possess labels.</span></p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS3.4.1.1" class="ltx_text">II-C</span>3 </span><span id="S2.SS3.SSS3.5.2" class="ltx_text" style="color:#000000;">Labels of target client</span>
</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p"><span id="S2.SS3.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">This type of methods assumes neither common IDs nor features, but instead assumes that all clients possess labels, allowing a common representation to be learned across clients. Since it is based on HFL, the participating entities are the same as in HFL. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> learns a common representation by exchanging the intermediate outputs with the server and reducing maximum mean discrepancy loss. Rakotomamonjy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposed a method to learn a common representation by using Wasserstein distance for intermediate outputs, which enables that the clients only need to exchange statistical information such as mean and variance with the server.
</span></p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2402.00342/assets/fig/LM_FTL.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="142" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overall of the FTL learning protocol.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Threats to Privacy in Each Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe threats to privacy in each federated learning. Table <a href="#S3.T1" title="TABLE I ‣ III Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows threads to privacy addressed in each federated learning. An inference attack uses data analysis to gather unauthorized information about a subject or database. If an attacker can confidently estimate the true value of a subject’s confidential information, it can be said to have been leaked. The most frequent variants of this approach are membership inference and feature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In addition, we address privacy threats of label inference and ID leakage.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Threads to privacy addressed in each federated learning</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t">Federated Learning</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Membership Inference</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Feature Inference</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Label Inference</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ID Leakage</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt">HFL</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Low or above</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Already known</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">None</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">None</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">VFL</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Already known</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">Low or above</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">Low or above</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">High</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">FTL (common features)</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Low</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">Low or above</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">Low or above</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center">None</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_rr">FTL (common IDs)</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Low</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Low or above</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Low or above</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">High</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Horizontal Federated Learning</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In HFL, client data is a major threat to privacy. Figure <a href="#S3.F5" title="Figure 5 ‣ III-A Horizontal Federated Learning ‣ III Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows threats to privacy in HFL. Possible attackers are as follows:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Server:
Inference attack against the model to infer client data.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Clients:
Inference attack against the global model received from the server to infer other clients’ data.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Third party:
Eavesdrop on models that pass through the communication channel and infer client data through inference attacks.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2402.00342/assets/fig/TP_HFL.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="262" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Threats to privacy in HFL.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Vertical Federated Learning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In VFL, a major threat to privacy is the leakage of identities due to identity matching between clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In addition to the leakage of identities, partial output from clients is also a threat. In case of ID matching, in order to create a single model for the overall system, it is necessary to match IDs that are common to each client’s data. This will reveal the presence of the same user to other clients.
Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Vertical Federated Learning ‣ III Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows threats to privacy in VFL in case of partial output from clients, and possible attackers are as follows:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Active client:
Inference attack against the output of lower model to infer client data.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Passive Clients:
Inference attack against the output of upper model received from the active client to infer other clients’ data.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Third party:
Eavesdrop on outputs that pass through the communication channel and infer client data through inference attacks.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2402.00342/assets/fig/TP_VFL.png" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Threads to privacy in VFL.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Federated Transfer Learning</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In federated transfer learning, threats to privacy vary depending on the information in common <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We explain the case when features are common and when IDs are common, respectively.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.4.1.1" class="ltx_text">III-C</span>1 </span>Common Features</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Figure <a href="#S3.F7" title="Figure 7 ‣ III-C1 Common Features ‣ III-C Federated Transfer Learning ‣ III Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows threats to privacy in case of common features in FTL, and possible attackers are as follows:</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">Client receiving a feature analogy network:
Inference attack against feature analogy network to infer client data.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">Client receiving a feature analogy network and prediction network:
Inference attack against feature analogy network and prediction network to infer client data.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">Third party:
Eavesdrop on feature analogy network and prediction network pass through the communication channel and infer client data through inference attacks.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CFTP_FTL.png" id="S3.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="236" height="199" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Threats to privacy in case of common features in FTL.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.4.1.1" class="ltx_text">III-C</span>2 </span>Common IDs</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">In case of Common IDs, a threat to privacy is the leakage of identities due to identity matching between clients as shown in VFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In addition to the leakage of identities, information required for feature similarity from clients is also a threat.
Figure <a href="#S3.F8" title="Figure 8 ‣ III-C2 Common IDs ‣ III-C Federated Transfer Learning ‣ III Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows threats to privacy in case of common IDs in FTL in case of information required for feature similarity, and possible attackers are as follows:</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<ol id="S3.I4" class="ltx_enumerate">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p">Client receiving information for feature similarity:
Inference attack against information required for feature similarity to infer client data.</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p">Third party:
Eavesdrop on information required for feature similarity pass through the communication channel and infer client data through inference attacks.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CITP_FTL.png" id="S3.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="118" height="221" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Threads to privacy in case of common IDs in FTL.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Countermeasures against Threats to Privacy in Each Federated Learning</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe countermeasures against threats to privacy in each federated learning. Table <a href="#S4.T2" title="TABLE II ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows countermeasures against privacy threats addressed in each federated learning. Despite the wide variety of previous efforts to secure privacy in federated learning, the proposed methods typically fall into one of these categories: differential privacy, secure computation, encryption of communication, and ID dummying <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Countermeasures against privacy threats addressed in each federated learning.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t">Federated Learning</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Differential Privacy</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Secure Computation</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Encryption of Communication</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ID Dummying</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt">HFL</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Client Side</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Server Side</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Communication Line</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">VFL</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">Active Client Side</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">Communication Line</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center">Client Table</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">FTL (common features)</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Feature Analogy Network Exchange</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">Communication Line</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_rr">FTL (common IDs)</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Gradient Exchange</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Communication Line</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">Client Table</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Horizontal Federated Learning</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In HFL, a typical privacy measure for client data is to protect attacks by the server side with secure computation and attacks by the client side with differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Figure <a href="#S4.F9" title="Figure 9 ‣ IV-A Horizontal Federated Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows countermeasures against threads to privacy in HFL. The position of the attacker by these privacy measures is described as follows.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Server:
Secure computation realizes global model integration calculations without seeing the model by the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Client:
Client A creates a model by adding noise through differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Client B receives the parameters of the global model via the server, but Client A’s model is protected by differential privacy.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Third party:
Achieved by encryption of communication.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CM_HFL.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="260" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Countermeasures against threads to privacy in HFL.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Vertical Federated Learning</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In VFL, the threads to privacy are the leakage of identities and partial output from clients. We show how to respond in the case of each threat.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>IDs Matching</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">In case of IDs matching, Dummy IDs are prepared in addition to the original IDs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. For the dummy part of the ID, dummy variables that have no effect on learning are sent. Figure <a href="#S4.F10" title="Figure 10 ‣ IV-B1 IDs Matching ‣ IV-B Vertical Federated Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows an example of dummy IDs.
Before dummy IDs are used, all IDs that match Client A are known to Client B (cf. ID 3,4). After dummy IDs are used, Client B does not know which of the IDs that match Client A is the real ID of Client A.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CMID_VFL.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="159" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example of dummy IDs.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Output from Clients</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In case of output from clients, the typical privacy measure is the use of secure calculations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Figure <a href="#S4.F11" title="Figure 11 ‣ IV-B2 Output from Clients ‣ IV-B Vertical Federated Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows countermeasures against threads in case of output from clients. The position of the attacker by these privacy measures is described as follows.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Active Client:
Secure computation realizes global model integration calculations without seeing the model by the active client. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Passive Clients:
Client B receives the information used for updating from the upper model via the active client, but it is protected by secure computation.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Third party:
Achieved by encryption of communication.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CMOC_VFL.png" id="S4.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Countermeasures against threads in case of output from clients.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Federated Transfer Learning</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In FTL, the threads to privacy depend on common information between clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We show how to respond in the case of each thread.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Common Features</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In case of common features, the threads to privacy are exchanges of feature analogy network and prediction network.
Figure <a href="#S4.F12" title="Figure 12 ‣ IV-C1 Common Features ‣ IV-C Federated Transfer Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows countermeasures against threads in case of common features.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Client receiving a feature analogy network:
Differential privacy makes it difficult to infer the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Client receiving a feature analogy network and prediction network:
Differential privacy makes it difficult to infer the model.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Third party:
Achieved by encryption of communication.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CMCF_FTL.png" id="S4.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="236" height="193" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Countermeasures against threads in case of common features.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Common IDs</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In case of common IDs, the threads to privacy are the leakage of identities and information required for feature similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. For the leakage of identities, Dummy IDs are prepared in addition to the original IDs as shown in Section <a href="#S4.SS2.SSS1" title="IV-B1 IDs Matching ‣ IV-B Vertical Federated Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>1</span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. For information required for feature similarity, figure <a href="#S4.F13" title="Figure 13 ‣ IV-C2 Common IDs ‣ IV-C Federated Transfer Learning ‣ IV Countermeasures against Threats to Privacy in Each Federated Learning ‣ Survey of Privacy Threats and Countermeasures in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows countermeasures against threads in case of common IDs.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<ol id="S4.I4" class="ltx_enumerate">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.1" class="ltx_p">Client receiving information for feature similarity:
Difficult to guess information due to secure computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S4.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="S4.I4.i2.p1" class="ltx_para">
<p id="S4.I4.i2.p1.1" class="ltx_p">Third party:
Achieved by encryption of communication.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2402.00342/assets/fig/CMCI_FTL.png" id="S4.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="118" height="171" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Countermeasures against threads in case of common IDs.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have described privacy threats and countermeasures for federated learning in terms of HFL, VFL, and FTL. Privacy measures for federated learning include differential privacy to reduce the leakage of training data from the model, secure computation to keep the model computation process secret between clients and servers, encryption of communications to prevent information leakage to third parties, and ID dummying to prevent ID leakage.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This R&amp;D includes the results of ” Research and development of optimized AI technology by secure data coordination (JPMI00316)” by the Ministry of Internal Affairs and Communications (MIC), Japan.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, vol. 10, no. 2,
2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-Efficient Learning of Deep Networks from Decentralized
Data,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, ser. Proceedings of Machine Learning
Research, vol. 54.   PMLR, 2017, pp.
1273–1282.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E. T. Martínez Beltrán, M. Q. Pérez, P. M. S. Sánchez, S. L. Bernal,
G. Bovet, M. G. Pérez, G. M. Pérez, and A. H. Celdrán, “Decentralized
federated learning: Fundamentals, state of the art, frameworks, trends, and
challenges,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, vol. 25,
no. 4, pp. 2983–3013, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L. Zhao, L. Ni, S. Hu, Y. Chen, P. Zhou, F. Xiao, and L. Wu, “Inprivate
digging: Enabling tree-based distributed data mining with differential
privacy,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2018 - IEEE Conference on Computer
Communications</em>, 2018, pp. 2087–2095.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Q. Li, Z. Wen, and B. He, “Practical federated gradient boosting decision
trees,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, vol. 34, no. 04, pp. 4642–4649, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F. Wang, J. Ou, and H. Lv, “Gradient boosting forest: a two-stage ensemble
method enabling federated learning of gbdts,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Neural Information
Processing</em>, T. Mantoro, M. Lee, M. A. Ayu, K. W. Wong, and A. N. Hidayanto,
Eds.   Cham: Springer International
Publishing, 2021, pp. 75–86.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Gascón, P. Schoppmann, B. Balle, M. Raykova, J. Doerner, S. Zahur, and
D. Evans, “Secure linear regression on vertically partitioned datasets,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IACR Cryptol. ePrint Arch.</em>, vol. 2016, p. 892, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Hardy, W. Henecka, H. Ivey-Law, R. Nock, G. Patrini, G. Smith, and
B. Thorne, “Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1711.10677, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Nock, S. Hardy, W. Henecka, H. Ivey-Law, G. Patrini, G. Smith, and
B. Thorne, “Entity resolution and federated learning get a federated
resolution,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1803.04035, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Yang, B. Ren, X. Zhou, and L. Liu, “Parallel distributed logistic
regression for vertical federated learning without third-party coordinator,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1911.09824, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Q. Zhang, B. Gu, C. Deng, and H. Huang, “Secure bilevel asynchronous vertical
federated learning with backward updating,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, vol. 35, no. 12, pp.
10 896–10 904, May 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Cheng, T. Fan, Y. Jin, Y. Liu, T. Chen, D. Papadopoulos, and Q. Yang,
“Secureboost: A lossless federated learning framework,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE
Intelligent Systems</em>, vol. 36, no. 6, pp. 87–98, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Vaidya, C. Clifton, M. Kantarcioglu, and A. S. Patterson,
“Privacy-preserving decision trees over vertically partitioned data,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Knowl. Discov. Data</em>, vol. 2, no. 3, oct 2008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Wu, S. Cai, X. Xiao, G. Chen, and B. C. Ooi, “Privacy preserving vertical
federated learning for tree-based models,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>,
vol. 13, no. 12, p. 2090–2103, jul 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Liu, Z. Liu, Y. Liang, C. Meng, J. Zhang, and Y. Zheng, “Federated
forest,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, pp. 1–1, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Z. Tian, R. Zhang, X. Hou, J. Liu, and K. Ren, “Federboost: Private federated
learning for GBDT,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2011.02796, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Hu, D. Niu, J. Yang, and S. Zhou, “Fdml: A collaborative machine learning
framework for distributed features,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, ser.
KDD ’19.   New York, NY, USA:
Association for Computing Machinery, 2019, p. 2232–2240.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Kang, X. Zhang, L. Li, Y. Cheng, T. Chen, M. Hong, and Q. Yang, “A
communication efficient collaborative learning framework for distributed
features,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1912.11187, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Romanini, A. J. Hall, P. Papadopoulos, T. Titcombe, A. Ismail, T. Cebere,
R. Sandmann, R. Roehm, and M. A. Hoeh, “Pyvertical: A vertical federated
learning framework for multi-headed splitnn,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2104.00489, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Q. He, W. Yang, B. Chen, Y. Geng, and L. Huang, “Transnet: Training
privacy-preserving neural network over transformed layer,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB
Endow.</em>, vol. 13, no. 12, p. 1849–1862, jul 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B. Gu, Z. Dang, X. Li, and H. Huang, “Federated doubly stochastic kernel
learning for vertically partitioned data,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery &amp; Data
Mining</em>.   New York, NY, USA:
Association for Computing Machinery, 2020, p. 2483–2493.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Xu, N. Baracaldo, Y. Zhou, A. Anwar, J. Joshi, and H. Ludwig, “Fedv:
Privacy-preserving federated learning over vertically partitioned data,”
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2103.03918, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Kang, C. Xing, T. Chen, and Q. Yang, “A secure federated transfer
learning framework,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>, vol. 35, no. 4, pp.
70–82, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. Sharma, C. Xing, Y. Liu, and Y. Kang, “Secure and efficient federated
transfer learning,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data
(Big Data)</em>, 2019, pp. 2569–2576.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
B. Zhang, C. Chen, and L. Wang, “Privacy-preserving transfer learning via
secure maximum mean discrepancy,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.11680</em>,
2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Gao, M. Gong, Y. Xie, A. K. Qin, K. Pan, and Y.-S. Ong, “Multiparty dual
learning,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cybernetics</em>, vol. 53, no. 5, pp.
2955–2968, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. Gao, Y. Liu, A. Huang, C. Ju, H. Yu, and Q. Yang, “Privacy-preserving
heterogeneous federated transfer learning,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International
Conference on Big Data (Big Data)</em>, 2019, pp. 2552–2559.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Mori, I. Teranishi, and R. Furukawa, “Continual horizontal federated
learning for heterogeneous data,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2022 International Joint
Conference on Neural Networks (IJCNN)</em>, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Gao, C. Ju, X. Wei, Y. Liu, T. Chen, and Q. Yang, “Hhhfl: Hierarchical
heterogeneous horizontal federated learning for electroencephalography,”
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05784</em>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Rakotomamonjy, M. Vono, H. J. M. Ruiz, and L. Ralaivola, “Personalised
federated learning on heterogeneous feature spaces,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2301.11447</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
E. Hallaji, R. Razavi-Far, and M. Saif, <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Federated and Transfer Learning:
A Survey on Adversaries and Defense Mechanisms</em>.   Cham: Springer International Publishing, 2023, pp. 29–55.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Hardy, W. Henecka, H. Ivey-Law, R. Nock, G. Patrini, G. Smith, and
B. Thorne, “Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1711.10677</em>, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for
privacy-preserving machine learning,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>, ser. CCS
’17.   New York, NY, USA: Association
for Computing Machinery, 2017, p. 1175–1191.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-preserving
machine learning,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy
(SP)</em>, 2017, pp. 19–38.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Araki, J. Furukawa, Y. Lindell, A. Nof, and K. Ohara, “High-throughput
semi-honest secure three-party computation with an honest majority,” in
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security</em>, ser. CCS ’16.   New York, NY, USA: Association for Computing Machinery, 2016, p.
805–817.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>,
ser. CCS ’16.   New York, NY, USA:
Association for Computing Machinery, 2016, p. 308–318.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
learning: A client level perspective,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.07557</em>, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Liu, X. Zhang, and L. Wang, “Asymmetrical vertical federated learning,”
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.07427</em>, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.00341" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.00342" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.00342">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.00342" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.00344" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 19:09:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
