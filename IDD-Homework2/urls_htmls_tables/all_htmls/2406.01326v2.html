<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.01326] TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy</title><meta property="og:description" content="Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension.
Previous methods generally design task-specific architectures and objectives fâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.01326">

<!--Generated on Fri Jul  5 23:54:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Weichao Zhao<sup id="id15.15.id1" class="ltx_sup"><span id="id15.15.id1.1" class="ltx_text ltx_font_italic">1,2,â™ ,</span></sup>
Â Â Â Hao Feng<sup id="id16.16.id2" class="ltx_sup"><span id="id16.16.id2.1" class="ltx_text ltx_font_italic">1,âˆ—</span></sup>
Â Â Â Qi Liu<sup id="id17.17.id3" class="ltx_sup"><span id="id17.17.id3.1" class="ltx_text ltx_font_italic">2,âˆ—</span></sup>
Â Â Â Jingqun Tang<sup id="id18.18.id4" class="ltx_sup"><span id="id18.18.id4.1" class="ltx_text ltx_font_italic">2</span></sup>
Â Â Â Shu Wei<sup id="id19.19.id5" class="ltx_sup"><span id="id19.19.id5.1" class="ltx_text ltx_font_italic">2</span></sup>
Â Â Â Binghong Wu<sup id="id20.20.id6" class="ltx_sup"><span id="id20.20.id6.1" class="ltx_text ltx_font_italic">2</span></sup>



<br class="ltx_break">
Lei Liao<sup id="id21.21.id7" class="ltx_sup"><span id="id21.21.id7.1" class="ltx_text ltx_font_italic">2</span></sup>
Â Â Â Yongjie Ye<sup id="id22.22.id8" class="ltx_sup"><span id="id22.22.id8.1" class="ltx_text ltx_font_italic">2</span></sup>
Â Â Â Hao Liu<sup id="id23.23.id9" class="ltx_sup"><span id="id23.23.id9.1" class="ltx_text ltx_font_italic">2,â€¡,</span></sup>
Â Â Â Houqiang Li<sup id="id24.24.id10" class="ltx_sup">1,</sup><sup id="id25.25.id11" class="ltx_sup"><span id="id25.25.id11.1" class="ltx_text ltx_font_italic">â€ </span></sup>
Â Â Â Can Huang<sup id="id26.26.id12" class="ltx_sup">2</sup>

<br class="ltx_break"><sup id="id27.27.id13" class="ltx_sup"><span id="id27.27.id13.1" class="ltx_text ltx_font_italic" style="font-size:90%;">1</span></sup><span id="id14.14.1" class="ltx_text" style="font-size:90%;"> University of Science and Technology of China, <sup id="id14.14.1.1" class="ltx_sup"><span id="id14.14.1.1.1" class="ltx_text ltx_font_italic">2</span></sup> ByteDance Inc.,</span>

<br class="ltx_break"><span id="id28.28.id14" class="ltx_text" style="font-size:90%;">{saruka, haof}@mail.ustc.edu.cn, lihq@ustc.edu.cn

<br class="ltx_break">{liuqi.nero, haoliu.0128, can.huang}@bytedance.com
</span>
</span><span class="ltx_author_notes">Equal contribution. <math id="id2.2.m1.1" class="ltx_Math" alttext="\spadesuit" display="inline"><semantics id="id2.2.m1.1a"><mi mathvariant="normal" id="id2.2.m1.1.1" xref="id2.2.m1.1.1.cmml">â™ </mi><annotation-xml encoding="MathML-Content" id="id2.2.m1.1b"><ci id="id2.2.m1.1.1.cmml" xref="id2.2.m1.1.1">â™ </ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m1.1c">\spadesuit</annotation></semantics></math><span id="id29.29.id1" class="ltx_text">Interns at ByteDance.</span><math id="id3.3.m2.1" class="ltx_Math" alttext="{\ddagger}" display="inline"><semantics id="id3.3.m2.1a"><mo id="id3.3.m2.1.1" xref="id3.3.m2.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="id3.3.m2.1b"><ci id="id3.3.m2.1.1.cmml" xref="id3.3.m2.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m2.1c">{\ddagger}</annotation></semantics></math><span id="id30.30.id2" class="ltx_text">Project lead.</span>âœ‰Â Corresponding authors: Houqiang Li and Hao Liu.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id31.id1" class="ltx_p">Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension.
Previous methods generally design task-specific architectures and objectives for individual tasks, resulting in modal isolation and intricate workflows.
In this paper, we present a novel large vision-language model, TabPedia, equipped with a <span id="id31.id1.1" class="ltx_text ltx_font_italic">concept synergy</span> mechanism. In this mechanism, all the involved diverse visual table understanding (VTU) tasks and multi-source visual embeddings are abstracted as concepts. This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs). Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings. Furthermore, to better evaluate the VTU task in real-world scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia. The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy. The benchmark ComTQA has been open-sourced at <a target="_blank" href="https://huggingface.co/datasets/ByteDance/ComTQA" title="" class="ltx_ref ltx_href">https://huggingface.co/datasets/ByteDance/ComTQA</a>. The source code and model will be released later.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the rapid advancement of digital technology, numerous paper documents must be converted into electronic formats for efficient storage and utilization. Tables, as indispensable components of documents, play a vital role in summarizing facts and quantitative data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The compact yet informative nature of tables makes them advantageous for various applications, thereby attracting widespread research attention toward Visual Table Understanding (VTU). VTU generally encompasses four subtasks: <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Table Detection</span> (TD), which locates tables within document images; <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Table Structure Recognition</span> (TSR), which parses the structure of tables in table-centric images; <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">Table Querying</span> (TQ), which recognizes the structure of a table from an entire image at a given location, a task that remains underexplored in the previous works; and <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">Table Question Answering</span> (TQA), which answers questions based on table contents. These tasks pose challenges from various perspectives due to the need for representations at different visual-semantic granularities and hierarchies.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the success achieved, many pioneering works have mainly centered on the specific subtask with various task-specific architectures, as shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a). For visual table perception tasks such as TD and TSR, one of most adopted approaches is in the detection mannerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In contrast, generative vision-language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are often employed to generate answers conditioned on the semantic content of tables for TQA task. Specifically, Vision Transformers (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> pretrained on CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or EVA-CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Swin-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and similar models serve as vision encoders, while language models operate in either encoder-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> or decoder-only frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Besides, recent fast-growing Large Vision Language ModelsÂ (LVLMs)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have shown their powerful capabilities to perceive and understand visual clues by integrating instruction following of Large Language ModelsÂ (LLMs)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Despite impressive progress, the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">status quo</span> begs for a question: â€œ<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Can we leverage the advantages of LVLMs to solve all the VTU tasks once and for all?</span>â€</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A straightforward solution would be to train the LVLM directly using all the VTU data. However, aside from the diverse table structure and the various relations of table contents, it remains a nontrivial issue due to two cruxes of table parsing and understanding: (i)Â discrepancy between the representation formats Â (two-dimensional structure VS. one-dimensional sequence); (ii)Â required image resolutions. Although some worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> represent table structure in markup formats like HTML, XML, Markdown, or LATEX. However, they neglect spatial coordinates for cells and only encode logical relationships implicitly. The generated code contains extensive formatted information from different markup languages, increasing output length and potentially causing parsing issues with illegal grammars.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To attack above issues, we in this paper propose a novel LVLM tailored for comprehensive VTU, TabPedia, to effectively solve all VTU tasks in a unified framework, as shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>Â (b). More concretely, we employ dual vision encoders, namely ViT-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and Swin-BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, to encode the global and fine-grained local information in the low- and high-resolution formats of the input image respectively, acquiring multi-source visual embeddings. Here, all the involved VTU tasks and multi-source visual embeddings are abstracted as <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">concepts</span> and <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">concept synergy</span> mechanism is implemented by introducing the <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">mediative tokens</span> to the LLM in our model. Thanks to this mechanism, all the concepts in TabPedia can work in synergy flexibly. Quantitative and qualitative experimental results on both table perception and comprehension tasks across various public benchmarks confirm the effectiveness of our proposed TabPedia. To further investigate the potential of our model in more challenging and realistic scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring round 1,500 images and 9,000 QA pairs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions are summarized as follows,</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel large vision-language model, TabPedia, to integrate various VTU tasks into a unified framework, including TD, TSR, TQ and TQA. Specifically, TabPedia fully leverages the comprehensive capabilities of LLMs to fertilize complex table understanding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We design a concept synergy mechanism to harmonize both table perception and comprehension tasks.
Through introducing the meditative tokens into our framework, TabPedia adaptively enables useful information in multi-source visual embeddings and task instructions, generating accurate and plausible responses.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive quantitative and qualitative experiments validate the effectiveness of our proposed TabPedia across various tasks and benchmarks. To further exploit the potential of our model in more complex scenarios, we build a new table VQA benchmark, ComTQA, involving multiple answers, mathematical calculation and logical reasoning, <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">etc</span>.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.01326/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Table Recognition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Table recognition is generally divided into table detection, table structure recognition and table content recognition
In our work, table content recognition is beyond our scope.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">For TD task, the earliest approaches are rule-based methods for locating tables inside documentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
With the rapid advances in deep learning, numerous CNN-based methods show impressive performance.
Most of these methods directly adopt top-down object detection frameworks to solve this problemÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
For instance, SunÂ <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> adopt Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> to detect table boxes and the
corresponding corner boxes simultaneously, and then adjust table boundaries according to the detected corners.
Some other methods model each document image as a graph and formulate TD as a graph labeling problemÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
In addition, TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> first applies the transformer-based detector, DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, to improve the detection accuracy without special customization.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">For TSR task, one of the most common modeling approaches is still to regard it as some form of object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Among them, DeepDeSRTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and TableNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> are both representative works exploring semantic segmentation to obtain table cell boundaries.
TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> first proposes to utilize DETR for this task.
TSRFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> introduces a cross-attention module into the DETR framework to improve the localization accuracy of row/column separators.
Some other methods attempt to parse table structure via modeling relationship among different table elementsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.
As the most relevant to our approach, markup generation-based methods directly generate markup (HTML or LaTeX) sequences from raw table imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. EDDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> introduces a cell decoder and a structures decoder to generate HTML codes. OmniParserÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> further integrates three task-specific decoders to enhance the table structure representation.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">While the previous methods have achieved promising results on table perceptive tasks, they are still limited in table intricate content understanding.
In our work, we jointly exploit table perception and comprehension tasks in a unified framework, concurrently enriching visual table understanding.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Large Vision-Language Models</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">LVLMs aim to equip LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> with visual comprehension capability.
The mainstream approaches attempt to connect visual encoders and LLMs with intermediate modules such as simple ProjectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, QFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Perceiver ResamplersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, achieving visual language understanding through pre-training alignment and instruction fine-tuning.
For text-rich document scene, several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> propose to enhance the LVLMsâ€™ capabilities in understanding textual elementsÂ (text-centric VQA, OCR, text spotting, <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">etc.</span>).
Among them, TextMonkeyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> employs shifted window attention and token resampler module to improve the training process.
DocOwl-1.5Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> collects a comprehensive dataset DocStruct4M to support unified structure learning.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Despite achieving extraordinary progress on visual understanding, existing LVLMs still face challenges in two-dimensional table parsing and understanding. In this paper, we propose a unified framework to concurrently achieve table perception and comprehension with the support of LLMs.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Additional Tokens</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the trend of Transformer-based approaches, extending the input sequence with special tokens is popularized for various intentions, such as extracting task-specific informationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, providing extra informationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> or improving model performanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.
For instance, ViTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> utilizes [CLS] token for classification.
Similarly, DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> proposes object queries for detection.
ATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> adopts tape tokens to obtain useful information from a memory bank.
In addition, the Memory TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> presents a simple approach to improve translation performance by attaching trainable memory tokens after the token sequence. DarcetÂ <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.,</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> further attempt to add extra tokens in ViT-based frameworks, <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and DINOv2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, thus improving visual tasks.
In our work, we inherit this spirit and design meditative tokens to enhance TabPediaâ€™s perceptive and comprehensive capability for visual tables.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.01326/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">The illustration of our proposed TabPedia. Given the input image, TabPedia feeds it into both vision encoders attached projections to extract different granular features. Then, the visual tokens are combined with instruction-derived tokens, and fed into the LLM. The LLM leverages its powerful understanding ability to generate a plausible response.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As shown in FigÂ <a href="#S2.F2" title="Figure 2 â€£ 2.3 Additional Tokens â€£ 2 Related Work â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present an overview of TabPedia. The overall training pipeline consists of two phases. Concretely, the pre-training stage aims to align the visual features to the large language model, and the fine-tuning stage focuses on visual table-aware understanding. In the following, we elaborate on the architecture of TabPedia, followed by the exposition of its two training phases.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p"><span id="S3.SS1.p1.7.1" class="ltx_text ltx_font_bold">High-Resolution Vision Encoder.</span> As proved by previous methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, the high-resolution image is critical to ensuring that the LLMs could grasp rich visual information. Following DonutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, we adopt Swin-BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to encode the high-resolution format of input image.
Given the input RGB image <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathit{I}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathit{I}</annotation></semantics></math>, we first resize it to pre-defined high-resolution scale of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{H}\times\mathrm{W}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">Ã—</mo><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">H</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">W</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathrm{H}\times\mathrm{W}</annotation></semantics></math>, denoted as <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathit{I}_{h}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathit{I}_{h}</annotation></semantics></math>.
By default, both <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathrm{H}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi mathvariant="normal" id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">H</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathrm{H}</annotation></semantics></math> and <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathrm{W}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi mathvariant="normal" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">W</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathrm{W}</annotation></semantics></math> are set to 2,560 and 1,920, respectively.
Notably, we maintain the aspect ratio during the resizing process to prevent distortion of table contents and structures.
Then, the resized image <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathit{I}_{h}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathit{I}_{h}</annotation></semantics></math> is fed into the vanilla Swin Transformer initialized fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to obtain a feature map <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathit{V}_{h}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">V</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘‰</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\mathit{V}_{h}</annotation></semantics></math> downsampled by a factor of 1/32, each token with 1,024 dimension.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p"><span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_bold">Low-Resolution Vision Encoder.</span> To keep the overall layout information, the raw image is also resized to a low-resolution one denoted as <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathit{I}_{l}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ¼</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathit{I}_{l}</annotation></semantics></math>. We choose the pre-trained CLIP visual encoder ViT-L/14Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to encode the low-resolution image with <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">224</cn><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">224\times 224</annotation></semantics></math>. The output sequenceÂ <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathit{V}_{l}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğ‘‰</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathit{V}_{l}</annotation></semantics></math> is composed of 256 tokens, each with 512 dimension.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.5" class="ltx_p"><span id="S3.SS1.p3.5.1" class="ltx_text ltx_font_bold">Projections.</span> The projections are designed to align visual tokens with the input token dimension of the subsequent large language modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>.
For the high-resolution feature map <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathit{V}_{h}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">V</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğ‘‰</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathit{V}_{h}</annotation></semantics></math>, due to the limitation of input text length, we employ a 2D convolutional layer with a kernel size of 3 and a stride of 2, and then flatten it into <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\frac{\mathrm{H}}{64}\times\frac{\mathrm{W}}{64}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mfrac id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml"><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.2.2" xref="S3.SS1.p3.2.m2.1.1.2.2.cmml">H</mi><mn id="S3.SS1.p3.2.m2.1.1.2.3" xref="S3.SS1.p3.2.m2.1.1.2.3.cmml">64</mn></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">Ã—</mo><mfrac id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">W</mi><mn id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml">64</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></times><apply id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2"><divide id="S3.SS1.p3.2.m2.1.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.1.2"></divide><ci id="S3.SS1.p3.2.m2.1.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2.2">H</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.1.2.3">64</cn></apply><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><divide id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3"></divide><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">W</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3">64</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\frac{\mathrm{H}}{64}\times\frac{\mathrm{W}}{64}</annotation></semantics></math> tokens, denoted as <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathit{\hat{V}}_{h}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mover accent="true" id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2.2" xref="S3.SS1.p3.3.m3.1.1.2.2.cmml">V</mi><mo id="S3.SS1.p3.3.m3.1.1.2.1" xref="S3.SS1.p3.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><apply id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2"><ci id="S3.SS1.p3.3.m3.1.1.2.1.cmml" xref="S3.SS1.p3.3.m3.1.1.2.1">^</ci><ci id="S3.SS1.p3.3.m3.1.1.2.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathit{\hat{V}}_{h}</annotation></semantics></math>.
For the low-resolution visual features <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathit{V}_{l}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">V</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">ğ‘‰</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\mathit{V}_{l}</annotation></semantics></math>, inspired from the paradigm of advanced LVLMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we adopt a linear layer to project visual tokens, denoted as <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathit{\hat{V}}_{l}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mover accent="true" id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2.2" xref="S3.SS1.p3.5.m5.1.1.2.2.cmml">V</mi><mo id="S3.SS1.p3.5.m5.1.1.2.1" xref="S3.SS1.p3.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2"><ci id="S3.SS1.p3.5.m5.1.1.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.2.1">^</ci><ci id="S3.SS1.p3.5.m5.1.1.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\mathit{\hat{V}}_{l}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.8" class="ltx_p"><span id="S3.SS1.p4.8.1" class="ltx_text ltx_font_bold">Concept Synergy.</span> Given the massive visual tokens and the embedding of textual instruction <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{Q}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">Q</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathrm{Q}</annotation></semantics></math>, we utilize Vicuna-7BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> as LLM to generate its response.
Taking into account the discrepancy of table perception and comprehension tasks, we introduce <span id="S3.SS1.p4.8.2" class="ltx_text ltx_font_italic">meditative tokens</span> <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{M}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">M</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\mathrm{M}</annotation></semantics></math> to implement the concept synergy for the LLM, which adaptively enable different region of visual tokens and understand the intentions of specific task question. Finally, we construct the whole input sequence as <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathit{X}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mathit{X}</annotation></semantics></math> = [<math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\mathrm{Q}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mi mathvariant="normal" id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">Q</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\mathrm{Q}</annotation></semantics></math>, <span id="S3.SS1.p4.8.3" class="ltx_text ltx_font_typewriter">&lt;IMG_S&gt;</span> ;<math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="\mathit{\hat{V}}_{l}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mover accent="true" id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2.2" xref="S3.SS1.p4.5.m5.1.1.2.2.cmml">V</mi><mo id="S3.SS1.p4.5.m5.1.1.2.1" xref="S3.SS1.p4.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2"><ci id="S3.SS1.p4.5.m5.1.1.2.1.cmml" xref="S3.SS1.p4.5.m5.1.1.2.1">^</ci><ci id="S3.SS1.p4.5.m5.1.1.2.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\mathit{\hat{V}}_{l}</annotation></semantics></math> ; <span id="S3.SS1.p4.8.4" class="ltx_text ltx_font_typewriter">&lt;IMG_SEP&gt;</span> ; <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="\mathit{\hat{V}}_{h}" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><msub id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml"><mover accent="true" id="S3.SS1.p4.6.m6.1.1.2" xref="S3.SS1.p4.6.m6.1.1.2.cmml"><mi id="S3.SS1.p4.6.m6.1.1.2.2" xref="S3.SS1.p4.6.m6.1.1.2.2.cmml">V</mi><mo id="S3.SS1.p4.6.m6.1.1.2.1" xref="S3.SS1.p4.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.p4.6.m6.1.1.3" xref="S3.SS1.p4.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><apply id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">subscript</csymbol><apply id="S3.SS1.p4.6.m6.1.1.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2"><ci id="S3.SS1.p4.6.m6.1.1.2.1.cmml" xref="S3.SS1.p4.6.m6.1.1.2.1">^</ci><ci id="S3.SS1.p4.6.m6.1.1.2.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.SS1.p4.6.m6.1.1.3.cmml" xref="S3.SS1.p4.6.m6.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">\mathit{\hat{V}}_{h}</annotation></semantics></math> ; <span id="S3.SS1.p4.8.5" class="ltx_text ltx_font_typewriter">&lt;IMG_E&gt;</span> ; <math id="S3.SS1.p4.7.m7.1" class="ltx_math_unparsed" alttext="\mathrm{M}]" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><mrow id="S3.SS1.p4.7.m7.1b"><mi mathvariant="normal" id="S3.SS1.p4.7.m7.1.1">M</mi><mo stretchy="false" id="S3.SS1.p4.7.m7.1.2">]</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">\mathrm{M}]</annotation></semantics></math>, where <math id="S3.SS1.p4.8.m8.1" class="ltx_math_unparsed" alttext="[;]" display="inline"><semantics id="S3.SS1.p4.8.m8.1a"><mrow id="S3.SS1.p4.8.m8.1b"><mo stretchy="false" id="S3.SS1.p4.8.m8.1.1">[</mo><mo id="S3.SS1.p4.8.m8.1.2">;</mo><mo stretchy="false" id="S3.SS1.p4.8.m8.1.3">]</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">[;]</annotation></semantics></math> means the concatenation operation.
<span id="S3.SS1.p4.8.6" class="ltx_text ltx_font_typewriter">&lt;IMG_S&gt;</span>, <span id="S3.SS1.p4.8.7" class="ltx_text ltx_font_typewriter">&lt;IMG_E&gt;</span> and <span id="S3.SS1.p4.8.8" class="ltx_text ltx_font_typewriter">&lt;IMG_SEP&gt;</span> are learnable special tokens, that denote the start and end of visual tokens as well as the separation of different resolution tokens, respectively.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Objective.</span> Since TabPedia is trained to predict the next tokens like other LLMs, it is optimized by maximizing the likelihood of prediction loss at training time.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pre-training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To enable the capable of vision encoders to capture text-rich information from high-resolution images and aligning embedding space with the large language modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, we first perform extensive text-aware pre-training.
As shown in Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2.3 Additional Tokens â€£ 2 Related Work â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we jointly optimize the high-resolution visual encoder with both projectors, while freezing the large language model and low-resolution vision encoder.
Specifically, followed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, our pre-training procedure involves a variety of perception tasks, <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, text detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, spottingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, long-text readingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and image captioningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
The first four tasks focuses on the various document images, while the last one targets natural scene images. These comprehensive tasks endow the vision encoders of TabPedia to effectively perceive textual and visual information from both document and natural scene images.
More detailed pre-training settings about dataset and experiment could be referred toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Table-aware Fine-tuning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Through pre-training, TabPedia could well understand text and structure of diverse document images but cannot follow instructions to perform different table understanding tasks.
In order to enhance the model capability of instruction following, we <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">first</span> construct a large-scale dataset for visual table understanding.
We will elaborate on the dataset construction in the Sec.Â <a href="#S4" title="4 Dataset Construction â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Based on this dataset, we introduce four table-related tasks, <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span>, TDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, TSRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, TQ and TQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> to simultaneously cultivate the perception and comprehension capabilities.
In this stage, we further unfreeze the LLM and fine-tune the entire framework except the low-resolution vision encoder.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:203.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.T2.fig1.3.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S3.T2.fig1.4.2" class="ltx_text" style="font-size:113%;">Summary of training data statistics in the fine-tuning stage. </span></figcaption>
<div id="S3.T2.fig1.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:266.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(99.6pt,-61.2pt) scale(1.85043785380173,1.85043785380173) ;">
<table id="S3.T2.fig1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.fig1.5.1.1" class="ltx_tr">
<td id="S3.T2.fig1.5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></td>
<td id="S3.T2.fig1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Subset</span></td>
<td id="S3.T2.fig1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S3.T2.fig1.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Num</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.2" class="ltx_tr">
<td id="S3.T2.fig1.5.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;" rowspan="3"><span id="S3.T2.fig1.5.1.2.1.1" class="ltx_text" style="font-size:80%;">PubTab1M</span></td>
<td id="S3.T2.fig1.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.2.2.1" class="ltx_text" style="font-size:80%;">PubTab1M-Det</span></td>
<td id="S3.T2.fig1.5.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.2.3.1" class="ltx_text" style="font-size:80%;">TD</span></td>
<td id="S3.T2.fig1.5.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.2.4.1" class="ltx_text" style="font-size:80%;">460k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.3" class="ltx_tr">
<td id="S3.T2.fig1.5.1.3.1" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.3.1.1" class="ltx_text" style="font-size:80%;">PubTab1M-Str</span></td>
<td id="S3.T2.fig1.5.1.3.2" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.3.2.1" class="ltx_text" style="font-size:80%;">TSR,TQA</span></td>
<td id="S3.T2.fig1.5.1.3.3" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.3.3.1" class="ltx_text" style="font-size:80%;">759k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.4" class="ltx_tr">
<td id="S3.T2.fig1.5.1.4.1" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.4.1.1" class="ltx_text" style="font-size:80%;">PubTab1M-Syn</span></td>
<td id="S3.T2.fig1.5.1.4.2" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.4.2.1" class="ltx_text" style="font-size:80%;">TQ</span></td>
<td id="S3.T2.fig1.5.1.4.3" class="ltx_td ltx_align_center" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.4.3.1" class="ltx_text" style="font-size:80%;">381k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.5" class="ltx_tr">
<td id="S3.T2.fig1.5.1.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.5.1.1" class="ltx_text" style="font-size:80%;">FinTabNet</span></td>
<td id="S3.T2.fig1.5.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.5.2.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S3.T2.fig1.5.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.5.3.1" class="ltx_text" style="font-size:80%;">TSR,TQA</span></td>
<td id="S3.T2.fig1.5.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.5.4.1" class="ltx_text" style="font-size:80%;">78k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.6" class="ltx_tr">
<td id="S3.T2.fig1.5.1.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.6.1.1" class="ltx_text" style="font-size:80%;">PubTabNet</span></td>
<td id="S3.T2.fig1.5.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.6.2.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S3.T2.fig1.5.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.6.3.1" class="ltx_text" style="font-size:80%;">TSR</span></td>
<td id="S3.T2.fig1.5.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.6.4.1" class="ltx_text" style="font-size:80%;">434k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.7" class="ltx_tr">
<td id="S3.T2.fig1.5.1.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.7.1.1" class="ltx_text" style="font-size:80%;">WTQ</span></td>
<td id="S3.T2.fig1.5.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.7.2.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S3.T2.fig1.5.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.7.3.1" class="ltx_text" style="font-size:80%;">TQA</span></td>
<td id="S3.T2.fig1.5.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.7.4.1" class="ltx_text" style="font-size:80%;">1k</span></td>
</tr>
<tr id="S3.T2.fig1.5.1.8" class="ltx_tr">
<td id="S3.T2.fig1.5.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.8.1.1" class="ltx_text" style="font-size:80%;">TabFact</span></td>
<td id="S3.T2.fig1.5.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.8.2.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S3.T2.fig1.5.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.8.3.1" class="ltx_text" style="font-size:80%;">TQA</span></td>
<td id="S3.T2.fig1.5.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig1.5.1.8.4.1" class="ltx_text" style="font-size:80%;">9k</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:203.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.T2.fig2.3.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S3.T2.fig2.4.2" class="ltx_text" style="font-size:113%;">Different task types and their instruction examples.</span></figcaption>
<div id="S3.T2.fig2.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:225.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(99.8pt,-51.8pt) scale(1.85268173074698,1.85268173074698) ;">
<table id="S3.T2.fig2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.fig2.5.1.1" class="ltx_tr">
<td id="S3.T2.fig2.5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig2.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S3.T2.fig2.5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding:0.15pt 10.0pt;">
<span id="S3.T2.fig2.5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.fig2.5.1.1.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S3.T2.fig2.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Example</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.fig2.5.1.2" class="ltx_tr">
<td id="S3.T2.fig2.5.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig2.5.1.2.1.1" class="ltx_text" style="font-size:80%;">TD</span></td>
<td id="S3.T2.fig2.5.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.15pt 10.0pt;">
<span id="S3.T2.fig2.5.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.fig2.5.1.2.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S3.T2.fig2.5.1.2.2.1.1.1" class="ltx_text" style="font-size:80%;">â€œGive me the areas where table elementâ€™s locations in this picture.â€</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.fig2.5.1.3" class="ltx_tr">
<td id="S3.T2.fig2.5.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig2.5.1.3.1.1" class="ltx_text" style="font-size:80%;">TSR</span></td>
<td id="S3.T2.fig2.5.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.15pt 10.0pt;">
<span id="S3.T2.fig2.5.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.fig2.5.1.3.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S3.T2.fig2.5.1.3.2.1.1.1" class="ltx_text" style="font-size:80%;">â€œParse the structural information of the cropped table in this picture.â€</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.fig2.5.1.4" class="ltx_tr">
<td id="S3.T2.fig2.5.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig2.5.1.4.1.1" class="ltx_text" style="font-size:80%;">TQ</span></td>
<td id="S3.T2.fig2.5.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.15pt 10.0pt;">
<span id="S3.T2.fig2.5.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.fig2.5.1.4.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S3.T2.fig2.5.1.4.2.1.1.1" class="ltx_text" style="font-size:80%;">â€œParse the table structure within the region [0.095, 0.673, 0.869, 0.851] in this picture.â€</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.fig2.5.1.5" class="ltx_tr">
<td id="S3.T2.fig2.5.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.15pt 10.0pt;"><span id="S3.T2.fig2.5.1.5.1.1" class="ltx_text" style="font-size:80%;">TQA</span></td>
<td id="S3.T2.fig2.5.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding:0.15pt 10.0pt;">
<span id="S3.T2.fig2.5.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.fig2.5.1.5.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S3.T2.fig2.5.1.5.2.1.1.1" class="ltx_text" style="font-size:80%;">â€œWhat was the lowest stock price in the fourth quarter of 2010?â€</span></span>
</span>
</td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Construction</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we aim to introduce the collected instruction following dataset.
The entire data is derived from five public datasets, including PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, PubTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, WikiTableQuestionsÂ (WTQ)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and TabFactÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>.
Among them, PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> contains two subsets, <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, PubTab1M-DetectionÂ (PubTab1M-Det) and PubTab1M-StructureÂ (PubTab1M-Str).
Moreover, since the table images in PubTab1M-Str are cropped from PubTab1M-Det, we transform the annotations of the table structure in PubTab1M-Str into the original images and synthesize a new subset PubTab1M-Syn, which could be utilized for TQ task.
The statistical data are summarized in Tab.Â <a href="#S3.T2" title="Table 2 â€£ 3.3 Table-aware Fine-tuning â€£ 3 Method â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
To ensure the instruction diversity, we generate multiple instructions for each task using GPT3.5Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. In Tab.Â <a href="#S3.T2" title="Table 2 â€£ 3.3 Table-aware Fine-tuning â€£ 3 Method â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we display one exemplar about userâ€™s question for each table task.
We will provide a detailed exposition of them in the following.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Table DetectionÂ (TD).</span> As a fundamental task, TD task targets to detect all table locations in a document image. Previous methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> mainly utilize DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or variants of R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> to predict numerous overlapping bboxes, that inevitably needs complex post-processing, such as non-maximization suppressionÂ (NMS), to generate final results. In contrast, we employ LLM to directly generate the locations of instance tables in the format of â€œ[x1, y1, x2, y2]â€, where x1, y1, x2, y2 represent the normalized coordinates of the top-left and bottom-right of the corresponding bbox.
Moreover, to facilitate detection results for multiple tables, we split multiple table positions with the special symbol â€œ\nâ€ in the output response.
We adopt PubTab1M-DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to perform TD task, where images are collected from PDF documents with different scale and rotation types of tables.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Table Structure RecognitionÂ (TSR).</span> The TSR targets to parse table structure in terms of rows, columns and cells. HTML and Markdown codes are mainly two kinds of text sequences used to represent a table. HTML could represent all kinds of tables, with or without cells spanning multiple rows and grids, but they contain massive markup grammarsÂ <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">i.e.</span>, â€œ<span id="S4.p3.1.3" class="ltx_text ltx_font_typewriter">&lt;div&gt;&lt;/div&gt;</span>â€ and â€œ<span id="S4.p3.1.4" class="ltx_text ltx_font_typewriter">&lt;td&gt;&lt;/td&gt;</span>â€, resulting in excessively lengthy output responses.
Compared with HTML, Markdown represents a table more succinctly, but it cannot represent cells spanning multiple rows or columns.
By weighing the simplicity of the output and the completeness of the table parsing, we propose a canonical table structure representation based on the detection format. Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we jointly adopt five object classes to model TSR, including <span id="S4.p3.1.5" class="ltx_text ltx_font_italic">table column</span>, <span id="S4.p3.1.6" class="ltx_text ltx_font_italic">table row</span>, <span id="S4.p3.1.7" class="ltx_text ltx_font_italic">table column header</span>, <span id="S4.p3.1.8" class="ltx_text ltx_font_italic">table projected row header</span> and <span id="S4.p3.1.9" class="ltx_text ltx_font_italic">table spanning cell</span>.
To better understanding, we display a representative sample in AppendixÂ <a href="#A2" title="Appendix B Annotation in TSR task â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.
Taking into account the serialized output of the LLM, we represent the table structure with a series of â€œ[object] [x1, y1, x2, y2]â€, which are also separated by â€œ\nâ€.
Notably, we standardize the order of the output objects to ensure uniqueness of the table parsing results.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We select the PubTab1M-StrÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and PubTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> to support the TSR task, where tables are collected from scientific and financial articles.
These datasets contain pairs of table images and HTML annotations.
We convert HTML codes into our designed annotation format using the pre-processing tool offered byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Table QueryingÂ (TQ).</span> Different from recognizing table structure from the cropped table-centric images in TSR task, the TQ task directly parses the table from the original document image based on the given table location.
This task is more challenging due to the degradation of the tableâ€™s resolution and the interference of other document contents around it.
Moreover, this task could potentially be combined with TD task to enable automatic parsing of all table structure information in original images.
Therefore, we introduce this task to fully unlock the comprehension capabilities of large language models for visual table understanding.
For the annotation of table parsing, we adopt the same
format as TSR.
Since there is no readily available dataset, we synthesize a large amount of available data based on the annotations from PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, namely PubTab1M-Syn.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Table Question AnsweringÂ (TQA).</span>
TQA aims to provide precise answers through table understanding and reasoning. For both public TQA datasets, <span id="S4.p6.1.2" class="ltx_text ltx_font_italic">i.e.</span>, WTQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and TabFactÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, the table images are collected from wikipedia tables with pairs of content-related question and answer. Thus, we could directly apply these available data to support this task.
However, the images of current TQA data are rendered from text-based tables with variations in background color and font size, resulting in poor generalization in real-world tables. In addition, the TQA data volume lags far behind other tasks.
To alleviate these obstacles, we generate numerous TQA data with partial images in FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> by employing the powerful multi-modal understanding capabilities of Gemini ProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>.
We provide more detailed descriptions of the procedure in the AppendixÂ <a href="#A1.SS1" title="A.1 QA Pairs Generation â€£ Appendix A More details about TQA datasets â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a></p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.2" class="ltx_p">To better evaluate TQA performance of various models on real-world table images, we build a complex TQA datasetÂ (ComTQA) based on test set of FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Compared to WTQ and TabFact, ComTQA has more challenging questions, such as multiple answers, mathematical calculations, and logical reasoning.
In total, we annotate <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.p7.1.m1.1a"><mo id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><csymbol cd="latexml" id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\sim</annotation></semantics></math>9k high-quality QA pairs from <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.p7.2.m2.1a"><mo id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><csymbol cd="latexml" id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\sim</annotation></semantics></math>1.5k images by expert annotation.
More statistics about ComTQA could be found in the AppendixÂ <a href="#A1.SS2" title="A.2 ComTQA Benchmark â€£ Appendix A More details about TQA datasets â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiment</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Parameter Settings.</span> For the hyper-parameters in model design, the number of meditative tokens is set to 256. The max length of text sequence is set to 4000 to satisfy task requirements.
To implement TabPedia, we adopt a cosine schedule with one-cycle learning rate strategyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
In the pre-training phase, the learning rate warms up in the first 2% of the training process and then decreases from the peak rateÂ (1e-3) with batch sizes of 64.
In the fine-tuning phase, we set the peak learning rate as 5e-6 with batch sizes of 16.
We employ the AdamW optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> in both phases.
All experiments are implemented by PyTorchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> and trained on 16<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><times id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\times</annotation></semantics></math> A100 GPUs.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Datasets.</span>
In order to comprehensively evaluate the capability of TabPedia, we employ multiple benchmarks for each task.
For performance assessment, we set the temperature parameter as 0.2 in both quantitative and qualitative evaluations.
For TD task, PubTab1M-DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> contains 57,125 images for testing.
For TSR task, FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, PubTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and PubTab1M-StrÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> are adopted for evaluation with 9,289, 9,115 and 93,834 testing samples, respectively.
For TQ task, the synthetic dataset PubTab1M-SynÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> also provides 47,186 samples for testing.
For TQA task, WTQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, TabFactÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and our annotated ComTQA contain 4,343, 12,722 and 9,070 QA pairs, respectively.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.3" class="ltx_p"><span id="S5.SS1.p3.3.1" class="ltx_text ltx_font_bold">Evaluation Metrics.</span>
For TD task, we report the results with object detection metrics, including precision, recall and f1-score with IoU@0.75.
For both TSR and TQ tasks, we utilize Structure Tree-EditDistance-based Similarity (S-TEDS)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, which evaluates table similarity of structural aspects in HTML format. The metric represents the HTML table as a tree, and the TEDS score is computed through the tree-edit distance between the ground truth and predicted trees. In order to convert the results of TabPedia into HTML format, we employ the post-processing algorithm provided byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Moreover, we report the recently proposed GriTS metricsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> for PubTab1M-Str to align its original metric.
Different from S-TEDS, GriTS represents tables as matrices, better capturing the two-dimensional structure and the orders of cells in a table. Further, GriTS enables TSR to be assessed from multiple perspectives, with <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{GriTS_{Top}}" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><msub id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">GriTS</mi><mi id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">Top</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">GriTS</ci><ci id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">Top</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\mathrm{GriTS_{Top}}</annotation></semantics></math> measuring cell topology recognition, <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathrm{GriTS_{Cont}}" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><msub id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">GriTS</mi><mi id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">Cont</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">GriTS</ci><ci id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3">Cont</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\mathrm{GriTS_{Cont}}</annotation></semantics></math> measuring cell content recognition, and <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathrm{GriTS_{Loc}}" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><msub id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">GriTS</mi><mi id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml">Loc</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">GriTS</ci><ci id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3">Loc</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\mathrm{GriTS_{Loc}}</annotation></semantics></math> measuring cell location recognition.
For TQA task, we adopt the accuracy metric where the response generated by the model is judged correct if it contains the string present in the ground truthÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T4.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:242.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.T4.fig1.6.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S5.T4.fig1.7.2" class="ltx_text" style="font-size:113%;">Comparison with the existing best table detection model TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. NMS denotes Non-Maximum Suppression.</span></figcaption>
<div id="S5.T4.fig1.8" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:103.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(27.6pt,-6.6pt) scale(1.14607096200293,1.14607096200293) ;">
<table id="S5.T4.fig1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.fig1.8.1.1" class="ltx_tr">
<td id="S5.T4.fig1.8.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;" rowspan="2"><span id="S5.T4.fig1.8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S5.T4.fig1.8.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;" rowspan="2"><span id="S5.T4.fig1.8.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Backbone</span></td>
<td id="S5.T4.fig1.8.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;" rowspan="2"><span id="S5.T4.fig1.8.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">NMS</span></td>
<td id="S5.T4.fig1.8.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;" colspan="3"><span id="S5.T4.fig1.8.1.1.4.1" class="ltx_text" style="font-size:80%;">IoU@0.75</span></td>
</tr>
<tr id="S5.T4.fig1.8.1.2" class="ltx_tr">
<td id="S5.T4.fig1.8.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Precision</span></td>
<td id="S5.T4.fig1.8.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Recall</span></td>
<td id="S5.T4.fig1.8.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1</span></td>
</tr>
<tr id="S5.T4.fig1.8.1.3" class="ltx_tr">
<td id="S5.T4.fig1.8.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;" rowspan="2"><span id="S5.T4.fig1.8.1.3.1.1" class="ltx_text" style="font-size:80%;">TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></td>
<td id="S5.T4.fig1.8.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.3.2.1" class="ltx_text" style="font-size:80%;">Faster R-CNN</span></td>
<td id="S5.T4.fig1.8.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.3.3.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T4.fig1.8.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.3.4.1" class="ltx_text" style="font-size:80%;">92.7</span></td>
<td id="S5.T4.fig1.8.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.3.5.1" class="ltx_text" style="font-size:80%;">86.6</span></td>
<td id="S5.T4.fig1.8.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.3.6.1" class="ltx_text" style="font-size:80%;">89.5</span></td>
</tr>
<tr id="S5.T4.fig1.8.1.4" class="ltx_tr">
<td id="S5.T4.fig1.8.1.4.1" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.4.1.1" class="ltx_text" style="font-size:80%;">DETR</span></td>
<td id="S5.T4.fig1.8.1.4.2" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.4.2.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T4.fig1.8.1.4.3" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.8</span></td>
<td id="S5.T4.fig1.8.1.4.4" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.4.4.1" class="ltx_text" style="font-size:80%;">98.1</span></td>
<td id="S5.T4.fig1.8.1.4.5" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.4</span></td>
</tr>
<tr id="S5.T4.fig1.8.1.5" class="ltx_tr">
<td id="S5.T4.fig1.8.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabPedia</span></td>
<td id="S5.T4.fig1.8.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.2.1" class="ltx_text" style="font-size:80%;">LVLM</span></td>
<td id="S5.T4.fig1.8.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.3.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T4.fig1.8.1.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.4.1" class="ltx_text" style="font-size:80%;">98.5</span></td>
<td id="S5.T4.fig1.8.1.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.4</span></td>
<td id="S5.T4.fig1.8.1.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="S5.T4.fig1.8.1.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.4</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T4.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:160.4pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.T4.3.10.2.1" class="ltx_text" style="font-size:113%;">Table 4</span>: </span><span id="S5.T4.2.2.1" class="ltx_text" style="font-size:113%;">Comparison with end-to-end TSR methods on two datasets. â€œ<math id="S5.T4.2.2.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S5.T4.2.2.1.m1.1b"><mo id="S5.T4.2.2.1.m1.1.1" xref="S5.T4.2.2.1.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.1.m1.1c"><ci id="S5.T4.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.1.m1.1d">\ast</annotation></semantics></math>â€ represents the results reported byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</span></figcaption>
<div id="S5.T4.3.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:152.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.1pt,-22.2pt) scale(1.41051231664164,1.41051231664164) ;">
<table id="S5.T4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.3.1.2" class="ltx_tr">
<td id="S5.T4.3.3.1.2.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T4.3.3.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S5.T4.3.3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T4.3.3.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Input Size</span></td>
<td id="S5.T4.3.3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTabNet</span></td>
<td id="S5.T4.3.3.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinTabNet</span></td>
</tr>
<tr id="S5.T4.3.3.1.3" class="ltx_tr">
<td id="S5.T4.3.3.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
<td id="S5.T4.3.3.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.3.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
</tr>
<tr id="S5.T4.3.3.1.1" class="ltx_tr">
<td id="S5.T4.3.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;">
<span id="S5.T4.3.3.1.1.1.1" class="ltx_text" style="font-size:80%;">DonutÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.3.3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S5.T4.3.3.1.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><sup id="S5.T4.3.3.1.1.1.4" class="ltx_sup"><span id="S5.T4.3.3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T4.3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.1.2.1" class="ltx_text" style="font-size:80%;">1,280</span></td>
<td id="S5.T4.3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.1.3.1" class="ltx_text" style="font-size:80%;">25.28</span></td>
<td id="S5.T4.3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.1.4.1" class="ltx_text" style="font-size:80%;">30.66</span></td>
</tr>
<tr id="S5.T4.3.3.1.4" class="ltx_tr">
<td id="S5.T4.3.3.1.4.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T4.3.3.1.4.1.1" class="ltx_text" style="font-size:80%;">EDDÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.3.3.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib64" title="" class="ltx_ref">64</a><span id="S5.T4.3.3.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T4.3.3.1.4.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.4.2.1" class="ltx_text" style="font-size:80%;">512</span></td>
<td id="S5.T4.3.3.1.4.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.4.3.1" class="ltx_text" style="font-size:80%;">89.90</span></td>
<td id="S5.T4.3.3.1.4.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.4.4.1" class="ltx_text" style="font-size:80%;">90.60</span></td>
</tr>
<tr id="S5.T4.3.3.1.5" class="ltx_tr">
<td id="S5.T4.3.3.1.5.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T4.3.3.1.5.1.1" class="ltx_text" style="font-size:80%;">OmniParserÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T4.3.3.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S5.T4.3.3.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T4.3.3.1.5.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.5.2.1" class="ltx_text" style="font-size:80%;">1,024</span></td>
<td id="S5.T4.3.3.1.5.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.5.3.1" class="ltx_text" style="font-size:80%;">90.45</span></td>
<td id="S5.T4.3.3.1.5.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.5.4.1" class="ltx_text" style="font-size:80%;">91.55</span></td>
</tr>
<tr id="S5.T4.3.3.1.6" class="ltx_tr">
<td id="S5.T4.3.3.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.6.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabPedia</span></td>
<td id="S5.T4.3.3.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.6.2.1" class="ltx_text" style="font-size:80%;">2,560</span></td>
<td id="S5.T4.3.3.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.41</span></td>
<td id="S5.T4.3.3.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T4.3.3.1.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.11</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<figure id="S5.T8" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T8.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:242.8pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.T8.fig1.1.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T8.fig1.2.2" class="ltx_text" style="font-size:90%;">Quantitative results on two subsets of PubTab1MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, including PubTab1M-Str and PubTab1M-Syn.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T6" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T6.10.1.1" class="ltx_text" style="font-size:113%;">Table 6</span>: </span><span id="S5.T6.11.2" class="ltx_text" style="font-size:113%;">Comparison with the task-specific model, TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> on TSR task. â€œCroppedâ€ denotes utilizing cropped table-centric images.</span></figcaption>
<div id="S5.T6.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:94.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.9pt,-2.4pt) scale(1.05308828278215,1.05308828278215) ;">
<table id="S5.T6.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.3.3.4" class="ltx_tr">
<td id="S5.T6.3.3.4.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T6.3.3.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S5.T6.3.3.4.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T6.3.3.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Backbone</span></td>
<td id="S5.T6.3.3.4.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T6.3.3.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></td>
<td id="S5.T6.3.3.4.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T6.3.3.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">NMS</span></td>
<td id="S5.T6.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" colspan="4"><span id="S5.T6.3.3.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTab1M-Str</span></td>
</tr>
<tr id="S5.T6.3.3.3" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Top}}" display="inline"><semantics id="S5.T6.1.1.1.1.m1.1a"><msub id="S5.T6.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S5.T6.1.1.1.1.m1.1.1.2" xref="S5.T6.1.1.1.1.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T6.1.1.1.1.m1.1.1.3" xref="S5.T6.1.1.1.1.m1.1.1.3.cmml">ğ“ğ¨ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><apply id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.1.1.1.1.m1.1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T6.1.1.1.1.m1.1.1.2.cmml" xref="S5.T6.1.1.1.1.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T6.1.1.1.1.m1.1.1.3.cmml" xref="S5.T6.1.1.1.1.m1.1.1.3">ğ“ğ¨ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\mathbf{GriTS_{Top}}</annotation></semantics></math></td>
<td id="S5.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T6.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Cont}}" display="inline"><semantics id="S5.T6.2.2.2.2.m1.1a"><msub id="S5.T6.2.2.2.2.m1.1.1" xref="S5.T6.2.2.2.2.m1.1.1.cmml"><mi mathsize="80%" id="S5.T6.2.2.2.2.m1.1.1.2" xref="S5.T6.2.2.2.2.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T6.2.2.2.2.m1.1.1.3" xref="S5.T6.2.2.2.2.m1.1.1.3.cmml">ğ‚ğ¨ğ§ğ­</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.m1.1b"><apply id="S5.T6.2.2.2.2.m1.1.1.cmml" xref="S5.T6.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.2.2.2.2.m1.1.1.1.cmml" xref="S5.T6.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T6.2.2.2.2.m1.1.1.2.cmml" xref="S5.T6.2.2.2.2.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T6.2.2.2.2.m1.1.1.3.cmml" xref="S5.T6.2.2.2.2.m1.1.1.3">ğ‚ğ¨ğ§ğ­</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.m1.1c">\mathbf{GriTS_{Cont}}</annotation></semantics></math></td>
<td id="S5.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T6.3.3.3.3.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Loc}}" display="inline"><semantics id="S5.T6.3.3.3.3.m1.1a"><msub id="S5.T6.3.3.3.3.m1.1.1" xref="S5.T6.3.3.3.3.m1.1.1.cmml"><mi mathsize="80%" id="S5.T6.3.3.3.3.m1.1.1.2" xref="S5.T6.3.3.3.3.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T6.3.3.3.3.m1.1.1.3" xref="S5.T6.3.3.3.3.m1.1.1.3.cmml">ğ‹ğ¨ğœ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.3.3.m1.1b"><apply id="S5.T6.3.3.3.3.m1.1.1.cmml" xref="S5.T6.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.3.3.3.3.m1.1.1.1.cmml" xref="S5.T6.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S5.T6.3.3.3.3.m1.1.1.2.cmml" xref="S5.T6.3.3.3.3.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T6.3.3.3.3.m1.1.1.3.cmml" xref="S5.T6.3.3.3.3.m1.1.1.3">ğ‹ğ¨ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.3.3.m1.1c">\mathbf{GriTS_{Loc}}</annotation></semantics></math></td>
<td id="S5.T6.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
</tr>
<tr id="S5.T6.3.3.5" class="ltx_tr">
<td id="S5.T6.3.3.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T6.3.3.5.1.1" class="ltx_text" style="font-size:80%;">TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></td>
<td id="S5.T6.3.3.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.2.1" class="ltx_text" style="font-size:80%;">Faster R-CNN</span></td>
<td id="S5.T6.3.3.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.3.1" class="ltx_text" style="font-size:80%;">Cropped</span></td>
<td id="S5.T6.3.3.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.4.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T6.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.5.1" class="ltx_text" style="font-size:80%;">86.16</span></td>
<td id="S5.T6.3.3.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.6.1" class="ltx_text" style="font-size:80%;">85.38</span></td>
<td id="S5.T6.3.3.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.7.1" class="ltx_text" style="font-size:80%;">72.11</span></td>
<td id="S5.T6.3.3.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.5.8.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
</tr>
<tr id="S5.T6.3.3.6" class="ltx_tr">
<td id="S5.T6.3.3.6.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.1.1" class="ltx_text" style="font-size:80%;">DETR</span></td>
<td id="S5.T6.3.3.6.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.2.1" class="ltx_text" style="font-size:80%;">Cropped</span></td>
<td id="S5.T6.3.3.6.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.3.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T6.3.3.6.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.46</span></td>
<td id="S5.T6.3.3.6.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">97.81</span></td>
<td id="S5.T6.3.3.6.6" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">97.81</span></td>
<td id="S5.T6.3.3.6.7" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">97.65</span></td>
</tr>
<tr id="S5.T6.3.3.7" class="ltx_tr">
<td id="S5.T6.3.3.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;">
<span id="S5.T6.3.3.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabPedia</span><span id="S5.T6.3.3.7.1.2" class="ltx_text" style="font-size:80%;">Â (TSR)</span>
</td>
<td id="S5.T6.3.3.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.2.1" class="ltx_text" style="font-size:80%;">LVLM</span></td>
<td id="S5.T6.3.3.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.3.1" class="ltx_text" style="font-size:80%;">Cropped</span></td>
<td id="S5.T6.3.3.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.4.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T6.3.3.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.5.1" class="ltx_text" style="font-size:80%;">96.52</span></td>
<td id="S5.T6.3.3.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.6.1" class="ltx_text" style="font-size:80%;">96.73</span></td>
<td id="S5.T6.3.3.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.7.1" class="ltx_text" style="font-size:80%;">95.54</span></td>
<td id="S5.T6.3.3.7.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T6.3.3.7.8.1" class="ltx_text" style="font-size:80%;">95.66</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T7" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T7.7.1.1" class="ltx_text" style="font-size:113%;">Table 7</span>: </span><span id="S5.T7.8.2" class="ltx_text" style="font-size:113%;">Quantitative results on both TQ and TD+TQ tasks.</span></figcaption>
<div id="S5.T7.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:113.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(79.8pt,-20.9pt) scale(1.5818719187692,1.5818719187692) ;">
<table id="S5.T7.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.3.3.4" class="ltx_tr">
<td id="S5.T7.3.3.4.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S5.T7.3.3.4.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Image</span></td>
<td id="S5.T7.3.3.4.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">NMS</span></td>
<td id="S5.T7.3.3.4.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S5.T7.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" colspan="4"><span id="S5.T7.3.3.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTab1M-Syn</span></td>
</tr>
<tr id="S5.T7.3.3.3" class="ltx_tr">
<td id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Top}}" display="inline"><semantics id="S5.T7.1.1.1.1.m1.1a"><msub id="S5.T7.1.1.1.1.m1.1.1" xref="S5.T7.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S5.T7.1.1.1.1.m1.1.1.2" xref="S5.T7.1.1.1.1.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T7.1.1.1.1.m1.1.1.3" xref="S5.T7.1.1.1.1.m1.1.1.3.cmml">ğ“ğ¨ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><apply id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.1.1.1.1.m1.1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T7.1.1.1.1.m1.1.1.2.cmml" xref="S5.T7.1.1.1.1.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T7.1.1.1.1.m1.1.1.3.cmml" xref="S5.T7.1.1.1.1.m1.1.1.3">ğ“ğ¨ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">\mathbf{GriTS_{Top}}</annotation></semantics></math></td>
<td id="S5.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T7.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Cont}}" display="inline"><semantics id="S5.T7.2.2.2.2.m1.1a"><msub id="S5.T7.2.2.2.2.m1.1.1" xref="S5.T7.2.2.2.2.m1.1.1.cmml"><mi mathsize="80%" id="S5.T7.2.2.2.2.m1.1.1.2" xref="S5.T7.2.2.2.2.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T7.2.2.2.2.m1.1.1.3" xref="S5.T7.2.2.2.2.m1.1.1.3.cmml">ğ‚ğ¨ğ§ğ­</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.2.m1.1b"><apply id="S5.T7.2.2.2.2.m1.1.1.cmml" xref="S5.T7.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.2.2.2.2.m1.1.1.1.cmml" xref="S5.T7.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T7.2.2.2.2.m1.1.1.2.cmml" xref="S5.T7.2.2.2.2.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T7.2.2.2.2.m1.1.1.3.cmml" xref="S5.T7.2.2.2.2.m1.1.1.3">ğ‚ğ¨ğ§ğ­</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.2.m1.1c">\mathbf{GriTS_{Cont}}</annotation></semantics></math></td>
<td id="S5.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><math id="S5.T7.3.3.3.3.m1.1" class="ltx_Math" alttext="\mathbf{GriTS_{Loc}}" display="inline"><semantics id="S5.T7.3.3.3.3.m1.1a"><msub id="S5.T7.3.3.3.3.m1.1.1" xref="S5.T7.3.3.3.3.m1.1.1.cmml"><mi mathsize="80%" id="S5.T7.3.3.3.3.m1.1.1.2" xref="S5.T7.3.3.3.3.m1.1.1.2.cmml">ğ†ğ«ğ¢ğ“ğ’</mi><mi mathsize="80%" id="S5.T7.3.3.3.3.m1.1.1.3" xref="S5.T7.3.3.3.3.m1.1.1.3.cmml">ğ‹ğ¨ğœ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.3.3.m1.1b"><apply id="S5.T7.3.3.3.3.m1.1.1.cmml" xref="S5.T7.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.3.3.3.3.m1.1.1.1.cmml" xref="S5.T7.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S5.T7.3.3.3.3.m1.1.1.2.cmml" xref="S5.T7.3.3.3.3.m1.1.1.2">ğ†ğ«ğ¢ğ“ğ’</ci><ci id="S5.T7.3.3.3.3.m1.1.1.3.cmml" xref="S5.T7.3.3.3.3.m1.1.1.3">ğ‹ğ¨ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.3.3.m1.1c">\mathbf{GriTS_{Loc}}</annotation></semantics></math></td>
<td id="S5.T7.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
</tr>
<tr id="S5.T7.3.3.5" class="ltx_tr">
<td id="S5.T7.3.3.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabPedia</span></td>
<td id="S5.T7.3.3.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Raw</span></td>
<td id="S5.T7.3.3.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T7.3.3.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">âœ—</span></td>
<td id="S5.T7.3.3.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.5.4.1" class="ltx_text" style="font-size:80%;">TQ</span></td>
<td id="S5.T7.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.5.5.1" class="ltx_text" style="font-size:80%;">96.04</span></td>
<td id="S5.T7.3.3.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.5.6.1" class="ltx_text" style="font-size:80%;">96.23</span></td>
<td id="S5.T7.3.3.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.5.7.1" class="ltx_text" style="font-size:80%;">94.95</span></td>
<td id="S5.T7.3.3.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.5.8.1" class="ltx_text" style="font-size:80%;">95.07</span></td>
</tr>
<tr id="S5.T7.3.3.6" class="ltx_tr">
<td id="S5.T7.3.3.6.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.6.1.1" class="ltx_text" style="font-size:80%;">TD+TQ</span></td>
<td id="S5.T7.3.3.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.6.2.1" class="ltx_text" style="font-size:80%;">94.54</span></td>
<td id="S5.T7.3.3.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.6.3.1" class="ltx_text" style="font-size:80%;">94.63</span></td>
<td id="S5.T7.3.3.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.6.4.1" class="ltx_text" style="font-size:80%;">93.25</span></td>
<td id="S5.T7.3.3.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 3.0pt;"><span id="S5.T7.3.3.6.5.1" class="ltx_text" style="font-size:80%;">93.38</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T8.13" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:164.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.T8.13.17.2.1" class="ltx_text" style="font-size:113%;">Table 8</span>: </span><span id="S5.T8.2.2.1" class="ltx_text" style="font-size:113%;">Comparison with existing LVLMs on TQA task. â€œ<math id="S5.T8.2.2.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S5.T8.2.2.1.m1.1b"><mo id="S5.T8.2.2.1.m1.1.1" xref="S5.T8.2.2.1.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S5.T8.2.2.1.m1.1c"><ci id="S5.T8.2.2.1.m1.1.1.cmml" xref="S5.T8.2.2.1.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.2.1.m1.1d">\ast</annotation></semantics></math>â€ denotes the results obtained through the open-source checkpoint or API of the closed-source model. ComTQA is our released new benchmark. The second best methods are underlined.</span></figcaption>
<div id="S5.T8.13.13" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:246.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(58.3pt,-33.1pt) scale(1.36779918326885,1.36779918326885) ;">
<table id="S5.T8.13.13.11" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.13.13.11.12" class="ltx_tr">
<td id="S5.T8.13.13.11.12.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T8.13.13.11.12.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S5.T8.13.13.11.12.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;" rowspan="2"><span id="S5.T8.13.13.11.12.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Input Size</span></td>
<td id="S5.T8.13.13.11.12.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.12.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WTQ</span></td>
<td id="S5.T8.13.13.11.12.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.12.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabFact</span></td>
<td id="S5.T8.13.13.11.12.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.12.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ComTQA</span></td>
</tr>
<tr id="S5.T8.13.13.11.13" class="ltx_tr">
<td id="S5.T8.13.13.11.13.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.13.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acc</span></td>
<td id="S5.T8.13.13.11.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.13.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acc</span></td>
<td id="S5.T8.13.13.11.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.13.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acc</span></td>
</tr>
<tr id="S5.T8.3.3.1.1" class="ltx_tr">
<td id="S5.T8.3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.3.3.1.1.2.1" class="ltx_text" style="font-size:80%;">TextMonkeyÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.3.3.1.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S5.T8.3.3.1.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.3.3.1.1.3.1" class="ltx_text" style="font-size:80%;">896</span></td>
<td id="S5.T8.3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.3.3.1.1.4.1" class="ltx_text" style="font-size:80%;">37.9</span></td>
<td id="S5.T8.3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.3.3.1.1.5.1" class="ltx_text" style="font-size:80%;">53.6</span></td>
<td id="S5.T8.3.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.3.3.1.1.1.1" class="ltx_text" style="font-size:80%;">13.9</span><sup id="S5.T8.3.3.1.1.1.2" class="ltx_sup"><span id="S5.T8.3.3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
</tr>
<tr id="S5.T8.4.4.2.2" class="ltx_tr">
<td id="S5.T8.4.4.2.2.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.4.4.2.2.2.1" class="ltx_text" style="font-size:80%;">MonkeyÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.4.4.2.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S5.T8.4.4.2.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.4.4.2.2.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.4.4.2.2.3.1" class="ltx_text" style="font-size:80%;">896</span></td>
<td id="S5.T8.4.4.2.2.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.4.4.2.2.1.1" class="ltx_text" style="font-size:80%;">25.3</span><sup id="S5.T8.4.4.2.2.1.2" class="ltx_sup"><span id="S5.T8.4.4.2.2.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.4.4.2.2.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.4.4.2.2.4.1" class="ltx_text" style="font-size:80%;">49.8</span></td>
<td id="S5.T8.4.4.2.2.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.4.4.2.2.5.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
</tr>
<tr id="S5.T8.6.6.4.4" class="ltx_tr">
<td id="S5.T8.6.6.4.4.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.6.6.4.4.3.1" class="ltx_text" style="font-size:80%;">CogagentÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.6.6.4.4.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S5.T8.6.6.4.4.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.6.6.4.4.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.6.6.4.4.4.1" class="ltx_text" style="font-size:80%;">1,120</span></td>
<td id="S5.T8.5.5.3.3.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.5.5.3.3.1.1" class="ltx_text" style="font-size:80%;">30.2</span><sup id="S5.T8.5.5.3.3.1.2" class="ltx_sup"><span id="S5.T8.5.5.3.3.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.6.6.4.4.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.6.6.4.4.2.1" class="ltx_text" style="font-size:80%;">51.7</span><sup id="S5.T8.6.6.4.4.2.2" class="ltx_sup"><span id="S5.T8.6.6.4.4.2.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.6.6.4.4.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.6.6.4.4.5.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
</tr>
<tr id="S5.T8.7.7.5.5" class="ltx_tr">
<td id="S5.T8.7.7.5.5.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.7.7.5.5.2.1" class="ltx_text" style="font-size:80%;">DocOwl 1.5Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.7.7.5.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S5.T8.7.7.5.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.7.7.5.5.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.7.7.5.5.3.1" class="ltx_text" style="font-size:80%;">1,344</span></td>
<td id="S5.T8.7.7.5.5.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.7.7.5.5.4.1" class="ltx_text" style="font-size:80%;">39.8</span></td>
<td id="S5.T8.7.7.5.5.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.7.7.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.4</span></td>
<td id="S5.T8.7.7.5.5.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.7.7.5.5.1.1" class="ltx_text" style="font-size:80%;">18.5</span><sup id="S5.T8.7.7.5.5.1.2" class="ltx_sup"><span id="S5.T8.7.7.5.5.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
</tr>
<tr id="S5.T8.10.10.8.8" class="ltx_tr">
<td id="S5.T8.10.10.8.8.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.10.10.8.8.4.1" class="ltx_text" style="font-size:80%;">GPT4VÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.10.10.8.8.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib95" title="" class="ltx_ref">95</a><span id="S5.T8.10.10.8.8.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.10.10.8.8.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.10.10.8.8.5.1" class="ltx_text" style="font-size:80%;">645</span></td>
<td id="S5.T8.8.8.6.6.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.8.8.6.6.1.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">45.5</span><sup id="S5.T8.8.8.6.6.1.2" class="ltx_sup"><span id="S5.T8.8.8.6.6.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.9.9.7.7.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.9.9.7.7.2.1" class="ltx_text" style="font-size:80%;">69.3</span><sup id="S5.T8.9.9.7.7.2.2" class="ltx_sup"><span id="S5.T8.9.9.7.7.2.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.10.10.8.8.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.10.10.8.8.3.1" class="ltx_text" style="font-size:80%;">27.2</span><sup id="S5.T8.10.10.8.8.3.2" class="ltx_sup"><span id="S5.T8.10.10.8.8.3.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
</tr>
<tr id="S5.T8.13.13.11.11" class="ltx_tr">
<td id="S5.T8.13.13.11.11.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.13.13.11.11.4.1" class="ltx_text" style="font-size:80%;">Gemini ProÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.13.13.11.11.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S5.T8.13.13.11.11.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.13.13.11.11.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.11.5.1" class="ltx_text" style="font-size:80%;">659</span></td>
<td id="S5.T8.11.11.9.9.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.11.11.9.9.1.1" class="ltx_text" style="font-size:80%;">32.3</span><sup id="S5.T8.11.11.9.9.1.2" class="ltx_sup"><span id="S5.T8.11.11.9.9.1.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.12.12.10.10.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.12.12.10.10.2.1" class="ltx_text" style="font-size:80%;">67.9</span><sup id="S5.T8.12.12.10.10.2.2" class="ltx_sup"><span id="S5.T8.12.12.10.10.2.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
<td id="S5.T8.13.13.11.11.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.13.13.11.11.3.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">29.3</span><sup id="S5.T8.13.13.11.11.3.2" class="ltx_sup"><span id="S5.T8.13.13.11.11.3.2.1" class="ltx_text" style="font-size:80%;">âˆ—</span></sup>
</td>
</tr>
<tr id="S5.T8.13.13.11.14" class="ltx_tr">
<td id="S5.T8.13.13.11.14.1" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;">
<span id="S5.T8.13.13.11.14.1.1" class="ltx_text" style="font-size:80%;">Xcomposer2Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T8.13.13.11.14.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib96" title="" class="ltx_ref">96</a><span id="S5.T8.13.13.11.14.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T8.13.13.11.14.2" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.14.2.1" class="ltx_text" style="font-size:80%;">511</span></td>
<td id="S5.T8.13.13.11.14.3" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.14.3.1" class="ltx_text" style="font-size:80%;">28.7</span></td>
<td id="S5.T8.13.13.11.14.4" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.14.4.1" class="ltx_text" style="font-size:80%;">62.3</span></td>
<td id="S5.T8.13.13.11.14.5" class="ltx_td ltx_align_center" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.14.5.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
</tr>
<tr id="S5.T8.13.13.11.15" class="ltx_tr">
<td id="S5.T8.13.13.11.15.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.15.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabPedia</span></td>
<td id="S5.T8.13.13.11.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.15.2.1" class="ltx_text" style="font-size:80%;">2,560</span></td>
<td id="S5.T8.13.13.11.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.15.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">47.8</span></td>
<td id="S5.T8.13.13.11.15.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.15.4.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">71.3</span></td>
<td id="S5.T8.13.13.11.15.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.15pt 3.0pt;"><span id="S5.T8.13.13.11.15.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">53.5</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Quantitative Results</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We conduct quantitative evaluations of current state-of-the-art methods for specific tasks in perception and comprehension, comparing them to our proposed TabPedia.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Evaluation on TD.</span>
In Tab.Â <a href="#S5.T4" title="Table 4 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare TabPedia with the previous state-of-the-art method, TATRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
TATR performs the table detection with two classic visual detection backbones, <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_italic">i.e,</span> DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.
Compared with them, TabPedia outperforms Faster R-CNN with a notable margin and achieves competitive performance with DETR.
Notably, since TabPedia directly generates the independent locations of instance tables without densely overlapped bboxes, there are no extra post-processing operations involved, <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_italic">i.e.</span>, Non-Maximum SuppressionÂ (NMS).
This advantage could enable TabPedia to perform more complex table understanding, such as parsing all tables by combining TD and TQ tasks.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Evaluation on TSR.</span> Tab.Â <a href="#S5.T4" title="Table 4 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the performance of TSR task compared to end-to-end TSR models on PubTabNet and FinTabNet datasets. Specifically, the OCR-free model DonutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is fine-tuned for TSR with the official default training configuration.
Although OmniParserÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> integrates multiple visually-situated text parsing tasks into a unified framework, it adopts three isolated decoders to perform different tasks.
Compared with OmniParser, TabPedia consistently surpasses it with 4.96% and 3.56% S-TEDS on both datasets, respectively.
In Tab.Â <a href="#S5.T6" title="Table 6 â€£ Table 8 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, TATR as the task-specific method, shows high performance with the DETR architecture.
Our proposed TabPedia, a generic model for tasks involving both perception and comprehension, still achieves comparable performance without the need for complex post-processing.
These results highlight the exceptional capability of TabPedia.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Evaluation on TQ.</span> As a new and unexplored task, the TQ task aims to parse table structures with the specific location directly from the raw image without additional cropping.
In the first row of Tab.Â <a href="#S5.T7" title="Table 7 â€£ Table 8 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we provide a strong baseline with 96.04% and 95.07% on <math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{GriTS}_{\mathrm{Top}}" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><msub id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mi id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml">GriTS</mi><mi id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml">Top</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">GriTS</ci><ci id="S5.SS2.p4.1.m1.1.1.3.cmml" xref="S5.SS2.p4.1.m1.1.1.3">Top</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">\mathrm{GriTS}_{\mathrm{Top}}</annotation></semantics></math> and S-TEDS, respectively, which nearly reaches the same performance as parsing from the cropped images under the interference of the document content around the table.
Furthermore, we integrate both TD and TQ tasks in the form of multi-round dialogue, which endows TabPedia to directly parse all existing tables in a document image. We report the final result in the second row of Tab.Â <a href="#S5.T7" title="Table 7 â€£ Table 8 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
These impressive results demonstrate that TabPedia has the potential to enable more holistic table understanding.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Evaluation on TQA.</span> Due to the complex structure of tables and the dense text, the understanding of the table contents remains a challenging issue. To thoroughly evaluate the performance of the understanding of table content and structure, we adopt two public benchmarks, <span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_italic">i.e.</span>, WTQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and TabFactÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, and our collected dataset ComTQA, as shown in Tab.Â <a href="#S5.T8" title="Table 8 â€£ 5.1 Implementation Details â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
On the WTQ and TabFact, TabPedia achieves promising performance among the open and close sources LVLMs.
In contrast to existing benchmarks, ComTQA contains real-world table images with more complex questions.
It is observed that current LVLMs show poor performance due to the incomplete understanding of real-world table structures.
Compared with them, TabPedia achieves the optimal result with a notable margin, which demonstrates the effectiveness of jointly learning perception and comprehension tasks.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2406.01326/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="447" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Qualitative results of TabPedia on diverse tasks. The first row shows its perception capability on both TD and TSR tasks. The second row further exhibits TabPediaâ€™s powerful ability by employing multiple instructions of different tasks. The bottom row showcases TabPediaâ€™s accurate responses based on intricate contents in visual tables. Zoom in for best view.</span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Qualitative Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We further conduct qualitative evaluation on TabPediaâ€™s perception and comprehension capabilities.
Firstly, we show the perception capability of TabPedia with solely TD and TSR tasks, as illustrated in the first row of Fig.Â <a href="#S5.F3" title="Figure 3 â€£ 5.2 Quantitative Results â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
TabPedia accurately generates reliable and formatted results, which are rendered to the original image for better observation.
Secondly, TabPedia performs a complex task to directly parsing all table structure information in a document image by integrating instructions of TD and TQ tasks within a multi-round dialogue.
As shown in the second row of Fig.Â <a href="#S5.F3" title="Figure 3 â€£ 5.2 Quantitative Results â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the example indicates that TabPedia is capable of exploring more holistic visual table understanding.
In the last row, we display the table comprehensive capability of TabPedia.
It is observed that the response not only contains concise and reliable answer, but also provides the specific contents in the table to support its answer.
Especially, TabPedia even acquires certain math calculation ability to capture the connections among table contents, as shown in the bottom right example in Fig.Â <a href="#S5.F3" title="Figure 3 â€£ 5.2 Quantitative Results â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
These results demonstrate Tabpediaâ€™s powerful multimodal comprehension
capabilities.
We also display more visualization results in the AppendixÂ <a href="#A4" title="Appendix D More Qualitative Results â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Studies</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In this section, we conduct ablation studies to validate the effectiveness of core settings and components in TabPedia.
All experiments are conducted on three datasets across three tasks: PubTab1M-DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, FinTabNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and WTQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Impact of meditative tokens.</span> In Tab.Â <a href="#S5.T10" title="Table 10 â€£ 5.4 Ablation Studies â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we conduct the experiment to investigate the impact of adding meditative tokens in TabPedia.
It is observed that adding meditative tokens significantly improves TabPediaâ€™s capabilities of table perception and comprehension. We also provide a detailed analysis of the attention map of meditative tokens in Fig.Â <a href="#A4.F5" title="Figure D5 â€£ Appendix D More Qualitative Results â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D5</span></a> of Appendix.Â <a href="#A4" title="Appendix D More Qualitative Results â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<figure id="S5.T10" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T10.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.T10.fig1.3.1.1" class="ltx_text" style="font-size:113%;">Table 9</span>: </span><span id="S5.T10.fig1.4.2" class="ltx_text" style="font-size:113%;">Impact of meditative tokens in TabPedia.</span></figcaption>
<div id="S5.T10.fig1.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:143.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(108.2pt,-35.9pt) scale(1.99620101319895,1.99620101319895) ;">
<table id="S5.T10.fig1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T10.fig1.5.1.1" class="ltx_tr">
<td id="S5.T10.fig1.5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;" rowspan="2"><span id="S5.T10.fig1.5.1.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T10.fig1.5.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T10.fig1.5.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T10.fig1.5.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">meditative</span></span>
<span id="S5.T10.fig1.5.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T10.fig1.5.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">token</span></span>
</span></span></td>
<td id="S5.T10.fig1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTab1M-Det</span></td>
<td id="S5.T10.fig1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinTabNet</span></td>
<td id="S5.T10.fig1.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WTQ</span></td>
</tr>
<tr id="S5.T10.fig1.5.1.2" class="ltx_tr">
<td id="S5.T10.fig1.5.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Precision</span></td>
<td id="S5.T10.fig1.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
<td id="S5.T10.fig1.5.1.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acc</span></td>
</tr>
<tr id="S5.T10.fig1.5.1.3" class="ltx_tr">
<td id="S5.T10.fig1.5.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span id="S5.T10.fig1.5.1.3.1.1" class="ltx_ERROR undefined">\usym</span><span id="S5.T10.fig1.5.1.3.1.2" class="ltx_text" style="font-size:80%;">2715</span>
</td>
<td id="S5.T10.fig1.5.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.3.2.1" class="ltx_text" style="font-size:80%;">93.5</span></td>
<td id="S5.T10.fig1.5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.3.3.1" class="ltx_text" style="font-size:80%;">92.17</span></td>
<td id="S5.T10.fig1.5.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.3.4.1" class="ltx_text" style="font-size:80%;">43.2</span></td>
</tr>
<tr id="S5.T10.fig1.5.1.4" class="ltx_tr">
<td id="S5.T10.fig1.5.1.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.4.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T10.fig1.5.1.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.5</span></td>
<td id="S5.T10.fig1.5.1.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.11</span></td>
<td id="S5.T10.fig1.5.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T10.fig1.5.1.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">47.8</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T10.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.T10.fig2.3.1.1" class="ltx_text" style="font-size:113%;">Table 10</span>: </span><span id="S5.T10.fig2.4.2" class="ltx_text" style="font-size:113%;">Impact of vision encoders in our TabPedia.</span></figcaption>
<div id="S5.T10.fig2.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:165.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.8pt,-37.7pt) scale(1.83667768632431,1.83667768632431) ;">
<table id="S5.T10.fig2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T10.fig2.5.1.1" class="ltx_tr">
<td id="S5.T10.fig2.5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 4.0pt;" rowspan="2"><span id="S5.T10.fig2.5.1.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T10.fig2.5.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T10.fig2.5.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T10.fig2.5.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 4.0pt;">High-Res</span></span>
<span id="S5.T10.fig2.5.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T10.fig2.5.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 4.0pt;">Encoder</span></span>
</span></span></td>
<td id="S5.T10.fig2.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 4.0pt;" rowspan="2"><span id="S5.T10.fig2.5.1.1.2.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T10.fig2.5.1.1.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T10.fig2.5.1.1.2.1.1.1" class="ltx_tr">
<span id="S5.T10.fig2.5.1.1.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 4.0pt;">Low-Res</span></span>
<span id="S5.T10.fig2.5.1.1.2.1.1.2" class="ltx_tr">
<span id="S5.T10.fig2.5.1.1.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 4.0pt;">Encoder</span></span>
</span></span></td>
<td id="S5.T10.fig2.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTab1M-Det</span></td>
<td id="S5.T10.fig2.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinTabNet</span></td>
<td id="S5.T10.fig2.5.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WTQ</span></td>
</tr>
<tr id="S5.T10.fig2.5.1.2" class="ltx_tr">
<td id="S5.T10.fig2.5.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Precision</span></td>
<td id="S5.T10.fig2.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">S-TEDS</span></td>
<td id="S5.T10.fig2.5.1.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acc</span></td>
</tr>
<tr id="S5.T10.fig2.5.1.3" class="ltx_tr">
<td id="S5.T10.fig2.5.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.3.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T10.fig2.5.1.3.2" class="ltx_td ltx_border_t" style="padding:0.15pt 4.0pt;"></td>
<td id="S5.T10.fig2.5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.3.3.1" class="ltx_text" style="font-size:80%;">96.5</span></td>
<td id="S5.T10.fig2.5.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.3.4.1" class="ltx_text" style="font-size:80%;">93.6</span></td>
<td id="S5.T10.fig2.5.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.3.5.1" class="ltx_text" style="font-size:80%;">44.9</span></td>
</tr>
<tr id="S5.T10.fig2.5.1.4" class="ltx_tr">
<td id="S5.T10.fig2.5.1.4.1" class="ltx_td" style="padding:0.15pt 4.0pt;"></td>
<td id="S5.T10.fig2.5.1.4.2" class="ltx_td ltx_align_center" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.4.2.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T10.fig2.5.1.4.3" class="ltx_td ltx_align_center" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.4.3.1" class="ltx_text" style="font-size:80%;">86.2</span></td>
<td id="S5.T10.fig2.5.1.4.4" class="ltx_td ltx_align_center" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.4.4.1" class="ltx_text" style="font-size:80%;">81.3</span></td>
<td id="S5.T10.fig2.5.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.4.5.1" class="ltx_text" style="font-size:80%;">24.7</span></td>
</tr>
<tr id="S5.T10.fig2.5.1.5" class="ltx_tr">
<td id="S5.T10.fig2.5.1.5.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.5.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T10.fig2.5.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.5.2.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T10.fig2.5.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.5</span></td>
<td id="S5.T10.fig2.5.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.11</span></td>
<td id="S5.T10.fig2.5.1.5.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.15pt 4.0pt;"><span id="S5.T10.fig2.5.1.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">47.8</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Impact of vision encoders.</span>
As shown in Tab.Â <a href="#S5.T10" title="Table 10 â€£ 5.4 Ablation Studies â€£ 5 Experiment â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we explore the impact of different vision encoders. In TabPedia, we propose both vision encoders to capture global and local information of the input image with different resolutions.
For the high-resolution encoder, it could extract more intricate information from the text-rich image and achieve better performance than solely utilizing the low-resolution encoder.
Furthermore, the low-resolution encoder plays a crucial role in furnishing comprehensive layout information, addressing the constrained receptive field of the high-resolution encoder.
The results clearly indicate that the synergy between both vision encoders enhances the extraction of structural and content-related details from tables, which effectively improves perception and comprehension tasks.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitation</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we discuss the limitations of our TabPedia.
Firstly, since we represent the table structure with regular rectangular boxes, TabPedia is currently not capable of accurately parsing structural information for twisted or distorted tables.
Secondly, all images in TQA datasets, including WTQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, TabFactÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and ComTQA are dominated by tables. Therefore, TabPedia still lacks the capability to directly answer the table question with original document image. In addition, it also exhibits a deficiency in table cell recognition.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we propose a novel large vision-language model to unify diverse visual table understanding tasks, namely TabPedia.
Specifically, we present a <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">concept synergy</span> mechanism to seamlessly integrate diverse tasks and multi-source visual tokens embedded from dual vision encoders as <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">concepts</span>.
This mechanism is implemented by introducing the <span id="S7.p1.1.3" class="ltx_text ltx_font_italic">meditative tokens</span> into the LLM.
Then, we fully leverage the capability of LLMs to effectively understand these concepts and generate accurate and plausible responses.
Extensive quantitative and qualitative experiments across various public benchmarks validate the effectiveness of our TabPedia.
To further investigate the potential of TabPedia, we establish a challenging table VQA dataset, ComTQA, featuring round 9,000 QA pairs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. [2019]</span>
<span class="ltx_bibblock">
Liangcai Gao, Yilun Huang, Herve Dejean, Jean-Luc Meunier, Qinqin Yan, YuÂ Fang, Florian Kleber, and Eva Lang.

</span>
<span class="ltx_bibblock">Icdar 2019 competition on table detection and recognition (ctdar).

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gobel etÂ al. [2013]</span>
<span class="ltx_bibblock">
Max Gobel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi.

</span>
<span class="ltx_bibblock">Icdar 2013 table competition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, 2013.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prasad etÂ al. [2020a]</span>
<span class="ltx_bibblock">
Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure.

</span>
<span class="ltx_bibblock">Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</em>, pages 572â€“573, 2020a.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schreiber etÂ al. [2017]</span>
<span class="ltx_bibblock">
Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed.

</span>
<span class="ltx_bibblock">Deepdesrt: Deep learning for detection and structure recognition of tables in document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">international conference on document analysis and recognition</em>, pages 1162â€“1167, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. [2021]</span>
<span class="ltx_bibblock">
Xinyi Zheng, Douglas Burdick, Lucian Popa, XuÂ Zhong, and Nancy XinÂ Ru Wang.

</span>
<span class="ltx_bibblock">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</em>, pages 697â€“706, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddiqui etÂ al. [2018]</span>
<span class="ltx_bibblock">
ShoaibÂ Ahmed Siddiqui, MuhammadÂ Imran Malik, Stefan Agne, Andreas Dengel, and Sheraz Ahmed.

</span>
<span class="ltx_bibblock">DeCNT: Deep deformable cnn for table detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, 6:74151â€“74161, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen [2022]</span>
<span class="ltx_bibblock">
Duc-Dung Nguyen.

</span>
<span class="ltx_bibblock">TableSegNet: a fully convolutional network for table detection and segmentation in document images.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Journal on Document Analysis and Recognition</em>, 25(1):1â€“14, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. [2023]</span>
<span class="ltx_bibblock">
Daqian Zhang, Ruibin Mao, Runting Guo, Yang Jiang, and Jing Zhu.

</span>
<span class="ltx_bibblock">YOLO-Table: disclosure document table detection with involution.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Journal on Document Analysis and Recognition</em>, 26(1):1â€“14, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smock etÂ al. [2022]</span>
<span class="ltx_bibblock">
Brandon Smock, Rohith Pesala, and Robin Abraham.

</span>
<span class="ltx_bibblock">PubTables-1M: Towards comprehensive table extraction from unstructured documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 4634â€“4642, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. [2023a]</span>
<span class="ltx_bibblock">
Hao Feng, QiÂ Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang.

</span>
<span class="ltx_bibblock">Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11810</em>, 2023a.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, YuÂ Han, Fei Huang, etÂ al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2024a]</span>
<span class="ltx_bibblock">
Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.

</span>
<span class="ltx_bibblock">Textmonkey: An ocr-free large multimodal model for understanding document.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04473</em>, 2024a.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. [2023a]</span>
<span class="ltx_bibblock">
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, etÂ al.

</span>
<span class="ltx_bibblock">mPLUG-DocOwl: Modularized multimodal large language model for document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.02499</em>, 2023a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al. [2020]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, etÂ al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 8748â€“8763, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. [2023]</span>
<span class="ltx_bibblock">
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.

</span>
<span class="ltx_bibblock">Eva: Exploring the limits of masked visual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 19358â€“19369, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2021a]</span>
<span class="ltx_bibblock">
ZeÂ Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.

</span>
<span class="ltx_bibblock">Swin Transformer: Hierarchical vision transformer using shifted windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision</em>, pages 10012â€“10022, 2021a.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. [2020]</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, pages 7871â€“7880, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1â€“67, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. [2018]</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. [2020a]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, pages 1877â€“1901, 2020a.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, etÂ al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac etÂ al. [2022]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, etÂ al.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, pages 23716â€“23736, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2022]</span>
<span class="ltx_bibblock">
XiÂ Chen, Xiao Wang, Soravit Changpinyo, AJÂ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, etÂ al.

</span>
<span class="ltx_bibblock">Pali: A jointly-scaled multilingual language-image model.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv:2209.06794</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2023a]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv:2301.12597</em>, 2023a.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. [2023]</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony MengÂ Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">InstructBLIP: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.06500</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. [2023]</span>
<span class="ltx_bibblock">
Shaohan Huang, LiÂ Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, OwaisÂ Khan Mohammed, Qiang Liu, etÂ al.

</span>
<span class="ltx_bibblock">Language is not all you need: Aligning perception with language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv:2302.14045</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. [2023]</span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, LiÂ Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding multimodal large language models to the world.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.14824</em>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. [2023]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">MiniGPT-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.10592</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2024b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36, 2024b.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. [2023b]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, etÂ al.

</span>
<span class="ltx_bibblock">mPLUG-Owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.14178</em>, 2023b.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2023]</span>
<span class="ltx_bibblock">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.

</span>
<span class="ltx_bibblock">Shikra: Unleashing multimodal llmâ€™s referential dialogue magic.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.15195</em>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2023b]</span>
<span class="ltx_bibblock">
BoÂ Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Otter: A multi-modal model with in-context instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.03726</em>, 2023b.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. [2023]</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.

</span>
<span class="ltx_bibblock">The dawn of lmms: Preliminary explorations with gpt-4v (ision).

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.17421</em>, 9(1):1, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. [2020b]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, 2020b.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil etÂ al. [2023]</span>
<span class="ltx_bibblock">
Rohan Anil, AndrewÂ M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, etÂ al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.10403</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. [2023]</span>
<span class="ltx_bibblock">
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, etÂ al.

</span>
<span class="ltx_bibblock">Llama-adapter v2: Parameter-efficient visual instruction model.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.15010</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen [2023]</span>
<span class="ltx_bibblock">
Qwen.

</span>
<span class="ltx_bibblock">Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts), 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/QwenLM/Qwen-7B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/QwenLM/Qwen-7B</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. [2024]</span>
<span class="ltx_bibblock">
Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, BoÂ Zhang, Chen Li, JiÂ Zhang, Qin Jin, Fei Huang, etÂ al.

</span>
<span class="ltx_bibblock">mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.12895</em>, 2024.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan etÂ al. [2024]</span>
<span class="ltx_bibblock">
Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang.

</span>
<span class="ltx_bibblock">OmniParser: A unified framework for text spotting, key information extraction and table recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.19128</em>, 2024.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. [2024]</span>
<span class="ltx_bibblock">
ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, and DuenÂ Horng Chau.

</span>
<span class="ltx_bibblock">UniTable: Towards a unified framework for table structure recognition via self-supervised pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04822</em>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. [2022]</span>
<span class="ltx_bibblock">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.

</span>
<span class="ltx_bibblock">OCR-free document understanding transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, pages 498â€“517, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kieninger and Dengel [1999]</span>
<span class="ltx_bibblock">
Thomas Kieninger and Andreas Dengel.

</span>
<span class="ltx_bibblock">The t-recs table recognition and analysis system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">International Association on Pattern Recognition</em>, pages 255â€“270, 1999.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatos etÂ al. [2005]</span>
<span class="ltx_bibblock">
Basilios Gatos, Dimitrios Danatsas, Ioannis Pratikakis, and StavrosÂ J Perantonis.

</span>
<span class="ltx_bibblock">Automatic table detection in document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International Conference on Advances in Pattern Recognition</em>, pages 609â€“618, 2005.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harit and Bansal [2012]</span>
<span class="ltx_bibblock">
Gaurav Harit and Anukriti Bansal.

</span>
<span class="ltx_bibblock">Table detection in document images using header and trailer patterns.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing</em>, pages 1â€“8, 2012.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vo etÂ al. [2018]</span>
<span class="ltx_bibblock">
NguyenÂ D Vo, Khanh Nguyen, TamÂ V Nguyen, and Khang Nguyen.

</span>
<span class="ltx_bibblock">Ensemble of deep object detectors for page object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Ubiquitous Information Management and Communication</em>, pages 1â€“6, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilani etÂ al. [2017]</span>
<span class="ltx_bibblock">
Azka Gilani, ShahÂ Rukh Qasim, Imran Malik, and Faisal Shafait.

</span>
<span class="ltx_bibblock">Table detection using deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">international conference on document analysis and recognition</em>, volumeÂ 1, pages 771â€“776, 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. [2019]</span>
<span class="ltx_bibblock">
Yilun Huang, Qinqin Yan, Yibo Li, Yifan Chen, Xiong Wang, Liangcai Gao, and Zhi Tang.

</span>
<span class="ltx_bibblock">A yolo-based table detection method.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pages 813â€“818, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prasad etÂ al. [2020b]</span>
<span class="ltx_bibblock">
Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure.

</span>
<span class="ltx_bibblock">CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</em>, pages 572â€“573, 2020b.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal etÂ al. [2021]</span>
<span class="ltx_bibblock">
Madhav Agarwal, Ajoy Mondal, and CVÂ Jawahar.

</span>
<span class="ltx_bibblock">Cdec-net: Composite deformable cascade network for table detection in document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">international conference on pattern recognition</em>, pages 9491â€“9498, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. [2019]</span>
<span class="ltx_bibblock">
Ningning Sun, Yuanping Zhu, and Xiaoming Hu.

</span>
<span class="ltx_bibblock">Faster r-cnn based table detection combining corner locating.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">international conference on document analysis and recognition</em>, pages 1314â€“1319, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riba etÂ al. [2019]</span>
<span class="ltx_bibblock">
Pau Riba, Anjan Dutta, Lutz Goldmann, Alicia FornÃ©s, Oriol Ramos, and Josep LladÃ³s.

</span>
<span class="ltx_bibblock">Table detection in invoice documents by graph neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pages 122â€“127, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HoleÄek etÂ al. [2019]</span>
<span class="ltx_bibblock">
Martin HoleÄek, AntonÃ­n Hoskovec, Petr BaudiÅ¡, and Pavel Klinger.

</span>
<span class="ltx_bibblock">Table understanding in structured documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition Workshops</em>, pages 158â€“164, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2020]</span>
<span class="ltx_bibblock">
Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou.

</span>
<span class="ltx_bibblock">Docbank: A benchmark dataset for document layout analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, pages 949â€“960, 2020.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion etÂ al. [2020]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, pages 213â€“229, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. [2022]</span>
<span class="ltx_bibblock">
Zengyuan Guo, Yuechen Yu, Pengyuan Lv, Chengquan Zhang, Haojie Li, Zhihui Wang, Kun Yao, Jingtuo Liu, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Trust: an accurate and end-to-end table structure recognizer using splitting-based transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.14687</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. [2022]</span>
<span class="ltx_bibblock">
Weihong Lin, Zheng Sun, Chixiang Ma, Mingze Li, Jiawei Wang, Lei Sun, and Qiang Huo.

</span>
<span class="ltx_bibblock">TSRFormer: Table structure recognition with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>, pages 6473â€“6482, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2024c]</span>
<span class="ltx_bibblock">
Hao Liu, Xin Li, Mingming Gong, Bing Liu, Yunfei Wu, Deqiang Jiang, Yinsong Liu, and Xing Sun.

</span>
<span class="ltx_bibblock">Grab what you need: Rethinking complex table structure recognition with flexible components deliberation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, pages 3603â€“3611, 2024c.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paliwal etÂ al. [2019]</span>
<span class="ltx_bibblock">
ShubhamÂ Singh Paliwal, DÂ Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig.

</span>
<span class="ltx_bibblock">TableNet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pages 128â€“133, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2021b]</span>
<span class="ltx_bibblock">
Hao Liu, Xin Li, Bing Liu, Deqiang Jiang, Yinsong Liu, BoÂ Ren, and Rongrong Ji.

</span>
<span class="ltx_bibblock">Show, read and reason.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM International Conference on Multimedia</em>, 2021b.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi etÂ al. [2019]</span>
<span class="ltx_bibblock">
Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, and Xian-Ling Mao.

</span>
<span class="ltx_bibblock">Complicated table structure recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.04729</em>, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2022]</span>
<span class="ltx_bibblock">
Hao Liu, Xin Li, Bing Liu, Deqiang Jiang, Yinsong Liu, and BoÂ Ren.

</span>
<span class="ltx_bibblock">Neural collaborative graph machines for table structure recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 4533â€“4542, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al. [2020]</span>
<span class="ltx_bibblock">
XuÂ Zhong, Elaheh ShafieiBavani, and Antonio JimenoÂ Yepes.

</span>
<span class="ltx_bibblock">Image-based table recognition: data, model, and evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, pages 564â€“580, 2020.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang etÂ al. [2023]</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, ZiÂ Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E Gonzalez, etÂ al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">See https://vicuna. lmsys. org (accessed 14 April 2023)</em>, 2(3):6, 2023.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. [2023b]</span>
<span class="ltx_bibblock">
Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.

</span>
<span class="ltx_bibblock">Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11592</em>, 2023b.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. [2023]</span>
<span class="ltx_bibblock">
Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.

</span>
<span class="ltx_bibblock">Vary: Scaling up the vision vocabulary for large vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.06109</em>, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fujitake [2024]</span>
<span class="ltx_bibblock">
Masato Fujitake.

</span>
<span class="ltx_bibblock">LayoutLLM: Large language model instruction tuning for visually rich document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.14252</em>, 2024.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge etÂ al. [2023]</span>
<span class="ltx_bibblock">
Tao Ge, HuÂ Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei.

</span>
<span class="ltx_bibblock">In-context autoencoder for context compression in a large language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. [2023]</span>
<span class="ltx_bibblock">
Fuzhao Xue, Valerii Likhosherstov, Anurag Arnab, Neil Houlsby, Mostafa Dehghani, and Yang You.

</span>
<span class="ltx_bibblock">Adaptive computation with elastic input sequence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 38971â€“38988, 2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burtsev etÂ al. [2020]</span>
<span class="ltx_bibblock">
MikhailÂ S Burtsev, Yuri Kuratov, Anton Peganov, and GrigoryÂ V Sapunov.

</span>
<span class="ltx_bibblock">Memory transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.11527</em>, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulatov etÂ al. [2022]</span>
<span class="ltx_bibblock">
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.

</span>
<span class="ltx_bibblock">Recurrent memory transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, 35:11079â€“11091, 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Darcet etÂ al. [2023]</span>
<span class="ltx_bibblock">
TimothÃ©e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski.

</span>
<span class="ltx_bibblock">Vision transformers need registers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. [2023]</span>
<span class="ltx_bibblock">
Sachin Goyal, Ziwei Ji, AnkitÂ Singh Rawat, AdityaÂ Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan.

</span>
<span class="ltx_bibblock">Think before you speak: Training language models with pause tokens.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.02226</em>, 2023.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab etÂ al. [2023]</span>
<span class="ltx_bibblock">
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, HuyÂ V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, etÂ al.

</span>
<span class="ltx_bibblock">DINOv2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. [2023]</span>
<span class="ltx_bibblock">
Kenton Lee, Mandar Joshi, IuliaÂ Raluca Turc, Hexiang Hu, Fangyu Liu, JulianÂ Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 18893â€“18912, 2023.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. [2023c]</span>
<span class="ltx_bibblock">
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, QiÂ Qian, JiÂ Zhang, etÂ al.

</span>
<span class="ltx_bibblock">UReader: Universal ocr-free visually-situated language understanding with multimodal large language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 2841â€“2858, 2023c.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao etÂ al. [2020]</span>
<span class="ltx_bibblock">
Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang Bai.

</span>
<span class="ltx_bibblock">Real-time scene text detection with differentiable binarization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, pages 11474â€“11481, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. [2011]</span>
<span class="ltx_bibblock">
Kai Wang, Boris Babenko, and Serge Belongie.

</span>
<span class="ltx_bibblock">End-to-end scene text recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision</em>, pages 1457â€“1464, 2011.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2018]</span>
<span class="ltx_bibblock">
Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, YuÂ Qiao, and Junjie Yan.

</span>
<span class="ltx_bibblock">Fots: Fast oriented text spotting with a unified network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 5676â€“5685, 2018.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hossain etÂ al. [2019]</span>
<span class="ltx_bibblock">
MDÂ Zakir Hossain, Ferdous Sohel, MohdÂ Fairuz Shiratuddin, and Hamid Laga.

</span>
<span class="ltx_bibblock">A comprehensive survey of deep learning for image captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 51(6):1â€“36, 2019.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang [2015]</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, pages 1470â€“1480, 2015.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2019]</span>
<span class="ltx_bibblock">
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and WilliamÂ Yang Wang.

</span>
<span class="ltx_bibblock">TabFact: A large-scale dataset for table-based fact verification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girshick [2015]</span>
<span class="ltx_bibblock">
Ross Girshick.

</span>
<span class="ltx_bibblock">Fast R-CNN.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision</em>, pages 1440â€“1448, 2015.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. [2015]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. [2017]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision</em>, pages 2961â€“2969, 2017.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid etÂ al. [2024]</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, etÂ al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith and Topin [2019]</span>
<span class="ltx_bibblock">
LeslieÂ N Smith and Nicholay Topin.

</span>
<span class="ltx_bibblock">Super-convergence: Very fast training of neural networks using large learning rates.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and machine learning for multi-domain operations applications</em>, pages 369â€“386, 2019.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2018]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke etÂ al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, etÂ al.

</span>
<span class="ltx_bibblock">PyTorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smock etÂ al. [2023]</span>
<span class="ltx_bibblock">
Brandon Smock, Rohith Pesala, and Robin Abraham.

</span>
<span class="ltx_bibblock">GriTS: Grid table similarity metric for table structure recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pages 535â€“549, 2023.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2023]</span>
<span class="ltx_bibblock">
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, etÂ al.

</span>
<span class="ltx_bibblock">On the hidden mystery of ocr in large multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07895</em>, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2023c]</span>
<span class="ltx_bibblock">
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.

</span>
<span class="ltx_bibblock">Monkey: Image resolution and text label are important things for large multi-modal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.06607</em>, 2023c.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong etÂ al. [2023]</span>
<span class="ltx_bibblock">
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, etÂ al.

</span>
<span class="ltx_bibblock">Cogagent: A visual language model for gui agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.08914</em>, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">gpt [2023]</span>
<span class="ltx_bibblock">
GPT-4V(ision) system card.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. [2024]</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, etÂ al.

</span>
<span class="ltx_bibblock">InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16420</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More details about TQA datasets</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>QA Pairs Generation</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We depict the procedure of collecting QA pairs with an example in Fig.Â <a href="#A1.F1" title="Figure A1 â€£ A.1 QA Pairs Generation â€£ Appendix A More details about TQA datasets â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A1</span></a>.
For input image, Gemini ProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> is prompted to first recognize the table structure with OCR results in the image, then generate several question and answer pairs according to OCR results.
In order to improve the reliability of the generated answers, we leverage various prompting techniques, <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_italic">i.e,</span> Chain-of-Thought and few-shot prompting.
According to the specific prompt, Gemini Pro will generate multiple QA pairs for each input image and return them in an agreed-upon format.
After obtaining raw responses generated by Gemini Pro, we utilize the regularized matching algorithm and the special character filter in turn to extract available question and answer pairs.</p>
</div>
<figure id="A1.F1" class="ltx_figure"><img src="/html/2406.01326/assets/x4.png" id="A1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure A1</span>: </span><span id="A1.F1.3.2" class="ltx_text" style="font-size:90%;">The illustration of an example for generating QA pairs with the powerful LVLM, Gemini ProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. The prompt includes several key rules to ensure the response quality as much as possible.</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>ComTQA Benchmark</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">In Tab.Â <a href="#A1.SS2" title="A.2 ComTQA Benchmark â€£ Appendix A More details about TQA datasets â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>, we present the distribution of both data sourcesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> within the ComTQA dataset.
Concretely, ComTQA comprises a total of 9,070 QA pairs across 1,591 images, averaging 5 questions per image.
Different from existing TQA benchmarksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, ComTQA contains more complex table questions in real-world table images to assess the robustness of various models.
As shown in Fig.Â <a href="#A1.F2" title="Figure A2 â€£ A.2 ComTQA Benchmark â€£ Appendix A More details about TQA datasets â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A2</span></a>, we showcase several representative examples, including multiple answers, mathematical calculation and logical inference, which are the question types lacking in previous benchmarks.
To this end, we hope that ComTQA could fill this gap and serve as a reasonable benchmark for community development.</p>
</div>
<figure id="A1.SS2.1.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="A1.SS2.1.fig1.4.1.1" class="ltx_text" style="font-size:113%;">Table A1</span>: </span><span id="A1.SS2.1.fig1.5.2" class="ltx_text" style="font-size:113%;">Statistics of ComTQA benchmark.</span></figcaption>
<div id="A1.SS2.1.fig1.6" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:153.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(107.5pt,-38.0pt) scale(1.98281303224346,1.98281303224346) ;">
<table id="A1.SS2.1.fig1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.SS2.1.fig1.6.1.1" class="ltx_tr">
<td id="A1.SS2.1.fig1.6.1.1.1" class="ltx_td ltx_border_tt" style="padding:0.15pt 8.0pt;"></td>
<td id="A1.SS2.1.fig1.6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">PubTab1M</span></td>
<td id="A1.SS2.1.fig1.6.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinTabNet</span></td>
<td id="A1.SS2.1.fig1.6.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Total</span></td>
</tr>
<tr id="A1.SS2.1.fig1.6.1.2" class="ltx_tr">
<td id="A1.SS2.1.fig1.6.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.2.1.1" class="ltx_text" style="font-size:80%;">#images</span></td>
<td id="A1.SS2.1.fig1.6.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.2.2.1" class="ltx_text" style="font-size:80%;">932</span></td>
<td id="A1.SS2.1.fig1.6.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.2.3.1" class="ltx_text" style="font-size:80%;">659</span></td>
<td id="A1.SS2.1.fig1.6.1.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.2.4.1" class="ltx_text" style="font-size:80%;">1,591</span></td>
</tr>
<tr id="A1.SS2.1.fig1.6.1.3" class="ltx_tr">
<td id="A1.SS2.1.fig1.6.1.3.1" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.3.1.1" class="ltx_text" style="font-size:80%;">#QA pairs</span></td>
<td id="A1.SS2.1.fig1.6.1.3.2" class="ltx_td ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.3.2.1" class="ltx_text" style="font-size:80%;">6,232</span></td>
<td id="A1.SS2.1.fig1.6.1.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.3.3.1" class="ltx_text" style="font-size:80%;">2,838</span></td>
<td id="A1.SS2.1.fig1.6.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.3.4.1" class="ltx_text" style="font-size:80%;">9,070</span></td>
</tr>
<tr id="A1.SS2.1.fig1.6.1.4" class="ltx_tr">
<td id="A1.SS2.1.fig1.6.1.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 8.0pt;">
<table id="A1.SS2.1.fig1.6.1.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.SS2.1.fig1.6.1.4.1.1.1" class="ltx_tr">
<td id="A1.SS2.1.fig1.6.1.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.4.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Avg. per image</span></td>
</tr>
</table>
</td>
<td id="A1.SS2.1.fig1.6.1.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.4.2.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="A1.SS2.1.fig1.6.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.4.3.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="A1.SS2.1.fig1.6.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.15pt 8.0pt;"><span id="A1.SS2.1.fig1.6.1.4.4.1" class="ltx_text" style="font-size:80%;">5</span></td>
</tr>
</table>
</span></div>
<div id="A1.SS2.2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2406.01326/assets/x5.png" id="A1.SS2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="460" height="200" alt="Refer to caption">
</div>
</figure>
<figure id="A1.F2" class="ltx_figure"><img src="/html/2406.01326/assets/x6.png" id="A1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure A2</span>: </span><span id="A1.F2.3.2" class="ltx_text" style="font-size:90%;">More visualization on ComTQA benchmark. We display several complex QA types, such as multiple answers, mathematical calculation and logical inference. Zoom in for best view.</span></figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Annotation in TSR task</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We illustrate the object classes utilized in TSR and TQ tasks as shown in Fig.Â <a href="#A2.F3" title="Figure B3 â€£ Appendix B Annotation in TSR task â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B3</span></a>.
The intersection of each pair of table column and table row objects can be regarded as table grid cells.
These objects construct a tableâ€™s hierarchical structure through physical overlapped rectangle boxes.</p>
</div>
<figure id="A2.F3" class="ltx_figure"><img src="/html/2406.01326/assets/x7.png" id="A2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure B3</span>: </span><span id="A2.F3.3.2" class="ltx_text" style="font-size:90%;">The illustration of an example table with dilated bounding box annotations for different object classes for modeling table structure recognition.</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Broader Impact</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Our proposed model targets to unify multiple visual form comprehension tasks.
This technology could help more people with visual impairments access tabular data through cooperating with improved screen readers and other assistive technologies.
Moreover, automating table understanding technology could reduce the need for time-consuming manual data entry and correction, freeing up human resources for more complex and creative tasks.
To be honest, this technology also brings some negative societal impacts.
As more table data is extracted and processed with automatic visual table understanding, there is a heightened risk of sensitive information being mishandled or exposed. It is crucial to ensure robust data privacy measures.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More Qualitative Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p"><span id="A4.p1.1.1" class="ltx_text ltx_font_bold">Results on in-the-wild cases.</span> For better investigating the generalization of our proposed TabPedia, we randomly select some document images from a <a target="_blank" href="https://arxiv.org/" title="" class="ltx_ref ltx_href">document website</a> and illustrate the generation results in Fig.Â <a href="#A4.F4" title="Figure D4 â€£ Appendix D More Qualitative Results â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D4</span></a>. For perception and comprehension tasks, TabPedia generates accurate and reasonable responses in TD, TSR and TQA tasks, which sufficiently proves the robustness of our method for visual table understanding.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Attention map of meditative tokens.</span> In order to analyze the information extraction of meditative tokens for different tasks, we visualized the attention maps of meditative tokens for input instructions with different granularity of visual feature tokens, as shown in Fig.Â <a href="#A4.F5" title="Figure D5 â€£ Appendix D More Qualitative Results â€£ TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D5</span></a>. For each task, we select the shallow and deep four-layer attention maps in the LLM for visualization, respectively.
The y-axis represents the meditative tokens, while the x-axis represents the sequence of instruction tokens and different granular visual tokens.
For perceptive tasks, meditative tokens are densely attentive to most of the input information in the shallow layers, while they showcase diverse attention regions in the deeper layers.This phenomenon illustrates that meditative tokens could adaptively capture task-related information with respect to diverse tasks.
For the comprehension task (TQA), meditative tokens show a different attention pattern from perception tasks, which maintain sparse attention with input tokens in the shallow layers.
These results validate that our proposed meditative tokens adaptively enable different regions of visual tokens and understand the intention of specific task questions.</p>
</div>
<figure id="A4.F4" class="ltx_figure"><img src="/html/2406.01326/assets/x8.png" id="A4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="438" height="650" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure D4</span>: </span><span id="A4.F4.3.2" class="ltx_text" style="font-size:90%;">Qualitative results of TabPedia on in-the-wild cases. TabPedia achieves impressive performance in these unseen images, which validates its robustness and generalization. Zoom in for best view.</span></figcaption>
</figure>
<figure id="A4.F5" class="ltx_figure"><img src="/html/2406.01326/assets/x9.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="415" height="662" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure D5</span>: </span><span id="A4.F5.3.2" class="ltx_text" style="font-size:90%;">Visualization of attention maps between meditative tokens and the sequence of instruction and visual tokens. â€œQâ€, â€œLow-Resâ€ and â€œHigh-Resâ€ denote the instruction tokens, global visual tokens and local visual tokens, respectively. Y-axis denotes the meditative tokens. Zoom in for best view.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.01325" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.01326" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.01326">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.01326" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.01327" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:54:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
