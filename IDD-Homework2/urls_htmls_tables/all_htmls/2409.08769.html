<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.08769] Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry</title><meta property="og:description" content="In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks.
Inspired by the successful examples, we propose a causal visual-inertial fusion transformâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.08769">

<!--Generated on Sun Oct  6 00:55:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="visual inertial odometry multi modal transformers deep neural networks rotation learning">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Dept. Electrical and Electronics Eng., METU, Ankara, TÃ¼rkiye </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Center for Image Analysis (OGAM), METU, Ankara, TÃ¼rkiye </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Codeway AI Research</span></span></span>
<h1 class="ltx_title ltx_title_document">Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunus Bilge Kurt 
</span><span class="ltx_author_notes">112233
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-1564-3450" title="ORCID identifier" class="ltx_ref">0000-0002-1564-3450</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmet Akman
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5112-6963" title="ORCID identifier" class="ltx_ref">0000-0001-5112-6963</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A. AydÄ±n Alatan
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5556-7301%0A" title="ORCID identifier" class="ltx_ref">0000-0001-5556-7301
</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks.
Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry.
This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods.
Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks.
Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally.
Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE<math id="id1.1.m1.1" class="ltx_Math" alttext="(3)" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.2.2"><mo stretchy="false" id="id1.1.m1.1.2.2.1">(</mo><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="id1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">(3)</annotation></semantics></math> group.
The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference.
Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at <a target="_blank" href="https://github.com/ybkurt/VIFT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ybkurt/VIFT</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>visual inertial odometry multi modal transformers deep neural networks rotation learning
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Inertial Odometry (VIO) is a fundamental approach for the estimation of the pose of a moving body by visual-inertial sensor fusion. These methods can leverage the complementary nature of visual and inertial data, where the visual data provides 3D information about the scene, and the inertial data offers robust motion cues. Geometry-based VIO methods show promising results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>; however, they often require careful initialization and calibration to perform accurately during operation. In contrast, end-to-end learning approaches in VIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> offer the potential to bypass these challenges by directly learning fuse sensory information. Integrating deep learning into VIO systems introduces the possibility of not only improving accuracy but also simplifying deployment, as the models can generalize across different environments and conditions.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While deep learning has shown promise in VIO, there are still significant areas for improvement, particularly in temporal modeling and rotation estimation. Current deep VIO methods often employ recurrent neural network (RNN) based methodologies to model temporal dependencies.
Nevertheless, the literature has decent alternatives to RNN-based methods for modeling complex temporal dynamics in VIO tasks. Additionally, rotation regression remains a challenge, as traditional representations such as quaternions or Euler angles are not optimal for deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> offers a promising alternative for better temporal modeling. By leveraging transformers, we hypothesize that we can achieve more accurate and robust pose estimation, particularly by focusing on refining the latent representations of sensor data and improving the rotation regression.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As illustrated in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our method uses frozen image and inertial encoders to obtain latent vectors for VIO. Then, transformer layers perform fusion and pose estimation using latent vectors. Our proposed VIFT method is a ViT-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> architecture without class token, and instead of image patches, VIFT uses latent visual and inertial representations for learning temporal relations. Instead of creating new vectors from scratch, we modify the latents with the transformer using temporal relations and use output directly to estimate poses. In the last stage, we use RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for manifold-aware gradient updates for rotation regression. We show that the VIFT improves the performance of deep VIO networks compared to RNN-based pose modules.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.08769/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.2.1" class="ltx_text" style="font-size:90%;">VIFT architecture. The network consists of two fundamental sides. The first side consists of two encoders with frozen weights that map visual and inertial information to a latent space. The second side consists of sequential transformer layers followed by a fully connected layer. For backpropagation enhancement of rotation, the output is projected to <math id="S1.F1.2.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S1.F1.2.1.m1.1b"><mrow id="S1.F1.2.1.m1.1.1" xref="S1.F1.2.1.m1.1.1.cmml"><mn id="S1.F1.2.1.m1.1.1.2" xref="S1.F1.2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S1.F1.2.1.m1.1.1.1" xref="S1.F1.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S1.F1.2.1.m1.1.1.3" xref="S1.F1.2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.2.1.m1.1c"><apply id="S1.F1.2.1.m1.1.1.cmml" xref="S1.F1.2.1.m1.1.1"><times id="S1.F1.2.1.m1.1.1.1.cmml" xref="S1.F1.2.1.m1.1.1.1"></times><cn type="integer" id="S1.F1.2.1.m1.1.1.2.cmml" xref="S1.F1.2.1.m1.1.1.2">3</cn><cn type="integer" id="S1.F1.2.1.m1.1.1.3.cmml" xref="S1.F1.2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.1.m1.1d">3\times 3</annotation></semantics></math> rotation matrix representation, and RPMG (Regularized Projective Manifold Gradient) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is used.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We propose visual-inertial fusion transformer (VIFT), a novel fusion and pose estimation module for VIO based on causal transformer encoder architecture. Our contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">VIFT uses transformer layers for visual-inertial fusion and pose estimation. We find inductive biases to make transformers perform better, eliminating the possible problems arising from the small data scale.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">VIFT exploits Riemannian manifold optimization techniques for rotations, enabling the network to learn the rotations better than Euler angles and quaternions used in previous works.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">VIFT achieves state-of-the-art results with the proposed transformer module and improves performance further with manifold-aware gradients.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual-Inertial Odometry</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Visual-inertial odometry (VIO) leverages the complementary strengths of visual and inertial sensors to provide robust and accurate motion estimation. While visual odometry relies on camera images to estimate motion, it can fail in textureless environments. In contrast, inertial odometry offers high update rates using data from Inertial Measurement Units (IMUs) but requires proper initialization and is prone to drift due to biases in accelerometer and gyroscope readings. With fusion visual and inertial estimates, VIO can mitigate the weaknesses of each individual sensor. Visual measurements help correct the drift in inertial measurements by providing pose updates, while inertial measurements enhance the robustness and temporal resolution of visual estimates. This has led to the development of several VIO methods.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Geometry-based VIO methods require addressing several challenges, including the excitation of IMU biases along all axes to track biases, proper camera-IMU extrinsic calibration, and robust initialization procedures. Despite these challenges, the fusion of visual and inertial data through sophisticated filtering based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and factor graph based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> methods has significantly advanced the field, enabling more accurate and reliable motion estimation in various applications.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Geometry-based visual inertial odometry methods have several drawbacks. They require good initialization with excitation in all axes to determine IMU biases, which might not be feasible in many scenarios. Additionally, tuning various parameters is necessary for robust and accurate operation.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">VINet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, being a seminal work in the field, approached visual-inertial odometry as a sequence-to-sequence learning problem. This led to development of supervised deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> which use ground truth transformation to learn correct
transformation via regression, and self-supervised deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which perform view synthesis with estimated relative pose between images and train on pixel-wise intensity errors.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">In supervised deep VIO, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed Soft Fusion and Hard Fusion for adaptively weighting visual and inertial embeddings during inference. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed training an adaptive modality selection module inside a deep VIO network to optionally disable the visual encoder. Until recent years, fusion and pose estimation modules consisted of RNN-based networks. ATVIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> suggested using the attention mechanism for fusion in VIO. External memory-aided attention-based module is proposed in EMA-VIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Recent methods showed improvements in VIO performance with attention-based modules.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">The recent advancements induced interest in exploring transformer-based architectures for fusion and pose estimation in VIO. Transformers, known for their ability to model long-range dependencies and capture complex temporal dynamics, present a compelling alternative to traditional RNN-based approaches. By leveraging self-attention mechanisms, transformers can selectively focus on the most relevant parts of the latent visual-inertial vectors, potentially leading to a more accurate and robust fusion of visual and inertial data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">End-to-end VIO methods consist of a visual encoder, an inertial encoder, a fusion module, and a pose estimation module. The visual encoder extracts visual features from consecutive frames to provide 3D understanding of their architecture. The inertial encoder takes input from the IMU measurements between frames. The rate of IMU data is usually higher than that of cameras.
The fusion module takes the visual and inertial representation presented by encoders. The pose estimation module uses fused representations of visual and inertial information to estimate the pose.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Feature Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">VIFT architecture can be seen in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. At each timestep <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">t</annotation></semantics></math>, VIFT takes two consecutive frames <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="\mathbf{I}_{t},\mathbf{I}_{t-1}\in\mathcal{R}^{C\times W\times H}" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml"><mrow id="S3.SS1.p1.2.m2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.3.cmml"><msub id="S3.SS1.p1.2.m2.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p1.2.m2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.2.m2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml">ğˆ</mi><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.3.1" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.SS1.p1.2.m2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.3.cmml">âˆˆ</mo><msup id="S3.SS1.p1.2.m2.2.2.4" xref="S3.SS1.p1.2.m2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.2.2.4.2" xref="S3.SS1.p1.2.m2.2.2.4.2.cmml">â„›</mi><mrow id="S3.SS1.p1.2.m2.2.2.4.3" xref="S3.SS1.p1.2.m2.2.2.4.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.4.3.2" xref="S3.SS1.p1.2.m2.2.2.4.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.2.2.4.3.1" xref="S3.SS1.p1.2.m2.2.2.4.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.2.m2.2.2.4.3.3" xref="S3.SS1.p1.2.m2.2.2.4.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.2.2.4.3.1a" xref="S3.SS1.p1.2.m2.2.2.4.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.2.m2.2.2.4.3.4" xref="S3.SS1.p1.2.m2.2.2.4.3.4.cmml">H</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2"><in id="S3.SS1.p1.2.m2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.3"></in><list id="S3.SS1.p1.2.m2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2"><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2">ğˆ</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2">ğˆ</ci><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3"><minus id="S3.SS1.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.1"></minus><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p1.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.3">1</cn></apply></apply></list><apply id="S3.SS1.p1.2.m2.2.2.4.cmml" xref="S3.SS1.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.4.1.cmml" xref="S3.SS1.p1.2.m2.2.2.4">superscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.4.2.cmml" xref="S3.SS1.p1.2.m2.2.2.4.2">â„›</ci><apply id="S3.SS1.p1.2.m2.2.2.4.3.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3"><times id="S3.SS1.p1.2.m2.2.2.4.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.1"></times><ci id="S3.SS1.p1.2.m2.2.2.4.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.2">ğ¶</ci><ci id="S3.SS1.p1.2.m2.2.2.4.3.3.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.3">ğ‘Š</ci><ci id="S3.SS1.p1.2.m2.2.2.4.3.4.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.4">ğ»</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">\mathbf{I}_{t},\mathbf{I}_{t-1}\in\mathcal{R}^{C\times W\times H}</annotation></semantics></math> and IMU measurements <math id="S3.SS1.p1.3.m3.6" class="ltx_Math" alttext="\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1}" display="inline"><semantics id="S3.SS1.p1.3.m3.6a"><mrow id="S3.SS1.p1.3.m3.6.6.2" xref="S3.SS1.p1.3.m3.6.6.3.cmml"><msub id="S3.SS1.p1.3.m3.5.5.1.1" xref="S3.SS1.p1.3.m3.5.5.1.1.cmml"><mi id="S3.SS1.p1.3.m3.5.5.1.1.2" xref="S3.SS1.p1.3.m3.5.5.1.1.2.cmml">ğš</mi><mrow id="S3.SS1.p1.3.m3.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml">t</mi><mo id="S3.SS1.p1.3.m3.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.p1.3.m3.2.2.2.2.1" xref="S3.SS1.p1.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.2.1.2" xref="S3.SS1.p1.3.m3.2.2.2.2.1.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.2.2.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.2.2.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.3.m3.2.2.2.2.1.3" xref="S3.SS1.p1.3.m3.2.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="S3.SS1.p1.3.m3.6.6.2.3" xref="S3.SS1.p1.3.m3.6.6.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.6.6.2.2" xref="S3.SS1.p1.3.m3.6.6.2.2.cmml"><mi id="S3.SS1.p1.3.m3.6.6.2.2.2" xref="S3.SS1.p1.3.m3.6.6.2.2.2.cmml">Ï‰</mi><mrow id="S3.SS1.p1.3.m3.4.4.2.2" xref="S3.SS1.p1.3.m3.4.4.2.3.cmml"><mi id="S3.SS1.p1.3.m3.3.3.1.1" xref="S3.SS1.p1.3.m3.3.3.1.1.cmml">t</mi><mo id="S3.SS1.p1.3.m3.4.4.2.2.2" xref="S3.SS1.p1.3.m3.4.4.2.3.cmml">,</mo><mrow id="S3.SS1.p1.3.m3.4.4.2.2.1" xref="S3.SS1.p1.3.m3.4.4.2.2.1.cmml"><mi id="S3.SS1.p1.3.m3.4.4.2.2.1.2" xref="S3.SS1.p1.3.m3.4.4.2.2.1.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.4.4.2.2.1.1" xref="S3.SS1.p1.3.m3.4.4.2.2.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.3.m3.4.4.2.2.1.3" xref="S3.SS1.p1.3.m3.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.6b"><list id="S3.SS1.p1.3.m3.6.6.3.cmml" xref="S3.SS1.p1.3.m3.6.6.2"><apply id="S3.SS1.p1.3.m3.5.5.1.1.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.5.5.1.1.1.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.5.5.1.1.2.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1.2">ğš</ci><list id="S3.SS1.p1.3.m3.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2"><ci id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1">ğ‘¡</ci><apply id="S3.SS1.p1.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1"><minus id="S3.SS1.p1.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1.1"></minus><ci id="S3.SS1.p1.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p1.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1.3">1</cn></apply></list></apply><apply id="S3.SS1.p1.3.m3.6.6.2.2.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.6.6.2.2.1.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.6.6.2.2.2.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2.2">ğœ”</ci><list id="S3.SS1.p1.3.m3.4.4.2.3.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2"><ci id="S3.SS1.p1.3.m3.3.3.1.1.cmml" xref="S3.SS1.p1.3.m3.3.3.1.1">ğ‘¡</ci><apply id="S3.SS1.p1.3.m3.4.4.2.2.1.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1"><minus id="S3.SS1.p1.3.m3.4.4.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1.1"></minus><ci id="S3.SS1.p1.3.m3.4.4.2.2.1.2.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p1.3.m3.4.4.2.2.1.3.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1.3">1</cn></apply></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.6c">\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1}</annotation></semantics></math> between frames, consisting of accelerometer and gyroscope readings.
Visual and inertial measurements are processed with different encoders for their modality as in previous deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, following the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> we use FlowNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> based image encoder and 1D CNN based inertial encoder.
Visual measurements are processed by image encoder <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{E}_{v}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ğ„</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ„</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathbf{E}_{v}</annotation></semantics></math> to produce one-dimensional visual encodings <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{x}^{v}_{t}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msubsup id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">ğ±</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">t</mi><mi id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml">v</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">ğ±</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3">ğ‘£</ci></apply><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{x}^{v}_{t}</annotation></semantics></math>.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathbf{x}^{v}_{t}=\mathbf{E}_{v}(\mathbf{I}_{t},\mathbf{I_{t-1}})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msubsup id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mi id="S3.E1.m1.2.2.4.2.2" xref="S3.E1.m1.2.2.4.2.2.cmml">ğ±</mi><mi id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml">t</mi><mi id="S3.E1.m1.2.2.4.2.3" xref="S3.E1.m1.2.2.4.2.3.cmml">v</mi></msubsup><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">ğ„</mi><mi id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml">ğˆ</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.2.2.3.2.cmml">ğ­</mi><mo id="S3.E1.m1.2.2.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.E1.m1.2.2.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.2.2.3.3.cmml">ğŸ</mn></mrow></msub><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.5" xref="S3.E1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4">subscript</csymbol><apply id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.1.cmml" xref="S3.E1.m1.2.2.4">superscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.cmml" xref="S3.E1.m1.2.2.4.2.2">ğ±</ci><ci id="S3.E1.m1.2.2.4.2.3.cmml" xref="S3.E1.m1.2.2.4.2.3">ğ‘£</ci></apply><ci id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">ğ„</ci><ci id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3">ğ‘£</ci></apply><interval closure="open" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">ğˆ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">ğˆ</ci><apply id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3"><minus id="S3.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.1"></minus><ci id="S3.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.2">ğ­</ci><cn type="integer" id="S3.E1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathbf{x}^{v}_{t}=\mathbf{E}_{v}(\mathbf{I}_{t},\mathbf{I_{t-1}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.7" class="ltx_p">Inertial measurements are processed by inertial encoder <math id="S3.SS1.p1.6.m1.1" class="ltx_Math" alttext="\mathbf{E}_{i}" display="inline"><semantics id="S3.SS1.p1.6.m1.1a"><msub id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml"><mi id="S3.SS1.p1.6.m1.1.1.2" xref="S3.SS1.p1.6.m1.1.1.2.cmml">ğ„</mi><mi id="S3.SS1.p1.6.m1.1.1.3" xref="S3.SS1.p1.6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><apply id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m1.1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m1.1.1.2.cmml" xref="S3.SS1.p1.6.m1.1.1.2">ğ„</ci><ci id="S3.SS1.p1.6.m1.1.1.3.cmml" xref="S3.SS1.p1.6.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">\mathbf{E}_{i}</annotation></semantics></math> to produce one-dimensional inertial encodings <math id="S3.SS1.p1.7.m2.1" class="ltx_Math" alttext="\mathbf{x}^{i}_{t}" display="inline"><semantics id="S3.SS1.p1.7.m2.1a"><msubsup id="S3.SS1.p1.7.m2.1.1" xref="S3.SS1.p1.7.m2.1.1.cmml"><mi id="S3.SS1.p1.7.m2.1.1.2.2" xref="S3.SS1.p1.7.m2.1.1.2.2.cmml">ğ±</mi><mi id="S3.SS1.p1.7.m2.1.1.3" xref="S3.SS1.p1.7.m2.1.1.3.cmml">t</mi><mi id="S3.SS1.p1.7.m2.1.1.2.3" xref="S3.SS1.p1.7.m2.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.1b"><apply id="S3.SS1.p1.7.m2.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.1.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.7.m2.1.1.2.cmml" xref="S3.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.1.1.2.1.cmml" xref="S3.SS1.p1.7.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m2.1.1.2.2.cmml" xref="S3.SS1.p1.7.m2.1.1.2.2">ğ±</ci><ci id="S3.SS1.p1.7.m2.1.1.2.3.cmml" xref="S3.SS1.p1.7.m2.1.1.2.3">ğ‘–</ci></apply><ci id="S3.SS1.p1.7.m2.1.1.3.cmml" xref="S3.SS1.p1.7.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.1c">\mathbf{x}^{i}_{t}</annotation></semantics></math>.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="\mathbf{x}^{i}_{t}=\mathbf{E}_{i}(\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1})" display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><msubsup id="S3.E2.m1.6.6.4" xref="S3.E2.m1.6.6.4.cmml"><mi id="S3.E2.m1.6.6.4.2.2" xref="S3.E2.m1.6.6.4.2.2.cmml">ğ±</mi><mi id="S3.E2.m1.6.6.4.3" xref="S3.E2.m1.6.6.4.3.cmml">t</mi><mi id="S3.E2.m1.6.6.4.2.3" xref="S3.E2.m1.6.6.4.2.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.6.6.3" xref="S3.E2.m1.6.6.3.cmml">=</mo><mrow id="S3.E2.m1.6.6.2" xref="S3.E2.m1.6.6.2.cmml"><msub id="S3.E2.m1.6.6.2.4" xref="S3.E2.m1.6.6.2.4.cmml"><mi id="S3.E2.m1.6.6.2.4.2" xref="S3.E2.m1.6.6.2.4.2.cmml">ğ„</mi><mi id="S3.E2.m1.6.6.2.4.3" xref="S3.E2.m1.6.6.2.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.2.3" xref="S3.E2.m1.6.6.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.6.6.2.2.2" xref="S3.E2.m1.6.6.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.2.2.2.3" xref="S3.E2.m1.6.6.2.2.3.cmml">(</mo><msub id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">ğš</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">t</mi><mo id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">t</mi><mo id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">âˆ’</mo><mn id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="S3.E2.m1.6.6.2.2.2.4" xref="S3.E2.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.2.2.2.2" xref="S3.E2.m1.6.6.2.2.2.2.cmml"><mi id="S3.E2.m1.6.6.2.2.2.2.2" xref="S3.E2.m1.6.6.2.2.2.2.2.cmml">Ï‰</mi><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.2.1.cmml"><mi id="S3.E2.m1.4.4.2.2.1.2" xref="S3.E2.m1.4.4.2.2.1.2.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.cmml">âˆ’</mo><mn id="S3.E2.m1.4.4.2.2.1.3" xref="S3.E2.m1.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo stretchy="false" id="S3.E2.m1.6.6.2.2.2.5" xref="S3.E2.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.3.cmml" xref="S3.E2.m1.6.6.3"></eq><apply id="S3.E2.m1.6.6.4.cmml" xref="S3.E2.m1.6.6.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.1.cmml" xref="S3.E2.m1.6.6.4">subscript</csymbol><apply id="S3.E2.m1.6.6.4.2.cmml" xref="S3.E2.m1.6.6.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.2.1.cmml" xref="S3.E2.m1.6.6.4">superscript</csymbol><ci id="S3.E2.m1.6.6.4.2.2.cmml" xref="S3.E2.m1.6.6.4.2.2">ğ±</ci><ci id="S3.E2.m1.6.6.4.2.3.cmml" xref="S3.E2.m1.6.6.4.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.6.6.4.3.cmml" xref="S3.E2.m1.6.6.4.3">ğ‘¡</ci></apply><apply id="S3.E2.m1.6.6.2.cmml" xref="S3.E2.m1.6.6.2"><times id="S3.E2.m1.6.6.2.3.cmml" xref="S3.E2.m1.6.6.2.3"></times><apply id="S3.E2.m1.6.6.2.4.cmml" xref="S3.E2.m1.6.6.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.4.1.cmml" xref="S3.E2.m1.6.6.2.4">subscript</csymbol><ci id="S3.E2.m1.6.6.2.4.2.cmml" xref="S3.E2.m1.6.6.2.4.2">ğ„</ci><ci id="S3.E2.m1.6.6.2.4.3.cmml" xref="S3.E2.m1.6.6.2.4.3">ğ‘–</ci></apply><interval closure="open" id="S3.E2.m1.6.6.2.2.3.cmml" xref="S3.E2.m1.6.6.2.2.2"><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2">ğš</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘¡</ci><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><minus id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></minus><ci id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">ğ‘¡</ci><cn type="integer" id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3">1</cn></apply></list></apply><apply id="S3.E2.m1.6.6.2.2.2.2.cmml" xref="S3.E2.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.2.2.2.1.cmml" xref="S3.E2.m1.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.2.2.2.2.2.cmml" xref="S3.E2.m1.6.6.2.2.2.2.2">ğœ”</ci><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2"><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">ğ‘¡</ci><apply id="S3.E2.m1.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1"><minus id="S3.E2.m1.4.4.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"></minus><ci id="S3.E2.m1.4.4.2.2.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.2">ğ‘¡</ci><cn type="integer" id="S3.E2.m1.4.4.2.2.1.3.cmml" xref="S3.E2.m1.4.4.2.2.1.3">1</cn></apply></list></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathbf{x}^{i}_{t}=\mathbf{E}_{i}(\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.10" class="ltx_p">At the end, we concatenate visual and inertial latent vectors.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\mathbf{x}_{t}=\textit{Concat}(\mathbf{x}^{v}_{t},\mathbf{x}^{i}_{t})" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><msub id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><mi id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml">ğ±</mi><mi id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml">t</mi></msub><mo id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">=</mo><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">ğ±</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">t</mi><mi id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S3.E3.m1.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml">ğ±</mi><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">t</mi><mi id="S3.E3.m1.2.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E3.m1.2.2.2.2.2.5" xref="S3.E3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"></eq><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4">subscript</csymbol><ci id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2">ğ±</ci><ci id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><ci id="S3.E3.m1.2.2.2.4a.cmml" xref="S3.E3.m1.2.2.2.4"><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">Concat</mtext></ci><interval closure="open" id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">ğ±</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">ğ‘£</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2">ğ±</ci><ci id="S3.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">ğ‘¡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\mathbf{x}_{t}=\textit{Concat}(\mathbf{x}^{v}_{t},\mathbf{x}^{i}_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.9" class="ltx_p">The corresponding <math id="S3.SS1.p1.8.m1.1" class="ltx_Math" alttext="\mathbf{x_{t}}" display="inline"><semantics id="S3.SS1.p1.8.m1.1a"><msub id="S3.SS1.p1.8.m1.1.1" xref="S3.SS1.p1.8.m1.1.1.cmml"><mi id="S3.SS1.p1.8.m1.1.1.2" xref="S3.SS1.p1.8.m1.1.1.2.cmml">ğ±</mi><mi id="S3.SS1.p1.8.m1.1.1.3" xref="S3.SS1.p1.8.m1.1.1.3.cmml">ğ­</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m1.1b"><apply id="S3.SS1.p1.8.m1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m1.1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m1.1.1.2.cmml" xref="S3.SS1.p1.8.m1.1.1.2">ğ±</ci><ci id="S3.SS1.p1.8.m1.1.1.3.cmml" xref="S3.SS1.p1.8.m1.1.1.3">ğ­</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m1.1c">\mathbf{x_{t}}</annotation></semantics></math> is labeled as Latent Vector in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The concatenated vector is passed to the transformer-based fusion and pose estimation module. The expected output is the translation and rotation of the camera between two timesteps <math id="S3.SS1.p1.9.m2.1" class="ltx_Math" alttext="\mathbf{T}_{t-1}^{t}\in\textbf{SE}(3)" display="inline"><semantics id="S3.SS1.p1.9.m2.1a"><mrow id="S3.SS1.p1.9.m2.1.2" xref="S3.SS1.p1.9.m2.1.2.cmml"><msubsup id="S3.SS1.p1.9.m2.1.2.2" xref="S3.SS1.p1.9.m2.1.2.2.cmml"><mi id="S3.SS1.p1.9.m2.1.2.2.2.2" xref="S3.SS1.p1.9.m2.1.2.2.2.2.cmml">ğ“</mi><mrow id="S3.SS1.p1.9.m2.1.2.2.2.3" xref="S3.SS1.p1.9.m2.1.2.2.2.3.cmml"><mi id="S3.SS1.p1.9.m2.1.2.2.2.3.2" xref="S3.SS1.p1.9.m2.1.2.2.2.3.2.cmml">t</mi><mo id="S3.SS1.p1.9.m2.1.2.2.2.3.1" xref="S3.SS1.p1.9.m2.1.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.9.m2.1.2.2.2.3.3" xref="S3.SS1.p1.9.m2.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.9.m2.1.2.2.3" xref="S3.SS1.p1.9.m2.1.2.2.3.cmml">t</mi></msubsup><mo id="S3.SS1.p1.9.m2.1.2.1" xref="S3.SS1.p1.9.m2.1.2.1.cmml">âˆˆ</mo><mrow id="S3.SS1.p1.9.m2.1.2.3" xref="S3.SS1.p1.9.m2.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.9.m2.1.2.3.2" xref="S3.SS1.p1.9.m2.1.2.3.2a.cmml">SE</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m2.1.2.3.1" xref="S3.SS1.p1.9.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.9.m2.1.2.3.3.2" xref="S3.SS1.p1.9.m2.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m2.1.2.3.3.2.1" xref="S3.SS1.p1.9.m2.1.2.3.cmml">(</mo><mn id="S3.SS1.p1.9.m2.1.1" xref="S3.SS1.p1.9.m2.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS1.p1.9.m2.1.2.3.3.2.2" xref="S3.SS1.p1.9.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m2.1b"><apply id="S3.SS1.p1.9.m2.1.2.cmml" xref="S3.SS1.p1.9.m2.1.2"><in id="S3.SS1.p1.9.m2.1.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.1"></in><apply id="S3.SS1.p1.9.m2.1.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m2.1.2.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2">superscript</csymbol><apply id="S3.SS1.p1.9.m2.1.2.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m2.1.2.2.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.9.m2.1.2.2.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.2">ğ“</ci><apply id="S3.SS1.p1.9.m2.1.2.2.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3"><minus id="S3.SS1.p1.9.m2.1.2.2.2.3.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3.1"></minus><ci id="S3.SS1.p1.9.m2.1.2.2.2.3.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS1.p1.9.m2.1.2.2.2.3.3.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.9.m2.1.2.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.2.3">ğ‘¡</ci></apply><apply id="S3.SS1.p1.9.m2.1.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.3"><times id="S3.SS1.p1.9.m2.1.2.3.1.cmml" xref="S3.SS1.p1.9.m2.1.2.3.1"></times><ci id="S3.SS1.p1.9.m2.1.2.3.2a.cmml" xref="S3.SS1.p1.9.m2.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.9.m2.1.2.3.2.cmml" xref="S3.SS1.p1.9.m2.1.2.3.2">SE</mtext></ci><cn type="integer" id="S3.SS1.p1.9.m2.1.1.cmml" xref="S3.SS1.p1.9.m2.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m2.1c">\mathbf{T}_{t-1}^{t}\in\textbf{SE}(3)</annotation></semantics></math>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.08769/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Causal transformer based architecture for fusion and pose estimation.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transformers For Fusion and Pose Estimation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use transformer encoder layers with causal masks to update latent vectors for pose estimation. The fusion and pose estimation module modifies each latent vector with an attention mechanism. Weighting the latent vectors based on data-dependent masks is introduced in Soft Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, where a soft mask function is determined based on the current latent visual inertial vector.
The difference in VIFT is determining the mask based on previous measurements in the local window and applying several masks multiple times through the layers of the transformer. In the end, the corresponding vector is modified with past information to provide more accurate pose estimation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">The fusion and pose estimation module of VIFT can be seen in Figure <a href="#S3.F2" title="Figure 2 â€£ 3.1 Feature Encoder â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In Figure <a href="#S3.F2" title="Figure 2 â€£ 3.1 Feature Encoder â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the input latent vectors <math id="S3.SS2.p2.1.m1.3" class="ltx_Math" alttext="\mathbf{x}_{t},\dots,\mathbf{x}_{t-N}" display="inline"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3.2" xref="S3.SS2.p2.1.m1.3.3.3.cmml"><msub id="S3.SS2.p2.1.m1.2.2.1.1" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS2.p2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p2.1.m1.2.2.1.1.3" xref="S3.SS2.p2.1.m1.2.2.1.1.3.cmml">t</mi></msub><mo id="S3.SS2.p2.1.m1.3.3.2.3" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">â€¦</mi><mo id="S3.SS2.p2.1.m1.3.3.2.4" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.3.3.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.2.cmml">ğ±</mi><mrow id="S3.SS2.p2.1.m1.3.3.2.2.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.3.2" xref="S3.SS2.p2.1.m1.3.3.2.2.3.2.cmml">t</mi><mo id="S3.SS2.p2.1.m1.3.3.2.2.3.1" xref="S3.SS2.p2.1.m1.3.3.2.2.3.1.cmml">âˆ’</mo><mi id="S3.SS2.p2.1.m1.3.3.2.2.3.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><list id="S3.SS2.p2.1.m1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2"><apply id="S3.SS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2">ğ±</ci><ci id="S3.SS2.p2.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.3">ğ‘¡</ci></apply><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">â€¦</ci><apply id="S3.SS2.p2.1.m1.3.3.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2">ğ±</ci><apply id="S3.SS2.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3"><minus id="S3.SS2.p2.1.m1.3.3.2.2.3.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.1"></minus><ci id="S3.SS2.p2.1.m1.3.3.2.2.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.2">ğ‘¡</ci><ci id="S3.SS2.p2.1.m1.3.3.2.2.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.3">ğ‘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">\mathbf{x}_{t},\dots,\mathbf{x}_{t-N}</annotation></semantics></math> are shown on the left. In the first step, similar to ViT, we apply linear projection to visual-inertial latent vector <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{x}_{t}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ±</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathbf{x}_{t}</annotation></semantics></math>. This projection applies learned weights to visual and inertial encodings. We keep the embedding dimension of the transformer the same with a concatenated visual inertial latent vector.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">After this step, transformer Layers take <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{t}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ğ±</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{x}_{t}</annotation></semantics></math> together with N-1 past measurements <math id="S3.SS2.p3.2.m2.3" class="ltx_Math" alttext="\mathbf{x}_{t-1},\dots,\mathbf{x}_{t-N}" display="inline"><semantics id="S3.SS2.p3.2.m2.3a"><mrow id="S3.SS2.p3.2.m2.3.3.2" xref="S3.SS2.p3.2.m2.3.3.3.cmml"><msub id="S3.SS2.p3.2.m2.2.2.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.2" xref="S3.SS2.p3.2.m2.2.2.1.1.2.cmml">ğ±</mi><mrow id="S3.SS2.p3.2.m2.2.2.1.1.3" xref="S3.SS2.p3.2.m2.2.2.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.3.2" xref="S3.SS2.p3.2.m2.2.2.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p3.2.m2.2.2.1.1.3.1" xref="S3.SS2.p3.2.m2.2.2.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p3.2.m2.2.2.1.1.3.3" xref="S3.SS2.p3.2.m2.2.2.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p3.2.m2.3.3.2.3" xref="S3.SS2.p3.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS2.p3.2.m2.3.3.2.4" xref="S3.SS2.p3.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS2.p3.2.m2.3.3.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.cmml"><mi id="S3.SS2.p3.2.m2.3.3.2.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.2.cmml">ğ±</mi><mrow id="S3.SS2.p3.2.m2.3.3.2.2.3" xref="S3.SS2.p3.2.m2.3.3.2.2.3.cmml"><mi id="S3.SS2.p3.2.m2.3.3.2.2.3.2" xref="S3.SS2.p3.2.m2.3.3.2.2.3.2.cmml">t</mi><mo id="S3.SS2.p3.2.m2.3.3.2.2.3.1" xref="S3.SS2.p3.2.m2.3.3.2.2.3.1.cmml">âˆ’</mo><mi id="S3.SS2.p3.2.m2.3.3.2.2.3.3" xref="S3.SS2.p3.2.m2.3.3.2.2.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.3b"><list id="S3.SS2.p3.2.m2.3.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2"><apply id="S3.SS2.p3.2.m2.2.2.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.2">ğ±</ci><apply id="S3.SS2.p3.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3"><minus id="S3.SS2.p3.2.m2.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3.1"></minus><ci id="S3.SS2.p3.2.m2.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.p3.2.m2.2.2.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">â€¦</ci><apply id="S3.SS2.p3.2.m2.3.3.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2">ğ±</ci><apply id="S3.SS2.p3.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3"><minus id="S3.SS2.p3.2.m2.3.3.2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.1"></minus><ci id="S3.SS2.p3.2.m2.3.3.2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.2">ğ‘¡</ci><ci id="S3.SS2.p3.2.m2.3.3.2.2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.3">ğ‘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.3c">\mathbf{x}_{t-1},\dots,\mathbf{x}_{t-N}</annotation></semantics></math> as input.
Then, we add Sinusoidal Positional Encodings to each latent vector according to their location in the sequence.
No temporal information flows between the linear projectionâ€™s latent visual inertial vector sequence and the Positional Encoding steps.
Transformer Layers modify the resulting latent vectors with attention by considering a local window.
In this step, latent vectors are weighted based on the previous and current measurements.
The operation of the transformer layer can be summarized as follows.
A transformer layer consists of a masked multi-head attention (MMHA) layer, feed-forward layer, residual connections, and layer normalizations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
The attention operation computes a weighted sum of value vectors based on the normalized dot product of query and key vectors. With a causal mask, we ensure that current estimates are not influenced by future measurements, which is crucial for real-time applications.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">After transformer layers, we apply 2-layer MLP to every feature to obtain pose output. We follow Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and give a 6-dimensional output consisting of translation and Euler angles for rotation. Ultimately, we obtain <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">N</annotation></semantics></math> relative poses for <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="N+1" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">+</mo><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><plus id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></plus><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ğ‘</ci><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">N+1</annotation></semantics></math> input images.
During inference, for the first <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="N+1" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">+</mo><mn id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><plus id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1"></plus><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ğ‘</ci><cn type="integer" id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">N+1</annotation></semantics></math> images, we use all the output pose estimates to initialize the process and take only the last estimate after shifting latent vectors and adding a new latent visual inertial vector to the end of the sequence.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deep Rotation Regression</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The manifold structure of <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">SO<math id="S3.SS3.p1.1.1.m1.1" class="ltx_Math" alttext="(3)" display="inline"><semantics id="S3.SS3.p1.1.1.m1.1a"><mrow id="S3.SS3.p1.1.1.m1.1.2.2"><mo stretchy="false" id="S3.SS3.p1.1.1.m1.1.2.2.1">(</mo><mn id="S3.SS3.p1.1.1.m1.1.1" xref="S3.SS3.p1.1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS3.p1.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.1.m1.1b"><cn type="integer" id="S3.SS3.p1.1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.1.m1.1c">(3)</annotation></semantics></math></span> space should be considered while performing optimizations on rotations. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> discuss the importance of continuous representations in neural network optimization. A continuous subset of rotations can be discontinuous in the Euler angle representation of rotations, which could create discontinuous training signals in training. Moreover, the interpolation problems in Euler angles and gimbal lock phenomena further motivate using the optimization techniques for manifolds.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">We use the Regularized Projective Manifold Gradient (RPMG) layer proposed by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to obtain training signals for rotations, which produces manifold-aware gradients in backward passes. We first convert our 3D estimation to a 9D rotation matrix. Then, we calculate the loss between ground truth rotation <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{R}_{\text{gt}}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ‘</mi><mtext id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3a.cmml">gt</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS3.p2.1.m1.1.1.3a.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathbf{R}_{\text{gt}}</annotation></semantics></math> and estimated rotation <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{R}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathbf{R}</annotation></semantics></math>.
For translations, we use direct regression as the space is already Euclidean. The total loss is the weighted sum of rotation and translation losses.</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\mathcal{L}_{r}=||\mathbf{R}-\mathbf{R}_{\text{gt}}||,\quad\mathcal{L}_{t}=||\mathbf{t}-\mathbf{t}_{\text{gt}}||,\quad\mathcal{L}=\mathcal{L}_{t}+\alpha\mathcal{L}_{r}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">â„’</mi><mi id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml">r</mi></msub><mo id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">ğ‘</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.3.2.cmml">ğ‘</mi><mtext id="S3.E4.m1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow></mrow><mo rspace="1.167em" id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.3.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><msub id="S3.E4.m1.2.2.2.2.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.1.1.3.2" xref="S3.E4.m1.2.2.2.2.1.1.3.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.3.3.cmml">t</mi></msub><mo id="S3.E4.m1.2.2.2.2.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml">ğ­</mi><mo id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.cmml">ğ­</mi><mtext id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo stretchy="false" id="S3.E4.m1.2.2.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml">â€–</mo></mrow></mrow><mo rspace="1.167em" id="S3.E4.m1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.2.cmml">â„’</mi><mo id="S3.E4.m1.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.E4.m1.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.cmml"><msub id="S3.E4.m1.2.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.3.2.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.2.2.3.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.2.3.cmml">t</mi></msub><mo id="S3.E4.m1.2.2.2.2.2.2.3.1" xref="S3.E4.m1.2.2.2.2.2.2.3.1.cmml">+</mo><mrow id="S3.E4.m1.2.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.3.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.3.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.2.2.3.3.1" xref="S3.E4.m1.2.2.2.2.2.2.3.3.1.cmml">â€‹</mo><msub id="S3.E4.m1.2.2.2.2.2.2.3.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.3.3.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.2.2.3.3.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.3.cmml">r</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">â„’</ci><ci id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3">ğ‘Ÿ</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">ğ‘</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1"><eq id="S3.E4.m1.2.2.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2"></eq><apply id="S3.E4.m1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.3">ğ‘¡</ci></apply><apply id="S3.E4.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1"><minus id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1"></minus><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2">ğ­</ci><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2">ğ­</ci><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2"><eq id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1"></eq><ci id="S3.E4.m1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.2">â„’</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3"><plus id="S3.E4.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.1"></plus><apply id="S3.E4.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.3.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2.3">ğ‘¡</ci></apply><apply id="S3.E4.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3"><times id="S3.E4.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.2">ğ›¼</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.3.3.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.3">ğ‘Ÿ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathcal{L}_{r}=||\mathbf{R}-\mathbf{R}_{\text{gt}}||,\quad\mathcal{L}_{t}=||\mathbf{t}-\mathbf{t}_{\text{gt}}||,\quad\mathcal{L}=\mathcal{L}_{t}+\alpha\mathcal{L}_{r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">where <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\alpha</annotation></semantics></math> is a constant factor to balance rotational and translational loss terms and <math id="S3.SS3.p3.2.m2.1" class="ltx_math_unparsed" alttext="||\cdot||" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS3.p3.2.m2.1.1">|</mo><mo fence="false" stretchy="false" id="S3.SS3.p3.2.m2.1.2">|</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.3">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS3.p3.2.m2.1.4">|</mo><mo fence="false" stretchy="false" id="S3.SS3.p3.2.m2.1.5">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">||\cdot||</annotation></semantics></math> is a norm. In the backward pass, we calculate RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for rotation loss, which uses Riemannian optimization to get a goal rotation <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{R}_{g}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">ğ‘</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathbf{R}_{g}</annotation></semantics></math> and maps it back to the representation manifold to find the closest element of ambient space to estimation, which is used for obtaining the gradients in the backward pass.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate VIFT and our choices in training settings and model selection. We show transformer based modules are effective in fusion and pose estimation in deep VIO.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">We utilize KITTI Odometry Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for training and benchmarking in this study. The dataset is widely used amongst the visual-inertial odometry research community and consists of 22 sequences where those sequences have stereo-recorded images and 6-Degree-of-Freedom IMU measurements. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we train our method with Sequences 00, 01, 02, 04, 06, and 09 while choosing Sequences 05, 07, and 10 for testing. We do not use Sequence 03 as it misses the IMU. The input images and ground truth poses are recorded at a rate of 10 Hz, whereas IMU data is recorded at 100 Hz. As the challenge requires monocular images, only left-camera frames are used throughout the study. For evaluation metric, relative translation error <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\textit{t}_{\textit{rel}}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2a.cmml">t</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2a.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">t</mtext></ci><ci id="S4.SS1.p1.1.m1.1.1.3a.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\textit{t}_{\textit{rel}}</annotation></semantics></math> and relative rotation error <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\textit{r}_{\textit{rel}}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2a.cmml" xref="S4.SS1.p1.2.m2.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">r</mtext></ci><ci id="S4.SS1.p1.2.m2.1.1.3a.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\textit{r}_{\textit{rel}}</annotation></semantics></math> are calculated, indicating the averaged translation and rotation drift of all subsequences with length of (100 m, â€¦, 800 m).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.8" class="ltx_p">We utilize pretrained FlowNet-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> based image encoder and 1D CNN based inertial encoder from Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and keep them fixed during training. Input images are resized to <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="512\times 256" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">512</cn><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">512\times 256</annotation></semantics></math> resolution. Our training scheduler follows a cosine annealing learning rate with warm restarts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, with restarts occurring every 25 epochs. We employ the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> optimizer with a learning rate of <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><msup id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p2.2.m2.1.1.3.3a" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS1.p2.2.m2.1.1.3.3.2" xref="S4.SS1.p2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">1</cn><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"><minus id="S4.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">1\times 10^{-4}</annotation></semantics></math>, we set <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p2.3.m3.1.1.2.3" xref="S4.SS1.p2.3.m3.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p2.3.m3.1.1.2.3.cmml" xref="S4.SS1.p2.3.m3.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><msub id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2.2" xref="S4.SS1.p2.4.m4.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p2.4.m4.1.1.2.3" xref="S4.SS1.p2.4.m4.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><eq id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></eq><apply id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.2.1.cmml" xref="S4.SS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p2.4.m4.1.1.2.3.cmml" xref="S4.SS1.p2.4.m4.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\beta_{2}=0.999</annotation></semantics></math>. The loss function in Equation <a href="#S3.E4" title="Equation 4 â€£ 3.3 Deep Rotation Regression â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> includes a rotation weight of <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\alpha=40" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mi id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><eq id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1"></eq><ci id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\alpha=40</annotation></semantics></math> and uses the L1 norm. We use RPMG layer for rotation estimation with <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\tau=\frac{1}{4}" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">Ï„</mi><mo id="S4.SS1.p2.6.m6.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.cmml">=</mo><mfrac id="S4.SS1.p2.6.m6.1.1.3" xref="S4.SS1.p2.6.m6.1.1.3.cmml"><mn id="S4.SS1.p2.6.m6.1.1.3.2" xref="S4.SS1.p2.6.m6.1.1.3.2.cmml">1</mn><mn id="S4.SS1.p2.6.m6.1.1.3.3" xref="S4.SS1.p2.6.m6.1.1.3.3.cmml">4</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><eq id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1"></eq><ci id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">ğœ</ci><apply id="S4.SS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3"><divide id="S4.SS1.p2.6.m6.1.1.3.1.cmml" xref="S4.SS1.p2.6.m6.1.1.3"></divide><cn type="integer" id="S4.SS1.p2.6.m6.1.1.3.2.cmml" xref="S4.SS1.p2.6.m6.1.1.3.2">1</cn><cn type="integer" id="S4.SS1.p2.6.m6.1.1.3.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\tau=\frac{1}{4}</annotation></semantics></math> and <math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="\lambda=0.01" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mrow id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">Î»</mi><mo id="S4.SS1.p2.7.m7.1.1.1" xref="S4.SS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><eq id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1.1"></eq><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">ğœ†</ci><cn type="float" id="S4.SS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">\lambda=0.01</annotation></semantics></math>. We use a sequence length of <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="N=11" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml">N</mi><mo id="S4.SS1.p2.8.m8.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><eq id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1"></eq><ci id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2">ğ‘</ci><cn type="integer" id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">N=11</annotation></semantics></math> during training.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Our transformer model is configured with an embedding dimension of 768 and a feed-forward layer dimension of 128. The embedding dimension is selected to be equal to the sum of 512 and 256, which are the output dimensions of the image encoder and inertial encoder, respectively. The model uses Sinusoidal Positional Encodings, consists of 4 transformer encoder layers with 6 attention heads, and employs masked self-attention with a causal mask. We do not apply dropout in the transformer. The network is trained for 200 epochs with a batch size of 128, totaling 27k training steps. Experiments are conducted using an NVIDIA GeForce RTX 4060 Laptop GPU.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.10.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.11.2" class="ltx_text" style="font-size:90%;">Comparison with prior VIO works in translational &amp; rotational error metrics of KITTI Odometry Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The best performances in each block are marked in <span id="S4.T1.11.2.1" class="ltx_text ltx_font_bold">bold</span> and overall bests are shown with <span id="S4.T1.11.2.2" class="ltx_text" style="background-color:#BFFFBF;">green</span> background. Loop closure is excluded for VINS-Mono. Results are taken from Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> except ours.</span></figcaption>
<table id="S4.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.6.7.1" class="ltx_tr">
<th id="S4.T1.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.6.7.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.6.7.1.2.1" class="ltx_text">Method</span></td>
<th id="S4.T1.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2">Seq. 05</th>
<th id="S4.T1.6.7.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2">Seq. 07</th>
<th id="S4.T1.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Seq. 10</th>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.6.6.7" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1b"><msub id="S4.T1.1.1.1.m1.1.1"><mi mathsize="50%" id="S4.T1.1.1.1.m1.1.1.2">t</mi><mrow id="S4.T1.1.1.1.m1.1.1.3"><mi mathsize="50%" id="S4.T1.1.1.1.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.1.1.1.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.1.1.1.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.1.1.1.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T1.1.1.1.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T1.1.1.1.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T1.1.1.1.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.2.2.2.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mrow id="S4.T1.2.2.2.m1.1b"><msub id="S4.T1.2.2.2.m1.1.1"><mi mathsize="50%" id="S4.T1.2.2.2.m1.1.1.2">r</mi><mrow id="S4.T1.2.2.2.m1.1.1.3"><mi mathsize="50%" id="S4.T1.2.2.2.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.2.2.2.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.2.2.2.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.2.2.2.m1.1.2"><msup id="S4.T1.2.2.2.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T1.2.2.2.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T1.2.2.2.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T1.2.2.2.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></td>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.3.3.3.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mrow id="S4.T1.3.3.3.m1.1b"><msub id="S4.T1.3.3.3.m1.1.1"><mi mathsize="50%" id="S4.T1.3.3.3.m1.1.1.2">t</mi><mrow id="S4.T1.3.3.3.m1.1.1.3"><mi mathsize="50%" id="S4.T1.3.3.3.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.3.3.3.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.3.3.3.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.3.3.3.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T1.3.3.3.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T1.3.3.3.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T1.3.3.3.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.4.4.4.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><mrow id="S4.T1.4.4.4.m1.1b"><msub id="S4.T1.4.4.4.m1.1.1"><mi mathsize="50%" id="S4.T1.4.4.4.m1.1.1.2">r</mi><mrow id="S4.T1.4.4.4.m1.1.1.3"><mi mathsize="50%" id="S4.T1.4.4.4.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.4.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.4.4.4.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.4.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.4.4.4.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.4.4.4.m1.1.2"><msup id="S4.T1.4.4.4.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T1.4.4.4.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T1.4.4.4.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T1.4.4.4.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></td>
<th id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T1.5.5.5.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T1.5.5.5.m1.1a"><mrow id="S4.T1.5.5.5.m1.1b"><msub id="S4.T1.5.5.5.m1.1.1"><mi mathsize="50%" id="S4.T1.5.5.5.m1.1.1.2">t</mi><mrow id="S4.T1.5.5.5.m1.1.1.3"><mi mathsize="50%" id="S4.T1.5.5.5.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.5.5.5.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.5.5.5.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.5.5.5.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.5.5.5.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.5.5.5.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T1.5.5.5.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T1.5.5.5.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T1.5.5.5.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_center"><math id="S4.T1.6.6.6.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T1.6.6.6.m1.1a"><mrow id="S4.T1.6.6.6.m1.1b"><msub id="S4.T1.6.6.6.m1.1.1"><mi mathsize="50%" id="S4.T1.6.6.6.m1.1.1.2">r</mi><mrow id="S4.T1.6.6.6.m1.1.1.3"><mi mathsize="50%" id="S4.T1.6.6.6.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.6.6.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T1.6.6.6.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.6.6.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T1.6.6.6.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T1.6.6.6.m1.1.2"><msup id="S4.T1.6.6.6.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T1.6.6.6.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T1.6.6.6.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T1.6.6.6.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.6.8.2" class="ltx_tr">
<th id="S4.T1.6.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Geo</th>
<td id="S4.T1.6.8.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VINS-Mono <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<th id="S4.T1.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.6.8.2.3.1" class="ltx_text ltx_font_bold">11.6</span></th>
<td id="S4.T1.6.8.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.6.8.2.4.1" class="ltx_text ltx_font_bold">1.26</span></td>
<th id="S4.T1.6.8.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.6.8.2.5.1" class="ltx_text ltx_font_bold">10.0</span></th>
<td id="S4.T1.6.8.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.6.8.2.6.1" class="ltx_text ltx_font_bold">1.72</span></td>
<th id="S4.T1.6.8.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.6.8.2.7.1" class="ltx_text ltx_font_bold">16.5</span></th>
<td id="S4.T1.6.8.2.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.6.8.2.8.1" class="ltx_text ltx_font_bold">2.34</span></td>
</tr>
<tr id="S4.T1.6.9.3" class="ltx_tr">
<th id="S4.T1.6.9.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T1.6.9.3.1.1" class="ltx_text">
<span id="S4.T1.6.9.3.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.9.3.1.1.1.1" class="ltx_tr">
<span id="S4.T1.6.9.3.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Self-</span></span>
<span id="S4.T1.6.9.3.1.1.1.2" class="ltx_tr">
<span id="S4.T1.6.9.3.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Sup.</span></span>
</span></span></th>
<td id="S4.T1.6.9.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VIOLearner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<th id="S4.T1.6.9.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3.00</th>
<td id="S4.T1.6.9.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.6.9.3.4.1" class="ltx_text ltx_font_bold">1.40</span></td>
<th id="S4.T1.6.9.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3.60</th>
<td id="S4.T1.6.9.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.06</td>
<th id="S4.T1.6.9.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">2.04</th>
<td id="S4.T1.6.9.3.8" class="ltx_td ltx_align_center ltx_border_t">1.37</td>
</tr>
<tr id="S4.T1.6.10.4" class="ltx_tr">
<td id="S4.T1.6.10.4.1" class="ltx_td ltx_align_center ltx_border_r">DeepVIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<th id="S4.T1.6.10.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T1.6.10.4.2.1" class="ltx_text ltx_font_bold">2.86</span></th>
<td id="S4.T1.6.10.4.3" class="ltx_td ltx_align_center ltx_border_r">2.32</td>
<th id="S4.T1.6.10.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T1.6.10.4.4.1" class="ltx_text ltx_font_bold">2.71</span></th>
<td id="S4.T1.6.10.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.6.10.4.5.1" class="ltx_text ltx_font_bold">1.66</span></td>
<th id="S4.T1.6.10.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="background-color:#BFFFBF;"><span id="S4.T1.6.10.4.6.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">0.85</span></th>
<td id="S4.T1.6.10.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.6.10.4.7.1" class="ltx_text ltx_font_bold">1.03</span></td>
</tr>
<tr id="S4.T1.6.11.5" class="ltx_tr">
<th id="S4.T1.6.11.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="6"><span id="S4.T1.6.11.5.1.1" class="ltx_text">Sup.</span></th>
<td id="S4.T1.6.11.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ATVIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<th id="S4.T1.6.11.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">4.93</th>
<td id="S4.T1.6.11.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.4</td>
<th id="S4.T1.6.11.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3.78</th>
<td id="S4.T1.6.11.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.59</td>
<th id="S4.T1.6.11.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">5.71</th>
<td id="S4.T1.6.11.5.8" class="ltx_td ltx_align_center ltx_border_t">2.96</td>
</tr>
<tr id="S4.T1.6.12.6" class="ltx_tr">
<td id="S4.T1.6.12.6.1" class="ltx_td ltx_align_center ltx_border_r">Soft Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<th id="S4.T1.6.12.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">4.44</th>
<td id="S4.T1.6.12.6.3" class="ltx_td ltx_align_center ltx_border_r">1.69</td>
<th id="S4.T1.6.12.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">2.95</th>
<td id="S4.T1.6.12.6.5" class="ltx_td ltx_align_center ltx_border_r">1.32</td>
<th id="S4.T1.6.12.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_row">3.41</th>
<td id="S4.T1.6.12.6.7" class="ltx_td ltx_align_center">1.41</td>
</tr>
<tr id="S4.T1.6.13.7" class="ltx_tr">
<td id="S4.T1.6.13.7.1" class="ltx_td ltx_align_center ltx_border_r">Hard Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<th id="S4.T1.6.13.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">4.11</th>
<td id="S4.T1.6.13.7.3" class="ltx_td ltx_align_center ltx_border_r">1.49</td>
<th id="S4.T1.6.13.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">3.44</th>
<td id="S4.T1.6.13.7.5" class="ltx_td ltx_align_center ltx_border_r">1.86</td>
<th id="S4.T1.6.13.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T1.6.13.7.6.1" class="ltx_text ltx_font_bold">1.51</span></th>
<td id="S4.T1.6.13.7.7" class="ltx_td ltx_align_center">0.91</td>
</tr>
<tr id="S4.T1.6.14.8" class="ltx_tr">
<td id="S4.T1.6.14.8.1" class="ltx_td ltx_align_center ltx_border_r">Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<th id="S4.T1.6.14.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2.01</th>
<td id="S4.T1.6.14.8.3" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<th id="S4.T1.6.14.8.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">1.79</th>
<td id="S4.T1.6.14.8.5" class="ltx_td ltx_align_center ltx_border_r">0.76</td>
<th id="S4.T1.6.14.8.6" class="ltx_td ltx_align_center ltx_th ltx_th_row">3.41</th>
<td id="S4.T1.6.14.8.7" class="ltx_td ltx_align_center">1.08</td>
</tr>
<tr id="S4.T1.6.15.9" class="ltx_tr">
<td id="S4.T1.6.15.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(Ours) Baseline</td>
<th id="S4.T1.6.15.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="background-color:#BFFFBF;"><span id="S4.T1.6.15.9.2.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">1.93</span></th>
<td id="S4.T1.6.15.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.68</td>
<th id="S4.T1.6.15.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="background-color:#BFFFBF;"><span id="S4.T1.6.15.9.4.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">1.55</span></th>
<td id="S4.T1.6.15.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.91</td>
<th id="S4.T1.6.15.9.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">2.57</th>
<td id="S4.T1.6.15.9.7" class="ltx_td ltx_align_center ltx_border_t">0.54</td>
</tr>
<tr id="S4.T1.6.16.10" class="ltx_tr">
<td id="S4.T1.6.16.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(Ours) w. RPMG</td>
<th id="S4.T1.6.16.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">2.02</th>
<td id="S4.T1.6.16.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T1.6.16.10.3.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">0.53</span></td>
<th id="S4.T1.6.16.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">1.75</th>
<td id="S4.T1.6.16.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T1.6.16.10.5.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">0.47</span></td>
<th id="S4.T1.6.16.10.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">2.11</th>
<td id="S4.T1.6.16.10.7" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#BFFFBF;"><span id="S4.T1.6.16.10.7.1" class="ltx_text ltx_font_bold" style="background-color:#BFFFBF;">0.39</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Main Results â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares our method against geometry-based, self-supervised, and supervised VIO approaches. Self-supervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> are trained on KITTI sequences <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="00" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">00</annotation></semantics></math> to <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="08" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">08</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">08</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">08</annotation></semantics></math>. We include supervised methods that utilize the same training and testing splits as our approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Monocular VIO with geometry-based methods requires excitation of all axes in initialization to correctly determine IMU biases and scale. Cars in the KITTI dataset mostly move forward and rotate in the yaw axis. This type of motion makes it hard to evaluate their performance fairly for VIO methods that require IMU initialization. ORB-SLAM3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> does not initialize in monocular inertial mode, and VINS-Mono <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> produces high errors even if it can initialize.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.5" class="ltx_p">As seen in Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Main Results â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, VIFT obtains state-of-the-art performance compared to learning-based methods. VIFT, without other additional modules, provides the lowest translation errors in Sequences <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="05" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">05</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">05</annotation></semantics></math> and <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="07" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">07</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="integer" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">07</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">07</annotation></semantics></math> and the lowest rotation errors in Sequences <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="05" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">05</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn type="integer" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">05</annotation></semantics></math> and <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mn id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><cn type="integer" id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">10</annotation></semantics></math> while obtaining comparable performances in other metrics. Moreover, with RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, VIFT decreases the rotation error by <math id="S4.SS2.p3.5.m5.1" class="ltx_Math" alttext="\approx 63.8\%" display="inline"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml"></mi><mo id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml"><mn id="S4.SS2.p3.5.m5.1.1.3.2" xref="S4.SS2.p3.5.m5.1.1.3.2.cmml">63.8</mn><mo id="S4.SS2.p3.5.m5.1.1.3.1" xref="S4.SS2.p3.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><approx id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"></approx><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">absent</csymbol><apply id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3"><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.3.1.cmml" xref="S4.SS2.p3.5.m5.1.1.3.1">percent</csymbol><cn type="float" id="S4.SS2.p3.5.m5.1.1.3.2.cmml" xref="S4.SS2.p3.5.m5.1.1.3.2">63.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">\approx 63.8\%</annotation></semantics></math> in test Sequence 10 compared to Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Our experiments show transformer-based fusion and pose estimation surpass the performance of methods that use the same visual and inertial features with the RNN-based networks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we look at the effect of different modules to understand the performance of VIFT. We show the KITTI evaluation metric results in Table <a href="#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and plot the estimated trajectories against ground truth trajectories in Figure <a href="#S4.F3" title="Figure 3 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We mark trajectories every 5 seconds to obtain intuition about the vehicleâ€™s speed along the trajectory and to make it easy to distinguish results. We emphasize that the camera and IMU provide 10 FPS and 100 Hz measurements, respectively, which are much more frequent than marked locations. We show the estimated trajectory in test sequences from above in the top row and vertical trajectory versus the bottom row. All trajectories start from the origin, and relative pose estimates from VIFT are applied sequentially to obtain absolute pose estimates for each time index.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.10.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.11.2" class="ltx_text" style="font-size:90%;">Ablation study. Modified parts from VIFT model are shown with <span id="S4.T2.11.2.1" class="ltx_text" style="background-color:#BFFFBF;">green</span> background. Best results are shown <span id="S4.T2.11.2.2" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.6.7.1" class="ltx_tr">
<th id="S4.T2.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.6.7.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T2.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.6.7.1.2.1" class="ltx_text" style="font-size:90%;">Sequence</span></th>
<th id="S4.T2.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.6.7.1.3.1" class="ltx_text" style="font-size:90%;">Criterion</span></th>
<th id="S4.T2.6.7.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.6.7.1.4.1" class="ltx_text" style="font-size:90%;">Data</span></th>
<th id="S4.T2.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.6.7.1.5.1" class="ltx_text" style="font-size:90%;">RPMG</span></th>
<th id="S4.T2.6.7.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S4.T2.6.7.1.6.1" class="ltx_text" style="font-size:90%;">Seq. 05</span></th>
<th id="S4.T2.6.7.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S4.T2.6.7.1.7.1" class="ltx_text" style="font-size:90%;">Seq. 07</span></th>
<th id="S4.T2.6.7.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S4.T2.6.7.1.8.1" class="ltx_text" style="font-size:90%;">Seq. 10</span></th>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<th id="S4.T2.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.6.6.7.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="S4.T2.6.6.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.6.6.8.1" class="ltx_text" style="font-size:90%;">Length</span></th>
<th id="S4.T2.6.6.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.6.6.9.1" class="ltx_text" style="font-size:90%;">Balancing</span></th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S4.T2.1.1.1.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1b"><msub id="S4.T2.1.1.1.m1.1.1"><mi mathsize="50%" id="S4.T2.1.1.1.m1.1.1.2">t</mi><mrow id="S4.T2.1.1.1.m1.1.1.3"><mi mathsize="50%" id="S4.T2.1.1.1.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.1.1.1.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.1.1.1.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.1.1.1.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T2.1.1.1.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T2.1.1.1.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T2.1.1.1.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><math id="S4.T2.2.2.2.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.m1.1b"><msub id="S4.T2.2.2.2.m1.1.1"><mi mathsize="50%" id="S4.T2.2.2.2.m1.1.1.2">r</mi><mrow id="S4.T2.2.2.2.m1.1.1.3"><mi mathsize="50%" id="S4.T2.2.2.2.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.2.2.2.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.2.2.2.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.2.2.2.m1.1.2"><msup id="S4.T2.2.2.2.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T2.2.2.2.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T2.2.2.2.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T2.2.2.2.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S4.T2.3.3.3.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.m1.1b"><msub id="S4.T2.3.3.3.m1.1.1"><mi mathsize="50%" id="S4.T2.3.3.3.m1.1.1.2">t</mi><mrow id="S4.T2.3.3.3.m1.1.1.3"><mi mathsize="50%" id="S4.T2.3.3.3.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.3.3.3.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.3.3.3.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.3.3.3.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T2.3.3.3.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T2.3.3.3.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T2.3.3.3.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><math id="S4.T2.4.4.4.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mrow id="S4.T2.4.4.4.m1.1b"><msub id="S4.T2.4.4.4.m1.1.1"><mi mathsize="50%" id="S4.T2.4.4.4.m1.1.1.2">r</mi><mrow id="S4.T2.4.4.4.m1.1.1.3"><mi mathsize="50%" id="S4.T2.4.4.4.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.4.4.4.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.4.4.4.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.4.4.4.m1.1.2"><msup id="S4.T2.4.4.4.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T2.4.4.4.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T2.4.4.4.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T2.4.4.4.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></th>
<th id="S4.T2.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S4.T2.5.5.5.m1.1" class="ltx_math_unparsed" alttext="t_{rel}(\%)" display="inline"><semantics id="S4.T2.5.5.5.m1.1a"><mrow id="S4.T2.5.5.5.m1.1b"><msub id="S4.T2.5.5.5.m1.1.1"><mi mathsize="50%" id="S4.T2.5.5.5.m1.1.1.2">t</mi><mrow id="S4.T2.5.5.5.m1.1.1.3"><mi mathsize="50%" id="S4.T2.5.5.5.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.5.5.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.5.5.5.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.5.5.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.5.5.5.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.5.5.5.m1.1.2"><mo maxsize="50%" minsize="50%" id="S4.T2.5.5.5.m1.1.2.1">(</mo><mo mathsize="50%" id="S4.T2.5.5.5.m1.1.2.2">%</mo><mo maxsize="50%" minsize="50%" id="S4.T2.5.5.5.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">t_{rel}(\%)</annotation></semantics></math></th>
<th id="S4.T2.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S4.T2.6.6.6.m1.1" class="ltx_math_unparsed" alttext="r_{rel}(^{\circ})" display="inline"><semantics id="S4.T2.6.6.6.m1.1a"><mrow id="S4.T2.6.6.6.m1.1b"><msub id="S4.T2.6.6.6.m1.1.1"><mi mathsize="50%" id="S4.T2.6.6.6.m1.1.1.2">r</mi><mrow id="S4.T2.6.6.6.m1.1.1.3"><mi mathsize="50%" id="S4.T2.6.6.6.m1.1.1.3.2">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.6.6.6.m1.1.1.3.1">â€‹</mo><mi mathsize="50%" id="S4.T2.6.6.6.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.6.6.6.m1.1.1.3.1a">â€‹</mo><mi mathsize="50%" id="S4.T2.6.6.6.m1.1.1.3.4">l</mi></mrow></msub><mrow id="S4.T2.6.6.6.m1.1.2"><msup id="S4.T2.6.6.6.m1.1.2.1"><mo maxsize="50%" minsize="50%" id="S4.T2.6.6.6.m1.1.2.1.2">(</mo><mo mathsize="50%" id="S4.T2.6.6.6.m1.1.2.1.3">âˆ˜</mo></msup><mo maxsize="50%" minsize="50%" id="S4.T2.6.6.6.m1.1.2.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">r_{rel}(^{\circ})</annotation></semantics></math></th>
</tr>
<tr id="S4.T2.6.8.2" class="ltx_tr">
<th id="S4.T2.6.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="background-color:#BFFFBF;"><span id="S4.T2.6.8.2.1.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">MLP</span></th>
<th id="S4.T2.6.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="background-color:#BFFFBF;"><span id="S4.T2.6.8.2.2.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">2</span></th>
<th id="S4.T2.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T2.6.8.2.3.1" class="ltx_text" style="font-size:90%;">L1</span></th>
<th id="S4.T2.6.8.2.4" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.6.8.2.5" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.6.8.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.6.8.2.6.1" class="ltx_text" style="font-size:90%;">2.03</span></th>
<th id="S4.T2.6.8.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T2.6.8.2.7.1" class="ltx_text" style="font-size:90%;">0.67</span></th>
<th id="S4.T2.6.8.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.6.8.2.8.1" class="ltx_text" style="font-size:90%;">3.04</span></th>
<th id="S4.T2.6.8.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T2.6.8.2.9.1" class="ltx_text" style="font-size:90%;">1.19</span></th>
<th id="S4.T2.6.8.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.6.8.2.10.1" class="ltx_text" style="font-size:90%;">3.60</span></th>
<th id="S4.T2.6.8.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.6.8.2.11.1" class="ltx_text" style="font-size:90%;">1.14</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.6.9.1" class="ltx_tr">
<td id="S4.T2.6.9.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T2.6.9.1.1.1" class="ltx_text" style="font-size:90%;">Ours</span></td>
<td id="S4.T2.6.9.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.9.1.2.1" class="ltx_text" style="font-size:90%;">11</span></td>
<td id="S4.T2.6.9.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;"><span id="S4.T2.6.9.1.3.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">L2</span></td>
<td id="S4.T2.6.9.1.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.9.1.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.9.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.1.6.1" class="ltx_text" style="font-size:90%;">4.35</span></td>
<td id="S4.T2.6.9.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.9.1.7.1" class="ltx_text" style="font-size:90%;">1.91</span></td>
<td id="S4.T2.6.9.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.1.8.1" class="ltx_text" style="font-size:90%;">2.97</span></td>
<td id="S4.T2.6.9.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.9.1.9.1" class="ltx_text" style="font-size:90%;">2.23</span></td>
<td id="S4.T2.6.9.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.1.10.1" class="ltx_text" style="font-size:90%;">3.66</span></td>
<td id="S4.T2.6.9.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.1.11.1" class="ltx_text" style="font-size:90%;">1.92</span></td>
</tr>
<tr id="S4.T2.6.10.2" class="ltx_tr">
<td id="S4.T2.6.10.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.10.2.1.1" class="ltx_text" style="font-size:90%;">11</span></td>
<td id="S4.T2.6.10.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.10.2.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S4.T2.6.10.2.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.6.10.2.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.6.10.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.2.5.1" class="ltx_text" style="font-size:90%;">1.93</span></td>
<td id="S4.T2.6.10.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.10.2.6.1" class="ltx_text" style="font-size:90%;">0.68</span></td>
<td id="S4.T2.6.10.2.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.2.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.55</span></td>
<td id="S4.T2.6.10.2.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.10.2.8.1" class="ltx_text" style="font-size:90%;">0.91</span></td>
<td id="S4.T2.6.10.2.9" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.2.9.1" class="ltx_text" style="font-size:90%;">2.57</span></td>
<td id="S4.T2.6.10.2.10" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.2.10.1" class="ltx_text" style="font-size:90%;">0.54</span></td>
</tr>
<tr id="S4.T2.6.11.3" class="ltx_tr">
<td id="S4.T2.6.11.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.11.3.1.1" class="ltx_text" style="font-size:90%;">11</span></td>
<td id="S4.T2.6.11.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.11.3.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S4.T2.6.11.3.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T2.6.11.3.3.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td id="S4.T2.6.11.3.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.6.11.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.11.3.5.1" class="ltx_text" style="font-size:90%;">2.37</span></td>
<td id="S4.T2.6.11.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.11.3.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.52</span></td>
<td id="S4.T2.6.11.3.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.11.3.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.55</span></td>
<td id="S4.T2.6.11.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.11.3.8.1" class="ltx_text" style="font-size:90%;">0.85</span></td>
<td id="S4.T2.6.11.3.9" class="ltx_td ltx_align_center"><span id="S4.T2.6.11.3.9.1" class="ltx_text" style="font-size:90%;">2.32</span></td>
<td id="S4.T2.6.11.3.10" class="ltx_td ltx_align_center"><span id="S4.T2.6.11.3.10.1" class="ltx_text" style="font-size:90%;">0.75</span></td>
</tr>
<tr id="S4.T2.6.12.4" class="ltx_tr">
<td id="S4.T2.6.12.4.1" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T2.6.12.4.1.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">65</span></td>
<td id="S4.T2.6.12.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.12.4.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S4.T2.6.12.4.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T2.6.12.4.3.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td id="S4.T2.6.12.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.12.4.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T2.6.12.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.4.5.1" class="ltx_text" style="font-size:90%;">2.37</span></td>
<td id="S4.T2.6.12.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.12.4.6.1" class="ltx_text" style="font-size:90%;">0.64</span></td>
<td id="S4.T2.6.12.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.4.7.1" class="ltx_text" style="font-size:90%;">1.98</span></td>
<td id="S4.T2.6.12.4.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.12.4.8.1" class="ltx_text" style="font-size:90%;">0.58</span></td>
<td id="S4.T2.6.12.4.9" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.4.9.1" class="ltx_text" style="font-size:90%;">2.97</span></td>
<td id="S4.T2.6.12.4.10" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.4.10.1" class="ltx_text" style="font-size:90%;">0.69</span></td>
</tr>
<tr id="S4.T2.6.13.5" class="ltx_tr">
<td id="S4.T2.6.13.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.13.5.1.1" class="ltx_text" style="font-size:90%;">11</span></td>
<td id="S4.T2.6.13.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.13.5.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S4.T2.6.13.5.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;"><span id="S4.T2.6.13.5.3.1" class="ltx_text" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td id="S4.T2.6.13.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.13.5.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T2.6.13.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.13.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.90</span></td>
<td id="S4.T2.6.13.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.13.5.6.1" class="ltx_text" style="font-size:90%;">0.53</span></td>
<td id="S4.T2.6.13.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.13.5.7.1" class="ltx_text" style="font-size:90%;">1.79</span></td>
<td id="S4.T2.6.13.5.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.13.5.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.45</span></td>
<td id="S4.T2.6.13.5.9" class="ltx_td ltx_align_center"><span id="S4.T2.6.13.5.9.1" class="ltx_text" style="font-size:90%;">2.40</span></td>
<td id="S4.T2.6.13.5.10" class="ltx_td ltx_align_center"><span id="S4.T2.6.13.5.10.1" class="ltx_text" style="font-size:90%;">0.60</span></td>
</tr>
<tr id="S4.T2.6.14.6" class="ltx_tr">
<td id="S4.T2.6.14.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.6.14.6.1.1" class="ltx_text" style="font-size:90%;">11</span></td>
<td id="S4.T2.6.14.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.6.14.6.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S4.T2.6.14.6.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S4.T2.6.14.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.6.14.6.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T2.6.14.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.14.6.5.1" class="ltx_text" style="font-size:90%;">2.02</span></td>
<td id="S4.T2.6.14.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.6.14.6.6.1" class="ltx_text" style="font-size:90%;">0.53</span></td>
<td id="S4.T2.6.14.6.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.14.6.7.1" class="ltx_text" style="font-size:90%;">1.75</span></td>
<td id="S4.T2.6.14.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.6.14.6.8.1" class="ltx_text" style="font-size:90%;">0.47</span></td>
<td id="S4.T2.6.14.6.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.14.6.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.11</span></td>
<td id="S4.T2.6.14.6.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.14.6.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.39</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.08769/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="156" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2409.08769/assets/x4.png" id="S4.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="158" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Proposed transformer based fusion and pose estimation module in VIFT evaluated under different training settings. We mark trajectories every 5 seconds for intuition about the vehicleâ€™s speed along the trajectory and easy distinction of results. We emphasize that the camera and IMU provide 10 FPS and 100 Hz measurements, respectively, which are much more frequent than marked locations. We show the estimated trajectory in test sequences from above in the top row and vertical trajectory versus the bottom row. All trajectories start from the origin, and relative pose estimates from VIFT are applied sequentially to obtain absolute pose estimates for each time index.</span></figcaption>
</figure>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Model Type</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">We first look at the performance of 4-layer MLP trained on latent visual inertial feature vectors. From results in rows 1 and 2 of Table <a href="#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe that the odometry performance is reasonably good even with a small MLP network. This performance supports the primary motivation of our architecture. The vectors in latent space already contain good properties for pose estimation. We use transformer-based fusion to correct these latent vectors with the transformer, based on past measurements, and a 2-layer MLP is used at the end of the transformer. VIFT utilizes history to improve pose estimation with transformer-based architecture.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Norm Type in Training Criterion</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.6" class="ltx_p">We found that using the L1 loss function resulted in better performance within the same training steps. As the errors decrease after the initial epochs, the gradients in L2 loss become smaller, leading to slower convergence and requiring more training iterations, according to our observations. Consequently, we observed that training with L2 loss was slower overall. We also tuned each scenarioâ€™s <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mi id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><ci id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">\alpha</annotation></semantics></math> parameter in the Equation <a href="#S3.E4" title="Equation 4 â€£ 3.3 Deep Rotation Regression â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Following previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we used <math id="S4.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha=100" display="inline"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><mrow id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.2.m2.1.1.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><apply id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1"><eq id="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">\alpha=100</annotation></semantics></math> with the L2 criterion. For L1 loss, we found that <math id="S4.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\alpha=10" display="inline"><semantics id="S4.SS3.SSS2.p1.3.m3.1a"><mrow id="S4.SS3.SSS2.p1.3.m3.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p1.3.m3.1.1.2" xref="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.3.m3.1.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.3.m3.1.1.3" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.3.m3.1b"><apply id="S4.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1"><eq id="S4.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.3.m3.1c">\alpha=10</annotation></semantics></math> worked better for Euler angles, while <math id="S4.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\alpha=40" display="inline"><semantics id="S4.SS3.SSS2.p1.4.m4.1a"><mrow id="S4.SS3.SSS2.p1.4.m4.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p1.4.m4.1.1.2" xref="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.4.m4.1.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.4.m4.1.1.3" xref="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.4.m4.1b"><apply id="S4.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1"><eq id="S4.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.4.m4.1c">\alpha=40</annotation></semantics></math> yielded better results in models incorporating the RPMG layer. We experimented with different <math id="S4.SS3.SSS2.p1.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.SSS2.p1.5.m5.1a"><mi id="S4.SS3.SSS2.p1.5.m5.1.1" xref="S4.SS3.SSS2.p1.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.5.m5.1b"><ci id="S4.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS2.p1.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.5.m5.1c">\alpha</annotation></semantics></math> values for each method and reported the results using the values that best balanced rotational and translational errors. Since the rotation loss is calculated based on the mean difference between elements of rotation matrices in RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we fine-tuned the <math id="S4.SS3.SSS2.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.SSS2.p1.6.m6.1a"><mi id="S4.SS3.SSS2.p1.6.m6.1.1" xref="S4.SS3.SSS2.p1.6.m6.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.6.m6.1b"><ci id="S4.SS3.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS3.SSS2.p1.6.m6.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.6.m6.1c">\alpha</annotation></semantics></math> parameter to identify the optimal balance.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Data Balancing</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In datasets like KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, specific rotational movements, such as sharp turns and sudden stops, are underrepresented. These motions are critical because they impact overall trajectory accuracy more, making errors in these scenarios more costly. To address this, we experimented with increasing the weights of these less frequent rotational updates during training with the histogram of rotations proposed by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, aiming to enhance the modelâ€™s performance in these critical cases.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">A comparison of rows 3,4,6, and 7 in Table <a href="#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that introducing data balancing during training does not lead to consistent improvements across the test sequences. These results indicate that data balancing might negatively affect the modelâ€™s ability. Although we obtained the best results in Sequences 05 and 07 when we applied data balancing, the improvements are inconsistent across all sequences.</p>
</div>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">Our observations suggest that while data balancing can be a helpful strategy, it must be carefully tuned, especially when combined with advanced optimization techniques like RPMG. Overemphasizing underrepresented rotations does not always yield the desired improvements and could potentially degrade performance, particularly in orientation accuracy. Therefore, a more nuanced approach may be required to balance the representation of various movements in the training data without compromising the modelâ€™s overall robustness.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>RPMG</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.2" class="ltx_p">The RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> layer has proven to be a crucial enhancement in the models we evaluated, particularly in terms of reducing orientation errors, denoted as <math id="S4.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\textit{r}_{\textit{rel}}" display="inline"><semantics id="S4.SS3.SSS4.p1.1.m1.1a"><msub id="S4.SS3.SSS4.p1.1.m1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.2" xref="S4.SS3.SSS4.p1.1.m1.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.3" xref="S4.SS3.SSS4.p1.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.1.m1.1b"><apply id="S4.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.1.m1.1c">\textit{r}_{\textit{rel}}</annotation></semantics></math>. When comparing models trained with RPMG to those without, we consistently observe significant improvements in orientation accuracy across all tested sequences. This consistent reduction in <math id="S4.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="\textit{r}_{\textit{rel}}" display="inline"><semantics id="S4.SS3.SSS4.p1.2.m2.1a"><msub id="S4.SS3.SSS4.p1.2.m2.1.1" xref="S4.SS3.SSS4.p1.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.2" xref="S4.SS3.SSS4.p1.2.m2.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.3" xref="S4.SS3.SSS4.p1.2.m2.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.2.m2.1b"><apply id="S4.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p1.2.m2.1.1.2a.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS3.SSS4.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.2.m2.1c">\textit{r}_{\textit{rel}}</annotation></semantics></math> highlights the effectiveness of incorporating Riemannian optimization techniques in the training process.</p>
</div>
<div id="S4.SS3.SSS4.p2" class="ltx_para">
<p id="S4.SS3.SSS4.p2.1" class="ltx_p">RPMG is particularly advantageous in scenarios involving rotations, where traditional optimization methods might struggle due to the non-Euclidean nature of orientation spaces. By operating directly on the manifold of rotations, RPMG ensures that updates to the orientation parameters are more geometrically appropriate, leading to better convergence properties and, ultimately, more accurate predictions.</p>
</div>
<div id="S4.SS3.SSS4.p3" class="ltx_para">
<p id="S4.SS3.SSS4.p3.1" class="ltx_p">In our ablation study, including RPMG improved orientation accuracy and demonstrated robustness across different sequence lengths and data balancing strategies. For instance, in Sequence 07, the use of RPMG led to a marked decrease in <math id="S4.SS3.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\textit{r}_{\textit{rel}}" display="inline"><semantics id="S4.SS3.SSS4.p3.1.m1.1a"><msub id="S4.SS3.SSS4.p3.1.m1.1.1" xref="S4.SS3.SSS4.p3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.2" xref="S4.SS3.SSS4.p3.1.m1.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.3" xref="S4.SS3.SSS4.p3.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p3.1.m1.1b"><apply id="S4.SS3.SSS4.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p3.1.m1.1.1.2a.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p3.1.m1.1.1.3a.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS3.SSS4.p3.1.m1.1.1.3.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p3.1.m1.1c">\textit{r}_{\textit{rel}}</annotation></semantics></math>, from 0.91 to 0.47, highlighting its substantial impact. We observed similar trends across Sequence 05 and Sequence 10, where RPMG consistently yielded lower orientation errors.</p>
</div>
</section>
<section id="S4.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Sequence Length</h4>

<div id="S4.SS3.SSS5.p1" class="ltx_para">
<p id="S4.SS3.SSS5.p1.1" class="ltx_p">When we compare models in rows 5 and 6 of Table <a href="#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe a performance drop on the same model trained with increased sequence length. We tried a sequence length of 65 compared to the original sequence length of 11 in the experiment. Modeling the relationships in longer sequences is a more complex task, and larger sequence lengths could require more training data.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce deep VIO network VIFT, which performs sensor fusion and pose estimation with a causal transformer. We show that our method outperforms previous methods with our experiments on the KITTI dataset. We also improve the VIFT by including the manifold optimization technique RPMG inside our pipeline. Our studyâ€™s consistent performance gains across different configurations demonstrate that VIFT and RPMG provide a fundamental enhancement that can significantly elevate the performance of visual-inertial odometry models.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We gratefully acknowledge the computational resources provided by TÃœBÄ°TAK ULAKBÄ°M High Performance and Grid Computing Center (TRUBA). Yunus Bilge Kurt is supported by the TÃœBÄ°TAK under the 2210-National MSc/MA Scholarship Program.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Almalioglu, Y., Turan, M., Saputra, M.R.U., deÂ GusmÃ£o, P.P., Markham, A., Trigoni, N.: Selfvio: Self-supervised deep monocular visualâ€“inertial odometry and depth estimation. Neural Networks <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">150</span>, 119â€“136 (2022)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bloesch, M., Omari, S., Hutter, M., Siegwart, R.: Robust visual inertial odometry using a direct ekf-based approach. In: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). pp. 298â€“304. IEEE (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Campos, C., Elvira, R., RodrÃ­guez, J.J.G., Montiel, J.M., TardÃ³s, J.D.: Orb-slam3: An accurate open-source library for visual, visualâ€“inertial, and multimap slam. IEEE Transactions on Robotics <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">37</span>(6), 1874â€“1890 (2021)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, C., Rosa, S., Miao, Y., Lu, C.X., Wu, W., Markham, A., Trigoni, N.: Selective sensor fusion for neural visual-inertial odometry. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10542â€“10551 (2019)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, J., Yin, Y., Birdal, T., Chen, B., Guibas, L.J., Wang, H.: Projective manifold gradient layer for deep rotation regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6646â€“6655 (2022)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Clark, R., Wang, S., Wen, H., Markham, A., Trigoni, N.: Vinet: Visual-inertial odometry as a sequence-to-sequence learning problem. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol.Â 31 (2017)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv <span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">abs/2010.11929</span> (2020), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:225039882" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:225039882</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., Van DerÂ Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical flow with convolutional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2758â€“2766 (2015)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Forster, C., Carlone, L., Dellaert, F., Scaramuzza, D.: On-manifold preintegration for real-time visualâ€“inertial odometry. IEEE Transactions on Robotics <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">33</span>(1), 1â€“21 (2016)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2012)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
van Goor, P., Mahony, R.: Eqvio: An equivariant filter for visual-inertial odometry. IEEE Transactions on Robotics (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Han, L., Lin, Y., Du, G., Lian, S.: Deepvio: Self-supervised deep learning of monocular visual inertial odometry using 3d geometric constraints. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 6906â€“6913. IEEE (2019)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Leutenegger, S., Lynen, S., Bosse, M., Siegwart, R., Furgale, P.: Keyframe-based visualâ€“inertial odometry using nonlinear optimization. The International Journal of Robotics Research <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">34</span>(3), 314â€“334 (2015)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Liu, L., Li, G., Li, T.H.: Atvio: attention guided visual-inertial odometry. In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 4125â€“4129. IEEE (2021)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017), <a target="_blank" href="https://openreview.net/forum?id=Skq89Scxx" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Skq89Scxx</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net (2019), <a target="_blank" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Bkg6RiCqY7</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mourikis, A.I., Roumeliotis, S.I.: A multi-state constraint kalman filter for vision-aided inertial navigation. In: Proceedings 2007 IEEE international conference on robotics and automation. pp. 3565â€“3572. IEEE (2007)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qin, T., Li, P., Shen, S.: Vins-mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">34</span>(4), 1004â€“1020 (2018)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shamwell, E.J., Leung, S., Nothwang, W.D.: Vision-aided absolute trajectory estimation using an unsupervised deep network with online error correction. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 2524â€“2531. IEEE (2018)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Shamwell, E.J., Lindgren, K., Leung, S., Nothwang, W.D.: Unsupervised deep visual-inertial odometry with online error correction for rgb-d imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">42</span>(10), 2478â€“2493 (2019)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
von Stumberg, L., Cremers, D.: DM-VIO: Delayed marginalization visual-inertial odometry. IEEE Robotics and Automation Letters (RA-L) &amp; International Conference on Robotics and Automation (ICRA) <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">7</span>(2), 1408â€“1415 (2022). https://doi.org/10.1109/LRA.2021.3140129

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Tu, Z., Chen, C., Pan, X., Liu, R., Cui, J., Mao, J.: Ema-vio: Deep visualâ€“inertial odometry with external memory attention. IEEE Sensors Journal <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">22</span>(21), 20877â€“20885 (2022)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol.Â 30. Curran Associates, Inc. (2017), <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Wagstaff, B., Wise, E., Kelly, J.: A self-supervised, differentiable kalman filter for uncertainty-aware visual-inertial odometry. In: 2022 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM). pp. 1388â€“1395. IEEE (2022)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Wang, Z., Zhu, Y., Lu, K., Freer, D., Wu, H., Chen, H.: Attention guided unsupervised learning of monocular visual-inertial odometry. In: 2022 IEEE Intelligent Vehicles Symposium (IV). pp. 651â€“657. IEEE (2022)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Wei, P., Hua, G., Huang, W., Meng, F., Liu, H.: Unsupervised monocular visual-inertial odometry network. In: Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. pp. 2347â€“2354 (2021)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Yang, M., Chen, Y., Kim, H.S.: Efficient deep visual and inertial odometry with adaptive visual modality selection. In: Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXXVIII. pp. 233â€“250. Springer (2022)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yang, Y., Zha, K., Chen, Y., Wang, H., Katabi, D.: Delving into deep imbalanced regression. In: International conference on machine learning. pp. 11842â€“11851. PMLR (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation representations in neural networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5745â€“5753 (2019)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.08768" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.08769" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.08769">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.08769" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.08770" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:55:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
