<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.01643] Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning</title><meta property="og:description" content="Privacy is important considering the financial Domain as such data is highly confidential and sensitive. Natural Language Processing (NLP) techniques can be applied for text classification and entity detection purposes…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.01643">

<!--Generated on Mon Feb 26 19:40:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Privacy enabled Financial Text Classification using
<br class="ltx_break">Differential Privacy and Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Priyam Basu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiasa Singha Roy 
<br class="ltx_break">Manipal Institute of Technology 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{priyam.basu1, tiasa.singharoy}@learner.manipal.edu</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\AND</span>Rakshit Naidu 
<br class="ltx_break">Carnegie Mellon University 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">rnemakal@andrew.cmu.edu</span> 
<br class="ltx_break">&amp;Zumrut Muftuoglu 
<br class="ltx_break">Yildiz Technical University
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">zumrutmuftuoglu@gmail.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Privacy is important considering the financial Domain as such data is highly confidential and sensitive. Natural Language Processing (NLP) techniques can be applied for text classification and entity detection purposes in financial domains such as customer feedback sentiment analysis, invoice entity detection, categorisation of financial documents by type etc. Due to the sensitive nature of such data, privacy measures need to be taken for handling and training large models with such data. In this work, we propose a contextualized transformer (BERT and RoBERTa) based text classification model integrated with privacy features such as Differential Privacy (DP) and Federated Learning (FL). We present how to privately train NLP models and desirable privacy-utility tradeoffs and evaluate them on the Financial Phrase Bank dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Divulging personally identifiable information during a business transaction has become a commonplace occurrence for most individuals. This activity can span from sharing of bank account numbers, loan account numbers, and credit/debit card numbers, to providing non-financial personally identifiable information such as name, social security number, driver’s license number, address, and e-mail address. Maintaining the privacy of confidential customer information has become essential for any firm which collects or stores personally identifiable data. The financial services industry operates and deals with a significant amount of confidential client and customer data for daily business transactions. Though many organizations are taking strides to improve their privacy practices, and consumers are
becoming more privacy-aware, it remains a tremendous burden for users to manage their privacy <cite class="ltx_cite ltx_citemacro_cite">Anton et al. (<a href="#bib.bib3" title="" class="ltx_ref">2004</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">NLP has major applications in the finance industry for many tasks such as detection of entities for gross tax calculation from invoice and payroll data, categorising different kinds of financial documents based on type, grouping of financial documents based on semantic similarity, sentiment analysis of financial text <cite class="ltx_cite ltx_citemacro_cite">Vicari and Gaspari (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>, conversational bots for banking systems, investment recommendation engines etc.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Text Classification can be extended to many NLP applications including sentiment analysis, question answering, and topic labeling . For example, financial or government institutions that wish to train a chatbot for their clients cannot be allowed to upload all text data from the client-side to their central server due to strict privacy protection statements <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. At this point, applying the federated learning paradigm presents an approach to solve the dilemma due to its advances in privacy preservation and collaborative training where the central server can train a powerful model with different local labeled data at client devices without uploading the raw data considering increasing privacy concerns in public.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The goal of this paper is to propose a privacy enabled text classification system, combining state-of-the-art transformers (BERT and RoBERTa) with differential privacy, on both centralized and FL based setups, exploring different privacy budgets to investigate the privacy-utility trade-off and see how they perform when trying to classify financial document-based text sequences. For the federated setups, we try to explore both IID (Independent and Identically Distributed) and non-IID distributions of data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.01643/assets/pipeline2.jpeg" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pipeline</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Deep learning techniques have often been used to learn text representations via neural models by language application. The input text can give us individual demographic information about the author. Sentiment analysis can be used for the classification or categorization of financial documents. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib32" title="" class="ltx_ref">Xing et al.</a></cite> investigate the error patterns of some widely acknowledged sentiment analysis methods in the finance domain. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib26" title="" class="ltx_ref">Mishev et al.</a></cite> perform more than one hundred experiments using publicly available datasets, labeled by financial experts. In their work, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib19" title="" class="ltx_ref">Liu et al.</a></cite> propose a domain-specific language model pre-trained on large-scale financial corpora and evaluate it on the Financial Phrase Bank dataset. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib4" title="" class="ltx_ref">Araci</a></cite> presents a BERT-based model which is pre-trained on a large amount of finance-based data in his study.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Studies have been conducted on training differentially private deep models with the formal differential privacy approach in the literature <cite class="ltx_cite ltx_citemacro_cite">Abadi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>); McMahan et al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>); Yu et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib12" title="" class="ltx_ref">Fernandes et al.</a></cite> discuss the security through differential privacy in textual data. <cite class="ltx_cite ltx_citemacro_cite">Panchal (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> in his work portrays the use of DP in the generation of contextually similar messages for Honey encryption which encrypts messages using low min-entropy keys such as passwords. Federated learning is another privacy-enhancing approach <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>); Yang et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Kairouz et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>); Jana and Biemann (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>); Priyanshu and Naidu (<a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>, which relies on distributed training of models on devices and sharing of model gradients.
 <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib17" title="" class="ltx_ref">Liu et al.</a></cite> show how FL can be used for decentralized training of heavy pre-trained NLP models. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib5" title="" class="ltx_ref">Basu et al.</a></cite> in their work have shown a detailed benchmark comparison of multiple BERT based models with DP and FL for depression detection. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib14" title="" class="ltx_ref">Jana and Biemann</a></cite> in their work show a differentially private sequence tagging system in a federated learning setup.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The key arguments for the low utilization of statistical techniques in financial sentiment analysis have been the difficulty of implementation for practical applications and the lack of high- quality training data for building such models. Especially in the case of finance and economic texts, annotated collections are a scarce resource and many are reserved for proprietary use only. For this reason, we use the Financial Phrase Bank dataset <cite class="ltx_cite ltx_citemacro_cite">Malo et al. (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> which was also used for benchmarking the pre-trained FinBERT model for sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">Araci (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. The dataset includes approximately 5000 phrases/sentences from financial news texts and company press releases. The objective of the phrase level annotation task is to classify each example sentence into a positive, negative or neutral category by considering only the information explicitly available in the given sentence. Since the study is focused only on financial and economic domains, the annotators were asked to consider the sentences from the viewpoint of an investor only; i.e. whether the news may have a positive, negative or neutral influence on the stock price. As a result, sentences that have a sentiment that is not relevant from an economic or financial perspective are considered neutral.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Given a large number of overlapping annotations (5 to 8 annotations per sentence), there are several ways to define a majority vote-based gold standard. To provide an objective comparison, the authors have formed 4 alternative reference datasets based on the strength of majority agreement. For the purpose of this task, we use those sentences with 75% or more agreement. The final dataset has 3453 sentences in total out of which 60% belong to the neutral class, 28% belong to the positive class and 12% belong to the positive class.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Preliminaries</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Today, the text is the most widely used communication instrument.For years, researchers are studies focusing on implementing different approaches that make possible machines to imitate human reading <cite class="ltx_cite ltx_citemacro_cite">Ly et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. Natural Language Processing(NLP) lays a bridge between computers and natural languages by helping machines to analyze human language <cite class="ltx_cite ltx_citemacro_cite">Manning and Schütze (<a href="#bib.bib22" title="" class="ltx_ref">1999</a>)</cite>
.<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib8" title="" class="ltx_ref">Devlin et al.</a></cite> developed a model which is based on bidirectional encoder representation <cite class="ltx_cite ltx_citemacro_cite">Alyafeai et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. RoBERTa is a modified form of BERT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>BERT</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Transformer-based models have been used since they use a self-attention mechanism and process the entire input data at once instead of as a sequence to capture long-term dependencies for obtaining contextual meaning. Bidirectional Encoder Representations from Transformers (BERT)<cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> tokenizes words into sub-words (using WordPiece) which are then given as input to the model. It also uses positional embeddings to replace recurrence.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>RoBERTa</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Robustly Optimized BERT-Pretraining Approach (RoBERTa) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> is a state-of-the-art transformer model which improves BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> that uses a multi-headed attention mechanism which enables it to capture long term dependencies. It essentially fine-tunes the original BERT model along with data manipulation and uses Byte-Pair Encoding for utilizing the character and word level representations and removed Next Sentence Prediction (NSP) to match or even slightly improve downstream task performance.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Differential Privacy</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Differential Privacy (DP) is a privacy standard which allows data use in any analysis by presenting mathematical guarantee  <cite class="ltx_cite ltx_citemacro_cite">Dwork and Roth (<a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite>. It provides strong confidentiality in statistical databases and machine learning approaches through mathematical definition. This definition is an acceptable measure of privacy concern <cite class="ltx_cite ltx_citemacro_cite">Dwork (<a href="#bib.bib9" title="" class="ltx_ref">2008</a>)</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.8" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Definition 1.1 : <math id="S4.SS3.p2.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS3.p2.1.1.m1.1a"><mi id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><ci id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.1c">M</annotation></semantics></math></span><span id="S4.SS3.p2.8.8" class="ltx_text ltx_font_italic"> and <math id="S4.SS3.p2.2.2.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS3.p2.2.2.m1.1a"><mi id="S4.SS3.p2.2.2.m1.1.1" xref="S4.SS3.p2.2.2.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.2.m1.1b"><ci id="S4.SS3.p2.2.2.m1.1.1.cmml" xref="S4.SS3.p2.2.2.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.2.m1.1c">E</annotation></semantics></math> denote a random mechanism and each event (output) respectively. <math id="S4.SS3.p2.3.3.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS3.p2.3.3.m2.1a"><mi id="S4.SS3.p2.3.3.m2.1.1" xref="S4.SS3.p2.3.3.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.3.m2.1b"><ci id="S4.SS3.p2.3.3.m2.1.1.cmml" xref="S4.SS3.p2.3.3.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.3.m2.1c">D</annotation></semantics></math> and <math id="S4.SS3.p2.4.4.m3.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS3.p2.4.4.m3.1a"><msup id="S4.SS3.p2.4.4.m3.1.1" xref="S4.SS3.p2.4.4.m3.1.1.cmml"><mi id="S4.SS3.p2.4.4.m3.1.1.2" xref="S4.SS3.p2.4.4.m3.1.1.2.cmml">D</mi><mo id="S4.SS3.p2.4.4.m3.1.1.3" xref="S4.SS3.p2.4.4.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.4.m3.1b"><apply id="S4.SS3.p2.4.4.m3.1.1.cmml" xref="S4.SS3.p2.4.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.4.m3.1.1.1.cmml" xref="S4.SS3.p2.4.4.m3.1.1">superscript</csymbol><ci id="S4.SS3.p2.4.4.m3.1.1.2.cmml" xref="S4.SS3.p2.4.4.m3.1.1.2">𝐷</ci><ci id="S4.SS3.p2.4.4.m3.1.1.3.cmml" xref="S4.SS3.p2.4.4.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.4.m3.1c">D^{\prime}</annotation></semantics></math> are defined neighboring datasets having difference with one record. (<math id="S4.SS3.p2.5.5.m4.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.p2.5.5.m4.1a"><mi id="S4.SS3.p2.5.5.m4.1.1" xref="S4.SS3.p2.5.5.m4.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.5.m4.1b"><ci id="S4.SS3.p2.5.5.m4.1.1.cmml" xref="S4.SS3.p2.5.5.m4.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.5.m4.1c">\varepsilon</annotation></semantics></math>, <math id="S4.SS3.p2.6.6.m5.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.p2.6.6.m5.1a"><mi id="S4.SS3.p2.6.6.m5.1.1" xref="S4.SS3.p2.6.6.m5.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.6.m5.1b"><ci id="S4.SS3.p2.6.6.m5.1.1.cmml" xref="S4.SS3.p2.6.6.m5.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.6.m5.1c">\delta</annotation></semantics></math>) protects confidentiality <cite class="ltx_cite ltx_citemacro_cite">Dwork (<a href="#bib.bib10" title="" class="ltx_ref">2011</a>)</cite>. M gives (<math id="S4.SS3.p2.7.7.m6.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.p2.7.7.m6.1a"><mi id="S4.SS3.p2.7.7.m6.1.1" xref="S4.SS3.p2.7.7.m6.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.7.m6.1b"><ci id="S4.SS3.p2.7.7.m6.1.1.cmml" xref="S4.SS3.p2.7.7.m6.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.7.m6.1c">\varepsilon</annotation></semantics></math>, <math id="S4.SS3.p2.8.8.m7.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.p2.8.8.m7.1a"><mi id="S4.SS3.p2.8.8.m7.1.1" xref="S4.SS3.p2.8.8.m7.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.8.m7.1b"><ci id="S4.SS3.p2.8.8.m7.1.1.cmml" xref="S4.SS3.p2.8.8.m7.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.8.m7.1c">\delta</annotation></semantics></math>)-differential privacy for and D and D’ if M satsifies:</span></p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.5" class="ltx_Math" alttext="\Pr\left[M\left(D\right)\in E\right]\leq e^{\epsilon}\cdot\Pr\left[M\left(D^{\prime}\right)\in E\right]+\delta" display="block"><semantics id="S4.E1.m1.5a"><mrow id="S4.E1.m1.5.5" xref="S4.E1.m1.5.5.cmml"><mrow id="S4.E1.m1.4.4.1.1" xref="S4.E1.m1.4.4.1.2.cmml"><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">Pr</mi><mo id="S4.E1.m1.4.4.1.1a" xref="S4.E1.m1.4.4.1.2.cmml">⁡</mo><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.2.cmml"><mo id="S4.E1.m1.4.4.1.1.1.2" xref="S4.E1.m1.4.4.1.2.cmml">[</mo><mrow id="S4.E1.m1.4.4.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.2.2" xref="S4.E1.m1.4.4.1.1.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.1.1.2.1" xref="S4.E1.m1.4.4.1.1.1.1.2.1.cmml">​</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.2.3.2" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.2.3.2.1" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">D</mi><mo id="S4.E1.m1.4.4.1.1.1.1.2.3.2.2" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.cmml">∈</mo><mi id="S4.E1.m1.4.4.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.3.cmml">E</mi></mrow><mo id="S4.E1.m1.4.4.1.1.1.3" xref="S4.E1.m1.4.4.1.2.cmml">]</mo></mrow></mrow><mo id="S4.E1.m1.5.5.3" xref="S4.E1.m1.5.5.3.cmml">≤</mo><mrow id="S4.E1.m1.5.5.2" xref="S4.E1.m1.5.5.2.cmml"><mrow id="S4.E1.m1.5.5.2.1" xref="S4.E1.m1.5.5.2.1.cmml"><msup id="S4.E1.m1.5.5.2.1.3" xref="S4.E1.m1.5.5.2.1.3.cmml"><mi id="S4.E1.m1.5.5.2.1.3.2" xref="S4.E1.m1.5.5.2.1.3.2.cmml">e</mi><mi id="S4.E1.m1.5.5.2.1.3.3" xref="S4.E1.m1.5.5.2.1.3.3.cmml">ϵ</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.5.5.2.1.2" xref="S4.E1.m1.5.5.2.1.2.cmml">⋅</mo><mrow id="S4.E1.m1.5.5.2.1.1.1" xref="S4.E1.m1.5.5.2.1.1.2.cmml"><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">Pr</mi><mo id="S4.E1.m1.5.5.2.1.1.1a" xref="S4.E1.m1.5.5.2.1.1.2.cmml">⁡</mo><mrow id="S4.E1.m1.5.5.2.1.1.1.1" xref="S4.E1.m1.5.5.2.1.1.2.cmml"><mo id="S4.E1.m1.5.5.2.1.1.1.1.2" xref="S4.E1.m1.5.5.2.1.1.2.cmml">[</mo><mrow id="S4.E1.m1.5.5.2.1.1.1.1.1" xref="S4.E1.m1.5.5.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.5.5.2.1.1.1.1.1.1" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.5.5.2.1.1.1.1.1.1.3" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.2.1.1.1.1.1.1.2" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mo id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.5.5.2.1.1.1.1.1.2" xref="S4.E1.m1.5.5.2.1.1.1.1.1.2.cmml">∈</mo><mi id="S4.E1.m1.5.5.2.1.1.1.1.1.3" xref="S4.E1.m1.5.5.2.1.1.1.1.1.3.cmml">E</mi></mrow><mo id="S4.E1.m1.5.5.2.1.1.1.1.3" xref="S4.E1.m1.5.5.2.1.1.2.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E1.m1.5.5.2.2" xref="S4.E1.m1.5.5.2.2.cmml">+</mo><mi id="S4.E1.m1.5.5.2.3" xref="S4.E1.m1.5.5.2.3.cmml">δ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.5b"><apply id="S4.E1.m1.5.5.cmml" xref="S4.E1.m1.5.5"><leq id="S4.E1.m1.5.5.3.cmml" xref="S4.E1.m1.5.5.3"></leq><apply id="S4.E1.m1.4.4.1.2.cmml" xref="S4.E1.m1.4.4.1.1"><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">Pr</ci><apply id="S4.E1.m1.4.4.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1"><in id="S4.E1.m1.4.4.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1"></in><apply id="S4.E1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2"><times id="S4.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.1"></times><ci id="S4.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2">𝑀</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝐷</ci></apply><ci id="S4.E1.m1.4.4.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.3">𝐸</ci></apply></apply><apply id="S4.E1.m1.5.5.2.cmml" xref="S4.E1.m1.5.5.2"><plus id="S4.E1.m1.5.5.2.2.cmml" xref="S4.E1.m1.5.5.2.2"></plus><apply id="S4.E1.m1.5.5.2.1.cmml" xref="S4.E1.m1.5.5.2.1"><ci id="S4.E1.m1.5.5.2.1.2.cmml" xref="S4.E1.m1.5.5.2.1.2">⋅</ci><apply id="S4.E1.m1.5.5.2.1.3.cmml" xref="S4.E1.m1.5.5.2.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.2.1.3.1.cmml" xref="S4.E1.m1.5.5.2.1.3">superscript</csymbol><ci id="S4.E1.m1.5.5.2.1.3.2.cmml" xref="S4.E1.m1.5.5.2.1.3.2">𝑒</ci><ci id="S4.E1.m1.5.5.2.1.3.3.cmml" xref="S4.E1.m1.5.5.2.1.3.3">italic-ϵ</ci></apply><apply id="S4.E1.m1.5.5.2.1.1.2.cmml" xref="S4.E1.m1.5.5.2.1.1.1"><ci id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">Pr</ci><apply id="S4.E1.m1.5.5.2.1.1.1.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1"><in id="S4.E1.m1.5.5.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.2"></in><apply id="S4.E1.m1.5.5.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1"><times id="S4.E1.m1.5.5.2.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.5.5.2.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.3">𝑀</ci><apply id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="S4.E1.m1.5.5.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.5.5.2.1.1.1.1.1.3">𝐸</ci></apply></apply></apply><ci id="S4.E1.m1.5.5.2.3.cmml" xref="S4.E1.m1.5.5.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.5c">\Pr\left[M\left(D\right)\in E\right]\leq e^{\epsilon}\cdot\Pr\left[M\left(D^{\prime}\right)\in E\right]+\delta</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.p2.10" class="ltx_p">where <math id="S4.SS3.p2.9.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.p2.9.m1.1a"><mi id="S4.SS3.p2.9.m1.1.1" xref="S4.SS3.p2.9.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m1.1b"><ci id="S4.SS3.p2.9.m1.1.1.cmml" xref="S4.SS3.p2.9.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m1.1c">\varepsilon</annotation></semantics></math> denotes the privacy budget and <math id="S4.SS3.p2.10.m2.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.p2.10.m2.1a"><mi id="S4.SS3.p2.10.m2.1.1" xref="S4.SS3.p2.10.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m2.1b"><ci id="S4.SS3.p2.10.m2.1.1.cmml" xref="S4.SS3.p2.10.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m2.1c">\delta</annotation></semantics></math> represents the probability of error.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>The Privacy Budget</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.2" class="ltx_p">The privacy guarantee level of <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mi id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><ci id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">M</annotation></semantics></math> is controlled through privacy budget of <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mi id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><ci id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">\epsilon</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Haeberlen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2011</a>)</cite>.There are two widely used privacy budget compositions as the sequential composition and the parallel composition.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.16" class="ltx_p">The ratio between the two mechanisms (<math id="S4.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="M(D)" display="inline"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mrow id="S4.SS3.SSS1.p2.1.m1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.2.cmml"><mi id="S4.SS3.SSS1.p2.1.m1.1.2.2" xref="S4.SS3.SSS1.p2.1.m1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p2.1.m1.1.2.1" xref="S4.SS3.SSS1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S4.SS3.SSS1.p2.1.m1.1.2.3.2" xref="S4.SS3.SSS1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p2.1.m1.1.2.3.2.1" xref="S4.SS3.SSS1.p2.1.m1.1.2.cmml">(</mo><mi id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml">D</mi><mo stretchy="false" id="S4.SS3.SSS1.p2.1.m1.1.2.3.2.2" xref="S4.SS3.SSS1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.2"><times id="S4.SS3.SSS1.p2.1.m1.1.2.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.2.1"></times><ci id="S4.SS3.SSS1.p2.1.m1.1.2.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.2.2">𝑀</ci><ci id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">M(D)</annotation></semantics></math> and <math id="S4.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="M(D^{\prime})" display="inline"><semantics id="S4.SS3.SSS1.p2.2.m2.1a"><mrow id="S4.SS3.SSS1.p2.2.m2.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p2.2.m2.1.1.3" xref="S4.SS3.SSS1.p2.2.m2.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p2.2.m2.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml">​</mo><mrow id="S4.SS3.SSS1.p2.2.m2.1.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><msup id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.2.cmml">D</mi><mo id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.3" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.3" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.2.m2.1b"><apply id="S4.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1"><times id="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2"></times><ci id="S4.SS3.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.3">𝑀</ci><apply id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1">superscript</csymbol><ci id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.2">𝐷</ci><ci id="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.1.1.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.2.m2.1c">M(D^{\prime})</annotation></semantics></math>) limits by <math id="S4.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="e^{\varepsilon}" display="inline"><semantics id="S4.SS3.SSS1.p2.3.m3.1a"><msup id="S4.SS3.SSS1.p2.3.m3.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p2.3.m3.1.1.2" xref="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml">e</mi><mi id="S4.SS3.SSS1.p2.3.m3.1.1.3" xref="S4.SS3.SSS1.p2.3.m3.1.1.3.cmml">ε</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.3.m3.1b"><apply id="S4.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1">superscript</csymbol><ci id="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.2">𝑒</ci><ci id="S4.SS3.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.3">𝜀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.3.m3.1c">e^{\varepsilon}</annotation></semantics></math>. For <math id="S4.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.SSS1.p2.4.m4.1a"><mi id="S4.SS3.SSS1.p2.4.m4.1.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.4.m4.1b"><ci id="S4.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.4.m4.1c">\delta</annotation></semantics></math> <math id="S4.SS3.SSS1.p2.5.m5.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.SS3.SSS1.p2.5.m5.1a"><mo id="S4.SS3.SSS1.p2.5.m5.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.5.m5.1b"><eq id="S4.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.5.m5.1c">=</annotation></semantics></math> <math id="S4.SS3.SSS1.p2.6.m6.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS3.SSS1.p2.6.m6.1a"><mn id="S4.SS3.SSS1.p2.6.m6.1.1" xref="S4.SS3.SSS1.p2.6.m6.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.6.m6.1b"><cn type="integer" id="S4.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1">0</cn></annotation-xml></semantics></math>, M gives <math id="S4.SS3.SSS1.p2.7.m7.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.SSS1.p2.7.m7.1a"><mi id="S4.SS3.SSS1.p2.7.m7.1.1" xref="S4.SS3.SSS1.p2.7.m7.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.7.m7.1b"><ci id="S4.SS3.SSS1.p2.7.m7.1.1.cmml" xref="S4.SS3.SSS1.p2.7.m7.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.7.m7.1c">\varepsilon</annotation></semantics></math>-differential privacy by its strictest definition. In other case, for some low probability cases, (<math id="S4.SS3.SSS1.p2.8.m8.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.SSS1.p2.8.m8.1a"><mi id="S4.SS3.SSS1.p2.8.m8.1.1" xref="S4.SS3.SSS1.p2.8.m8.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.8.m8.1b"><ci id="S4.SS3.SSS1.p2.8.m8.1.1.cmml" xref="S4.SS3.SSS1.p2.8.m8.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.8.m8.1c">\varepsilon</annotation></semantics></math>,<math id="S4.SS3.SSS1.p2.9.m9.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.SSS1.p2.9.m9.1a"><mi id="S4.SS3.SSS1.p2.9.m9.1.1" xref="S4.SS3.SSS1.p2.9.m9.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.9.m9.1b"><ci id="S4.SS3.SSS1.p2.9.m9.1.1.cmml" xref="S4.SS3.SSS1.p2.9.m9.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.9.m9.1c">\delta</annotation></semantics></math>)-differential privacy provides latitude to invade strict <math id="S4.SS3.SSS1.p2.10.m10.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.SSS1.p2.10.m10.1a"><mi id="S4.SS3.SSS1.p2.10.m10.1.1" xref="S4.SS3.SSS1.p2.10.m10.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.10.m10.1b"><ci id="S4.SS3.SSS1.p2.10.m10.1.1.cmml" xref="S4.SS3.SSS1.p2.10.m10.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.10.m10.1c">\varepsilon</annotation></semantics></math>-differential privacy. <math id="S4.SS3.SSS1.p2.11.m11.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.SSS1.p2.11.m11.1a"><mi id="S4.SS3.SSS1.p2.11.m11.1.1" xref="S4.SS3.SSS1.p2.11.m11.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.11.m11.1b"><ci id="S4.SS3.SSS1.p2.11.m11.1.1.cmml" xref="S4.SS3.SSS1.p2.11.m11.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.11.m11.1c">\varepsilon</annotation></semantics></math>-differential privacy is called as pure differential privacy and (<math id="S4.SS3.SSS1.p2.12.m12.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS3.SSS1.p2.12.m12.1a"><mi id="S4.SS3.SSS1.p2.12.m12.1.1" xref="S4.SS3.SSS1.p2.12.m12.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.12.m12.1b"><ci id="S4.SS3.SSS1.p2.12.m12.1.1.cmml" xref="S4.SS3.SSS1.p2.12.m12.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.12.m12.1c">\varepsilon</annotation></semantics></math>, <math id="S4.SS3.SSS1.p2.13.m13.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.SSS1.p2.13.m13.1a"><mi id="S4.SS3.SSS1.p2.13.m13.1.1" xref="S4.SS3.SSS1.p2.13.m13.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.13.m13.1b"><ci id="S4.SS3.SSS1.p2.13.m13.1.1.cmml" xref="S4.SS3.SSS1.p2.13.m13.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.13.m13.1c">\delta</annotation></semantics></math>)-differential privacy, where <math id="S4.SS3.SSS1.p2.14.m14.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS3.SSS1.p2.14.m14.1a"><mi id="S4.SS3.SSS1.p2.14.m14.1.1" xref="S4.SS3.SSS1.p2.14.m14.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.14.m14.1b"><ci id="S4.SS3.SSS1.p2.14.m14.1.1.cmml" xref="S4.SS3.SSS1.p2.14.m14.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.14.m14.1c">\delta</annotation></semantics></math> <math id="S4.SS3.SSS1.p2.15.m15.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS3.SSS1.p2.15.m15.1a"><mo id="S4.SS3.SSS1.p2.15.m15.1.1" xref="S4.SS3.SSS1.p2.15.m15.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.15.m15.1b"><gt id="S4.SS3.SSS1.p2.15.m15.1.1.cmml" xref="S4.SS3.SSS1.p2.15.m15.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.15.m15.1c">&gt;</annotation></semantics></math> <math id="S4.SS3.SSS1.p2.16.m16.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS3.SSS1.p2.16.m16.1a"><mn id="S4.SS3.SSS1.p2.16.m16.1.1" xref="S4.SS3.SSS1.p2.16.m16.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.16.m16.1b"><cn type="integer" id="S4.SS3.SSS1.p2.16.m16.1.1.cmml" xref="S4.SS3.SSS1.p2.16.m16.1.1">0</cn></annotation-xml></semantics></math>, is called as <span id="S4.SS3.SSS1.p2.16.1" class="ltx_text ltx_font_italic">approximate differential privacy</span> <cite class="ltx_cite ltx_citemacro_cite">Beimel et al. (<a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite>. Differential privacy has two implementation settings: Centralized DP (CDP) via DP-SGD and Local DP (LDP) <cite class="ltx_cite ltx_citemacro_cite">Qu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">In CDP, a trusted data curator answers queries or releases differentially private models by using randomisation algorithms <cite class="ltx_cite ltx_citemacro_cite">Dwork and Roth (<a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite>. In this article, we use DP-SGD (Differentially Private Stochastic Gradient Descent) <cite class="ltx_cite ltx_citemacro_cite">Abadi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite> to train our models.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Federated Learning</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.7" class="ltx_p">As conventional centralized learning systems require that all training data produced on different devices be uploaded to a server or cloud for training, it may give rise to serious privacy concerns  <cite class="ltx_cite ltx_citemacro_cite">Privacy (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>.
FL allows training an algorithm in a decentralized way  <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>, <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>. It ensures multiple parties collectively train a machine learning model without exchanging the local data  <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. To define mathematically, it is assumed that there are <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">N</annotation></semantics></math> parties, and each party is showed with <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">T</mi><mi id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">𝑇</ci><ci id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">T_{i}</annotation></semantics></math>, where <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mi id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><ci id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">i</annotation></semantics></math> <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mo id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><in id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">\in</annotation></semantics></math> <math id="S4.SS4.p1.5.m5.2" class="ltx_Math" alttext="[1,N]" display="inline"><semantics id="S4.SS4.p1.5.m5.2a"><mrow id="S4.SS4.p1.5.m5.2.3.2" xref="S4.SS4.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.5.m5.2.3.2.1" xref="S4.SS4.p1.5.m5.2.3.1.cmml">[</mo><mn id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml">1</mn><mo id="S4.SS4.p1.5.m5.2.3.2.2" xref="S4.SS4.p1.5.m5.2.3.1.cmml">,</mo><mi id="S4.SS4.p1.5.m5.2.2" xref="S4.SS4.p1.5.m5.2.2.cmml">N</mi><mo stretchy="false" id="S4.SS4.p1.5.m5.2.3.2.3" xref="S4.SS4.p1.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.2b"><interval closure="closed" id="S4.SS4.p1.5.m5.2.3.1.cmml" xref="S4.SS4.p1.5.m5.2.3.2"><cn type="integer" id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1">1</cn><ci id="S4.SS4.p1.5.m5.2.2.cmml" xref="S4.SS4.p1.5.m5.2.2">𝑁</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.2c">[1,N]</annotation></semantics></math>. For the non-federated setting, each party uses its local data and depicted by <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><msub id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mi id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">D</mi><mi id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p1.6.m6.1.1.2.cmml" xref="S4.SS4.p1.6.m6.1.1.2">𝐷</ci><ci id="S4.SS4.p1.6.m6.1.1.3.cmml" xref="S4.SS4.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">D_{i}</annotation></semantics></math> to train a local model <math id="S4.SS4.p1.7.m7.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S4.SS4.p1.7.m7.1a"><msub id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml"><mi id="S4.SS4.p1.7.m7.1.1.2" xref="S4.SS4.p1.7.m7.1.1.2.cmml">M</mi><mi id="S4.SS4.p1.7.m7.1.1.3" xref="S4.SS4.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><apply id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.7.m7.1.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS4.p1.7.m7.1.1.2.cmml" xref="S4.SS4.p1.7.m7.1.1.2">𝑀</ci><ci id="S4.SS4.p1.7.m7.1.1.3.cmml" xref="S4.SS4.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">M_{i}</annotation></semantics></math> and send the local model parameters to the server. The predictive data is sent only the local model parameters to the FL server. Most centralized setups have just the IID assumption for train test data but in a federated learning based decentralized setup, non-IID poses the problem of high skewness of different devices due to different data distribution <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">In federated language modeling, existing works <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> use FedAvg as the federated optimization algorithm. In FedAvg, gradients that are computed locally over a large population of clients are aggregated by the server to build a novel global model. Every client is trained by locally stored data and computes the average gradient with the current global model via one or more steps of SGD.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Applying FL to text classification can cause problems such as designing proper aggregating algorithms for handling the gradients or weights uploaded by different client models. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib36" title="" class="ltx_ref">Zhu et al.</a></cite> proposed a text classification using the standard FedAvg algorithm to update the model parameter with local trained models. Model compression has also been introduced to federated classification tasks due to the dilemma of computation restraints on the client-side, where an attempt to reduce the model size on the client-side to enable the real application of federated learning was made. For overcoming the communication dilemma of FL, central server can successfully train the central model with only one or a few rounds of communication under poor communication scenarios in a one-shot or few-shot setting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the scope of the study, the FinBERT (pre-trained) model is used as the base model. Two NLP models were trained by implementing DP and FL. In this section, the results presented in the tables are discussed. The results placed in the tables are the average and the standard deviation of the results obtained after running the models thrice.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The dataset was split into train set and test set with 80:20 train test ratio. BERT and RoBERTa based models were used for the language modelling part. It should be noted that the table contain the average and the standard deviation of the results obtained after running the models 3 times. Table 1 shows a comparison according to epsilon values between both the language models using Centralized DP and in a Federated Learning set up. The Opacus library was used along with PyTorch for the experiments. We implement DP, FL and DP-FL on BERT and RoBERTa for <math id="S5.p2.1.m1.5" class="ltx_Math" alttext="\epsilon=0.5,5,15,20,25" display="inline"><semantics id="S5.p2.1.m1.5a"><mrow id="S5.p2.1.m1.5.6" xref="S5.p2.1.m1.5.6.cmml"><mi id="S5.p2.1.m1.5.6.2" xref="S5.p2.1.m1.5.6.2.cmml">ϵ</mi><mo id="S5.p2.1.m1.5.6.1" xref="S5.p2.1.m1.5.6.1.cmml">=</mo><mrow id="S5.p2.1.m1.5.6.3.2" xref="S5.p2.1.m1.5.6.3.1.cmml"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">0.5</mn><mo id="S5.p2.1.m1.5.6.3.2.1" xref="S5.p2.1.m1.5.6.3.1.cmml">,</mo><mn id="S5.p2.1.m1.2.2" xref="S5.p2.1.m1.2.2.cmml">5</mn><mo id="S5.p2.1.m1.5.6.3.2.2" xref="S5.p2.1.m1.5.6.3.1.cmml">,</mo><mn id="S5.p2.1.m1.3.3" xref="S5.p2.1.m1.3.3.cmml">15</mn><mo id="S5.p2.1.m1.5.6.3.2.3" xref="S5.p2.1.m1.5.6.3.1.cmml">,</mo><mn id="S5.p2.1.m1.4.4" xref="S5.p2.1.m1.4.4.cmml">20</mn><mo id="S5.p2.1.m1.5.6.3.2.4" xref="S5.p2.1.m1.5.6.3.1.cmml">,</mo><mn id="S5.p2.1.m1.5.5" xref="S5.p2.1.m1.5.5.cmml">25</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.5b"><apply id="S5.p2.1.m1.5.6.cmml" xref="S5.p2.1.m1.5.6"><eq id="S5.p2.1.m1.5.6.1.cmml" xref="S5.p2.1.m1.5.6.1"></eq><ci id="S5.p2.1.m1.5.6.2.cmml" xref="S5.p2.1.m1.5.6.2">italic-ϵ</ci><list id="S5.p2.1.m1.5.6.3.1.cmml" xref="S5.p2.1.m1.5.6.3.2"><cn type="float" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">0.5</cn><cn type="integer" id="S5.p2.1.m1.2.2.cmml" xref="S5.p2.1.m1.2.2">5</cn><cn type="integer" id="S5.p2.1.m1.3.3.cmml" xref="S5.p2.1.m1.3.3">15</cn><cn type="integer" id="S5.p2.1.m1.4.4.cmml" xref="S5.p2.1.m1.4.4">20</cn><cn type="integer" id="S5.p2.1.m1.5.5.cmml" xref="S5.p2.1.m1.5.5">25</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.5c">\epsilon=0.5,5,15,20,25</annotation></semantics></math>. Our baseline model (with no noise) achieves an accuracy of <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">67.71%</span> and <span id="S5.p2.1.2" class="ltx_text ltx_font_bold">68.37%</span> on BERT and RoBERTa respectively.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In baseline mode, we can see that RoBERTa has a slight improvement over BERT because of its robustness owing to a heavier pre-training procedure. We also notice that with the increase in epsilon values, the amount of standard deviation decreases as the model approaches towards its vanilla variant (without DP noise).</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.6" class="ltx_p">Table 2 also shows us the results obtained when DP was applied in a federated learning mode, both in IID (Identical and Independently distributed) and Non-IID data silos. For Non-IID scenarios, we assume <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.p4.1.m1.1a"><mn id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><cn type="integer" id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">10</annotation></semantics></math> shards of size <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="240" display="inline"><semantics id="S5.p4.2.m2.1a"><mn id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">240</mn><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><cn type="integer" id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">240</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">240</annotation></semantics></math> assigned to each client. We run it over <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.p4.3.m3.1a"><mn id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><cn type="integer" id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">10</annotation></semantics></math> clients in total, selecting only a fraction of <math id="S5.p4.4.m4.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S5.p4.4.m4.1a"><mn id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.1b"><cn type="float" id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.1c">0.5</annotation></semantics></math> in each round for training. We add DP locally, that is, to each client model at every iteration and aggregate them to perform Federated Averaging. We observe the best accuracies with RoBERTa for the centralised DP implementation, particularly with <math id="S5.p4.5.m5.1" class="ltx_Math" alttext="\epsilon=25" display="inline"><semantics id="S5.p4.5.m5.1a"><mrow id="S5.p4.5.m5.1.1" xref="S5.p4.5.m5.1.1.cmml"><mi id="S5.p4.5.m5.1.1.2" xref="S5.p4.5.m5.1.1.2.cmml">ϵ</mi><mo id="S5.p4.5.m5.1.1.1" xref="S5.p4.5.m5.1.1.1.cmml">=</mo><mn id="S5.p4.5.m5.1.1.3" xref="S5.p4.5.m5.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.1b"><apply id="S5.p4.5.m5.1.1.cmml" xref="S5.p4.5.m5.1.1"><eq id="S5.p4.5.m5.1.1.1.cmml" xref="S5.p4.5.m5.1.1.1"></eq><ci id="S5.p4.5.m5.1.1.2.cmml" xref="S5.p4.5.m5.1.1.2">italic-ϵ</ci><cn type="integer" id="S5.p4.5.m5.1.1.3.cmml" xref="S5.p4.5.m5.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.1c">\epsilon=25</annotation></semantics></math> with an accuracy of 62.6%. BERT in a centralised DP setting does come close at <math id="S5.p4.6.m6.1" class="ltx_Math" alttext="\epsilon=25" display="inline"><semantics id="S5.p4.6.m6.1a"><mrow id="S5.p4.6.m6.1.1" xref="S5.p4.6.m6.1.1.cmml"><mi id="S5.p4.6.m6.1.1.2" xref="S5.p4.6.m6.1.1.2.cmml">ϵ</mi><mo id="S5.p4.6.m6.1.1.1" xref="S5.p4.6.m6.1.1.1.cmml">=</mo><mn id="S5.p4.6.m6.1.1.3" xref="S5.p4.6.m6.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.6.m6.1b"><apply id="S5.p4.6.m6.1.1.cmml" xref="S5.p4.6.m6.1.1"><eq id="S5.p4.6.m6.1.1.1.cmml" xref="S5.p4.6.m6.1.1.1"></eq><ci id="S5.p4.6.m6.1.1.2.cmml" xref="S5.p4.6.m6.1.1.2">italic-ϵ</ci><cn type="integer" id="S5.p4.6.m6.1.1.3.cmml" xref="S5.p4.6.m6.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.6.m6.1c">\epsilon=25</annotation></semantics></math> with an accuracy of 60.03%. The results also show that the accuracy decreases by adding FL to the DP implementations.</p>
</div>
<figure id="S5.33" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Averaged Test Accuracies of FL and DPFL models</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.31.31" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:441.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.3pt,-76.6pt) scale(1.53160322300612,1.53160322300612) ;">
<table id="S5.31.31.31" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.1.1.1.1" class="ltx_tr">
<th id="S5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Setup</span></th>
<td id="S5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Epsilon(<math id="S5.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S5.1.1.1.1.1.1.m1.1a"><mi id="S5.1.1.1.1.1.1.m1.1.1" xref="S5.1.1.1.1.1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.1.1.1.1.1.1.m1.1b"><ci id="S5.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.1.1.1.1.1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.1.1.1.1.1.1.m1.1c">\epsilon</annotation></semantics></math>)</span></td>
<td id="S5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">BERT</span></td>
<td id="S5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">RoBERTa</span></td>
</tr>
<tr id="S5.3.3.3.3" class="ltx_tr">
<th id="S5.3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S5.3.3.3.3.3.1" class="ltx_text">Centralized DP</span></th>
<td id="S5.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S5.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.5<math id="S5.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.2.2.2.2.1.m1.1a"><mo id="S5.2.2.2.2.1.m1.1.1" xref="S5.2.2.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.2.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S5.2.2.2.2.1.m1.1.1.cmml" xref="S5.2.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.2.2.2.2.1.m1.1c">\pm</annotation></semantics></math>23.94</td>
<td id="S5.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.36 <math id="S5.3.3.3.3.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.3.3.3.3.2.m1.1a"><mo id="S5.3.3.3.3.2.m1.1.1" xref="S5.3.3.3.3.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.3.3.3.3.2.m1.1b"><csymbol cd="latexml" id="S5.3.3.3.3.2.m1.1.1.cmml" xref="S5.3.3.3.3.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.3.3.3.3.2.m1.1c">\pm</annotation></semantics></math> 26.35</td>
</tr>
<tr id="S5.5.5.5.5" class="ltx_tr">
<td id="S5.5.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S5.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r">37.48<math id="S5.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.4.4.4.4.1.m1.1a"><mo id="S5.4.4.4.4.1.m1.1.1" xref="S5.4.4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.4.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.4.4.4.4.1.m1.1.1.cmml" xref="S5.4.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.4.4.4.4.1.m1.1c">\pm</annotation></semantics></math>20.42</td>
<td id="S5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r">38.34 <math id="S5.5.5.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.5.5.5.5.2.m1.1a"><mo id="S5.5.5.5.5.2.m1.1.1" xref="S5.5.5.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.5.5.5.5.2.m1.1b"><csymbol cd="latexml" id="S5.5.5.5.5.2.m1.1.1.cmml" xref="S5.5.5.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.5.5.5.5.2.m1.1c">\pm</annotation></semantics></math>20.08</td>
</tr>
<tr id="S5.7.7.7.7" class="ltx_tr">
<td id="S5.7.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r">15</td>
<td id="S5.6.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r">51.71 <math id="S5.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.6.6.6.6.1.m1.1a"><mo id="S5.6.6.6.6.1.m1.1.1" xref="S5.6.6.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.6.6.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.6.6.6.6.1.m1.1.1.cmml" xref="S5.6.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.6.6.6.6.1.m1.1c">\pm</annotation></semantics></math>14.71</td>
<td id="S5.7.7.7.7.2" class="ltx_td ltx_align_center ltx_border_r">51.34 <math id="S5.7.7.7.7.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.7.7.7.7.2.m1.1a"><mo id="S5.7.7.7.7.2.m1.1.1" xref="S5.7.7.7.7.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.7.7.7.7.2.m1.1b"><csymbol cd="latexml" id="S5.7.7.7.7.2.m1.1.1.cmml" xref="S5.7.7.7.7.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.7.7.7.7.2.m1.1c">\pm</annotation></semantics></math> 15.45</td>
</tr>
<tr id="S5.9.9.9.9" class="ltx_tr">
<td id="S5.9.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S5.8.8.8.8.1" class="ltx_td ltx_align_center ltx_border_r">55.37 <math id="S5.8.8.8.8.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.8.8.8.8.1.m1.1a"><mo id="S5.8.8.8.8.1.m1.1.1" xref="S5.8.8.8.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.8.8.8.8.1.m1.1b"><csymbol cd="latexml" id="S5.8.8.8.8.1.m1.1.1.cmml" xref="S5.8.8.8.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.8.8.8.8.1.m1.1c">\pm</annotation></semantics></math> 5.49</td>
<td id="S5.9.9.9.9.2" class="ltx_td ltx_align_center ltx_border_r">55.54 <math id="S5.9.9.9.9.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.9.9.9.9.2.m1.1a"><mo id="S5.9.9.9.9.2.m1.1.1" xref="S5.9.9.9.9.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.9.9.9.9.2.m1.1b"><csymbol cd="latexml" id="S5.9.9.9.9.2.m1.1.1.cmml" xref="S5.9.9.9.9.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.9.9.9.9.2.m1.1c">\pm</annotation></semantics></math> 5.54</td>
</tr>
<tr id="S5.11.11.11.11" class="ltx_tr">
<td id="S5.11.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="S5.10.10.10.10.1" class="ltx_td ltx_align_center ltx_border_r">60.03 <math id="S5.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.10.10.10.10.1.m1.1a"><mo id="S5.10.10.10.10.1.m1.1.1" xref="S5.10.10.10.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.10.10.10.10.1.m1.1b"><csymbol cd="latexml" id="S5.10.10.10.10.1.m1.1.1.cmml" xref="S5.10.10.10.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.10.10.10.10.1.m1.1c">\pm</annotation></semantics></math> 1.37</td>
<td id="S5.11.11.11.11.2" class="ltx_td ltx_align_center ltx_border_r">62.6 <math id="S5.11.11.11.11.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.11.11.11.11.2.m1.1a"><mo id="S5.11.11.11.11.2.m1.1.1" xref="S5.11.11.11.11.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.11.11.11.11.2.m1.1b"><csymbol cd="latexml" id="S5.11.11.11.11.2.m1.1.1.cmml" xref="S5.11.11.11.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.11.11.11.11.2.m1.1c">\pm</annotation></semantics></math> 4.24</td>
</tr>
<tr id="S5.13.13.13.13" class="ltx_tr">
<th id="S5.13.13.13.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="5">
<span id="S5.13.13.13.13.3.1" class="ltx_text">DP-FL</span>
IID</th>
<td id="S5.13.13.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S5.12.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.57 <math id="S5.12.12.12.12.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.12.12.12.12.1.m1.1a"><mo id="S5.12.12.12.12.1.m1.1.1" xref="S5.12.12.12.12.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.12.12.12.12.1.m1.1b"><csymbol cd="latexml" id="S5.12.12.12.12.1.m1.1.1.cmml" xref="S5.12.12.12.12.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.12.12.12.12.1.m1.1c">\pm</annotation></semantics></math> 2.86</td>
<td id="S5.13.13.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.11 <math id="S5.13.13.13.13.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.13.13.13.13.2.m1.1a"><mo id="S5.13.13.13.13.2.m1.1.1" xref="S5.13.13.13.13.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.13.13.13.13.2.m1.1b"><csymbol cd="latexml" id="S5.13.13.13.13.2.m1.1.1.cmml" xref="S5.13.13.13.13.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.13.13.13.13.2.m1.1c">\pm</annotation></semantics></math> 7.68</td>
</tr>
<tr id="S5.15.15.15.15" class="ltx_tr">
<td id="S5.15.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S5.14.14.14.14.1" class="ltx_td ltx_align_center ltx_border_r">30 <math id="S5.14.14.14.14.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.14.14.14.14.1.m1.1a"><mo id="S5.14.14.14.14.1.m1.1.1" xref="S5.14.14.14.14.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.14.14.14.14.1.m1.1b"><csymbol cd="latexml" id="S5.14.14.14.14.1.m1.1.1.cmml" xref="S5.14.14.14.14.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.14.14.14.14.1.m1.1c">\pm</annotation></semantics></math>25.6</td>
<td id="S5.15.15.15.15.2" class="ltx_td ltx_align_center ltx_border_r">30.04 <math id="S5.15.15.15.15.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.15.15.15.15.2.m1.1a"><mo id="S5.15.15.15.15.2.m1.1.1" xref="S5.15.15.15.15.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.15.15.15.15.2.m1.1b"><csymbol cd="latexml" id="S5.15.15.15.15.2.m1.1.1.cmml" xref="S5.15.15.15.15.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.15.15.15.15.2.m1.1c">\pm</annotation></semantics></math> 28.22</td>
</tr>
<tr id="S5.17.17.17.17" class="ltx_tr">
<td id="S5.17.17.17.17.3" class="ltx_td ltx_align_center ltx_border_r">15</td>
<td id="S5.16.16.16.16.1" class="ltx_td ltx_align_center ltx_border_r">40.34 <math id="S5.16.16.16.16.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.16.16.16.16.1.m1.1a"><mo id="S5.16.16.16.16.1.m1.1.1" xref="S5.16.16.16.16.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.16.16.16.16.1.m1.1b"><csymbol cd="latexml" id="S5.16.16.16.16.1.m1.1.1.cmml" xref="S5.16.16.16.16.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.16.16.16.16.1.m1.1c">\pm</annotation></semantics></math> 20.55</td>
<td id="S5.17.17.17.17.2" class="ltx_td ltx_align_center ltx_border_r">50.26 <math id="S5.17.17.17.17.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.17.17.17.17.2.m1.1a"><mo id="S5.17.17.17.17.2.m1.1.1" xref="S5.17.17.17.17.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.17.17.17.17.2.m1.1b"><csymbol cd="latexml" id="S5.17.17.17.17.2.m1.1.1.cmml" xref="S5.17.17.17.17.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.17.17.17.17.2.m1.1c">\pm</annotation></semantics></math> 20.84</td>
</tr>
<tr id="S5.19.19.19.19" class="ltx_tr">
<td id="S5.19.19.19.19.3" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S5.18.18.18.18.1" class="ltx_td ltx_align_center ltx_border_r">51.05 <math id="S5.18.18.18.18.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.18.18.18.18.1.m1.1a"><mo id="S5.18.18.18.18.1.m1.1.1" xref="S5.18.18.18.18.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.18.18.18.18.1.m1.1b"><csymbol cd="latexml" id="S5.18.18.18.18.1.m1.1.1.cmml" xref="S5.18.18.18.18.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.18.18.18.18.1.m1.1c">\pm</annotation></semantics></math> 7.95</td>
<td id="S5.19.19.19.19.2" class="ltx_td ltx_align_center ltx_border_r">54.78 <math id="S5.19.19.19.19.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.19.19.19.19.2.m1.1a"><mo id="S5.19.19.19.19.2.m1.1.1" xref="S5.19.19.19.19.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.19.19.19.19.2.m1.1b"><csymbol cd="latexml" id="S5.19.19.19.19.2.m1.1.1.cmml" xref="S5.19.19.19.19.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.19.19.19.19.2.m1.1c">\pm</annotation></semantics></math> 2.99</td>
</tr>
<tr id="S5.21.21.21.21" class="ltx_tr">
<td id="S5.21.21.21.21.3" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="S5.20.20.20.20.1" class="ltx_td ltx_align_center ltx_border_r">53.47 <math id="S5.20.20.20.20.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.20.20.20.20.1.m1.1a"><mo id="S5.20.20.20.20.1.m1.1.1" xref="S5.20.20.20.20.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.20.20.20.20.1.m1.1b"><csymbol cd="latexml" id="S5.20.20.20.20.1.m1.1.1.cmml" xref="S5.20.20.20.20.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.20.20.20.20.1.m1.1c">\pm</annotation></semantics></math> 6.48</td>
<td id="S5.21.21.21.21.2" class="ltx_td ltx_align_center ltx_border_r">61.38 <math id="S5.21.21.21.21.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.21.21.21.21.2.m1.1a"><mo id="S5.21.21.21.21.2.m1.1.1" xref="S5.21.21.21.21.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.21.21.21.21.2.m1.1b"><csymbol cd="latexml" id="S5.21.21.21.21.2.m1.1.1.cmml" xref="S5.21.21.21.21.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.21.21.21.21.2.m1.1c">\pm</annotation></semantics></math> 0.93</td>
</tr>
<tr id="S5.23.23.23.23" class="ltx_tr">
<th id="S5.23.23.23.23.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="5">
<span id="S5.23.23.23.23.3.1" class="ltx_text">DP-FL</span>
Non IID</th>
<td id="S5.23.23.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S5.22.22.22.22.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.82 <math id="S5.22.22.22.22.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.22.22.22.22.1.m1.1a"><mo id="S5.22.22.22.22.1.m1.1.1" xref="S5.22.22.22.22.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.22.22.22.22.1.m1.1b"><csymbol cd="latexml" id="S5.22.22.22.22.1.m1.1.1.cmml" xref="S5.22.22.22.22.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.22.22.22.22.1.m1.1c">\pm</annotation></semantics></math> 5.97</td>
<td id="S5.23.23.23.23.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.13 <math id="S5.23.23.23.23.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.23.23.23.23.2.m1.1a"><mo id="S5.23.23.23.23.2.m1.1.1" xref="S5.23.23.23.23.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.23.23.23.23.2.m1.1b"><csymbol cd="latexml" id="S5.23.23.23.23.2.m1.1.1.cmml" xref="S5.23.23.23.23.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.23.23.23.23.2.m1.1c">\pm</annotation></semantics></math> 25.41</td>
</tr>
<tr id="S5.25.25.25.25" class="ltx_tr">
<td id="S5.25.25.25.25.3" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S5.24.24.24.24.1" class="ltx_td ltx_align_center ltx_border_r">35.74 <math id="S5.24.24.24.24.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.24.24.24.24.1.m1.1a"><mo id="S5.24.24.24.24.1.m1.1.1" xref="S5.24.24.24.24.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.24.24.24.24.1.m1.1b"><csymbol cd="latexml" id="S5.24.24.24.24.1.m1.1.1.cmml" xref="S5.24.24.24.24.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.24.24.24.24.1.m1.1c">\pm</annotation></semantics></math> 21.48</td>
<td id="S5.25.25.25.25.2" class="ltx_td ltx_align_center ltx_border_r">36.51 <math id="S5.25.25.25.25.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.25.25.25.25.2.m1.1a"><mo id="S5.25.25.25.25.2.m1.1.1" xref="S5.25.25.25.25.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.25.25.25.25.2.m1.1b"><csymbol cd="latexml" id="S5.25.25.25.25.2.m1.1.1.cmml" xref="S5.25.25.25.25.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.25.25.25.25.2.m1.1c">\pm</annotation></semantics></math> 26.87</td>
</tr>
<tr id="S5.27.27.27.27" class="ltx_tr">
<td id="S5.27.27.27.27.3" class="ltx_td ltx_align_center ltx_border_r">15</td>
<td id="S5.26.26.26.26.1" class="ltx_td ltx_align_center ltx_border_r">45.87 <math id="S5.26.26.26.26.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.26.26.26.26.1.m1.1a"><mo id="S5.26.26.26.26.1.m1.1.1" xref="S5.26.26.26.26.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.26.26.26.26.1.m1.1b"><csymbol cd="latexml" id="S5.26.26.26.26.1.m1.1.1.cmml" xref="S5.26.26.26.26.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.26.26.26.26.1.m1.1c">\pm</annotation></semantics></math> 15.56</td>
<td id="S5.27.27.27.27.2" class="ltx_td ltx_align_center ltx_border_r">49.83 <math id="S5.27.27.27.27.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.27.27.27.27.2.m1.1a"><mo id="S5.27.27.27.27.2.m1.1.1" xref="S5.27.27.27.27.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.27.27.27.27.2.m1.1b"><csymbol cd="latexml" id="S5.27.27.27.27.2.m1.1.1.cmml" xref="S5.27.27.27.27.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.27.27.27.27.2.m1.1c">\pm</annotation></semantics></math> 20.6</td>
</tr>
<tr id="S5.29.29.29.29" class="ltx_tr">
<td id="S5.29.29.29.29.3" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S5.28.28.28.28.1" class="ltx_td ltx_align_center ltx_border_r">52.43 <math id="S5.28.28.28.28.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.28.28.28.28.1.m1.1a"><mo id="S5.28.28.28.28.1.m1.1.1" xref="S5.28.28.28.28.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.28.28.28.28.1.m1.1b"><csymbol cd="latexml" id="S5.28.28.28.28.1.m1.1.1.cmml" xref="S5.28.28.28.28.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.28.28.28.28.1.m1.1c">\pm</annotation></semantics></math> 4.08</td>
<td id="S5.29.29.29.29.2" class="ltx_td ltx_align_center ltx_border_r">53.36 <math id="S5.29.29.29.29.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.29.29.29.29.2.m1.1a"><mo id="S5.29.29.29.29.2.m1.1.1" xref="S5.29.29.29.29.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.29.29.29.29.2.m1.1b"><csymbol cd="latexml" id="S5.29.29.29.29.2.m1.1.1.cmml" xref="S5.29.29.29.29.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.29.29.29.29.2.m1.1c">\pm</annotation></semantics></math> 3.27</td>
</tr>
<tr id="S5.31.31.31.31" class="ltx_tr">
<td id="S5.31.31.31.31.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25</td>
<td id="S5.30.30.30.30.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">58.96 <math id="S5.30.30.30.30.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.30.30.30.30.1.m1.1a"><mo id="S5.30.30.30.30.1.m1.1.1" xref="S5.30.30.30.30.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.30.30.30.30.1.m1.1b"><csymbol cd="latexml" id="S5.30.30.30.30.1.m1.1.1.cmml" xref="S5.30.30.30.30.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.30.30.30.30.1.m1.1c">\pm</annotation></semantics></math> 2.56</td>
<td id="S5.31.31.31.31.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">60.83 <math id="S5.31.31.31.31.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.31.31.31.31.2.m1.1a"><mo id="S5.31.31.31.31.2.m1.1.1" xref="S5.31.31.31.31.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.31.31.31.31.2.m1.1b"><csymbol cd="latexml" id="S5.31.31.31.31.2.m1.1.1.cmml" xref="S5.31.31.31.31.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.31.31.31.31.2.m1.1c">\pm</annotation></semantics></math> 0.53</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.33.33" class="ltx_p ltx_figure_panel">We also empirically observe that with increase in <math id="S5.32.32.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S5.32.32.m1.1a"><mi id="S5.32.32.m1.1.1" xref="S5.32.32.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.32.32.m1.1b"><ci id="S5.32.32.m1.1.1.cmml" xref="S5.32.32.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.32.32.m1.1c">\epsilon</annotation></semantics></math> , accuracy of the models also increases. This happens because as the value of <math id="S5.33.33.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S5.33.33.m2.1a"><mi id="S5.33.33.m2.1.1" xref="S5.33.33.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.33.33.m2.1b"><ci id="S5.33.33.m2.1.1.cmml" xref="S5.33.33.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.33.33.m2.1c">\epsilon</annotation></semantics></math> increases, privacy decreases with the addition of noise from a smaller range which results in smaller variance. Consequently, the accuracy of the model increases. Inherently, applying DP to deep learning yields loss of utility due to the addition of noise and clipping. We can also observe that the performance of federated language models still lies behind that of centralized ones.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S6" class="ltx_section ltx_figure_panel">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">Financial data is highly sensitive , hence the risks of collecting and sharing data can limit studies. Financial organizations work with a lot of confidential user data and therefore highly value protecting the data to retain the integrity of the user and we need to delve into research of private training of machine learning models to ensure this. During this study, we benchmark the utility of privacy models while attempting to preserve the performance of SOTA transformer models such as BERT and RoBERTa. Our empirical results show that the models show better performance with increasing <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S6.p1.1.m1.1a"><mi id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\varepsilon</annotation></semantics></math> as expected with the decrease in noise. The models come close to the performance of the baseline models near the higher <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S6.p1.2.m2.1a"><mi id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><ci id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">\varepsilon</annotation></semantics></math> values.The DP + FL shows a similar trend which showcases a greater protection feature without compromising the performance. As future work, we hope to improve our models further by hyper-parameter tuning, freezing partial layers of the NLP model and implementing focal loss on the unbalanced dataset to better the results. The complete code to this paper can be found here: <a href="here" title="" class="ltx_ref ltx_href">https://www.github.com/tiasa2/Privacy-enabled-Financial-Text-Classification-using-Differential-Privacy-and-Federated-Learning</a>.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/2976749.2978318" title="" class="ltx_ref ltx_href">Deep learning with
differential privacy</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alyafeai et al. (2020)</span>
<span class="ltx_bibblock">
Zaid Alyafeai, Maged Saeed AlShaibani, and Irfan Ahmad. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2007.04239" title="" class="ltx_ref ltx_href">A survey on transfer
learning in natural language processing</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anton et al. (2004)</span>
<span class="ltx_bibblock">
A.I. Anton, J.B. Earp, Qingfeng He, W. Stufflebeam, D. Bolchini, and C. Jensen.
2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MSECP.2004.1281243" title="" class="ltx_ref ltx_href">Financial privacy
policies and the need for standardization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Security Privacy</em>, 2(2):36–45.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Araci (2019)</span>
<span class="ltx_bibblock">
Dogu Araci. 2019.

</span>
<span class="ltx_bibblock">Finbert: Financial sentiment analysis with pre-trained language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.10063</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Basu et al. (2021)</span>
<span class="ltx_bibblock">
Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib Singh,
and Fatemehsadat Mireshghallah. 2021.

</span>
<span class="ltx_bibblock">Benchmarking differential privacy and federated learning for bert
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.13973</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beimel et al. (2014)</span>
<span class="ltx_bibblock">
Amos Beimel, Kobbi Nissim, and Uri Stemmer. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1407.2674" title="" class="ltx_ref ltx_href">Private learning and
sanitization: Pure vs. approximate differential privacy</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2008.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Theory and Applications of Models of Computation</em>, pages
1–19, Berlin, Heidelberg. Springer Berlin Heidelberg.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2011)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/1866739.1866758" title="" class="ltx_ref ltx_href">A firm foundation
for private data analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em>, 54(1):86–95.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork and Roth (2014)</span>
<span class="ltx_bibblock">
Cynthia Dwork and Aaron Roth. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1561/0400000042" title="" class="ltx_ref ltx_href">The algorithmic
foundations of differential privacy</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Found. Trends Theor. Comput. Sci.</em>, 9(3–4):211–407.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2019)</span>
<span class="ltx_bibblock">
Natasha Fernandes, Mark Dras, and Annabelle McIver. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-17138-4_6" title="" class="ltx_ref ltx_href">Generalised
differential privacy for text document processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Principles of Security and Trust</em>, Lecture Notes in Computer
Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics), pages 123–148, Germany.
Springer-VDI-Verlag GmbH &amp; Co. KG.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haeberlen et al. (2011)</span>
<span class="ltx_bibblock">
Andreas Haeberlen, Benjamin C. Pierce, and Arjun Narayan. 2011.

</span>
<span class="ltx_bibblock">Differential privacy under fire.

</span>
<span class="ltx_bibblock">SEC’11, page 33, USA. USENIX Association.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jana and Biemann (2021)</span>
<span class="ltx_bibblock">
Abhik Jana and Chris Biemann. 2021.

</span>
<span class="ltx_bibblock">An investigation towards differentially private sequence tagging in a
federated framework.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Privacy in Natural
Language Processing</em>, pages 30–35.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie
He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,
Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz
Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,
Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova,
Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U.
Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,
and Sen Zhao. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1912.04977" title="" class="ltx_ref ltx_href">Advances and open problems
in federated learning</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.09693" title="" class="ltx_ref ltx_href">A survey on federated
learning systems: Vision, hype and reality for data privacy and protection</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, and He Zhang. 2021.

</span>
<span class="ltx_bibblock">Federated learning meets natural language processing: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.12603</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly
optimized bert pretraining approach</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020.

</span>
<span class="ltx_bibblock">Finbert: A pre-trained financial language representation model for
financial text mining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, pages 4513–4519.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ly et al. (2020)</span>
<span class="ltx_bibblock">
Antoine Ly, Benno Uthayasooriyar, and Tingting Wang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2010.00462" title="" class="ltx_ref ltx_href">A survey on natural language
processing (nlp) and applications in insurance</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malo et al. (2014)</span>
<span class="ltx_bibblock">
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala.
2014.

</span>
<span class="ltx_bibblock">Good debt or bad debt: Detecting semantic orientations in economic
texts.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of the Association for Information Science and
Technology</em>, 65(4):782–796.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning and Schütze (1999)</span>
<span class="ltx_bibblock">
Christopher D. Manning and Hinrich Schütze. 1999.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Foundations of Statistical Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">MIT Press, Cambridge, MA, USA.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2016)</span>
<span class="ltx_bibblock">
H. B. McMahan, Eider Moore, D. Ramage, and B. A. Y. Arcas. 2016.

</span>
<span class="ltx_bibblock">Federated learning of deep networks using model averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1602.05629.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1602.05629" title="" class="ltx_ref ltx_href">Communication-efficient
learning of deep networks from decentralized data</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2018)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1710.06963" title="" class="ltx_ref ltx_href">Learning differentially
private recurrent language models</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishev et al. (2020)</span>
<span class="ltx_bibblock">
Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lubomir T. Chitkushev, and
Dimitar Trajanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ACCESS.2020.3009626" title="" class="ltx_ref ltx_href">Evaluation of
sentiment analysis in finance: From lexicons to transformers</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8:131662–131682.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panchal (2020)</span>
<span class="ltx_bibblock">
Kunjal Panchal. 2020.

</span>
<span class="ltx_bibblock">Differential privacy and natural language processing to generate
contextually similar decoy messages in honey encryption scheme.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.15985</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Privacy (2017)</span>
<span class="ltx_bibblock">
Apple Differential Privacy. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://machinelearning.apple.com/research/learning-with-privacy-at-scale" title="" class="ltx_ref ltx_href">Learning with privacy at scale</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Priyanshu and Naidu (2021)</span>
<span class="ltx_bibblock">
Aman Priyanshu and Rakshit Naidu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.01864" title="" class="ltx_ref ltx_href">Fedpandemic: A cross-device
federated learning approach towards elementary prognosis of diseases during a
pandemic</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2021)</span>
<span class="ltx_bibblock">
Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc
Najork. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.07504" title="" class="ltx_ref ltx_href">Privacy-adaptive bert for
natural language understanding</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vicari and Gaspari (2020)</span>
<span class="ltx_bibblock">
Mattia Vicari and Mauro Gaspari. 2020.

</span>
<span class="ltx_bibblock">Analysis of news sentiments using natural language processing and
deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Ai &amp; Society</em>, pages 1–7.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al. (2020)</span>
<span class="ltx_bibblock">
Frank Xing, Lorenzo Malandri, Yue Zhang, and Erik Cambria. 2020.

</span>
<span class="ltx_bibblock">Financial sentiment analysis: an investigation into common mistakes
and silver bullets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on
Computational Linguistics</em>, pages 978–987.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.04885" title="" class="ltx_ref ltx_href">Federated machine learning:
Concept and applications</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays. 2018.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Lei Yu, Ling Liu, Calton Pu, Mehmet Emre Gursoy, and Stacey Truex. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/sp.2019.00019" title="" class="ltx_ref ltx_href">Differentially private
model publishing for deep learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Xinghua Zhu, Jianzong Wang, Zhenhou Hong, and Jing Xiao. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.55" title="" class="ltx_ref ltx_href">Empirical
studies of institutional federated learning for natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 625–634, Online. Association for Computational
Linguistics.

</span>
</li>
</ul>
</section>
</section>
</div>
</div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.01642" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.01643" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.01643">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.01643" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.01644" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 19:40:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
