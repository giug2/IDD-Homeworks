<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?</title>
<!--Generated on Mon Sep 16 21:16:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10775v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S1" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S1.SS1" title="In 1 Introduction ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Contributions of This Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S2" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Background Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS1" title="In 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS2" title="In 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Augmentations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="In 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Statistical Tests</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS4" title="In 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Human Labeling Procedure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>The Image Recognition Under Occlusion (IRUO) Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Human Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS1" title="In 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>The Human Testing Subset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS2" title="In 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Testing Protocol</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS1" title="In 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Which models are most accurate under occlusion?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="In 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Is synthetic occlusion a good proxy for real occlusion?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="In 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>How occlusion-robust are models compared to humans?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS4" title="In 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Further analysis: are models robust to diffuse occlusion?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S7" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A1" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Full result tables for model evaluation on real and synthetic occlusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A2" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Details of training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A3" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional details of human data collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A4" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Calculation of Friedman Q-value</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A5" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Details of synthetic data generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A6" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Details of diffuse synthetic data generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A7" title="In Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Additional Vision Transformer attention maps</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\AtBeginShipout</span><span class="ltx_ERROR undefined" id="p1.2">\AtBeginShipoutAddToBox</span><svg class="ltx_picture" height="19.23" id="p1.pic1" overflow="visible" version="1.1" width="251.35"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.23) matrix(1 0 0 -1 0 0) translate(-299.33,0) translate(0,-1060.86)"><path d="M 299.6 1061.13 h 250.8 v 18.68 h -250.8 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 304.21 1065.74)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="241.96"><span class="ltx_text" id="p1.pic1.1.1.1.1.1">APPROVED FOR PUBLIC RELEASE</span></foreignobject></g></g></svg><svg class="ltx_picture" height="19.23" id="p1.pic2" overflow="visible" version="1.1" width="251.35"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.23) matrix(1 0 0 -1 0 0) translate(-299.33,0) translate(0,-19.91)"><path d="M 299.6 20.19 h 250.8 v 18.68 h -250.8 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 304.21 24.8)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="241.96"><span class="ltx_text" id="p1.pic2.1.1.1.1.1">APPROVED FOR PUBLIC RELEASE</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">[1]<span class="ltx_ERROR undefined" id="p2.1.1">\fnm</span>Kaleb <span class="ltx_ERROR undefined" id="p2.1.2">\sur</span>Kassaw</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">1]<span class="ltx_ERROR undefined" id="p3.1.1">\orgdiv</span>Department of Electrical and Computer Engineering, <span class="ltx_ERROR undefined" id="p3.1.2">\orgname</span>Duke University, <span class="ltx_ERROR undefined" id="p3.1.3">\orgaddress</span><span class="ltx_ERROR undefined" id="p3.1.4">\city</span>Durham, <span class="ltx_ERROR undefined" id="p3.1.5">\state</span>North Carolina, <span class="ltx_ERROR undefined" id="p3.1.6">\postcode</span>27708, <span class="ltx_ERROR undefined" id="p3.1.7">\country</span>United States</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">2]<span class="ltx_ERROR undefined" id="p4.1.1">\orgdiv</span>Department of Electrical Engineering and Computer Science, <span class="ltx_ERROR undefined" id="p4.1.2">\orgname</span>University of Missouri, <span class="ltx_ERROR undefined" id="p4.1.3">\orgaddress</span><span class="ltx_ERROR undefined" id="p4.1.4">\city</span>Columbia, <span class="ltx_ERROR undefined" id="p4.1.5">\state</span>Missouri, <span class="ltx_ERROR undefined" id="p4.1.6">\postcode</span>65211, <span class="ltx_ERROR undefined" id="p4.1.7">\country</span>United States</p>
</div>
<h1 class="ltx_title ltx_title_document">Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kaleb.kassaw@duke.edu">kaleb.kassaw@duke.edu</a>
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\fnm</span>Francesco <span class="ltx_ERROR undefined" id="id2.2.id2">\sur</span>Luzi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:francesco.luzi@duke.edu">francesco.luzi@duke.edu</a>
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id3.1.id1">\fnm</span>Leslie M. <span class="ltx_ERROR undefined" id="id4.2.id2">\sur</span>Collins
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:leslie.collins@duke.edu">leslie.collins@duke.edu</a>
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id5.1.id1">\fnm</span>Jordan M. <span class="ltx_ERROR undefined" id="id6.2.id2">\sur</span>Malof
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jmdrp@missouri.edu">jmdrp@missouri.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Image classification models, including convolutional neural networks (CNNs), perform well on a variety of classification tasks but struggle under conditions of partial occlusion, i.e., conditions in which objects are partially covered from the view of a camera. Methods to improve performance under occlusion, including data augmentation, part-based clustering, and more inherently robust architectures, including Vision Transformer (ViT) models, have, to some extent, been evaluated on their ability to classify objects under partial occlusion. However, evaluations of these methods have largely relied on images containing artificial occlusion, which are typically computer-generated and therefore inexpensive to label. Additionally, methods are rarely compared against each other, and many methods are compared against early, now outdated, deep learning models. We contribute the Image Recognition Under Occlusion (IRUO) dataset, based on the recently developed Occluded Video Instance Segmentation (OVIS) dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>). IRUO utilizes real-world and artificially occluded images to test and benchmark leading methods‚Äô robustness to partial occlusion in visual recognition tasks. In addition, we contribute the design and results of a human study using images from IRUO that evaluates human classification performance at multiple levels and types of occlusion, including diffuse occlusion. We find that modern CNN-based models show improved recognition accuracy on occluded images compared to earlier CNN-based models, and ViT-based models are more accurate than CNN-based models on occluded images, performing only modestly worse than human accuracy. We also find that certain types of occlusion, including diffuse occlusion, where relevant objects are seen through ‚Äùholes‚Äù in occluders such as fences and leaves, can greatly reduce the accuracy of deep recognition models as compared to humans, especially those with CNN backbones.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>occlusion, computer vision, machine learning, deep learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep learning models, particularly deep neural networks (DNNs), have demonstrated tremendous success on visual recognition tasks, even matching or exceeding human performance on some benchmarks (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>). However, one important real-world condition in which DNNs still struggle is occlusion, wherein the target object is partially obscured, or occluded, by other objects in the scene. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates a scenario where a target object, of the class <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">cat</span>, is occluded to varying degrees by scene content, including other objects of the target class. In this work we aim to address two important and fundamental open questions regarding image recognition under occlusion.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.F1.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.F1.3.3.3">
<td class="ltx_td ltx_align_center" id="S1.F1.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S1.F1.1.1.1.1.g1" src="extracted/5858817/181069.jpg" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S1.F1.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S1.F1.2.2.2.2.g1" src="extracted/5858817/333772.jpg" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S1.F1.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="120" id="S1.F1.3.3.3.3.g1" src="extracted/5858817/259819.jpg" width="180"/></td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.3.4.1">
<td class="ltx_td ltx_align_center" id="S1.F1.3.3.4.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S1.F1.3.3.4.1.2">(b)</td>
<td class="ltx_td ltx_align_center" id="S1.F1.3.3.4.1.3">(c)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Images from Microsoft‚Äôs COCO dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib13" title="">13</a>]</cite>), a popular dataset for visual recognition tasks including object detection, demonstrating partial object occlusion. In each of these images, the object class ‚Äùcat‚Äù is partially occluded by (a) a suitcase, among the object classes listed in COCO, (b) another cat, and (c) a boot, a class of object not among the object clases listed in COCO</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_bold" id="S1.p2.1.1">(i) Which existing models are most accurate on occluded imagery?</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The answer to this question is useful for practitioners facing occlusion in imagery and would provide guidance for future research. Several recent publications have proposed occlusion-robust recognition models (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S2" title="2 Related Work ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">2</span></a>), and demonstrated that their proposed approaches improve visual recognition accuracy over conventional DNNs. However, these comparisons, e.g., in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>, suffer from several limitations, e.g., comparisons to only early deep learning models such as AlexNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib12" title="">12</a>]</cite>) and VGG (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>) and other models expressly designed for occluded scenes, datasets containing few classes (e.g., vehicles), and datasets with few examples of real-world occlusion. This is an especially important set of comparisons to make, as deep learning has advanced significantly since these early models, greatly improving top-1 classification accuracy on large-scale datasets such as ImageNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib4" title="">4</a>]</cite>). Therefore, to our knowledge, no existing work has performed a rigorous empirical comparison ‚Äì or benchmark ‚Äì of both existing conventional DNNs and/or recent occlusion-robust models.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">(ii) Are existing models robust to occlusion? Human and model comparison</span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">For a model to be ‚Äùrobust‚Äù to occlusion, we mean that it does not degrade much faster than necessary (e.g., compared to an optimal model) in the presence of occlusion. Numerous publications have demonstrated that DNN recognition accuracy degrades in the presence of occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>]</cite>). Note however that the accuracy of any recognition model ‚Äì even an optimal model ‚Äì might <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">necessarily</span> degrade in the presence of occlusion, due to loss of image information. Therefore it is unclear whether or not the accuracy degradation observed in existing research can be mitigated through improved modeling and, therefore, whether existing models are robust to occlusion.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To evaluate whether a particular model is occlusion-robust, one must compare against an optimal model, yet estimating such a model‚Äôs performance is difficult. Human visual recognition accuracy is often used as such an estimator, and one recent publication has compared human and DNN-based recognition accuracy as occlusion increases (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>]</cite>). This study concluded that DNN-based models do indeed degrade more quickly than human observers, and it provides one occlusion-robustness estimate (i.e., how much performance do we stand to gain with research investment). Although this work provides valuable insights, the test dataset used for this study (based on the <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">VehicleOcclusion</span> dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib23" title="">23</a>]</cite>) suffers from several limitations that undermine the ability to draw generalizable conclusions. First, the testing dataset is limited in both size (e.g., only 500 test images) and variability (e.g., only vehicles classes). Second, the authors predominantly use synthetic occlusions consisting of highly unrealistic objects and scenarios, and it is unclear if such scenarios are a good proxy for real-world occlusions. Lastly, the work utilized relatively old models (e.g., AlexNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib12" title="">12</a>]</cite>), VGG (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>) that exhibit inferior performance to modern models, and as we confirm in this work, also yield inferior performance under occlusion.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions of This Work</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">In this work we attempt to provide comprehensive answers to the aforementioned questions. First, we curated a large and diverse dataset of real-world occlusions termed the Image Recognition Under Occlusion (IRUO) dataset, which we built upon the Occluded Video Instance Segmentation (OVIS) dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>). The IRUO contains 23 classes of objects and 88 thousand images and addresses several limitations associated with previous datasets, such as their limited size, limited target object diversity, or their lack of real-world occlusion. Using IRUO, we address question (i) by comparing state-of-the-art representatives from each of three relevant model types: convolutional models (e.g., ResNeXt (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>]</cite>)), Vision Transformers (e.g., Swin (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>)), and models that are especially designed to be occlusion robust (e.g., CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib11" title="">11</a>]</cite>)). We also leverage our benchmark results to examine whether the usage of synthetically occluded imagery (e.g., in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F8" title="Figure 8 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">8</span></a>) is an accurate proxy for real-world occlusion. Synthetic occlusions are much easier to generate and control, and therefore a positive answer to this question would empower future investigation of occlusion, as well as help validate historical studies that made use of synthetic occlusion. To address question (ii) we obtain classification predictions from twenty human observers on our IRUO dataset, and compare their accuracy under occlusion against all of the data-driven models in our study.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Contributions of this work can be summarized as follows.</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The Image Recognition Under Occlusion (IRUO) Dataset: the first large-scale public benchmark for image recognition under occlusion). This dataset is built upon the OVIS dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>) and comprises unprecedented size and diversity.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A rigorous comparison of state-of-the-art image classification models under varying levels of occlusion. This includes convolutional models, Vision Transformers, and models specifically designed for occlusion.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A rigorous comparison of model and human recognition accuracy on our benchmark, making it possible to determine whether, and to what extent, existing DNN-based models can be made more robust to occlusion.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">An investigation of whether synthetically occluded imagery (e.g., see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F8" title="Figure 8 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">8</span></a>) is an accurate proxy for real-world occlusion.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">To support future research, we publish all of our datasets, models, and results.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/kalebkassaw/iruo</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Existing occlusion-robust methods.</span> Recent methods have been proposed to address the decrease in model accuracy under conditions of occlusion. These methods include data augmentation (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>), part-based modeling techniques (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib27" title="">27</a>]</cite>), and development of architectures inherently more robust to partial occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>]</cite>). Several methods of data augmentation are used to encourage models to focus on image parts. Mixup and CutMix are augmentations that replace single training images with combinations of multiple images, through weighted averages over all pixels (Mixup) or cut-and-paste image areas (CutMix). Cutout (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib6" title="">6</a>]</cite>), a closely related augmentation to CutMix, does not add pieces of a second image; instead, cutouts are replaced with gray boxes. The authors in each paper hypothesize that, by forcing deep networks to make inferences on parts, they can confer part-based knowledge to these models. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>, in proposing TransMix, an extension of CutMix that re-weights proportions of images based on relative transformer attention values, the authors hypothesize that this part-based knowledge is linked to improved performance under occlusion. Data augmentation techniques, including CutMix
and TransMix, are hypothesized to improve recognition accuracy on occluded images as they splice parts of images and encourage models to learn labels that are proportional to image area. Data augmentation has also been proposed at the convolutional deep feature level to improve recognition accuracy under occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>); this method collects differences in deep vectors between unoccluded and occluded objects to confer occlusion robustness to models. Part-based modeling techniques have been used to improve performance under some occluded scenes while preserving the convolutional backbones of many deep learning models; CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>), TDAPNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>]</cite>), and TDMPNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib25" title="">25</a>]</cite>) cluster deep feature representations in high-dimensional space and use various methods to assign these features to either object classes or occluders, explicitly for the purpose of robustness to occlusion. Although not explicitly designed for the purpose, the Vision Transformer (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>]</cite>) has been shown to perform well under conditions of constant-valued occlusions coinciding with input image patches (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>]</cite>). We include CompositionalNet in our experiments, in addition to Deep Feature Augmentation, as these methods have been shown to improve performance under some conditions of occlusion, compared to VGG (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>). We additionally include evaluations of CutMix and Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>), as they are widely deployed in modern models (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>) to improve overall recognition accuracy and suggested to improve accuracy under conditions of occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Benchmarking human and algorithm performance under occlusion.</span> Human visual recognition performance on occlusion has been evaluated in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>]</cite>, where accuracy of humans on a five-class vehicle dataset with artificial occlusion is compared to the accuracy of deep learning models including AlexNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib12" title="">12</a>]</cite>) and VGG (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>). This study suggests that human performance is not attained by deep learning models on images with objects that are highly occluded, as human performance on highly occluded objects is significantly higher than that of any model tested. However, this study has several limitations; the dataset contains only five vehicle classes, compares against aging models instead of more modern models of the time, e.g., ResNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib9" title="">9</a>]</cite>), and it is not publicly available.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Recent research has suggested that Vision Transformer-based models may perform better under conditions such as mask occlusion and adversarial patches, suggesting that newer attention-based deep learning models may demonstrate higher performance than convolutional neural networks under conditions of real-world occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>]</cite>). We include these algorithms in a benchmark of visual recognition accuracy under occlusion, in addition to comparing these results with human accuracy, for the first time.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background Methods</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Models</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We evaluate several state-of-the-art vision models from three relevant categories: convolutional models, transformers, and models designed specifically for occlusion robustness. Among occlusion-specific models, we include the CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>), because it was recently shown to outperform other such models (e.g., TDAPNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>]</cite>), and ‚Äùtwo-stage voting‚Äù (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>]</cite>)). Among convolutional models we include VGG (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>) for continuity with prior work, as VGG was included in several previous benchmarks (e.g., in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>]</cite>). We also include state-of-the-art convolutional models such as ResNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib9" title="">9</a>]</cite>) and ResNeXt (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>]</cite>). Among state-of-the-art transformer-based models, we include the Vision Transformer (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>]</cite>), DeiT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>]</cite>), and Swin (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Augmentations</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We also propose the testing of various augmentation and model regularization methods, including Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>), CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>), and Deep Feature Augmentation (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>). Mixup and CutMix are state-of-the-art augmentation methods that are commonly used in leading deep learning models regardless of the presence of occlusion in the images they classify, and their usage is hypothesized to improve model robustness to partial occlusion by conferring knowledge of object parts to models (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>). Deep Feature Augmentation is explicitly designed to improve model performance in scenes of partial occlusion (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>) over an otherwise equivalent CNN-based model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Statistical Tests</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.12"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.12.1">Friedman Test (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>)</span>. To determine whether or not models are ranked differently by real versus synthetic occlusions (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a>), we use the Friedman test in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>. This test assumes that we have a collection of <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> objects to rank, and that we have <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_n</annotation></semantics></math> evaluation metrics, or ‚Äùjudges‚Äù, for ranking the objects. The objective of the test is to determine whether the judges produce random rankings. Specifically, the null hypothesis of this test is that the rankings of each judge are produced by randomly drawing each of the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_k</annotation></semantics></math> objects without replacement, and then assigning their rank based upon the order in which they were drawn. Rejection of the null hypothesis then suggests that the rankings of each judge are non-random so that some objects are ranked consistently above others across the judges. To test this hypothesis, the Friedman test prescribes the calculation of a <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_Q</annotation></semantics></math> statistic based upon the observed <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_k</annotation></semantics></math> rankings of each of the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_n</annotation></semantics></math> available judges. For a sufficiently large <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><mi id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><ci id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_k</annotation></semantics></math>, the <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.1"><semantics id="S3.SS3.p1.8.m8.1a"><mi id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><ci id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.1d">italic_Q</annotation></semantics></math> statistic is drawn from a <math alttext="\chi^{2}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.1"><semantics id="S3.SS3.p1.9.m9.1a"><msup id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml"><mi id="S3.SS3.p1.9.m9.1.1.2" xref="S3.SS3.p1.9.m9.1.1.2.cmml">œá</mi><mn id="S3.SS3.p1.9.m9.1.1.3" xref="S3.SS3.p1.9.m9.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><apply id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS3.p1.9.m9.1.1.2.cmml" xref="S3.SS3.p1.9.m9.1.1.2">ùúí</ci><cn id="S3.SS3.p1.9.m9.1.1.3.cmml" type="integer" xref="S3.SS3.p1.9.m9.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">\chi^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.1d">italic_œá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> distribution with <math alttext="k-1" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m10.1"><semantics id="S3.SS3.p1.10.m10.1a"><mrow id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml"><mi id="S3.SS3.p1.10.m10.1.1.2" xref="S3.SS3.p1.10.m10.1.1.2.cmml">k</mi><mo id="S3.SS3.p1.10.m10.1.1.1" xref="S3.SS3.p1.10.m10.1.1.1.cmml">‚àí</mo><mn id="S3.SS3.p1.10.m10.1.1.3" xref="S3.SS3.p1.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><apply id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1"><minus id="S3.SS3.p1.10.m10.1.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1.1"></minus><ci id="S3.SS3.p1.10.m10.1.1.2.cmml" xref="S3.SS3.p1.10.m10.1.1.2">ùëò</ci><cn id="S3.SS3.p1.10.m10.1.1.3.cmml" type="integer" xref="S3.SS3.p1.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">k-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m10.1d">italic_k - 1</annotation></semantics></math> degrees of freedom, and we reject the null hypothesis if Q is above the 95th percentile of this <math alttext="\chi^{2}" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m11.1"><semantics id="S3.SS3.p1.11.m11.1a"><msup id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml"><mi id="S3.SS3.p1.11.m11.1.1.2" xref="S3.SS3.p1.11.m11.1.1.2.cmml">œá</mi><mn id="S3.SS3.p1.11.m11.1.1.3" xref="S3.SS3.p1.11.m11.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><apply id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">superscript</csymbol><ci id="S3.SS3.p1.11.m11.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1.2">ùúí</ci><cn id="S3.SS3.p1.11.m11.1.1.3.cmml" type="integer" xref="S3.SS3.p1.11.m11.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">\chi^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m11.1d">italic_œá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> distribution (equivalent to <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S3.SS3.p1.12.m12.1"><semantics id="S3.SS3.p1.12.m12.1a"><mrow id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml"><mi id="S3.SS3.p1.12.m12.1.1.2" xref="S3.SS3.p1.12.m12.1.1.2.cmml">p</mi><mo id="S3.SS3.p1.12.m12.1.1.1" xref="S3.SS3.p1.12.m12.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p1.12.m12.1.1.3" xref="S3.SS3.p1.12.m12.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.1b"><apply id="S3.SS3.p1.12.m12.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1"><lt id="S3.SS3.p1.12.m12.1.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1.1"></lt><ci id="S3.SS3.p1.12.m12.1.1.2.cmml" xref="S3.SS3.p1.12.m12.1.1.2">ùëù</ci><cn id="S3.SS3.p1.12.m12.1.1.3.cmml" type="float" xref="S3.SS3.p1.12.m12.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.12.m12.1d">italic_p &lt; 0.05</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Tukey‚Äôs Rule (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib22" title="">22</a>]</cite>)</span>. In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a> we estimate human recognition accuracy in the presence of occlusion, and we use Tukey‚Äôs rule (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib22" title="">22</a>]</cite>) to remove unrepresentative human subjects (i.e., outliers) from consideration. Tukey‚Äôs rule prescribes that data is drawn from the same underlying distribution with a common mean and variance. By this rule, data points in a set falling 1.5 interquartile ranges outside the interquartile range are classified as outliers, and the removal of these points for calculating summary statistics is justified.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Human Labeling Procedure</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We employ a method of human labeling known as <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">Scalable Multi-Label Annotation</span>, first used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib5" title="">5</a>]</cite>. This method of data collection attempts to minimize human error due to factors other than strict identification of classes. For a (typically large) set of classes within a dataset, Scalable Multi-Label Annotation asks a series of informative categorical questions to narrow human responses to a much smaller set of classes within the dataset. These categorical questions can be broad at first, e.g., ‚ÄùIs an animal present?‚Äù. A sample hierarchy reflecting such an iterative procedure in this study is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F5" title="Figure 5 ‚Ä£ 5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>. This iterative collection procedure is designed to remove the need for 1-of-n-class labeling (i.e., asking whether or not each class in a dataset is present for every image) while also addressing humans‚Äô limited memory (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib5" title="">5</a>]</cite>). DNN-based models, by nature of utilizing large computational resources, are capable of remembering characteristics distinctive to each class; humans, by comparison, perform best with few-class, hierarchical prompts (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib5" title="">5</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In addition, this hierarchical approach enables the analysis of whether or not humans (and models) are selecting classes that are relatively close (e.g., labeling a tiger in place of a cat) or relatively distant (e.g., labeling a tiger in place of an airplane), as categorical responses can be used in place of fine labels for certain analyses. This allows us to compare the types of errors made by both humans and models beyond simply comparing top-1 accuracy scores.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>The Image Recognition Under Occlusion (IRUO) Dataset</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We build the IRUO dataset based upon the Occluded Video Instance Segmentation (OVIS) dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>), first used in video segmentation problems. The OVIS dataset is comprised of a training set of approximately 3.5k objects in 600 videos, and validation and test sets of 750 objects in 150 videos each. Each of these objects is labeled with a corresponding occlusion level, (0) no occlusion; (1) some occlusion, in which up to 50 percent of the object is hidden from view; and (2) severe occlusion, in which more than 50 percent of the object is hidden from view. To create IRUO, we needed to curate the OVIS video dataset so that it is appropriate for the task of image classification. We chose to build an image classification dataset (as opposed to segmentation or detection) because, to our knowledge, all occlusion-robust image recognition methods (e.g., CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>), TDAPNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib24" title="">24</a>]</cite>)) are suitable only for the classification task. This was done by cropping individual target objects from OVIS video frames using the bounding boxes provided with OVIS for each target instance. To account for minor bounding box label errors and preserve object aspect ratios, we then expand the shorter dimension from the center to match the longer dimension, expand all dimensions 20 pixels from the center point, and use the subsequent crops to generate our image classification dataset. Sample images are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4.F2" title="Figure 2 ‚Ä£ 4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">2</span></a>, where a parrot is shown at each level of occlusion defined in IRUO. These occlusion levels are exactly the same as those defined in the OVIS dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F2.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F2.3.3.3">
<td class="ltx_td ltx_align_center" id="S4.F2.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S4.F2.1.1.1.1.g1" src="extracted/5858817/bird_0.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S4.F2.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S4.F2.2.2.2.2.g1" src="extracted/5858817/bird_1.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S4.F2.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S4.F2.3.3.3.3.g1" src="extracted/5858817/bird_2.jpg" width="162"/></td>
</tr>
<tr class="ltx_tr" id="S4.F2.3.3.4.1">
<td class="ltx_td ltx_align_center" id="S4.F2.3.3.4.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S4.F2.3.3.4.1.2">(b)</td>
<td class="ltx_td ltx_align_center" id="S4.F2.3.3.4.1.3">(c)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Images showing various levels of real occlusion, increasing from left to right, in the Image Recognition Under Occlusion (IRUO) dataset. We propose to use images including these for evaluation of models‚Äô robustness to occlusion in recognition tasks, owing to realistic occlusions and a wide diversity of scenes. Occlusion levels: (a) 0: no occlusion, (b) 1: some occlusion, in which up to 50 percent of the object is hidden from view, (c) 2: severe occlusion, in which more than 50 percent of an object is hidden from view</figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">There exists an official, author-defined train-test split for OVIS; however, labels for the test partition are unavailable due to the ongoing OVIS competition (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>). Therefore, in this work we partitioned the official OVIS training set into training and test sets for IRUO. We then remove images in the <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">boat</span> and <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">vehicle</span> classes, as there are very few examples of each at occlusion level 0, leaving 23 remaining classes of objects. For each of these 23 classes of objects, we split the set of <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">videos</span> (not images) such that approximately 70 percent of instances of the class appear in the training data and 30 percent appear in the test data. We ensure that, per-class, no instances of objects in the training set appear in the test set, to prevent positively-biased estimates of model accuracy.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">From the base IRUO dataset, we derive three additional datasets: IRUO-HTS, IRUO-Synthetic, and IRUO-Diffuse. IRUO-HTS refers to the subset of IRUO utilized to estimate human recognition accuracy under occlusion in <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>. We use IRUO-Synthetic to examine whether synthetic occlusions are a good proxy for real-world occlusions, in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a>. We utilize IRUO-Diffuse to examine the relative accuracy of human and DNN-based models to diffuse occluders, a particular type of occlusion, in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS4" title="6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.4</span></a>. We provide further details about the derivation of each of these datasets in the sections associated with each one; however, a summary is provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4.F3" title="Figure 3 ‚Ä£ 4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3</span></a>. We provide a quantitative summary of all four IRUO partitions in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4.T1" title="Table 1 ‚Ä£ 4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S4.F3.1.g1" src="extracted/5858817/iruo-generation-diagram-2.png" width="550"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Diagram showing the data generation process of all dataset partitions included in IRUO. A description of each partition is found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4.T1" title="Table 1 ‚Ä£ 4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">1</span></a></figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1" style="width:86.7pt;">Name</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.2.1">
<span class="ltx_p" id="S4.T1.1.1.1.2.1.1" style="width:43.4pt;">Training images</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.3.1">
<span class="ltx_p" id="S4.T1.1.1.1.3.1.1" style="width:43.4pt;">Test images</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.4.1">
<span class="ltx_p" id="S4.T1.1.1.1.4.1.1" style="width:216.8pt;">Description</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.1.1">
<span class="ltx_p" id="S4.T1.1.2.1.1.1.1" style="width:86.7pt;">IRUO-Base</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.2.1">
<span class="ltx_p" id="S4.T1.1.2.1.2.1.1" style="width:43.4pt;">61.4k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.3.1">
<span class="ltx_p" id="S4.T1.1.2.1.3.1.1" style="width:43.4pt;">26.4k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.4.1">
<span class="ltx_p" id="S4.T1.1.2.1.4.1.1" style="width:216.8pt;">Original partition of IRUO, created using object instances from OVIS (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.1.1">
<span class="ltx_p" id="S4.T1.1.3.2.1.1.1" style="width:86.7pt;">IRUO-HTS</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.2.1">
<span class="ltx_p" id="S4.T1.1.3.2.2.1.1" style="width:43.4pt;">0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.3.1">
<span class="ltx_p" id="S4.T1.1.3.2.3.1.1" style="width:43.4pt;">552</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.4.1">
<span class="ltx_p" id="S4.T1.1.3.2.4.1.1" style="width:216.8pt;">Subsampled <span class="ltx_text ltx_font_italic" id="S4.T1.1.3.2.4.1.1.1">human testing subset</span> version of IRUO, with blur and small-target filters, 8 images per class per occlusion level.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.1.1">
<span class="ltx_p" id="S4.T1.1.4.3.1.1.1" style="width:86.7pt;">IRUO-Synthetic</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.2.1">
<span class="ltx_p" id="S4.T1.1.4.3.2.1.1" style="width:43.4pt;">64.0k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.3.1">
<span class="ltx_p" id="S4.T1.1.4.3.3.1.1" style="width:43.4pt;">24.5k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.4.1">
<span class="ltx_p" id="S4.T1.1.4.3.4.1.1" style="width:216.8pt;">Dataset partition generated from unoccluded images in IRUO. Contains synthetic occluders such as noise, textures, solid boxes, and superimposed objects.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.1.1">
<span class="ltx_p" id="S4.T1.1.5.4.1.1.1" style="width:86.7pt;">IRUO-Diffuse</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.2.1">
<span class="ltx_p" id="S4.T1.1.5.4.2.1.1" style="width:43.4pt;">114.9k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.3.1">
<span class="ltx_p" id="S4.T1.1.5.4.3.1.1" style="width:43.4pt;">68.5k</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.4.1">
<span class="ltx_p" id="S4.T1.1.5.4.4.1.1" style="width:216.8pt;">Dataset partition of IRUO generated from unoccluded images in IRUO. Contains synthetic occluders such as regular lines, grids, diagonal grids, and small squares at 1, 2, 4, 8, and 16px size.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Details of IRUO dataset partitions used in this study</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Human Study</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section we describe the process for estimating human-level recognition accuracy under varying levels of occlusion. To do this, we recruited <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">20</span> human subjects to classify a curated subset of images from the testing set of our IRUO dataset, termed the Human Testing Subset (IRUO-HTS). Below we describe how we designed and utilized IRUO-HTS, as well as the testing protocol used to collect the subjects‚Äô predictions on IRUO-HTS.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>The Human Testing Subset</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To mitigate the risks of subject fatigue, each human subject was only asked to classify 200 test images which was chosen to require about 1 hour to complete using our testing protocol<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The human subjects research described in this publication is pursuant to Duke University Campus IRB protocol #2023-0336.</span></span></span>. These 200 images were sampled from a curated subset of 552 images from the full IRUO testing dataset, which we term the ‚ÄùHuman Testing Subset‚Äù (IRUO-HTS). To ensure that IRUO-HTS was representative of the IRUO test dataset, we randomly sampled 8 images for each of the 23 target object classes, and did so for each of 3 occlusion levels, resulting in a total of 552 images. We sub-sample images from IRUO-HTS (e.g., instead of the full IRUO dataset) to ensure that multiple subjects classify each image, on average. This has the drawback of limiting the total number of unique images in the test set, while allowing us to identify and remove human subjects that consistently performed worse than others (i.e., outliers). Because each test image varies in its difficulty, some subjects may perform worse simply because their subset of testing images is more difficult. By enforcing some overlap in test images, we can compare the accuracy of each subject to several other subjects over the same images. Using our sampling procedure on IRUO-HTS, and our total of 20 human subjects, each image was classified by about 7 subjects on average.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We choose to include 8 samples per class per occlusion level to balance variation between humans and variation between images, i.e., too many images may result in few human observers seeing each image, and too few images may risk not accurately reflecting human accuracy. We additionally isolate the effect of occlusion from other factors, including small imagery and blur, by applying two additional selection criteria for images in this subset. We approximate blur using the variance of the Laplacian operator, as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib1" title="">1</a>]</cite>, and we approximate image size by using the number of cropped pixels on image targets. We choose values for minimum Laplacian variance and image size that give us approximately 90% accuracy on top models at occlusion level 0; in our experiments, these values are 20 and 10,000, respectively. Images filtered using this blur algorithm are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F4" title="Figure 4 ‚Ä£ 5.1 The Human Testing Subset ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.F4.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F4.2.2.2">
<td class="ltx_td ltx_align_center" id="S5.F4.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="268" id="S5.F4.1.1.1.1.g1" src="extracted/5858817/blur_a1.png" width="269"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="268" id="S5.F4.2.2.2.2.g1" src="extracted/5858817/blur_a2.png" width="269"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.4.4.5.1">
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.5.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.5.1.2">(b)</td>
</tr>
<tr class="ltx_tr" id="S5.F4.4.4.4">
<td class="ltx_td ltx_align_center" id="S5.F4.3.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="271" id="S5.F4.3.3.3.1.g1" src="extracted/5858817/blur_f1.jpg" width="269"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="268" id="S5.F4.4.4.4.2.g1" src="extracted/5858817/blur_f2.jpg" width="269"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.4.4.6.2">
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.6.2.1">(c)</td>
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.6.2.2">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top row, (a), (b): sample images kept by blur filter algorithm; bottom row, (c), (d): sample images rejected by blur filter algorithm. The blur filter parameters used are a minimum Laplacian variance of 20 and minimum image size of 10,000 pixels</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Testing Protocol</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We employ a method of human labeling known as <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Scalable Multi-Label Annotation</span>, first used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib5" title="">5</a>]</cite>. This method of data collection attempts to minimize human error due to factors other than strict identification of classes. As described in detail in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS4" title="3.4 Human Labeling Procedure ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.4</span></a>, this collection procedure is designed to address the difference between model memorization of classes and humans‚Äô comparatively limited memory by relying on hierarchical prompts. We use the hierarchy in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F5" title="Figure 5 ‚Ä£ 5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>, derived from WordNet, a commonly-used lexical database of the English language (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib15" title="">15</a>]</cite>). This hierarchy gives all classes in IRUO a hierarchy ‚Äùlevel,‚Äù i.e., the number of clicks required to obtain this class in the human study program. For example, the class ‚Äùfish‚Äù does not appear at level 1 but appears at levels 2-5, and the class ‚Äùdog‚Äù does not appear at levels 1-3 but appears at levels 4-5.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.F5.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F5.1.1.1">
<td class="ltx_td ltx_align_center" id="S5.F5.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="279" id="S5.F5.1.1.1.1.g1" src="extracted/5858817/ovis-hierarchy.png" width="538"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Hierarchy of classes in IRUO. In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F11" title="Figure 11 ‚Ä£ 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">11</span></a>, results are reported according to <span class="ltx_text ltx_font_italic" id="S5.F5.3.1">classification level</span>, or the tree depth reported for each class on this tree; e.g., the class ‚Äùzebra‚Äù is located at level 4, and the class ‚Äùfish‚Äù is located at level 2. Note that classes that appear to be subclasses of themselves refer to classes in which there is an ‚Äùother‚Äù category, e.g., the subset ‚Äùcat‚Äù refers to superset ‚Äùcat‚Äù objects that are not ‚Äùtiger‚Äù objects</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To ensure that human observers select the correct object within an image (e.g., in cases where there are multiple classifiable objects in frame), we additionally ask humans to label which object they are selecting by clicking a point on the image. The correct object is usually centered on the image, so we place a cross directly at the center of each image for human observers. This guides human observers toward the correct object, but it does not give away information about which object to classify that is not available to models; this cross does not move throughout data collection.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental Results</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Each subsection below aims at addressing a specific scientific question, given by its title. In each case we report experimental results with our IRUO dataset that address the given question. All models in our experiments utilize encoders that have already been pre-trained on the ImageNet-1k dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib4" title="">4</a>]</cite>). We consider a large number of models that combine different architectures (e.g., convolutional, transformer, etc) and augmentation strategies); we fine-tune each model on the IRUO dataset. Following previous work suggesting that training on occluded imagery is not beneficial for models (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>]</cite>) we fine-tune the models only on unoccluded imagery. We independently optimize several key hyperparameters (e.g., learning rates, momentum) for each model using a grid search to find highest overall accuracy on IRUO. Full details of the hyperparameter grid search, optimized hyperparameter choices (including hyperparameters unique to models designed for occlusion), stopping criteria, and other training details can be found in the Appendix.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Which models are most accurate under occlusion?</h3>
<figure class="ltx_figure" id="S6.F6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F6.1.1.1">
<td class="ltx_td ltx_align_center" id="S6.F6.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="603" id="S6.F6.1.1.1.1.g1" src="extracted/5858817/iruo_acc_all_all.png" width="592"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy per level of occlusion on the partition of the IRUO-Base dataset containing real occlusion, measured as proportion of correct top-1 classifications for each model at each occlusion level. The x-labels 0, 1, and 2 correspond to the occlusion levels in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4" title="4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">4</span></a>. Convolutional models are highlighted in green, Vision Transformer-based models are highlighted in blue, and models explicitly designed for occlusion robustness are highlighted in pink. The line style (solid, dot, dash) indicate augmentations used for selected models. Deep Feature Augmentation (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>) is abbreviated as DFA for clarity</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">To address this question, we report in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F6" title="Figure 6 ‚Ä£ 6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6</span></a> the accuracy of the models and augmentations described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS1" title="3.1 Models ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.1</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS2" title="3.2 Augmentations ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.2</span></a>, respectively, under varying levels of occlusion. The results indicate that Transformer-based models (blue and purple lines) perform best, by a large margin, across all levels of occlusion, including Level 0 (no occlusion). The Swin model (blue lines) achieves the best overall performance among Transformers, making it the best-performing model in our benchmark. The convolutional models (green lines in all but one case) achieve the next best accuracy. Among the convolutional models we see that ResNeXt performs best, followed by ResNet, and then VGG. Similar to transformer-based models, the relative rank of the convolutional models (compared to others in the benchmark) is similar across all levels of occlusion. Surprisingly, the worst-performing model is the CompositionalNet. We observe mixed and minimal impact of the two augmentation schemes that we considered: Mixup and CutMix. The best-performing model is the Swin model with Mixup; however, the performance gain over Swin alone is small, and the impact of Mixup on other models is more modest, or even sometimes negative.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Our findings expand upon and in some cases modify findings of prior work. Recent work (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>]</cite>) found that transformers outperform convolutional models in the presence of occlusion; our work corroborates this finding on a more comprehensive and controlled experimental setting. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>]</cite> hypothesized that this increase in accuracy under occlusion is due to the ability of receptive fields that result from self-attention layers to adapt to occlusion and attend to unoccluded data while down-weighting the relative importance of occluded data. We corroborate these results qualitatively in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F7" title="Figure 7 ‚Ä£ 6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">7</span></a>; an image of a highly-occluded tiger is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F7" title="Figure 7 ‚Ä£ 6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">7</span></a>(a) alongside corresponding attention maps generated by ViT. To calculate the attention masks, we select all patches with object pixels, and we sum the values of attention at all other locations for all attention heads. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F7" title="Figure 7 ‚Ä£ 6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">7</span></a>, objects are clearly separated from background and occluders, and parts of the tiger in frame attend to each other. This suggests that Vision Transformer-based models can localize relevant objects and ignore occluders. Recent work reported that the CompositionalNet model outperforms convolutional models (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>); however, we find here that all DNN-based models outperform it. We hypothesize that this discrepancy is due to the significantly greater complexity of the IRUO dataset here (e.g., in terms of scene and target class diversity) compared to the <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.1">VehicleOcclusion</span> dataset utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>. Lastly, Deep Feature Augmentation (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>) demonstrated higher accuracy under occlusion however we find more mixed results. This possibly reflects a difference in our implementation compared to the original; we do not use the same synthetic occlusions in training and testing, as is done in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F7.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F7.2.2.2">
<td class="ltx_td ltx_align_center" id="S6.F7.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="281" id="S6.F7.1.1.1.1.g1" src="extracted/5858817/tiger_samp_605.jpg" width="281"/></td>
<td class="ltx_td ltx_align_center" id="S6.F7.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="281" id="S6.F7.2.2.2.2.g1" src="extracted/5858817/605_00011_561f6380_img_0000012_vit_layer_2.jpg" width="281"/></td>
</tr>
<tr class="ltx_tr" id="S6.F7.4.4.5.1">
<td class="ltx_td ltx_align_center" id="S6.F7.4.4.5.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F7.4.4.5.1.2">(b)</td>
</tr>
<tr class="ltx_tr" id="S6.F7.4.4.4">
<td class="ltx_td ltx_align_center" id="S6.F7.3.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="281" id="S6.F7.3.3.3.1.g1" src="extracted/5858817/605_00011_561f6380_img_0000012_vit_layer_6.jpg" width="281"/></td>
<td class="ltx_td ltx_align_center" id="S6.F7.4.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="281" id="S6.F7.4.4.4.2.g1" src="extracted/5858817/605_00011_561f6380_img_0000012_vit_layer_10.jpg" width="281"/></td>
</tr>
<tr class="ltx_tr" id="S6.F7.4.4.6.2">
<td class="ltx_td ltx_align_center" id="S6.F7.4.4.6.2.1">(c)</td>
<td class="ltx_td ltx_align_center" id="S6.F7.4.4.6.2.2">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Sample image of a tiger (a) at occlusion level 2 with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (b) third, (c) seventh, and (d) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. ViT and other transformer models demonstrate the ability to attend to relevant objects and ignore occluders in certain cases. Lighter colors (e.g., yellow) denote higher values for attention</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">As noted above, our results indicate that the accuracy of every model degrades at approximately the same rate as the level of occlusion increases. Consequently, the relative accuracy of a given model in the presence of occlusion (i.e., compared to other models) is well-predicted by its relative accuracy on unoccluded imagery. For example, Transformer-based models (blue lines) perform best on unoccluded imagery, and we observe that they likewise also perform best on all other levels of occlusion. Interestingly, these results imply that none of the models considered in our benchmark are advantageous <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.1">specifically for occlusion</span>: i.e., any advantages/disadvantages of each model in the presence of occlusion are also similarly advantageous for unoccluded imagery, and vice versa. A corollary of this finding is that one can improve accuracy under occlusion by improving accuracy on unoccluded imagery. Although this pattern holds for a large number of models, examined on a diverse dataset, this observation is empirical and therefore its generality may ultimately be limited.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Is synthetic occlusion a good proxy for real occlusion?</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In this section we investigate whether synthetic occlusions are an accurate proxy for real-world occlusions, when estimating the <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">relative</span> accuracy of competing models. In other words, we ask whether the evaluation of model accuracy using synthetic occlusions would lead to the same rank-ordering of models as real-world occlusion. To address this question, we evaluate model accuracy using two datasets: one with real-world occlusions, and one with synthetic occlusions. If the rank-orderings produced by each dataset are highly dissimilar, then we conclude that synthetic occlusions are not a reasonable proxy for real-world occlusions. We use the IRUO-Base and IRUO-Synthetic partitions of the IRUO dataset to rank-order models on real-world and synthetic occlusions, respectively. The construction of the IRUO-Base dataset is detailed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4" title="4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">4</span></a>. The IRUO-Synthetic dataset was created by adding artificial occlusion to all of the images in IRUO-Base that do not contain occlusion. To create synthetic occlusions, we replicate several strategies utilized in recent prior work (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>), which share the same basic strategy of placing (i.e., inpainting) occluding objects at pseudo-random locations over the target object in the imagery. The primary difference among prior approaches is the design of the occluding objects. We consider several previously explored occluder designs: white boxes, black boxes, zebra-print texture boxes, uniformly-distributed noise boxes, and objects from the COCO dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib13" title="">13</a>]</cite>). Examples of the synthetic occlusions are presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F8" title="Figure 8 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">8</span></a>. Full details of our occluder designs and the occluder placement strategy can be found in the Appendix.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Qualitative Assessment.</span> First we qualitatively evaluate the agreement of the rankings provided by real and synthetic occlusion. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F9" title="Figure 9 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">9</span></a>(a), we present a line plot showing the ranking of each model as we change the occlusion type for occlusion level 1. The results indicate that no pair of occlusion types produces the same ranking of models; however, the the rankings are always very similar. For example, models often don‚Äôt change ranking, and when they do, it is only by one or two positions. The variability of ranking is even more limited if we only consider changes in architecture (i.e., crossings between solid lines). In terms of similarity to real occlusion, it appears that solid boxes (white or black boxes), or juxtaposed objects, provide the most similar rankings. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F9" title="Figure 9 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">9</span></a>(b), we present the rankings of models for occlusion level 2, where we observe somewhat lower consistency in the rankings; however, the conclusions are otherwise the same.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Our qualitative results therefore suggest that synthetic occlusions often provide good approximate <span class="ltx_text ltx_font_italic" id="S6.SS2.p3.1.1">rankings</span> of model accuracy under occlusion. The rankings provided by synthetic occlusion are not identical to real-world occlusions, however; therefore, synthetic occlusions will be a better proxy for large differences in model accuracy. It is also noteworthy that the variations in rankings by each occlusion type may not be due to systematic differences among occlusion types, and instead may be due to random variations in the data; i.e., testing with a bootstrap sample of the test imagery of one occlusion type may produce different rankings, as well.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.10"><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.10.1">Statistical Assessment.</span> To provide a more precise notion of similarity among the rankings, we utilize the Friedman <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p4.1.m1.1"><semantics id="S6.SS2.p4.1.m1.1a"><mi id="S6.SS2.p4.1.m1.1.1" xref="S6.SS2.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.1.m1.1b"><ci id="S6.SS2.p4.1.m1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.1.m1.1d">italic_Q</annotation></semantics></math>-value test (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>), summarized in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We treat the benchmark models as ranked ‚Äùobjects‚Äù in the Friedman test and each of occlusion type as ‚Äùjudges‚Äù that rank the objects. Using the rank-ordering of models provided by each occlusion type, we can then calculate the Friendman <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p4.2.m2.1"><semantics id="S6.SS2.p4.2.m2.1a"><mi id="S6.SS2.p4.2.m2.1.1" xref="S6.SS2.p4.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.2.m2.1b"><ci id="S6.SS2.p4.2.m2.1.1.cmml" xref="S6.SS2.p4.2.m2.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.2.m2.1d">italic_Q</annotation></semantics></math> statistic to test the null hypothesis, which supposes that the rankings provided by the occlusion types are random. Therefore, if we reject the null hypothesis, particularly with very large values of <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p4.3.m3.1"><semantics id="S6.SS2.p4.3.m3.1a"><mi id="S6.SS2.p4.3.m3.1.1" xref="S6.SS2.p4.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.3.m3.1b"><ci id="S6.SS2.p4.3.m3.1.1.cmml" xref="S6.SS2.p4.3.m3.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.3.m3.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.3.m3.1d">italic_Q</annotation></semantics></math>, this suggests there is high similarity in the rankings provided by each occlusion. We perform the Friedman test separately for each of two occlusion levels: level 1 (0-50%) and level 2 (<math alttext="&gt;" class="ltx_Math" display="inline" id="S6.SS2.p4.4.m4.1"><semantics id="S6.SS2.p4.4.m4.1a"><mo id="S6.SS2.p4.4.m4.1.1" xref="S6.SS2.p4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.4.m4.1b"><gt id="S6.SS2.p4.4.m4.1.1.cmml" xref="S6.SS2.p4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.4.m4.1d">&gt;</annotation></semantics></math>50%). For each occlusion level, we treat each of the six types of occlusion (real occluders, white boxes, black boxes, noise boxes, texture boxes, and synthetically added objects) as a set of six judges. The calculated Friedman <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p4.5.m5.1"><semantics id="S6.SS2.p4.5.m5.1a"><mi id="S6.SS2.p4.5.m5.1.1" xref="S6.SS2.p4.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.5.m5.1b"><ci id="S6.SS2.p4.5.m5.1.1.cmml" xref="S6.SS2.p4.5.m5.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.5.m5.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.5.m5.1d">italic_Q</annotation></semantics></math>-value follows a chi-squared null distribution for either a large number of models (<math alttext="k&gt;4" class="ltx_Math" display="inline" id="S6.SS2.p4.6.m6.1"><semantics id="S6.SS2.p4.6.m6.1a"><mrow id="S6.SS2.p4.6.m6.1.1" xref="S6.SS2.p4.6.m6.1.1.cmml"><mi id="S6.SS2.p4.6.m6.1.1.2" xref="S6.SS2.p4.6.m6.1.1.2.cmml">k</mi><mo id="S6.SS2.p4.6.m6.1.1.1" xref="S6.SS2.p4.6.m6.1.1.1.cmml">&gt;</mo><mn id="S6.SS2.p4.6.m6.1.1.3" xref="S6.SS2.p4.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.6.m6.1b"><apply id="S6.SS2.p4.6.m6.1.1.cmml" xref="S6.SS2.p4.6.m6.1.1"><gt id="S6.SS2.p4.6.m6.1.1.1.cmml" xref="S6.SS2.p4.6.m6.1.1.1"></gt><ci id="S6.SS2.p4.6.m6.1.1.2.cmml" xref="S6.SS2.p4.6.m6.1.1.2">ùëò</ci><cn id="S6.SS2.p4.6.m6.1.1.3.cmml" type="integer" xref="S6.SS2.p4.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.6.m6.1c">k&gt;4</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.6.m6.1d">italic_k &gt; 4</annotation></semantics></math>) or types of occlusion (<math alttext="n&gt;15" class="ltx_Math" display="inline" id="S6.SS2.p4.7.m7.1"><semantics id="S6.SS2.p4.7.m7.1a"><mrow id="S6.SS2.p4.7.m7.1.1" xref="S6.SS2.p4.7.m7.1.1.cmml"><mi id="S6.SS2.p4.7.m7.1.1.2" xref="S6.SS2.p4.7.m7.1.1.2.cmml">n</mi><mo id="S6.SS2.p4.7.m7.1.1.1" xref="S6.SS2.p4.7.m7.1.1.1.cmml">&gt;</mo><mn id="S6.SS2.p4.7.m7.1.1.3" xref="S6.SS2.p4.7.m7.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.7.m7.1b"><apply id="S6.SS2.p4.7.m7.1.1.cmml" xref="S6.SS2.p4.7.m7.1.1"><gt id="S6.SS2.p4.7.m7.1.1.1.cmml" xref="S6.SS2.p4.7.m7.1.1.1"></gt><ci id="S6.SS2.p4.7.m7.1.1.2.cmml" xref="S6.SS2.p4.7.m7.1.1.2">ùëõ</ci><cn id="S6.SS2.p4.7.m7.1.1.3.cmml" type="integer" xref="S6.SS2.p4.7.m7.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.7.m7.1c">n&gt;15</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.7.m7.1d">italic_n &gt; 15</annotation></semantics></math>) (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>); as our number of models, <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p4.8.m8.1"><semantics id="S6.SS2.p4.8.m8.1a"><mi id="S6.SS2.p4.8.m8.1.1" xref="S6.SS2.p4.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.8.m8.1b"><ci id="S6.SS2.p4.8.m8.1.1.cmml" xref="S6.SS2.p4.8.m8.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.8.m8.1d">italic_k</annotation></semantics></math>, is 14, the former condition is met in our study, and therefore our calculated <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p4.9.m9.1"><semantics id="S6.SS2.p4.9.m9.1a"><mi id="S6.SS2.p4.9.m9.1.1" xref="S6.SS2.p4.9.m9.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.9.m9.1b"><ci id="S6.SS2.p4.9.m9.1.1.cmml" xref="S6.SS2.p4.9.m9.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.9.m9.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.9.m9.1d">italic_Q</annotation></semantics></math> value is approximately <math alttext="\chi^{2}_{13}" class="ltx_Math" display="inline" id="S6.SS2.p4.10.m10.1"><semantics id="S6.SS2.p4.10.m10.1a"><msubsup id="S6.SS2.p4.10.m10.1.1" xref="S6.SS2.p4.10.m10.1.1.cmml"><mi id="S6.SS2.p4.10.m10.1.1.2.2" xref="S6.SS2.p4.10.m10.1.1.2.2.cmml">œá</mi><mn id="S6.SS2.p4.10.m10.1.1.3" xref="S6.SS2.p4.10.m10.1.1.3.cmml">13</mn><mn id="S6.SS2.p4.10.m10.1.1.2.3" xref="S6.SS2.p4.10.m10.1.1.2.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.10.m10.1b"><apply id="S6.SS2.p4.10.m10.1.1.cmml" xref="S6.SS2.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S6.SS2.p4.10.m10.1.1.1.cmml" xref="S6.SS2.p4.10.m10.1.1">subscript</csymbol><apply id="S6.SS2.p4.10.m10.1.1.2.cmml" xref="S6.SS2.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S6.SS2.p4.10.m10.1.1.2.1.cmml" xref="S6.SS2.p4.10.m10.1.1">superscript</csymbol><ci id="S6.SS2.p4.10.m10.1.1.2.2.cmml" xref="S6.SS2.p4.10.m10.1.1.2.2">ùúí</ci><cn id="S6.SS2.p4.10.m10.1.1.2.3.cmml" type="integer" xref="S6.SS2.p4.10.m10.1.1.2.3">2</cn></apply><cn id="S6.SS2.p4.10.m10.1.1.3.cmml" type="integer" xref="S6.SS2.p4.10.m10.1.1.3">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.10.m10.1c">\chi^{2}_{13}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.10.m10.1d">italic_œá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 13 end_POSTSUBSCRIPT</annotation></semantics></math>-distributed. Full calculations for this statistic can be found in the Appendix.</p>
</div>
<figure class="ltx_figure" id="S6.F8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F8.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F8.2.2.2">
<td class="ltx_td ltx_align_center" id="S6.F8.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="240" id="S6.F8.1.1.1.1.g1" src="extracted/5858817/artificial_picture2.png" width="240"/></td>
<td class="ltx_td ltx_align_center" id="S6.F8.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="240" id="S6.F8.2.2.2.2.g1" src="extracted/5858817/artificial_picture4.png" width="240"/></td>
</tr>
<tr class="ltx_tr" id="S6.F8.4.4.5.1">
<td class="ltx_td ltx_align_center" id="S6.F8.4.4.5.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F8.4.4.5.1.2">(b)</td>
</tr>
<tr class="ltx_tr" id="S6.F8.4.4.4">
<td class="ltx_td ltx_align_center" id="S6.F8.3.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="241" id="S6.F8.3.3.3.1.g1" src="extracted/5858817/artificial_picture7.png" width="240"/></td>
<td class="ltx_td ltx_align_center" id="S6.F8.4.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="238" id="S6.F8.4.4.4.2.g1" src="extracted/5858817/artificial_picture8.png" width="240"/></td>
</tr>
<tr class="ltx_tr" id="S6.F8.4.4.6.2">
<td class="ltx_td ltx_align_center" id="S6.F8.4.4.6.2.1">(c)</td>
<td class="ltx_td ltx_align_center" id="S6.F8.4.4.6.2.2">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Images showing various types of artificial occlusion used in our benchmark. In (a) black mask occlusion, (b) texture occlusion, (c) noise occlusion, and (d) superimposed object occlusion using objects from COCO (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib13" title="">13</a>]</cite>), all based on artificial occlusions used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>]</cite></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.2">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.T2" title="Table 2 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the calculated values of <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p5.1.m1.1"><semantics id="S6.SS2.p5.1.m1.1a"><mi id="S6.SS2.p5.1.m1.1.1" xref="S6.SS2.p5.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p5.1.m1.1b"><ci id="S6.SS2.p5.1.m1.1.1.cmml" xref="S6.SS2.p5.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p5.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p5.1.m1.1d">italic_Q</annotation></semantics></math> for occlusion levels 1 and 2. Based on the very large values of <math alttext="Q" class="ltx_Math" display="inline" id="S6.SS2.p5.2.m2.1"><semantics id="S6.SS2.p5.2.m2.1a"><mi id="S6.SS2.p5.2.m2.1.1" xref="S6.SS2.p5.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p5.2.m2.1b"><ci id="S6.SS2.p5.2.m2.1.1.cmml" xref="S6.SS2.p5.2.m2.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p5.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p5.2.m2.1d">italic_Q</annotation></semantics></math> for both levels of occlusion and the resulting small p-values (far less than 0.05), we reject our null hypothesis, determining that there is a consistent rank order of models at each occlusion level, regardless of the specific type of occlusion. This implies that, for occlusion levels 1 (0-50%) and 2 (&gt;50%), any type of occlusion tested is appropriate for determining the effect of occluded objects on classification accuracy. A summary of these values may be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.T2" title="Table 2 ‚Ä£ 6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">2</span></a>, and full rankings and calculations of these values may be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A1.T5" title="Table 5 ‚Ä£ Appendix A Full result tables for model evaluation on real and synthetic occlusion ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.1.1.2">Occlusion level</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.1.1.1"><math alttext="Q" class="ltx_Math" display="inline" id="S6.T2.1.1.1.m1.1"><semantics id="S6.T2.1.1.1.m1.1a"><mi id="S6.T2.1.1.1.m1.1.1" xref="S6.T2.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.m1.1b"><ci id="S6.T2.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S6.T2.1.1.1.m1.1d">italic_Q</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.1.1.3">p-value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2">1 (0-50%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.3">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.1"><math alttext="5.0\times 10^{-10}" class="ltx_Math" display="inline" id="S6.T2.2.2.1.m1.1"><semantics id="S6.T2.2.2.1.m1.1a"><mrow id="S6.T2.2.2.1.m1.1.1" xref="S6.T2.2.2.1.m1.1.1.cmml"><mn id="S6.T2.2.2.1.m1.1.1.2" xref="S6.T2.2.2.1.m1.1.1.2.cmml">5.0</mn><mo id="S6.T2.2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S6.T2.2.2.1.m1.1.1.1.cmml">√ó</mo><msup id="S6.T2.2.2.1.m1.1.1.3" xref="S6.T2.2.2.1.m1.1.1.3.cmml"><mn id="S6.T2.2.2.1.m1.1.1.3.2" xref="S6.T2.2.2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.T2.2.2.1.m1.1.1.3.3" xref="S6.T2.2.2.1.m1.1.1.3.3.cmml"><mo id="S6.T2.2.2.1.m1.1.1.3.3a" xref="S6.T2.2.2.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S6.T2.2.2.1.m1.1.1.3.3.2" xref="S6.T2.2.2.1.m1.1.1.3.3.2.cmml">10</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.1.m1.1b"><apply id="S6.T2.2.2.1.m1.1.1.cmml" xref="S6.T2.2.2.1.m1.1.1"><times id="S6.T2.2.2.1.m1.1.1.1.cmml" xref="S6.T2.2.2.1.m1.1.1.1"></times><cn id="S6.T2.2.2.1.m1.1.1.2.cmml" type="float" xref="S6.T2.2.2.1.m1.1.1.2">5.0</cn><apply id="S6.T2.2.2.1.m1.1.1.3.cmml" xref="S6.T2.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.T2.2.2.1.m1.1.1.3.1.cmml" xref="S6.T2.2.2.1.m1.1.1.3">superscript</csymbol><cn id="S6.T2.2.2.1.m1.1.1.3.2.cmml" type="integer" xref="S6.T2.2.2.1.m1.1.1.3.2">10</cn><apply id="S6.T2.2.2.1.m1.1.1.3.3.cmml" xref="S6.T2.2.2.1.m1.1.1.3.3"><minus id="S6.T2.2.2.1.m1.1.1.3.3.1.cmml" xref="S6.T2.2.2.1.m1.1.1.3.3"></minus><cn id="S6.T2.2.2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S6.T2.2.2.1.m1.1.1.3.3.2">10</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.1.m1.1c">5.0\times 10^{-10}</annotation><annotation encoding="application/x-llamapun" id="S6.T2.2.2.1.m1.1d">5.0 √ó 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T2.3.3.2">2 (&gt;50%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.3.3">65.5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.3.1"><math alttext="5.3\times 10^{-9}" class="ltx_Math" display="inline" id="S6.T2.3.3.1.m1.1"><semantics id="S6.T2.3.3.1.m1.1a"><mrow id="S6.T2.3.3.1.m1.1.1" xref="S6.T2.3.3.1.m1.1.1.cmml"><mn id="S6.T2.3.3.1.m1.1.1.2" xref="S6.T2.3.3.1.m1.1.1.2.cmml">5.3</mn><mo id="S6.T2.3.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S6.T2.3.3.1.m1.1.1.1.cmml">√ó</mo><msup id="S6.T2.3.3.1.m1.1.1.3" xref="S6.T2.3.3.1.m1.1.1.3.cmml"><mn id="S6.T2.3.3.1.m1.1.1.3.2" xref="S6.T2.3.3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.T2.3.3.1.m1.1.1.3.3" xref="S6.T2.3.3.1.m1.1.1.3.3.cmml"><mo id="S6.T2.3.3.1.m1.1.1.3.3a" xref="S6.T2.3.3.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S6.T2.3.3.1.m1.1.1.3.3.2" xref="S6.T2.3.3.1.m1.1.1.3.3.2.cmml">9</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.3.3.1.m1.1b"><apply id="S6.T2.3.3.1.m1.1.1.cmml" xref="S6.T2.3.3.1.m1.1.1"><times id="S6.T2.3.3.1.m1.1.1.1.cmml" xref="S6.T2.3.3.1.m1.1.1.1"></times><cn id="S6.T2.3.3.1.m1.1.1.2.cmml" type="float" xref="S6.T2.3.3.1.m1.1.1.2">5.3</cn><apply id="S6.T2.3.3.1.m1.1.1.3.cmml" xref="S6.T2.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.T2.3.3.1.m1.1.1.3.1.cmml" xref="S6.T2.3.3.1.m1.1.1.3">superscript</csymbol><cn id="S6.T2.3.3.1.m1.1.1.3.2.cmml" type="integer" xref="S6.T2.3.3.1.m1.1.1.3.2">10</cn><apply id="S6.T2.3.3.1.m1.1.1.3.3.cmml" xref="S6.T2.3.3.1.m1.1.1.3.3"><minus id="S6.T2.3.3.1.m1.1.1.3.3.1.cmml" xref="S6.T2.3.3.1.m1.1.1.3.3"></minus><cn id="S6.T2.3.3.1.m1.1.1.3.3.2.cmml" type="integer" xref="S6.T2.3.3.1.m1.1.1.3.3.2">9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.3.1.m1.1c">5.3\times 10^{-9}</annotation><annotation encoding="application/x-llamapun" id="S6.T2.3.3.1.m1.1d">5.3 √ó 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Calculated values of <math alttext="Q" class="ltx_Math" display="inline" id="S6.T2.7.m1.1"><semantics id="S6.T2.7.m1.1b"><mi id="S6.T2.7.m1.1.1" xref="S6.T2.7.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.T2.7.m1.1c"><ci id="S6.T2.7.m1.1.1.cmml" xref="S6.T2.7.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.7.m1.1d">Q</annotation><annotation encoding="application/x-llamapun" id="S6.T2.7.m1.1e">italic_Q</annotation></semantics></math> and corresponding p-values assuming a null distribution with <math alttext="k" class="ltx_Math" display="inline" id="S6.T2.8.m2.1"><semantics id="S6.T2.8.m2.1b"><mi id="S6.T2.8.m2.1.1" xref="S6.T2.8.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.T2.8.m2.1c"><ci id="S6.T2.8.m2.1.1.cmml" xref="S6.T2.8.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.8.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S6.T2.8.m2.1e">italic_k</annotation></semantics></math> degrees of freedom, following the Friedman <math alttext="Q" class="ltx_Math" display="inline" id="S6.T2.9.m3.1"><semantics id="S6.T2.9.m3.1b"><mi id="S6.T2.9.m3.1.1" xref="S6.T2.9.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S6.T2.9.m3.1c"><ci id="S6.T2.9.m3.1.1.cmml" xref="S6.T2.9.m3.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.9.m3.1d">Q</annotation><annotation encoding="application/x-llamapun" id="S6.T2.9.m3.1e">italic_Q</annotation></semantics></math> procedure in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a>, based on comparisons between the IRUO and IRUO-Synthetic datasets</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F9">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F9.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F9.2.2.2">
<td class="ltx_td ltx_align_center" id="S6.F9.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S6.F9.1.1.1.1.g1" src="extracted/5858817/bump_1.png" width="287"/></td>
<td class="ltx_td ltx_align_center" id="S6.F9.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S6.F9.2.2.2.2.g1" src="extracted/5858817/bump_2.png" width="287"/></td>
</tr>
<tr class="ltx_tr" id="S6.F9.2.2.3.1">
<td class="ltx_td ltx_align_center" id="S6.F9.2.2.3.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F9.2.2.3.1.2">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Rankings of models across occlusion types at each of (a) occlusion level 1 and (b) occlusion level 2</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>How occlusion-robust are models compared to humans?</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In this section we investigate how occlusion-robust DNNs are to occlusion. We determine robustness by comparing the accuracy of DNNs to the accuracy of a hypothetical optimal model, which we approximate using human recognition accuracy under occlusion (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5" title="5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Human Study Design.</span> To assess human accuracy, we use the <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.2">IRUO-HTS</span> partition of our IRUO dataset, the creation of which is described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS1" title="5.1 The Human Testing Subset ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5.1</span></a>. We ask each of 20 people, or <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.3">observers</span>, to label 200 randomly-selected images from this partition (of size 552 images). We follow the method in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS2" title="5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5.2</span></a>, asking respondents for image labels at each level of the hierarchy in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F5" title="Figure 5 ‚Ä£ 5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a> until a specific class label belonging to IRUO is selected. A sample step of the program interface used to collect human responses is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F10" title="Figure 10 ‚Ä£ 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">10</span></a>. We then estimate the human accuracy for the <math alttext="j^{th}" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.1"><semantics id="S6.SS3.p2.1.m1.1a"><msup id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml"><mi id="S6.SS3.p2.1.m1.1.1.2" xref="S6.SS3.p2.1.m1.1.1.2.cmml">j</mi><mrow id="S6.SS3.p2.1.m1.1.1.3" xref="S6.SS3.p2.1.m1.1.1.3.cmml"><mi id="S6.SS3.p2.1.m1.1.1.3.2" xref="S6.SS3.p2.1.m1.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p2.1.m1.1.1.3.1" xref="S6.SS3.p2.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p2.1.m1.1.1.3.3" xref="S6.SS3.p2.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><apply id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.1.m1.1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">superscript</csymbol><ci id="S6.SS3.p2.1.m1.1.1.2.cmml" xref="S6.SS3.p2.1.m1.1.1.2">ùëó</ci><apply id="S6.SS3.p2.1.m1.1.1.3.cmml" xref="S6.SS3.p2.1.m1.1.1.3"><times id="S6.SS3.p2.1.m1.1.1.3.1.cmml" xref="S6.SS3.p2.1.m1.1.1.3.1"></times><ci id="S6.SS3.p2.1.m1.1.1.3.2.cmml" xref="S6.SS3.p2.1.m1.1.1.3.2">ùë°</ci><ci id="S6.SS3.p2.1.m1.1.1.3.3.cmml" xref="S6.SS3.p2.1.m1.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> image by</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="q_{j}=\frac{1}{N_{j}}\sum_{i\in S_{j}}\mathbbm{1}(\hat{t}_{i,j}=t_{j})" class="ltx_Math" display="block" id="S6.E1.m1.3"><semantics id="S6.E1.m1.3a"><mrow id="S6.E1.m1.3.3" xref="S6.E1.m1.3.3.cmml"><msub id="S6.E1.m1.3.3.3" xref="S6.E1.m1.3.3.3.cmml"><mi id="S6.E1.m1.3.3.3.2" xref="S6.E1.m1.3.3.3.2.cmml">q</mi><mi id="S6.E1.m1.3.3.3.3" xref="S6.E1.m1.3.3.3.3.cmml">j</mi></msub><mo id="S6.E1.m1.3.3.2" xref="S6.E1.m1.3.3.2.cmml">=</mo><mrow id="S6.E1.m1.3.3.1" xref="S6.E1.m1.3.3.1.cmml"><mfrac id="S6.E1.m1.3.3.1.3" xref="S6.E1.m1.3.3.1.3.cmml"><mn id="S6.E1.m1.3.3.1.3.2" xref="S6.E1.m1.3.3.1.3.2.cmml">1</mn><msub id="S6.E1.m1.3.3.1.3.3" xref="S6.E1.m1.3.3.1.3.3.cmml"><mi id="S6.E1.m1.3.3.1.3.3.2" xref="S6.E1.m1.3.3.1.3.3.2.cmml">N</mi><mi id="S6.E1.m1.3.3.1.3.3.3" xref="S6.E1.m1.3.3.1.3.3.3.cmml">j</mi></msub></mfrac><mo id="S6.E1.m1.3.3.1.2" xref="S6.E1.m1.3.3.1.2.cmml">‚Å¢</mo><mrow id="S6.E1.m1.3.3.1.1" xref="S6.E1.m1.3.3.1.1.cmml"><munder id="S6.E1.m1.3.3.1.1.2" xref="S6.E1.m1.3.3.1.1.2.cmml"><mo id="S6.E1.m1.3.3.1.1.2.2" movablelimits="false" xref="S6.E1.m1.3.3.1.1.2.2.cmml">‚àë</mo><mrow id="S6.E1.m1.3.3.1.1.2.3" xref="S6.E1.m1.3.3.1.1.2.3.cmml"><mi id="S6.E1.m1.3.3.1.1.2.3.2" xref="S6.E1.m1.3.3.1.1.2.3.2.cmml">i</mi><mo id="S6.E1.m1.3.3.1.1.2.3.1" xref="S6.E1.m1.3.3.1.1.2.3.1.cmml">‚àà</mo><msub id="S6.E1.m1.3.3.1.1.2.3.3" xref="S6.E1.m1.3.3.1.1.2.3.3.cmml"><mi id="S6.E1.m1.3.3.1.1.2.3.3.2" xref="S6.E1.m1.3.3.1.1.2.3.3.2.cmml">S</mi><mi id="S6.E1.m1.3.3.1.1.2.3.3.3" xref="S6.E1.m1.3.3.1.1.2.3.3.3.cmml">j</mi></msub></mrow></munder><mrow id="S6.E1.m1.3.3.1.1.1" xref="S6.E1.m1.3.3.1.1.1.cmml"><mn id="S6.E1.m1.3.3.1.1.1.3" xref="S6.E1.m1.3.3.1.1.1.3.cmml">ùüô</mn><mo id="S6.E1.m1.3.3.1.1.1.2" xref="S6.E1.m1.3.3.1.1.1.2.cmml">‚Å¢</mo><mrow id="S6.E1.m1.3.3.1.1.1.1.1" xref="S6.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S6.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S6.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E1.m1.3.3.1.1.1.1.1.1" xref="S6.E1.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S6.E1.m1.3.3.1.1.1.1.1.1.2" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">t</mi><mo id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.1" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mrow id="S6.E1.m1.2.2.2.4" xref="S6.E1.m1.2.2.2.3.cmml"><mi id="S6.E1.m1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml">i</mi><mo id="S6.E1.m1.2.2.2.4.1" xref="S6.E1.m1.2.2.2.3.cmml">,</mo><mi id="S6.E1.m1.2.2.2.2" xref="S6.E1.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S6.E1.m1.3.3.1.1.1.1.1.1.1" xref="S6.E1.m1.3.3.1.1.1.1.1.1.1.cmml">=</mo><msub id="S6.E1.m1.3.3.1.1.1.1.1.1.3" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S6.E1.m1.3.3.1.1.1.1.1.1.3.2" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml">t</mi><mi id="S6.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S6.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S6.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.3b"><apply id="S6.E1.m1.3.3.cmml" xref="S6.E1.m1.3.3"><eq id="S6.E1.m1.3.3.2.cmml" xref="S6.E1.m1.3.3.2"></eq><apply id="S6.E1.m1.3.3.3.cmml" xref="S6.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.3.1.cmml" xref="S6.E1.m1.3.3.3">subscript</csymbol><ci id="S6.E1.m1.3.3.3.2.cmml" xref="S6.E1.m1.3.3.3.2">ùëû</ci><ci id="S6.E1.m1.3.3.3.3.cmml" xref="S6.E1.m1.3.3.3.3">ùëó</ci></apply><apply id="S6.E1.m1.3.3.1.cmml" xref="S6.E1.m1.3.3.1"><times id="S6.E1.m1.3.3.1.2.cmml" xref="S6.E1.m1.3.3.1.2"></times><apply id="S6.E1.m1.3.3.1.3.cmml" xref="S6.E1.m1.3.3.1.3"><divide id="S6.E1.m1.3.3.1.3.1.cmml" xref="S6.E1.m1.3.3.1.3"></divide><cn id="S6.E1.m1.3.3.1.3.2.cmml" type="integer" xref="S6.E1.m1.3.3.1.3.2">1</cn><apply id="S6.E1.m1.3.3.1.3.3.cmml" xref="S6.E1.m1.3.3.1.3.3"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.3.3.1.cmml" xref="S6.E1.m1.3.3.1.3.3">subscript</csymbol><ci id="S6.E1.m1.3.3.1.3.3.2.cmml" xref="S6.E1.m1.3.3.1.3.3.2">ùëÅ</ci><ci id="S6.E1.m1.3.3.1.3.3.3.cmml" xref="S6.E1.m1.3.3.1.3.3.3">ùëó</ci></apply></apply><apply id="S6.E1.m1.3.3.1.1.cmml" xref="S6.E1.m1.3.3.1.1"><apply id="S6.E1.m1.3.3.1.1.2.cmml" xref="S6.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.1.2.1.cmml" xref="S6.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S6.E1.m1.3.3.1.1.2.2.cmml" xref="S6.E1.m1.3.3.1.1.2.2"></sum><apply id="S6.E1.m1.3.3.1.1.2.3.cmml" xref="S6.E1.m1.3.3.1.1.2.3"><in id="S6.E1.m1.3.3.1.1.2.3.1.cmml" xref="S6.E1.m1.3.3.1.1.2.3.1"></in><ci id="S6.E1.m1.3.3.1.1.2.3.2.cmml" xref="S6.E1.m1.3.3.1.1.2.3.2">ùëñ</ci><apply id="S6.E1.m1.3.3.1.1.2.3.3.cmml" xref="S6.E1.m1.3.3.1.1.2.3.3"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.1.2.3.3.1.cmml" xref="S6.E1.m1.3.3.1.1.2.3.3">subscript</csymbol><ci id="S6.E1.m1.3.3.1.1.2.3.3.2.cmml" xref="S6.E1.m1.3.3.1.1.2.3.3.2">ùëÜ</ci><ci id="S6.E1.m1.3.3.1.1.2.3.3.3.cmml" xref="S6.E1.m1.3.3.1.1.2.3.3.3">ùëó</ci></apply></apply></apply><apply id="S6.E1.m1.3.3.1.1.1.cmml" xref="S6.E1.m1.3.3.1.1.1"><times id="S6.E1.m1.3.3.1.1.1.2.cmml" xref="S6.E1.m1.3.3.1.1.1.2"></times><cn id="S6.E1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S6.E1.m1.3.3.1.1.1.3">1</cn><apply id="S6.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1"><eq id="S6.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.1"></eq><apply id="S6.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><apply id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2"><ci id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.1">^</ci><ci id="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.2.2.2">ùë°</ci></apply><list id="S6.E1.m1.2.2.2.3.cmml" xref="S6.E1.m1.2.2.2.4"><ci id="S6.E1.m1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1">ùëñ</ci><ci id="S6.E1.m1.2.2.2.2.cmml" xref="S6.E1.m1.2.2.2.2">ùëó</ci></list></apply><apply id="S6.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3.2">ùë°</ci><ci id="S6.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.1.3.3">ùëó</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.3c">q_{j}=\frac{1}{N_{j}}\sum_{i\in S_{j}}\mathbbm{1}(\hat{t}_{i,j}=t_{j})</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.3d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_1 ( over^ start_ARG italic_t end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.SS3.p2.9">where <math alttext="S_{j}" class="ltx_Math" display="inline" id="S6.SS3.p2.2.m1.1"><semantics id="S6.SS3.p2.2.m1.1a"><msub id="S6.SS3.p2.2.m1.1.1" xref="S6.SS3.p2.2.m1.1.1.cmml"><mi id="S6.SS3.p2.2.m1.1.1.2" xref="S6.SS3.p2.2.m1.1.1.2.cmml">S</mi><mi id="S6.SS3.p2.2.m1.1.1.3" xref="S6.SS3.p2.2.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m1.1b"><apply id="S6.SS3.p2.2.m1.1.1.cmml" xref="S6.SS3.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.2.m1.1.1.1.cmml" xref="S6.SS3.p2.2.m1.1.1">subscript</csymbol><ci id="S6.SS3.p2.2.m1.1.1.2.cmml" xref="S6.SS3.p2.2.m1.1.1.2">ùëÜ</ci><ci id="S6.SS3.p2.2.m1.1.1.3.cmml" xref="S6.SS3.p2.2.m1.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m1.1c">S_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.2.m1.1d">italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the set of observers that labeled the <math alttext="j^{th}" class="ltx_Math" display="inline" id="S6.SS3.p2.3.m2.1"><semantics id="S6.SS3.p2.3.m2.1a"><msup id="S6.SS3.p2.3.m2.1.1" xref="S6.SS3.p2.3.m2.1.1.cmml"><mi id="S6.SS3.p2.3.m2.1.1.2" xref="S6.SS3.p2.3.m2.1.1.2.cmml">j</mi><mrow id="S6.SS3.p2.3.m2.1.1.3" xref="S6.SS3.p2.3.m2.1.1.3.cmml"><mi id="S6.SS3.p2.3.m2.1.1.3.2" xref="S6.SS3.p2.3.m2.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p2.3.m2.1.1.3.1" xref="S6.SS3.p2.3.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p2.3.m2.1.1.3.3" xref="S6.SS3.p2.3.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.3.m2.1b"><apply id="S6.SS3.p2.3.m2.1.1.cmml" xref="S6.SS3.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.3.m2.1.1.1.cmml" xref="S6.SS3.p2.3.m2.1.1">superscript</csymbol><ci id="S6.SS3.p2.3.m2.1.1.2.cmml" xref="S6.SS3.p2.3.m2.1.1.2">ùëó</ci><apply id="S6.SS3.p2.3.m2.1.1.3.cmml" xref="S6.SS3.p2.3.m2.1.1.3"><times id="S6.SS3.p2.3.m2.1.1.3.1.cmml" xref="S6.SS3.p2.3.m2.1.1.3.1"></times><ci id="S6.SS3.p2.3.m2.1.1.3.2.cmml" xref="S6.SS3.p2.3.m2.1.1.3.2">ùë°</ci><ci id="S6.SS3.p2.3.m2.1.1.3.3.cmml" xref="S6.SS3.p2.3.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.3.m2.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.3.m2.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> image, <math alttext="\hat{t}_{i,j}" class="ltx_Math" display="inline" id="S6.SS3.p2.4.m3.2"><semantics id="S6.SS3.p2.4.m3.2a"><msub id="S6.SS3.p2.4.m3.2.3" xref="S6.SS3.p2.4.m3.2.3.cmml"><mover accent="true" id="S6.SS3.p2.4.m3.2.3.2" xref="S6.SS3.p2.4.m3.2.3.2.cmml"><mi id="S6.SS3.p2.4.m3.2.3.2.2" xref="S6.SS3.p2.4.m3.2.3.2.2.cmml">t</mi><mo id="S6.SS3.p2.4.m3.2.3.2.1" xref="S6.SS3.p2.4.m3.2.3.2.1.cmml">^</mo></mover><mrow id="S6.SS3.p2.4.m3.2.2.2.4" xref="S6.SS3.p2.4.m3.2.2.2.3.cmml"><mi id="S6.SS3.p2.4.m3.1.1.1.1" xref="S6.SS3.p2.4.m3.1.1.1.1.cmml">i</mi><mo id="S6.SS3.p2.4.m3.2.2.2.4.1" xref="S6.SS3.p2.4.m3.2.2.2.3.cmml">,</mo><mi id="S6.SS3.p2.4.m3.2.2.2.2" xref="S6.SS3.p2.4.m3.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.4.m3.2b"><apply id="S6.SS3.p2.4.m3.2.3.cmml" xref="S6.SS3.p2.4.m3.2.3"><csymbol cd="ambiguous" id="S6.SS3.p2.4.m3.2.3.1.cmml" xref="S6.SS3.p2.4.m3.2.3">subscript</csymbol><apply id="S6.SS3.p2.4.m3.2.3.2.cmml" xref="S6.SS3.p2.4.m3.2.3.2"><ci id="S6.SS3.p2.4.m3.2.3.2.1.cmml" xref="S6.SS3.p2.4.m3.2.3.2.1">^</ci><ci id="S6.SS3.p2.4.m3.2.3.2.2.cmml" xref="S6.SS3.p2.4.m3.2.3.2.2">ùë°</ci></apply><list id="S6.SS3.p2.4.m3.2.2.2.3.cmml" xref="S6.SS3.p2.4.m3.2.2.2.4"><ci id="S6.SS3.p2.4.m3.1.1.1.1.cmml" xref="S6.SS3.p2.4.m3.1.1.1.1">ùëñ</ci><ci id="S6.SS3.p2.4.m3.2.2.2.2.cmml" xref="S6.SS3.p2.4.m3.2.2.2.2">ùëó</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.4.m3.2c">\hat{t}_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.4.m3.2d">over^ start_ARG italic_t end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the label assigned to the <math alttext="j^{th}" class="ltx_Math" display="inline" id="S6.SS3.p2.5.m4.1"><semantics id="S6.SS3.p2.5.m4.1a"><msup id="S6.SS3.p2.5.m4.1.1" xref="S6.SS3.p2.5.m4.1.1.cmml"><mi id="S6.SS3.p2.5.m4.1.1.2" xref="S6.SS3.p2.5.m4.1.1.2.cmml">j</mi><mrow id="S6.SS3.p2.5.m4.1.1.3" xref="S6.SS3.p2.5.m4.1.1.3.cmml"><mi id="S6.SS3.p2.5.m4.1.1.3.2" xref="S6.SS3.p2.5.m4.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p2.5.m4.1.1.3.1" xref="S6.SS3.p2.5.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p2.5.m4.1.1.3.3" xref="S6.SS3.p2.5.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.5.m4.1b"><apply id="S6.SS3.p2.5.m4.1.1.cmml" xref="S6.SS3.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.5.m4.1.1.1.cmml" xref="S6.SS3.p2.5.m4.1.1">superscript</csymbol><ci id="S6.SS3.p2.5.m4.1.1.2.cmml" xref="S6.SS3.p2.5.m4.1.1.2">ùëó</ci><apply id="S6.SS3.p2.5.m4.1.1.3.cmml" xref="S6.SS3.p2.5.m4.1.1.3"><times id="S6.SS3.p2.5.m4.1.1.3.1.cmml" xref="S6.SS3.p2.5.m4.1.1.3.1"></times><ci id="S6.SS3.p2.5.m4.1.1.3.2.cmml" xref="S6.SS3.p2.5.m4.1.1.3.2">ùë°</ci><ci id="S6.SS3.p2.5.m4.1.1.3.3.cmml" xref="S6.SS3.p2.5.m4.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.5.m4.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.5.m4.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> image by the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S6.SS3.p2.6.m5.1"><semantics id="S6.SS3.p2.6.m5.1a"><msup id="S6.SS3.p2.6.m5.1.1" xref="S6.SS3.p2.6.m5.1.1.cmml"><mi id="S6.SS3.p2.6.m5.1.1.2" xref="S6.SS3.p2.6.m5.1.1.2.cmml">i</mi><mrow id="S6.SS3.p2.6.m5.1.1.3" xref="S6.SS3.p2.6.m5.1.1.3.cmml"><mi id="S6.SS3.p2.6.m5.1.1.3.2" xref="S6.SS3.p2.6.m5.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p2.6.m5.1.1.3.1" xref="S6.SS3.p2.6.m5.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p2.6.m5.1.1.3.3" xref="S6.SS3.p2.6.m5.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.6.m5.1b"><apply id="S6.SS3.p2.6.m5.1.1.cmml" xref="S6.SS3.p2.6.m5.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.6.m5.1.1.1.cmml" xref="S6.SS3.p2.6.m5.1.1">superscript</csymbol><ci id="S6.SS3.p2.6.m5.1.1.2.cmml" xref="S6.SS3.p2.6.m5.1.1.2">ùëñ</ci><apply id="S6.SS3.p2.6.m5.1.1.3.cmml" xref="S6.SS3.p2.6.m5.1.1.3"><times id="S6.SS3.p2.6.m5.1.1.3.1.cmml" xref="S6.SS3.p2.6.m5.1.1.3.1"></times><ci id="S6.SS3.p2.6.m5.1.1.3.2.cmml" xref="S6.SS3.p2.6.m5.1.1.3.2">ùë°</ci><ci id="S6.SS3.p2.6.m5.1.1.3.3.cmml" xref="S6.SS3.p2.6.m5.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.6.m5.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.6.m5.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> observer, and <math alttext="t_{j}" class="ltx_Math" display="inline" id="S6.SS3.p2.7.m6.1"><semantics id="S6.SS3.p2.7.m6.1a"><msub id="S6.SS3.p2.7.m6.1.1" xref="S6.SS3.p2.7.m6.1.1.cmml"><mi id="S6.SS3.p2.7.m6.1.1.2" xref="S6.SS3.p2.7.m6.1.1.2.cmml">t</mi><mi id="S6.SS3.p2.7.m6.1.1.3" xref="S6.SS3.p2.7.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.7.m6.1b"><apply id="S6.SS3.p2.7.m6.1.1.cmml" xref="S6.SS3.p2.7.m6.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.7.m6.1.1.1.cmml" xref="S6.SS3.p2.7.m6.1.1">subscript</csymbol><ci id="S6.SS3.p2.7.m6.1.1.2.cmml" xref="S6.SS3.p2.7.m6.1.1.2">ùë°</ci><ci id="S6.SS3.p2.7.m6.1.1.3.cmml" xref="S6.SS3.p2.7.m6.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.7.m6.1c">t_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.7.m6.1d">italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the ground truth label for the <math alttext="j^{th}" class="ltx_Math" display="inline" id="S6.SS3.p2.8.m7.1"><semantics id="S6.SS3.p2.8.m7.1a"><msup id="S6.SS3.p2.8.m7.1.1" xref="S6.SS3.p2.8.m7.1.1.cmml"><mi id="S6.SS3.p2.8.m7.1.1.2" xref="S6.SS3.p2.8.m7.1.1.2.cmml">j</mi><mrow id="S6.SS3.p2.8.m7.1.1.3" xref="S6.SS3.p2.8.m7.1.1.3.cmml"><mi id="S6.SS3.p2.8.m7.1.1.3.2" xref="S6.SS3.p2.8.m7.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p2.8.m7.1.1.3.1" xref="S6.SS3.p2.8.m7.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p2.8.m7.1.1.3.3" xref="S6.SS3.p2.8.m7.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.8.m7.1b"><apply id="S6.SS3.p2.8.m7.1.1.cmml" xref="S6.SS3.p2.8.m7.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.8.m7.1.1.1.cmml" xref="S6.SS3.p2.8.m7.1.1">superscript</csymbol><ci id="S6.SS3.p2.8.m7.1.1.2.cmml" xref="S6.SS3.p2.8.m7.1.1.2">ùëó</ci><apply id="S6.SS3.p2.8.m7.1.1.3.cmml" xref="S6.SS3.p2.8.m7.1.1.3"><times id="S6.SS3.p2.8.m7.1.1.3.1.cmml" xref="S6.SS3.p2.8.m7.1.1.3.1"></times><ci id="S6.SS3.p2.8.m7.1.1.3.2.cmml" xref="S6.SS3.p2.8.m7.1.1.3.2">ùë°</ci><ci id="S6.SS3.p2.8.m7.1.1.3.3.cmml" xref="S6.SS3.p2.8.m7.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.8.m7.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.8.m7.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> image. Then, to calculate average human accuracy, we take the average of <math alttext="q_{j}" class="ltx_Math" display="inline" id="S6.SS3.p2.9.m8.1"><semantics id="S6.SS3.p2.9.m8.1a"><msub id="S6.SS3.p2.9.m8.1.1" xref="S6.SS3.p2.9.m8.1.1.cmml"><mi id="S6.SS3.p2.9.m8.1.1.2" xref="S6.SS3.p2.9.m8.1.1.2.cmml">q</mi><mi id="S6.SS3.p2.9.m8.1.1.3" xref="S6.SS3.p2.9.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.9.m8.1b"><apply id="S6.SS3.p2.9.m8.1.1.cmml" xref="S6.SS3.p2.9.m8.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.9.m8.1.1.1.cmml" xref="S6.SS3.p2.9.m8.1.1">subscript</csymbol><ci id="S6.SS3.p2.9.m8.1.1.2.cmml" xref="S6.SS3.p2.9.m8.1.1.2">ùëû</ci><ci id="S6.SS3.p2.9.m8.1.1.3.cmml" xref="S6.SS3.p2.9.m8.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.9.m8.1c">q_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.9.m8.1d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> values across all images in the IRUO-HTS partition.</p>
</div>
<figure class="ltx_figure" id="S6.F10">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F10.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F10.1.1.1">
<td class="ltx_td ltx_align_center" id="S6.F10.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="379" id="S6.F10.1.1.1.1.g1" src="extracted/5858817/sample_program.png" width="538"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Sample interface from our human study collection program. Human subjects are asked to label images as one of 23 classes listed in the IRUO dataset using an interface following guidelines described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib5" title="">5</a>]</cite></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p3.1.1">Identification of Outlier Observers.</span> Some human observers may consistently obtain lower recognition accuracy than others (e.g., due to fatigue, or misunderstanding directions), implying that they may negatively bias our estimate of optimal recognition. To mitigate this potential problem, we remove human observers from our estimate that consistently perform worse than other observers <span class="ltx_text ltx_font_italic" id="S6.SS3.p3.1.2">when examined on the same imagery</span>. It is crucial to compare humans (or any recognition model) on a shared set of imagery because classification difficulty varies greatly between images. Therefore we compute a ‚Äùnormalized accuracy‚Äù for each observer, which is their accuracy relative to other observers that rated the same imagery. Once we obtain the normalized accuracy for each observer, we use Tukey‚Äôs Rule (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a>) to remove outliers: i.e., observers that consistently perform worse than others over the same imagery.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.7">We define the normalized accuracy of the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S6.SS3.p4.1.m1.1"><semantics id="S6.SS3.p4.1.m1.1a"><msup id="S6.SS3.p4.1.m1.1.1" xref="S6.SS3.p4.1.m1.1.1.cmml"><mi id="S6.SS3.p4.1.m1.1.1.2" xref="S6.SS3.p4.1.m1.1.1.2.cmml">i</mi><mrow id="S6.SS3.p4.1.m1.1.1.3" xref="S6.SS3.p4.1.m1.1.1.3.cmml"><mi id="S6.SS3.p4.1.m1.1.1.3.2" xref="S6.SS3.p4.1.m1.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p4.1.m1.1.1.3.1" xref="S6.SS3.p4.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p4.1.m1.1.1.3.3" xref="S6.SS3.p4.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.1.m1.1b"><apply id="S6.SS3.p4.1.m1.1.1.cmml" xref="S6.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.1.m1.1.1.1.cmml" xref="S6.SS3.p4.1.m1.1.1">superscript</csymbol><ci id="S6.SS3.p4.1.m1.1.1.2.cmml" xref="S6.SS3.p4.1.m1.1.1.2">ùëñ</ci><apply id="S6.SS3.p4.1.m1.1.1.3.cmml" xref="S6.SS3.p4.1.m1.1.1.3"><times id="S6.SS3.p4.1.m1.1.1.3.1.cmml" xref="S6.SS3.p4.1.m1.1.1.3.1"></times><ci id="S6.SS3.p4.1.m1.1.1.3.2.cmml" xref="S6.SS3.p4.1.m1.1.1.3.2">ùë°</ci><ci id="S6.SS3.p4.1.m1.1.1.3.3.cmml" xref="S6.SS3.p4.1.m1.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.1.m1.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.1.m1.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> observer, as <math alttext="a^{A}_{i}=a_{i}-\bar{a}_{i}" class="ltx_Math" display="inline" id="S6.SS3.p4.2.m2.1"><semantics id="S6.SS3.p4.2.m2.1a"><mrow id="S6.SS3.p4.2.m2.1.1" xref="S6.SS3.p4.2.m2.1.1.cmml"><msubsup id="S6.SS3.p4.2.m2.1.1.2" xref="S6.SS3.p4.2.m2.1.1.2.cmml"><mi id="S6.SS3.p4.2.m2.1.1.2.2.2" xref="S6.SS3.p4.2.m2.1.1.2.2.2.cmml">a</mi><mi id="S6.SS3.p4.2.m2.1.1.2.3" xref="S6.SS3.p4.2.m2.1.1.2.3.cmml">i</mi><mi id="S6.SS3.p4.2.m2.1.1.2.2.3" xref="S6.SS3.p4.2.m2.1.1.2.2.3.cmml">A</mi></msubsup><mo id="S6.SS3.p4.2.m2.1.1.1" xref="S6.SS3.p4.2.m2.1.1.1.cmml">=</mo><mrow id="S6.SS3.p4.2.m2.1.1.3" xref="S6.SS3.p4.2.m2.1.1.3.cmml"><msub id="S6.SS3.p4.2.m2.1.1.3.2" xref="S6.SS3.p4.2.m2.1.1.3.2.cmml"><mi id="S6.SS3.p4.2.m2.1.1.3.2.2" xref="S6.SS3.p4.2.m2.1.1.3.2.2.cmml">a</mi><mi id="S6.SS3.p4.2.m2.1.1.3.2.3" xref="S6.SS3.p4.2.m2.1.1.3.2.3.cmml">i</mi></msub><mo id="S6.SS3.p4.2.m2.1.1.3.1" xref="S6.SS3.p4.2.m2.1.1.3.1.cmml">‚àí</mo><msub id="S6.SS3.p4.2.m2.1.1.3.3" xref="S6.SS3.p4.2.m2.1.1.3.3.cmml"><mover accent="true" id="S6.SS3.p4.2.m2.1.1.3.3.2" xref="S6.SS3.p4.2.m2.1.1.3.3.2.cmml"><mi id="S6.SS3.p4.2.m2.1.1.3.3.2.2" xref="S6.SS3.p4.2.m2.1.1.3.3.2.2.cmml">a</mi><mo id="S6.SS3.p4.2.m2.1.1.3.3.2.1" xref="S6.SS3.p4.2.m2.1.1.3.3.2.1.cmml">¬Ø</mo></mover><mi id="S6.SS3.p4.2.m2.1.1.3.3.3" xref="S6.SS3.p4.2.m2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.2.m2.1b"><apply id="S6.SS3.p4.2.m2.1.1.cmml" xref="S6.SS3.p4.2.m2.1.1"><eq id="S6.SS3.p4.2.m2.1.1.1.cmml" xref="S6.SS3.p4.2.m2.1.1.1"></eq><apply id="S6.SS3.p4.2.m2.1.1.2.cmml" xref="S6.SS3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p4.2.m2.1.1.2.1.cmml" xref="S6.SS3.p4.2.m2.1.1.2">subscript</csymbol><apply id="S6.SS3.p4.2.m2.1.1.2.2.cmml" xref="S6.SS3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p4.2.m2.1.1.2.2.1.cmml" xref="S6.SS3.p4.2.m2.1.1.2">superscript</csymbol><ci id="S6.SS3.p4.2.m2.1.1.2.2.2.cmml" xref="S6.SS3.p4.2.m2.1.1.2.2.2">ùëé</ci><ci id="S6.SS3.p4.2.m2.1.1.2.2.3.cmml" xref="S6.SS3.p4.2.m2.1.1.2.2.3">ùê¥</ci></apply><ci id="S6.SS3.p4.2.m2.1.1.2.3.cmml" xref="S6.SS3.p4.2.m2.1.1.2.3">ùëñ</ci></apply><apply id="S6.SS3.p4.2.m2.1.1.3.cmml" xref="S6.SS3.p4.2.m2.1.1.3"><minus id="S6.SS3.p4.2.m2.1.1.3.1.cmml" xref="S6.SS3.p4.2.m2.1.1.3.1"></minus><apply id="S6.SS3.p4.2.m2.1.1.3.2.cmml" xref="S6.SS3.p4.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S6.SS3.p4.2.m2.1.1.3.2.1.cmml" xref="S6.SS3.p4.2.m2.1.1.3.2">subscript</csymbol><ci id="S6.SS3.p4.2.m2.1.1.3.2.2.cmml" xref="S6.SS3.p4.2.m2.1.1.3.2.2">ùëé</ci><ci id="S6.SS3.p4.2.m2.1.1.3.2.3.cmml" xref="S6.SS3.p4.2.m2.1.1.3.2.3">ùëñ</ci></apply><apply id="S6.SS3.p4.2.m2.1.1.3.3.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S6.SS3.p4.2.m2.1.1.3.3.1.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3">subscript</csymbol><apply id="S6.SS3.p4.2.m2.1.1.3.3.2.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3.2"><ci id="S6.SS3.p4.2.m2.1.1.3.3.2.1.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3.2.1">¬Ø</ci><ci id="S6.SS3.p4.2.m2.1.1.3.3.2.2.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3.2.2">ùëé</ci></apply><ci id="S6.SS3.p4.2.m2.1.1.3.3.3.cmml" xref="S6.SS3.p4.2.m2.1.1.3.3.3">ùëñ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.2.m2.1c">a^{A}_{i}=a_{i}-\bar{a}_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.2.m2.1d">italic_a start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¬Ø start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="a_{i}" class="ltx_Math" display="inline" id="S6.SS3.p4.3.m3.1"><semantics id="S6.SS3.p4.3.m3.1a"><msub id="S6.SS3.p4.3.m3.1.1" xref="S6.SS3.p4.3.m3.1.1.cmml"><mi id="S6.SS3.p4.3.m3.1.1.2" xref="S6.SS3.p4.3.m3.1.1.2.cmml">a</mi><mi id="S6.SS3.p4.3.m3.1.1.3" xref="S6.SS3.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.3.m3.1b"><apply id="S6.SS3.p4.3.m3.1.1.cmml" xref="S6.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.3.m3.1.1.1.cmml" xref="S6.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S6.SS3.p4.3.m3.1.1.2.cmml" xref="S6.SS3.p4.3.m3.1.1.2">ùëé</ci><ci id="S6.SS3.p4.3.m3.1.1.3.cmml" xref="S6.SS3.p4.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.3.m3.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.3.m3.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the proportion of top-1 accuracy of the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S6.SS3.p4.4.m4.1"><semantics id="S6.SS3.p4.4.m4.1a"><msup id="S6.SS3.p4.4.m4.1.1" xref="S6.SS3.p4.4.m4.1.1.cmml"><mi id="S6.SS3.p4.4.m4.1.1.2" xref="S6.SS3.p4.4.m4.1.1.2.cmml">i</mi><mrow id="S6.SS3.p4.4.m4.1.1.3" xref="S6.SS3.p4.4.m4.1.1.3.cmml"><mi id="S6.SS3.p4.4.m4.1.1.3.2" xref="S6.SS3.p4.4.m4.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p4.4.m4.1.1.3.1" xref="S6.SS3.p4.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p4.4.m4.1.1.3.3" xref="S6.SS3.p4.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.4.m4.1b"><apply id="S6.SS3.p4.4.m4.1.1.cmml" xref="S6.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.4.m4.1.1.1.cmml" xref="S6.SS3.p4.4.m4.1.1">superscript</csymbol><ci id="S6.SS3.p4.4.m4.1.1.2.cmml" xref="S6.SS3.p4.4.m4.1.1.2">ùëñ</ci><apply id="S6.SS3.p4.4.m4.1.1.3.cmml" xref="S6.SS3.p4.4.m4.1.1.3"><times id="S6.SS3.p4.4.m4.1.1.3.1.cmml" xref="S6.SS3.p4.4.m4.1.1.3.1"></times><ci id="S6.SS3.p4.4.m4.1.1.3.2.cmml" xref="S6.SS3.p4.4.m4.1.1.3.2">ùë°</ci><ci id="S6.SS3.p4.4.m4.1.1.3.3.cmml" xref="S6.SS3.p4.4.m4.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.4.m4.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.4.m4.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> observer, and <math alttext="\bar{a}_{i}" class="ltx_Math" display="inline" id="S6.SS3.p4.5.m5.1"><semantics id="S6.SS3.p4.5.m5.1a"><msub id="S6.SS3.p4.5.m5.1.1" xref="S6.SS3.p4.5.m5.1.1.cmml"><mover accent="true" id="S6.SS3.p4.5.m5.1.1.2" xref="S6.SS3.p4.5.m5.1.1.2.cmml"><mi id="S6.SS3.p4.5.m5.1.1.2.2" xref="S6.SS3.p4.5.m5.1.1.2.2.cmml">a</mi><mo id="S6.SS3.p4.5.m5.1.1.2.1" xref="S6.SS3.p4.5.m5.1.1.2.1.cmml">¬Ø</mo></mover><mi id="S6.SS3.p4.5.m5.1.1.3" xref="S6.SS3.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.5.m5.1b"><apply id="S6.SS3.p4.5.m5.1.1.cmml" xref="S6.SS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.5.m5.1.1.1.cmml" xref="S6.SS3.p4.5.m5.1.1">subscript</csymbol><apply id="S6.SS3.p4.5.m5.1.1.2.cmml" xref="S6.SS3.p4.5.m5.1.1.2"><ci id="S6.SS3.p4.5.m5.1.1.2.1.cmml" xref="S6.SS3.p4.5.m5.1.1.2.1">¬Ø</ci><ci id="S6.SS3.p4.5.m5.1.1.2.2.cmml" xref="S6.SS3.p4.5.m5.1.1.2.2">ùëé</ci></apply><ci id="S6.SS3.p4.5.m5.1.1.3.cmml" xref="S6.SS3.p4.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.5.m5.1c">\bar{a}_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.5.m5.1d">over¬Ø start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is an estimate of the average top-1 accuracy of all observers on the subset of 200 images shown to the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S6.SS3.p4.6.m6.1"><semantics id="S6.SS3.p4.6.m6.1a"><msup id="S6.SS3.p4.6.m6.1.1" xref="S6.SS3.p4.6.m6.1.1.cmml"><mi id="S6.SS3.p4.6.m6.1.1.2" xref="S6.SS3.p4.6.m6.1.1.2.cmml">i</mi><mrow id="S6.SS3.p4.6.m6.1.1.3" xref="S6.SS3.p4.6.m6.1.1.3.cmml"><mi id="S6.SS3.p4.6.m6.1.1.3.2" xref="S6.SS3.p4.6.m6.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p4.6.m6.1.1.3.1" xref="S6.SS3.p4.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p4.6.m6.1.1.3.3" xref="S6.SS3.p4.6.m6.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.6.m6.1b"><apply id="S6.SS3.p4.6.m6.1.1.cmml" xref="S6.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.6.m6.1.1.1.cmml" xref="S6.SS3.p4.6.m6.1.1">superscript</csymbol><ci id="S6.SS3.p4.6.m6.1.1.2.cmml" xref="S6.SS3.p4.6.m6.1.1.2">ùëñ</ci><apply id="S6.SS3.p4.6.m6.1.1.3.cmml" xref="S6.SS3.p4.6.m6.1.1.3"><times id="S6.SS3.p4.6.m6.1.1.3.1.cmml" xref="S6.SS3.p4.6.m6.1.1.3.1"></times><ci id="S6.SS3.p4.6.m6.1.1.3.2.cmml" xref="S6.SS3.p4.6.m6.1.1.3.2">ùë°</ci><ci id="S6.SS3.p4.6.m6.1.1.3.3.cmml" xref="S6.SS3.p4.6.m6.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.6.m6.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.6.m6.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> observer. We compute <math alttext="\bar{a}_{i}" class="ltx_Math" display="inline" id="S6.SS3.p4.7.m7.1"><semantics id="S6.SS3.p4.7.m7.1a"><msub id="S6.SS3.p4.7.m7.1.1" xref="S6.SS3.p4.7.m7.1.1.cmml"><mover accent="true" id="S6.SS3.p4.7.m7.1.1.2" xref="S6.SS3.p4.7.m7.1.1.2.cmml"><mi id="S6.SS3.p4.7.m7.1.1.2.2" xref="S6.SS3.p4.7.m7.1.1.2.2.cmml">a</mi><mo id="S6.SS3.p4.7.m7.1.1.2.1" xref="S6.SS3.p4.7.m7.1.1.2.1.cmml">¬Ø</mo></mover><mi id="S6.SS3.p4.7.m7.1.1.3" xref="S6.SS3.p4.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.7.m7.1b"><apply id="S6.SS3.p4.7.m7.1.1.cmml" xref="S6.SS3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.7.m7.1.1.1.cmml" xref="S6.SS3.p4.7.m7.1.1">subscript</csymbol><apply id="S6.SS3.p4.7.m7.1.1.2.cmml" xref="S6.SS3.p4.7.m7.1.1.2"><ci id="S6.SS3.p4.7.m7.1.1.2.1.cmml" xref="S6.SS3.p4.7.m7.1.1.2.1">¬Ø</ci><ci id="S6.SS3.p4.7.m7.1.1.2.2.cmml" xref="S6.SS3.p4.7.m7.1.1.2.2">ùëé</ci></apply><ci id="S6.SS3.p4.7.m7.1.1.3.cmml" xref="S6.SS3.p4.7.m7.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.7.m7.1c">\bar{a}_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.7.m7.1d">over¬Ø start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\bar{a}_{i}=\frac{1}{200}\sum_{j\in S_{i}}{q_{j}}" class="ltx_Math" display="block" id="S6.E2.m1.1"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><msub id="S6.E2.m1.1.1.2" xref="S6.E2.m1.1.1.2.cmml"><mover accent="true" id="S6.E2.m1.1.1.2.2" xref="S6.E2.m1.1.1.2.2.cmml"><mi id="S6.E2.m1.1.1.2.2.2" xref="S6.E2.m1.1.1.2.2.2.cmml">a</mi><mo id="S6.E2.m1.1.1.2.2.1" xref="S6.E2.m1.1.1.2.2.1.cmml">¬Ø</mo></mover><mi id="S6.E2.m1.1.1.2.3" xref="S6.E2.m1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.cmml">=</mo><mrow id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml"><mfrac id="S6.E2.m1.1.1.3.2" xref="S6.E2.m1.1.1.3.2.cmml"><mn id="S6.E2.m1.1.1.3.2.2" xref="S6.E2.m1.1.1.3.2.2.cmml">1</mn><mn id="S6.E2.m1.1.1.3.2.3" xref="S6.E2.m1.1.1.3.2.3.cmml">200</mn></mfrac><mo id="S6.E2.m1.1.1.3.1" xref="S6.E2.m1.1.1.3.1.cmml">‚Å¢</mo><mrow id="S6.E2.m1.1.1.3.3" xref="S6.E2.m1.1.1.3.3.cmml"><munder id="S6.E2.m1.1.1.3.3.1" xref="S6.E2.m1.1.1.3.3.1.cmml"><mo id="S6.E2.m1.1.1.3.3.1.2" movablelimits="false" xref="S6.E2.m1.1.1.3.3.1.2.cmml">‚àë</mo><mrow id="S6.E2.m1.1.1.3.3.1.3" xref="S6.E2.m1.1.1.3.3.1.3.cmml"><mi id="S6.E2.m1.1.1.3.3.1.3.2" xref="S6.E2.m1.1.1.3.3.1.3.2.cmml">j</mi><mo id="S6.E2.m1.1.1.3.3.1.3.1" xref="S6.E2.m1.1.1.3.3.1.3.1.cmml">‚àà</mo><msub id="S6.E2.m1.1.1.3.3.1.3.3" xref="S6.E2.m1.1.1.3.3.1.3.3.cmml"><mi id="S6.E2.m1.1.1.3.3.1.3.3.2" xref="S6.E2.m1.1.1.3.3.1.3.3.2.cmml">S</mi><mi id="S6.E2.m1.1.1.3.3.1.3.3.3" xref="S6.E2.m1.1.1.3.3.1.3.3.3.cmml">i</mi></msub></mrow></munder><msub id="S6.E2.m1.1.1.3.3.2" xref="S6.E2.m1.1.1.3.3.2.cmml"><mi id="S6.E2.m1.1.1.3.3.2.2" xref="S6.E2.m1.1.1.3.3.2.2.cmml">q</mi><mi id="S6.E2.m1.1.1.3.3.2.3" xref="S6.E2.m1.1.1.3.3.2.3.cmml">j</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><eq id="S6.E2.m1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"></eq><apply id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.2">subscript</csymbol><apply id="S6.E2.m1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.2.2"><ci id="S6.E2.m1.1.1.2.2.1.cmml" xref="S6.E2.m1.1.1.2.2.1">¬Ø</ci><ci id="S6.E2.m1.1.1.2.2.2.cmml" xref="S6.E2.m1.1.1.2.2.2">ùëé</ci></apply><ci id="S6.E2.m1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.2.3">ùëñ</ci></apply><apply id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3"><times id="S6.E2.m1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.3.1"></times><apply id="S6.E2.m1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.3.2"><divide id="S6.E2.m1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.3.2"></divide><cn id="S6.E2.m1.1.1.3.2.2.cmml" type="integer" xref="S6.E2.m1.1.1.3.2.2">1</cn><cn id="S6.E2.m1.1.1.3.2.3.cmml" type="integer" xref="S6.E2.m1.1.1.3.2.3">200</cn></apply><apply id="S6.E2.m1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.3.3"><apply id="S6.E2.m1.1.1.3.3.1.cmml" xref="S6.E2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.1.1.cmml" xref="S6.E2.m1.1.1.3.3.1">subscript</csymbol><sum id="S6.E2.m1.1.1.3.3.1.2.cmml" xref="S6.E2.m1.1.1.3.3.1.2"></sum><apply id="S6.E2.m1.1.1.3.3.1.3.cmml" xref="S6.E2.m1.1.1.3.3.1.3"><in id="S6.E2.m1.1.1.3.3.1.3.1.cmml" xref="S6.E2.m1.1.1.3.3.1.3.1"></in><ci id="S6.E2.m1.1.1.3.3.1.3.2.cmml" xref="S6.E2.m1.1.1.3.3.1.3.2">ùëó</ci><apply id="S6.E2.m1.1.1.3.3.1.3.3.cmml" xref="S6.E2.m1.1.1.3.3.1.3.3"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.1.3.3.1.cmml" xref="S6.E2.m1.1.1.3.3.1.3.3">subscript</csymbol><ci id="S6.E2.m1.1.1.3.3.1.3.3.2.cmml" xref="S6.E2.m1.1.1.3.3.1.3.3.2">ùëÜ</ci><ci id="S6.E2.m1.1.1.3.3.1.3.3.3.cmml" xref="S6.E2.m1.1.1.3.3.1.3.3.3">ùëñ</ci></apply></apply></apply><apply id="S6.E2.m1.1.1.3.3.2.cmml" xref="S6.E2.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.2.1.cmml" xref="S6.E2.m1.1.1.3.3.2">subscript</csymbol><ci id="S6.E2.m1.1.1.3.3.2.2.cmml" xref="S6.E2.m1.1.1.3.3.2.2">ùëû</ci><ci id="S6.E2.m1.1.1.3.3.2.3.cmml" xref="S6.E2.m1.1.1.3.3.2.3">ùëó</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">\bar{a}_{i}=\frac{1}{200}\sum_{j\in S_{i}}{q_{j}}</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.1d">over¬Ø start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 200 end_ARG ‚àë start_POSTSUBSCRIPT italic_j ‚àà italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.SS3.p4.12">where <math alttext="S_{i}" class="ltx_Math" display="inline" id="S6.SS3.p4.8.m1.1"><semantics id="S6.SS3.p4.8.m1.1a"><msub id="S6.SS3.p4.8.m1.1.1" xref="S6.SS3.p4.8.m1.1.1.cmml"><mi id="S6.SS3.p4.8.m1.1.1.2" xref="S6.SS3.p4.8.m1.1.1.2.cmml">S</mi><mi id="S6.SS3.p4.8.m1.1.1.3" xref="S6.SS3.p4.8.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.8.m1.1b"><apply id="S6.SS3.p4.8.m1.1.1.cmml" xref="S6.SS3.p4.8.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.8.m1.1.1.1.cmml" xref="S6.SS3.p4.8.m1.1.1">subscript</csymbol><ci id="S6.SS3.p4.8.m1.1.1.2.cmml" xref="S6.SS3.p4.8.m1.1.1.2">ùëÜ</ci><ci id="S6.SS3.p4.8.m1.1.1.3.cmml" xref="S6.SS3.p4.8.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.8.m1.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.8.m1.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the subset of 200 images shown to the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S6.SS3.p4.9.m2.1"><semantics id="S6.SS3.p4.9.m2.1a"><msup id="S6.SS3.p4.9.m2.1.1" xref="S6.SS3.p4.9.m2.1.1.cmml"><mi id="S6.SS3.p4.9.m2.1.1.2" xref="S6.SS3.p4.9.m2.1.1.2.cmml">i</mi><mrow id="S6.SS3.p4.9.m2.1.1.3" xref="S6.SS3.p4.9.m2.1.1.3.cmml"><mi id="S6.SS3.p4.9.m2.1.1.3.2" xref="S6.SS3.p4.9.m2.1.1.3.2.cmml">t</mi><mo id="S6.SS3.p4.9.m2.1.1.3.1" xref="S6.SS3.p4.9.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S6.SS3.p4.9.m2.1.1.3.3" xref="S6.SS3.p4.9.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.9.m2.1b"><apply id="S6.SS3.p4.9.m2.1.1.cmml" xref="S6.SS3.p4.9.m2.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.9.m2.1.1.1.cmml" xref="S6.SS3.p4.9.m2.1.1">superscript</csymbol><ci id="S6.SS3.p4.9.m2.1.1.2.cmml" xref="S6.SS3.p4.9.m2.1.1.2">ùëñ</ci><apply id="S6.SS3.p4.9.m2.1.1.3.cmml" xref="S6.SS3.p4.9.m2.1.1.3"><times id="S6.SS3.p4.9.m2.1.1.3.1.cmml" xref="S6.SS3.p4.9.m2.1.1.3.1"></times><ci id="S6.SS3.p4.9.m2.1.1.3.2.cmml" xref="S6.SS3.p4.9.m2.1.1.3.2">ùë°</ci><ci id="S6.SS3.p4.9.m2.1.1.3.3.cmml" xref="S6.SS3.p4.9.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.9.m2.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.9.m2.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> observer, and <math alttext="q_{j}" class="ltx_Math" display="inline" id="S6.SS3.p4.10.m3.1"><semantics id="S6.SS3.p4.10.m3.1a"><msub id="S6.SS3.p4.10.m3.1.1" xref="S6.SS3.p4.10.m3.1.1.cmml"><mi id="S6.SS3.p4.10.m3.1.1.2" xref="S6.SS3.p4.10.m3.1.1.2.cmml">q</mi><mi id="S6.SS3.p4.10.m3.1.1.3" xref="S6.SS3.p4.10.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.10.m3.1b"><apply id="S6.SS3.p4.10.m3.1.1.cmml" xref="S6.SS3.p4.10.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.10.m3.1.1.1.cmml" xref="S6.SS3.p4.10.m3.1.1">subscript</csymbol><ci id="S6.SS3.p4.10.m3.1.1.2.cmml" xref="S6.SS3.p4.10.m3.1.1.2">ùëû</ci><ci id="S6.SS3.p4.10.m3.1.1.3.cmml" xref="S6.SS3.p4.10.m3.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.10.m3.1c">q_{j}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.10.m3.1d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the human accuracy for image <math alttext="j" class="ltx_Math" display="inline" id="S6.SS3.p4.11.m4.1"><semantics id="S6.SS3.p4.11.m4.1a"><mi id="S6.SS3.p4.11.m4.1.1" xref="S6.SS3.p4.11.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.11.m4.1b"><ci id="S6.SS3.p4.11.m4.1.1.cmml" xref="S6.SS3.p4.11.m4.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.11.m4.1c">j</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.11.m4.1d">italic_j</annotation></semantics></math>, as defined in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.E1" title="In 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">1</span></a>. We then apply Tukey‚Äôs rule to identify and remove outliers from the set of normalized accuracy values for our 20 observers, denoted <math alttext="S^{A}={\{s^{A}_{i}\}}_{i=1}^{20}" class="ltx_Math" display="inline" id="S6.SS3.p4.12.m5.1"><semantics id="S6.SS3.p4.12.m5.1a"><mrow id="S6.SS3.p4.12.m5.1.1" xref="S6.SS3.p4.12.m5.1.1.cmml"><msup id="S6.SS3.p4.12.m5.1.1.3" xref="S6.SS3.p4.12.m5.1.1.3.cmml"><mi id="S6.SS3.p4.12.m5.1.1.3.2" xref="S6.SS3.p4.12.m5.1.1.3.2.cmml">S</mi><mi id="S6.SS3.p4.12.m5.1.1.3.3" xref="S6.SS3.p4.12.m5.1.1.3.3.cmml">A</mi></msup><mo id="S6.SS3.p4.12.m5.1.1.2" xref="S6.SS3.p4.12.m5.1.1.2.cmml">=</mo><msubsup id="S6.SS3.p4.12.m5.1.1.1" xref="S6.SS3.p4.12.m5.1.1.1.cmml"><mrow id="S6.SS3.p4.12.m5.1.1.1.1.1.1" xref="S6.SS3.p4.12.m5.1.1.1.1.1.2.cmml"><mo id="S6.SS3.p4.12.m5.1.1.1.1.1.1.2" stretchy="false" xref="S6.SS3.p4.12.m5.1.1.1.1.1.2.cmml">{</mo><msubsup id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.cmml"><mi id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.2" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.2.cmml">s</mi><mi id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.3" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.3.cmml">i</mi><mi id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.3" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.3.cmml">A</mi></msubsup><mo id="S6.SS3.p4.12.m5.1.1.1.1.1.1.3" stretchy="false" xref="S6.SS3.p4.12.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS3.p4.12.m5.1.1.1.1.3" xref="S6.SS3.p4.12.m5.1.1.1.1.3.cmml"><mi id="S6.SS3.p4.12.m5.1.1.1.1.3.2" xref="S6.SS3.p4.12.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S6.SS3.p4.12.m5.1.1.1.1.3.1" xref="S6.SS3.p4.12.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS3.p4.12.m5.1.1.1.1.3.3" xref="S6.SS3.p4.12.m5.1.1.1.1.3.3.cmml">1</mn></mrow><mn id="S6.SS3.p4.12.m5.1.1.1.3" xref="S6.SS3.p4.12.m5.1.1.1.3.cmml">20</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.12.m5.1b"><apply id="S6.SS3.p4.12.m5.1.1.cmml" xref="S6.SS3.p4.12.m5.1.1"><eq id="S6.SS3.p4.12.m5.1.1.2.cmml" xref="S6.SS3.p4.12.m5.1.1.2"></eq><apply id="S6.SS3.p4.12.m5.1.1.3.cmml" xref="S6.SS3.p4.12.m5.1.1.3"><csymbol cd="ambiguous" id="S6.SS3.p4.12.m5.1.1.3.1.cmml" xref="S6.SS3.p4.12.m5.1.1.3">superscript</csymbol><ci id="S6.SS3.p4.12.m5.1.1.3.2.cmml" xref="S6.SS3.p4.12.m5.1.1.3.2">ùëÜ</ci><ci id="S6.SS3.p4.12.m5.1.1.3.3.cmml" xref="S6.SS3.p4.12.m5.1.1.3.3">ùê¥</ci></apply><apply id="S6.SS3.p4.12.m5.1.1.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.12.m5.1.1.1.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1">superscript</csymbol><apply id="S6.SS3.p4.12.m5.1.1.1.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.12.m5.1.1.1.1.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1">subscript</csymbol><set id="S6.SS3.p4.12.m5.1.1.1.1.1.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1"><apply id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1">subscript</csymbol><apply id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1">superscript</csymbol><ci id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.2">ùë†</ci><ci id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.3.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.2.3">ùê¥</ci></apply><ci id="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.1.1.1.3">ùëñ</ci></apply></set><apply id="S6.SS3.p4.12.m5.1.1.1.1.3.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.3"><eq id="S6.SS3.p4.12.m5.1.1.1.1.3.1.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.3.1"></eq><ci id="S6.SS3.p4.12.m5.1.1.1.1.3.2.cmml" xref="S6.SS3.p4.12.m5.1.1.1.1.3.2">ùëñ</ci><cn id="S6.SS3.p4.12.m5.1.1.1.1.3.3.cmml" type="integer" xref="S6.SS3.p4.12.m5.1.1.1.1.3.3">1</cn></apply></apply><cn id="S6.SS3.p4.12.m5.1.1.1.3.cmml" type="integer" xref="S6.SS3.p4.12.m5.1.1.1.3">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.12.m5.1c">S^{A}={\{s^{A}_{i}\}}_{i=1}^{20}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.12.m5.1d">italic_S start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT = { italic_s start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT</annotation></semantics></math>. This resulted in the detection and removal of the two lowest-performing participants from the study.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:433.6pt;height:215.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.6pt,45.1pt) scale(0.70539041192272,0.70539041192272) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S6.T3.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.1.1.2" rowspan="2"><span class="ltx_text" id="S6.T3.1.1.1.1.2.1">Augmentation</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S6.T3.1.1.1.1.3">Accuracy: occlusion level</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.2.2.1">0</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.2.2.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.2.2.3">2</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.3.3.1">VGG-16 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.3.3.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.3">0.755</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.4">0.674</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.5">0.554</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.4.4.1">CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.4.4.2">-</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.3">0.679</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.4">0.543</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.5">0.440</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.5.5.1">ViT-B (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.5.5.2">-</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.5.3">0.674</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.5.4">0.663</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.5.5">0.614</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.6.6.1">ResNet-50 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib9" title="">9</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.6.6.2">-</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.3">0.761</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.4">0.696</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.5">0.587</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.7.7.1" rowspan="4"><span class="ltx_text" id="S6.T3.1.1.7.7.1.1">ResNeXt-50 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.7.7.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.7.7.3">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.7.7.4">0.690</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.7.7.5">0.620</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.8.8.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.2">0.772</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.3">0.658</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.4">0.576</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.9.9.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.9.2">0.772</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.9.3">0.636</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.9.9.4">0.582</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.10.10.1">Deep Feature (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.2">0.734</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.3">0.668</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.4">0.576</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.11.11.1" rowspan="3"><span class="ltx_text" id="S6.T3.1.1.11.11.1.1">DeiT-B (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.11.11.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.11.11.3">0.842</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.11.11.4">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.11.11.5">0.723</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.12.12.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.12.2">0.880</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.12.3">0.821</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.12.12.4">0.723</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.13.13.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>))</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.2">0.886</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.3">0.832</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.4">0.734</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.14.14.1" rowspan="3"><span class="ltx_text" id="S6.T3.1.1.14.14.1.1">Swin-B (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.1.14.14.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.14.14.3">0.918</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.14.14.4">0.826</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.14.14.5">0.744</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.15.15.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.15.15.2.1">0.940</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.15.15.3.1">0.837</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.15.15.4.1">0.772</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T3.1.1.16.16.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.16.16.2">0.929</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.16.16.3">0.815</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.16.16.4">0.755</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S6.T3.1.1.17.17.1">Human (average, outliers removed)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S6.T3.1.1.17.17.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.17.17.3">0.947</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.17.17.4">0.862</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.17.17.5">0.800</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of models and humans on IRUO-HTS. Outliers are removed according to the procedure in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The entry labeled ‚ÄùHuman (top responses)‚Äù indicates the average score of the top nine respondents</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F11">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F11.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F11.1.1.1">
<td class="ltx_td ltx_align_center" id="S6.F11.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="139" id="S6.F11.1.1.1.1.g1" src="extracted/5858817/analysis_human_swin.png" width="580"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Accuracy per level of occlusion on the partition of IRUO containing real occlusion, measured as proportion of correct top-1 classifications for each model at each occlusion level, considering <span class="ltx_text ltx_font_italic" id="S6.F11.3.1">only</span> images seen by the highest-scoring human respondents. Occlusion level 0 corresponds to no occlusion, 1 corresponds to 0-50% occlusion, and 2 corresponds to &gt;50% occlusion. Classification level is defined as the tree depth of each IRUO class listed in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F5" title="Figure 5 ‚Ä£ 5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>. For this set of humans, outliers are removed according to the procedure in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a>. Full results, including all models and augmentations, evaluated on the IRUO subset (at classification level 5) shown to humans, are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.T3" title="Table 3 ‚Ä£ 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3</span></a></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p5">
<p class="ltx_p" id="S6.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p5.1.1">Results.</span> The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.T3" title="Table 3 ‚Ä£ 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3</span></a>, where we report the accuracy of all benchmark models, as well as an average of the human observers (with outliers removed), on the IRUO-HTS dataset. Among the benchmark models, the results are similar to those of Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS1" title="6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.1</span></a> with the larger IRUO dataset, and the overall conclusions are the same. In particular, the Swin model with Mixup (denoted Swin-Mixup here) achieves the best accuracy among DNN-based models across all levels of occlusion, including none. Therefore we treat Swin-Mixup as a representative of state-of-the-art DNN-based models for both occluded and non-occluded tasks, and focus our discussion on comparing Swin-Mixup against human-level accuracy. The results indicate that the Swin-Mixup model obtains nearly the same accuracy as humans on unoccluded imagery, which is consistent with recent comparisons human and DNN image recognition accuracy (e.g., in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S6.SS3.p6">
<p class="ltx_p" id="S6.SS3.p6.2">However, when occlusions are present in the imagery, we find that humans outperform Swin-B with Mixup (and therefore all of our DNN-based models), indicating that DNN-based models are not yet fully occlusion-robust. However, surprisingly, we find that the performance gap between Swin-Mixup and humans is relatively modest: approximately <math alttext="3\%" class="ltx_Math" display="inline" id="S6.SS3.p6.1.m1.1"><semantics id="S6.SS3.p6.1.m1.1a"><mrow id="S6.SS3.p6.1.m1.1.1" xref="S6.SS3.p6.1.m1.1.1.cmml"><mn id="S6.SS3.p6.1.m1.1.1.2" xref="S6.SS3.p6.1.m1.1.1.2.cmml">3</mn><mo id="S6.SS3.p6.1.m1.1.1.1" xref="S6.SS3.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p6.1.m1.1b"><apply id="S6.SS3.p6.1.m1.1.1.cmml" xref="S6.SS3.p6.1.m1.1.1"><csymbol cd="latexml" id="S6.SS3.p6.1.m1.1.1.1.cmml" xref="S6.SS3.p6.1.m1.1.1.1">percent</csymbol><cn id="S6.SS3.p6.1.m1.1.1.2.cmml" type="integer" xref="S6.SS3.p6.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p6.1.m1.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p6.1.m1.1d">3 %</annotation></semantics></math> for Level 1, and <math alttext="3.5\%" class="ltx_Math" display="inline" id="S6.SS3.p6.2.m2.1"><semantics id="S6.SS3.p6.2.m2.1a"><mrow id="S6.SS3.p6.2.m2.1.1" xref="S6.SS3.p6.2.m2.1.1.cmml"><mn id="S6.SS3.p6.2.m2.1.1.2" xref="S6.SS3.p6.2.m2.1.1.2.cmml">3.5</mn><mo id="S6.SS3.p6.2.m2.1.1.1" xref="S6.SS3.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p6.2.m2.1b"><apply id="S6.SS3.p6.2.m2.1.1.cmml" xref="S6.SS3.p6.2.m2.1.1"><csymbol cd="latexml" id="S6.SS3.p6.2.m2.1.1.1.cmml" xref="S6.SS3.p6.2.m2.1.1.1">percent</csymbol><cn id="S6.SS3.p6.2.m2.1.1.2.cmml" type="float" xref="S6.SS3.p6.2.m2.1.1.2">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p6.2.m2.1c">3.5\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p6.2.m2.1d">3.5 %</annotation></semantics></math> for Level 2. The performance gaps are somewhat larger for other variants of Swin (e.g., without augmentation, or with CutMix), however, the gaps are still relatively small. The results therefore suggest that modeling improvements are possible; however, the potential benefits are relatively modest when considering state-of-the-art models, such as vision transformers. Crucially, these results provide some insight into precisely what level of improvements can be expected when investing in further methodological improvements, and our IRUO dataset provides a benchmark to assess the effectiveness of new models.</p>
</div>
<div class="ltx_para" id="S6.SS3.p7">
<p class="ltx_p" id="S6.SS3.p7.1">One potential explanation for the relatively small human-to-DNN performance gap may be that the number of classes in IRUO is large, and fairly nuanced: e.g., with classes as specific as <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.1">bird</span>, <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.2">parrot</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.3">poultry</span>. Due to their limited knowledge and/or memory capacity, humans may be unable to remember the precise visual differences between some target classes, leading to underestimation of the optimal recognition accuracy. To exclude this potential problem, we re-score the labels of both Swin-B and Mixup at varying levels of our classification hierarchy defined in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS2" title="5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5.2</span></a>, with the full class hierarchy of IRUO in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F5" title="Figure 5 ‚Ä£ 5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>. For each level, we re-score human and model predictions, combining high-level predictions into single labels as the classification level decreases, e.g., to score at level 2, we combine labels for <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.4">bird</span>, <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.5">parrot</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.6">poultry</span> into a single <span class="ltx_text ltx_font_italic" id="S6.SS3.p7.1.7">bird</span> label and re-score.The re-scored results for Swin-Mixup and human observers at each classification level are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F11" title="Figure 11 ‚Ä£ 6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">11</span></a>. The difference between accuracy achieved by Swin-Mixup and humans is not substantially different as we vary the hierarchy level, suggesting that our human population was not significantly disadvantaged when we use the highest level of the hierarchy (i.e., the most granular class labels).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Further analysis: are models robust to diffuse occlusion?</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">The results in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.SS2" title="5.2 Testing Protocol ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5.2</span></a> suggest that contemporary DNNs (e.g., Swin) achieve nearly the same accuracy under occlusion as humans. In this section we investigate whether DNN robustness depends upon the <span class="ltx_text ltx_font_italic" id="S6.SS4.p1.1.1">properties</span> of the occluding objects. Our investigation reveals that DNN occlusion-robustness depends strongly upon at least one property, which we term ‚Äùdiffuseness‚Äù. We define diffuseness as the average proportion of neighbors of occluding pixels that are <span class="ltx_text ltx_font_bold" id="S6.SS4.p1.1.2">not</span> occluders. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F13" title="Figure 13 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">13</span></a> presents three images with synthetic occlusions, and where the occlusions have varying levels of diffuseness (e.g., (a) is most, while (b) is least). Our primary hypothesis is that the occlusion-robustness of DNNs reduces as diffuseness increases, suggesting that there is still substantial potential to improve DNNs for some types of occlusion.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p2.1.1">Experimental Design.</span> We conduct two experiments to evaluate the effect of diffuse occluders on the occlusion-robustness of DNNs. To conduct these experiments, we leverage our findings in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a> where we found that synthetic occlusions are a reasonable substitute for real-world occlusions if we wish to rank-order the performance of models under occlusion. Using this finding, we created an additional partition of the IRUO dataset, termed <span class="ltx_text ltx_font_italic" id="S6.SS4.p2.1.2">IRUO-Diffuse</span> (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S4" title="4 The Image Recognition Under Occlusion (IRUO) Dataset ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">4</span></a>), wherein we generate several types of synthetic occlusions with varying types and levels of ‚Äùdiffuseness‚Äù to evaluate both experiments.</p>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1">Our first experiment evaluates whether DNNs might be less robust to diffuse occlusion than solid occluders. To conduct this experiment, in IRUO-Diffuse, we created solid gray box occluders and two types of diffuse gray occluders: right-angle and oblique line occluders, both of which are in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F12" title="Figure 12 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">12</span></a>. Solid box occluders are randomly generated with a random size and center point location, while the diffuse occluders are randomly generated with randomly selected line width and spacing, along with randomly selected angles for oblique occluders. Full details of image generation in IRUO-Diffuse are in the Appendix. By varying these parameters, we generate both solid and diffuse occlusions at each occlusion level, allowing us to compare the effect of diffuse occluders on the occlusion robustness of DNNs. Since we are able to control the precise level of occlusion generated via this method, we can also evaluate model accuracy for more granular levels of occlusion than in previous sections. In Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS1" title="6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a> we utilized three occlusion levels, whereas here we utilize five occlusion levels: 0% (unoccluded imagery), 10-30%, 30-50%, 50-70%, and 70-90% occlusion.</p>
</div>
<figure class="ltx_figure" id="S6.F12">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F12.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F12.3.3.3">
<td class="ltx_td ltx_align_center" id="S6.F12.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F12.1.1.1.1.g1" src="extracted/5858817/diffuse4.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S6.F12.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F12.2.2.2.2.g1" src="extracted/5858817/diffuse1.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S6.F12.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F12.3.3.3.3.g1" src="extracted/5858817/diffuse3.jpg" width="162"/></td>
</tr>
<tr class="ltx_tr" id="S6.F12.3.3.4.1">
<td class="ltx_td ltx_align_center" id="S6.F12.3.3.4.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F12.3.3.4.1.2">(b)</td>
<td class="ltx_td ltx_align_center" id="S6.F12.3.3.4.1.3">(c)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Images showing types of diffuse occlusion included in the IRUO-Diffuse subset. Occlusion types: (a) cross-hatch (grid) occlusions, (b) horizontal line occlusions, and (c) occlusions by lines at non-right angles</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1">Our second experiment evaluates the effect of the <span class="ltx_text ltx_font_italic" id="S6.SS4.p4.1.1">level</span> of diffuseness on the occlusion-robustness of DNNs, since the first experiment provides evidence that DNNs are indeed less robust to diffuse occlusion. We include in IRUO-Diffuse images containing occluders with each of five specific levels of diffuseness: denoted 1, 2, 4, 8, or 16. Each number corresponds to the size (in pixels) of evenly-spaced contiguous groups of occluding pixels, so that <span class="ltx_text ltx_font_italic" id="S6.SS4.p4.1.2">larger numbers correspond to lower diffuseness</span>. Examples of images illustrating these occluder patterns are presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F13" title="Figure 13 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">13</span></a>. Like in the first experiment, the use of synthetic occlusions allows us to generate more granular levels of occlusion than in previous sections; in this experiment, we utilize four levels of occlusion: 0% (unoccluded imagery), 25%, 50%, and 75% occlusion. Additional details of the generation of this subset of IRUO-Diffuse are in the Appendix.</p>
</div>
<figure class="ltx_figure" id="S6.F13">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.F13.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F13.3.3.3">
<td class="ltx_td ltx_align_center" id="S6.F13.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F13.1.1.1.1.g1" src="extracted/5858817/supdif1.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S6.F13.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F13.2.2.2.2.g1" src="extracted/5858817/supdif2.jpg" width="162"/></td>
<td class="ltx_td ltx_align_center" id="S6.F13.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="162" id="S6.F13.3.3.3.3.g1" src="extracted/5858817/supdif3.jpg" width="162"/></td>
</tr>
<tr class="ltx_tr" id="S6.F13.3.3.4.1">
<td class="ltx_td ltx_align_center" id="S6.F13.3.3.4.1.1">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F13.3.3.4.1.2">(b)</td>
<td class="ltx_td ltx_align_center" id="S6.F13.3.3.4.1.3">(c)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Images showing levels of diffuse occlusion in the IRUO-Diffuse subset, i.e., occlusion comprised of small occlusions visually dividing objects, applied, where occlusion patterns contain (a) 1x1 pixel occluders, (b) 4x4-pixel occluders, (c) 16x16-pixel occluders</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p5">
<p class="ltx_p" id="S6.SS4.p5.1">For both experiments, to evaluate the <span class="ltx_text ltx_font_italic" id="S6.SS4.p5.1.1">robustness</span> ‚Äì as opposed to pure accuracy ‚Äì of Swin, we needed to compare Swin to humans. We recruited additional human observers (distinct from Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>) to label two subsets of the IRUO-diffuse subset: one for each of the two experiments outlined above. For <span class="ltx_text ltx_font_italic" id="S6.SS4.p5.1.2">each experiment</span> we recruited five human observers, and each observer rated 50 images, for a total of 250 rated images per experiment. Human responses are collected using the same method described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5" title="5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">5</span></a>. Similar to the generation of IRUO-HTS, we subsample 2-3 images from each class at each occlusion level for each type of occlusion, resulting in a total of approximately 250 images for each of two sets: one containing images with the solid and diffuse occluders described in the first experiment above and in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F12" title="Figure 12 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">12</span></a>, and the other containing images with the various-sized occluders described in the second experiment above and in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F13" title="Figure 13 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">13</span></a>. For both experiments, the reported accuracies for Swin and human observers at occlusion level 0 are the same accuracies as in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>, as the images with occlusion level 0 (0% occlusion) are unchanged in this section.</p>
</div>
<div class="ltx_para" id="S6.SS4.p6">
<p class="ltx_p" id="S6.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p6.1.1">Results.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F14" title="Figure 14 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">14</span></a> presents the results of our first experiment, investigating DNN occlusion-robustness for different types of occlusion by comparing Swin and human accuracy for several diffuse occluders. The gap between human observers and Swin is minimal for solid box occluders at higher levels of occlusion and much larger for both types of diffuse occlusion (right-angle grids and oblique lines/grids, as tested). This indicates that diffuse occluders reduce the performance of models more than equivalent solid occluders at the same levels of occlusion. Further, this gap in accuracy (about 9 percent between box occlusions and the two diffuse occluders at the highest level of occlusion) is larger than the gap in accuracy between the average human observer and Swin in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>, suggesting that diffuse occlusion contributes to the overall gap in human and DNN performance on occluded imagery.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="579" id="S6.F14.1.g1" src="extracted/5858817/humanswin_normdiffuse.png" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Plots of results on diffuse occlusion, including right-angle and oblique grids, compared to solid gray occluders, for human observers and Swin (the best-performing model from Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>)</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p7">
<p class="ltx_p" id="S6.SS4.p7.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F15" title="Figure 15 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">15</span></a> presents the results of our second experiment, where we observe that smaller occluders (those with pixel sizes of 4 or less) degrade the accuracy of Swin, as compared to humans, more than larger occluders (those with pixel size of 8 or more). These larger gaps in accuracy between Swin and humans for smaller (i.e., more diffuse) occluders indicate that the level of diffuseness of occluders has an inverse relationship with model accuracy at higher levels of occlusion. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F15" title="Figure 15 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">15</span></a>, the results corresponding to 16x16-pixel occlusions (the least diffuse in this set) suggest minimal difference between human and Swin accuracy, on par with that of solid occluders (as in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F14" title="Figure 14 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">14</span></a>). Some of the difference in accuracy between human observers and Swin, especially on the diffuse occlusions in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F13" title="Figure 13 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">13</span></a> may result from the choice of patch sizes in Swin (typically 4x4 pixels each); larger occluders are more likely to coincide with entire patches than smaller occluders, possibly allowing models to ignore these occlusions more effectively.</p>
</div>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="579" id="S6.F15.1.g1" src="extracted/5858817/humanswin_highdiffuse.png" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Plots of results on various levels of diffuse occlusion, including pixel occlusions of 1x1- to 16x16-pixel size, for human observers and Swin (the best-performing model from Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS3" title="6.3 How occlusion-robust are models compared to humans? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.3</span></a>)</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p8">
<p class="ltx_p" id="S6.SS4.p8.1">To investigate why smaller occluders cause model accuracy to degrade more than larger occluders, we find the attention maps returned by Vision Transformer, a model similar to Swin in its attention computation but with more interpretable image-wide attention output. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F16" title="Figure 16 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">16</span></a>, we plot attention maps at stages 3, 7, and 11 (identical to those used in the procedure used for Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F7" title="Figure 7 ‚Ä£ 6.1 Which models are most accurate under occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">7</span></a>) for an identical image with occluders of size 4, 8, and 16 pixels. The results here are representative of those for most images; additional images and their corresponding attention maps are located in the Appendix. We observe that images show high values of attention to points that are not occluded when occluders are large, and that this attention becomes much lower as the size of occluders decreases, particularly as occluders become smaller than the patch size of the Vision Transformer. We hypothesize that when occluders become smaller than the patch size of models (assuming the same level of occlusion and therefore higher number of occluders), these occluders contaminate patch-level computations in transformer models and greatly reduce their ability to make accurate predictions.</p>
</div>
<figure class="ltx_figure" id="S6.F16">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.F16.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F16.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.F16.4.4.4.5">(16)</th>
<td class="ltx_td ltx_align_center" id="S6.F16.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.1.1.1.1.g1" src="extracted/5858817/164_00140_us4.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.2.2.2.2.g1" src="extracted/5858817/t50_us4_o50_b49_164_00140_78a1f4b1_img_0000141_vit_layer_2.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.3.3.3.3.g1" src="extracted/5858817/t50_us4_o50_b49_164_00140_78a1f4b1_img_0000141_vit_layer_6.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.4.4.4.4.g1" src="extracted/5858817/t50_us4_o50_b49_164_00140_78a1f4b1_img_0000141_vit_layer_10.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S6.F16.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.F16.8.8.8.5">(8)</th>
<td class="ltx_td ltx_align_center" id="S6.F16.5.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.5.5.5.1.g1" src="extracted/5858817/164_00140_us3.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.6.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.6.6.6.2.g1" src="extracted/5858817/t50_us3_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_2.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.7.7.7.3.g1" src="extracted/5858817/t50_us3_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_6.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.8.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.8.8.8.4.g1" src="extracted/5858817/t50_us3_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_10.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S6.F16.12.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.F16.12.12.12.5">(4)</th>
<td class="ltx_td ltx_align_center" id="S6.F16.9.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.9.9.9.1.g1" src="extracted/5858817/164_00140_us2.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.10.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.10.10.10.2.g1" src="extracted/5858817/t50_us2_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_2.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.11.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.11.11.11.3.g1" src="extracted/5858817/t50_us2_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_6.jpg" width="120"/></td>
<td class="ltx_td ltx_align_center" id="S6.F16.12.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="120" id="S6.F16.12.12.12.4.g1" src="extracted/5858817/t50_us2_o49_b50_164_00140_78a1f4b1_img_0000141_vit_layer_10.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S6.F16.12.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.F16.12.12.13.1.1">(size, px)</th>
<td class="ltx_td ltx_align_center" id="S6.F16.12.12.13.1.2">(a)</td>
<td class="ltx_td ltx_align_center" id="S6.F16.12.12.13.1.3">(b)</td>
<td class="ltx_td ltx_align_center" id="S6.F16.12.12.13.1.4">(c)</td>
<td class="ltx_td ltx_align_center" id="S6.F16.12.12.13.1.5">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Sample images of a person (column (a)) at 50% occlusion, with various occluder sizes (in pixels, shown in row names) with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (column (b)) third, (column (c)) seventh, and (column (d)) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. Lighter colors (e.g., yellow) denote higher values for attention</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Consistent with existing literature, we observe that occlusion is a significant challenge for deep learning models that are otherwise able to achieve human accuracy on visual recognition tasks. We observe that some models designed to perform well under conditions of partial occlusion may rely on assumptions, including a small number of classes and known occlusion types, that do not necessarily reflect real-world applications. The benchmark results presented in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6" title="6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrate that, while some models, such as Swin (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>) nearly achieve human accuracy on occluded data, some types of occlusion, including diffuse occlusion, are still challenging for these models.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In this work, we provide evidence for the following:</p>
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i1.p1.1.1">Comparison of occlusion-robust models.</span>
Transformer models outperform convolutional models, typically by a large margin, on both occluded and unoccluded imagery.
Swin with Mixup augmentation achieved the best results across all levels of occlusion, including none.
Augmentations suggested to work well for occlusion (e.g., Mixup, CutMix, Deep Feature Augmentation) provide mixed benefit.
Models especially designed for occlusion (e.g., CompositionalNet) perform worse than all other models tested on our benchmark.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i2.p1.1.1">Comparison of real and synthetic imagery for occlusion evaluation.</span>
Synthetic occlusions are a reasonable substitute for real occlusions when comparing models on occluded imagery. As shown in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a>, we find that the rank-order of models is highly similar, regardless of the type of occlusion used. This finding reduces the difficulty of data collection to evaluate model performance on various levels of occlusion.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i3.p1.1.1">Robustness of DNN models.</span>
We find that humans perform somewhat better than the top-performing model, Swin with Mixup; however, this gap in performance is somewhat small. This result suggests that some improvement can be made to this model, but the overall potential for improvement is likely limited.
Model robustness depends on the properties of occluders. Specifically, diffuse occluders (such as those in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F12" title="Figure 12 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">12</span></a>) are more challenging for models, even state-of-the-art models such as Swin, than solid occluders at equivalent levels of occlusion.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">This benchmark does not stratify results by certain <span class="ltx_text ltx_font_italic" id="S7.p3.1.1">types</span> of real occlusion, including inter- and intra-class occlusion, because OVIS (and, by extension, IRUO) does not contain these distinctions, and additional manual labeling is beyond the scope of this publication. Notably, intra-class occlusion (e.g., in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S5.F4" title="Figure 4 ‚Ä£ 5.1 The Human Testing Subset ‚Ä£ 5 Human Study ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">4</span></a>(a)) poses a difficult challenge in evaluation on occlusion; an object in the background may be mostly occluded by an object of the same class that is mostly visible to the camera. More recent datasets, including HOOT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib18" title="">18</a>]</cite>), include labels making these distinctions, but they do not serve as full replacements for IRUO, since they contain more limited stratifications of occlusion levels. The authors propose additional experiments such as those evaluating models on specific types of occlusions as future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research was supported by the U.S. Army Combat Capabilities Development Command (DEVCOM), C5ISR Center, RTI Directorate via Grants W15P7T-19-D-0082 and W909MY-19-F-0095. The authors would like to thank the participants in our human research studies, who will remain anonymous.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Data Availability</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Code for data generation, tested models, and other results, are available online at https://github.com/kalebkassaw/iruo. Usage of this repository requires the original OVIS dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib17" title="">17</a>]</cite>) to be downloaded.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Full result tables for model evaluation on real and synthetic occlusion</h2>
<figure class="ltx_table" id="A1.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T4.1" style="width:433.6pt;height:129pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-299.0pt,88.7pt) scale(0.420309468430037,0.420309468430037) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.1.1.1.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.1.1.2" rowspan="3"><span class="ltx_text" id="A1.T4.1.1.1.1.2.1">Augmentation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="13" id="A1.T4.1.1.1.1.3">Accuracy</th>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.2.2.1">0 (None)</th>
<td class="ltx_td ltx_align_center" colspan="6" id="A1.T4.1.1.2.2.2">1 (0-50%)</td>
<td class="ltx_td ltx_align_center" colspan="6" id="A1.T4.1.1.2.2.3">2 (50-100%)</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.3.3.1">-</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.2">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.3">White</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.4">Black</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.5">Noise</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.6">Texture</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.3.3.7">Objects</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.8">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.9">White</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.10">Black</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.11">Noise</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.12">Texture</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.3.3.13">Objects</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.1.4.4.1">VGG-16 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.4.4.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.4.4.3">0.755</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.4">0.609</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.5">0.706</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.6">0.707</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.7">0.688</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.8">0.668</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.1.4.4.9">0.593</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.10">0.430</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.11">0.493</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.12">0.518</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.13">0.474</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.14">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.4.4.15">0.480</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.1.1.5.5.1">CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.5.5.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.5.5.3">0.667</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.4">0.508</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.5">0.635</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.6">0.646</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.7">0.625</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.8">0.600</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.5.5.9">0.565</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.10">0.321</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.11">0.436</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.12">0.464</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.13">0.437</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.14">0.249</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.5.5.15">0.437</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.1.1.6.6.1">ViT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.6.6.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.6.6.3">0.800</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.4">0.659</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.5">0.411</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.6">0.367</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.7">0.429</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.8">0.363</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.6.6.9">0.376</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.10">0.477</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.11">0.317</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.12">0.258</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.13">0.366</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.14">0.156</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.6.6.15">0.354</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.1.1.7.7.1">ResNet-50 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib9" title="">9</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.7.7.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.7.7.3">0.793</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.4">0.651</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.5">0.744</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.6">0.755</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.7">0.734</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.8">0.701</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.7.7.9">0.587</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.10">0.468</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.11">0.503</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.12">0.540</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.13">0.460</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.14">0.405</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.7.7.15">0.413</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.1.8.8.1" rowspan="4"><span class="ltx_text" id="A1.T4.1.1.8.8.1.1">ResNeXt-50 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.8.8.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.8.8.3">0.811</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.4">0.665</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.5">0.769</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.6">0.777</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.7">0.742</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.8">0.657</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.1.8.8.9">0.666</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.10">0.496</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.11">0.547</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.12">0.574</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.13">0.514</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.14">0.267</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.8.8.15">0.510</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.9.9.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.9.9.2">0.793</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.3">0.655</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.4">0.761</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.5">0.756</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.6">0.732</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.7">0.670</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.9.9.8">0.669</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.9">0.490</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.10">0.538</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.11">0.548</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.12">0.528</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.13">0.357</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.9.9.14">0.523</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.10.10.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.10.10.2">0.785</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.3">0.640</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.4">0.753</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.5">0.755</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.6">0.733</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.7">0.705</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.10.10.8">0.688</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.9">0.480</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.10">0.527</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.11">0.556</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.12">0.550</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.13">0.484</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.10.10.14">0.545</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.11.11.1">Deep Feature (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.11.11.2">0.781</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.3">0.643</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.4">0.737</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.5">0.736</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.6">0.713</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.7">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.11.11.8">0.638</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.9">0.479</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.10">0.519</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.11">0.519</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.12">0.489</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.13">0.188</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.11.11.14">0.482</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.1.12.12.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.1.12.12.1.1">DeiT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.12.12.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.12.12.3">0.862</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.4">0.729</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.5">0.854</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.6">0.862</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.7">0.858</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.8">0.708</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.1.12.12.9">0.842</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.10">0.574</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.11">0.728</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.12">0.748</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.13">0.785</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.14">0.359</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.12.12.15">0.731</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.13.13.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.13.13.2">0.868</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.3">0.728</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.4">0.863</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.5">0.868</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.6">0.859</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.7">0.733</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.13.13.8">0.851</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.9">0.568</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.10">0.730</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.11">0.766</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.12">0.799</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.13">0.411</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.13.13.14">0.736</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.14.14.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.14.14.2">0.863</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.3">0.729</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.4">0.861</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.5">0.862</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.6">0.859</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.7">0.680</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.14.14.8">0.848</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.9">0.567</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.10">0.730</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.11">0.766</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.12">0.784</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.13">0.357</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.14.14.14">0.740</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A1.T4.1.1.15.15.1" rowspan="3"><span class="ltx_text" id="A1.T4.1.1.15.15.1.1">Swin (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.15.15.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.15.15.3">0.891</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.4">0.751</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.5">0.883</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.6">0.882</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.7">0.882</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.8">0.870</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.1.15.15.9">0.855</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.10">0.594</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.11">0.722</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.12">0.785</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.13">0.771</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.14">0.762</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.15.15.15">0.698</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.16.16.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.1.16.16.2">0.894</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.3">0.760</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.4">0.882</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.5">0.882</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.6">0.878</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.7">0.873</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.1.16.16.8">0.863</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.9">0.597</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.10">0.758</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.11">0.789</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.12">0.793</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.13">0.754</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.1.16.16.14">0.731</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A1.T4.1.1.17.17.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A1.T4.1.1.17.17.2">0.885</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.3">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.4">0.872</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.5">0.863</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.6">0.869</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.7">0.864</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T4.1.1.17.17.8">0.860</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.9">0.581</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.10">0.743</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.11">0.780</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.12">0.772</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.13">0.731</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T4.1.1.17.17.14">0.726</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Table showing raw accuracy scores for each model configuration tested with each type of occlusion. For each level of occlusion greater than 0, we stratified occlusion into one of many types: real occlusion plus five types of artificial occlusions commonly used in the literature</figcaption>
</figure>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T5.5" style="width:433.6pt;height:136.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-297.8pt,93.7pt) scale(0.421330492608519,0.421330492608519) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.5.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T5.5.5.6.1.1" rowspan="3"><span class="ltx_text" id="A1.T5.5.5.6.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.6.1.2" rowspan="3"><span class="ltx_text" id="A1.T5.5.5.6.1.2.1">Augmentation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="13" id="A1.T5.5.5.6.1.3">Accuracy</th>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.7.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.7.2.1">0 (None)</th>
<td class="ltx_td ltx_align_center" colspan="6" id="A1.T5.5.5.7.2.2">1 (0-50%)</td>
<td class="ltx_td ltx_align_center" colspan="6" id="A1.T5.5.5.7.2.3">2 (50-100%)</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.8.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.8.3.1">-</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.2">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.3">White</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.4">Black</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.5">Noise</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.6">Texture</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.8.3.7">Objects</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.8">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.9">White</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.10">Black</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.11">Noise</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.12">Texture</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.8.3.13">Objects</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.9.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T5.5.5.9.4.1">VGG-16 (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib19" title="">19</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.9.4.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.9.4.3">13</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.4">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.5">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.6">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.7">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.8">10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.5.9.4.9">11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.10">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.11">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.12">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.13">11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.14">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.9.4.15">11</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.10.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T5.5.5.10.5.1">CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.10.5.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.10.5.3">14</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.4">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.5">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.6">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.7">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.8">12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.10.5.9">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.10">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.11">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.12">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.13">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.14">12</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.10.5.15">12</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.11.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T5.5.5.11.6.1">ViT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib7" title="">7</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.11.6.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.11.6.3">8</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.4">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.5">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.6">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.7">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.8">14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.11.6.9">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.10">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.11">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.12">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.13">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.14">14</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.11.6.15">14</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.12.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T5.5.5.12.7.1">ResNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib9" title="">9</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.12.7.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.12.7.3">9.5</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.4">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.5">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.6">9.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.7">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.8">7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.12.7.9">12</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.10">12</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.11">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.12">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.13">12</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.14">6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.12.7.15">13</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.13.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T5.5.5.13.8.1" rowspan="4"><span class="ltx_text" id="A1.T5.5.5.13.8.1.1">ResNeXt (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib26" title="">26</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.13.8.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.13.8.3">7</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.4">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.5">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.6">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.7">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.8">11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.5.13.8.9">9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.10">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.11">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.12">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.13">9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.14">11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.13.8.15">9</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.14.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.14.9.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.14.9.2">9.5</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.3">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.4">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.5">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.6">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.7">9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.14.9.8">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.9">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.10">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.11">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.12">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.13">9.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.14.9.14">8</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.15.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.15.10.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.15.10.2">11</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.3">12</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.4">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.5">9.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.6">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.7">6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.15.10.8">7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.9">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.10">9</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.11">8</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.12">7</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.13">4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.15.10.14">7</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.16.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.16.11.1">Deep Feature (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.16.11.2">12</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.3">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.4">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.5">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.6">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.7">13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.16.11.8">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.9">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.10">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.11">11</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.12">10</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.13">13</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.16.11.14">10</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.17.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T5.5.5.17.12.1" rowspan="3"><span class="ltx_text" id="A1.T5.5.5.17.12.1.1">DeiT (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib21" title="">21</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.17.12.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.17.12.3">6</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.4">4.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.5">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.6">5.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.7">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.8">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.5.17.12.9">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.10">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.11">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.12">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.13">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.14">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.17.12.15">3.5</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.18.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.18.13.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.18.13.2">4</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.3">6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.4">4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.5">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.6">4.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.7">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.18.13.8">4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.9">5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.10">3.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.11">4.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.12">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.13">5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.18.13.14">2</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.19.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.19.14.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.19.14.2">5</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.3">4.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.4">5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.5">5.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.6">4.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.7">8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.19.14.8">5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.9">6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.10">3.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.11">4.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.12">4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.13">9.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.19.14.14">1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.20.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T5.5.5.20.15.1" rowspan="3"><span class="ltx_text" id="A1.T5.5.5.20.15.1.1">Swin (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib14" title="">14</a>]</cite>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.20.15.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.5.20.15.3">2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.4">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.6">1.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.8">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.5.20.15.9">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.10">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.11">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.12">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.13">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.14">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.5.20.15.15">6</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.21.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.21.16.1">Mixup (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib29" title="">29</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.21.16.2">1</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.3">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.4">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.5">1.5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.6">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.21.16.8">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.9">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.10">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.11">1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.12">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.13">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.21.16.14">3.5</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.22.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.22.17.1">CutMix (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib28" title="">28</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.5.22.17.2">3</th>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.3">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.4">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.5">4</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.6">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.7">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.5.22.17.8">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.9">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.10">2</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.11">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.12">5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.13">3</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.5.22.17.14">5</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" colspan="2" id="A1.T5.1.1.1.1">Rank ordering <math alttext="Q" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mi id="A1.T5.1.1.1.1.m1.1.1" xref="A1.T5.1.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">italic_Q</annotation></semantics></math> (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.5.5.5.6">-</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" colspan="6" id="A1.T5.3.3.3.3">
<math alttext="Q=71.0" class="ltx_Math" display="inline" id="A1.T5.2.2.2.2.m1.1"><semantics id="A1.T5.2.2.2.2.m1.1a"><mrow id="A1.T5.2.2.2.2.m1.1.1" xref="A1.T5.2.2.2.2.m1.1.1.cmml"><mi id="A1.T5.2.2.2.2.m1.1.1.2" xref="A1.T5.2.2.2.2.m1.1.1.2.cmml">Q</mi><mo id="A1.T5.2.2.2.2.m1.1.1.1" xref="A1.T5.2.2.2.2.m1.1.1.1.cmml">=</mo><mn id="A1.T5.2.2.2.2.m1.1.1.3" xref="A1.T5.2.2.2.2.m1.1.1.3.cmml">71.0</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.2.m1.1b"><apply id="A1.T5.2.2.2.2.m1.1.1.cmml" xref="A1.T5.2.2.2.2.m1.1.1"><eq id="A1.T5.2.2.2.2.m1.1.1.1.cmml" xref="A1.T5.2.2.2.2.m1.1.1.1"></eq><ci id="A1.T5.2.2.2.2.m1.1.1.2.cmml" xref="A1.T5.2.2.2.2.m1.1.1.2">ùëÑ</ci><cn id="A1.T5.2.2.2.2.m1.1.1.3.cmml" type="float" xref="A1.T5.2.2.2.2.m1.1.1.3">71.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.2.m1.1c">Q=71.0</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.2.m1.1d">italic_Q = 71.0</annotation></semantics></math>; <math alttext="p=5.0\times 10^{-10}" class="ltx_Math" display="inline" id="A1.T5.3.3.3.3.m2.1"><semantics id="A1.T5.3.3.3.3.m2.1a"><mrow id="A1.T5.3.3.3.3.m2.1.1" xref="A1.T5.3.3.3.3.m2.1.1.cmml"><mi id="A1.T5.3.3.3.3.m2.1.1.2" xref="A1.T5.3.3.3.3.m2.1.1.2.cmml">p</mi><mo id="A1.T5.3.3.3.3.m2.1.1.1" xref="A1.T5.3.3.3.3.m2.1.1.1.cmml">=</mo><mrow id="A1.T5.3.3.3.3.m2.1.1.3" xref="A1.T5.3.3.3.3.m2.1.1.3.cmml"><mn id="A1.T5.3.3.3.3.m2.1.1.3.2" xref="A1.T5.3.3.3.3.m2.1.1.3.2.cmml">5.0</mn><mo id="A1.T5.3.3.3.3.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A1.T5.3.3.3.3.m2.1.1.3.1.cmml">√ó</mo><msup id="A1.T5.3.3.3.3.m2.1.1.3.3" xref="A1.T5.3.3.3.3.m2.1.1.3.3.cmml"><mn id="A1.T5.3.3.3.3.m2.1.1.3.3.2" xref="A1.T5.3.3.3.3.m2.1.1.3.3.2.cmml">10</mn><mrow id="A1.T5.3.3.3.3.m2.1.1.3.3.3" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3.cmml"><mo id="A1.T5.3.3.3.3.m2.1.1.3.3.3a" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3.cmml">‚àí</mo><mn id="A1.T5.3.3.3.3.m2.1.1.3.3.3.2" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3.2.cmml">10</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.3.m2.1b"><apply id="A1.T5.3.3.3.3.m2.1.1.cmml" xref="A1.T5.3.3.3.3.m2.1.1"><eq id="A1.T5.3.3.3.3.m2.1.1.1.cmml" xref="A1.T5.3.3.3.3.m2.1.1.1"></eq><ci id="A1.T5.3.3.3.3.m2.1.1.2.cmml" xref="A1.T5.3.3.3.3.m2.1.1.2">ùëù</ci><apply id="A1.T5.3.3.3.3.m2.1.1.3.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3"><times id="A1.T5.3.3.3.3.m2.1.1.3.1.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3.1"></times><cn id="A1.T5.3.3.3.3.m2.1.1.3.2.cmml" type="float" xref="A1.T5.3.3.3.3.m2.1.1.3.2">5.0</cn><apply id="A1.T5.3.3.3.3.m2.1.1.3.3.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3.3"><csymbol cd="ambiguous" id="A1.T5.3.3.3.3.m2.1.1.3.3.1.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3.3">superscript</csymbol><cn id="A1.T5.3.3.3.3.m2.1.1.3.3.2.cmml" type="integer" xref="A1.T5.3.3.3.3.m2.1.1.3.3.2">10</cn><apply id="A1.T5.3.3.3.3.m2.1.1.3.3.3.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3"><minus id="A1.T5.3.3.3.3.m2.1.1.3.3.3.1.cmml" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3"></minus><cn id="A1.T5.3.3.3.3.m2.1.1.3.3.3.2.cmml" type="integer" xref="A1.T5.3.3.3.3.m2.1.1.3.3.3.2">10</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.3.m2.1c">p=5.0\times 10^{-10}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.3.m2.1d">italic_p = 5.0 √ó 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" colspan="6" id="A1.T5.5.5.5.5">
<math alttext="Q=65.5" class="ltx_Math" display="inline" id="A1.T5.4.4.4.4.m1.1"><semantics id="A1.T5.4.4.4.4.m1.1a"><mrow id="A1.T5.4.4.4.4.m1.1.1" xref="A1.T5.4.4.4.4.m1.1.1.cmml"><mi id="A1.T5.4.4.4.4.m1.1.1.2" xref="A1.T5.4.4.4.4.m1.1.1.2.cmml">Q</mi><mo id="A1.T5.4.4.4.4.m1.1.1.1" xref="A1.T5.4.4.4.4.m1.1.1.1.cmml">=</mo><mn id="A1.T5.4.4.4.4.m1.1.1.3" xref="A1.T5.4.4.4.4.m1.1.1.3.cmml">65.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.4.m1.1b"><apply id="A1.T5.4.4.4.4.m1.1.1.cmml" xref="A1.T5.4.4.4.4.m1.1.1"><eq id="A1.T5.4.4.4.4.m1.1.1.1.cmml" xref="A1.T5.4.4.4.4.m1.1.1.1"></eq><ci id="A1.T5.4.4.4.4.m1.1.1.2.cmml" xref="A1.T5.4.4.4.4.m1.1.1.2">ùëÑ</ci><cn id="A1.T5.4.4.4.4.m1.1.1.3.cmml" type="float" xref="A1.T5.4.4.4.4.m1.1.1.3">65.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.4.m1.1c">Q=65.5</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.4.m1.1d">italic_Q = 65.5</annotation></semantics></math>; <math alttext="p=5.3\times 10^{-9}" class="ltx_Math" display="inline" id="A1.T5.5.5.5.5.m2.1"><semantics id="A1.T5.5.5.5.5.m2.1a"><mrow id="A1.T5.5.5.5.5.m2.1.1" xref="A1.T5.5.5.5.5.m2.1.1.cmml"><mi id="A1.T5.5.5.5.5.m2.1.1.2" xref="A1.T5.5.5.5.5.m2.1.1.2.cmml">p</mi><mo id="A1.T5.5.5.5.5.m2.1.1.1" xref="A1.T5.5.5.5.5.m2.1.1.1.cmml">=</mo><mrow id="A1.T5.5.5.5.5.m2.1.1.3" xref="A1.T5.5.5.5.5.m2.1.1.3.cmml"><mn id="A1.T5.5.5.5.5.m2.1.1.3.2" xref="A1.T5.5.5.5.5.m2.1.1.3.2.cmml">5.3</mn><mo id="A1.T5.5.5.5.5.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A1.T5.5.5.5.5.m2.1.1.3.1.cmml">√ó</mo><msup id="A1.T5.5.5.5.5.m2.1.1.3.3" xref="A1.T5.5.5.5.5.m2.1.1.3.3.cmml"><mn id="A1.T5.5.5.5.5.m2.1.1.3.3.2" xref="A1.T5.5.5.5.5.m2.1.1.3.3.2.cmml">10</mn><mrow id="A1.T5.5.5.5.5.m2.1.1.3.3.3" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3.cmml"><mo id="A1.T5.5.5.5.5.m2.1.1.3.3.3a" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3.cmml">‚àí</mo><mn id="A1.T5.5.5.5.5.m2.1.1.3.3.3.2" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3.2.cmml">9</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.5.5.5.5.m2.1b"><apply id="A1.T5.5.5.5.5.m2.1.1.cmml" xref="A1.T5.5.5.5.5.m2.1.1"><eq id="A1.T5.5.5.5.5.m2.1.1.1.cmml" xref="A1.T5.5.5.5.5.m2.1.1.1"></eq><ci id="A1.T5.5.5.5.5.m2.1.1.2.cmml" xref="A1.T5.5.5.5.5.m2.1.1.2">ùëù</ci><apply id="A1.T5.5.5.5.5.m2.1.1.3.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3"><times id="A1.T5.5.5.5.5.m2.1.1.3.1.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3.1"></times><cn id="A1.T5.5.5.5.5.m2.1.1.3.2.cmml" type="float" xref="A1.T5.5.5.5.5.m2.1.1.3.2">5.3</cn><apply id="A1.T5.5.5.5.5.m2.1.1.3.3.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3.3"><csymbol cd="ambiguous" id="A1.T5.5.5.5.5.m2.1.1.3.3.1.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3.3">superscript</csymbol><cn id="A1.T5.5.5.5.5.m2.1.1.3.3.2.cmml" type="integer" xref="A1.T5.5.5.5.5.m2.1.1.3.3.2">10</cn><apply id="A1.T5.5.5.5.5.m2.1.1.3.3.3.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3"><minus id="A1.T5.5.5.5.5.m2.1.1.3.3.3.1.cmml" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3"></minus><cn id="A1.T5.5.5.5.5.m2.1.1.3.3.3.2.cmml" type="integer" xref="A1.T5.5.5.5.5.m2.1.1.3.3.3.2">9</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.5.5.5.5.m2.1c">p=5.3\times 10^{-9}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.5.5.5.5.m2.1d">italic_p = 5.3 √ó 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Table showing the rankings of models per each experimental setup with corresponding Friedman <math alttext="Q" class="ltx_Math" display="inline" id="A1.T5.8.m1.1"><semantics id="A1.T5.8.m1.1b"><mi id="A1.T5.8.m1.1.1" xref="A1.T5.8.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.T5.8.m1.1c"><ci id="A1.T5.8.m1.1.1.cmml" xref="A1.T5.8.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.8.m1.1d">Q</annotation><annotation encoding="application/x-llamapun" id="A1.T5.8.m1.1e">italic_Q</annotation></semantics></math> values and their statistical significance. The type of occlusion does not appear to affect rankings of models in a meaningful way, as determined by the high values of <math alttext="Q" class="ltx_Math" display="inline" id="A1.T5.9.m2.1"><semantics id="A1.T5.9.m2.1b"><mi id="A1.T5.9.m2.1.1" xref="A1.T5.9.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.T5.9.m2.1c"><ci id="A1.T5.9.m2.1.1.cmml" xref="A1.T5.9.m2.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.9.m2.1d">Q</annotation><annotation encoding="application/x-llamapun" id="A1.T5.9.m2.1e">italic_Q</annotation></semantics></math> for both occlusion levels 1 and 2</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of training</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Each model is pretrained on ImageNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib4" title="">4</a>]</cite>) and fine-tuned on the subset of the IRUO dataset containing no occlusion. (This setup is consistent with other training schemes, e.g., in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>.) For each model, we use a stochastic gradient descent optimizer, and we train until convergence to highest validation accuracy, up to 50 training epochs. Most models and augmentations are derived from MMClassification, a PyTorch-based deep learning package. We use a grid search to find ideal learning rates, momentums, and weight decays for each model and augmentation configuration. For CompositionalNet (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>]</cite>), we additionally initialize the von Mises-Fisher clusters, similarity matrices, and mixture models on IRUO images at occlusion level 0, and we optimize the weights of cluster center and mixture model losses during training. For Deep Feature Augmentation (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite>), we use our best-performing ResNeXt-50 model trained on IRUO and fine-tune for up to 20 additional epochs, retaining highest validation accuracy. We also perform a grid search over learning rate, deep vector probability, and deep vector weight. The deep feature vectors and occluder images used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib2" title="">2</a>]</cite> are not publicly available; therefore, we use a different set of objects cropped from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib13" title="">13</a>]</cite> containing only objects that are otherwise not part of IRUO, e.g., stop signs and refrigerators.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional details of human data collection</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">To mitigate and track errors due to selection of incorrect objects, we ask humans to label which object they are selecting by clicking a single point on the image. This choice is guided without directly revealing which object to label; a cross is placed at the center of the screen to remind participants that objects are expected to be near this point, but it is not moved from this fixed point, regardless of where objects are actually centered.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Calculation of Friedman Q-value</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.2">For the model rank comparison described in <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S3.SS3" title="3.3 Statistical Tests ‚Ä£ 3 Background Methods ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3.3</span></a> and used in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.SS2" title="6.2 Is synthetic occlusion a good proxy for real occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">6.2</span></a>, we opt to use the Friedman <math alttext="Q" class="ltx_Math" display="inline" id="A4.p1.1.m1.1"><semantics id="A4.p1.1.m1.1a"><mi id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">italic_Q</annotation></semantics></math>-value in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>, defined below in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A4.E3" title="In Appendix D Calculation of Friedman Q-value ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">3</span></a>. Specifically, we compare the rank ordering of models in our evaluations using real and synthetic occlusion, as opposed to using raw scores that may not be as informative for different types of occluders. We evaluate <math alttext="Q" class="ltx_Math" display="inline" id="A4.p1.2.m2.1"><semantics id="A4.p1.2.m2.1a"><mi id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><ci id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">italic_Q</annotation></semantics></math> in the formula</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="Q=\frac{12n}{k(k+1)}\sum_{j=1}^{k}(\bar{r_{j}}-\frac{k+1}{2})^{2}" class="ltx_Math" display="block" id="A4.E3.m1.2"><semantics id="A4.E3.m1.2a"><mrow id="A4.E3.m1.2.2" xref="A4.E3.m1.2.2.cmml"><mi id="A4.E3.m1.2.2.3" xref="A4.E3.m1.2.2.3.cmml">Q</mi><mo id="A4.E3.m1.2.2.2" xref="A4.E3.m1.2.2.2.cmml">=</mo><mrow id="A4.E3.m1.2.2.1" xref="A4.E3.m1.2.2.1.cmml"><mfrac id="A4.E3.m1.1.1" xref="A4.E3.m1.1.1.cmml"><mrow id="A4.E3.m1.1.1.3" xref="A4.E3.m1.1.1.3.cmml"><mn id="A4.E3.m1.1.1.3.2" xref="A4.E3.m1.1.1.3.2.cmml">12</mn><mo id="A4.E3.m1.1.1.3.1" xref="A4.E3.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="A4.E3.m1.1.1.3.3" xref="A4.E3.m1.1.1.3.3.cmml">n</mi></mrow><mrow id="A4.E3.m1.1.1.1" xref="A4.E3.m1.1.1.1.cmml"><mi id="A4.E3.m1.1.1.1.3" xref="A4.E3.m1.1.1.1.3.cmml">k</mi><mo id="A4.E3.m1.1.1.1.2" xref="A4.E3.m1.1.1.1.2.cmml">‚Å¢</mo><mrow id="A4.E3.m1.1.1.1.1.1" xref="A4.E3.m1.1.1.1.1.1.1.cmml"><mo id="A4.E3.m1.1.1.1.1.1.2" stretchy="false" xref="A4.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.E3.m1.1.1.1.1.1.1" xref="A4.E3.m1.1.1.1.1.1.1.cmml"><mi id="A4.E3.m1.1.1.1.1.1.1.2" xref="A4.E3.m1.1.1.1.1.1.1.2.cmml">k</mi><mo id="A4.E3.m1.1.1.1.1.1.1.1" xref="A4.E3.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="A4.E3.m1.1.1.1.1.1.1.3" xref="A4.E3.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A4.E3.m1.1.1.1.1.1.3" stretchy="false" xref="A4.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="A4.E3.m1.2.2.1.2" xref="A4.E3.m1.2.2.1.2.cmml">‚Å¢</mo><mrow id="A4.E3.m1.2.2.1.1" xref="A4.E3.m1.2.2.1.1.cmml"><munderover id="A4.E3.m1.2.2.1.1.2" xref="A4.E3.m1.2.2.1.1.2.cmml"><mo id="A4.E3.m1.2.2.1.1.2.2.2" movablelimits="false" rspace="0em" xref="A4.E3.m1.2.2.1.1.2.2.2.cmml">‚àë</mo><mrow id="A4.E3.m1.2.2.1.1.2.2.3" xref="A4.E3.m1.2.2.1.1.2.2.3.cmml"><mi id="A4.E3.m1.2.2.1.1.2.2.3.2" xref="A4.E3.m1.2.2.1.1.2.2.3.2.cmml">j</mi><mo id="A4.E3.m1.2.2.1.1.2.2.3.1" xref="A4.E3.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="A4.E3.m1.2.2.1.1.2.2.3.3" xref="A4.E3.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="A4.E3.m1.2.2.1.1.2.3" xref="A4.E3.m1.2.2.1.1.2.3.cmml">k</mi></munderover><msup id="A4.E3.m1.2.2.1.1.1" xref="A4.E3.m1.2.2.1.1.1.cmml"><mrow id="A4.E3.m1.2.2.1.1.1.1.1" xref="A4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo id="A4.E3.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="A4.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.E3.m1.2.2.1.1.1.1.1.1" xref="A4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mover accent="true" id="A4.E3.m1.2.2.1.1.1.1.1.1.2" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><msub id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml"><mi id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.2" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.2.cmml">r</mi><mi id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.3" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="A4.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">¬Ø</mo></mover><mo id="A4.E3.m1.2.2.1.1.1.1.1.1.1" xref="A4.E3.m1.2.2.1.1.1.1.1.1.1.cmml">‚àí</mo><mfrac id="A4.E3.m1.2.2.1.1.1.1.1.1.3" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.cmml"><mrow id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.2" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.2.cmml">k</mi><mo id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.1" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.1.cmml">+</mo><mn id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.3" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.3.cmml">1</mn></mrow><mn id="A4.E3.m1.2.2.1.1.1.1.1.1.3.3" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml">2</mn></mfrac></mrow><mo id="A4.E3.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="A4.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A4.E3.m1.2.2.1.1.1.3" xref="A4.E3.m1.2.2.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E3.m1.2b"><apply id="A4.E3.m1.2.2.cmml" xref="A4.E3.m1.2.2"><eq id="A4.E3.m1.2.2.2.cmml" xref="A4.E3.m1.2.2.2"></eq><ci id="A4.E3.m1.2.2.3.cmml" xref="A4.E3.m1.2.2.3">ùëÑ</ci><apply id="A4.E3.m1.2.2.1.cmml" xref="A4.E3.m1.2.2.1"><times id="A4.E3.m1.2.2.1.2.cmml" xref="A4.E3.m1.2.2.1.2"></times><apply id="A4.E3.m1.1.1.cmml" xref="A4.E3.m1.1.1"><divide id="A4.E3.m1.1.1.2.cmml" xref="A4.E3.m1.1.1"></divide><apply id="A4.E3.m1.1.1.3.cmml" xref="A4.E3.m1.1.1.3"><times id="A4.E3.m1.1.1.3.1.cmml" xref="A4.E3.m1.1.1.3.1"></times><cn id="A4.E3.m1.1.1.3.2.cmml" type="integer" xref="A4.E3.m1.1.1.3.2">12</cn><ci id="A4.E3.m1.1.1.3.3.cmml" xref="A4.E3.m1.1.1.3.3">ùëõ</ci></apply><apply id="A4.E3.m1.1.1.1.cmml" xref="A4.E3.m1.1.1.1"><times id="A4.E3.m1.1.1.1.2.cmml" xref="A4.E3.m1.1.1.1.2"></times><ci id="A4.E3.m1.1.1.1.3.cmml" xref="A4.E3.m1.1.1.1.3">ùëò</ci><apply id="A4.E3.m1.1.1.1.1.1.1.cmml" xref="A4.E3.m1.1.1.1.1.1"><plus id="A4.E3.m1.1.1.1.1.1.1.1.cmml" xref="A4.E3.m1.1.1.1.1.1.1.1"></plus><ci id="A4.E3.m1.1.1.1.1.1.1.2.cmml" xref="A4.E3.m1.1.1.1.1.1.1.2">ùëò</ci><cn id="A4.E3.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="A4.E3.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="A4.E3.m1.2.2.1.1.cmml" xref="A4.E3.m1.2.2.1.1"><apply id="A4.E3.m1.2.2.1.1.2.cmml" xref="A4.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="A4.E3.m1.2.2.1.1.2.1.cmml" xref="A4.E3.m1.2.2.1.1.2">superscript</csymbol><apply id="A4.E3.m1.2.2.1.1.2.2.cmml" xref="A4.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="A4.E3.m1.2.2.1.1.2.2.1.cmml" xref="A4.E3.m1.2.2.1.1.2">subscript</csymbol><sum id="A4.E3.m1.2.2.1.1.2.2.2.cmml" xref="A4.E3.m1.2.2.1.1.2.2.2"></sum><apply id="A4.E3.m1.2.2.1.1.2.2.3.cmml" xref="A4.E3.m1.2.2.1.1.2.2.3"><eq id="A4.E3.m1.2.2.1.1.2.2.3.1.cmml" xref="A4.E3.m1.2.2.1.1.2.2.3.1"></eq><ci id="A4.E3.m1.2.2.1.1.2.2.3.2.cmml" xref="A4.E3.m1.2.2.1.1.2.2.3.2">ùëó</ci><cn id="A4.E3.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="A4.E3.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="A4.E3.m1.2.2.1.1.2.3.cmml" xref="A4.E3.m1.2.2.1.1.2.3">ùëò</ci></apply><apply id="A4.E3.m1.2.2.1.1.1.cmml" xref="A4.E3.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A4.E3.m1.2.2.1.1.1.2.cmml" xref="A4.E3.m1.2.2.1.1.1">superscript</csymbol><apply id="A4.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1"><minus id="A4.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.1"></minus><apply id="A4.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2"><ci id="A4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.1">¬Ø</ci><apply id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.2">ùëü</ci><ci id="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.2.2.3">ùëó</ci></apply></apply><apply id="A4.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3"><divide id="A4.E3.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3"></divide><apply id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2"><plus id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.1"></plus><ci id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.2">ùëò</ci><cn id="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.3.cmml" type="integer" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.2.3">1</cn></apply><cn id="A4.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="A4.E3.m1.2.2.1.1.1.1.1.1.3.3">2</cn></apply></apply><cn id="A4.E3.m1.2.2.1.1.1.3.cmml" type="integer" xref="A4.E3.m1.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E3.m1.2c">Q=\frac{12n}{k(k+1)}\sum_{j=1}^{k}(\bar{r_{j}}-\frac{k+1}{2})^{2}</annotation><annotation encoding="application/x-llamapun" id="A4.E3.m1.2d">italic_Q = divide start_ARG 12 italic_n end_ARG start_ARG italic_k ( italic_k + 1 ) end_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( over¬Ø start_ARG italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - divide start_ARG italic_k + 1 end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A4.p1.9">where <math alttext="n" class="ltx_Math" display="inline" id="A4.p1.3.m1.1"><semantics id="A4.p1.3.m1.1a"><mi id="A4.p1.3.m1.1.1" xref="A4.p1.3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A4.p1.3.m1.1b"><ci id="A4.p1.3.m1.1.1.cmml" xref="A4.p1.3.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A4.p1.3.m1.1d">italic_n</annotation></semantics></math> is the number of models evaluated for each occlusion type, <math alttext="k" class="ltx_Math" display="inline" id="A4.p1.4.m2.1"><semantics id="A4.p1.4.m2.1a"><mi id="A4.p1.4.m2.1.1" xref="A4.p1.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p1.4.m2.1b"><ci id="A4.p1.4.m2.1.1.cmml" xref="A4.p1.4.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A4.p1.4.m2.1d">italic_k</annotation></semantics></math> is the number of occlusion types, and <math alttext="r_{j}" class="ltx_Math" display="inline" id="A4.p1.5.m3.1"><semantics id="A4.p1.5.m3.1a"><msub id="A4.p1.5.m3.1.1" xref="A4.p1.5.m3.1.1.cmml"><mi id="A4.p1.5.m3.1.1.2" xref="A4.p1.5.m3.1.1.2.cmml">r</mi><mi id="A4.p1.5.m3.1.1.3" xref="A4.p1.5.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p1.5.m3.1b"><apply id="A4.p1.5.m3.1.1.cmml" xref="A4.p1.5.m3.1.1"><csymbol cd="ambiguous" id="A4.p1.5.m3.1.1.1.cmml" xref="A4.p1.5.m3.1.1">subscript</csymbol><ci id="A4.p1.5.m3.1.1.2.cmml" xref="A4.p1.5.m3.1.1.2">ùëü</ci><ci id="A4.p1.5.m3.1.1.3.cmml" xref="A4.p1.5.m3.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m3.1c">r_{j}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.5.m3.1d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the average rank order assigned to each model over all occlusion types. We split this evaluation into two parts, determining whether models have specific rank orders at various occlusion levels, regardless of the type of occlusion. For <math alttext="k&gt;4" class="ltx_Math" display="inline" id="A4.p1.6.m4.1"><semantics id="A4.p1.6.m4.1a"><mrow id="A4.p1.6.m4.1.1" xref="A4.p1.6.m4.1.1.cmml"><mi id="A4.p1.6.m4.1.1.2" xref="A4.p1.6.m4.1.1.2.cmml">k</mi><mo id="A4.p1.6.m4.1.1.1" xref="A4.p1.6.m4.1.1.1.cmml">&gt;</mo><mn id="A4.p1.6.m4.1.1.3" xref="A4.p1.6.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.6.m4.1b"><apply id="A4.p1.6.m4.1.1.cmml" xref="A4.p1.6.m4.1.1"><gt id="A4.p1.6.m4.1.1.1.cmml" xref="A4.p1.6.m4.1.1.1"></gt><ci id="A4.p1.6.m4.1.1.2.cmml" xref="A4.p1.6.m4.1.1.2">ùëò</ci><cn id="A4.p1.6.m4.1.1.3.cmml" type="integer" xref="A4.p1.6.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.6.m4.1c">k&gt;4</annotation><annotation encoding="application/x-llamapun" id="A4.p1.6.m4.1d">italic_k &gt; 4</annotation></semantics></math> or <math alttext="n&gt;15" class="ltx_Math" display="inline" id="A4.p1.7.m5.1"><semantics id="A4.p1.7.m5.1a"><mrow id="A4.p1.7.m5.1.1" xref="A4.p1.7.m5.1.1.cmml"><mi id="A4.p1.7.m5.1.1.2" xref="A4.p1.7.m5.1.1.2.cmml">n</mi><mo id="A4.p1.7.m5.1.1.1" xref="A4.p1.7.m5.1.1.1.cmml">&gt;</mo><mn id="A4.p1.7.m5.1.1.3" xref="A4.p1.7.m5.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.7.m5.1b"><apply id="A4.p1.7.m5.1.1.cmml" xref="A4.p1.7.m5.1.1"><gt id="A4.p1.7.m5.1.1.1.cmml" xref="A4.p1.7.m5.1.1.1"></gt><ci id="A4.p1.7.m5.1.1.2.cmml" xref="A4.p1.7.m5.1.1.2">ùëõ</ci><cn id="A4.p1.7.m5.1.1.3.cmml" type="integer" xref="A4.p1.7.m5.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.7.m5.1c">n&gt;15</annotation><annotation encoding="application/x-llamapun" id="A4.p1.7.m5.1d">italic_n &gt; 15</annotation></semantics></math>, <math alttext="Q" class="ltx_Math" display="inline" id="A4.p1.8.m6.1"><semantics id="A4.p1.8.m6.1a"><mi id="A4.p1.8.m6.1.1" xref="A4.p1.8.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A4.p1.8.m6.1b"><ci id="A4.p1.8.m6.1.1.cmml" xref="A4.p1.8.m6.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.8.m6.1c">Q</annotation><annotation encoding="application/x-llamapun" id="A4.p1.8.m6.1d">italic_Q</annotation></semantics></math> has a chi-squared null distribution with <math alttext="k" class="ltx_Math" display="inline" id="A4.p1.9.m7.1"><semantics id="A4.p1.9.m7.1a"><mi id="A4.p1.9.m7.1.1" xref="A4.p1.9.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p1.9.m7.1b"><ci id="A4.p1.9.m7.1.1.cmml" xref="A4.p1.9.m7.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.9.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="A4.p1.9.m7.1d">italic_k</annotation></semantics></math> degrees of freedom (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib8" title="">8</a>]</cite>).</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Details of synthetic data generation</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.3">Synthetic occlusions in IRUO-Synthetic are one of five types: 1) black boxes, 2) white boxes, 3) noise boxes generated from a uniform distribution over red, green, and blue channels, 4) zebra-print texture boxes, and 5) objects from the COCO dataset. These synthetic occlusion types are based on those used in the studies in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#bib.bib3" title="">3</a>]</cite>.
We generate boxes by randomly selecting a box center point (from a uniform distribution over image dimensions) and an image size (from a Gaussian distribution of both dimensions, mean <math alttext="S/2" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mi id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">S</mi><mo id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">/</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><divide id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></divide><ci id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">ùëÜ</ci><cn id="A5.p1.1.m1.1.1.3.cmml" type="integer" xref="A5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">S/2</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">italic_S / 2</annotation></semantics></math>, standard deviation <math alttext="3S/10" class="ltx_Math" display="inline" id="A5.p1.2.m2.1"><semantics id="A5.p1.2.m2.1a"><mrow id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><mrow id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml"><mn id="A5.p1.2.m2.1.1.2.2" xref="A5.p1.2.m2.1.1.2.2.cmml">3</mn><mo id="A5.p1.2.m2.1.1.2.1" xref="A5.p1.2.m2.1.1.2.1.cmml">‚Å¢</mo><mi id="A5.p1.2.m2.1.1.2.3" xref="A5.p1.2.m2.1.1.2.3.cmml">S</mi></mrow><mo id="A5.p1.2.m2.1.1.1" xref="A5.p1.2.m2.1.1.1.cmml">/</mo><mn id="A5.p1.2.m2.1.1.3" xref="A5.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><divide id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1.1"></divide><apply id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2"><times id="A5.p1.2.m2.1.1.2.1.cmml" xref="A5.p1.2.m2.1.1.2.1"></times><cn id="A5.p1.2.m2.1.1.2.2.cmml" type="integer" xref="A5.p1.2.m2.1.1.2.2">3</cn><ci id="A5.p1.2.m2.1.1.2.3.cmml" xref="A5.p1.2.m2.1.1.2.3">ùëÜ</ci></apply><cn id="A5.p1.2.m2.1.1.3.cmml" type="integer" xref="A5.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">3S/10</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.1d">3 italic_S / 10</annotation></semantics></math>, where <math alttext="S" class="ltx_Math" display="inline" id="A5.p1.3.m3.1"><semantics id="A5.p1.3.m3.1a"><mi id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><ci id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="A5.p1.3.m3.1d">italic_S</annotation></semantics></math> is the side dimension of the square input image). While these parameters result in images containing object occlusion levels spread out over the 0-100 percent range, some images containing unoccluded or fully occluded objects result. Occlusions that cover less than 5 percent or greater than 95 percent of images are removed.
Synthetic objects are resized to 1/4 of image area and placed at random on top of objects, with center points randomly distributed over the middle half of each dimension.
To balance computation time with result accuracy, we generate one occluded image per unoccluded IRUO image per type of synthetic occlusion, and we calculate object occlusion percentages by calculating the intersection of occlusions and original object masks, dividing it by the object mask area, and subtracting this value from 1.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Details of diffuse synthetic data generation</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.2">Because we determine that synthetic occlusions can serve as an appropriate proxy for real-world occlusion scenes, we generate IRUO-Diffuse, another dataset comprised of synthetic occlusions to evaluate the effect of diffuse occluders, e.g., sparsely arranged occlusions such as fences and leaves. To compare the effects of solid versus diffuse occluders on human and model accuracy, we generate occlusion boxes using largely similar parameters to that in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#A5" title="Appendix E Details of synthetic data generation ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">E</span></a>, except with size standard deviation <math alttext="4S/10" class="ltx_Math" display="inline" id="A6.p1.1.m1.1"><semantics id="A6.p1.1.m1.1a"><mrow id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml"><mrow id="A6.p1.1.m1.1.1.2" xref="A6.p1.1.m1.1.1.2.cmml"><mn id="A6.p1.1.m1.1.1.2.2" xref="A6.p1.1.m1.1.1.2.2.cmml">4</mn><mo id="A6.p1.1.m1.1.1.2.1" xref="A6.p1.1.m1.1.1.2.1.cmml">‚Å¢</mo><mi id="A6.p1.1.m1.1.1.2.3" xref="A6.p1.1.m1.1.1.2.3.cmml">S</mi></mrow><mo id="A6.p1.1.m1.1.1.1" xref="A6.p1.1.m1.1.1.1.cmml">/</mo><mn id="A6.p1.1.m1.1.1.3" xref="A6.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><apply id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1"><divide id="A6.p1.1.m1.1.1.1.cmml" xref="A6.p1.1.m1.1.1.1"></divide><apply id="A6.p1.1.m1.1.1.2.cmml" xref="A6.p1.1.m1.1.1.2"><times id="A6.p1.1.m1.1.1.2.1.cmml" xref="A6.p1.1.m1.1.1.2.1"></times><cn id="A6.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A6.p1.1.m1.1.1.2.2">4</cn><ci id="A6.p1.1.m1.1.1.2.3.cmml" xref="A6.p1.1.m1.1.1.2.3">ùëÜ</ci></apply><cn id="A6.p1.1.m1.1.1.3.cmml" type="integer" xref="A6.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">4S/10</annotation><annotation encoding="application/x-llamapun" id="A6.p1.1.m1.1d">4 italic_S / 10</annotation></semantics></math>, where <math alttext="S" class="ltx_Math" display="inline" id="A6.p1.2.m2.1"><semantics id="A6.p1.2.m2.1a"><mi id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><ci id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="A6.p1.2.m2.1d">italic_S</annotation></semantics></math> is the side dimension of the square input image, to account for the overall lower amount of occlusion resulting from diffuse occluders in similar window sizes. If images have much higher levels of background occlusion than equivalent solid occluders (e.g., the percentage of background occlusion is more than twice that of object occlusion), they are re-generated.</p>
</div>
<div class="ltx_para" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">To measure the effect of <span class="ltx_text ltx_font_italic" id="A6.p2.1.1">diffuseness</span>, i.e., the numerical sparsity of occluders on human and model accuracy, we generate diffuse occlusions using simple grid patterns comprising of 25, 50, and 75 percent occlusion. These grid patterns are expressed as tiles that are repeated over an entire image, and a value of 1 returns an occluded pixel, while a value of 0 returns the original image pixel. To generate data over multiple levels of diffuseness, we upscale these tiles (such that fewer, larger tiles are used) by 2x in both spatial dimensions.</p>
</div>
<div class="ltx_para" id="A6.p3">
<p class="ltx_p" id="A6.p3.3">Sample occluder tile masks are as follows, denoted <math alttext="O_{x,y}" class="ltx_Math" display="inline" id="A6.p3.1.m1.2"><semantics id="A6.p3.1.m1.2a"><msub id="A6.p3.1.m1.2.3" xref="A6.p3.1.m1.2.3.cmml"><mi id="A6.p3.1.m1.2.3.2" xref="A6.p3.1.m1.2.3.2.cmml">O</mi><mrow id="A6.p3.1.m1.2.2.2.4" xref="A6.p3.1.m1.2.2.2.3.cmml"><mi id="A6.p3.1.m1.1.1.1.1" xref="A6.p3.1.m1.1.1.1.1.cmml">x</mi><mo id="A6.p3.1.m1.2.2.2.4.1" xref="A6.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="A6.p3.1.m1.2.2.2.2" xref="A6.p3.1.m1.2.2.2.2.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A6.p3.1.m1.2b"><apply id="A6.p3.1.m1.2.3.cmml" xref="A6.p3.1.m1.2.3"><csymbol cd="ambiguous" id="A6.p3.1.m1.2.3.1.cmml" xref="A6.p3.1.m1.2.3">subscript</csymbol><ci id="A6.p3.1.m1.2.3.2.cmml" xref="A6.p3.1.m1.2.3.2">ùëÇ</ci><list id="A6.p3.1.m1.2.2.2.3.cmml" xref="A6.p3.1.m1.2.2.2.4"><ci id="A6.p3.1.m1.1.1.1.1.cmml" xref="A6.p3.1.m1.1.1.1.1">ùë•</ci><ci id="A6.p3.1.m1.2.2.2.2.cmml" xref="A6.p3.1.m1.2.2.2.2">ùë¶</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.1.m1.2c">O_{x,y}</annotation><annotation encoding="application/x-llamapun" id="A6.p3.1.m1.2d">italic_O start_POSTSUBSCRIPT italic_x , italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="A6.p3.2.m2.1"><semantics id="A6.p3.2.m2.1a"><mi id="A6.p3.2.m2.1.1" xref="A6.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A6.p3.2.m2.1b"><ci id="A6.p3.2.m2.1.1.cmml" xref="A6.p3.2.m2.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="A6.p3.2.m2.1d">italic_x</annotation></semantics></math> denotes the percentage of object occlusion and <math alttext="y" class="ltx_Math" display="inline" id="A6.p3.3.m3.1"><semantics id="A6.p3.3.m3.1a"><mi id="A6.p3.3.m3.1.1" xref="A6.p3.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A6.p3.3.m3.1b"><ci id="A6.p3.3.m3.1.1.cmml" xref="A6.p3.3.m3.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p3.3.m3.1c">y</annotation><annotation encoding="application/x-llamapun" id="A6.p3.3.m3.1d">italic_y</annotation></semantics></math> denotes the number of 2x upscalings:</p>
</div>
<figure class="ltx_table" id="A6.3">
<div class="ltx_inline-block ltx_transformed_outer" id="A6.3.3" style="width:433.6pt;height:55.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(145.4pt,-18.6pt) scale(3.03407306809222,3.03407306809222) ;">
<table class="ltx_tabular ltx_align_middle" id="A6.3.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.3.3.3.3">
<td class="ltx_td ltx_align_center" id="A6.1.1.1.1.1"><math alttext="O_{25,0}=\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix}" class="ltx_Math" display="inline" id="A6.1.1.1.1.1.m1.3"><semantics id="A6.1.1.1.1.1.m1.3a"><mrow id="A6.1.1.1.1.1.m1.3.4" xref="A6.1.1.1.1.1.m1.3.4.cmml"><msub id="A6.1.1.1.1.1.m1.3.4.2" xref="A6.1.1.1.1.1.m1.3.4.2.cmml"><mi id="A6.1.1.1.1.1.m1.3.4.2.2" xref="A6.1.1.1.1.1.m1.3.4.2.2.cmml">O</mi><mrow id="A6.1.1.1.1.1.m1.3.3.2.4" xref="A6.1.1.1.1.1.m1.3.3.2.3.cmml"><mn id="A6.1.1.1.1.1.m1.2.2.1.1" xref="A6.1.1.1.1.1.m1.2.2.1.1.cmml">25</mn><mo id="A6.1.1.1.1.1.m1.3.3.2.4.1" xref="A6.1.1.1.1.1.m1.3.3.2.3.cmml">,</mo><mn id="A6.1.1.1.1.1.m1.3.3.2.2" xref="A6.1.1.1.1.1.m1.3.3.2.2.cmml">0</mn></mrow></msub><mo id="A6.1.1.1.1.1.m1.3.4.1" xref="A6.1.1.1.1.1.m1.3.4.1.cmml">=</mo><mrow id="A6.1.1.1.1.1.m1.1.1.3" xref="A6.1.1.1.1.1.m1.1.1.2.cmml"><mo id="A6.1.1.1.1.1.m1.1.1.3.1" xref="A6.1.1.1.1.1.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="A6.1.1.1.1.1.m1.1.1.1.1" rowspacing="0pt" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mtr id="A6.1.1.1.1.1.m1.1.1.1.1a" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.1.1.1.1.1.m1.1.1.1.1b" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="A6.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="A6.1.1.1.1.1.m1.1.1.1.1c" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.1.1.1.1.1.m1.1.1.1.1.1.2.1" xref="A6.1.1.1.1.1.m1.1.1.1.1.1.2.1.cmml">0</mn></mtd></mtr><mtr id="A6.1.1.1.1.1.m1.1.1.1.1d" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.1.1.1.1.1.m1.1.1.1.1e" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.1.1.1.1.1.m1.1.1.1.1.2.1.1" xref="A6.1.1.1.1.1.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="A6.1.1.1.1.1.m1.1.1.1.1f" xref="A6.1.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.1.1.1.1.1.m1.1.1.1.1.2.2.1" xref="A6.1.1.1.1.1.m1.1.1.1.1.2.2.1.cmml">0</mn></mtd></mtr></mtable><mo id="A6.1.1.1.1.1.m1.1.1.3.2" xref="A6.1.1.1.1.1.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.1.1.1.1.1.m1.3b"><apply id="A6.1.1.1.1.1.m1.3.4.cmml" xref="A6.1.1.1.1.1.m1.3.4"><eq id="A6.1.1.1.1.1.m1.3.4.1.cmml" xref="A6.1.1.1.1.1.m1.3.4.1"></eq><apply id="A6.1.1.1.1.1.m1.3.4.2.cmml" xref="A6.1.1.1.1.1.m1.3.4.2"><csymbol cd="ambiguous" id="A6.1.1.1.1.1.m1.3.4.2.1.cmml" xref="A6.1.1.1.1.1.m1.3.4.2">subscript</csymbol><ci id="A6.1.1.1.1.1.m1.3.4.2.2.cmml" xref="A6.1.1.1.1.1.m1.3.4.2.2">ùëÇ</ci><list id="A6.1.1.1.1.1.m1.3.3.2.3.cmml" xref="A6.1.1.1.1.1.m1.3.3.2.4"><cn id="A6.1.1.1.1.1.m1.2.2.1.1.cmml" type="integer" xref="A6.1.1.1.1.1.m1.2.2.1.1">25</cn><cn id="A6.1.1.1.1.1.m1.3.3.2.2.cmml" type="integer" xref="A6.1.1.1.1.1.m1.3.3.2.2">0</cn></list></apply><apply id="A6.1.1.1.1.1.m1.1.1.2.cmml" xref="A6.1.1.1.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.1.1.1.1.1.m1.1.1.2.1.cmml" xref="A6.1.1.1.1.1.m1.1.1.3.1">matrix</csymbol><matrix id="A6.1.1.1.1.1.m1.1.1.1.1.cmml" xref="A6.1.1.1.1.1.m1.1.1.1.1"><matrixrow id="A6.1.1.1.1.1.m1.1.1.1.1a.cmml" xref="A6.1.1.1.1.1.m1.1.1.1.1"><cn id="A6.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="A6.1.1.1.1.1.m1.1.1.1.1.1.1.1">1</cn><cn id="A6.1.1.1.1.1.m1.1.1.1.1.1.2.1.cmml" type="integer" xref="A6.1.1.1.1.1.m1.1.1.1.1.1.2.1">0</cn></matrixrow><matrixrow id="A6.1.1.1.1.1.m1.1.1.1.1b.cmml" xref="A6.1.1.1.1.1.m1.1.1.1.1"><cn id="A6.1.1.1.1.1.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="A6.1.1.1.1.1.m1.1.1.1.1.2.1.1">0</cn><cn id="A6.1.1.1.1.1.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="A6.1.1.1.1.1.m1.1.1.1.1.2.2.1">0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.1.1.1.1.1.m1.3c">O_{25,0}=\begin{bmatrix}1&amp;0\\
0&amp;0\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="A6.1.1.1.1.1.m1.3d">italic_O start_POSTSUBSCRIPT 25 , 0 end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="A6.2.2.2.2.2"><math alttext="O_{50,0}=\begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix}" class="ltx_Math" display="inline" id="A6.2.2.2.2.2.m1.3"><semantics id="A6.2.2.2.2.2.m1.3a"><mrow id="A6.2.2.2.2.2.m1.3.4" xref="A6.2.2.2.2.2.m1.3.4.cmml"><msub id="A6.2.2.2.2.2.m1.3.4.2" xref="A6.2.2.2.2.2.m1.3.4.2.cmml"><mi id="A6.2.2.2.2.2.m1.3.4.2.2" xref="A6.2.2.2.2.2.m1.3.4.2.2.cmml">O</mi><mrow id="A6.2.2.2.2.2.m1.3.3.2.4" xref="A6.2.2.2.2.2.m1.3.3.2.3.cmml"><mn id="A6.2.2.2.2.2.m1.2.2.1.1" xref="A6.2.2.2.2.2.m1.2.2.1.1.cmml">50</mn><mo id="A6.2.2.2.2.2.m1.3.3.2.4.1" xref="A6.2.2.2.2.2.m1.3.3.2.3.cmml">,</mo><mn id="A6.2.2.2.2.2.m1.3.3.2.2" xref="A6.2.2.2.2.2.m1.3.3.2.2.cmml">0</mn></mrow></msub><mo id="A6.2.2.2.2.2.m1.3.4.1" xref="A6.2.2.2.2.2.m1.3.4.1.cmml">=</mo><mrow id="A6.2.2.2.2.2.m1.1.1.3" xref="A6.2.2.2.2.2.m1.1.1.2.cmml"><mo id="A6.2.2.2.2.2.m1.1.1.3.1" xref="A6.2.2.2.2.2.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="A6.2.2.2.2.2.m1.1.1.1.1" rowspacing="0pt" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mtr id="A6.2.2.2.2.2.m1.1.1.1.1a" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.2.2.2.2.2.m1.1.1.1.1b" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.2.2.2.2.2.m1.1.1.1.1.1.1.1" xref="A6.2.2.2.2.2.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="A6.2.2.2.2.2.m1.1.1.1.1c" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.2.2.2.2.2.m1.1.1.1.1.1.2.1" xref="A6.2.2.2.2.2.m1.1.1.1.1.1.2.1.cmml">0</mn></mtd></mtr><mtr id="A6.2.2.2.2.2.m1.1.1.1.1d" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.2.2.2.2.2.m1.1.1.1.1e" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.2.2.2.2.2.m1.1.1.1.1.2.1.1" xref="A6.2.2.2.2.2.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="A6.2.2.2.2.2.m1.1.1.1.1f" xref="A6.2.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.2.2.2.2.2.m1.1.1.1.1.2.2.1" xref="A6.2.2.2.2.2.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd></mtr></mtable><mo id="A6.2.2.2.2.2.m1.1.1.3.2" xref="A6.2.2.2.2.2.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.2.2.2.2.2.m1.3b"><apply id="A6.2.2.2.2.2.m1.3.4.cmml" xref="A6.2.2.2.2.2.m1.3.4"><eq id="A6.2.2.2.2.2.m1.3.4.1.cmml" xref="A6.2.2.2.2.2.m1.3.4.1"></eq><apply id="A6.2.2.2.2.2.m1.3.4.2.cmml" xref="A6.2.2.2.2.2.m1.3.4.2"><csymbol cd="ambiguous" id="A6.2.2.2.2.2.m1.3.4.2.1.cmml" xref="A6.2.2.2.2.2.m1.3.4.2">subscript</csymbol><ci id="A6.2.2.2.2.2.m1.3.4.2.2.cmml" xref="A6.2.2.2.2.2.m1.3.4.2.2">ùëÇ</ci><list id="A6.2.2.2.2.2.m1.3.3.2.3.cmml" xref="A6.2.2.2.2.2.m1.3.3.2.4"><cn id="A6.2.2.2.2.2.m1.2.2.1.1.cmml" type="integer" xref="A6.2.2.2.2.2.m1.2.2.1.1">50</cn><cn id="A6.2.2.2.2.2.m1.3.3.2.2.cmml" type="integer" xref="A6.2.2.2.2.2.m1.3.3.2.2">0</cn></list></apply><apply id="A6.2.2.2.2.2.m1.1.1.2.cmml" xref="A6.2.2.2.2.2.m1.1.1.3"><csymbol cd="latexml" id="A6.2.2.2.2.2.m1.1.1.2.1.cmml" xref="A6.2.2.2.2.2.m1.1.1.3.1">matrix</csymbol><matrix id="A6.2.2.2.2.2.m1.1.1.1.1.cmml" xref="A6.2.2.2.2.2.m1.1.1.1.1"><matrixrow id="A6.2.2.2.2.2.m1.1.1.1.1a.cmml" xref="A6.2.2.2.2.2.m1.1.1.1.1"><cn id="A6.2.2.2.2.2.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="A6.2.2.2.2.2.m1.1.1.1.1.1.1.1">1</cn><cn id="A6.2.2.2.2.2.m1.1.1.1.1.1.2.1.cmml" type="integer" xref="A6.2.2.2.2.2.m1.1.1.1.1.1.2.1">0</cn></matrixrow><matrixrow id="A6.2.2.2.2.2.m1.1.1.1.1b.cmml" xref="A6.2.2.2.2.2.m1.1.1.1.1"><cn id="A6.2.2.2.2.2.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="A6.2.2.2.2.2.m1.1.1.1.1.2.1.1">0</cn><cn id="A6.2.2.2.2.2.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="A6.2.2.2.2.2.m1.1.1.1.1.2.2.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.2.2.2.2.2.m1.3c">O_{50,0}=\begin{bmatrix}1&amp;0\\
0&amp;1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="A6.2.2.2.2.2.m1.3d">italic_O start_POSTSUBSCRIPT 50 , 0 end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="A6.3.3.3.3.3"><math alttext="O_{75,0}=\begin{bmatrix}1&amp;1\\
0&amp;1\end{bmatrix}" class="ltx_Math" display="inline" id="A6.3.3.3.3.3.m1.3"><semantics id="A6.3.3.3.3.3.m1.3a"><mrow id="A6.3.3.3.3.3.m1.3.4" xref="A6.3.3.3.3.3.m1.3.4.cmml"><msub id="A6.3.3.3.3.3.m1.3.4.2" xref="A6.3.3.3.3.3.m1.3.4.2.cmml"><mi id="A6.3.3.3.3.3.m1.3.4.2.2" xref="A6.3.3.3.3.3.m1.3.4.2.2.cmml">O</mi><mrow id="A6.3.3.3.3.3.m1.3.3.2.4" xref="A6.3.3.3.3.3.m1.3.3.2.3.cmml"><mn id="A6.3.3.3.3.3.m1.2.2.1.1" xref="A6.3.3.3.3.3.m1.2.2.1.1.cmml">75</mn><mo id="A6.3.3.3.3.3.m1.3.3.2.4.1" xref="A6.3.3.3.3.3.m1.3.3.2.3.cmml">,</mo><mn id="A6.3.3.3.3.3.m1.3.3.2.2" xref="A6.3.3.3.3.3.m1.3.3.2.2.cmml">0</mn></mrow></msub><mo id="A6.3.3.3.3.3.m1.3.4.1" xref="A6.3.3.3.3.3.m1.3.4.1.cmml">=</mo><mrow id="A6.3.3.3.3.3.m1.1.1.3" xref="A6.3.3.3.3.3.m1.1.1.2.cmml"><mo id="A6.3.3.3.3.3.m1.1.1.3.1" xref="A6.3.3.3.3.3.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="A6.3.3.3.3.3.m1.1.1.1.1" rowspacing="0pt" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mtr id="A6.3.3.3.3.3.m1.1.1.1.1a" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mtd id="A6.3.3.3.3.3.m1.1.1.1.1b" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mn id="A6.3.3.3.3.3.m1.1.1.1.1.1.1.1" xref="A6.3.3.3.3.3.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="A6.3.3.3.3.3.m1.1.1.1.1c" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mn id="A6.3.3.3.3.3.m1.1.1.1.1.1.2.1" xref="A6.3.3.3.3.3.m1.1.1.1.1.1.2.1.cmml">1</mn></mtd></mtr><mtr id="A6.3.3.3.3.3.m1.1.1.1.1d" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mtd id="A6.3.3.3.3.3.m1.1.1.1.1e" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mn id="A6.3.3.3.3.3.m1.1.1.1.1.2.1.1" xref="A6.3.3.3.3.3.m1.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="A6.3.3.3.3.3.m1.1.1.1.1f" xref="A6.3.3.3.3.3.m1.1.1.1.1.cmml"><mn id="A6.3.3.3.3.3.m1.1.1.1.1.2.2.1" xref="A6.3.3.3.3.3.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd></mtr></mtable><mo id="A6.3.3.3.3.3.m1.1.1.3.2" xref="A6.3.3.3.3.3.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.3.3.3.3.3.m1.3b"><apply id="A6.3.3.3.3.3.m1.3.4.cmml" xref="A6.3.3.3.3.3.m1.3.4"><eq id="A6.3.3.3.3.3.m1.3.4.1.cmml" xref="A6.3.3.3.3.3.m1.3.4.1"></eq><apply id="A6.3.3.3.3.3.m1.3.4.2.cmml" xref="A6.3.3.3.3.3.m1.3.4.2"><csymbol cd="ambiguous" id="A6.3.3.3.3.3.m1.3.4.2.1.cmml" xref="A6.3.3.3.3.3.m1.3.4.2">subscript</csymbol><ci id="A6.3.3.3.3.3.m1.3.4.2.2.cmml" xref="A6.3.3.3.3.3.m1.3.4.2.2">ùëÇ</ci><list id="A6.3.3.3.3.3.m1.3.3.2.3.cmml" xref="A6.3.3.3.3.3.m1.3.3.2.4"><cn id="A6.3.3.3.3.3.m1.2.2.1.1.cmml" type="integer" xref="A6.3.3.3.3.3.m1.2.2.1.1">75</cn><cn id="A6.3.3.3.3.3.m1.3.3.2.2.cmml" type="integer" xref="A6.3.3.3.3.3.m1.3.3.2.2">0</cn></list></apply><apply id="A6.3.3.3.3.3.m1.1.1.2.cmml" xref="A6.3.3.3.3.3.m1.1.1.3"><csymbol cd="latexml" id="A6.3.3.3.3.3.m1.1.1.2.1.cmml" xref="A6.3.3.3.3.3.m1.1.1.3.1">matrix</csymbol><matrix id="A6.3.3.3.3.3.m1.1.1.1.1.cmml" xref="A6.3.3.3.3.3.m1.1.1.1.1"><matrixrow id="A6.3.3.3.3.3.m1.1.1.1.1a.cmml" xref="A6.3.3.3.3.3.m1.1.1.1.1"><cn id="A6.3.3.3.3.3.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="A6.3.3.3.3.3.m1.1.1.1.1.1.1.1">1</cn><cn id="A6.3.3.3.3.3.m1.1.1.1.1.1.2.1.cmml" type="integer" xref="A6.3.3.3.3.3.m1.1.1.1.1.1.2.1">1</cn></matrixrow><matrixrow id="A6.3.3.3.3.3.m1.1.1.1.1b.cmml" xref="A6.3.3.3.3.3.m1.1.1.1.1"><cn id="A6.3.3.3.3.3.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="A6.3.3.3.3.3.m1.1.1.1.1.2.1.1">0</cn><cn id="A6.3.3.3.3.3.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="A6.3.3.3.3.3.m1.1.1.1.1.2.2.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.3.3.3.3.3.m1.3c">O_{75,0}=\begin{bmatrix}1&amp;1\\
0&amp;1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="A6.3.3.3.3.3.m1.3d">italic_O start_POSTSUBSCRIPT 75 , 0 end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A6.5">
<div class="ltx_inline-block ltx_transformed_outer" id="A6.5.2" style="width:433.6pt;height:182.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(149.2pt,-62.8pt) scale(3.20539958026425,3.20539958026425) ;">
<table class="ltx_tabular ltx_align_middle" id="A6.5.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.5.2.2.2">
<td class="ltx_td ltx_align_center" id="A6.4.1.1.1.1"><math alttext="O_{50,1}=\begin{bmatrix}1&amp;1&amp;0&amp;0\\
1&amp;1&amp;0&amp;0\\
0&amp;0&amp;1&amp;1\\
0&amp;0&amp;1&amp;1\end{bmatrix}" class="ltx_Math" display="inline" id="A6.4.1.1.1.1.m1.3"><semantics id="A6.4.1.1.1.1.m1.3a"><mrow id="A6.4.1.1.1.1.m1.3.4" xref="A6.4.1.1.1.1.m1.3.4.cmml"><msub id="A6.4.1.1.1.1.m1.3.4.2" xref="A6.4.1.1.1.1.m1.3.4.2.cmml"><mi id="A6.4.1.1.1.1.m1.3.4.2.2" xref="A6.4.1.1.1.1.m1.3.4.2.2.cmml">O</mi><mrow id="A6.4.1.1.1.1.m1.3.3.2.4" xref="A6.4.1.1.1.1.m1.3.3.2.3.cmml"><mn id="A6.4.1.1.1.1.m1.2.2.1.1" xref="A6.4.1.1.1.1.m1.2.2.1.1.cmml">50</mn><mo id="A6.4.1.1.1.1.m1.3.3.2.4.1" xref="A6.4.1.1.1.1.m1.3.3.2.3.cmml">,</mo><mn id="A6.4.1.1.1.1.m1.3.3.2.2" xref="A6.4.1.1.1.1.m1.3.3.2.2.cmml">1</mn></mrow></msub><mo id="A6.4.1.1.1.1.m1.3.4.1" xref="A6.4.1.1.1.1.m1.3.4.1.cmml">=</mo><mrow id="A6.4.1.1.1.1.m1.1.1.3" xref="A6.4.1.1.1.1.m1.1.1.2.cmml"><mo id="A6.4.1.1.1.1.m1.1.1.3.1" xref="A6.4.1.1.1.1.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="A6.4.1.1.1.1.m1.1.1.1.1" rowspacing="0pt" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mtr id="A6.4.1.1.1.1.m1.1.1.1.1a" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.4.1.1.1.1.m1.1.1.1.1b" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.1.1.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1c" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.1.2.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.2.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1d" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.1.3.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.3.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1e" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.1.4.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.4.1.cmml">0</mn></mtd></mtr><mtr id="A6.4.1.1.1.1.m1.1.1.1.1f" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.4.1.1.1.1.m1.1.1.1.1g" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.2.1.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.1.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1h" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.2.2.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1i" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.2.3.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.3.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1j" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.2.4.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.4.1.cmml">0</mn></mtd></mtr><mtr id="A6.4.1.1.1.1.m1.1.1.1.1k" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.4.1.1.1.1.m1.1.1.1.1l" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.3.1.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.1.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1m" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.3.2.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.2.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1n" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.3.3.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.3.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1o" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.3.4.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.4.1.cmml">1</mn></mtd></mtr><mtr id="A6.4.1.1.1.1.m1.1.1.1.1p" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mtd id="A6.4.1.1.1.1.m1.1.1.1.1q" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.4.1.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.1.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1r" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.4.2.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.2.1.cmml">0</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1s" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.4.3.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.3.1.cmml">1</mn></mtd><mtd id="A6.4.1.1.1.1.m1.1.1.1.1t" xref="A6.4.1.1.1.1.m1.1.1.1.1.cmml"><mn id="A6.4.1.1.1.1.m1.1.1.1.1.4.4.1" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.4.1.cmml">1</mn></mtd></mtr></mtable><mo id="A6.4.1.1.1.1.m1.1.1.3.2" xref="A6.4.1.1.1.1.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.4.1.1.1.1.m1.3b"><apply id="A6.4.1.1.1.1.m1.3.4.cmml" xref="A6.4.1.1.1.1.m1.3.4"><eq id="A6.4.1.1.1.1.m1.3.4.1.cmml" xref="A6.4.1.1.1.1.m1.3.4.1"></eq><apply id="A6.4.1.1.1.1.m1.3.4.2.cmml" xref="A6.4.1.1.1.1.m1.3.4.2"><csymbol cd="ambiguous" id="A6.4.1.1.1.1.m1.3.4.2.1.cmml" xref="A6.4.1.1.1.1.m1.3.4.2">subscript</csymbol><ci id="A6.4.1.1.1.1.m1.3.4.2.2.cmml" xref="A6.4.1.1.1.1.m1.3.4.2.2">ùëÇ</ci><list id="A6.4.1.1.1.1.m1.3.3.2.3.cmml" xref="A6.4.1.1.1.1.m1.3.3.2.4"><cn id="A6.4.1.1.1.1.m1.2.2.1.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.2.2.1.1">50</cn><cn id="A6.4.1.1.1.1.m1.3.3.2.2.cmml" type="integer" xref="A6.4.1.1.1.1.m1.3.3.2.2">1</cn></list></apply><apply id="A6.4.1.1.1.1.m1.1.1.2.cmml" xref="A6.4.1.1.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.4.1.1.1.1.m1.1.1.2.1.cmml" xref="A6.4.1.1.1.1.m1.1.1.3.1">matrix</csymbol><matrix id="A6.4.1.1.1.1.m1.1.1.1.1.cmml" xref="A6.4.1.1.1.1.m1.1.1.1.1"><matrixrow id="A6.4.1.1.1.1.m1.1.1.1.1a.cmml" xref="A6.4.1.1.1.1.m1.1.1.1.1"><cn id="A6.4.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.1.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.1.2.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.2.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.1.3.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.3.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.1.4.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.1.4.1">0</cn></matrixrow><matrixrow id="A6.4.1.1.1.1.m1.1.1.1.1b.cmml" xref="A6.4.1.1.1.1.m1.1.1.1.1"><cn id="A6.4.1.1.1.1.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.1.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.2.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.2.3.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.3.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.2.4.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.2.4.1">0</cn></matrixrow><matrixrow id="A6.4.1.1.1.1.m1.1.1.1.1c.cmml" xref="A6.4.1.1.1.1.m1.1.1.1.1"><cn id="A6.4.1.1.1.1.m1.1.1.1.1.3.1.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.1.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.3.2.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.2.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.3.3.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.3.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.3.4.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.3.4.1">1</cn></matrixrow><matrixrow id="A6.4.1.1.1.1.m1.1.1.1.1d.cmml" xref="A6.4.1.1.1.1.m1.1.1.1.1"><cn id="A6.4.1.1.1.1.m1.1.1.1.1.4.1.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.1.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.4.2.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.2.1">0</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.4.3.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.3.1">1</cn><cn id="A6.4.1.1.1.1.m1.1.1.1.1.4.4.1.cmml" type="integer" xref="A6.4.1.1.1.1.m1.1.1.1.1.4.4.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.4.1.1.1.1.m1.3c">O_{50,1}=\begin{bmatrix}1&amp;1&amp;0&amp;0\\
1&amp;1&amp;0&amp;0\\
0&amp;0&amp;1&amp;1\\
0&amp;0&amp;1&amp;1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="A6.4.1.1.1.1.m1.3d">italic_O start_POSTSUBSCRIPT 50 , 1 end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="A6.5.2.2.2.2"><math alttext="O_{50,2}=\begin{bmatrix}1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\end{bmatrix}" class="ltx_Math" display="inline" id="A6.5.2.2.2.2.m1.3"><semantics id="A6.5.2.2.2.2.m1.3a"><mrow id="A6.5.2.2.2.2.m1.3.4" xref="A6.5.2.2.2.2.m1.3.4.cmml"><msub id="A6.5.2.2.2.2.m1.3.4.2" xref="A6.5.2.2.2.2.m1.3.4.2.cmml"><mi id="A6.5.2.2.2.2.m1.3.4.2.2" xref="A6.5.2.2.2.2.m1.3.4.2.2.cmml">O</mi><mrow id="A6.5.2.2.2.2.m1.3.3.2.4" xref="A6.5.2.2.2.2.m1.3.3.2.3.cmml"><mn id="A6.5.2.2.2.2.m1.2.2.1.1" xref="A6.5.2.2.2.2.m1.2.2.1.1.cmml">50</mn><mo id="A6.5.2.2.2.2.m1.3.3.2.4.1" xref="A6.5.2.2.2.2.m1.3.3.2.3.cmml">,</mo><mn id="A6.5.2.2.2.2.m1.3.3.2.2" xref="A6.5.2.2.2.2.m1.3.3.2.2.cmml">2</mn></mrow></msub><mo id="A6.5.2.2.2.2.m1.3.4.1" xref="A6.5.2.2.2.2.m1.3.4.1.cmml">=</mo><mrow id="A6.5.2.2.2.2.m1.1.1.3" xref="A6.5.2.2.2.2.m1.1.1.2.cmml"><mo id="A6.5.2.2.2.2.m1.1.1.3.1" xref="A6.5.2.2.2.2.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="A6.5.2.2.2.2.m1.1.1.1.1" rowspacing="0pt" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtr id="A6.5.2.2.2.2.m1.1.1.1.1a" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1b" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1c" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.2.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1d" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.3.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1e" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.4.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1f" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.5.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1g" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.6.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1h" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.7.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1i" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.1.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.8.1.cmml">0</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1j" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1k" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.1.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1l" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1m" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.3.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1n" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.4.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1o" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.5.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1p" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.6.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1q" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.7.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1r" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.2.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.8.1.cmml">0</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1s" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1t" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.1.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1u" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.2.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1v" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.3.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1w" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.4.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1x" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.5.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1y" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.6.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1z" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.7.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1aa" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.3.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.8.1.cmml">0</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1ab" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ac" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.1.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ad" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.2.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ae" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.3.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1af" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.4.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ag" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.5.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ah" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.6.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ai" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.7.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1aj" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.4.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.8.1.cmml">0</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1ak" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1al" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.1.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1am" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.2.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1an" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.3.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ao" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.4.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ap" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.5.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1aq" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.6.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ar" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.7.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1as" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.5.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.8.1.cmml">1</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1at" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1au" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.1.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1av" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.2.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1aw" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.3.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ax" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.4.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ay" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.5.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1az" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.6.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1ba" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.7.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bb" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.6.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.8.1.cmml">1</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1bc" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bd" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.1.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1be" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.2.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bf" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.3.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bg" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.4.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bh" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.5.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bi" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.6.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bj" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.7.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bk" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.7.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.8.1.cmml">1</mn></mtd></mtr><mtr id="A6.5.2.2.2.2.m1.1.1.1.1bl" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bm" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.1.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.1.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bn" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.2.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.2.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bo" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.3.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.3.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bp" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.4.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.4.1.cmml">0</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bq" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.5.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.5.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1br" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.6.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.6.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bs" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.7.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.7.1.cmml">1</mn></mtd><mtd id="A6.5.2.2.2.2.m1.1.1.1.1bt" xref="A6.5.2.2.2.2.m1.1.1.1.1.cmml"><mn id="A6.5.2.2.2.2.m1.1.1.1.1.8.8.1" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.8.1.cmml">1</mn></mtd></mtr></mtable><mo id="A6.5.2.2.2.2.m1.1.1.3.2" xref="A6.5.2.2.2.2.m1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.5.2.2.2.2.m1.3b"><apply id="A6.5.2.2.2.2.m1.3.4.cmml" xref="A6.5.2.2.2.2.m1.3.4"><eq id="A6.5.2.2.2.2.m1.3.4.1.cmml" xref="A6.5.2.2.2.2.m1.3.4.1"></eq><apply id="A6.5.2.2.2.2.m1.3.4.2.cmml" xref="A6.5.2.2.2.2.m1.3.4.2"><csymbol cd="ambiguous" id="A6.5.2.2.2.2.m1.3.4.2.1.cmml" xref="A6.5.2.2.2.2.m1.3.4.2">subscript</csymbol><ci id="A6.5.2.2.2.2.m1.3.4.2.2.cmml" xref="A6.5.2.2.2.2.m1.3.4.2.2">ùëÇ</ci><list id="A6.5.2.2.2.2.m1.3.3.2.3.cmml" xref="A6.5.2.2.2.2.m1.3.3.2.4"><cn id="A6.5.2.2.2.2.m1.2.2.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.2.2.1.1">50</cn><cn id="A6.5.2.2.2.2.m1.3.3.2.2.cmml" type="integer" xref="A6.5.2.2.2.2.m1.3.3.2.2">2</cn></list></apply><apply id="A6.5.2.2.2.2.m1.1.1.2.cmml" xref="A6.5.2.2.2.2.m1.1.1.3"><csymbol cd="latexml" id="A6.5.2.2.2.2.m1.1.1.2.1.cmml" xref="A6.5.2.2.2.2.m1.1.1.3.1">matrix</csymbol><matrix id="A6.5.2.2.2.2.m1.1.1.1.1.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1a.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.1.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.2.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.3.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.4.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.5.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.6.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.7.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.1.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.1.8.1">0</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1b.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.1.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.2.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.3.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.4.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.5.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.6.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.7.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.2.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.2.8.1">0</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1c.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.1.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.2.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.3.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.4.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.5.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.6.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.7.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.3.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.3.8.1">0</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1d.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.1.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.2.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.3.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.4.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.5.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.6.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.7.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.4.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.4.8.1">0</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1e.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.1.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.2.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.3.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.4.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.5.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.6.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.7.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.5.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.5.8.1">1</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1f.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.1.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.2.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.3.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.4.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.5.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.6.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.7.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.6.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.6.8.1">1</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1g.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.1.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.2.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.3.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.4.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.5.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.6.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.7.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.7.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.7.8.1">1</cn></matrixrow><matrixrow id="A6.5.2.2.2.2.m1.1.1.1.1h.cmml" xref="A6.5.2.2.2.2.m1.1.1.1.1"><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.1.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.1.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.2.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.2.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.3.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.3.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.4.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.4.1">0</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.5.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.5.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.6.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.6.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.7.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.7.1">1</cn><cn id="A6.5.2.2.2.2.m1.1.1.1.1.8.8.1.cmml" type="integer" xref="A6.5.2.2.2.2.m1.1.1.1.1.8.8.1">1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.5.2.2.2.2.m1.3c">O_{50,2}=\begin{bmatrix}1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
1&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\
0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\end{bmatrix}</annotation><annotation encoding="application/x-llamapun" id="A6.5.2.2.2.2.m1.3d">italic_O start_POSTSUBSCRIPT 50 , 2 end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Additional Vision Transformer attention maps</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">In this section we provide additional images such as those in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10775v1#S6.F16" title="Figure 16 ‚Ä£ 6.4 Further analysis: are models robust to diffuse occlusion? ‚Ä£ 6 Experimental Results ‚Ä£ Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?"><span class="ltx_text ltx_ref_tag">16</span></a> to corroborate results in this section. As shown in these figures, there exists a pattern in which Vision Transformer models are able to attend to pixels on target and successfully ignore occluders when they are large, and that this relative attention becomes less clear as the occluders get smaller.</p>
</div>
<figure class="ltx_figure" id="A7.F17">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A7.F17.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A7.F17.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F17.4.4.4.5">(16)</th>
<td class="ltx_td ltx_align_center" id="A7.F17.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.1.1.1.1.g1" src="extracted/5858817/us4o48_b50_344_00024_6adad2f7_img_0000025.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.2.2.2.2.g1" src="extracted/5858817/t50_us4_o48_b50_344_00024_6adad2f7_img_0000025_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.3.3.3.3.g1" src="extracted/5858817/t50_us4_o48_b50_344_00024_6adad2f7_img_0000025_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.4.4.4.4.g1" src="extracted/5858817/t50_us4_o48_b50_344_00024_6adad2f7_img_0000025_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F17.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F17.8.8.8.5">(8)</th>
<td class="ltx_td ltx_align_center" id="A7.F17.5.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.5.5.5.1.g1" src="extracted/5858817/us3o50_b49_344_00024_6adad2f7_img_0000025.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.6.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.6.6.6.2.g1" src="extracted/5858817/t50_us3_o50_b49_344_00024_6adad2f7_img_0000025_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.7.7.7.3.g1" src="extracted/5858817/t50_us3_o50_b49_344_00024_6adad2f7_img_0000025_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.8.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.8.8.8.4.g1" src="extracted/5858817/t50_us3_o50_b49_344_00024_6adad2f7_img_0000025_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F17.12.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F17.12.12.12.5">(4)</th>
<td class="ltx_td ltx_align_center" id="A7.F17.9.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.9.9.9.1.g1" src="extracted/5858817/us2o49_b50_344_00024_6adad2f7_img_0000025.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.10.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.10.10.10.2.g1" src="extracted/5858817/t50_us2_o49_b50_344_00024_6adad2f7_img_0000025_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.11.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.11.11.11.3.g1" src="extracted/5858817/t50_us2_o49_b50_344_00024_6adad2f7_img_0000025_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F17.12.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F17.12.12.12.4.g1" src="extracted/5858817/t50_us2_o49_b50_344_00024_6adad2f7_img_0000025_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F17.12.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F17.12.12.13.1.1">(size, px)</th>
<td class="ltx_td ltx_align_center" id="A7.F17.12.12.13.1.2">(a)</td>
<td class="ltx_td ltx_align_center" id="A7.F17.12.12.13.1.3">(b)</td>
<td class="ltx_td ltx_align_center" id="A7.F17.12.12.13.1.4">(c)</td>
<td class="ltx_td ltx_align_center" id="A7.F17.12.12.13.1.5">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Sample images of a cat (column (a)) at 50% occlusion, with various occluder sizes (in pixels, shown in row names) with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (column (b)) third, (column (c)) seventh, and (column (d)) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. Higher values (e.g., yellow) denote higher values for attention</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F18">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A7.F18.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A7.F18.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F18.4.4.4.5">(16)</th>
<td class="ltx_td ltx_align_center" id="A7.F18.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.1.1.1.1.g1" src="extracted/5858817/us4o49_b50_513_00014_a6adef89_img_0000015.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.2.2.2.2.g1" src="extracted/5858817/t50_us4_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.3.3.3.3.g1" src="extracted/5858817/t50_us4_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.4.4.4.4.g1" src="extracted/5858817/t50_us4_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F18.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F18.8.8.8.5">(8)</th>
<td class="ltx_td ltx_align_center" id="A7.F18.5.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.5.5.5.1.g1" src="extracted/5858817/us3o49_b50_513_00014_a6adef89_img_0000015.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.6.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.6.6.6.2.g1" src="extracted/5858817/t50_us3_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.7.7.7.3.g1" src="extracted/5858817/t50_us3_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.8.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.8.8.8.4.g1" src="extracted/5858817/t50_us3_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F18.12.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F18.12.12.12.5">(4)</th>
<td class="ltx_td ltx_align_center" id="A7.F18.9.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.9.9.9.1.g1" src="extracted/5858817/us2o49_b50_513_00014_a6adef89_img_0000015.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.10.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.10.10.10.2.g1" src="extracted/5858817/t50_us2_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.11.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.11.11.11.3.g1" src="extracted/5858817/t50_us2_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F18.12.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F18.12.12.12.4.g1" src="extracted/5858817/t50_us2_o49_b50_513_00014_a6adef89_img_0000015_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F18.12.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F18.12.12.13.1.1">(size, px)</th>
<td class="ltx_td ltx_align_center" id="A7.F18.12.12.13.1.2">(a)</td>
<td class="ltx_td ltx_align_center" id="A7.F18.12.12.13.1.3">(b)</td>
<td class="ltx_td ltx_align_center" id="A7.F18.12.12.13.1.4">(c)</td>
<td class="ltx_td ltx_align_center" id="A7.F18.12.12.13.1.5">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Sample images of an airplane (column (a)) at 50% occlusion, with various occluder sizes (in pixels, shown in row names) with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (column (b)) third, (column (c)) seventh, and (column (d)) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. Higher values (e.g., yellow) denote higher values for attention</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F19">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A7.F19.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A7.F19.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F19.4.4.4.5">(16)</th>
<td class="ltx_td ltx_align_center" id="A7.F19.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.1.1.1.1.g1" src="extracted/5858817/us4o50_b49_391_00064_4a41cbf8_img_0000065.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.2.2.2.2.g1" src="extracted/5858817/t50_us4_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.3.3.3.3.g1" src="extracted/5858817/t50_us4_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.4.4.4.4.g1" src="extracted/5858817/t50_us4_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F19.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F19.8.8.8.5">(8)</th>
<td class="ltx_td ltx_align_center" id="A7.F19.5.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.5.5.5.1.g1" src="extracted/5858817/us3o50_b49_391_00064_4a41cbf8_img_0000065.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.6.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.6.6.6.2.g1" src="extracted/5858817/t50_us3_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.7.7.7.3.g1" src="extracted/5858817/t50_us3_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.8.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.8.8.8.4.g1" src="extracted/5858817/t50_us3_o50_b49_391_00064_4a41cbf8_img_0000065_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F19.12.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F19.12.12.12.5">(4)</th>
<td class="ltx_td ltx_align_center" id="A7.F19.9.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.9.9.9.1.g1" src="extracted/5858817/us2o49_b50_391_00064_4a41cbf8_img_0000065.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.10.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.10.10.10.2.g1" src="extracted/5858817/t50_us2_o49_b50_391_00064_4a41cbf8_img_0000065_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.11.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.11.11.11.3.g1" src="extracted/5858817/t50_us2_o49_b50_391_00064_4a41cbf8_img_0000065_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F19.12.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F19.12.12.12.4.g1" src="extracted/5858817/t50_us2_o49_b50_391_00064_4a41cbf8_img_0000065_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F19.12.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F19.12.12.13.1.1">(size, px)</th>
<td class="ltx_td ltx_align_center" id="A7.F19.12.12.13.1.2">(a)</td>
<td class="ltx_td ltx_align_center" id="A7.F19.12.12.13.1.3">(b)</td>
<td class="ltx_td ltx_align_center" id="A7.F19.12.12.13.1.4">(c)</td>
<td class="ltx_td ltx_align_center" id="A7.F19.12.12.13.1.5">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Sample images of a fish (column (a)) at 50% occlusion, with various occluder sizes (in pixels, shown in row names) with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (column (b)) third, (column (c)) seventh, and (column (d)) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. Higher values (e.g., yellow) denote higher values for attention</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F20">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A7.F20.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A7.F20.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F20.4.4.4.5">(16)</th>
<td class="ltx_td ltx_align_center" id="A7.F20.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.1.1.1.1.g1" src="extracted/5858817/us4o47_b50_600_00037_733f2a02_img_0000038.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.2.2.2.2.g1" src="extracted/5858817/t50_us4_o47_b50_600_00037_733f2a02_img_0000038_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.3.3.3.3.g1" src="extracted/5858817/t50_us4_o47_b50_600_00037_733f2a02_img_0000038_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.4.4.4.4.g1" src="extracted/5858817/t50_us4_o47_b50_600_00037_733f2a02_img_0000038_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F20.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F20.8.8.8.5">(8)</th>
<td class="ltx_td ltx_align_center" id="A7.F20.5.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.5.5.5.1.g1" src="extracted/5858817/us3o50_b49_600_00037_733f2a02_img_0000038.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.6.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.6.6.6.2.g1" src="extracted/5858817/t50_us3_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.7.7.7.3.g1" src="extracted/5858817/t50_us3_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.8.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.8.8.8.4.g1" src="extracted/5858817/t50_us3_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F20.12.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F20.12.12.12.5">(4)</th>
<td class="ltx_td ltx_align_center" id="A7.F20.9.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.9.9.9.1.g1" src="extracted/5858817/us2o50_b49_600_00037_733f2a02_img_0000038.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.10.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.10.10.10.2.g1" src="extracted/5858817/t50_us2_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_2.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.11.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.11.11.11.3.g1" src="extracted/5858817/t50_us2_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_6.jpg" width="84"/></td>
<td class="ltx_td ltx_align_center" id="A7.F20.12.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="84" id="A7.F20.12.12.12.4.g1" src="extracted/5858817/t50_us2_o50_b49_600_00037_733f2a02_img_0000038_vit_layer_10.jpg" width="84"/></td>
</tr>
<tr class="ltx_tr" id="A7.F20.12.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A7.F20.12.12.13.1.1">(size, px)</th>
<td class="ltx_td ltx_align_center" id="A7.F20.12.12.13.1.2">(a)</td>
<td class="ltx_td ltx_align_center" id="A7.F20.12.12.13.1.3">(b)</td>
<td class="ltx_td ltx_align_center" id="A7.F20.12.12.13.1.4">(c)</td>
<td class="ltx_td ltx_align_center" id="A7.F20.12.12.13.1.5">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Sample images of a person (column (a)) at 50% occlusion, with various occluder sizes (in pixels, shown in row names) with corresponding self-attention maps returned by the ViT-B model trained on IRUO at the (column (b)) third, (column (c)) seventh, and (column (d)) eleventh transformer stages. Self-attention maps are summed over all points on target and all heads for each location in each layer displayed. Higher values (e.g., yellow) denote higher values for attention</figcaption>
</figure>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.1.1">
<span class="ltx_bibblock"><span class="ltx_ERROR undefined" id="bib.1.1.1.1">\bibcommenthead</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et¬†al. [2016]</span>
<span class="ltx_bibblock">
Bansal R, Raj G, Choudhury T (2016) Blur image detection using Laplacian operator and Open-CV. In: 2016 International Conference System Modeling &amp; Advancement in Research Trends (SMART), pp 63‚Äì67, <a class="ltx_ref" href="https:/doi.org/10.1109/SYSMART.2016.7894491" title="">10.1109/SYSMART.2016.7894491</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cen et¬†al. [2021]</span>
<span class="ltx_bibblock">
Cen F, Zhao X, Li W, et¬†al. (2021) Deep feature augmentation for occluded image classification. Pattern Recognition 111:107737. <a class="ltx_ref" href="https:/doi.org/10.1016/j.patcog.2020.107737" title="">10.1016/j.patcog.2020.107737</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0031320320305409" title="">https://www.sciencedirect.com/science/article/pii/S0031320320305409</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. [2022]</span>
<span class="ltx_bibblock">
Chen JN, Sun S, He J, et¬†al. (2022) TransMix: Attend to Mix for Vision Transformers. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, New Orleans, LA, USA, pp 12125‚Äì12134, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR52688.2022.01182" title="">10.1109/CVPR52688.2022.01182</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9879590/" title="">https://ieeexplore.ieee.org/document/9879590/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et¬†al. [2009]</span>
<span class="ltx_bibblock">
Deng J, Dong W, Socher R, et¬†al. (2009) ImageNet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp 248‚Äì255, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2009.5206848" title="">10.1109/CVPR.2009.5206848</a>, iSSN: 1063-6919

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et¬†al. [2014]</span>
<span class="ltx_bibblock">
Deng J, Russakovsky O, Krause J, et¬†al. (2014) Scalable multi-label annotation. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, Toronto Ontario Canada, pp 3099‚Äì3102, <a class="ltx_ref" href="https:/doi.org/10.1145/2556288.2557011" title="">10.1145/2556288.2557011</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/2556288.2557011" title="">https://dl.acm.org/doi/10.1145/2556288.2557011</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeVries and Taylor [2017]</span>
<span class="ltx_bibblock">
DeVries T, Taylor GW (2017) Improved Regularization of Convolutional Neural Networks with Cutout. <a class="ltx_ref" href="https:/doi.org/10.48550/arXiv.1708.04552" title="">10.48550/arXiv.1708.04552</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1708.04552" title="">http://arxiv.org/abs/1708.04552</a>, arXiv:1708.04552 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et¬†al. [2021]</span>
<span class="ltx_bibblock">
Dosovitskiy A, Beyer L, Kolesnikov A, et¬†al. (2021) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=YicbFdNTTy" title="">https://openreview.net/forum?id=YicbFdNTTy</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman [1937]</span>
<span class="ltx_bibblock">
Friedman M (1937) The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance. Journal of the American Statistical Association 32(200):675‚Äì701. <a class="ltx_ref" href="https:/doi.org/10.2307/2279372" title="">10.2307/2279372</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.jstor.org/stable/2279372" title="">https://www.jstor.org/stable/2279372</a>, publisher: [American Statistical Association, Taylor &amp; Francis, Ltd.]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. [2016]</span>
<span class="ltx_bibblock">
He K, Zhang X, Ren S, et¬†al. (2016) Deep Residual Learning for Image Recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Las Vegas, NV, USA, pp 770‚Äì778, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2016.90" title="">10.1109/CVPR.2016.90</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/7780459/" title="">http://ieeexplore.ieee.org/document/7780459/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kortylewski et¬†al. [2020a]</span>
<span class="ltx_bibblock">
Kortylewski A, He J, Liu Q, et¬†al. (2020a) Compositional Convolutional Neural Networks: A Deep Architecture With Innate Robustness to Partial Occlusion. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Seattle, WA, USA, pp 8937‚Äì8946, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR42600.2020.00896" title="">10.1109/CVPR42600.2020.00896</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9157227/" title="">https://ieeexplore.ieee.org/document/9157227/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kortylewski et¬†al. [2020b]</span>
<span class="ltx_bibblock">
Kortylewski A, Liu Q, Wang H, et¬†al. (2020b) Combining Compositional Models and Deep Networks For Robust Object Classification under Occlusion. In: 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, Snowmass Village, CO, USA, pp 1322‚Äì1330, <a class="ltx_ref" href="https:/doi.org/10.1109/WACV45572.2020.9093560" title="">10.1109/WACV45572.2020.9093560</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9093560/" title="">https://ieeexplore.ieee.org/document/9093560/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et¬†al. [2012]</span>
<span class="ltx_bibblock">
Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet Classification with Deep Convolutional Neural Networks. In: Advances in Neural Information Processing Systems, vol¬†25. Curran Associates, Inc., URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" title="">https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et¬†al. [2015]</span>
<span class="ltx_bibblock">
Lin TY, Maire M, Belongie S, et¬†al. (2015) Microsoft COCO: Common Objects in Context. <a class="ltx_ref" href="https:/doi.org/10.48550/arXiv.1405.0312" title="">10.48550/arXiv.1405.0312</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1405.0312" title="">http://arxiv.org/abs/1405.0312</a>, arXiv:1405.0312 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. [2021]</span>
<span class="ltx_bibblock">
Liu Z, Lin Y, Cao Y, et¬†al. (2021) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Montreal, QC, Canada, pp 9992‚Äì10002, <a class="ltx_ref" href="https:/doi.org/10.1109/ICCV48922.2021.00986" title="">10.1109/ICCV48922.2021.00986</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9710580/" title="">https://ieeexplore.ieee.org/document/9710580/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller [1995]</span>
<span class="ltx_bibblock">
Miller GA (1995) WordNet: a lexical database for English. Communications of the ACM 38(11):39‚Äì41. <a class="ltx_ref" href="https:/doi.org/10.1145/219717.219748" title="">10.1145/219717.219748</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/219717.219748" title="">https://dl.acm.org/doi/10.1145/219717.219748</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naseer et¬†al. [2021]</span>
<span class="ltx_bibblock">
Naseer MM, Ranasinghe K, Khan SH, et¬†al. (2021) Intriguing Properties of Vision Transformers. In: Advances in Neural Information Processing Systems, vol¬†34. Curran Associates, Inc., pp 23296‚Äì23308, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html" title="">https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et¬†al. [2022]</span>
<span class="ltx_bibblock">
Qi J, Gao Y, Hu Y, et¬†al. (2022) Occluded Video Instance Segmentation: A Benchmark. International Journal of Computer Vision 130(8):2022‚Äì2039. <a class="ltx_ref" href="https:/doi.org/10.1007/s11263-022-01629-1" title="">10.1007/s11263-022-01629-1</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11263-022-01629-1" title="">https://doi.org/10.1007/s11263-022-01629-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahin and Itti [2023]</span>
<span class="ltx_bibblock">
Sahin G, Itti L (2023) HOOT: Heavy Occlusions in Object Tracking Benchmark. In: 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, Waikoloa, HI, USA, pp 4819‚Äì4828, <a class="ltx_ref" href="https:/doi.org/10.1109/WACV56688.2023.00481" title="">10.1109/WACV56688.2023.00481</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10030507/" title="">https://ieeexplore.ieee.org/document/10030507/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2015]</span>
<span class="ltx_bibblock">
Simonyan K, Zisserman A (2015) Very Deep Convolutional Networks for Large-Scale Image Recognition. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1409.1556" title="">http://arxiv.org/abs/1409.1556</a>, arXiv:1409.1556 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et¬†al. [2018]</span>
<span class="ltx_bibblock">
Tang H, Schrimpf M, Lotter W, et¬†al. (2018) Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences 115(35):8835‚Äì8840. <a class="ltx_ref" href="https:/doi.org/10.1073/pnas.1719397115" title="">10.1073/pnas.1719397115</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/10.1073/pnas.1719397115" title="">https://www.pnas.org/doi/10.1073/pnas.1719397115</a>, publisher: Proceedings of the National Academy of Sciences

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et¬†al. [2021]</span>
<span class="ltx_bibblock">
Touvron H, Cord M, Douze M, et¬†al. (2021) Training data-efficient image transformers &amp; distillation through attention. In: Proceedings of the 38th International Conference on Machine Learning. PMLR, pp 10347‚Äì10357, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v139/touvron21a.html" title="">https://proceedings.mlr.press/v139/touvron21a.html</a>, iSSN: 2640-3498

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tukey [1977]</span>
<span class="ltx_bibblock">
Tukey JW (1977) Exploratory data analysis. Addison-Wesley series in behavioral sciences, Addison-Wesley Pub. Co., Reading, Mass., URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.gbv.de/dms/bowker/toc/9780201076165.pdf" title="">http://www.gbv.de/dms/bowker/toc/9780201076165.pdf</a>, oCLC: 3058187

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. [2017]</span>
<span class="ltx_bibblock">
Wang J, Xie C, Zhang Z, et¬†al. (2017) Detecting Semantic Parts on Partially Occluded Objects. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.07819" title="">http://arxiv.org/abs/1707.07819</a>, arXiv:1707.07819 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et¬†al. [2019]</span>
<span class="ltx_bibblock">
Xiao M, Kortylewski A, Wu R, et¬†al. (2019) TDAPNet: Prototype Network with Recurrent Top-Down Attention for Robust Object Classification under Partial Occlusion. <a class="ltx_ref" href="https:/doi.org/10.48550/arXiv.1909.03879" title="">10.48550/arXiv.1909.03879</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.03879" title="">http://arxiv.org/abs/1909.03879</a>, arXiv:1909.03879 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et¬†al. [2022]</span>
<span class="ltx_bibblock">
Xiao M, Kortylewski A, Wu R, et¬†al. (2022) TDMPNet: Prototype Network with Recurrent Top-Down Modulation for Robust Object Classification under Partial Occlusion. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=v_KSmk9B5kt" title="">https://openreview.net/forum?id=v_KSmk9B5kt</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et¬†al. [2017]</span>
<span class="ltx_bibblock">
Xie S, Girshick R, Dollar P, et¬†al. (2017) Aggregated Residual Transformations for Deep Neural Networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Honolulu, HI, pp 5987‚Äì5995, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2017.634" title="">10.1109/CVPR.2017.634</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/8100117/" title="">http://ieeexplore.ieee.org/document/8100117/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. [2018]</span>
<span class="ltx_bibblock">
Yang HM, Zhang XY, Yin F, et¬†al. (2018) Robust Classification with Convolutional Prototype Learning. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, Salt Lake City, UT, USA, pp 3474‚Äì3482, <a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2018.00366" title="">10.1109/CVPR.2018.00366</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8578464/" title="">https://ieeexplore.ieee.org/document/8578464/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yun et¬†al. [2019]</span>
<span class="ltx_bibblock">
Yun S, Han D, Chun S, et¬†al. (2019) CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Seoul, Korea (South), pp 6022‚Äì6031, <a class="ltx_ref" href="https:/doi.org/10.1109/ICCV.2019.00612" title="">10.1109/ICCV.2019.00612</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9008296/" title="">https://ieeexplore.ieee.org/document/9008296/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2018]</span>
<span class="ltx_bibblock">
Zhang H, Cisse M, Dauphin YN, et¬†al. (2018) mixup: Beyond Empirical Risk Minimization. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=r1Ddp1-Rb" title="">https://openreview.net/forum?id=r1Ddp1-Rb</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. [2019]</span>
<span class="ltx_bibblock">
Zhu H, Tang P, Park J, et¬†al. (2019) Robustness of Object Recognition under Extreme Occlusion in Humans and Computational Models. <a class="ltx_ref" href="https:/doi.org/10.48550/arXiv.1905.04598" title="">10.48550/arXiv.1905.04598</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.04598" title="">http://arxiv.org/abs/1905.04598</a>, arXiv:1905.04598 [cs]

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 21:16:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
