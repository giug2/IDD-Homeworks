<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.13376] End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things</title><meta property="og:description" content="Federated learning (FL) and split neural networks (SplitNN) are state-of-art distributed machine learning techniques to enable machine learning without directly accessing raw data on clients or end devices. In theory, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.13376">

<!--Generated on Sat Mar  2 12:07:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
split learning,  federated learning,  distributed machine learning,  IoT
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yansong Gao12, Minki Kim23, Sharif Abuadbba12, Yeonjae Kim23, Chandra Thapa2, 
<br class="ltx_break">Kyuyeon Kim23, Seyit A. Camtepe2, Hyoungshick Kim23, and Surya Nepal12
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">1 Cyber Security Cooperative Research Centre</span>, Australia. {garrison.gao; sharif.abuadbba; surya.nepal}@data61.csiro.au
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">2 Data61, CSIRO</span>, Syndey, Australia. {minki.kim;chandra.thapa;seyit.camtepe;hyoung.kim}@data61.csiro.au.
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.3.id1" class="ltx_text ltx_font_italic">3 Sungkyunkwan University</span>, Suwon, Republic of Korea.
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated learning (FL) and split neural networks (SplitNN) are state-of-art distributed machine learning techniques to enable machine learning without directly accessing raw data on clients or end devices. In theory, such distributed machine learning techniques have great potential in distributed applications, in which data are typically generated and collected at the client-side while the collected data should be processed by the application deployed at the server-side. However, there is still a significant gap in evaluating the performance of those techniques concerning their practicality in the Internet of Things (IoT)-enabled distributed systems constituted by resource-constrained devices.</p>
<p id="id5.id2" class="ltx_p">This work is the first attempt to provide empirical comparisons of FL and SplitNN in real-world IoT settings in terms of learning performance and device implementation overhead. We consider a variety of datasets, different model architectures, multiple clients, and various performance metrics. For the learning performance (i.e., model accuracy and convergence time), we empirically evaluate both FL and SplitNN under different types of data distributions such as imbalanced and non-independent and identically distributed (non-IID) data. We show that the learning performance of SplitNN is better than FL under an imbalanced data distribution but worse than FL under an extreme non-IID data distribution. For implementation overhead, we mount both FL and SplitNN on Raspberry Pi devices and comprehensively evaluate their overhead, including training time, communication overhead, power consumption, and memory usage. Our key observations are that under the IoT scenario where the communication traffic is the primary concern, FL appears to perform better over SplitNN because FL has a significantly lower communication overhead compared with SplitNN. However, our experimental results also demonstrate that neither FL or SplitNN can be applied to a heavy model, e.g., with several million parameters, on resource-constrained IoT devices because its training cost would be too expensive for such devices. Source code is released and available: <a target="_blank" href="https://github.com/Minki-Kim95/Federated-Learning-and-Split-Learning-with-raspberry-pi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Minki-Kim95/Federated-Learning-and-Split-Learning-with-raspberry-pi</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
split learning, federated learning, distributed machine learning, IoT

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the rapid proliferation of deep learning has led to stunning transformations in a wide variety of applications, including computer vision, disease diagnosis, financial fraud detection, malware detection, access control, surveillance, and so on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In general, deep learning models learn high-level invariant features by training them on rich data and using those features to solve the problems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, data can often be highly private or sensitive; for example, data collected from medical sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and microphones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> would be such cases. Consequently, users may resist sharing their data with service/cloud providers who are trying to build a deep learning model. On the other hand, the centralized data could be mishandled or incorrectly managed by service providers—e.g., incidentally accessed by unauthorized parties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, or used for unsolicited analytics, or compromised through network and system security vulnerability—resulting in the data breach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Therefore, there is a demand for training a deep learning model without aggregating and accessing sensitive raw data resided in the client-side.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this context, distributed learning techniques are being developed to tackle the above issues by training a joint model without accessing decentralized raw data held by clients in a distributed manner. Such techniques offer great potential for distributed system applications to reap the benefits from rich data generated/collected by IoT devices in distributed Internet of Things (IoT) architectures. Distributed learning techniques keep the data locally and utilize private data (e.g., medical records, voice records, and text inputs) during the learning process to reduce privacy leakage risks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we consider two distributed learning techniques, namely federated learning (FL) and split neural network (SplitNN) (also referred to as split learning). FL is a well-known distributed learning technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In FL, a joint model is built through aggregating (e.g., averaging) models trained on each client’s local data. SplitNN is a recently introduced distributed learning technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In general, a neural network is split into two parts <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">vertically</span>. The first <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">few</span> layers belong to the client (e.g., IoT device), and the remaining layers belong to the server (e.g., cloud). The client and the server collaboratively train the whole network. There have been theoretical and empirical evaluations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> on the FL. To the best of knowledge, however, there is no research analyzing the performance of SplitNN under diverse distributed data conditions. A recent study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> compared both models in terms of communication efficiency only. However, they do not consider learning performance, such as model accuracy and convergence speed, especially when the data are imbalanced or non-IID (non-independent and identically distributed). We note that such imbalanced data situations would frequently happen in practice—e.g., IoT devices generate different types or/and sizes of data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Furthermore, there is no empirical study on the end-to-end evaluation of FL and SplitNN for real-world IoT settings in terms of their implementation overhead, such as communication cost, power consumption, and training time. Indeed, as highlighted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, there is a demand to understand the deep learning performances on resource-constrained IoT/edge device hardware like Raspberry Pi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Experimental results with real-world IoT devices would be useful for service providers considering the deployment of FL or SplitNN. Thereby, this paper aims to take the first step of empirically evaluate FL and SplitNN in real-world IoT applications. Main contributions/results of this work are summarized as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We are the first to evaluate SplitNN learning performance in terms of model accuracy and convergence under non-IID and imbalanced data distributions, and then compare it with FL under the same settings. Our <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">empirical</span> results—up to simulated 100 clients—demonstrate that SplitNN exhibits better learning performance than FL under imbalanced data, but worse than FL under (extreme) non-IID data, indicating that SplitNN accuracy is sensitive to the characteristics of the distributed data.
</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We evaluate the applicability of mounting FL and SplitNN on resource-constrained IoT devices such as Raspberry Pi. Our intensive evaluation results suggest that complicated models (e.g., MobileNet) with several million parameters would be infeasible for such devices. For distributed <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">learning or training</span> with resource-constrained IoT devices, we recommend using a 1D CNN model with fewer parameters to deal with sequential data, which we thus have focused and extensively evaluated. An experimental video demo is available from <a target="_blank" href="https://www.youtube.com/watch?v=x5mD1_EA2ps" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=x5mD1_EA2ps</a>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We take the first step toward fairly <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">training</span> performance comparisons between FL and SplitNN by mounting both on Raspberry Pi. We provide detailed performance overhead evaluations of training time, amount of memory used, amount of power consumed, communication overhead, peak power, and temperature to serve as a reference for practitioners. Under IoT scenarios in which the communication cost reduction is more important than the training time and energy consumption, FL seems to be a better option due to its significantly lower communication overhead compared with SplitNN.
</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We are the first to empirically extend SplitNN for ensemble learning by exploiting the sequential learning process of SplitNN and the high computational power of cloud services to gain multiple models during the learning process of SplitNN while reducing the training expense.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of this paper is organized as follows: Section <a href="#S2" title="II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> details background on distributed learning models followed by our experiments and datasets used. We comprehensively evaluate the learning performance of both FL and SplitNN under both imbalanced and non-IID data distributions in Section <a href="#S3" title="III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Section <a href="#S4" title="IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> mounts both FL and SplitNN on Raspberry Pi to empirically evaluate and compare their implementation overhead. We discuss the insight gained and provide future work in Section <a href="#S5" title="V Discussion and Future Work ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, followed by the conclusion in Section <a href="#S6" title="VI Conclusion ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Distributed Learning and Datasets</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We firstly describe background on FL, SplitNN, and ensemble learning techniques. Then we describe the datasets used in this work.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.8" class="ltx_p">The FL is illustrated in Fig. <a href="#S2.F1.sf1" title="In Figure 1 ‣ II-A Federated Learning ‣ II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>. During the training process, the server first initializes the global model <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="w_{t}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">w</mi><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝑤</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">w_{t}</annotation></semantics></math> and sends it to all participating clients. After receiving the model <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="w_{t}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">w</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝑤</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">w_{t}</annotation></semantics></math>, each client <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">k</annotation></semantics></math> trains the global model on its local data—<math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="s_{k}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝑠</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">s_{k}</annotation></semantics></math> is the number of training samples held by client <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">k</annotation></semantics></math> while <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">s</annotation></semantics></math> is the total number of training samples across all clients. Afterward, each client returns the updated model <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="w_{t}^{k}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><msubsup id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2.2" xref="S2.SS1.p1.7.m7.1.1.2.2.cmml">w</mi><mi id="S2.SS1.p1.7.m7.1.1.2.3" xref="S2.SS1.p1.7.m7.1.1.2.3.cmml">t</mi><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">superscript</csymbol><apply id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2.2">𝑤</ci><ci id="S2.SS1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS1.p1.7.m7.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">w_{t}^{k}</annotation></semantics></math> to the server. The server then aggregates all those models to update the global model to get <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="w_{t+1}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">w</mi><mrow id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.3.2.cmml">t</mi><mo id="S2.SS1.p1.8.m8.1.1.3.1" xref="S2.SS1.p1.8.m8.1.1.3.1.cmml">+</mo><mn id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑤</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3"><plus id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.1"></plus><ci id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">w_{t+1}</annotation></semantics></math>. The above process (often called <em id="S2.SS1.p1.8.1" class="ltx_emph ltx_font_italic">round</em>) repeatedly continues until the model converges.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2003.13376/assets/x1.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">FL with four clients</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2003.13376/assets/x2.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">SplitNN with seven layers</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Illustrated examples of FL and SplitNN.</span></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, instead of training the model with local data only one epoch, each client trains the local model for several epochs before sending it to the server in one communication round, which is referred to as <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> that is one commonly used method for FL optimization. Although <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> usually works well, specifically for non-convex problems, there are no convergence guarantees. FL may diverge in practical settings, especially if data are non-IID and imbalanced distributed across clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Split Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.3" class="ltx_p">Unlike FL, in which each client trains the whole neural network, The SplitNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> divides a neural network model into at least two sub-networks, and then trains the sub-networks, separately, on distributed parties (e.g., client and server). The SplitNN is illustrated in Fig <a href="#S2.F1.sf2" title="In Figure 1 ‣ II-A Federated Learning ‣ II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>, where <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="C_{3}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">C</mi><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">C_{3}</annotation></semantics></math> is the cut layer that divides the whole network into two sub-networks. The first sub-network <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">h</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ℎ</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">h_{t}</annotation></semantics></math> is trained and accessed by the client; the second sub-network <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="w_{t}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">w</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝑤</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">w_{t}</annotation></semantics></math> is trained and accessed by the server. Therefore, the server has no access to clients’ sub-networks and data, which provides privacy protection. Besides privacy benefit, each client only needs to train a sub-network consisting of a few layers while most layers reside in the server. Therefore, as the other benefit, the client’s computation load can be reduced.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The learning performance (e.g., model accuracy and convergence) of SplitNN has not been investigated yet when the data is non-IID or distributed in an imbalanced manner, which will be evaluated in this work.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Ensemble Learning</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Deep learning models are nonlinear methods that learn via a stochastic training algorithm, which will result in models suffering high variance. One can opt for multiple models for the same problem to address this, and their predictions are combined to make the final decision. This approach is called model averaging and belongs to a family of techniques, namely, ensemble learning.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.20" class="ltx_p">For SplitNN, the server interacts with each client a round-robin fashion <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>To the best of our knowledge, there is no work on performing SplitNN among clients in a parallel manner upon the writing of this work.</span></span></span>. On the one hand, at a time, there is only one active client while the rest are waiting to be called. On the other hand, the server is computationally powerful with a cluster of GPUs. In this context, we can utilize the resourceful server to obtain multiple models by taking advantage of idle clients during SplitNN training. To be precise, we exemplify an ensemble learning process with two clients and two model architectures M<sub id="S2.SS3.p2.20.1" class="ltx_sub">1</sub> and M<sub id="S2.SS3.p2.20.2" class="ltx_sub">2</sub>. The server first trains M<sub id="S2.SS3.p2.20.3" class="ltx_sub">1</sub> with client<sub id="S2.SS3.p2.20.4" class="ltx_sub">1</sub> on D<sub id="S2.SS3.p2.20.5" class="ltx_sub">1</sub> (data held by client<sub id="S2.SS3.p2.20.6" class="ltx_sub">1</sub>) and trains M<sub id="S2.SS3.p2.20.7" class="ltx_sub">2</sub> with client<sub id="S2.SS3.p2.20.8" class="ltx_sub">2</sub> on D<sub id="S2.SS3.p2.20.9" class="ltx_sub">2</sub> (data held by client<sub id="S2.SS3.p2.20.10" class="ltx_sub">2</sub>), which are performed simultaneously—both clients train at same time. Afterward, the server continues to train M<sub id="S2.SS3.p2.20.11" class="ltx_sub">1</sub> with client<sub id="S2.SS3.p2.20.12" class="ltx_sub">2</sub> on D<sub id="S2.SS3.p2.20.13" class="ltx_sub">2</sub> and trains M<sub id="S2.SS3.p2.20.14" class="ltx_sub">2</sub> with client<sub id="S2.SS3.p2.20.15" class="ltx_sub">1</sub> on D<sub id="S2.SS3.p2.20.16" class="ltx_sub">1</sub> in parallel. Once D<sub id="S2.SS3.p2.20.17" class="ltx_sub">1</sub> and D<sub id="S2.SS3.p2.20.18" class="ltx_sub">2</sub> are trained for both M<sub id="S2.SS3.p2.20.19" class="ltx_sub">1</sub> and M<sub id="S2.SS3.p2.20.20" class="ltx_sub">2</sub>, one round is completed—equal to one global epoch for all the data across clients. In this manner, the server can train and consequentially gain two models compatible with the SplitNN training process. This assembling training can reduce computational overhead (e.g., time) to gain multiple models compared to train each model sequentially.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Sequential data or time-series data is pervasively collected and processed by IoT devices. For example, people can order and purchase goods using speech commands at their voice assistant. Wearable medical sensors are used to monitor users’ health status in real-time. Consequentially, we choose two such popular datasets: speech command (SC) and ECG for experimental evaluations, as summarized in Table <a href="#S2.T1" title="TABLE I ‣ II-D2 Electrocardiogram (ECG) ‣ II-D Datasets ‣ II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. The SC is a personalized dataset, and the ECG is a medical dataset. Both datasets would be privacy-sensitive, where users are unwilling to share.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS1.5.1.1" class="ltx_text">II-D</span>1 </span>Speech Commands (SC)</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">This task is for speech command recognition. The SC contains many one-second .wav audio files: each sample has a single spoken English word <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These words are from a small set of commands and are spoken by a variety of different speakers. In our experiments, we use 10 classes: ‘zero’, ‘one’, ‘two’, ‘three’, ‘four’, ‘five’, ‘six’, ‘seven’, ‘eight’, and ‘nine.’ There are 20,827 samples where 11,360 samples are used for training, and the remaining samples are used for testing.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS2.5.1.1" class="ltx_text">II-D</span>2 </span>Electrocardiogram (ECG)</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.5" class="ltx_p">MIT-BIH arrhythmia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a popular dataset for ECG signal classification or arrhythmia diagnosis detection models. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, we collect 26,490 samples in total which represent 5 heartbeat types as classification targets: <math id="S2.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS4.SSS2.p1.1.m1.1a"><mi id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.1b"><ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.1c">N</annotation></semantics></math> (normal beat), <math id="S2.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS4.SSS2.p1.2.m2.1a"><mi id="S2.SS4.SSS2.p1.2.m2.1.1" xref="S2.SS4.SSS2.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.2.m2.1b"><ci id="S2.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.2.m2.1c">L</annotation></semantics></math> (left bundle branch block), <math id="S2.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S2.SS4.SSS2.p1.3.m3.1a"><mi id="S2.SS4.SSS2.p1.3.m3.1.1" xref="S2.SS4.SSS2.p1.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.3.m3.1b"><ci id="S2.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.3.m3.1c">R</annotation></semantics></math> (right bundle branch block), <math id="S2.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS4.SSS2.p1.4.m4.1a"><mi id="S2.SS4.SSS2.p1.4.m4.1.1" xref="S2.SS4.SSS2.p1.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.4.m4.1b"><ci id="S2.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.4.m4.1c">A</annotation></semantics></math> (atrial premature contraction), and <math id="S2.SS4.SSS2.p1.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS4.SSS2.p1.5.m5.1a"><mi id="S2.SS4.SSS2.p1.5.m5.1.1" xref="S2.SS4.SSS2.p1.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.5.m5.1b"><ci id="S2.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p1.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.5.m5.1c">V</annotation></semantics></math> (ventricular premature contraction). Half of them are randomly chosen for training, while the rest samples are for testing.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.4.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.5.2" class="ltx_text" style="font-size:90%;">Datasets and Models.</span></figcaption>
<div id="S2.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:53.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-144.6pt,35.5pt) scale(0.428524699388877,0.428524699388877) ;">
<table id="S2.T1.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.2.2" class="ltx_tr">
<th id="S2.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">Dataset</th>
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">
<table id="S2.T1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S2.T1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S2.T1.1.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S2.T1.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.m1.1c">\#</annotation></semantics></math> of</td>
</tr>
<tr id="S2.T1.1.1.1.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">labels</td>
</tr>
</table>
</th>
<th id="S2.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">
<table id="S2.T1.2.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.2.4.1.1" class="ltx_tr">
<td id="S2.T1.2.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Input</td>
</tr>
<tr id="S2.T1.2.2.2.4.1.2" class="ltx_tr">
<td id="S2.T1.2.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">size</td>
</tr>
</table>
</th>
<th id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">
<table id="S2.T1.2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.2.2.1.1" class="ltx_tr">
<td id="S2.T1.2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S2.T1.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S2.T1.2.2.2.2.1.1.1.m1.1a"><mi mathvariant="normal" id="S2.T1.2.2.2.2.1.1.1.m1.1.1" xref="S2.T1.2.2.2.2.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.1.1.1.m1.1b"><ci id="S2.T1.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S2.T1.2.2.2.2.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.1.1.1.m1.1c">\#</annotation></semantics></math> of</td>
</tr>
<tr id="S2.T1.2.2.2.2.1.2" class="ltx_tr">
<td id="S2.T1.2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">samples</td>
</tr>
</table>
</th>
<th id="S2.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">
<table id="S2.T1.2.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.2.5.1.1" class="ltx_tr">
<td id="S2.T1.2.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Model</td>
</tr>
<tr id="S2.T1.2.2.2.5.1.2" class="ltx_tr">
<td id="S2.T1.2.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Architecture</td>
</tr>
</table>
</th>
<th id="S2.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt">
<table id="S2.T1.2.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.2.6.1.1" class="ltx_tr">
<td id="S2.T1.2.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Total</td>
</tr>
<tr id="S2.T1.2.2.2.6.1.2" class="ltx_tr">
<td id="S2.T1.2.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Parameters</td>
</tr>
</table>
</th>
<th id="S2.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt">
<table id="S2.T1.2.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.2.7.1.1" class="ltx_tr">
<td id="S2.T1.2.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Total Model Accuracy</td>
</tr>
<tr id="S2.T1.2.2.2.7.1.2" class="ltx_tr">
<td id="S2.T1.2.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Centralized data)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.3.1" class="ltx_tr">
<td id="S2.T1.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ECG</td>
<td id="S2.T1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">124</td>
<td id="S2.T1.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26,490</td>
<td id="S2.T1.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S2.T1.2.2.3.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.3.1.5.1.1" class="ltx_tr">
<td id="S2.T1.2.2.3.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">4conv + 2dense</td>
</tr>
<tr id="S2.T1.2.2.3.1.5.1.2" class="ltx_tr">
<td id="S2.T1.2.2.3.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">1D CNN</td>
</tr>
</table>
</td>
<td id="S2.T1.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S2.T1.2.2.3.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.3.1.6.1.1" class="ltx_tr">
<td id="S2.T1.2.2.3.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">68,901</td>
</tr>
</table>
</td>
<td id="S2.T1.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">97.78%</td>
</tr>
<tr id="S2.T1.2.2.4.2" class="ltx_tr">
<td id="S2.T1.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<table id="S2.T1.2.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.4.2.1.1.1" class="ltx_tr">
<td id="S2.T1.2.2.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Speech Command</td>
</tr>
<tr id="S2.T1.2.2.4.2.1.1.2" class="ltx_tr">
<td id="S2.T1.2.2.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(SC)</td>
</tr>
</table>
</td>
<td id="S2.T1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">8,000</td>
<td id="S2.T1.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">32,187</td>
<td id="S2.T1.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<table id="S2.T1.2.2.4.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.4.2.5.1.1" class="ltx_tr">
<td id="S2.T1.2.2.4.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">4conv + 2dense</td>
</tr>
<tr id="S2.T1.2.2.4.2.5.1.2" class="ltx_tr">
<td id="S2.T1.2.2.4.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">1D CNN</td>
</tr>
</table>
</td>
<td id="S2.T1.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<table id="S2.T1.2.2.4.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.2.4.2.6.1.1" class="ltx_tr">
<td id="S2.T1.2.2.4.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">522,586</td>
</tr>
</table>
</td>
<td id="S2.T1.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">85.29%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Learning Performance Evaluation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In practice, data is often distributed among clients in an imbalanced manner, e.g., some sensors are more active than others—with more data, and non-IID distributed, e.g., a single person’s data can only be collected <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This situation corresponds to imbalanced data and non-IID data settings. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> studied the performance of FL under these settings. However, there is no work to analyze SplitNN in such a scenario yet.
Therefore, we are interested in conducting experiments based on the following research questions (<span id="S3.p1.1.1" class="ltx_text ltx_font_bold">RQ</span>).

<br class="ltx_break"></p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">RQ1: What factors/settings (e.g., number of clients, non-IID data, and imbalanced data) affect SplitNN learning performance?</span>

<br class="ltx_break"></p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">RQ2: Which setting will the SplitNN learning performance outperform FL?</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">IID and Balanced Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Starting with ideal IID and balanced data distribution, we evaluate FL and SplitNN using both SC and ECG dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For all tests in this section if there is no explicit statement, the 4conv+2dense 1D CNN model architecture is used. The learning rate is set to be 0.001. The batch size = 32.</span></span></span>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A IID and Balanced Dataset ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F3" title="Figure 3 ‣ III-A IID and Balanced Dataset ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> detail the testing accuracy over the number of rounds when FL and SplitNN are trained by a different number of clients—2, 5, 50, and 100 clients. Results indicate that SplitNN can always converge relatively faster than FL with one local epoch—notably for SplitNN, the local epoch is always 1. FL struggles with converging, especially when the number of clients becomes large.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2003.13376/assets/x3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="44" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Testing accuracy of FL (1 and 5 local epochs for 1 round) and SplitNN over rounds for the ECG data, which is IID and distributed in a balanced manner.</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2003.13376/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="44" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Testing accuracy of FL (1 and 5 local epochs for 1 round) and SplitNN over rounds for the SC data, which is IID and distributed in a balanced manner.</span></figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">SplitNN testing accuracy starts dropping after it reaches an optimum point. Thus, an increasing number of rounds will not help improve accuracy. Stopping at the optimal point saves training time. In addition, SplitNN always exhibits an unstable learning curve with a high number of spikes.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Furthermore, the model accuracy of SplitNN cannot reach the baseline accuracy of the centralized model—85.29% for the SC and 97.78% for the ECG, as detailed in Table <a href="#S2.T1" title="TABLE I ‣ II-D2 Electrocardiogram (ECG) ‣ II-D Datasets ‣ II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. This limitation is clearly shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A IID and Balanced Dataset ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, when the number of clients is 50 or 100.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">These results indicate that the SplitNN model accuracy and convergence performance are not always the same as that of training a model through centralized data. Our findings are consistent with the previous conclusion in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, we note that our findings are more generalized because we do not assume that the order of the data that arrived at multiple entities should be preserved, and the same initialization is used for assigning weights.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Remark1:</span> For <span id="S3.SS1.p6.1.2" class="ltx_text ltx_font_bold">RQ1</span>, SplitNN learning performance is affected by the number of clients. For <span id="S3.SS1.p6.1.3" class="ltx_text ltx_font_bold">RQ2</span>, SplitNN always outperforms the FL in terms of convergence speed in our experiments.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Imbalanced Data Distribution</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We assume the data are distributed among clients following the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">normal distribution</span> to simulate the realistic imbalanced data distribution. Larger the sigma/variance, more imbalanced the data distributed. For example, when the number of clients is 10, and the total number of SC training dataset is 11360, the minimum number of training samples held by one client could be as few as 48 while the maximum number of training samples held by a client could be 3855—this is the setting for Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Imbalanced Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (d). We simulated clients up to 100. Given the same number of clients, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">same data distribution</span> is applied to both FL and SplitNN.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2003.13376/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;"> Imbalanced data setting.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">According to Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Imbalanced Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, FL is hard to achieve the baseline accuracy of the centralized model, even when multiple local epochs per round is adopted for a large number of clients. For the SplitNN, its model accuracy deteriorates when the number of clients is large, e.g., Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Imbalanced Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (e) and (f). In addition, FL converges slower, especially when the number of clients goes up, e.g., 50 and 100 cases. Usage of more local epochs per round can expedite the convergence issue—but it cannot completely prevent—<span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">given the similar communication overhead</span>. However, we note more local epochs proportionally prolongs training time on the client-side, although it can reduce the communication overhead. SplitNN is less sensitive to imbalance data distribution since it can always quickly converge. In Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Imbalanced Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (f), we can see that the training of SplitNN does not learn for the first 50 rounds/epochs. Once it starts learning, it indeed finds convergence quickly.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Remark2:</span> For <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_bold">RQ1</span>, the SplitNN learning performance is affected by both the number of clients and imbalanced data distribution. For <span id="S3.SS2.p3.1.3" class="ltx_text ltx_font_bold">RQ2</span>, SplitNN converges faster than FL in our experiments.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Non-IID Data Distribution</span>
</h3>

<figure id="S3.F5" class="ltx_figure"><img src="/html/2003.13376/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Non-IID dataset setting.</span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For the non-IID setting, the SC and ECG datasets are first sorted by class. Each client then receives data partition from only one single class, two classes, three classes, four classes, and five classes, respectively.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">FL.</span> When each client has only one class data, FL convergence significantly fluctuates and is slow, as shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-C Non-IID Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a) and (c). It is clear from the figures that high skewness in the distributed data yields slower convergence and higher model accuracy drop. This finding is consistent with previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Nonetheless, FL can still converge in most cases, except 1 client with 1 class data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">SplitNN.</span> As shown in Fig <a href="#S3.F5" title="Figure 5 ‣ III-C Non-IID Data Distribution ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b) and (d) that, unlike FL, which learns slowly under extreme non-IID distributed data, SplitNN does not learn at all. To be precise, as for the ECG with 5 total number of classes, the SplitNN model does not learn when one client holds 1 or 2 or 3 classes since the testing accuracy is always around 22.65%, which is similar to guess (5 classes in total). As for the SC with 10 total number of classes, the SplitNN model does not learn on the condition when the client holds 1, 2, 3, or 4 classes. Therefore, in contrast to FL, SplitNN fails often to learn in non-IID settings. Note that in this experiments we have batch size = 32, and reducing the batch size (e.g., batch size = 4) may help SplitNN performance in case with one clients with 2 or 3 or 4 classes—but the trade-off is prolonged training time.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Remark3:</span> For <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_bold">RQ1</span>, SplitNN is very sensitive to non-IID data.
In fact, for both FL and SplitNN, some extend of knowledge forgetting while learning is evident when trained on the non-IID settings. Moreover, in our experiments, for <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_bold">RQ2</span>, FL outperforms SplitNN under non-IID data setting, especially extreme cases. The possible reason lies in the approach of model training. FL aggregates (averages) the local models trained on the local data present at each client. The aggregated model usually has more knowledge of the data classes even though there is one class per client than the cases where the model is sequentially learned over the clients without aggregation, such as in SplitNN.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">SplitNN enabled Ensemble Learning</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">Here, we validate the SplitNN compatibility with ensemble learning. We use two different model architectures and simply train across two clients without loss of generality. Model M<sub id="S3.SS4.p1.2.1" class="ltx_sub">1</sub> is with 4 1D CNN layers and two dense layers, while M<sub id="S3.SS4.p1.2.2" class="ltx_sub">2</sub> is with 5 1D CNN layers and two dense layers. For both models, the client runs the first two CNN layers while the server runs the remaining layers.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">Both server and clients are simulated in the same desktop with one GTX1050 GPU and one i7-7700HQ CPU. Only the server uses the GPU, and the clients use CPU since the clients here are used to simulate IoT devices with low computational capability. Under this setting, the ensemble learning—following steps detailed in Section <a href="#S2.SS3" title="II-C Ensemble Learning ‣ II Distributed Learning and Datasets ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>—takes 5178s in total to build two models. We further use SplitNN to train M<sub id="S3.SS4.p2.4.1" class="ltx_sub">1</sub> and M<sub id="S3.SS4.p2.4.2" class="ltx_sub">2</sub> across two clients individually—without ensemble. M<sub id="S3.SS4.p2.4.3" class="ltx_sub">1</sub> takes 3456s and M<sub id="S3.SS4.p2.4.4" class="ltx_sub">2</sub> takes 3625s. Therefore, the ensemble is faster to obtain two models—5178s versus 7081s (3456s + 3625s), which reduces the time overhead by 26.8%. In terms of learning accuracy and convergence performance, there is no apparent difference when using ensemble learning to obtain two models and training them separately (marked as default), as depicted in Fig. <a href="#S3.F6" title="Figure 6 ‣ III-D SplitNN enabled Ensemble Learning ‣ III Learning Performance Evaluation ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. By taking advantage of the parallel computation capability of the server, multiple models can be obtained via SplitNN enabled ensemble learning. Notably, the server can further use distillation to obtain a single model from multiple models to achieve improved prediction accuracy and fewer variance  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2003.13376/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="159" height="53" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.8.3.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text" style="font-size:90%;">Ensemble learning that learns two models across two clients (ECG). (a) Model M<sub id="S3.F6.4.2.1" class="ltx_sub">1</sub>, is with four 1D CNN layers and two dense layers. (b) Model M<sub id="S3.F6.4.2.2" class="ltx_sub">2</sub>, is with five 1D CNN layers and two dense layers.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Implementation Overhead Evaluation on Raspberry Pi</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Using the ECG dataset, we evaluate time, power, communication, and memory overhead when running FL and SplitNN on real IoT devices, Raspberry Pi, to provide a benchmark under real-world IoT settings. In particular, we simulate one typical IoT application scenario, as illustrated in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, which can be a smart home setting.
According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the IoT device can be generally categorized to <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">high-end</em> IoT device and <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">low-end</em> IoT device. The low-end IoT devices are temperature, motion sensors, and RFID cards, which are usually strictly resource-constraint. They may not even support an OS such as Linux to run a machine learning algorithm. High-end IoT devices are simple devices like Raspberry Pi. Hence, in this simulated IoT application scenario, Pi serves as a gateway, which aggregates data from low-end IoT devices, e.g., sensors, and interacts with the server to perform distributed learning tasks.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2003.13376/assets/x8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="159" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">A typical IoT application setting. The IoT gateway (e.g., Raspberry Pi) aggregates data (e.g., from various IoT sensors) and interacts with the server to perform distributed learning.</span></figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We considered the following test settings<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We set <span id="footnote3.1" class="ltx_text ltx_font_italic">one local epoch per round for FL</span> in all experiments. We compare implementation overhead by presetting a fixed number of <span id="footnote3.2" class="ltx_text ltx_font_italic">100 rounds</span> for both FL and SplitNN. In other words, 100 epochs for both of them. We always use the learning rate of 0.001.</span></span></span>:</p>
</div>
<div id="S4.p3" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.4" class="ltx_p">Ensemble learning to train model M<sub id="S4.I1.i1.p1.4.1" class="ltx_sub">1</sub> and M<sub id="S4.I1.i1.p1.4.2" class="ltx_sub">2</sub> concurrently across 2 clients, as well as training M<sub id="S4.I1.i1.p1.4.3" class="ltx_sub">1</sub> and M<sub id="S4.I1.i1.p1.4.4" class="ltx_sub">1</sub> individually across two clients via SplitNN (Section <a href="#S4.SS4" title="IV-D Ensemble Learning ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>);</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Evaluating FL and SplitNN across a range of clients from two to five with the same model architecture (Section <a href="#S4.SS5" title="IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>);</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Evaluating FL and SplitNN across five clients with different model architectures. For the SplitNN, two split layers run on clients regardless of model architectures (Section <a href="#S4.SS6" title="IV-F Effects of Number of Split Layers in SplitNN ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>);</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Evaluating SplitNN when a different number of layers is split and running on the client, given the same model architecture. Specifically, one, two, three layers are split and running on the client (Section <a href="#S4.SS7" title="IV-G Effects of Different Models ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-G</span></span></a>).</p>
</div>
</li>
</ol>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In the experiments, we use one Raspberry Pi device to act as the IoT gateway.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We use the Raspberry Pi 3 model BV1.2 (Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) with the following settings: PyTorch version 1.0.0, OS Raspbian GNU/Linux 10 (buster), and Python version 3.7.3. We note that CUDA is not available for the model. The server (laptop) has the following settings: CPU i7-7700HQ, GPU GTX 1050, Pytorch version 1.0.0, OS windows 10, Python version 3.6.8 using Anaconda, and the CUDA version 10.1.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2003.13376/assets/x9.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Four Raspberry Pi devices and a power meter are shown.
See the demo video for more details, <a href="%The%20top-right%20one%20shows%20how%20a%20small%20fan%20is%20attached%20for%20cooling.%20Plug-in%20powermeter%20measures%20the%20Raspberry%20Pi%20power%20consumption.https://www.youtube.com/watch?v=x5mD1_EA2ps" title="" class="ltx_ref ltx_url ltx_font_typewriter">%The␣top-right␣one␣shows␣how␣a␣small␣fan␣is␣attached␣for␣cooling.␣Plug-in␣powermeter␣measures␣the␣Raspberry␣Pi␣power␣consumption.https://www.youtube.com/watch?v=x5mD1_EA2ps</a>.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Measurement Methods of Performance Metrics</span>
</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training Time</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.3" class="ltx_p">We use Python’s <span id="S4.SS2.SSS0.Px1.p1.3.1" class="ltx_text ltx_font_sansserif">time</span> library to measure the training time containing the communication time between client and server. We set the time as <math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="T_{\rm start}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">T</mi><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">start</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3">start</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">T_{\rm start}</annotation></semantics></math> when the model starts training. Once the training is finished, we set the time as <math id="S4.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="T_{\rm end}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><msub id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">T</mi><mi id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">end</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3">end</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">T_{\rm end}</annotation></semantics></math>. As a result, training time is <math id="S4.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="T_{\rm end}-T_{\rm start}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><msub id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">T</mi><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.3.cmml">end</mi></msub><mo id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">−</mo><msub id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">T</mi><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.3.cmml">start</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1"><minus id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1"></minus><apply id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.3">end</ci></apply><apply id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.3">start</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.1c">T_{\rm end}-T_{\rm start}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Memory Usage</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">We use Linux <span id="S4.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">free -h</span> command for measuring the memory usage. This command provides the memory information of <span id="S4.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">total</span>, <span id="S4.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_typewriter">used</span>, <span id="S4.SS2.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_typewriter">free</span>, <span id="S4.SS2.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_typewriter">cached</span> and <span id="S4.SS2.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_typewriter">available</span>.
The total memory of the Raspberry Pi device used in this experiment is 926 MB. The focus here is to record and report the <span id="S4.SS2.SSS0.Px2.p1.1.7" class="ltx_text ltx_font_typewriter">used</span> memory during training.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Power Consumption</h5>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We use a plug-in powermeter, as shown in Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, to measure the power consumption. We measured the power consumption in the kilowatt-hour (kWh) unit.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temperature</h5>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">We use Python’s <span id="S4.SS2.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_sansserif">CPUTemperature</span>() function from the <span id="S4.SS2.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_typewriter">CPUTemperature</span> library to monitor the temperature of the Raspberry Pi CPU.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Communication Overhead</h5>

<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.1" class="ltx_p">We measure the transmitted data size from each client to the server and vice versa. We use the <span id="S4.SS2.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_typewriter">pickle</span> library to monitor the size of the transmitted data. We use the router DGN2200 v4 (N300 Wireless ADSL2+ Modem Router) for wireless communication between Raspberry Pi and the server.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Implementation Considerations</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Low Performance</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Although Raspberry Pi is regarded as a high-end IoT device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, its computational resources are still quite limited compared with traditional computing devices such as servers and PCs. We first tried to run MobileNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. When we trained CIFAR10 dataset with MobileNetv1<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Source code is adopted from <a target="_blank" href="https://github.com/Tshzzz/cifar10.classifer/blob/master/models/mobilenet.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Tshzzz/cifar10.classifer/blob/master/models/mobilenet.py</a>.</span></span></span> (20 conv2D layers with 3,228,170 model parameters in total), it took 8 hours 41 minutes for FL per round with 1 local epoch. For SplitNN, it took about 2.5 hours for one epoch across five Raspberry Pi devices<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Since the training sample for CIFAR10 is 50,000, we use 5 clients. Therefore, each client holds 10,000 images for both FL and SplitNN.</span></span></span>, when only the first two layers are running on the Raspberry Pi device.
In addition, we tried to run ResNet20<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Source code is adopted from <a target="_blank" href="https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py</a>.</span></span></span> (with 20 conv2D layers and 269,722 parameters). When we ran SplitNN across 5 Raspberry Pi devices, it took about 1 hour to finish one round when the first two layers are only running on the Pi devices. When we also ran FL across 5 Raspberry Pi devices, it took 37 minutes for one round with one local epoch.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Based on those results, it seems challenging to run either ResNet20 or MobleNet V1 on Raspberry Pi devices. Because those models have several million parameters, they might be computationally heavy for simple IoT devices even though these 2D CNN models are known as light models for the GPU platform<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>This does not mean <span id="footnote7.1" class="ltx_text ltx_font_italic">inference task</span> cannot be properly performed by Rasberry Pi given the model already trained and optimized.</span></span></span>. Therefore, for full end-to-end tests, we opt for evaluating a relatively simple model, such as 1D CNN, on sequential time series data. This evaluation would be valuable for IoT settings because sequential time series data are pervasive data sources generated and collected by various IoT devices, e.g., sensors.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Install Pytorch on Raspberry Pi</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">We have made a unified manual guide of installing Pytorch v1.0.0 on Raspberry Pi<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This manual is available via <a target="_blank" href="https://github.com/Minki-Kim95/RaspberryPi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Minki-Kim95/RaspberryPi</a>.</span></span></span>. We believe that this manual will help developers because we explain how to address the errors during installation, which are hard to resolve, and there are no solutions online.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.5.1.1" class="ltx_text">IV-C</span>3 </span>Temperature of Raspberry Pi</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.3" class="ltx_p">When training on the Raspberry Pi device, especially FL that runs the entire model on the device, the temperature goes high—usually more than 80<math id="S4.SS3.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\celsius" display="inline"><semantics id="S4.SS3.SSS3.p1.1.m1.1a"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.1.m1.1.1" xref="S4.SS3.SSS3.p1.1.m1.1.1b.cmml"><mtext id="S4.SS3.SSS3.p1.1.m1.1.1a" xref="S4.SS3.SSS3.p1.1.m1.1.1b.cmml">\celsius</mtext></merror><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p1.1.m1.1b"><ci id="S4.SS3.SSS3.p1.1.m1.1.1b.cmml" xref="S4.SS3.SSS3.p1.1.m1.1.1"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p1.1.m1.1.1"><mtext id="S4.SS3.SSS3.p1.1.m1.1.1a.cmml" xref="S4.SS3.SSS3.p1.1.m1.1.1">\celsius</mtext></merror></ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p1.1.m1.1c">\celsius</annotation></semantics></math>. Therefore, it is necessary to cool down the device. A cooling fan can be attached to the Raspberry Pi (Fig. <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). This practice can efficiently cool it down from 83<math id="S4.SS3.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\celsius" display="inline"><semantics id="S4.SS3.SSS3.p1.2.m2.1a"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.2.m2.1.1" xref="S4.SS3.SSS3.p1.2.m2.1.1b.cmml"><mtext id="S4.SS3.SSS3.p1.2.m2.1.1a" xref="S4.SS3.SSS3.p1.2.m2.1.1b.cmml">\celsius</mtext></merror><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p1.2.m2.1b"><ci id="S4.SS3.SSS3.p1.2.m2.1.1b.cmml" xref="S4.SS3.SSS3.p1.2.m2.1.1"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS3.p1.2.m2.1.1"><mtext id="S4.SS3.SSS3.p1.2.m2.1.1a.cmml" xref="S4.SS3.SSS3.p1.2.m2.1.1">\celsius</mtext></merror></ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p1.2.m2.1c">\celsius</annotation></semantics></math> to 54<math id="S4.SS3.SSS3.p1.3.m3.1" class="ltx_Math" alttext="\celsius" display="inline"><semantics id="S4.SS3.SSS3.p1.3.m3.1a"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.3.m3.1.1" xref="S4.SS3.SSS3.p1.3.m3.1.1b.cmml"><mtext id="S4.SS3.SSS3.p1.3.m3.1.1a" xref="S4.SS3.SSS3.p1.3.m3.1.1b.cmml">\celsius</mtext></merror><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p1.3.m3.1b"><ci id="S4.SS3.SSS3.p1.3.m3.1.1b.cmml" xref="S4.SS3.SSS3.p1.3.m3.1.1"><merror class="ltx_ERROR undefined undefined" id="S4.SS3.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS3.p1.3.m3.1.1"><mtext id="S4.SS3.SSS3.p1.3.m3.1.1a.cmml" xref="S4.SS3.SSS3.p1.3.m3.1.1">\celsius</mtext></merror></ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p1.3.m3.1c">\celsius</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Ensemble Learning</span>
</h3>

<figure id="S4.F9" class="ltx_figure"><img src="/html/2003.13376/assets/x10.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Ensemble learning vs learning model individually (100 rounds are commonly used for both approaches.).</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.20" class="ltx_p">If we train M<sub id="S4.SS4.p1.20.1" class="ltx_sub">1</sub> with <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="T_{1}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">T</mi><mn id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">T_{1}</annotation></semantics></math> and <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><msub id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">T</mi><mn id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">T_{2}</annotation></semantics></math>, respectively, the SplitNN enabled ensemble training should roughly take <span id="S4.SS4.p1.20.2" class="ltx_text ltx_font_sansserif">max</span>(<math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="T_{1}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msub id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">T</mi><mn id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">T_{1}</annotation></semantics></math>, <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><msub id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mi id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">T</mi><mn id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.p1.5.m5.1.1.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.5.m5.1.1.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">T_{2}</annotation></semantics></math>) on the condition that the server has unlimited computational power because the server can train the remaining layers of M<sub id="S4.SS4.p1.20.3" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.4" class="ltx_sub">1</sub> in parallel. As we treat the laptop equipped with only a low-end GPU as a server, this ideal case is not met. Because the laptop is unable to ideally parallel train the remaining layers of M<sub id="S4.SS4.p1.20.5" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.6" class="ltx_sub">2</sub>. But, as shown in Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-D Ensemble Learning ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (a), the time for ensemble training, <math id="S4.SS4.p1.10.m10.1" class="ltx_Math" alttext="T_{\rm ensem}" display="inline"><semantics id="S4.SS4.p1.10.m10.1a"><msub id="S4.SS4.p1.10.m10.1.1" xref="S4.SS4.p1.10.m10.1.1.cmml"><mi id="S4.SS4.p1.10.m10.1.1.2" xref="S4.SS4.p1.10.m10.1.1.2.cmml">T</mi><mi id="S4.SS4.p1.10.m10.1.1.3" xref="S4.SS4.p1.10.m10.1.1.3.cmml">ensem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.10.m10.1b"><apply id="S4.SS4.p1.10.m10.1.1.cmml" xref="S4.SS4.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.10.m10.1.1.1.cmml" xref="S4.SS4.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS4.p1.10.m10.1.1.2.cmml" xref="S4.SS4.p1.10.m10.1.1.2">𝑇</ci><ci id="S4.SS4.p1.10.m10.1.1.3.cmml" xref="S4.SS4.p1.10.m10.1.1.3">ensem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.10.m10.1c">T_{\rm ensem}</annotation></semantics></math> is indeed always smaller than sequentially train each model, <math id="S4.SS4.p1.11.m11.1" class="ltx_Math" alttext="T_{1}+T_{2}" display="inline"><semantics id="S4.SS4.p1.11.m11.1a"><mrow id="S4.SS4.p1.11.m11.1.1" xref="S4.SS4.p1.11.m11.1.1.cmml"><msub id="S4.SS4.p1.11.m11.1.1.2" xref="S4.SS4.p1.11.m11.1.1.2.cmml"><mi id="S4.SS4.p1.11.m11.1.1.2.2" xref="S4.SS4.p1.11.m11.1.1.2.2.cmml">T</mi><mn id="S4.SS4.p1.11.m11.1.1.2.3" xref="S4.SS4.p1.11.m11.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS4.p1.11.m11.1.1.1" xref="S4.SS4.p1.11.m11.1.1.1.cmml">+</mo><msub id="S4.SS4.p1.11.m11.1.1.3" xref="S4.SS4.p1.11.m11.1.1.3.cmml"><mi id="S4.SS4.p1.11.m11.1.1.3.2" xref="S4.SS4.p1.11.m11.1.1.3.2.cmml">T</mi><mn id="S4.SS4.p1.11.m11.1.1.3.3" xref="S4.SS4.p1.11.m11.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.11.m11.1b"><apply id="S4.SS4.p1.11.m11.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1"><plus id="S4.SS4.p1.11.m11.1.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1.1"></plus><apply id="S4.SS4.p1.11.m11.1.1.2.cmml" xref="S4.SS4.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p1.11.m11.1.1.2.1.cmml" xref="S4.SS4.p1.11.m11.1.1.2">subscript</csymbol><ci id="S4.SS4.p1.11.m11.1.1.2.2.cmml" xref="S4.SS4.p1.11.m11.1.1.2.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.11.m11.1.1.2.3.cmml" xref="S4.SS4.p1.11.m11.1.1.2.3">1</cn></apply><apply id="S4.SS4.p1.11.m11.1.1.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.p1.11.m11.1.1.3.1.cmml" xref="S4.SS4.p1.11.m11.1.1.3">subscript</csymbol><ci id="S4.SS4.p1.11.m11.1.1.3.2.cmml" xref="S4.SS4.p1.11.m11.1.1.3.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.11.m11.1.1.3.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.11.m11.1c">T_{1}+T_{2}</annotation></semantics></math>. Specifically, <math id="S4.SS4.p1.12.m12.1" class="ltx_Math" alttext="T_{\rm ensem}" display="inline"><semantics id="S4.SS4.p1.12.m12.1a"><msub id="S4.SS4.p1.12.m12.1.1" xref="S4.SS4.p1.12.m12.1.1.cmml"><mi id="S4.SS4.p1.12.m12.1.1.2" xref="S4.SS4.p1.12.m12.1.1.2.cmml">T</mi><mi id="S4.SS4.p1.12.m12.1.1.3" xref="S4.SS4.p1.12.m12.1.1.3.cmml">ensem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.12.m12.1b"><apply id="S4.SS4.p1.12.m12.1.1.cmml" xref="S4.SS4.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.12.m12.1.1.1.cmml" xref="S4.SS4.p1.12.m12.1.1">subscript</csymbol><ci id="S4.SS4.p1.12.m12.1.1.2.cmml" xref="S4.SS4.p1.12.m12.1.1.2">𝑇</ci><ci id="S4.SS4.p1.12.m12.1.1.3.cmml" xref="S4.SS4.p1.12.m12.1.1.3">ensem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.12.m12.1c">T_{\rm ensem}</annotation></semantics></math> takes 11704 seconds to concurrently obtain both M<sub id="S4.SS4.p1.20.7" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.8" class="ltx_sub">2</sub>. Training M<sub id="S4.SS4.p1.20.9" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.10" class="ltx_sub">2</sub> individually costs 8267 seconds and 6908 seconds, respectively. Therefore, ensemble learning reduces the time overhead by 22.87%. For the communication overhead, ensemble learning is the same as individually training each model (Fig. <a href="#S4.F9" title="Figure 9 ‣ IV-D Ensemble Learning ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (b)). For the memory usage, we rely on the <span id="S4.SS4.p1.20.11" class="ltx_text ltx_font_typewriter">used</span> memory for comparison<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>This includes 119MB memory used by the OS by default as it tends to be fair to include the memory occupied by the OS.</span></span></span>. Used memories of individually training M<sub id="S4.SS4.p1.20.12" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.13" class="ltx_sub">2</sub> are 185MB and 186MB. In contrast, the used memory of ensemble training is 222MB.
For power consumption, sequentially training M<sub id="S4.SS4.p1.20.14" class="ltx_sub">1</sub> and M<sub id="S4.SS4.p1.20.15" class="ltx_sub">2</sub> consumes 4Wh (watt per hour) and 3Wh, while ensemble training consumes 6Wh. Therefore, ensemble learning can reduce power consumption, as well.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">In summary, ensemble learning does not reduce communication overhead. However, it has the potential to reduce training time, used memory, and power consumption.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Effects of Number of Clients</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Here, we evaluate both FL and SplitNN when the number of Raspberry Pi devices (clients) varies from two to five.
This experiment focuses on the overhead brought to individual Raspberry Pi rather than the server side. One can easily extend the number of clients beyond five by adopting the released artifact (source code, user guide and demo) of our experiment.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">The model architecture has four 1D CNN layers and two dense layers. For SplitNN, the first two 1D CNN layers run on Raspberry Pi devices.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2003.13376/assets/x11.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="315" height="84" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.3.2" class="ltx_text" style="font-size:90%;">FL and SplitNN evaluation when the number of Raspberry Pi devices (clients) varies from two to five. All tests were performed with the 10 Gbit/s dedicated LAN.</span></figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">We report the performance overhead for a single Raspberry Pi device because we are interested in the client’s overhead. The performance results are presented in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
As for the time overhead in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (a), FL reduces as the number of devices increases. This is due to the decrease in the local data size. SplitNN slightly increases since each device runs the training sequentially. Overall, SplitNN usually takes several times longer than the FL, given the same number of rounds.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">The communication overhead is presented in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b)<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We note that the communication overhead of FL is not displayed because it is orders of magnitudes smaller than that of SplitNN, e.g., megabytes vs. gigabytes.</span></span></span>. FL stays relatively constantly around 28,552,161 bytes because the model parameters determine the FL’s communication overhead rather than the local data size. SplitNN communication overhead decreases as it is highly related to the local data size. This corroborates with the statistical analysis result in a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, where the communication overhead of SplitNN is shown significantly higher than that of FL per round for low model complexity and fewer clients.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">For the used memory, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (c), the FL is always higher than that of SplitNN, because the FL needs to train the entire model in the Raspberry Pi, while the SplitNN only needs to train a small part—few split layers. This also leads to the high power peak, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (e), and high temperature during training, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (f), in the FL case. Without cooling, the Raspberry Pi device’s temperature can be up to <math id="S4.SS5.p5.1.m1.1" class="ltx_Math" alttext="83\celsius" display="inline"><semantics id="S4.SS5.p5.1.m1.1a"><mrow id="S4.SS5.p5.1.m1.1.1" xref="S4.SS5.p5.1.m1.1.1.cmml"><mn id="S4.SS5.p5.1.m1.1.1.2" xref="S4.SS5.p5.1.m1.1.1.2.cmml">83</mn><mo lspace="0em" rspace="0em" id="S4.SS5.p5.1.m1.1.1.1" xref="S4.SS5.p5.1.m1.1.1.1.cmml">​</mo><merror class="ltx_ERROR undefined undefined" id="S4.SS5.p5.1.m1.1.1.3" xref="S4.SS5.p5.1.m1.1.1.3b.cmml"><mtext id="S4.SS5.p5.1.m1.1.1.3a" xref="S4.SS5.p5.1.m1.1.1.3b.cmml">\celsius</mtext></merror></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p5.1.m1.1b"><apply id="S4.SS5.p5.1.m1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1"><times id="S4.SS5.p5.1.m1.1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1.1"></times><cn type="integer" id="S4.SS5.p5.1.m1.1.1.2.cmml" xref="S4.SS5.p5.1.m1.1.1.2">83</cn><ci id="S4.SS5.p5.1.m1.1.1.3b.cmml" xref="S4.SS5.p5.1.m1.1.1.3"><merror class="ltx_ERROR undefined undefined" id="S4.SS5.p5.1.m1.1.1.3.cmml" xref="S4.SS5.p5.1.m1.1.1.3"><mtext id="S4.SS5.p5.1.m1.1.1.3a.cmml" xref="S4.SS5.p5.1.m1.1.1.3">\celsius</mtext></merror></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p5.1.m1.1c">83\celsius</annotation></semantics></math> during the FL learning.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p id="S4.SS5.p6.1" class="ltx_p">However, although FL has a high power peak, the energy is lesser than that of SplitNN for the same number of rounds, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (d). This is because, in FL, each client trains local model <span id="S4.SS5.p6.1.1" class="ltx_text ltx_font_italic">in parallel</span>, and consequently, the total time (accumulated computation and communication) for running a given number of rounds is less than that of SplitNN.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2003.13376/assets/x12.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.4.2" class="ltx_text" style="font-size:90%;">SplitNN performance when a different number of split layers—1 to 3 convolutional layers—run on Raspberry Pi devices. All tests run <span id="S4.F11.4.2.1" class="ltx_text ltx_font_italic">across 5 Raspberry Pi devices</span> and in a lab environment equipped with the 100 Gbit/s dedicated LAN. The model is with 4 convolutional layers and 2 dense layers.</span></figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Effects of Number of Split Layers in SplitNN</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Experiments are carried on five Raspberry Pi devices to observe the effect of the number of split layers for SplitNN. We note that this experiment is only applicable to SplitNN because FL has to train the entire model on each client.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">As shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (b), the communication overhead remains the same regardless of the number of layers at the client-side because the communication overhead in SplitNN depends on the number of parameters in the cut layer rather than the number of split layers. As for the memory usage in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (c), we observe only a slight increase with the number of split layers. Most noticeably, time overhead (depicted in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (a)) and energy overhead (depicted in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (d)) increase with the number of split layers because the number of parameters to be trained on each client increases. Therefore, in practice, from the overhead reduction perspective, it is preferred to run a few layers only at the client-side for SplitNN.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS7.5.1.1" class="ltx_text">IV-G</span> </span><span id="S4.SS7.6.2" class="ltx_text ltx_font_italic">Effects of Different Models</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">To observe the effects of different models for SplitNN and FL, we perform experiments on five Raspberry Pi devices. The models have a varying number of convolutional layers ranging from four to eight. For SplitNN, we use a fixed number of split layers running on each client regardless of the number of layers in the entire model.</p>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2003.13376/assets/x13.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S4.F12.3.2" class="ltx_text" style="font-size:90%;">Overhead performance of FL and SplitNN when the number of layers of the model varies from 4 conv to 8 conv. All tests were performed with the 100 Gbit/s dedicated LAN. For SplitNN, the first two convolutional layers run at the client-side. For communication overhead, we only show the results for FL because SplitNN’s communication overhead (1054Mbytes) is not changed as well as significantly greater than FL’s communication overhead, as shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV-E Effects of Number of Clients ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</span></figcaption>
</figure>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">According to results depicted in Fig. <a href="#S4.F12" title="Figure 12 ‣ IV-G Effects of Different Models ‣ IV Implementation Overhead Evaluation on Raspberry Pi ‣ End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, the overhead, including time, communication, memory used, and energy consumed by Raspberry Pi devices (linearly) increases with the model complexity (defined by the number of layers in the model) for FL. In contrast, the overhead remains more or less constant for SplitNN because the number of layers running on each client is fixed. Based on these findings, SplitNN becomes more advantageous when we consider a complicated model.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Future Work</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Summary of the Evaluation Results</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Firstly, we evaluated the learning performance of both FL and SplitNN in terms of model accuracy and convergence speed. For <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">RQ1</span> and <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_bold">RQ2</span>, our experimental results provide following insights:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">SplitNN learning performance is indeed sensitive to various settings, including i) the number of clients, ii) non-IID, and imbalanced data distribution.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">In comparison with FL, SplitNN always converges much faster under imbalanced data distribution. However, SplitNN is more sensitive to non-IID data, where it even does not learn at all under extreme non-IID cases, e.g., one client with 2 classes or one client with 3 classes.
</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Next, we fully implemented both FL and SplitNN on Raspberry Pi devices using 1D CNN models with a small number of parameters to evaluate the performance of both models in the case of IoT devices with low computational capability. Our experimental setup simulates a practical scenario for edge distributed learning. Our experimental results on those devices suggest the following findings:</p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">If we consider the communication cost as the most critical metric in IoT applications, FL is preferred over SplitNN because FL has relatively lower communication overhead. Interestingly, we found that reducing communication overhead can also benefit training time and energy consumption.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">For applications where the communication is not a significant concern (e.g., Ethernet or 5G are available), SplitNN is recommended to ensure better model accuracy and guarantee (fast) convergence except the case of (extreme) non-IID data distribution.
</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Optimization of the Implementation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This work follows a typical setting of FL and SplitNN, and optimization is out of scope, especially when mounting on the IoT devices. We empirically found that <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">training</span> 2D CNN models, such as ResNet and MobileNet, on Raspberry Pi devices is computationally infeasible. For building a more memory and computation efficient model, one possible optimization is to use XNOR-NET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. We can also consider training convolutional neural networks using addition operations only without multiplication operations based on the previous study results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> since multiplication operations are significantly computationally heavier than addition operations in CNN models. Besides, to reduce the training overhead due to sequential training in SplitNN, a parallel machine learning model update paradigm of FL is applicable in its client-side section <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Splitting Sequential Models</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To process sequential time-series data for experiments, we used a 1D CNN model because 1D CNN models can be split vertically, which is easy to apply SplitNN. Actually, before choosing the 1D CNN to deal with sequential data, we tried to apply SplitNN on other sequential models such as LSTM and RNN that are popularly used state-of-the-art machine learning models to deal with sequential data. However, we found that such sequential models are hard to be applied to SplitNN<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>FL applies to sequential models.</span></span></span>. Therefore, the applicability of SplitNN for sequential models leaves future work.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work is the first to empirically evaluate SplitNN and compare it with FL in real-world IoT settings. We comprehensively evaluated the learning performance in terms of model accuracy and convergence speed of FL and SplitNN. For our experiments, we mainly considered imbalanced and non-IID distributions, which would be more suitable for IoT scenarios. Similar to FL, SplitNN is also inevitably influenced by data distribution. In general, SplitNN performs better than FL in the case of imbalanced data distributions but can rather worsen than FL in the case of extreme non-IID data distributions.
Beyond empirical learning performance evaluation and comparison, we extensively evaluated the practicality of mounting FL and SplitNN on Raspberry Pi devices to simulate real-world IoT scenarios. We mainly dealt with pervasive sequential time-series data and provided useful comprehensive results—various implementation overhead—to the community. Overall, for the IoT scenario, the FL would be a more practical recommendation because it requires less overall communication, time, and power consumption overhead when a simple 1D CNN model is used. However, we also found that for both FL and SplitNN, the use of more complicated models would still be infeasible to mount <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">training</span> on low-capacity IoT devices such as Raspberry Pi.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The work has been supported by the Cyber Security Research Centre Limited whose activities are partially funded by the Australian Government’s Cooperative Research Centres Programme. This work was also supported in part by the ITRC support program (IITP-2019-2015-0-00403). The authors would like to thank all the anonymous reviewers for their valuable feedback.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Nature</span>, 521(7553):436, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G Ororbia II, Xinyu Xing,
Xue Liu, and C Lee Giles.

</span>
<span class="ltx_bibblock">Adversary resistant deep neural networks with an application to
malware detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</span>, pages 1145–1153. ACM, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tuan A Tang, Lotfi Mhamdi, Des McLernon, Syed Ali Raza Zaidi, and Mounir
Ghogho.

</span>
<span class="ltx_bibblock">Deep learning approach for network intrusion detection in software
defined networking.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">International Conference on Wireless Networks and Mobile
Communications (WINCOM)</span>, pages 258–263. IEEE, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Li Huang, Andrew L Shea, Huining Qian, Aditya Masurkar, Hao Deng, and Dianbo
Liu.

</span>
<span class="ltx_bibblock">Patient clustering improves efficiency of federated machine learning
to predict mortality and hospital stay time using distributed electronic
medical records.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Journal of Biomedical Informatics</span>, 99:103291, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
InvenSense Announces World’s Smallest, Lowest Power I2S Microphone for IoT and
Wearable Markets.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Amazon Alexa User Receives 1,700 Audio Recordings of a Stranger through ‘Human
Error’.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Supreeth Shastri, Melissa Wasserman, and Vijay Chidambaram.

</span>
<span class="ltx_bibblock">The seven sins of personal-data processing systems under GDPR.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">USENIX HotCloud</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Major breach found in biometrics system used by banks, UK police and defence
firms.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Brendan McMahan.

</span>
<span class="ltx_bibblock">A survey of algorithms and analysis for adaptive online learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 18:90:1–90:50, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Konecný, Stefano
Mazzocchi, H. Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel
Ramage, and Jason Roselander.

</span>
<span class="ltx_bibblock">Towards Federated Learning at Scale: System Design.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1902.01046, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Otkrist Gupta and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Distributed learning of deep neural network over multiple agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Journal of Network and Computer Applications</span>, 116:1–8, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, and
Abhimanyu Dubey.

</span>
<span class="ltx_bibblock">No peek: A survey of private distributed deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1812.03288, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.00582</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang.

</span>
<span class="ltx_bibblock">Federated continual learning with adaptive parameter communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.03196</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Detailed comparison of communication efficiency of split learning and
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">https://arxiv.org/abs/1909.09145</span>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiasi Chen and Xukan Ran.

</span>
<span class="ltx_bibblock">Deep Learning With Edge Computing: A Review.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 107(8):1655–1674, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Teach, Learn, and Make with Raspberry Pi Raspberry Pi.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.05629</span>, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.07873</span>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Speech Commands.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
George B Moody and Roger G Mark.

</span>
<span class="ltx_bibblock">The impact of the mit-bih arrhythmia database.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Engineering in Medicine and Biology Magazine</span>,
20(3):45–50, 2001.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Serkan Kiranyaz, Turker Ince, and Moncef Gabbouj.

</span>
<span class="ltx_bibblock">Real-time patient-specific ecg classification by 1-d convolutional
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Biomedical Engineering</span>, 63(3):664–675,
2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Dan Li, Jianxin Zhang, Qiang Zhang, and Xiaopeng Wei.

</span>
<span class="ltx_bibblock">Classification of ECG signals based on 1D convolution neural
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2017 IEEE 19th International Conference on e-Health
Networking, Apps and Services</span>, pages 1–6. IEEE, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the Knowledge in a Neural Network.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni,
N Asokan, and Ahmad-Reza Sadeghi.

</span>
<span class="ltx_bibblock">Dïot: A federated self-learning anomaly detection system for iot.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">2019 IEEE 39th International Conference on Distributed
Computing Systems (ICDCS)</span>, pages 756–767. IEEE, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Arslan Musaddiq, Yousaf Bin Zikria, Oliver Hahm, Heejung Yu, Ali Kashif Bashir,
and Sung Won Kim.

</span>
<span class="ltx_bibblock">A survey on resource management in IoT operating systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 6:8459–8482, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Xnor-net: Imagenet classification using binary convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 525–542.
Springer, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Mark D McDonnell.

</span>
<span class="ltx_bibblock">Training wide residual networks for deployment using a single bit for
each weight.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.08530</span>, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang
Xu.

</span>
<span class="ltx_bibblock">AdderNet: Do We Really Need Multiplications in Deep Learning?

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Chandra Thapa, M. A. P. Chamikara, and Seyit Camtepe.

</span>
<span class="ltx_bibblock">Splitfed: When federated learning meets split learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.12088</span>, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.13375" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.13376" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.13376">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.13376" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.13377" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 12:07:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
