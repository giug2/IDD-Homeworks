<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.16443] Synthetic data enable experiments in atomistic machine learning</title><meta property="og:description" content="Machine-learning models are increasingly used to predict properties of atoms in chemical systems.
There have been major advances in developing descriptors and regression frameworks for this task, typically starting froâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic data enable experiments in atomistic machine learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic data enable experiments in atomistic machine learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.16443">

<!--Generated on Thu Mar 14 18:59:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic data enable experiments in atomistic machine learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">John L. A. Gardner
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">ZoÃ© Faure Beaulieu
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Volker L. Deringer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:volker.deringer@chem.ox.ac.uk">volker.deringer@chem.ox.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Chemistry, Inorganic Chemistry Laboratory, University of Oxford, Oxford OX1 3QR, UK
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Machine-learning models are increasingly used to predict properties of atoms in chemical systems.
There have been major advances in developing descriptors and regression frameworks for this task, typically starting from (relatively) small sets of quantum-mechanical reference data.
Larger datasets of this kind are becoming available, but remain expensive to generate.
Here we demonstrate the use of a large dataset that we have â€œsyntheticallyâ€ labelled with per-atom energies from an existing ML potential model.
The cheapness of this process, compared to the quantum-mechanical ground truth, allows us to generate millions of datapoints, in turn enabling rapid experimentation with atomistic ML models from the small- to the large-data regime.
This approach allows us here to compare regression frameworks in depth, and to explore visualisation based on learned representations.
We also show that learning synthetic data labels can be a useful pre-training task for subsequent fine-tuning on small datasets.
In the future, we expect that our open-sourced dataset, and similar ones, will be useful in rapidly exploring deep-learning models in the limit of abundant chemical data.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Chemical research aims to understand existing, and to discover new, molecules and materials.
The vast size of compositional and configurational chemical space means that physical experiments will quickly reach their limits for these tasks.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
Digital â€œexperimentsâ€, powered by large datasets and machine learning (ML) models, provide high-throughput approaches to chemical discovery,
and can help to answer questions that their physical counterparts on their own can not.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
However, because ML methods generally rely on large datasets rather than on empirical physical knowledge, they require new insight into the methodology itself â€“ one example in this context is the active research into interpretability and explainability of ML models. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Among the central tasks in ML for chemistry is the prediction of atomistic properties as a function of a given atomâ€™s chemical environment. Atomistic ML models have now been developed to predict scalar (<span id="Sx1.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, isotropic chemical shifts),<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> vector (<span id="Sx1.p2.1.2" class="ltx_text ltx_font_italic">e.g.</span>, dipole moments),<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and higher-order tensor properties (<span id="Sx1.p2.1.3" class="ltx_text ltx_font_italic">e.g.</span>, the dielectric response).<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
ML methods are also increasingly enabling accurate, large-scale atomistic simulations based on the â€œlearningâ€ of a given quantum-mechanical potential-energy surface.
Widely used approaches for ML interatomic potential models include neural networks (NNs), <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
kernel-based methods,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
and linear fitting.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
The most suitable choice out of these options may depend on the task and chemical domain. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
For instance, the ability of NNs to scalably learn compressed, hierarchical, and meaningful representations has allowed them to converge to â€œchemical accuracyâ€ in the small-molecule setting on the established QM9 dataset.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">When exploring a new chemical system for which there is no established, large dataset, it is not necessarily obvious which model class will be suitable for a given task, or how a model will perform.
Unfortunately, creating the high-quality, quantum-mechanically accurate data needed to train such ML models is very expensive.
For instance, using density-functional theory (DFT) to generate and label the 1.2 million structures within the OC20 database required the use of large-scale compute resources, and millions of CPU hours.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
This cost often limits the size of dataset available when exploring different model classes on new chemical domains, favouring simpler models with high data economy over more complex ones that benefit from large data quantities.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_01.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic data for atomistic ML. <span id="Sx1.F1.7.1" class="ltx_text ltx_font_italic">Left:</span> Quantum-mechanical (QM) data are used to label a set of structures, <math id="Sx1.F1.3.m1.1" class="ltx_Math" alttext="\textbf{x}_{0}" display="inline"><semantics id="Sx1.F1.3.m1.1b"><msub id="Sx1.F1.3.m1.1.1" xref="Sx1.F1.3.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="Sx1.F1.3.m1.1.1.2" xref="Sx1.F1.3.m1.1.1.2a.cmml">x</mtext><mn id="Sx1.F1.3.m1.1.1.3" xref="Sx1.F1.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Sx1.F1.3.m1.1c"><apply id="Sx1.F1.3.m1.1.1.cmml" xref="Sx1.F1.3.m1.1.1"><csymbol cd="ambiguous" id="Sx1.F1.3.m1.1.1.1.cmml" xref="Sx1.F1.3.m1.1.1">subscript</csymbol><ci id="Sx1.F1.3.m1.1.1.2a.cmml" xref="Sx1.F1.3.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="Sx1.F1.3.m1.1.1.2.cmml" xref="Sx1.F1.3.m1.1.1.2">x</mtext></ci><cn type="integer" id="Sx1.F1.3.m1.1.1.3.cmml" xref="Sx1.F1.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F1.3.m1.1d">\textbf{x}_{0}</annotation></semantics></math>, with energy and force data, <math id="Sx1.F1.4.m2.1" class="ltx_Math" alttext="y_{0}" display="inline"><semantics id="Sx1.F1.4.m2.1b"><msub id="Sx1.F1.4.m2.1.1" xref="Sx1.F1.4.m2.1.1.cmml"><mi id="Sx1.F1.4.m2.1.1.2" xref="Sx1.F1.4.m2.1.1.2.cmml">y</mi><mn id="Sx1.F1.4.m2.1.1.3" xref="Sx1.F1.4.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Sx1.F1.4.m2.1c"><apply id="Sx1.F1.4.m2.1.1.cmml" xref="Sx1.F1.4.m2.1.1"><csymbol cd="ambiguous" id="Sx1.F1.4.m2.1.1.1.cmml" xref="Sx1.F1.4.m2.1.1">subscript</csymbol><ci id="Sx1.F1.4.m2.1.1.2.cmml" xref="Sx1.F1.4.m2.1.1.2">ğ‘¦</ci><cn type="integer" id="Sx1.F1.4.m2.1.1.3.cmml" xref="Sx1.F1.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F1.4.m2.1d">y_{0}</annotation></semantics></math>, and these serve as input for an ML model of the potential-energy surface. <span id="Sx1.F1.8.2" class="ltx_text ltx_font_italic">Right:</span> QM labels are expensive, and so we here use an existing ML model to cheaply generate and label a much larger dataset. The data in this set are â€œsyntheticâ€ as they are not labelled with the ground-truth QM method itself, yet represent its behaviour.
(Note that whilst the QM method describes energies and forces on atoms, our synthetic dataset is labelled only with per-atom energies in the present study.)
</figcaption>
</figure>
<figure id="Sx1.F2" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_02.png" id="Sx1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of ML approaches used in the present work.
(a) A synthetic dataset of atomic energies, predicted by the C-GAP-17 model, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for different categories of carbon environments as sketched. The distributions are shown by kernel density estimates, normalised to the same value for each (note there are much fewer sp than sp<sup id="Sx1.F2.12.1" class="ltx_sup"><span id="Sx1.F2.12.1.1" class="ltx_text ltx_font_italic">2</span></sup> atoms overall). Energies are referenced to that of a free atom.
(b) Smooth Overlap of Atomic Positions (SOAP) power spectrum.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> The power-spectrum vector is an invariant fingerprint of the atomic environment, and illustrated in grey.
(c) Construction of the SOAP kernel, as a dot-product of the power-spectrum vectors for two atomic environments, raised to a power of <math id="Sx1.F2.6.m2.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="Sx1.F2.6.m2.1b"><mi id="Sx1.F2.6.m2.1.1" xref="Sx1.F2.6.m2.1.1.cmml">Î¶</mi><annotation-xml encoding="MathML-Content" id="Sx1.F2.6.m2.1c"><ci id="Sx1.F2.6.m2.1.1.cmml" xref="Sx1.F2.6.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F2.6.m2.1d">\zeta</annotation></semantics></math>.
(d) Neural-network model. We use the power-spectrum vector [<span id="Sx1.F2.13.2" class="ltx_text ltx_font_italic">cf.</span> panel (b)] to directly construct the input layer, and train a network to predict a new value, <math id="Sx1.F2.7.m3.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="Sx1.F2.7.m3.1b"><mover accent="true" id="Sx1.F2.7.m3.1.1" xref="Sx1.F2.7.m3.1.1.cmml"><mi id="Sx1.F2.7.m3.1.1.2" xref="Sx1.F2.7.m3.1.1.2.cmml">y</mi><mo id="Sx1.F2.7.m3.1.1.1" xref="Sx1.F2.7.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="Sx1.F2.7.m3.1c"><apply id="Sx1.F2.7.m3.1.1.cmml" xref="Sx1.F2.7.m3.1.1"><ci id="Sx1.F2.7.m3.1.1.1.cmml" xref="Sx1.F2.7.m3.1.1.1">^</ci><ci id="Sx1.F2.7.m3.1.1.2.cmml" xref="Sx1.F2.7.m3.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F2.7.m3.1d">\hat{y}</annotation></semantics></math>, from this.
(e) Deep kernel learning. A neural network is used to learn a compressed representation of atomic environments, indicated in red, from the original SOAP vectors. Gaussian process regression is then used to make predictions in this compressed space, using a learned set of coefficients, <span id="Sx1.F2.14.3" class="ltx_text ltx_markedasmath ltx_font_bold">c</span>, and the similarity of a new point to each entry in the data set.
Panel (b) is adapted from Ref.Â <cite class="ltx_cite ltx_citemacro_citenum"><a href="#bib.bib30" title="" class="ltx_ref">30</a></cite>, originally published under a CC BY licence (https://creativecommons.org/licenses/by/4.0/).
</figcaption>
</figure>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Here we demonstrate the use of synthetic data labels, obtained from an existing ML potential model (Fig.Â <a href="#Sx1.F1" title="Figure 1 â€£ Introduction â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), as a means to sidestep the high computational cost of quantum-accurate labelling that would otherwise be required for experimenting with atomistic ML approaches.
Concretely, we introduce an open-sourced dataset containing 22.9 million atomic environments drawn from ML-driven molecular-dynamics (MD) simulations of diverse disordered carbon structures, subsequently labelled in less than a day on local, consumer-level compute.
The size of this dataset enables us to study the behaviour of different ML models in the small- and large-data limits.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Dataset</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Our dataset consists of 546 independent MD trajectories describing the melt-quenching and thermal annealing of elemental carbon. The development of ML potentials for carbon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and their application to scientific problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> have been widely documented in the literature, and the existence of established potentials such as C-GAP-17 (Ref.Â <cite class="ltx_cite ltx_citemacro_citenum"><a href="#bib.bib28" title="" class="ltx_ref">28</a></cite>) means that there is a direct route for creating synthetic data.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.2" class="ltx_p">Initial randomised configurations of 200 atoms per cell at varying densities, from 1.0 to 3.5â€‰gâ€‰cm<sup id="Sx2.p2.2.1" class="ltx_sup"><span id="Sx2.p2.2.1.1" class="ltx_text ltx_font_italic">-3</span></sup> in 0.1â€‰gâ€‰cm<sup id="Sx2.p2.2.2" class="ltx_sup"><span id="Sx2.p2.2.2.1" class="ltx_text ltx_font_italic">-3</span></sup> increments, were generated using ASE.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
Each structure then underwent an MD simulation driven by C-GAP-17, as implemented in LAMMPS.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> First, each structure was melted at 9000â€‰K for 5â€‰ps before being quenched to 300â€‰K over a further 5â€‰ps. Second, each structure was reheated to a specific temperature at which it was annealed for 100â€‰ps, before finally being cooled back down to 300â€‰K over 50â€‰ps. The annealing temperatures ranged from 2000 to 4000â€‰K in 100â€‰K increments. These protocols are in line with prior quenching-and-annealing type simulations with empirical and machine-learned potentials. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite></p>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.7" class="ltx_p">The resulting database captures a wide variety of chemical environments, including graphitic structures, buckyball-esque clusters, grains of cubic and hexagonal diamond, and tetrahedral amorphous carbon.
Every atom in the dataset was labelled using the C-GAP-17 potential, which predicts per-atom energies as a function of a given atomâ€™s environment. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> Figure <a href="#Sx1.F2" title="Figure 2 â€£ Introduction â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a shows the distribution of these energies in the dataset, categorised in a simplified manner by their coordination number: â€œspâ€ as in carbon chains (<math id="Sx2.p3.1.m1.1" class="ltx_Math" alttext="N=2" display="inline"><semantics id="Sx2.p3.1.m1.1a"><mrow id="Sx2.p3.1.m1.1.1" xref="Sx2.p3.1.m1.1.1.cmml"><mi id="Sx2.p3.1.m1.1.1.2" xref="Sx2.p3.1.m1.1.1.2.cmml">N</mi><mo id="Sx2.p3.1.m1.1.1.1" xref="Sx2.p3.1.m1.1.1.1.cmml">=</mo><mn id="Sx2.p3.1.m1.1.1.3" xref="Sx2.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.p3.1.m1.1b"><apply id="Sx2.p3.1.m1.1.1.cmml" xref="Sx2.p3.1.m1.1.1"><eq id="Sx2.p3.1.m1.1.1.1.cmml" xref="Sx2.p3.1.m1.1.1.1"></eq><ci id="Sx2.p3.1.m1.1.1.2.cmml" xref="Sx2.p3.1.m1.1.1.2">ğ‘</ci><cn type="integer" id="Sx2.p3.1.m1.1.1.3.cmml" xref="Sx2.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.1.m1.1c">N=2</annotation></semantics></math>), â€œsp<sup id="Sx2.p3.7.1" class="ltx_sup"><span id="Sx2.p3.7.1.1" class="ltx_text ltx_font_italic">2</span></sup>â€ as in graphite (<math id="Sx2.p3.3.m3.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="Sx2.p3.3.m3.1a"><mrow id="Sx2.p3.3.m3.1.1" xref="Sx2.p3.3.m3.1.1.cmml"><mi id="Sx2.p3.3.m3.1.1.2" xref="Sx2.p3.3.m3.1.1.2.cmml">N</mi><mo id="Sx2.p3.3.m3.1.1.1" xref="Sx2.p3.3.m3.1.1.1.cmml">=</mo><mn id="Sx2.p3.3.m3.1.1.3" xref="Sx2.p3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.p3.3.m3.1b"><apply id="Sx2.p3.3.m3.1.1.cmml" xref="Sx2.p3.3.m3.1.1"><eq id="Sx2.p3.3.m3.1.1.1.cmml" xref="Sx2.p3.3.m3.1.1.1"></eq><ci id="Sx2.p3.3.m3.1.1.2.cmml" xref="Sx2.p3.3.m3.1.1.2">ğ‘</ci><cn type="integer" id="Sx2.p3.3.m3.1.1.3.cmml" xref="Sx2.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.3.m3.1c">N=3</annotation></semantics></math>), and â€œsp<sup id="Sx2.p3.7.2" class="ltx_sup"><span id="Sx2.p3.7.2.1" class="ltx_text ltx_font_italic">3</span></sup>â€ as in diamond (<math id="Sx2.p3.5.m5.1" class="ltx_Math" alttext="N=4" display="inline"><semantics id="Sx2.p3.5.m5.1a"><mrow id="Sx2.p3.5.m5.1.1" xref="Sx2.p3.5.m5.1.1.cmml"><mi id="Sx2.p3.5.m5.1.1.2" xref="Sx2.p3.5.m5.1.1.2.cmml">N</mi><mo id="Sx2.p3.5.m5.1.1.1" xref="Sx2.p3.5.m5.1.1.1.cmml">=</mo><mn id="Sx2.p3.5.m5.1.1.3" xref="Sx2.p3.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.p3.5.m5.1b"><apply id="Sx2.p3.5.m5.1.1.cmml" xref="Sx2.p3.5.m5.1.1"><eq id="Sx2.p3.5.m5.1.1.1.cmml" xref="Sx2.p3.5.m5.1.1.1"></eq><ci id="Sx2.p3.5.m5.1.1.2.cmml" xref="Sx2.p3.5.m5.1.1.2">ğ‘</ci><cn type="integer" id="Sx2.p3.5.m5.1.1.3.cmml" xref="Sx2.p3.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.5.m5.1c">N=4</annotation></semantics></math>). The energies for the sp<sup id="Sx2.p3.7.3" class="ltx_sup"><span id="Sx2.p3.7.3.1" class="ltx_text ltx_font_italic">2</span></sup> and sp<sup id="Sx2.p3.7.4" class="ltx_sup"><span id="Sx2.p3.7.4.1" class="ltx_text ltx_font_italic">3</span></sup> environments are rather similar, consistent with the very similar energy of graphite and diamond; those for sp atoms are notably higher.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Methods</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p"><span id="Sx3.p1.1.1" class="ltx_text ltx_font_bold">Structural descriptors.</span> We describe (â€œfeaturiseâ€) atomic environments using the Smooth Overlap of Atomic Positions (SOAP) technique.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> SOAP is based on the idea of a local-basis expansion of the atomic neighbour density and subsequent construction of a rotationally invariant power spectrum (Fig.Â <a href="#Sx1.F2" title="Figure 2 â€£ Introduction â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b).<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
Initially developed as a similarity measure between pairs of local neighbourhood densities, SOAP can also provide a descriptor of a single local environment, and be used as input to other ML techniques. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
In the present work, we use SOAP power-spectrum vectors in two ways: to construct kernel matrices for Gaussian process regression (GPR), and as a base from which to learn richer and compressed descriptions using neural network models (Fig.Â <a href="#Sx1.F2" title="Figure 2 â€£ Introduction â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>câ€“e).</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.10" class="ltx_p">The SOAP descriptor is controlled by four (hyper-) parameters.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> Two convergence parameters, <math id="Sx3.p2.1.m1.1" class="ltx_Math" alttext="n_{\max}" display="inline"><semantics id="Sx3.p2.1.m1.1a"><msub id="Sx3.p2.1.m1.1.1" xref="Sx3.p2.1.m1.1.1.cmml"><mi id="Sx3.p2.1.m1.1.1.2" xref="Sx3.p2.1.m1.1.1.2.cmml">n</mi><mi id="Sx3.p2.1.m1.1.1.3" xref="Sx3.p2.1.m1.1.1.3.cmml">max</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.1.m1.1b"><apply id="Sx3.p2.1.m1.1.1.cmml" xref="Sx3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.p2.1.m1.1.1.1.cmml" xref="Sx3.p2.1.m1.1.1">subscript</csymbol><ci id="Sx3.p2.1.m1.1.1.2.cmml" xref="Sx3.p2.1.m1.1.1.2">ğ‘›</ci><max id="Sx3.p2.1.m1.1.1.3.cmml" xref="Sx3.p2.1.m1.1.1.3"></max></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.1.m1.1c">n_{\max}</annotation></semantics></math> and <math id="Sx3.p2.2.m2.1" class="ltx_Math" alttext="l_{\max}" display="inline"><semantics id="Sx3.p2.2.m2.1a"><msub id="Sx3.p2.2.m2.1.1" xref="Sx3.p2.2.m2.1.1.cmml"><mi id="Sx3.p2.2.m2.1.1.2" xref="Sx3.p2.2.m2.1.1.2.cmml">l</mi><mi id="Sx3.p2.2.m2.1.1.3" xref="Sx3.p2.2.m2.1.1.3.cmml">max</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.2.m2.1b"><apply id="Sx3.p2.2.m2.1.1.cmml" xref="Sx3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.p2.2.m2.1.1.1.cmml" xref="Sx3.p2.2.m2.1.1">subscript</csymbol><ci id="Sx3.p2.2.m2.1.1.2.cmml" xref="Sx3.p2.2.m2.1.1.2">ğ‘™</ci><max id="Sx3.p2.2.m2.1.1.3.cmml" xref="Sx3.p2.2.m2.1.1.3"></max></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.2.m2.1c">l_{\max}</annotation></semantics></math>, control the number of radial and angular basis functions, respectively; the radial cut-off, <math id="Sx3.p2.3.m3.1" class="ltx_Math" alttext="r_{\textrm{cut}}" display="inline"><semantics id="Sx3.p2.3.m3.1a"><msub id="Sx3.p2.3.m3.1.1" xref="Sx3.p2.3.m3.1.1.cmml"><mi id="Sx3.p2.3.m3.1.1.2" xref="Sx3.p2.3.m3.1.1.2.cmml">r</mi><mtext id="Sx3.p2.3.m3.1.1.3" xref="Sx3.p2.3.m3.1.1.3a.cmml">cut</mtext></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.3.m3.1b"><apply id="Sx3.p2.3.m3.1.1.cmml" xref="Sx3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="Sx3.p2.3.m3.1.1.1.cmml" xref="Sx3.p2.3.m3.1.1">subscript</csymbol><ci id="Sx3.p2.3.m3.1.1.2.cmml" xref="Sx3.p2.3.m3.1.1.2">ğ‘Ÿ</ci><ci id="Sx3.p2.3.m3.1.1.3a.cmml" xref="Sx3.p2.3.m3.1.1.3"><mtext mathsize="70%" id="Sx3.p2.3.m3.1.1.3.cmml" xref="Sx3.p2.3.m3.1.1.3">cut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.3.m3.1c">r_{\textrm{cut}}</annotation></semantics></math>, defines the locality of the environment, and a Gaussian broadening width, <math id="Sx3.p2.4.m4.1" class="ltx_Math" alttext="\sigma_{\textrm{at}}" display="inline"><semantics id="Sx3.p2.4.m4.1a"><msub id="Sx3.p2.4.m4.1.1" xref="Sx3.p2.4.m4.1.1.cmml"><mi id="Sx3.p2.4.m4.1.1.2" xref="Sx3.p2.4.m4.1.1.2.cmml">Ïƒ</mi><mtext id="Sx3.p2.4.m4.1.1.3" xref="Sx3.p2.4.m4.1.1.3a.cmml">at</mtext></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.4.m4.1b"><apply id="Sx3.p2.4.m4.1.1.cmml" xref="Sx3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="Sx3.p2.4.m4.1.1.1.cmml" xref="Sx3.p2.4.m4.1.1">subscript</csymbol><ci id="Sx3.p2.4.m4.1.1.2.cmml" xref="Sx3.p2.4.m4.1.1.2">ğœ</ci><ci id="Sx3.p2.4.m4.1.1.3a.cmml" xref="Sx3.p2.4.m4.1.1.3"><mtext mathsize="70%" id="Sx3.p2.4.m4.1.1.3.cmml" xref="Sx3.p2.4.m4.1.1.3">at</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.4.m4.1c">\sigma_{\textrm{at}}</annotation></semantics></math>, controls the smoothness of the atomic neighbourhood densities.
Here, descriptor vectors pre-calculated using <math id="Sx3.p2.5.m5.4" class="ltx_Math" alttext="(n_{\max},l_{\max})=(12,6)" display="inline"><semantics id="Sx3.p2.5.m5.4a"><mrow id="Sx3.p2.5.m5.4.4" xref="Sx3.p2.5.m5.4.4.cmml"><mrow id="Sx3.p2.5.m5.4.4.2.2" xref="Sx3.p2.5.m5.4.4.2.3.cmml"><mo stretchy="false" id="Sx3.p2.5.m5.4.4.2.2.3" xref="Sx3.p2.5.m5.4.4.2.3.cmml">(</mo><msub id="Sx3.p2.5.m5.3.3.1.1.1" xref="Sx3.p2.5.m5.3.3.1.1.1.cmml"><mi id="Sx3.p2.5.m5.3.3.1.1.1.2" xref="Sx3.p2.5.m5.3.3.1.1.1.2.cmml">n</mi><mi id="Sx3.p2.5.m5.3.3.1.1.1.3" xref="Sx3.p2.5.m5.3.3.1.1.1.3.cmml">max</mi></msub><mo id="Sx3.p2.5.m5.4.4.2.2.4" xref="Sx3.p2.5.m5.4.4.2.3.cmml">,</mo><msub id="Sx3.p2.5.m5.4.4.2.2.2" xref="Sx3.p2.5.m5.4.4.2.2.2.cmml"><mi id="Sx3.p2.5.m5.4.4.2.2.2.2" xref="Sx3.p2.5.m5.4.4.2.2.2.2.cmml">l</mi><mi id="Sx3.p2.5.m5.4.4.2.2.2.3" xref="Sx3.p2.5.m5.4.4.2.2.2.3.cmml">max</mi></msub><mo stretchy="false" id="Sx3.p2.5.m5.4.4.2.2.5" xref="Sx3.p2.5.m5.4.4.2.3.cmml">)</mo></mrow><mo id="Sx3.p2.5.m5.4.4.3" xref="Sx3.p2.5.m5.4.4.3.cmml">=</mo><mrow id="Sx3.p2.5.m5.4.4.4.2" xref="Sx3.p2.5.m5.4.4.4.1.cmml"><mo stretchy="false" id="Sx3.p2.5.m5.4.4.4.2.1" xref="Sx3.p2.5.m5.4.4.4.1.cmml">(</mo><mn id="Sx3.p2.5.m5.1.1" xref="Sx3.p2.5.m5.1.1.cmml">12</mn><mo id="Sx3.p2.5.m5.4.4.4.2.2" xref="Sx3.p2.5.m5.4.4.4.1.cmml">,</mo><mn id="Sx3.p2.5.m5.2.2" xref="Sx3.p2.5.m5.2.2.cmml">6</mn><mo stretchy="false" id="Sx3.p2.5.m5.4.4.4.2.3" xref="Sx3.p2.5.m5.4.4.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p2.5.m5.4b"><apply id="Sx3.p2.5.m5.4.4.cmml" xref="Sx3.p2.5.m5.4.4"><eq id="Sx3.p2.5.m5.4.4.3.cmml" xref="Sx3.p2.5.m5.4.4.3"></eq><interval closure="open" id="Sx3.p2.5.m5.4.4.2.3.cmml" xref="Sx3.p2.5.m5.4.4.2.2"><apply id="Sx3.p2.5.m5.3.3.1.1.1.cmml" xref="Sx3.p2.5.m5.3.3.1.1.1"><csymbol cd="ambiguous" id="Sx3.p2.5.m5.3.3.1.1.1.1.cmml" xref="Sx3.p2.5.m5.3.3.1.1.1">subscript</csymbol><ci id="Sx3.p2.5.m5.3.3.1.1.1.2.cmml" xref="Sx3.p2.5.m5.3.3.1.1.1.2">ğ‘›</ci><max id="Sx3.p2.5.m5.3.3.1.1.1.3.cmml" xref="Sx3.p2.5.m5.3.3.1.1.1.3"></max></apply><apply id="Sx3.p2.5.m5.4.4.2.2.2.cmml" xref="Sx3.p2.5.m5.4.4.2.2.2"><csymbol cd="ambiguous" id="Sx3.p2.5.m5.4.4.2.2.2.1.cmml" xref="Sx3.p2.5.m5.4.4.2.2.2">subscript</csymbol><ci id="Sx3.p2.5.m5.4.4.2.2.2.2.cmml" xref="Sx3.p2.5.m5.4.4.2.2.2.2">ğ‘™</ci><max id="Sx3.p2.5.m5.4.4.2.2.2.3.cmml" xref="Sx3.p2.5.m5.4.4.2.2.2.3"></max></apply></interval><interval closure="open" id="Sx3.p2.5.m5.4.4.4.1.cmml" xref="Sx3.p2.5.m5.4.4.4.2"><cn type="integer" id="Sx3.p2.5.m5.1.1.cmml" xref="Sx3.p2.5.m5.1.1">12</cn><cn type="integer" id="Sx3.p2.5.m5.2.2.cmml" xref="Sx3.p2.5.m5.2.2">6</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.5.m5.4c">(n_{\max},l_{\max})=(12,6)</annotation></semantics></math> led to convergence for the average value in the SOAP similarity matrix for a 200-atom structure to within 0.01%, as compared to <math id="Sx3.p2.6.m6.2" class="ltx_Math" alttext="(16,16)" display="inline"><semantics id="Sx3.p2.6.m6.2a"><mrow id="Sx3.p2.6.m6.2.3.2" xref="Sx3.p2.6.m6.2.3.1.cmml"><mo stretchy="false" id="Sx3.p2.6.m6.2.3.2.1" xref="Sx3.p2.6.m6.2.3.1.cmml">(</mo><mn id="Sx3.p2.6.m6.1.1" xref="Sx3.p2.6.m6.1.1.cmml">16</mn><mo id="Sx3.p2.6.m6.2.3.2.2" xref="Sx3.p2.6.m6.2.3.1.cmml">,</mo><mn id="Sx3.p2.6.m6.2.2" xref="Sx3.p2.6.m6.2.2.cmml">16</mn><mo stretchy="false" id="Sx3.p2.6.m6.2.3.2.3" xref="Sx3.p2.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p2.6.m6.2b"><interval closure="open" id="Sx3.p2.6.m6.2.3.1.cmml" xref="Sx3.p2.6.m6.2.3.2"><cn type="integer" id="Sx3.p2.6.m6.1.1.cmml" xref="Sx3.p2.6.m6.1.1">16</cn><cn type="integer" id="Sx3.p2.6.m6.2.2.cmml" xref="Sx3.p2.6.m6.2.2">16</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.6.m6.2c">(16,16)</annotation></semantics></math>.
Values of <math id="Sx3.p2.7.m7.1" class="ltx_Math" alttext="3.7" display="inline"><semantics id="Sx3.p2.7.m7.1a"><mn id="Sx3.p2.7.m7.1.1" xref="Sx3.p2.7.m7.1.1.cmml">3.7</mn><annotation-xml encoding="MathML-Content" id="Sx3.p2.7.m7.1b"><cn type="float" id="Sx3.p2.7.m7.1.1.cmml" xref="Sx3.p2.7.m7.1.1">3.7</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.7.m7.1c">3.7</annotation></semantics></math>â€‰Ã… and <math id="Sx3.p2.8.m8.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="Sx3.p2.8.m8.1a"><mn id="Sx3.p2.8.m8.1.1" xref="Sx3.p2.8.m8.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="Sx3.p2.8.m8.1b"><cn type="float" id="Sx3.p2.8.m8.1.1.cmml" xref="Sx3.p2.8.m8.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.8.m8.1c">0.5</annotation></semantics></math>â€‰Ã… for <math id="Sx3.p2.9.m9.1" class="ltx_Math" alttext="r_{\textrm{cut}}" display="inline"><semantics id="Sx3.p2.9.m9.1a"><msub id="Sx3.p2.9.m9.1.1" xref="Sx3.p2.9.m9.1.1.cmml"><mi id="Sx3.p2.9.m9.1.1.2" xref="Sx3.p2.9.m9.1.1.2.cmml">r</mi><mtext id="Sx3.p2.9.m9.1.1.3" xref="Sx3.p2.9.m9.1.1.3a.cmml">cut</mtext></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.9.m9.1b"><apply id="Sx3.p2.9.m9.1.1.cmml" xref="Sx3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="Sx3.p2.9.m9.1.1.1.cmml" xref="Sx3.p2.9.m9.1.1">subscript</csymbol><ci id="Sx3.p2.9.m9.1.1.2.cmml" xref="Sx3.p2.9.m9.1.1.2">ğ‘Ÿ</ci><ci id="Sx3.p2.9.m9.1.1.3a.cmml" xref="Sx3.p2.9.m9.1.1.3"><mtext mathsize="70%" id="Sx3.p2.9.m9.1.1.3.cmml" xref="Sx3.p2.9.m9.1.1.3">cut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.9.m9.1c">r_{\textrm{cut}}</annotation></semantics></math> and <math id="Sx3.p2.10.m10.1" class="ltx_Math" alttext="\sigma_{\textrm{at}}" display="inline"><semantics id="Sx3.p2.10.m10.1a"><msub id="Sx3.p2.10.m10.1.1" xref="Sx3.p2.10.m10.1.1.cmml"><mi id="Sx3.p2.10.m10.1.1.2" xref="Sx3.p2.10.m10.1.1.2.cmml">Ïƒ</mi><mtext id="Sx3.p2.10.m10.1.1.3" xref="Sx3.p2.10.m10.1.1.3a.cmml">at</mtext></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.10.m10.1b"><apply id="Sx3.p2.10.m10.1.1.cmml" xref="Sx3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="Sx3.p2.10.m10.1.1.1.cmml" xref="Sx3.p2.10.m10.1.1">subscript</csymbol><ci id="Sx3.p2.10.m10.1.1.2.cmml" xref="Sx3.p2.10.m10.1.1.2">ğœ</ci><ci id="Sx3.p2.10.m10.1.1.3a.cmml" xref="Sx3.p2.10.m10.1.1.3"><mtext mathsize="70%" id="Sx3.p2.10.m10.1.1.3.cmml" xref="Sx3.p2.10.m10.1.1.3">at</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.10.m10.1c">\sigma_{\textrm{at}}</annotation></semantics></math>, respectively, were used, in line with the settings for the C-GAP-17 model.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.5" class="ltx_p"><span id="Sx3.p3.5.1" class="ltx_text ltx_font_bold">Gaussian process regression (GPR).</span>
GPR non-parametrically fits a probabilistic model to high-dimensional data.
For a detailed introduction to GPR, see Ref.Â <cite class="ltx_cite ltx_citemacro_citenum"><a href="#bib.bib47" title="" class="ltx_ref">47</a></cite>, and for its applications in chemistry, see Ref.Â <cite class="ltx_cite ltx_citemacro_citenum"><a href="#bib.bib30" title="" class="ltx_ref">30</a></cite>.
At a high level, prediction at a test point, <math id="Sx3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{x}^{\prime}" display="inline"><semantics id="Sx3.p3.1.m1.1a"><msup id="Sx3.p3.1.m1.1.1" xref="Sx3.p3.1.m1.1.1.cmml"><mi id="Sx3.p3.1.m1.1.1.2" xref="Sx3.p3.1.m1.1.1.2.cmml">ğ±</mi><mo id="Sx3.p3.1.m1.1.1.3" xref="Sx3.p3.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="Sx3.p3.1.m1.1b"><apply id="Sx3.p3.1.m1.1.1.cmml" xref="Sx3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.p3.1.m1.1.1.1.cmml" xref="Sx3.p3.1.m1.1.1">superscript</csymbol><ci id="Sx3.p3.1.m1.1.1.2.cmml" xref="Sx3.p3.1.m1.1.1.2">ğ±</ci><ci id="Sx3.p3.1.m1.1.1.3.cmml" xref="Sx3.p3.1.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.1.m1.1c">\mathbf{x}^{\prime}</annotation></semantics></math>, involves calculating its similarity to each data location in the training set, <math id="Sx3.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{x}_{i}" display="inline"><semantics id="Sx3.p3.2.m2.1a"><msub id="Sx3.p3.2.m2.1.1" xref="Sx3.p3.2.m2.1.1.cmml"><mi id="Sx3.p3.2.m2.1.1.2" xref="Sx3.p3.2.m2.1.1.2.cmml">ğ±</mi><mi id="Sx3.p3.2.m2.1.1.3" xref="Sx3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.p3.2.m2.1b"><apply id="Sx3.p3.2.m2.1.1.cmml" xref="Sx3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.p3.2.m2.1.1.1.cmml" xref="Sx3.p3.2.m2.1.1">subscript</csymbol><ci id="Sx3.p3.2.m2.1.1.2.cmml" xref="Sx3.p3.2.m2.1.1.2">ğ±</ci><ci id="Sx3.p3.2.m2.1.1.3.cmml" xref="Sx3.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.2.m2.1c">\mathbf{x}_{i}</annotation></semantics></math>, using a specified kernel, <math id="Sx3.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="Sx3.p3.3.m3.1a"><mi id="Sx3.p3.3.m3.1.1" xref="Sx3.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Sx3.p3.3.m3.1b"><ci id="Sx3.p3.3.m3.1.1.cmml" xref="Sx3.p3.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.3.m3.1c">k</annotation></semantics></math>.
Each of these similarities, <math id="Sx3.p3.4.m4.2" class="ltx_Math" alttext="k(\mathbf{x}_{i},\mathbf{x}^{\prime})" display="inline"><semantics id="Sx3.p3.4.m4.2a"><mrow id="Sx3.p3.4.m4.2.2" xref="Sx3.p3.4.m4.2.2.cmml"><mi id="Sx3.p3.4.m4.2.2.4" xref="Sx3.p3.4.m4.2.2.4.cmml">k</mi><mo lspace="0em" rspace="0em" id="Sx3.p3.4.m4.2.2.3" xref="Sx3.p3.4.m4.2.2.3.cmml">â€‹</mo><mrow id="Sx3.p3.4.m4.2.2.2.2" xref="Sx3.p3.4.m4.2.2.2.3.cmml"><mo stretchy="false" id="Sx3.p3.4.m4.2.2.2.2.3" xref="Sx3.p3.4.m4.2.2.2.3.cmml">(</mo><msub id="Sx3.p3.4.m4.1.1.1.1.1" xref="Sx3.p3.4.m4.1.1.1.1.1.cmml"><mi id="Sx3.p3.4.m4.1.1.1.1.1.2" xref="Sx3.p3.4.m4.1.1.1.1.1.2.cmml">ğ±</mi><mi id="Sx3.p3.4.m4.1.1.1.1.1.3" xref="Sx3.p3.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo id="Sx3.p3.4.m4.2.2.2.2.4" xref="Sx3.p3.4.m4.2.2.2.3.cmml">,</mo><msup id="Sx3.p3.4.m4.2.2.2.2.2" xref="Sx3.p3.4.m4.2.2.2.2.2.cmml"><mi id="Sx3.p3.4.m4.2.2.2.2.2.2" xref="Sx3.p3.4.m4.2.2.2.2.2.2.cmml">ğ±</mi><mo id="Sx3.p3.4.m4.2.2.2.2.2.3" xref="Sx3.p3.4.m4.2.2.2.2.2.3.cmml">â€²</mo></msup><mo stretchy="false" id="Sx3.p3.4.m4.2.2.2.2.5" xref="Sx3.p3.4.m4.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p3.4.m4.2b"><apply id="Sx3.p3.4.m4.2.2.cmml" xref="Sx3.p3.4.m4.2.2"><times id="Sx3.p3.4.m4.2.2.3.cmml" xref="Sx3.p3.4.m4.2.2.3"></times><ci id="Sx3.p3.4.m4.2.2.4.cmml" xref="Sx3.p3.4.m4.2.2.4">ğ‘˜</ci><interval closure="open" id="Sx3.p3.4.m4.2.2.2.3.cmml" xref="Sx3.p3.4.m4.2.2.2.2"><apply id="Sx3.p3.4.m4.1.1.1.1.1.cmml" xref="Sx3.p3.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.p3.4.m4.1.1.1.1.1.1.cmml" xref="Sx3.p3.4.m4.1.1.1.1.1">subscript</csymbol><ci id="Sx3.p3.4.m4.1.1.1.1.1.2.cmml" xref="Sx3.p3.4.m4.1.1.1.1.1.2">ğ±</ci><ci id="Sx3.p3.4.m4.1.1.1.1.1.3.cmml" xref="Sx3.p3.4.m4.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="Sx3.p3.4.m4.2.2.2.2.2.cmml" xref="Sx3.p3.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="Sx3.p3.4.m4.2.2.2.2.2.1.cmml" xref="Sx3.p3.4.m4.2.2.2.2.2">superscript</csymbol><ci id="Sx3.p3.4.m4.2.2.2.2.2.2.cmml" xref="Sx3.p3.4.m4.2.2.2.2.2.2">ğ±</ci><ci id="Sx3.p3.4.m4.2.2.2.2.2.3.cmml" xref="Sx3.p3.4.m4.2.2.2.2.2.3">â€²</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.4.m4.2c">k(\mathbf{x}_{i},\mathbf{x}^{\prime})</annotation></semantics></math>, then modulates a coefficient, <math id="Sx3.p3.5.m5.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="Sx3.p3.5.m5.1a"><msub id="Sx3.p3.5.m5.1.1" xref="Sx3.p3.5.m5.1.1.cmml"><mi id="Sx3.p3.5.m5.1.1.2" xref="Sx3.p3.5.m5.1.1.2.cmml">c</mi><mi id="Sx3.p3.5.m5.1.1.3" xref="Sx3.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.p3.5.m5.1b"><apply id="Sx3.p3.5.m5.1.1.cmml" xref="Sx3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="Sx3.p3.5.m5.1.1.1.cmml" xref="Sx3.p3.5.m5.1.1">subscript</csymbol><ci id="Sx3.p3.5.m5.1.1.2.cmml" xref="Sx3.p3.5.m5.1.1.2">ğ‘</ci><ci id="Sx3.p3.5.m5.1.1.3.cmml" xref="Sx3.p3.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.5.m5.1c">c_{i}</annotation></semantics></math>, learned during training, such that the prediction is</p>
<table id="Sx3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx3.E1.m1.1" class="ltx_math_unparsed" alttext="\hat{y}(\mathbf{x}^{\prime})=\sum_{i}^{N}c_{i}\cdot k(\mathbf{x}_{i},\mathbf{x}^{\prime})\quad\equiv\mathbf{c}\cdot k(\mathbf{X},\mathbf{x}^{\prime})." display="block"><semantics id="Sx3.E1.m1.1a"><mrow id="Sx3.E1.m1.1b"><mover accent="true" id="Sx3.E1.m1.1.1"><mi id="Sx3.E1.m1.1.1.2">y</mi><mo id="Sx3.E1.m1.1.1.1">^</mo></mover><mrow id="Sx3.E1.m1.1.2"><mo stretchy="false" id="Sx3.E1.m1.1.2.1">(</mo><msup id="Sx3.E1.m1.1.2.2"><mi id="Sx3.E1.m1.1.2.2.2">ğ±</mi><mo id="Sx3.E1.m1.1.2.2.3">â€²</mo></msup><mo stretchy="false" id="Sx3.E1.m1.1.2.3">)</mo></mrow><mo rspace="0.111em" id="Sx3.E1.m1.1.3">=</mo><munderover id="Sx3.E1.m1.1.4"><mo movablelimits="false" id="Sx3.E1.m1.1.4.2.2">âˆ‘</mo><mi id="Sx3.E1.m1.1.4.2.3">i</mi><mi id="Sx3.E1.m1.1.4.3">N</mi></munderover><msub id="Sx3.E1.m1.1.5"><mi id="Sx3.E1.m1.1.5.2">c</mi><mi id="Sx3.E1.m1.1.5.3">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="Sx3.E1.m1.1.6">â‹…</mo><mi id="Sx3.E1.m1.1.7">k</mi><mrow id="Sx3.E1.m1.1.8"><mo stretchy="false" id="Sx3.E1.m1.1.8.1">(</mo><msub id="Sx3.E1.m1.1.8.2"><mi id="Sx3.E1.m1.1.8.2.2">ğ±</mi><mi id="Sx3.E1.m1.1.8.2.3">i</mi></msub><mo id="Sx3.E1.m1.1.8.3">,</mo><msup id="Sx3.E1.m1.1.8.4"><mi id="Sx3.E1.m1.1.8.4.2">ğ±</mi><mo id="Sx3.E1.m1.1.8.4.3">â€²</mo></msup><mo stretchy="false" id="Sx3.E1.m1.1.8.5">)</mo></mrow><mspace width="1em" id="Sx3.E1.m1.1.9"></mspace><mo id="Sx3.E1.m1.1.10">â‰¡</mo><mi id="Sx3.E1.m1.1.11">ğœ</mi><mo lspace="0.222em" rspace="0.222em" id="Sx3.E1.m1.1.12">â‹…</mo><mi id="Sx3.E1.m1.1.13">k</mi><mrow id="Sx3.E1.m1.1.14"><mo stretchy="false" id="Sx3.E1.m1.1.14.1">(</mo><mi id="Sx3.E1.m1.1.14.2">ğ—</mi><mo id="Sx3.E1.m1.1.14.3">,</mo><msup id="Sx3.E1.m1.1.14.4"><mi id="Sx3.E1.m1.1.14.4.2">ğ±</mi><mo id="Sx3.E1.m1.1.14.4.3">â€²</mo></msup><mo stretchy="false" id="Sx3.E1.m1.1.14.5">)</mo></mrow><mo lspace="0em" id="Sx3.E1.m1.1.15">.</mo></mrow><annotation encoding="application/x-tex" id="Sx3.E1.m1.1c">\hat{y}(\mathbf{x}^{\prime})=\sum_{i}^{N}c_{i}\cdot k(\mathbf{x}_{i},\mathbf{x}^{\prime})\quad\equiv\mathbf{c}\cdot k(\mathbf{X},\mathbf{x}^{\prime}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx3.p3.7" class="ltx_p">In this work, we evaluated <math id="Sx3.p3.6.m1.2" class="ltx_Math" alttext="k(\mathbf{x},\mathbf{x}^{\prime})" display="inline"><semantics id="Sx3.p3.6.m1.2a"><mrow id="Sx3.p3.6.m1.2.2" xref="Sx3.p3.6.m1.2.2.cmml"><mi id="Sx3.p3.6.m1.2.2.3" xref="Sx3.p3.6.m1.2.2.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="Sx3.p3.6.m1.2.2.2" xref="Sx3.p3.6.m1.2.2.2.cmml">â€‹</mo><mrow id="Sx3.p3.6.m1.2.2.1.1" xref="Sx3.p3.6.m1.2.2.1.2.cmml"><mo stretchy="false" id="Sx3.p3.6.m1.2.2.1.1.2" xref="Sx3.p3.6.m1.2.2.1.2.cmml">(</mo><mi id="Sx3.p3.6.m1.1.1" xref="Sx3.p3.6.m1.1.1.cmml">ğ±</mi><mo id="Sx3.p3.6.m1.2.2.1.1.3" xref="Sx3.p3.6.m1.2.2.1.2.cmml">,</mo><msup id="Sx3.p3.6.m1.2.2.1.1.1" xref="Sx3.p3.6.m1.2.2.1.1.1.cmml"><mi id="Sx3.p3.6.m1.2.2.1.1.1.2" xref="Sx3.p3.6.m1.2.2.1.1.1.2.cmml">ğ±</mi><mo id="Sx3.p3.6.m1.2.2.1.1.1.3" xref="Sx3.p3.6.m1.2.2.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="Sx3.p3.6.m1.2.2.1.1.4" xref="Sx3.p3.6.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p3.6.m1.2b"><apply id="Sx3.p3.6.m1.2.2.cmml" xref="Sx3.p3.6.m1.2.2"><times id="Sx3.p3.6.m1.2.2.2.cmml" xref="Sx3.p3.6.m1.2.2.2"></times><ci id="Sx3.p3.6.m1.2.2.3.cmml" xref="Sx3.p3.6.m1.2.2.3">ğ‘˜</ci><interval closure="open" id="Sx3.p3.6.m1.2.2.1.2.cmml" xref="Sx3.p3.6.m1.2.2.1.1"><ci id="Sx3.p3.6.m1.1.1.cmml" xref="Sx3.p3.6.m1.1.1">ğ±</ci><apply id="Sx3.p3.6.m1.2.2.1.1.1.cmml" xref="Sx3.p3.6.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx3.p3.6.m1.2.2.1.1.1.1.cmml" xref="Sx3.p3.6.m1.2.2.1.1.1">superscript</csymbol><ci id="Sx3.p3.6.m1.2.2.1.1.1.2.cmml" xref="Sx3.p3.6.m1.2.2.1.1.1.2">ğ±</ci><ci id="Sx3.p3.6.m1.2.2.1.1.1.3.cmml" xref="Sx3.p3.6.m1.2.2.1.1.1.3">â€²</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.6.m1.2c">k(\mathbf{x},\mathbf{x}^{\prime})</annotation></semantics></math> as the dot product of the respective SOAP power-spectrum vectors, raised to the power of <math id="Sx3.p3.7.m2.1" class="ltx_Math" alttext="\zeta=4" display="inline"><semantics id="Sx3.p3.7.m2.1a"><mrow id="Sx3.p3.7.m2.1.1" xref="Sx3.p3.7.m2.1.1.cmml"><mi id="Sx3.p3.7.m2.1.1.2" xref="Sx3.p3.7.m2.1.1.2.cmml">Î¶</mi><mo id="Sx3.p3.7.m2.1.1.1" xref="Sx3.p3.7.m2.1.1.1.cmml">=</mo><mn id="Sx3.p3.7.m2.1.1.3" xref="Sx3.p3.7.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p3.7.m2.1b"><apply id="Sx3.p3.7.m2.1.1.cmml" xref="Sx3.p3.7.m2.1.1"><eq id="Sx3.p3.7.m2.1.1.1.cmml" xref="Sx3.p3.7.m2.1.1.1"></eq><ci id="Sx3.p3.7.m2.1.1.2.cmml" xref="Sx3.p3.7.m2.1.1.2">ğœ</ci><cn type="integer" id="Sx3.p3.7.m2.1.1.3.cmml" xref="Sx3.p3.7.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p3.7.m2.1c">\zeta=4</annotation></semantics></math> as is common practice.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></p>
</div>
<div id="Sx3.p4" class="ltx_para">
<p id="Sx3.p4.15" class="ltx_p">For an exact implementation of GPR, the time complexity for predicting <math id="Sx3.p4.1.m1.1" class="ltx_Math" alttext="\hat{y}(\mathbf{x}^{\prime})" display="inline"><semantics id="Sx3.p4.1.m1.1a"><mrow id="Sx3.p4.1.m1.1.1" xref="Sx3.p4.1.m1.1.1.cmml"><mover accent="true" id="Sx3.p4.1.m1.1.1.3" xref="Sx3.p4.1.m1.1.1.3.cmml"><mi id="Sx3.p4.1.m1.1.1.3.2" xref="Sx3.p4.1.m1.1.1.3.2.cmml">y</mi><mo id="Sx3.p4.1.m1.1.1.3.1" xref="Sx3.p4.1.m1.1.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="Sx3.p4.1.m1.1.1.2" xref="Sx3.p4.1.m1.1.1.2.cmml">â€‹</mo><mrow id="Sx3.p4.1.m1.1.1.1.1" xref="Sx3.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.p4.1.m1.1.1.1.1.2" xref="Sx3.p4.1.m1.1.1.1.1.1.cmml">(</mo><msup id="Sx3.p4.1.m1.1.1.1.1.1" xref="Sx3.p4.1.m1.1.1.1.1.1.cmml"><mi id="Sx3.p4.1.m1.1.1.1.1.1.2" xref="Sx3.p4.1.m1.1.1.1.1.1.2.cmml">ğ±</mi><mo id="Sx3.p4.1.m1.1.1.1.1.1.3" xref="Sx3.p4.1.m1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="Sx3.p4.1.m1.1.1.1.1.3" xref="Sx3.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.1.m1.1b"><apply id="Sx3.p4.1.m1.1.1.cmml" xref="Sx3.p4.1.m1.1.1"><times id="Sx3.p4.1.m1.1.1.2.cmml" xref="Sx3.p4.1.m1.1.1.2"></times><apply id="Sx3.p4.1.m1.1.1.3.cmml" xref="Sx3.p4.1.m1.1.1.3"><ci id="Sx3.p4.1.m1.1.1.3.1.cmml" xref="Sx3.p4.1.m1.1.1.3.1">^</ci><ci id="Sx3.p4.1.m1.1.1.3.2.cmml" xref="Sx3.p4.1.m1.1.1.3.2">ğ‘¦</ci></apply><apply id="Sx3.p4.1.m1.1.1.1.1.1.cmml" xref="Sx3.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.p4.1.m1.1.1.1.1.1.1.cmml" xref="Sx3.p4.1.m1.1.1.1.1">superscript</csymbol><ci id="Sx3.p4.1.m1.1.1.1.1.1.2.cmml" xref="Sx3.p4.1.m1.1.1.1.1.1.2">ğ±</ci><ci id="Sx3.p4.1.m1.1.1.1.1.1.3.cmml" xref="Sx3.p4.1.m1.1.1.1.1.1.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.1.m1.1c">\hat{y}(\mathbf{x}^{\prime})</annotation></semantics></math> is <math id="Sx3.p4.2.m2.1" class="ltx_Math" alttext="O(N)" display="inline"><semantics id="Sx3.p4.2.m2.1a"><mrow id="Sx3.p4.2.m2.1.2" xref="Sx3.p4.2.m2.1.2.cmml"><mi id="Sx3.p4.2.m2.1.2.2" xref="Sx3.p4.2.m2.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.2.m2.1.2.1" xref="Sx3.p4.2.m2.1.2.1.cmml">â€‹</mo><mrow id="Sx3.p4.2.m2.1.2.3.2" xref="Sx3.p4.2.m2.1.2.cmml"><mo stretchy="false" id="Sx3.p4.2.m2.1.2.3.2.1" xref="Sx3.p4.2.m2.1.2.cmml">(</mo><mi id="Sx3.p4.2.m2.1.1" xref="Sx3.p4.2.m2.1.1.cmml">N</mi><mo stretchy="false" id="Sx3.p4.2.m2.1.2.3.2.2" xref="Sx3.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.2.m2.1b"><apply id="Sx3.p4.2.m2.1.2.cmml" xref="Sx3.p4.2.m2.1.2"><times id="Sx3.p4.2.m2.1.2.1.cmml" xref="Sx3.p4.2.m2.1.2.1"></times><ci id="Sx3.p4.2.m2.1.2.2.cmml" xref="Sx3.p4.2.m2.1.2.2">ğ‘‚</ci><ci id="Sx3.p4.2.m2.1.1.cmml" xref="Sx3.p4.2.m2.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.2.m2.1c">O(N)</annotation></semantics></math>, where <math id="Sx3.p4.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx3.p4.3.m3.1a"><mi id="Sx3.p4.3.m3.1.1" xref="Sx3.p4.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx3.p4.3.m3.1b"><ci id="Sx3.p4.3.m3.1.1.cmml" xref="Sx3.p4.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.3.m3.1c">N</annotation></semantics></math> is the number of training example pairs, <math id="Sx3.p4.4.m4.2" class="ltx_Math" alttext="\{\mathbf{x}_{i},y_{i}\}" display="inline"><semantics id="Sx3.p4.4.m4.2a"><mrow id="Sx3.p4.4.m4.2.2.2" xref="Sx3.p4.4.m4.2.2.3.cmml"><mo stretchy="false" id="Sx3.p4.4.m4.2.2.2.3" xref="Sx3.p4.4.m4.2.2.3.cmml">{</mo><msub id="Sx3.p4.4.m4.1.1.1.1" xref="Sx3.p4.4.m4.1.1.1.1.cmml"><mi id="Sx3.p4.4.m4.1.1.1.1.2" xref="Sx3.p4.4.m4.1.1.1.1.2.cmml">ğ±</mi><mi id="Sx3.p4.4.m4.1.1.1.1.3" xref="Sx3.p4.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="Sx3.p4.4.m4.2.2.2.4" xref="Sx3.p4.4.m4.2.2.3.cmml">,</mo><msub id="Sx3.p4.4.m4.2.2.2.2" xref="Sx3.p4.4.m4.2.2.2.2.cmml"><mi id="Sx3.p4.4.m4.2.2.2.2.2" xref="Sx3.p4.4.m4.2.2.2.2.2.cmml">y</mi><mi id="Sx3.p4.4.m4.2.2.2.2.3" xref="Sx3.p4.4.m4.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="Sx3.p4.4.m4.2.2.2.5" xref="Sx3.p4.4.m4.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.4.m4.2b"><set id="Sx3.p4.4.m4.2.2.3.cmml" xref="Sx3.p4.4.m4.2.2.2"><apply id="Sx3.p4.4.m4.1.1.1.1.cmml" xref="Sx3.p4.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.p4.4.m4.1.1.1.1.1.cmml" xref="Sx3.p4.4.m4.1.1.1.1">subscript</csymbol><ci id="Sx3.p4.4.m4.1.1.1.1.2.cmml" xref="Sx3.p4.4.m4.1.1.1.1.2">ğ±</ci><ci id="Sx3.p4.4.m4.1.1.1.1.3.cmml" xref="Sx3.p4.4.m4.1.1.1.1.3">ğ‘–</ci></apply><apply id="Sx3.p4.4.m4.2.2.2.2.cmml" xref="Sx3.p4.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="Sx3.p4.4.m4.2.2.2.2.1.cmml" xref="Sx3.p4.4.m4.2.2.2.2">subscript</csymbol><ci id="Sx3.p4.4.m4.2.2.2.2.2.cmml" xref="Sx3.p4.4.m4.2.2.2.2.2">ğ‘¦</ci><ci id="Sx3.p4.4.m4.2.2.2.2.3.cmml" xref="Sx3.p4.4.m4.2.2.2.2.3">ğ‘–</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.4.m4.2c">\{\mathbf{x}_{i},y_{i}\}</annotation></semantics></math>. However, solving for <math id="Sx3.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="Sx3.p4.5.m5.1a"><mi id="Sx3.p4.5.m5.1.1" xref="Sx3.p4.5.m5.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="Sx3.p4.5.m5.1b"><ci id="Sx3.p4.5.m5.1.1.cmml" xref="Sx3.p4.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.5.m5.1c">\mathbf{c}</annotation></semantics></math> during training entails an <math id="Sx3.p4.6.m6.1" class="ltx_Math" alttext="O(N^{3})" display="inline"><semantics id="Sx3.p4.6.m6.1a"><mrow id="Sx3.p4.6.m6.1.1" xref="Sx3.p4.6.m6.1.1.cmml"><mi id="Sx3.p4.6.m6.1.1.3" xref="Sx3.p4.6.m6.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.6.m6.1.1.2" xref="Sx3.p4.6.m6.1.1.2.cmml">â€‹</mo><mrow id="Sx3.p4.6.m6.1.1.1.1" xref="Sx3.p4.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.p4.6.m6.1.1.1.1.2" xref="Sx3.p4.6.m6.1.1.1.1.1.cmml">(</mo><msup id="Sx3.p4.6.m6.1.1.1.1.1" xref="Sx3.p4.6.m6.1.1.1.1.1.cmml"><mi id="Sx3.p4.6.m6.1.1.1.1.1.2" xref="Sx3.p4.6.m6.1.1.1.1.1.2.cmml">N</mi><mn id="Sx3.p4.6.m6.1.1.1.1.1.3" xref="Sx3.p4.6.m6.1.1.1.1.1.3.cmml">3</mn></msup><mo stretchy="false" id="Sx3.p4.6.m6.1.1.1.1.3" xref="Sx3.p4.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.6.m6.1b"><apply id="Sx3.p4.6.m6.1.1.cmml" xref="Sx3.p4.6.m6.1.1"><times id="Sx3.p4.6.m6.1.1.2.cmml" xref="Sx3.p4.6.m6.1.1.2"></times><ci id="Sx3.p4.6.m6.1.1.3.cmml" xref="Sx3.p4.6.m6.1.1.3">ğ‘‚</ci><apply id="Sx3.p4.6.m6.1.1.1.1.1.cmml" xref="Sx3.p4.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.p4.6.m6.1.1.1.1.1.1.cmml" xref="Sx3.p4.6.m6.1.1.1.1">superscript</csymbol><ci id="Sx3.p4.6.m6.1.1.1.1.1.2.cmml" xref="Sx3.p4.6.m6.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="Sx3.p4.6.m6.1.1.1.1.1.3.cmml" xref="Sx3.p4.6.m6.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.6.m6.1c">O(N^{3})</annotation></semantics></math> time and <math id="Sx3.p4.7.m7.1" class="ltx_Math" alttext="O(N^{2})" display="inline"><semantics id="Sx3.p4.7.m7.1a"><mrow id="Sx3.p4.7.m7.1.1" xref="Sx3.p4.7.m7.1.1.cmml"><mi id="Sx3.p4.7.m7.1.1.3" xref="Sx3.p4.7.m7.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.7.m7.1.1.2" xref="Sx3.p4.7.m7.1.1.2.cmml">â€‹</mo><mrow id="Sx3.p4.7.m7.1.1.1.1" xref="Sx3.p4.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.p4.7.m7.1.1.1.1.2" xref="Sx3.p4.7.m7.1.1.1.1.1.cmml">(</mo><msup id="Sx3.p4.7.m7.1.1.1.1.1" xref="Sx3.p4.7.m7.1.1.1.1.1.cmml"><mi id="Sx3.p4.7.m7.1.1.1.1.1.2" xref="Sx3.p4.7.m7.1.1.1.1.1.2.cmml">N</mi><mn id="Sx3.p4.7.m7.1.1.1.1.1.3" xref="Sx3.p4.7.m7.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="Sx3.p4.7.m7.1.1.1.1.3" xref="Sx3.p4.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.7.m7.1b"><apply id="Sx3.p4.7.m7.1.1.cmml" xref="Sx3.p4.7.m7.1.1"><times id="Sx3.p4.7.m7.1.1.2.cmml" xref="Sx3.p4.7.m7.1.1.2"></times><ci id="Sx3.p4.7.m7.1.1.3.cmml" xref="Sx3.p4.7.m7.1.1.3">ğ‘‚</ci><apply id="Sx3.p4.7.m7.1.1.1.1.1.cmml" xref="Sx3.p4.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="Sx3.p4.7.m7.1.1.1.1.1.1.cmml" xref="Sx3.p4.7.m7.1.1.1.1">superscript</csymbol><ci id="Sx3.p4.7.m7.1.1.1.1.1.2.cmml" xref="Sx3.p4.7.m7.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="Sx3.p4.7.m7.1.1.1.1.1.3.cmml" xref="Sx3.p4.7.m7.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.7.m7.1c">O(N^{2})</annotation></semantics></math> storage cost. In practical terms, this limits â€œfull GPRâ€ to at most a few thousand data points.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
One approach to circumventing this unfavourable scaling is referred to as sparse GPR,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
which only considers <math id="Sx3.p4.8.m8.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx3.p4.8.m8.1a"><mi id="Sx3.p4.8.m8.1.1" xref="Sx3.p4.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx3.p4.8.m8.1b"><ci id="Sx3.p4.8.m8.1.1.cmml" xref="Sx3.p4.8.m8.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.8.m8.1c">M</annotation></semantics></math> representative data locations when making predictions.
Prediction time complexity is therefore <math id="Sx3.p4.9.m9.1" class="ltx_Math" alttext="O(M)" display="inline"><semantics id="Sx3.p4.9.m9.1a"><mrow id="Sx3.p4.9.m9.1.2" xref="Sx3.p4.9.m9.1.2.cmml"><mi id="Sx3.p4.9.m9.1.2.2" xref="Sx3.p4.9.m9.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.9.m9.1.2.1" xref="Sx3.p4.9.m9.1.2.1.cmml">â€‹</mo><mrow id="Sx3.p4.9.m9.1.2.3.2" xref="Sx3.p4.9.m9.1.2.cmml"><mo stretchy="false" id="Sx3.p4.9.m9.1.2.3.2.1" xref="Sx3.p4.9.m9.1.2.cmml">(</mo><mi id="Sx3.p4.9.m9.1.1" xref="Sx3.p4.9.m9.1.1.cmml">M</mi><mo stretchy="false" id="Sx3.p4.9.m9.1.2.3.2.2" xref="Sx3.p4.9.m9.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.9.m9.1b"><apply id="Sx3.p4.9.m9.1.2.cmml" xref="Sx3.p4.9.m9.1.2"><times id="Sx3.p4.9.m9.1.2.1.cmml" xref="Sx3.p4.9.m9.1.2.1"></times><ci id="Sx3.p4.9.m9.1.2.2.cmml" xref="Sx3.p4.9.m9.1.2.2">ğ‘‚</ci><ci id="Sx3.p4.9.m9.1.1.cmml" xref="Sx3.p4.9.m9.1.1">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.9.m9.1c">O(M)</annotation></semantics></math>, while training entails <math id="Sx3.p4.10.m10.1" class="ltx_Math" alttext="O(M^{2}N+M^{3})" display="inline"><semantics id="Sx3.p4.10.m10.1a"><mrow id="Sx3.p4.10.m10.1.1" xref="Sx3.p4.10.m10.1.1.cmml"><mi id="Sx3.p4.10.m10.1.1.3" xref="Sx3.p4.10.m10.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.10.m10.1.1.2" xref="Sx3.p4.10.m10.1.1.2.cmml">â€‹</mo><mrow id="Sx3.p4.10.m10.1.1.1.1" xref="Sx3.p4.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.p4.10.m10.1.1.1.1.2" xref="Sx3.p4.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.p4.10.m10.1.1.1.1.1" xref="Sx3.p4.10.m10.1.1.1.1.1.cmml"><mrow id="Sx3.p4.10.m10.1.1.1.1.1.2" xref="Sx3.p4.10.m10.1.1.1.1.1.2.cmml"><msup id="Sx3.p4.10.m10.1.1.1.1.1.2.2" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2.cmml"><mi id="Sx3.p4.10.m10.1.1.1.1.1.2.2.2" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2.2.cmml">M</mi><mn id="Sx3.p4.10.m10.1.1.1.1.1.2.2.3" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="Sx3.p4.10.m10.1.1.1.1.1.2.1" xref="Sx3.p4.10.m10.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.p4.10.m10.1.1.1.1.1.2.3" xref="Sx3.p4.10.m10.1.1.1.1.1.2.3.cmml">N</mi></mrow><mo id="Sx3.p4.10.m10.1.1.1.1.1.1" xref="Sx3.p4.10.m10.1.1.1.1.1.1.cmml">+</mo><msup id="Sx3.p4.10.m10.1.1.1.1.1.3" xref="Sx3.p4.10.m10.1.1.1.1.1.3.cmml"><mi id="Sx3.p4.10.m10.1.1.1.1.1.3.2" xref="Sx3.p4.10.m10.1.1.1.1.1.3.2.cmml">M</mi><mn id="Sx3.p4.10.m10.1.1.1.1.1.3.3" xref="Sx3.p4.10.m10.1.1.1.1.1.3.3.cmml">3</mn></msup></mrow><mo stretchy="false" id="Sx3.p4.10.m10.1.1.1.1.3" xref="Sx3.p4.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.10.m10.1b"><apply id="Sx3.p4.10.m10.1.1.cmml" xref="Sx3.p4.10.m10.1.1"><times id="Sx3.p4.10.m10.1.1.2.cmml" xref="Sx3.p4.10.m10.1.1.2"></times><ci id="Sx3.p4.10.m10.1.1.3.cmml" xref="Sx3.p4.10.m10.1.1.3">ğ‘‚</ci><apply id="Sx3.p4.10.m10.1.1.1.1.1.cmml" xref="Sx3.p4.10.m10.1.1.1.1"><plus id="Sx3.p4.10.m10.1.1.1.1.1.1.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.1"></plus><apply id="Sx3.p4.10.m10.1.1.1.1.1.2.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2"><times id="Sx3.p4.10.m10.1.1.1.1.1.2.1.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.1"></times><apply id="Sx3.p4.10.m10.1.1.1.1.1.2.2.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Sx3.p4.10.m10.1.1.1.1.1.2.2.1.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2">superscript</csymbol><ci id="Sx3.p4.10.m10.1.1.1.1.1.2.2.2.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2.2">ğ‘€</ci><cn type="integer" id="Sx3.p4.10.m10.1.1.1.1.1.2.2.3.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.2.3">2</cn></apply><ci id="Sx3.p4.10.m10.1.1.1.1.1.2.3.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.2.3">ğ‘</ci></apply><apply id="Sx3.p4.10.m10.1.1.1.1.1.3.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.p4.10.m10.1.1.1.1.1.3.1.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.3">superscript</csymbol><ci id="Sx3.p4.10.m10.1.1.1.1.1.3.2.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.3.2">ğ‘€</ci><cn type="integer" id="Sx3.p4.10.m10.1.1.1.1.1.3.3.cmml" xref="Sx3.p4.10.m10.1.1.1.1.1.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.10.m10.1c">O(M^{2}N+M^{3})</annotation></semantics></math> time and <math id="Sx3.p4.11.m11.1" class="ltx_Math" alttext="O(NM+M^{2})" display="inline"><semantics id="Sx3.p4.11.m11.1a"><mrow id="Sx3.p4.11.m11.1.1" xref="Sx3.p4.11.m11.1.1.cmml"><mi id="Sx3.p4.11.m11.1.1.3" xref="Sx3.p4.11.m11.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.11.m11.1.1.2" xref="Sx3.p4.11.m11.1.1.2.cmml">â€‹</mo><mrow id="Sx3.p4.11.m11.1.1.1.1" xref="Sx3.p4.11.m11.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.p4.11.m11.1.1.1.1.2" xref="Sx3.p4.11.m11.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.p4.11.m11.1.1.1.1.1" xref="Sx3.p4.11.m11.1.1.1.1.1.cmml"><mrow id="Sx3.p4.11.m11.1.1.1.1.1.2" xref="Sx3.p4.11.m11.1.1.1.1.1.2.cmml"><mi id="Sx3.p4.11.m11.1.1.1.1.1.2.2" xref="Sx3.p4.11.m11.1.1.1.1.1.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="Sx3.p4.11.m11.1.1.1.1.1.2.1" xref="Sx3.p4.11.m11.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.p4.11.m11.1.1.1.1.1.2.3" xref="Sx3.p4.11.m11.1.1.1.1.1.2.3.cmml">M</mi></mrow><mo id="Sx3.p4.11.m11.1.1.1.1.1.1" xref="Sx3.p4.11.m11.1.1.1.1.1.1.cmml">+</mo><msup id="Sx3.p4.11.m11.1.1.1.1.1.3" xref="Sx3.p4.11.m11.1.1.1.1.1.3.cmml"><mi id="Sx3.p4.11.m11.1.1.1.1.1.3.2" xref="Sx3.p4.11.m11.1.1.1.1.1.3.2.cmml">M</mi><mn id="Sx3.p4.11.m11.1.1.1.1.1.3.3" xref="Sx3.p4.11.m11.1.1.1.1.1.3.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="Sx3.p4.11.m11.1.1.1.1.3" xref="Sx3.p4.11.m11.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.11.m11.1b"><apply id="Sx3.p4.11.m11.1.1.cmml" xref="Sx3.p4.11.m11.1.1"><times id="Sx3.p4.11.m11.1.1.2.cmml" xref="Sx3.p4.11.m11.1.1.2"></times><ci id="Sx3.p4.11.m11.1.1.3.cmml" xref="Sx3.p4.11.m11.1.1.3">ğ‘‚</ci><apply id="Sx3.p4.11.m11.1.1.1.1.1.cmml" xref="Sx3.p4.11.m11.1.1.1.1"><plus id="Sx3.p4.11.m11.1.1.1.1.1.1.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.1"></plus><apply id="Sx3.p4.11.m11.1.1.1.1.1.2.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.2"><times id="Sx3.p4.11.m11.1.1.1.1.1.2.1.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.2.1"></times><ci id="Sx3.p4.11.m11.1.1.1.1.1.2.2.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.2.2">ğ‘</ci><ci id="Sx3.p4.11.m11.1.1.1.1.1.2.3.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.2.3">ğ‘€</ci></apply><apply id="Sx3.p4.11.m11.1.1.1.1.1.3.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx3.p4.11.m11.1.1.1.1.1.3.1.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.3">superscript</csymbol><ci id="Sx3.p4.11.m11.1.1.1.1.1.3.2.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.3.2">ğ‘€</ci><cn type="integer" id="Sx3.p4.11.m11.1.1.1.1.1.3.3.cmml" xref="Sx3.p4.11.m11.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.11.m11.1c">O(NM+M^{2})</annotation></semantics></math> space scaling. Provided <math id="Sx3.p4.12.m12.1" class="ltx_Math" alttext="M\ll N" display="inline"><semantics id="Sx3.p4.12.m12.1a"><mrow id="Sx3.p4.12.m12.1.1" xref="Sx3.p4.12.m12.1.1.cmml"><mi id="Sx3.p4.12.m12.1.1.2" xref="Sx3.p4.12.m12.1.1.2.cmml">M</mi><mo id="Sx3.p4.12.m12.1.1.1" xref="Sx3.p4.12.m12.1.1.1.cmml">â‰ª</mo><mi id="Sx3.p4.12.m12.1.1.3" xref="Sx3.p4.12.m12.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.12.m12.1b"><apply id="Sx3.p4.12.m12.1.1.cmml" xref="Sx3.p4.12.m12.1.1"><csymbol cd="latexml" id="Sx3.p4.12.m12.1.1.1.cmml" xref="Sx3.p4.12.m12.1.1.1">much-less-than</csymbol><ci id="Sx3.p4.12.m12.1.1.2.cmml" xref="Sx3.p4.12.m12.1.1.2">ğ‘€</ci><ci id="Sx3.p4.12.m12.1.1.3.cmml" xref="Sx3.p4.12.m12.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.12.m12.1c">M\ll N</annotation></semantics></math>, this can significantly increase the amount of data that can be used for training in practice.
In the present work, we used <math id="Sx3.p4.13.m13.1" class="ltx_Math" alttext="M=5000" display="inline"><semantics id="Sx3.p4.13.m13.1a"><mrow id="Sx3.p4.13.m13.1.1" xref="Sx3.p4.13.m13.1.1.cmml"><mi id="Sx3.p4.13.m13.1.1.2" xref="Sx3.p4.13.m13.1.1.2.cmml">M</mi><mo id="Sx3.p4.13.m13.1.1.1" xref="Sx3.p4.13.m13.1.1.1.cmml">=</mo><mn id="Sx3.p4.13.m13.1.1.3" xref="Sx3.p4.13.m13.1.1.3.cmml">5000</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p4.13.m13.1b"><apply id="Sx3.p4.13.m13.1.1.cmml" xref="Sx3.p4.13.m13.1.1"><eq id="Sx3.p4.13.m13.1.1.1.cmml" xref="Sx3.p4.13.m13.1.1.1"></eq><ci id="Sx3.p4.13.m13.1.1.2.cmml" xref="Sx3.p4.13.m13.1.1.2">ğ‘€</ci><cn type="integer" id="Sx3.p4.13.m13.1.1.3.cmml" xref="Sx3.p4.13.m13.1.1.3">5000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.13.m13.1c">M=5000</annotation></semantics></math>, and varied <math id="Sx3.p4.14.m14.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx3.p4.14.m14.1a"><mi id="Sx3.p4.14.m14.1.1" xref="Sx3.p4.14.m14.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx3.p4.14.m14.1b"><ci id="Sx3.p4.14.m14.1.1.cmml" xref="Sx3.p4.14.m14.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.14.m14.1c">N</annotation></semantics></math> up to <math id="Sx3.p4.15.m15.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="Sx3.p4.15.m15.1a"><msup id="Sx3.p4.15.m15.1.1" xref="Sx3.p4.15.m15.1.1.cmml"><mn id="Sx3.p4.15.m15.1.1.2" xref="Sx3.p4.15.m15.1.1.2.cmml">10</mn><mn id="Sx3.p4.15.m15.1.1.3" xref="Sx3.p4.15.m15.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.p4.15.m15.1b"><apply id="Sx3.p4.15.m15.1.1.cmml" xref="Sx3.p4.15.m15.1.1"><csymbol cd="ambiguous" id="Sx3.p4.15.m15.1.1.1.cmml" xref="Sx3.p4.15.m15.1.1">superscript</csymbol><cn type="integer" id="Sx3.p4.15.m15.1.1.2.cmml" xref="Sx3.p4.15.m15.1.1.2">10</cn><cn type="integer" id="Sx3.p4.15.m15.1.1.3.cmml" xref="Sx3.p4.15.m15.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p4.15.m15.1c">10^{6}</annotation></semantics></math>.</p>
</div>
<div id="Sx3.p5" class="ltx_para">
<p id="Sx3.p5.1" class="ltx_p"><span id="Sx3.p5.1.1" class="ltx_text ltx_font_bold">Neural-network (NN) models.</span>
Artificial NNs can provably represent any function given sufficient parameterisation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
For an overview of the inspiration for, workings of, and theory behind NNs, we refer to Ref.Â <cite class="ltx_cite ltx_citemacro_citenum"><a href="#bib.bib50" title="" class="ltx_ref">50</a></cite>. In brief, NNs make predictions by repeatedly applying alternating linear and non-linear transforms, parameterised by weights and biases. These are learned using backpropagation to iteratively reduce a loss function.</p>
</div>
<div id="Sx3.p6" class="ltx_para">
<p id="Sx3.p6.1" class="ltx_p">Throughout this work, we train NNs using standard forward and backward propagation techniques using the Adam optimiser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and CELU activation functions, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> all as implemented in PyTorch.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
The performance of a deep NN depends heavily on the choice of hyperparameters for the model architecture and training, including the depth and width of the network, and the learning rate of the optimiser.
We establish optimised values for these hyperparameters using an automated process: random sweep over values, and validating on a test set (see below).</p>
</div>
<div id="Sx3.p7" class="ltx_para">
<p id="Sx3.p7.1" class="ltx_p"><span id="Sx3.p7.1.1" class="ltx_text ltx_font_bold">Deep kernel learning (DKL).</span>
DKL models make predictions through the sequential application of deep NN and GPR models:<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> the NN takes high-dimensional data as input and outputs a compressed representation in a space where the Euclidean distance between two data points, relative to the learned length scale of the GPR model, is representative of their (dis-) similarity.</p>
</div>
<div id="Sx3.p8" class="ltx_para">
<p id="Sx3.p8.1" class="ltx_p">During training, the parameters of both the NN and GPR model are jointly optimised by maximising the log posterior marginal likelihood. These models were implemented using the PyTorch and GPyTorch libraries.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite></p>
</div>
<figure id="Sx3.F3" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_03.png" id="Sx3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="423" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Learning curves for our synthetic dataset.
Mean absolute error (MAE) values for the prediction of C-GAP-17 labelled local energies, as a function of training set size (â€œlearning curvesâ€), for the most accurate instance of each model class.
The dashed line indicates GPR models that required specialist compute (<math id="Sx3.F3.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="Sx3.F3.2.m1.1b"><mo id="Sx3.F3.2.m1.1.1" xref="Sx3.F3.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Sx3.F3.2.m1.1c"><gt id="Sx3.F3.2.m1.1.1.cmml" xref="Sx3.F3.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Sx3.F3.2.m1.1d">&gt;</annotation></semantics></math>64GB RAM) to train.
</figcaption>
</figure>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Learning curves</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">The first result of this paper is the demonstration that our synthetic data, <span id="Sx4.p1.1.1" class="ltx_text ltx_font_italic">viz.</span> ML atomic energies, can indeed be machine-learned, and how the quality of this learning depends on the size of the training dataset.
In Fig.Â <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we test the ability of our GPR, NN, and DKL models to learn atomic energies from the dataset of carbon structures described above.
We show â€œlearning curvesâ€ that allow us to quantify and compare the errors of the three model classes considered.
In each case, mean absolute error (MAE) values are quoted as averaged using a 5-fold cross-validation procedure, where the structures from a single MD trajectory are dedicated completely to either the training or the test set, to avoid training example data leakage. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
We note that this learning of ML-predicted data is related to the recently proposed â€œindirect learningâ€ approach, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> but it is distinctly different in that the latter does not regress per-atom energies, rather aiming to create teacherâ€“student ML potential models.</p>
</div>
<div id="Sx4.p2" class="ltx_para">
<p id="Sx4.p2.4" class="ltx_p">In the low-data regime, the learning curves in Fig.Â <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show the behaviour known for other atomistic ML models: the error decreases linearly on the double-logarithmic plot. There is a clear advantage of the GPR models (blue) over either network-based technique (NNs, black; DKL, red) in this regime, with <math id="Sx4.p2.1.m1.1" class="ltx_Math" alttext="10^{4}" display="inline"><semantics id="Sx4.p2.1.m1.1a"><msup id="Sx4.p2.1.m1.1.1" xref="Sx4.p2.1.m1.1.1.cmml"><mn id="Sx4.p2.1.m1.1.1.2" xref="Sx4.p2.1.m1.1.1.2.cmml">10</mn><mn id="Sx4.p2.1.m1.1.1.3" xref="Sx4.p2.1.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="Sx4.p2.1.m1.1b"><apply id="Sx4.p2.1.m1.1.1.cmml" xref="Sx4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.p2.1.m1.1.1.1.cmml" xref="Sx4.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="Sx4.p2.1.m1.1.1.2.cmml" xref="Sx4.p2.1.m1.1.1.2">10</cn><cn type="integer" id="Sx4.p2.1.m1.1.1.3.cmml" xref="Sx4.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p2.1.m1.1c">10^{4}</annotation></semantics></math> data points perhaps being representative of a specialised learning problem in quantum chemistry requiring expensive data labels. However, this effect is diminished upon moving to larger datasets. Typical ML potentials use on the order of <math id="Sx4.p2.2.m2.1" class="ltx_Math" alttext="10^{5}" display="inline"><semantics id="Sx4.p2.2.m2.1a"><msup id="Sx4.p2.2.m2.1.1" xref="Sx4.p2.2.m2.1.1.cmml"><mn id="Sx4.p2.2.m2.1.1.2" xref="Sx4.p2.2.m2.1.1.2.cmml">10</mn><mn id="Sx4.p2.2.m2.1.1.3" xref="Sx4.p2.2.m2.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="Sx4.p2.2.m2.1b"><apply id="Sx4.p2.2.m2.1.1.cmml" xref="Sx4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Sx4.p2.2.m2.1.1.1.cmml" xref="Sx4.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="Sx4.p2.2.m2.1.1.2.cmml" xref="Sx4.p2.2.m2.1.1.2">10</cn><cn type="integer" id="Sx4.p2.2.m2.1.1.3.cmml" xref="Sx4.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p2.2.m2.1c">10^{5}</annotation></semantics></math> data points for training, and in this region the learning curve for the GPR models visibly saturates. We emphasise that we use sparse GPR, and so the actual number of points in the regression, <math id="Sx4.p2.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx4.p2.3.m3.1a"><mi id="Sx4.p2.3.m3.1.1" xref="Sx4.p2.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx4.p2.3.m3.1b"><ci id="Sx4.p2.3.m3.1.1.cmml" xref="Sx4.p2.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p2.3.m3.1c">M</annotation></semantics></math>, is much lower than <math id="Sx4.p2.4.m4.1" class="ltx_Math" alttext="10^{5}" display="inline"><semantics id="Sx4.p2.4.m4.1a"><msup id="Sx4.p2.4.m4.1.1" xref="Sx4.p2.4.m4.1.1.cmml"><mn id="Sx4.p2.4.m4.1.1.2" xref="Sx4.p2.4.m4.1.1.2.cmml">10</mn><mn id="Sx4.p2.4.m4.1.1.3" xref="Sx4.p2.4.m4.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="Sx4.p2.4.m4.1b"><apply id="Sx4.p2.4.m4.1.1.cmml" xref="Sx4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="Sx4.p2.4.m4.1.1.1.cmml" xref="Sx4.p2.4.m4.1.1">superscript</csymbol><cn type="integer" id="Sx4.p2.4.m4.1.1.2.cmml" xref="Sx4.p2.4.m4.1.1.2">10</cn><cn type="integer" id="Sx4.p2.4.m4.1.1.3.cmml" xref="Sx4.p2.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p2.4.m4.1c">10^{5}</annotation></semantics></math>; this aspect will be discussed in the following section.</p>
</div>
<div id="Sx4.p3" class="ltx_para">
<p id="Sx4.p3.1" class="ltx_p">Comparing the NN and DKL models side-by-side, we find no notable advantage of DKL over regular NNs in this context â€“ a slight gain in accuracy comes at a cost of approximately 100-fold slower prediction. In the remainder of this paper, we will therefore focus on a deeper analysis of GPR and NN models for atomistic ML, and report on numerical experiments with these two model classes.</p>
</div>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">GPR insights</h3>

<figure id="Sx5.F4" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_04.png" id="Sx5.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="809" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Aspects of GPR models and their effect on prediction quality.
(a) MAE for a series of GPR models with varied numbers of representative points, <math id="Sx5.F4.7.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.F4.7.m1.1b"><mi id="Sx5.F4.7.m1.1.1" xref="Sx5.F4.7.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.F4.7.m1.1c"><ci id="Sx5.F4.7.m1.1.1.cmml" xref="Sx5.F4.7.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.7.m1.1d">M</annotation></semantics></math>, and training points, <math id="Sx5.F4.8.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx5.F4.8.m2.1b"><mi id="Sx5.F4.8.m2.1.1" xref="Sx5.F4.8.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx5.F4.8.m2.1c"><ci id="Sx5.F4.8.m2.1.1.cmml" xref="Sx5.F4.8.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.8.m2.1d">N</annotation></semantics></math>. We used <math id="Sx5.F4.9.m3.1" class="ltx_Math" alttext="M=" display="inline"><semantics id="Sx5.F4.9.m3.1b"><mrow id="Sx5.F4.9.m3.1.1" xref="Sx5.F4.9.m3.1.1.cmml"><mi id="Sx5.F4.9.m3.1.1.2" xref="Sx5.F4.9.m3.1.1.2.cmml">M</mi><mo id="Sx5.F4.9.m3.1.1.1" xref="Sx5.F4.9.m3.1.1.1.cmml">=</mo><mi id="Sx5.F4.9.m3.1.1.3" xref="Sx5.F4.9.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.F4.9.m3.1c"><apply id="Sx5.F4.9.m3.1.1.cmml" xref="Sx5.F4.9.m3.1.1"><eq id="Sx5.F4.9.m3.1.1.1.cmml" xref="Sx5.F4.9.m3.1.1.1"></eq><ci id="Sx5.F4.9.m3.1.1.2.cmml" xref="Sx5.F4.9.m3.1.1.2">ğ‘€</ci><csymbol cd="latexml" id="Sx5.F4.9.m3.1.1.3.cmml" xref="Sx5.F4.9.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.9.m3.1d">M=</annotation></semantics></math> 5000 in the present work, limited by memory availability for <math id="Sx5.F4.10.m4.1" class="ltx_Math" alttext="N=10^{6}" display="inline"><semantics id="Sx5.F4.10.m4.1b"><mrow id="Sx5.F4.10.m4.1.1" xref="Sx5.F4.10.m4.1.1.cmml"><mi id="Sx5.F4.10.m4.1.1.2" xref="Sx5.F4.10.m4.1.1.2.cmml">N</mi><mo id="Sx5.F4.10.m4.1.1.1" xref="Sx5.F4.10.m4.1.1.1.cmml">=</mo><msup id="Sx5.F4.10.m4.1.1.3" xref="Sx5.F4.10.m4.1.1.3.cmml"><mn id="Sx5.F4.10.m4.1.1.3.2" xref="Sx5.F4.10.m4.1.1.3.2.cmml">10</mn><mn id="Sx5.F4.10.m4.1.1.3.3" xref="Sx5.F4.10.m4.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx5.F4.10.m4.1c"><apply id="Sx5.F4.10.m4.1.1.cmml" xref="Sx5.F4.10.m4.1.1"><eq id="Sx5.F4.10.m4.1.1.1.cmml" xref="Sx5.F4.10.m4.1.1.1"></eq><ci id="Sx5.F4.10.m4.1.1.2.cmml" xref="Sx5.F4.10.m4.1.1.2">ğ‘</ci><apply id="Sx5.F4.10.m4.1.1.3.cmml" xref="Sx5.F4.10.m4.1.1.3"><csymbol cd="ambiguous" id="Sx5.F4.10.m4.1.1.3.1.cmml" xref="Sx5.F4.10.m4.1.1.3">superscript</csymbol><cn type="integer" id="Sx5.F4.10.m4.1.1.3.2.cmml" xref="Sx5.F4.10.m4.1.1.3.2">10</cn><cn type="integer" id="Sx5.F4.10.m4.1.1.3.3.cmml" xref="Sx5.F4.10.m4.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.10.m4.1d">N=10^{6}</annotation></semantics></math>. We include results for <math id="Sx5.F4.11.m5.1" class="ltx_Math" alttext="M=" display="inline"><semantics id="Sx5.F4.11.m5.1b"><mrow id="Sx5.F4.11.m5.1.1" xref="Sx5.F4.11.m5.1.1.cmml"><mi id="Sx5.F4.11.m5.1.1.2" xref="Sx5.F4.11.m5.1.1.2.cmml">M</mi><mo id="Sx5.F4.11.m5.1.1.1" xref="Sx5.F4.11.m5.1.1.1.cmml">=</mo><mi id="Sx5.F4.11.m5.1.1.3" xref="Sx5.F4.11.m5.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.F4.11.m5.1c"><apply id="Sx5.F4.11.m5.1.1.cmml" xref="Sx5.F4.11.m5.1.1"><eq id="Sx5.F4.11.m5.1.1.1.cmml" xref="Sx5.F4.11.m5.1.1.1"></eq><ci id="Sx5.F4.11.m5.1.1.2.cmml" xref="Sx5.F4.11.m5.1.1.2">ğ‘€</ci><csymbol cd="latexml" id="Sx5.F4.11.m5.1.1.3.cmml" xref="Sx5.F4.11.m5.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.11.m5.1d">M=</annotation></semantics></math> 10,000 as far as practicable, and find that those do not lead to substantial improvements in the region tested.
(b) MAE for a series of GPR models with varied regularisation terms. Too low values will cause the model to overfit to data, whereas too high values (too high â€œexpected errorâ€) will diminish the quality of the prediction. The minimum value is found around 10 meV for most values of <math id="Sx5.F4.12.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx5.F4.12.m6.1b"><mi id="Sx5.F4.12.m6.1.1" xref="Sx5.F4.12.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx5.F4.12.m6.1c"><ci id="Sx5.F4.12.m6.1.1.cmml" xref="Sx5.F4.12.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F4.12.m6.1d">N</annotation></semantics></math>, and this setting was used for all other GPR results shown in this work.
</figcaption>
</figure>
<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">Having identified synthetic atomic energies as a â€œmachine-learnableâ€ and readily available target quantity (Fig.Â <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), we can use these synthetic data to gain further insight into GPR models.
There are two important considerations when constructing sparse GPR models that we address here.</p>
</div>
<div id="Sx5.SSx1.p2" class="ltx_para">
<p id="Sx5.SSx1.p2.2" class="ltx_p">The first aspect is the choice of the number of representative points, <math id="Sx5.SSx1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.SSx1.p2.1.m1.1a"><mi id="Sx5.SSx1.p2.1.m1.1.1" xref="Sx5.SSx1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.1.m1.1b"><ci id="Sx5.SSx1.p2.1.m1.1.1.cmml" xref="Sx5.SSx1.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.1.m1.1c">M</annotation></semantics></math>, that are used in the sparse GPR fit.
In a full GPR setting, the fitting coefficients, <math id="Sx5.SSx1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="Sx5.SSx1.p2.2.m2.1a"><mi id="Sx5.SSx1.p2.2.m2.1.1" xref="Sx5.SSx1.p2.2.m2.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.2.m2.1b"><ci id="Sx5.SSx1.p2.2.m2.1.1.cmml" xref="Sx5.SSx1.p2.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.2.m2.1c">\mathbf{c}</annotation></semantics></math>, would be obtained at training time as</p>
<table id="Sx5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E2.m1.1" class="ltx_Math" alttext="\mathbf{c}=(\mathbf{K}_{NN}+\mathbf{\Sigma})^{-1}\mathbf{y}," display="block"><semantics id="Sx5.E2.m1.1a"><mrow id="Sx5.E2.m1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.cmml"><mrow id="Sx5.E2.m1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.cmml"><mi id="Sx5.E2.m1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.3.cmml">ğœ</mi><mo id="Sx5.E2.m1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx5.E2.m1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.cmml"><msup id="Sx5.E2.m1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.cmml"><mrow id="Sx5.E2.m1.1.1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.E2.m1.1.1.1.1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğŠ</mi><mrow id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.1" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">N</mi></mrow></msub><mo id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">ğšº</mi></mrow><mo stretchy="false" id="Sx5.E2.m1.1.1.1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="Sx5.E2.m1.1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.1.3.cmml"><mo id="Sx5.E2.m1.1.1.1.1.1.1.3a" xref="Sx5.E2.m1.1.1.1.1.1.1.3.cmml">âˆ’</mo><mn id="Sx5.E2.m1.1.1.1.1.1.1.3.2" xref="Sx5.E2.m1.1.1.1.1.1.1.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="Sx5.E2.m1.1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="Sx5.E2.m1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.3.cmml">ğ²</mi></mrow></mrow><mo id="Sx5.E2.m1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E2.m1.1b"><apply id="Sx5.E2.m1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1"><eq id="Sx5.E2.m1.1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1.1.2"></eq><ci id="Sx5.E2.m1.1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.1.3">ğœ</ci><apply id="Sx5.E2.m1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1"><times id="Sx5.E2.m1.1.1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.2"></times><apply id="Sx5.E2.m1.1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1">superscript</csymbol><apply id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1"><plus id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.2">ğŠ</ci><apply id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3"><times id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘</ci><ci id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1.1.1.3">ğšº</ci></apply><apply id="Sx5.E2.m1.1.1.1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3"><minus id="Sx5.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3"></minus><cn type="integer" id="Sx5.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3.2">1</cn></apply></apply><ci id="Sx5.E2.m1.1.1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.3">ğ²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E2.m1.1c">\mathbf{c}=(\mathbf{K}_{NN}+\mathbf{\Sigma})^{-1}\mathbf{y},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx1.p2.6" class="ltx_p">where <math id="Sx5.SSx1.p2.3.m1.1" class="ltx_Math" alttext="\mathbf{K}_{NN}" display="inline"><semantics id="Sx5.SSx1.p2.3.m1.1a"><msub id="Sx5.SSx1.p2.3.m1.1.1" xref="Sx5.SSx1.p2.3.m1.1.1.cmml"><mi id="Sx5.SSx1.p2.3.m1.1.1.2" xref="Sx5.SSx1.p2.3.m1.1.1.2.cmml">ğŠ</mi><mrow id="Sx5.SSx1.p2.3.m1.1.1.3" xref="Sx5.SSx1.p2.3.m1.1.1.3.cmml"><mi id="Sx5.SSx1.p2.3.m1.1.1.3.2" xref="Sx5.SSx1.p2.3.m1.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="Sx5.SSx1.p2.3.m1.1.1.3.1" xref="Sx5.SSx1.p2.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx5.SSx1.p2.3.m1.1.1.3.3" xref="Sx5.SSx1.p2.3.m1.1.1.3.3.cmml">N</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.3.m1.1b"><apply id="Sx5.SSx1.p2.3.m1.1.1.cmml" xref="Sx5.SSx1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="Sx5.SSx1.p2.3.m1.1.1.1.cmml" xref="Sx5.SSx1.p2.3.m1.1.1">subscript</csymbol><ci id="Sx5.SSx1.p2.3.m1.1.1.2.cmml" xref="Sx5.SSx1.p2.3.m1.1.1.2">ğŠ</ci><apply id="Sx5.SSx1.p2.3.m1.1.1.3.cmml" xref="Sx5.SSx1.p2.3.m1.1.1.3"><times id="Sx5.SSx1.p2.3.m1.1.1.3.1.cmml" xref="Sx5.SSx1.p2.3.m1.1.1.3.1"></times><ci id="Sx5.SSx1.p2.3.m1.1.1.3.2.cmml" xref="Sx5.SSx1.p2.3.m1.1.1.3.2">ğ‘</ci><ci id="Sx5.SSx1.p2.3.m1.1.1.3.3.cmml" xref="Sx5.SSx1.p2.3.m1.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.3.m1.1c">\mathbf{K}_{NN}</annotation></semantics></math> is a matrix of kernel similarity values between any two data locations, <span id="Sx5.SSx1.p2.6.1" class="ltx_text ltx_font_italic">viz.</span> <math id="Sx5.SSx1.p2.4.m2.5" class="ltx_Math" alttext="(K_{NN})_{i,j}=k(\mathbf{x}_{i},\mathbf{x}_{j})" display="inline"><semantics id="Sx5.SSx1.p2.4.m2.5a"><mrow id="Sx5.SSx1.p2.4.m2.5.5" xref="Sx5.SSx1.p2.4.m2.5.5.cmml"><msub id="Sx5.SSx1.p2.4.m2.3.3.1" xref="Sx5.SSx1.p2.4.m2.3.3.1.cmml"><mrow id="Sx5.SSx1.p2.4.m2.3.3.1.1.1" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.2" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.cmml">(</mo><msub id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.cmml"><mi id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.2" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.2.cmml">K</mi><mrow id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.cmml"><mi id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.2" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.1" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.1.cmml">â€‹</mo><mi id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.3" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.3.cmml">N</mi></mrow></msub><mo stretchy="false" id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.3" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.cmml">)</mo></mrow><mrow id="Sx5.SSx1.p2.4.m2.2.2.2.4" xref="Sx5.SSx1.p2.4.m2.2.2.2.3.cmml"><mi id="Sx5.SSx1.p2.4.m2.1.1.1.1" xref="Sx5.SSx1.p2.4.m2.1.1.1.1.cmml">i</mi><mo id="Sx5.SSx1.p2.4.m2.2.2.2.4.1" xref="Sx5.SSx1.p2.4.m2.2.2.2.3.cmml">,</mo><mi id="Sx5.SSx1.p2.4.m2.2.2.2.2" xref="Sx5.SSx1.p2.4.m2.2.2.2.2.cmml">j</mi></mrow></msub><mo id="Sx5.SSx1.p2.4.m2.5.5.4" xref="Sx5.SSx1.p2.4.m2.5.5.4.cmml">=</mo><mrow id="Sx5.SSx1.p2.4.m2.5.5.3" xref="Sx5.SSx1.p2.4.m2.5.5.3.cmml"><mi id="Sx5.SSx1.p2.4.m2.5.5.3.4" xref="Sx5.SSx1.p2.4.m2.5.5.3.4.cmml">k</mi><mo lspace="0em" rspace="0em" id="Sx5.SSx1.p2.4.m2.5.5.3.3" xref="Sx5.SSx1.p2.4.m2.5.5.3.3.cmml">â€‹</mo><mrow id="Sx5.SSx1.p2.4.m2.5.5.3.2.2" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.3.cmml"><mo stretchy="false" id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.3" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.3.cmml">(</mo><msub id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.cmml"><mi id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.2" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.2.cmml">ğ±</mi><mi id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.3" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.3.cmml">i</mi></msub><mo id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.4" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.3.cmml">,</mo><msub id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.cmml"><mi id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.2" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.2.cmml">ğ±</mi><mi id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.3" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.5" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.4.m2.5b"><apply id="Sx5.SSx1.p2.4.m2.5.5.cmml" xref="Sx5.SSx1.p2.4.m2.5.5"><eq id="Sx5.SSx1.p2.4.m2.5.5.4.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.4"></eq><apply id="Sx5.SSx1.p2.4.m2.3.3.1.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1"><csymbol cd="ambiguous" id="Sx5.SSx1.p2.4.m2.3.3.1.2.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1">subscript</csymbol><apply id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.1.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1">subscript</csymbol><ci id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.2.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.2">ğ¾</ci><apply id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3"><times id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.1.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.1"></times><ci id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.2.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.2">ğ‘</ci><ci id="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.3.cmml" xref="Sx5.SSx1.p2.4.m2.3.3.1.1.1.1.3.3">ğ‘</ci></apply></apply><list id="Sx5.SSx1.p2.4.m2.2.2.2.3.cmml" xref="Sx5.SSx1.p2.4.m2.2.2.2.4"><ci id="Sx5.SSx1.p2.4.m2.1.1.1.1.cmml" xref="Sx5.SSx1.p2.4.m2.1.1.1.1">ğ‘–</ci><ci id="Sx5.SSx1.p2.4.m2.2.2.2.2.cmml" xref="Sx5.SSx1.p2.4.m2.2.2.2.2">ğ‘—</ci></list></apply><apply id="Sx5.SSx1.p2.4.m2.5.5.3.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3"><times id="Sx5.SSx1.p2.4.m2.5.5.3.3.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.3"></times><ci id="Sx5.SSx1.p2.4.m2.5.5.3.4.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.4">ğ‘˜</ci><interval closure="open" id="Sx5.SSx1.p2.4.m2.5.5.3.2.3.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2"><apply id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.cmml" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1"><csymbol cd="ambiguous" id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.1.cmml" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1">subscript</csymbol><ci id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.2.cmml" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.2">ğ±</ci><ci id="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.3.cmml" xref="Sx5.SSx1.p2.4.m2.4.4.2.1.1.1.3">ğ‘–</ci></apply><apply id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2"><csymbol cd="ambiguous" id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.1.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2">subscript</csymbol><ci id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.2.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.2">ğ±</ci><ci id="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.3.cmml" xref="Sx5.SSx1.p2.4.m2.5.5.3.2.2.2.3">ğ‘—</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.4.m2.5c">(K_{NN})_{i,j}=k(\mathbf{x}_{i},\mathbf{x}_{j})</annotation></semantics></math>, <math id="Sx5.SSx1.p2.5.m3.1" class="ltx_Math" alttext="\mathbf{\Sigma}" display="inline"><semantics id="Sx5.SSx1.p2.5.m3.1a"><mi id="Sx5.SSx1.p2.5.m3.1.1" xref="Sx5.SSx1.p2.5.m3.1.1.cmml">ğšº</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.5.m3.1b"><ci id="Sx5.SSx1.p2.5.m3.1.1.cmml" xref="Sx5.SSx1.p2.5.m3.1.1">ğšº</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.5.m3.1c">\mathbf{\Sigma}</annotation></semantics></math> is a regularisation term, and the vector <math id="Sx5.SSx1.p2.6.m4.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="Sx5.SSx1.p2.6.m4.1a"><mi id="Sx5.SSx1.p2.6.m4.1.1" xref="Sx5.SSx1.p2.6.m4.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.6.m4.1b"><ci id="Sx5.SSx1.p2.6.m4.1.1.cmml" xref="Sx5.SSx1.p2.6.m4.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.6.m4.1c">\mathbf{y}</annotation></semantics></math> collects all labels in the training dataset. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> In sparse GPR, as we use here, the analogous equation for obtaining the fitting coefficients reads:<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></p>
<table id="Sx5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E3.m1.1" class="ltx_Math" alttext="\mathbf{c}=[\mathbf{K}_{MM}+\mathbf{K}_{MN}\mathbf{\Sigma}^{-1}\mathbf{K}_{NM}]^{-1}\mathbf{K}_{MN}\mathbf{\Sigma}^{-1}\mathbf{y}," display="block"><semantics id="Sx5.E3.m1.1a"><mrow id="Sx5.E3.m1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.cmml"><mrow id="Sx5.E3.m1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.cmml"><mi id="Sx5.E3.m1.1.1.1.1.3" xref="Sx5.E3.m1.1.1.1.1.3.cmml">ğœ</mi><mo id="Sx5.E3.m1.1.1.1.1.2" xref="Sx5.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx5.E3.m1.1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.1.cmml"><msup id="Sx5.E3.m1.1.1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.1.1.cmml"><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğŠ</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">M</mi></mrow></msub><mo id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml"><msub id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğŠ</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">N</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><msup id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">ğšº</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mo id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3a" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">âˆ’</mo><mn id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1a" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><msub id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.2.cmml">ğŠ</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.1" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.1.cmml">â€‹</mo><mi id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.3.cmml">M</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.3" xref="Sx5.E3.m1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="Sx5.E3.m1.1.1.1.1.1.1.3" xref="Sx5.E3.m1.1.1.1.1.1.1.3.cmml"><mo id="Sx5.E3.m1.1.1.1.1.1.1.3a" xref="Sx5.E3.m1.1.1.1.1.1.1.3.cmml">âˆ’</mo><mn id="Sx5.E3.m1.1.1.1.1.1.1.3.2" xref="Sx5.E3.m1.1.1.1.1.1.1.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.2" xref="Sx5.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><msub id="Sx5.E3.m1.1.1.1.1.1.3" xref="Sx5.E3.m1.1.1.1.1.1.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.3.2" xref="Sx5.E3.m1.1.1.1.1.1.3.2.cmml">ğŠ</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.3.3" xref="Sx5.E3.m1.1.1.1.1.1.3.3.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.3.3.2" xref="Sx5.E3.m1.1.1.1.1.1.3.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.3.3.1" xref="Sx5.E3.m1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="Sx5.E3.m1.1.1.1.1.1.3.3.3" xref="Sx5.E3.m1.1.1.1.1.1.3.3.3.cmml">N</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.2a" xref="Sx5.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><msup id="Sx5.E3.m1.1.1.1.1.1.4" xref="Sx5.E3.m1.1.1.1.1.1.4.cmml"><mi id="Sx5.E3.m1.1.1.1.1.1.4.2" xref="Sx5.E3.m1.1.1.1.1.1.4.2.cmml">ğšº</mi><mrow id="Sx5.E3.m1.1.1.1.1.1.4.3" xref="Sx5.E3.m1.1.1.1.1.1.4.3.cmml"><mo id="Sx5.E3.m1.1.1.1.1.1.4.3a" xref="Sx5.E3.m1.1.1.1.1.1.4.3.cmml">âˆ’</mo><mn id="Sx5.E3.m1.1.1.1.1.1.4.3.2" xref="Sx5.E3.m1.1.1.1.1.1.4.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="Sx5.E3.m1.1.1.1.1.1.2b" xref="Sx5.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="Sx5.E3.m1.1.1.1.1.1.5" xref="Sx5.E3.m1.1.1.1.1.1.5.cmml">ğ²</mi></mrow></mrow><mo id="Sx5.E3.m1.1.1.1.2" xref="Sx5.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E3.m1.1b"><apply id="Sx5.E3.m1.1.1.1.1.cmml" xref="Sx5.E3.m1.1.1.1"><eq id="Sx5.E3.m1.1.1.1.1.2.cmml" xref="Sx5.E3.m1.1.1.1.1.2"></eq><ci id="Sx5.E3.m1.1.1.1.1.3.cmml" xref="Sx5.E3.m1.1.1.1.1.3">ğœ</ci><apply id="Sx5.E3.m1.1.1.1.1.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1"><times id="Sx5.E3.m1.1.1.1.1.1.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.2"></times><apply id="Sx5.E3.m1.1.1.1.1.1.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.1.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1">superscript</csymbol><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx5.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1"><plus id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.2">ğŠ</ci><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3"><times id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘€</ci><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘€</ci></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3"><times id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.1"></times><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.2">ğŠ</ci><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3"><times id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.2">ğ‘€</ci><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.3.3">ğ‘</ci></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2">ğšº</ci><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3"><minus id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3"></minus><cn type="integer" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.2">1</cn></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.2">ğŠ</ci><apply id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3"><times id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.1"></times><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.2">ğ‘</ci><ci id="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.1.1.1.3.4.3.3">ğ‘€</ci></apply></apply></apply></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.1.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.3"><minus id="Sx5.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.3"></minus><cn type="integer" id="Sx5.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.1.3.2">1</cn></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3.2">ğŠ</ci><apply id="Sx5.E3.m1.1.1.1.1.1.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3.3"><times id="Sx5.E3.m1.1.1.1.1.1.3.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3.3.1"></times><ci id="Sx5.E3.m1.1.1.1.1.1.3.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3.3.2">ğ‘€</ci><ci id="Sx5.E3.m1.1.1.1.1.1.3.3.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.3.3.3">ğ‘</ci></apply></apply><apply id="Sx5.E3.m1.1.1.1.1.1.4.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="Sx5.E3.m1.1.1.1.1.1.4.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4">superscript</csymbol><ci id="Sx5.E3.m1.1.1.1.1.1.4.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4.2">ğšº</ci><apply id="Sx5.E3.m1.1.1.1.1.1.4.3.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4.3"><minus id="Sx5.E3.m1.1.1.1.1.1.4.3.1.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4.3"></minus><cn type="integer" id="Sx5.E3.m1.1.1.1.1.1.4.3.2.cmml" xref="Sx5.E3.m1.1.1.1.1.1.4.3.2">1</cn></apply></apply><ci id="Sx5.E3.m1.1.1.1.1.1.5.cmml" xref="Sx5.E3.m1.1.1.1.1.1.5">ğ²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E3.m1.1c">\mathbf{c}=[\mathbf{K}_{MM}+\mathbf{K}_{MN}\mathbf{\Sigma}^{-1}\mathbf{K}_{NM}]^{-1}\mathbf{K}_{MN}\mathbf{\Sigma}^{-1}\mathbf{y},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx1.p2.11" class="ltx_p">where similarly defined kernel matrices, now of different sizes, are used to quantify similarities between individual atomic environments. The resulting coefficient vector, <math id="Sx5.SSx1.p2.7.m1.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="Sx5.SSx1.p2.7.m1.1a"><mi id="Sx5.SSx1.p2.7.m1.1.1" xref="Sx5.SSx1.p2.7.m1.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.7.m1.1b"><ci id="Sx5.SSx1.p2.7.m1.1.1.cmml" xref="Sx5.SSx1.p2.7.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.7.m1.1c">\mathbf{c}</annotation></semantics></math>, now has length <math id="Sx5.SSx1.p2.8.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.SSx1.p2.8.m2.1a"><mi id="Sx5.SSx1.p2.8.m2.1.1" xref="Sx5.SSx1.p2.8.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.8.m2.1b"><ci id="Sx5.SSx1.p2.8.m2.1.1.cmml" xref="Sx5.SSx1.p2.8.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.8.m2.1c">M</annotation></semantics></math> (not <math id="Sx5.SSx1.p2.9.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx5.SSx1.p2.9.m3.1a"><mi id="Sx5.SSx1.p2.9.m3.1.1" xref="Sx5.SSx1.p2.9.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.9.m3.1b"><ci id="Sx5.SSx1.p2.9.m3.1.1.cmml" xref="Sx5.SSx1.p2.9.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.9.m3.1c">N</annotation></semantics></math>), and therefore the number of <span id="Sx5.SSx1.p2.11.1" class="ltx_text ltx_font_italic">representative</span> points, <math id="Sx5.SSx1.p2.10.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.SSx1.p2.10.m4.1a"><mi id="Sx5.SSx1.p2.10.m4.1.1" xref="Sx5.SSx1.p2.10.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.10.m4.1b"><ci id="Sx5.SSx1.p2.10.m4.1.1.cmml" xref="Sx5.SSx1.p2.10.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.10.m4.1c">M</annotation></semantics></math>, is what effectively controls the computational cost at runtime. For example, in a widely used GPR-based potential for elemental silicon, the reference database includes hundreds of thousands of atomic force components, whilst <math id="Sx5.SSx1.p2.11.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.SSx1.p2.11.m5.1a"><mi id="Sx5.SSx1.p2.11.m5.1.1" xref="Sx5.SSx1.p2.11.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p2.11.m5.1b"><ci id="Sx5.SSx1.p2.11.m5.1.1.cmml" xref="Sx5.SSx1.p2.11.m5.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p2.11.m5.1c">M</annotation></semantics></math> is only 9000.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></p>
</div>
<div id="Sx5.SSx1.p3" class="ltx_para">
<p id="Sx5.SSx1.p3.3" class="ltx_p">In Fig.Â <a href="#Sx5.F4" title="Figure 4 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a, we show learning curves as in the previous section, but now for different values of <math id="Sx5.SSx1.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Sx5.SSx1.p3.1.m1.1a"><mi id="Sx5.SSx1.p3.1.m1.1.1" xref="Sx5.SSx1.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p3.1.m1.1b"><ci id="Sx5.SSx1.p3.1.m1.1.1.cmml" xref="Sx5.SSx1.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p3.1.m1.1c">M</annotation></semantics></math> in otherwise similar GPR models.
We find that the change from <math id="Sx5.SSx1.p3.2.m2.1" class="ltx_Math" alttext="M=" display="inline"><semantics id="Sx5.SSx1.p3.2.m2.1a"><mrow id="Sx5.SSx1.p3.2.m2.1.1" xref="Sx5.SSx1.p3.2.m2.1.1.cmml"><mi id="Sx5.SSx1.p3.2.m2.1.1.2" xref="Sx5.SSx1.p3.2.m2.1.1.2.cmml">M</mi><mo id="Sx5.SSx1.p3.2.m2.1.1.1" xref="Sx5.SSx1.p3.2.m2.1.1.1.cmml">=</mo><mi id="Sx5.SSx1.p3.2.m2.1.1.3" xref="Sx5.SSx1.p3.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p3.2.m2.1b"><apply id="Sx5.SSx1.p3.2.m2.1.1.cmml" xref="Sx5.SSx1.p3.2.m2.1.1"><eq id="Sx5.SSx1.p3.2.m2.1.1.1.cmml" xref="Sx5.SSx1.p3.2.m2.1.1.1"></eq><ci id="Sx5.SSx1.p3.2.m2.1.1.2.cmml" xref="Sx5.SSx1.p3.2.m2.1.1.2">ğ‘€</ci><csymbol cd="latexml" id="Sx5.SSx1.p3.2.m2.1.1.3.cmml" xref="Sx5.SSx1.p3.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p3.2.m2.1c">M=</annotation></semantics></math> 5000 to <math id="Sx5.SSx1.p3.3.m3.1" class="ltx_Math" alttext="M=" display="inline"><semantics id="Sx5.SSx1.p3.3.m3.1a"><mrow id="Sx5.SSx1.p3.3.m3.1.1" xref="Sx5.SSx1.p3.3.m3.1.1.cmml"><mi id="Sx5.SSx1.p3.3.m3.1.1.2" xref="Sx5.SSx1.p3.3.m3.1.1.2.cmml">M</mi><mo id="Sx5.SSx1.p3.3.m3.1.1.1" xref="Sx5.SSx1.p3.3.m3.1.1.1.cmml">=</mo><mi id="Sx5.SSx1.p3.3.m3.1.1.3" xref="Sx5.SSx1.p3.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p3.3.m3.1b"><apply id="Sx5.SSx1.p3.3.m3.1.1.cmml" xref="Sx5.SSx1.p3.3.m3.1.1"><eq id="Sx5.SSx1.p3.3.m3.1.1.1.cmml" xref="Sx5.SSx1.p3.3.m3.1.1.1"></eq><ci id="Sx5.SSx1.p3.3.m3.1.1.2.cmml" xref="Sx5.SSx1.p3.3.m3.1.1.2">ğ‘€</ci><csymbol cd="latexml" id="Sx5.SSx1.p3.3.m3.1.1.3.cmml" xref="Sx5.SSx1.p3.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p3.3.m3.1c">M=</annotation></semantics></math> 10,000 does not seem to lead to a major change any more, at least up to the range investigated.</p>
</div>
<div id="Sx5.SSx1.p4" class="ltx_para">
<p id="Sx5.SSx1.p4.1" class="ltx_p">The second aspect that we explore for our GPR models is the regularisation, controlled by the matrix <math id="Sx5.SSx1.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{\Sigma}" display="inline"><semantics id="Sx5.SSx1.p4.1.m1.1a"><mi id="Sx5.SSx1.p4.1.m1.1.1" xref="Sx5.SSx1.p4.1.m1.1.1.cmml">ğšº</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p4.1.m1.1b"><ci id="Sx5.SSx1.p4.1.m1.1.1.cmml" xref="Sx5.SSx1.p4.1.m1.1.1">ğšº</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p4.1.m1.1c">\mathbf{\Sigma}</annotation></semantics></math> in Eqs.Â <a href="#Sx5.E2" title="In GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#Sx5.E3" title="In GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> above.
The regularisation applied during training can be interpreted as â€œexpected errorâ€ of the data (in the context of interatomic potentials, this might be due to accuracy and convergence limits of the quantum-mechanical training data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>).
Another interpretation is as the driving force applied during training that affects the extent to with the final fit passes through all the data points.
For simplicity, we use a constant regularisation value for all atoms in the database; that is,</p>
<table id="Sx5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E4.m1.1" class="ltx_Math" alttext="\mathbf{\Sigma}=\sigma\mathbf{I}," display="block"><semantics id="Sx5.E4.m1.1a"><mrow id="Sx5.E4.m1.1.1.1" xref="Sx5.E4.m1.1.1.1.1.cmml"><mrow id="Sx5.E4.m1.1.1.1.1" xref="Sx5.E4.m1.1.1.1.1.cmml"><mi id="Sx5.E4.m1.1.1.1.1.2" xref="Sx5.E4.m1.1.1.1.1.2.cmml">ğšº</mi><mo id="Sx5.E4.m1.1.1.1.1.1" xref="Sx5.E4.m1.1.1.1.1.1.cmml">=</mo><mrow id="Sx5.E4.m1.1.1.1.1.3" xref="Sx5.E4.m1.1.1.1.1.3.cmml"><mi id="Sx5.E4.m1.1.1.1.1.3.2" xref="Sx5.E4.m1.1.1.1.1.3.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="Sx5.E4.m1.1.1.1.1.3.1" xref="Sx5.E4.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="Sx5.E4.m1.1.1.1.1.3.3" xref="Sx5.E4.m1.1.1.1.1.3.3.cmml">ğˆ</mi></mrow></mrow><mo id="Sx5.E4.m1.1.1.1.2" xref="Sx5.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E4.m1.1b"><apply id="Sx5.E4.m1.1.1.1.1.cmml" xref="Sx5.E4.m1.1.1.1"><eq id="Sx5.E4.m1.1.1.1.1.1.cmml" xref="Sx5.E4.m1.1.1.1.1.1"></eq><ci id="Sx5.E4.m1.1.1.1.1.2.cmml" xref="Sx5.E4.m1.1.1.1.1.2">ğšº</ci><apply id="Sx5.E4.m1.1.1.1.1.3.cmml" xref="Sx5.E4.m1.1.1.1.1.3"><times id="Sx5.E4.m1.1.1.1.1.3.1.cmml" xref="Sx5.E4.m1.1.1.1.1.3.1"></times><ci id="Sx5.E4.m1.1.1.1.1.3.2.cmml" xref="Sx5.E4.m1.1.1.1.1.3.2">ğœ</ci><ci id="Sx5.E4.m1.1.1.1.1.3.3.cmml" xref="Sx5.E4.m1.1.1.1.1.3.3">ğˆ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E4.m1.1c">\mathbf{\Sigma}=\sigma\mathbf{I},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx1.p4.2" class="ltx_p">with <math id="Sx5.SSx1.p4.2.m1.1" class="ltx_Math" alttext="\sigma=10" display="inline"><semantics id="Sx5.SSx1.p4.2.m1.1a"><mrow id="Sx5.SSx1.p4.2.m1.1.1" xref="Sx5.SSx1.p4.2.m1.1.1.cmml"><mi id="Sx5.SSx1.p4.2.m1.1.1.2" xref="Sx5.SSx1.p4.2.m1.1.1.2.cmml">Ïƒ</mi><mo id="Sx5.SSx1.p4.2.m1.1.1.1" xref="Sx5.SSx1.p4.2.m1.1.1.1.cmml">=</mo><mn id="Sx5.SSx1.p4.2.m1.1.1.3" xref="Sx5.SSx1.p4.2.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p4.2.m1.1b"><apply id="Sx5.SSx1.p4.2.m1.1.1.cmml" xref="Sx5.SSx1.p4.2.m1.1.1"><eq id="Sx5.SSx1.p4.2.m1.1.1.1.cmml" xref="Sx5.SSx1.p4.2.m1.1.1.1"></eq><ci id="Sx5.SSx1.p4.2.m1.1.1.2.cmml" xref="Sx5.SSx1.p4.2.m1.1.1.2">ğœ</ci><cn type="integer" id="Sx5.SSx1.p4.2.m1.1.1.3.cmml" xref="Sx5.SSx1.p4.2.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p4.2.m1.1c">\sigma=10</annotation></semantics></math> meV unless noted otherwise.
We note in passing that more adaptive approaches are possible, such as an individual regularisation for each atom, as exemplified before in GAP fitting. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite></p>
</div>
<div id="Sx5.SSx1.p5" class="ltx_para">
<p id="Sx5.SSx1.p5.3" class="ltx_p">We tested the effect of varying the regularisation value, <math id="Sx5.SSx1.p5.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="Sx5.SSx1.p5.1.m1.1a"><mi id="Sx5.SSx1.p5.1.m1.1.1" xref="Sx5.SSx1.p5.1.m1.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p5.1.m1.1b"><ci id="Sx5.SSx1.p5.1.m1.1.1.cmml" xref="Sx5.SSx1.p5.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p5.1.m1.1c">\sigma</annotation></semantics></math>, over a wide range of values â€“ the ease with which synthetic data labels are accessible means that we can rapidly fit many candidate models, both in terms of <math id="Sx5.SSx1.p5.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="Sx5.SSx1.p5.2.m2.1a"><mi id="Sx5.SSx1.p5.2.m2.1.1" xref="Sx5.SSx1.p5.2.m2.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p5.2.m2.1b"><ci id="Sx5.SSx1.p5.2.m2.1.1.cmml" xref="Sx5.SSx1.p5.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p5.2.m2.1c">\sigma</annotation></semantics></math>, and of the number of training points, <math id="Sx5.SSx1.p5.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx5.SSx1.p5.3.m3.1a"><mi id="Sx5.SSx1.p5.3.m3.1.1" xref="Sx5.SSx1.p5.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p5.3.m3.1b"><ci id="Sx5.SSx1.p5.3.m3.1.1.cmml" xref="Sx5.SSx1.p5.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p5.3.m3.1c">N</annotation></semantics></math>. The results are shown in Fig.Â <a href="#Sx5.F4" title="Figure 4 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b.</p>
</div>
<div id="Sx5.SSx1.p6" class="ltx_para">
<p id="Sx5.SSx1.p6.5" class="ltx_p">Interestingly, the dependence on the regularisation becomes less pronounced with larger <math id="Sx5.SSx1.p6.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx5.SSx1.p6.1.m1.1a"><mi id="Sx5.SSx1.p6.1.m1.1.1" xref="Sx5.SSx1.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p6.1.m1.1b"><ci id="Sx5.SSx1.p6.1.m1.1.1.cmml" xref="Sx5.SSx1.p6.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p6.1.m1.1c">N</annotation></semantics></math>: the curves visibly flatten as one moves to larger datasets. At the same time, there is still a difference between the <math id="Sx5.SSx1.p6.2.m2.1" class="ltx_Math" alttext="N=10^{5}" display="inline"><semantics id="Sx5.SSx1.p6.2.m2.1a"><mrow id="Sx5.SSx1.p6.2.m2.1.1" xref="Sx5.SSx1.p6.2.m2.1.1.cmml"><mi id="Sx5.SSx1.p6.2.m2.1.1.2" xref="Sx5.SSx1.p6.2.m2.1.1.2.cmml">N</mi><mo id="Sx5.SSx1.p6.2.m2.1.1.1" xref="Sx5.SSx1.p6.2.m2.1.1.1.cmml">=</mo><msup id="Sx5.SSx1.p6.2.m2.1.1.3" xref="Sx5.SSx1.p6.2.m2.1.1.3.cmml"><mn id="Sx5.SSx1.p6.2.m2.1.1.3.2" xref="Sx5.SSx1.p6.2.m2.1.1.3.2.cmml">10</mn><mn id="Sx5.SSx1.p6.2.m2.1.1.3.3" xref="Sx5.SSx1.p6.2.m2.1.1.3.3.cmml">5</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p6.2.m2.1b"><apply id="Sx5.SSx1.p6.2.m2.1.1.cmml" xref="Sx5.SSx1.p6.2.m2.1.1"><eq id="Sx5.SSx1.p6.2.m2.1.1.1.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.1"></eq><ci id="Sx5.SSx1.p6.2.m2.1.1.2.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.2">ğ‘</ci><apply id="Sx5.SSx1.p6.2.m2.1.1.3.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="Sx5.SSx1.p6.2.m2.1.1.3.1.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="Sx5.SSx1.p6.2.m2.1.1.3.2.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.3.2">10</cn><cn type="integer" id="Sx5.SSx1.p6.2.m2.1.1.3.3.cmml" xref="Sx5.SSx1.p6.2.m2.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p6.2.m2.1c">N=10^{5}</annotation></semantics></math> and <math id="Sx5.SSx1.p6.3.m3.1" class="ltx_Math" alttext="N=10^{6}" display="inline"><semantics id="Sx5.SSx1.p6.3.m3.1a"><mrow id="Sx5.SSx1.p6.3.m3.1.1" xref="Sx5.SSx1.p6.3.m3.1.1.cmml"><mi id="Sx5.SSx1.p6.3.m3.1.1.2" xref="Sx5.SSx1.p6.3.m3.1.1.2.cmml">N</mi><mo id="Sx5.SSx1.p6.3.m3.1.1.1" xref="Sx5.SSx1.p6.3.m3.1.1.1.cmml">=</mo><msup id="Sx5.SSx1.p6.3.m3.1.1.3" xref="Sx5.SSx1.p6.3.m3.1.1.3.cmml"><mn id="Sx5.SSx1.p6.3.m3.1.1.3.2" xref="Sx5.SSx1.p6.3.m3.1.1.3.2.cmml">10</mn><mn id="Sx5.SSx1.p6.3.m3.1.1.3.3" xref="Sx5.SSx1.p6.3.m3.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p6.3.m3.1b"><apply id="Sx5.SSx1.p6.3.m3.1.1.cmml" xref="Sx5.SSx1.p6.3.m3.1.1"><eq id="Sx5.SSx1.p6.3.m3.1.1.1.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.1"></eq><ci id="Sx5.SSx1.p6.3.m3.1.1.2.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.2">ğ‘</ci><apply id="Sx5.SSx1.p6.3.m3.1.1.3.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.3"><csymbol cd="ambiguous" id="Sx5.SSx1.p6.3.m3.1.1.3.1.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="Sx5.SSx1.p6.3.m3.1.1.3.2.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.3.2">10</cn><cn type="integer" id="Sx5.SSx1.p6.3.m3.1.1.3.3.cmml" xref="Sx5.SSx1.p6.3.m3.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p6.3.m3.1c">N=10^{6}</annotation></semantics></math> curves in Fig.Â <a href="#Sx5.F4" title="Figure 4 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b, even though the learning curve itself had already â€œlevelled offâ€ (Fig.Â <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
We observe a flatter curve for the larger dataset, and a shift of the location of the minimum to higher <math id="Sx5.SSx1.p6.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="Sx5.SSx1.p6.4.m4.1a"><mi id="Sx5.SSx1.p6.4.m4.1.1" xref="Sx5.SSx1.p6.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p6.4.m4.1b"><ci id="Sx5.SSx1.p6.4.m4.1.1.cmml" xref="Sx5.SSx1.p6.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p6.4.m4.1c">\sigma</annotation></semantics></math>, although it remains to be explored how significant the latter is.
GPR is a fundamentally Bayesian technique, and so the behaviour seen in Fig.Â <a href="#Sx5.F4" title="Figure 4 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b can be rationalised by noting that using more data reduces the importance of any priors, in this case, of the exact value of <math id="Sx5.SSx1.p6.5.m5.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="Sx5.SSx1.p6.5.m5.1a"><mi id="Sx5.SSx1.p6.5.m5.1.1" xref="Sx5.SSx1.p6.5.m5.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx1.p6.5.m5.1b"><ci id="Sx5.SSx1.p6.5.m5.1.1.cmml" xref="Sx5.SSx1.p6.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx1.p6.5.m5.1c">\sigma</annotation></semantics></math>.
As larger and larger datasets become used for GPR-based models, tuning of the regularisation is therefore expected to become less important.</p>
</div>
<div id="Sx5.SSx1.p7" class="ltx_para">
<p id="Sx5.SSx1.p7.1" class="ltx_p">In retrospect, both plots in Fig.Â <a href="#Sx5.F4" title="Figure 4 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> seem to confirm settings that have been intuitively used in ML potential fitting using the GAP framework. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> We are curious whether large-scale experiments on synthetic (proxy) data can, in the future, inform the choice of regularisation and other hyperparameters in new and more complex â€œreal-worldâ€ GPR models for chemistry.</p>
</div>
<figure id="Sx5.F5" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_05.png" id="Sx5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Pre-training neural networks with synthetic atomistic data.
(a) Schematic of the experiment. We pre-train an NN (orange) on local energies from a large synthetic dataset, <span id="Sx5.F5.17.1" class="ltx_text ltx_font_bold">D1</span> (<math id="Sx5.F5.5.m1.2" class="ltx_Math" alttext="\equiv\left\{\mathbf{x}_{1},\tilde{y}_{1}\right\}" display="inline"><semantics id="Sx5.F5.5.m1.2b"><mrow id="Sx5.F5.5.m1.2.2" xref="Sx5.F5.5.m1.2.2.cmml"><mi id="Sx5.F5.5.m1.2.2.4" xref="Sx5.F5.5.m1.2.2.4.cmml"></mi><mo id="Sx5.F5.5.m1.2.2.3" xref="Sx5.F5.5.m1.2.2.3.cmml">â‰¡</mo><mrow id="Sx5.F5.5.m1.2.2.2.2" xref="Sx5.F5.5.m1.2.2.2.3.cmml"><mo id="Sx5.F5.5.m1.2.2.2.2.3" xref="Sx5.F5.5.m1.2.2.2.3.cmml">{</mo><msub id="Sx5.F5.5.m1.1.1.1.1.1" xref="Sx5.F5.5.m1.1.1.1.1.1.cmml"><mi id="Sx5.F5.5.m1.1.1.1.1.1.2" xref="Sx5.F5.5.m1.1.1.1.1.1.2.cmml">ğ±</mi><mn id="Sx5.F5.5.m1.1.1.1.1.1.3" xref="Sx5.F5.5.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Sx5.F5.5.m1.2.2.2.2.4" xref="Sx5.F5.5.m1.2.2.2.3.cmml">,</mo><msub id="Sx5.F5.5.m1.2.2.2.2.2" xref="Sx5.F5.5.m1.2.2.2.2.2.cmml"><mover accent="true" id="Sx5.F5.5.m1.2.2.2.2.2.2" xref="Sx5.F5.5.m1.2.2.2.2.2.2.cmml"><mi id="Sx5.F5.5.m1.2.2.2.2.2.2.2" xref="Sx5.F5.5.m1.2.2.2.2.2.2.2.cmml">y</mi><mo id="Sx5.F5.5.m1.2.2.2.2.2.2.1" xref="Sx5.F5.5.m1.2.2.2.2.2.2.1.cmml">~</mo></mover><mn id="Sx5.F5.5.m1.2.2.2.2.2.3" xref="Sx5.F5.5.m1.2.2.2.2.2.3.cmml">1</mn></msub><mo id="Sx5.F5.5.m1.2.2.2.2.5" xref="Sx5.F5.5.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx5.F5.5.m1.2c"><apply id="Sx5.F5.5.m1.2.2.cmml" xref="Sx5.F5.5.m1.2.2"><equivalent id="Sx5.F5.5.m1.2.2.3.cmml" xref="Sx5.F5.5.m1.2.2.3"></equivalent><csymbol cd="latexml" id="Sx5.F5.5.m1.2.2.4.cmml" xref="Sx5.F5.5.m1.2.2.4">absent</csymbol><set id="Sx5.F5.5.m1.2.2.2.3.cmml" xref="Sx5.F5.5.m1.2.2.2.2"><apply id="Sx5.F5.5.m1.1.1.1.1.1.cmml" xref="Sx5.F5.5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.F5.5.m1.1.1.1.1.1.1.cmml" xref="Sx5.F5.5.m1.1.1.1.1.1">subscript</csymbol><ci id="Sx5.F5.5.m1.1.1.1.1.1.2.cmml" xref="Sx5.F5.5.m1.1.1.1.1.1.2">ğ±</ci><cn type="integer" id="Sx5.F5.5.m1.1.1.1.1.1.3.cmml" xref="Sx5.F5.5.m1.1.1.1.1.1.3">1</cn></apply><apply id="Sx5.F5.5.m1.2.2.2.2.2.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="Sx5.F5.5.m1.2.2.2.2.2.1.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2">subscript</csymbol><apply id="Sx5.F5.5.m1.2.2.2.2.2.2.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2.2"><ci id="Sx5.F5.5.m1.2.2.2.2.2.2.1.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2.2.1">~</ci><ci id="Sx5.F5.5.m1.2.2.2.2.2.2.2.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2.2.2">ğ‘¦</ci></apply><cn type="integer" id="Sx5.F5.5.m1.2.2.2.2.2.3.cmml" xref="Sx5.F5.5.m1.2.2.2.2.2.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F5.5.m1.2d">\equiv\left\{\mathbf{x}_{1},\tilde{y}_{1}\right\}</annotation></semantics></math>), then use the optimised parameters as a starting point for training (â€œfine-tuningâ€) another NN (red) on quantum-mechanical (â€œQMâ€) total energies from a smaller dataset, <span id="Sx5.F5.18.2" class="ltx_text ltx_font_bold">D0</span>.
We compare against the more conventional approach of directly training on <span id="Sx5.F5.19.3" class="ltx_text ltx_font_bold">D0</span> (purple).
(b) Parity plots for local (per-atom) energy predictions, testing against <span id="Sx5.F5.20.4" class="ltx_text ltx_font_bold">D1</span>, <span id="Sx5.F5.21.5" class="ltx_text ltx_font_italic">i.e.</span>, against the performance of C-GAP-17. From left to right: the directly trained NN (which learns to assign local energies in a different manner from GAP), the pre-trained NN (with tight correlation), and finally the fine-tuned model.
(c) Effect of varying the number of pre-training environments on final test-set accuracy when fine-tuned on the complete <span id="Sx5.F5.22.6" class="ltx_text ltx_font_bold">D0</span>. Low numbers of pretraining environments have little effect on the final accuracy (<math id="Sx5.F5.6.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx5.F5.6.m2.1b"><mo id="Sx5.F5.6.m2.1.1" xref="Sx5.F5.6.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="Sx5.F5.6.m2.1c"><csymbol cd="latexml" id="Sx5.F5.6.m2.1.1.cmml" xref="Sx5.F5.6.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F5.6.m2.1d">\sim</annotation></semantics></math> no change as compared to direct training). Increasing the number of pre-training environments beyond about <math id="Sx5.F5.7.m3.1" class="ltx_Math" alttext="10^{5}" display="inline"><semantics id="Sx5.F5.7.m3.1b"><msup id="Sx5.F5.7.m3.1.1" xref="Sx5.F5.7.m3.1.1.cmml"><mn id="Sx5.F5.7.m3.1.1.2" xref="Sx5.F5.7.m3.1.1.2.cmml">10</mn><mn id="Sx5.F5.7.m3.1.1.3" xref="Sx5.F5.7.m3.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="Sx5.F5.7.m3.1c"><apply id="Sx5.F5.7.m3.1.1.cmml" xref="Sx5.F5.7.m3.1.1"><csymbol cd="ambiguous" id="Sx5.F5.7.m3.1.1.1.cmml" xref="Sx5.F5.7.m3.1.1">superscript</csymbol><cn type="integer" id="Sx5.F5.7.m3.1.1.2.cmml" xref="Sx5.F5.7.m3.1.1.2">10</cn><cn type="integer" id="Sx5.F5.7.m3.1.1.3.cmml" xref="Sx5.F5.7.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F5.7.m3.1d">10^{5}</annotation></semantics></math> (<span id="Sx5.F5.23.7" class="ltx_text ltx_font_italic">i.e.</span>, roughly the number of environments in <span id="Sx5.F5.24.8" class="ltx_text ltx_font_bold">D0</span>) leads to increasing performance of the fine-tuned over the directly trained model.
(d) Learning curves that show the dependence on the number of QM labels seen during training.
To obtain a model with the same predictive power, in this case, <math id="Sx5.F5.8.m4.1" class="ltx_math_unparsed" alttext="\sim 8\times" display="inline"><semantics id="Sx5.F5.8.m4.1b"><mrow id="Sx5.F5.8.m4.1c"><mo id="Sx5.F5.8.m4.1.1">âˆ¼</mo><mn id="Sx5.F5.8.m4.1.2">8</mn><mo lspace="0.222em" id="Sx5.F5.8.m4.1.3">Ã—</mo></mrow><annotation encoding="application/x-tex" id="Sx5.F5.8.m4.1d">\sim 8\times</annotation></semantics></math> fewer QM labels are required when starting from a pre-trained NN, as compared to random initialisation.
Note that we truncate the plot at 100 QM labels for direct training, but extend it to as low as 25 for the fine-tuned NNs.
</figcaption>
</figure>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Pre-training</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">We now move to the discussion of neural-network methodology for atomistic ML.
We hypothesised that our synthetic dataset can be used to pre-train an atomistic NN model, which can then be fine-tuned for predicting a related property.
For this approach to be useful, it needs to lead to a better final model than training an NN directly without prior information. The idea behind this experiment is sketched in Fig.Â <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a.</p>
</div>
<div id="Sx5.SSx2.p2" class="ltx_para">
<p id="Sx5.SSx2.p2.5" class="ltx_p">The task on which we have focused so far is to minimise the atom-wise (squared) error of our model predictions as compared to synthetic labels:</p>
<table id="Sx5.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E5.m1.2" class="ltx_Math" alttext="\underset{\lambda}{\textrm{argmin}}\quad\mathcal{L}\left(\sum_{i}\left|\tilde{\varepsilon}_{i}-\hat{\varepsilon}_{\lambda}(\mathbf{x}_{i})\right|^{2}\right)," display="block"><semantics id="Sx5.E5.m1.2a"><mrow id="Sx5.E5.m1.2.2.1"><mrow id="Sx5.E5.m1.2.2.1.1.1" xref="Sx5.E5.m1.2.2.1.1.2.cmml"><munder accentunder="true" id="Sx5.E5.m1.1.1" xref="Sx5.E5.m1.1.1.cmml"><mtext id="Sx5.E5.m1.1.1.2" xref="Sx5.E5.m1.1.1.2a.cmml">argmin</mtext><mo id="Sx5.E5.m1.1.1.1" xref="Sx5.E5.m1.1.1.1.cmml">ğœ†</mo></munder><mspace width="1em" id="Sx5.E5.m1.2.2.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.2.cmml"></mspace><mrow id="Sx5.E5.m1.2.2.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx5.E5.m1.2.2.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="Sx5.E5.m1.2.2.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><munder id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" movablelimits="false" rspace="0em" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">i</mi></munder><msup id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">Îµ</mi><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">~</mo></mover><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">Îµ</mi><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">Î»</mi></msub><mo lspace="0em" rspace="0em" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="Sx5.E5.m1.2.2.1.1.1.1.1.1.3" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx5.E5.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E5.m1.2b"><list id="Sx5.E5.m1.2.2.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1"><apply id="Sx5.E5.m1.1.1.cmml" xref="Sx5.E5.m1.1.1"><ci id="Sx5.E5.m1.1.1.1.cmml" xref="Sx5.E5.m1.1.1.1">ğœ†</ci><ci id="Sx5.E5.m1.1.1.2a.cmml" xref="Sx5.E5.m1.1.1.2"><mtext id="Sx5.E5.m1.1.1.2.cmml" xref="Sx5.E5.m1.1.1.2">argmin</mtext></ci></apply><apply id="Sx5.E5.m1.2.2.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1"><times id="Sx5.E5.m1.2.2.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.2"></times><ci id="Sx5.E5.m1.2.2.1.1.1.1.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.3">â„’</ci><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1"><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.2"></sum><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1"><abs id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2"></abs><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><minus id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2"></minus><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2"><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.1">~</ci><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğœ€</ci></apply><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><times id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2"><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğœ€</ci></apply><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğœ†</ci></apply><apply id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ±</ci><ci id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply><cn type="integer" id="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E5.m1.2.2.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E5.m1.2c">\underset{\lambda}{\textrm{argmin}}\quad\mathcal{L}\left(\sum_{i}\left|\tilde{\varepsilon}_{i}-\hat{\varepsilon}_{\lambda}(\mathbf{x}_{i})\right|^{2}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx2.p2.4" class="ltx_p">where <math id="Sx5.SSx2.p2.1.m1.1" class="ltx_Math" alttext="\tilde{\varepsilon}" display="inline"><semantics id="Sx5.SSx2.p2.1.m1.1a"><mover accent="true" id="Sx5.SSx2.p2.1.m1.1.1" xref="Sx5.SSx2.p2.1.m1.1.1.cmml"><mi id="Sx5.SSx2.p2.1.m1.1.1.2" xref="Sx5.SSx2.p2.1.m1.1.1.2.cmml">Îµ</mi><mo id="Sx5.SSx2.p2.1.m1.1.1.1" xref="Sx5.SSx2.p2.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p2.1.m1.1b"><apply id="Sx5.SSx2.p2.1.m1.1.1.cmml" xref="Sx5.SSx2.p2.1.m1.1.1"><ci id="Sx5.SSx2.p2.1.m1.1.1.1.cmml" xref="Sx5.SSx2.p2.1.m1.1.1.1">~</ci><ci id="Sx5.SSx2.p2.1.m1.1.1.2.cmml" xref="Sx5.SSx2.p2.1.m1.1.1.2">ğœ€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p2.1.m1.1c">\tilde{\varepsilon}</annotation></semantics></math> are the ML atomic energies, as labelled by C-GAP-17 and used here as synthetic training data, and <math id="Sx5.SSx2.p2.2.m2.1" class="ltx_Math" alttext="\hat{\varepsilon}" display="inline"><semantics id="Sx5.SSx2.p2.2.m2.1a"><mover accent="true" id="Sx5.SSx2.p2.2.m2.1.1" xref="Sx5.SSx2.p2.2.m2.1.1.cmml"><mi id="Sx5.SSx2.p2.2.m2.1.1.2" xref="Sx5.SSx2.p2.2.m2.1.1.2.cmml">Îµ</mi><mo id="Sx5.SSx2.p2.2.m2.1.1.1" xref="Sx5.SSx2.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p2.2.m2.1b"><apply id="Sx5.SSx2.p2.2.m2.1.1.cmml" xref="Sx5.SSx2.p2.2.m2.1.1"><ci id="Sx5.SSx2.p2.2.m2.1.1.1.cmml" xref="Sx5.SSx2.p2.2.m2.1.1.1">^</ci><ci id="Sx5.SSx2.p2.2.m2.1.1.2.cmml" xref="Sx5.SSx2.p2.2.m2.1.1.2">ğœ€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p2.2.m2.1c">\hat{\varepsilon}</annotation></semantics></math> are our modelâ€™s predictions of this property. The loss function, <math id="Sx5.SSx2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="Sx5.SSx2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Sx5.SSx2.p2.3.m3.1.1" xref="Sx5.SSx2.p2.3.m3.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p2.3.m3.1b"><ci id="Sx5.SSx2.p2.3.m3.1.1.cmml" xref="Sx5.SSx2.p2.3.m3.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p2.3.m3.1c">\mathcal{L}</annotation></semantics></math>, is optimised with respect to the set of model parameters, <math id="Sx5.SSx2.p2.4.m4.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="Sx5.SSx2.p2.4.m4.1a"><mi id="Sx5.SSx2.p2.4.m4.1.1" xref="Sx5.SSx2.p2.4.m4.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p2.4.m4.1b"><ci id="Sx5.SSx2.p2.4.m4.1.1.cmml" xref="Sx5.SSx2.p2.4.m4.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p2.4.m4.1c">\lambda</annotation></semantics></math>.</p>
</div>
<div id="Sx5.SSx2.p3" class="ltx_para">
<p id="Sx5.SSx2.p3.5" class="ltx_p">The task that we ultimately want to perform is the prediction of quantum-mechanical, per-cell energies, <math id="Sx5.SSx2.p3.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="Sx5.SSx2.p3.1.m1.1a"><mi id="Sx5.SSx2.p3.1.m1.1.1" xref="Sx5.SSx2.p3.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.1.m1.1b"><ci id="Sx5.SSx2.p3.1.m1.1.1.cmml" xref="Sx5.SSx2.p3.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.1.m1.1c">E</annotation></semantics></math>:
we have a dataset, <span id="Sx5.SSx2.p3.5.1" class="ltx_text ltx_font_bold">D0</span>, consisting of pairs <math id="Sx5.SSx2.p3.2.m2.2" class="ltx_Math" alttext="\{\mathbf{X},E_{\mathrm{DFT}}\}" display="inline"><semantics id="Sx5.SSx2.p3.2.m2.2a"><mrow id="Sx5.SSx2.p3.2.m2.2.2.1" xref="Sx5.SSx2.p3.2.m2.2.2.2.cmml"><mo stretchy="false" id="Sx5.SSx2.p3.2.m2.2.2.1.2" xref="Sx5.SSx2.p3.2.m2.2.2.2.cmml">{</mo><mi id="Sx5.SSx2.p3.2.m2.1.1" xref="Sx5.SSx2.p3.2.m2.1.1.cmml">ğ—</mi><mo id="Sx5.SSx2.p3.2.m2.2.2.1.3" xref="Sx5.SSx2.p3.2.m2.2.2.2.cmml">,</mo><msub id="Sx5.SSx2.p3.2.m2.2.2.1.1" xref="Sx5.SSx2.p3.2.m2.2.2.1.1.cmml"><mi id="Sx5.SSx2.p3.2.m2.2.2.1.1.2" xref="Sx5.SSx2.p3.2.m2.2.2.1.1.2.cmml">E</mi><mi id="Sx5.SSx2.p3.2.m2.2.2.1.1.3" xref="Sx5.SSx2.p3.2.m2.2.2.1.1.3.cmml">DFT</mi></msub><mo stretchy="false" id="Sx5.SSx2.p3.2.m2.2.2.1.4" xref="Sx5.SSx2.p3.2.m2.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.2.m2.2b"><set id="Sx5.SSx2.p3.2.m2.2.2.2.cmml" xref="Sx5.SSx2.p3.2.m2.2.2.1"><ci id="Sx5.SSx2.p3.2.m2.1.1.cmml" xref="Sx5.SSx2.p3.2.m2.1.1">ğ—</ci><apply id="Sx5.SSx2.p3.2.m2.2.2.1.1.cmml" xref="Sx5.SSx2.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="Sx5.SSx2.p3.2.m2.2.2.1.1.1.cmml" xref="Sx5.SSx2.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="Sx5.SSx2.p3.2.m2.2.2.1.1.2.cmml" xref="Sx5.SSx2.p3.2.m2.2.2.1.1.2">ğ¸</ci><ci id="Sx5.SSx2.p3.2.m2.2.2.1.1.3.cmml" xref="Sx5.SSx2.p3.2.m2.2.2.1.1.3">DFT</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.2.m2.2c">\{\mathbf{X},E_{\mathrm{DFT}}\}</annotation></semantics></math>, where <math id="Sx5.SSx2.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="Sx5.SSx2.p3.3.m3.1a"><mi id="Sx5.SSx2.p3.3.m3.1.1" xref="Sx5.SSx2.p3.3.m3.1.1.cmml">ğ—</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.3.m3.1b"><ci id="Sx5.SSx2.p3.3.m3.1.1.cmml" xref="Sx5.SSx2.p3.3.m3.1.1">ğ—</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.3.m3.1c">\mathbf{X}</annotation></semantics></math> is the set of descriptor vectors, <math id="Sx5.SSx2.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{x}_{i}" display="inline"><semantics id="Sx5.SSx2.p3.4.m4.1a"><msub id="Sx5.SSx2.p3.4.m4.1.1" xref="Sx5.SSx2.p3.4.m4.1.1.cmml"><mi id="Sx5.SSx2.p3.4.m4.1.1.2" xref="Sx5.SSx2.p3.4.m4.1.1.2.cmml">ğ±</mi><mi id="Sx5.SSx2.p3.4.m4.1.1.3" xref="Sx5.SSx2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.4.m4.1b"><apply id="Sx5.SSx2.p3.4.m4.1.1.cmml" xref="Sx5.SSx2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="Sx5.SSx2.p3.4.m4.1.1.1.cmml" xref="Sx5.SSx2.p3.4.m4.1.1">subscript</csymbol><ci id="Sx5.SSx2.p3.4.m4.1.1.2.cmml" xref="Sx5.SSx2.p3.4.m4.1.1.2">ğ±</ci><ci id="Sx5.SSx2.p3.4.m4.1.1.3.cmml" xref="Sx5.SSx2.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.4.m4.1c">\mathbf{x}_{i}</annotation></semantics></math>, together describing all atoms in a given unit cell, and <math id="Sx5.SSx2.p3.5.m5.1" class="ltx_Math" alttext="E_{\mathrm{DFT}}" display="inline"><semantics id="Sx5.SSx2.p3.5.m5.1a"><msub id="Sx5.SSx2.p3.5.m5.1.1" xref="Sx5.SSx2.p3.5.m5.1.1.cmml"><mi id="Sx5.SSx2.p3.5.m5.1.1.2" xref="Sx5.SSx2.p3.5.m5.1.1.2.cmml">E</mi><mi id="Sx5.SSx2.p3.5.m5.1.1.3" xref="Sx5.SSx2.p3.5.m5.1.1.3.cmml">DFT</mi></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.5.m5.1b"><apply id="Sx5.SSx2.p3.5.m5.1.1.cmml" xref="Sx5.SSx2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="Sx5.SSx2.p3.5.m5.1.1.1.cmml" xref="Sx5.SSx2.p3.5.m5.1.1">subscript</csymbol><ci id="Sx5.SSx2.p3.5.m5.1.1.2.cmml" xref="Sx5.SSx2.p3.5.m5.1.1.2">ğ¸</ci><ci id="Sx5.SSx2.p3.5.m5.1.1.3.cmml" xref="Sx5.SSx2.p3.5.m5.1.1.3">DFT</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.5.m5.1c">E_{\mathrm{DFT}}</annotation></semantics></math> is the per-cell energy as calculated using DFT (in this proof-of-concept, <span id="Sx5.SSx2.p3.5.2" class="ltx_text ltx_font_bold">D0</span> is the subset of all 64-atom amorphous carbon structures taken from the C-GAP-17 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>).
In many currently used NN models for chemistry, this task involves predicting per-cell energies as a sum of local atomic energies: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite></p>
<table id="Sx5.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E6.m1.1" class="ltx_Math" alttext="\hat{E}_{c}=\sum_{i\in c}\hat{\varepsilon}_{\lambda}(\mathbf{x}_{i})." display="block"><semantics id="Sx5.E6.m1.1a"><mrow id="Sx5.E6.m1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.cmml"><mrow id="Sx5.E6.m1.1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.cmml"><msub id="Sx5.E6.m1.1.1.1.1.3" xref="Sx5.E6.m1.1.1.1.1.3.cmml"><mover accent="true" id="Sx5.E6.m1.1.1.1.1.3.2" xref="Sx5.E6.m1.1.1.1.1.3.2.cmml"><mi id="Sx5.E6.m1.1.1.1.1.3.2.2" xref="Sx5.E6.m1.1.1.1.1.3.2.2.cmml">E</mi><mo id="Sx5.E6.m1.1.1.1.1.3.2.1" xref="Sx5.E6.m1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="Sx5.E6.m1.1.1.1.1.3.3" xref="Sx5.E6.m1.1.1.1.1.3.3.cmml">c</mi></msub><mo rspace="0.111em" id="Sx5.E6.m1.1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx5.E6.m1.1.1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.1.cmml"><munder id="Sx5.E6.m1.1.1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="Sx5.E6.m1.1.1.1.1.1.2.2" xref="Sx5.E6.m1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="Sx5.E6.m1.1.1.1.1.1.2.3" xref="Sx5.E6.m1.1.1.1.1.1.2.3.cmml"><mi id="Sx5.E6.m1.1.1.1.1.1.2.3.2" xref="Sx5.E6.m1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="Sx5.E6.m1.1.1.1.1.1.2.3.1" xref="Sx5.E6.m1.1.1.1.1.1.2.3.1.cmml">âˆˆ</mo><mi id="Sx5.E6.m1.1.1.1.1.1.2.3.3" xref="Sx5.E6.m1.1.1.1.1.1.2.3.3.cmml">c</mi></mrow></munder><mrow id="Sx5.E6.m1.1.1.1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.1.1.cmml"><msub id="Sx5.E6.m1.1.1.1.1.1.1.3" xref="Sx5.E6.m1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="Sx5.E6.m1.1.1.1.1.1.1.3.2" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx5.E6.m1.1.1.1.1.1.1.3.2.2" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2.2.cmml">Îµ</mi><mo id="Sx5.E6.m1.1.1.1.1.1.1.3.2.1" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="Sx5.E6.m1.1.1.1.1.1.1.3.3" xref="Sx5.E6.m1.1.1.1.1.1.1.3.3.cmml">Î»</mi></msub><mo lspace="0em" rspace="0em" id="Sx5.E6.m1.1.1.1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx5.E6.m1.1.1.1.1.1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.E6.m1.1.1.1.1.1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx5.E6.m1.1.1.1.1.1.1.1.1.3" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="Sx5.E6.m1.1.1.1.2" xref="Sx5.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E6.m1.1b"><apply id="Sx5.E6.m1.1.1.1.1.cmml" xref="Sx5.E6.m1.1.1.1"><eq id="Sx5.E6.m1.1.1.1.1.2.cmml" xref="Sx5.E6.m1.1.1.1.1.2"></eq><apply id="Sx5.E6.m1.1.1.1.1.3.cmml" xref="Sx5.E6.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E6.m1.1.1.1.1.3.1.cmml" xref="Sx5.E6.m1.1.1.1.1.3">subscript</csymbol><apply id="Sx5.E6.m1.1.1.1.1.3.2.cmml" xref="Sx5.E6.m1.1.1.1.1.3.2"><ci id="Sx5.E6.m1.1.1.1.1.3.2.1.cmml" xref="Sx5.E6.m1.1.1.1.1.3.2.1">^</ci><ci id="Sx5.E6.m1.1.1.1.1.3.2.2.cmml" xref="Sx5.E6.m1.1.1.1.1.3.2.2">ğ¸</ci></apply><ci id="Sx5.E6.m1.1.1.1.1.3.3.cmml" xref="Sx5.E6.m1.1.1.1.1.3.3">ğ‘</ci></apply><apply id="Sx5.E6.m1.1.1.1.1.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1"><apply id="Sx5.E6.m1.1.1.1.1.1.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E6.m1.1.1.1.1.1.2.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2">subscript</csymbol><sum id="Sx5.E6.m1.1.1.1.1.1.2.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2.2"></sum><apply id="Sx5.E6.m1.1.1.1.1.1.2.3.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2.3"><in id="Sx5.E6.m1.1.1.1.1.1.2.3.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2.3.1"></in><ci id="Sx5.E6.m1.1.1.1.1.1.2.3.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2.3.2">ğ‘–</ci><ci id="Sx5.E6.m1.1.1.1.1.1.2.3.3.cmml" xref="Sx5.E6.m1.1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="Sx5.E6.m1.1.1.1.1.1.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1"><times id="Sx5.E6.m1.1.1.1.1.1.1.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.2"></times><apply id="Sx5.E6.m1.1.1.1.1.1.1.3.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="Sx5.E6.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2"><ci id="Sx5.E6.m1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2.1">^</ci><ci id="Sx5.E6.m1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3.2.2">ğœ€</ci></apply><ci id="Sx5.E6.m1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.3.3">ğœ†</ci></apply><apply id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.2">ğ±</ci><ci id="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E6.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E6.m1.1c">\hat{E}_{c}=\sum_{i\in c}\hat{\varepsilon}_{\lambda}(\mathbf{x}_{i}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx2.p3.9" class="ltx_p">The optimisation problem then becomes</p>
<table id="Sx5.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E7.m1.4" class="ltx_Math" alttext="\underset{\lambda}{\textrm{argmin}}\quad\mathcal{L}\left(\sum_{c\in\mathbf{D0}}\left|E_{{\rm DFT},c}-\sum_{\mathbf{x}_{i}\in\mathbf{X}_{c}}\varepsilon_{\lambda}(\mathbf{x}_{i})\right|^{2}\right)," display="block"><semantics id="Sx5.E7.m1.4a"><mrow id="Sx5.E7.m1.4.4.1"><mrow id="Sx5.E7.m1.4.4.1.1.1" xref="Sx5.E7.m1.4.4.1.1.2.cmml"><munder accentunder="true" id="Sx5.E7.m1.3.3" xref="Sx5.E7.m1.3.3.cmml"><mtext id="Sx5.E7.m1.3.3.2" xref="Sx5.E7.m1.3.3.2a.cmml">argmin</mtext><mo id="Sx5.E7.m1.3.3.1" xref="Sx5.E7.m1.3.3.1.cmml">ğœ†</mo></munder><mspace width="1em" id="Sx5.E7.m1.4.4.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.2.cmml"></mspace><mrow id="Sx5.E7.m1.4.4.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx5.E7.m1.4.4.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="Sx5.E7.m1.4.4.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.cmml"><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.cmml"><munder id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" movablelimits="false" rspace="0em" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.2.cmml">c</mi><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.1.cmml">âˆˆ</mo><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.3.cmml">ğƒğŸ</mi></mrow></munder><msup id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">E</mi><mrow id="Sx5.E7.m1.2.2.2.4" xref="Sx5.E7.m1.2.2.2.3.cmml"><mi id="Sx5.E7.m1.1.1.1.1" xref="Sx5.E7.m1.1.1.1.1.cmml">DFT</mi><mo id="Sx5.E7.m1.2.2.2.4.1" xref="Sx5.E7.m1.2.2.2.3.cmml">,</mo><mi id="Sx5.E7.m1.2.2.2.2" xref="Sx5.E7.m1.2.2.2.2.cmml">c</mi></mrow></msub><mo rspace="0.055em" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><munder id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">ğ±</mi><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">i</mi></msub><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">âˆˆ</mo><msub id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">ğ—</mi><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">c</mi></msub></mrow></munder><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">Îµ</mi><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">Î»</mi></msub><mo lspace="0em" rspace="0em" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="Sx5.E7.m1.4.4.1.1.1.1.1.1.3" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx5.E7.m1.4.4.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E7.m1.4b"><list id="Sx5.E7.m1.4.4.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1"><apply id="Sx5.E7.m1.3.3.cmml" xref="Sx5.E7.m1.3.3"><ci id="Sx5.E7.m1.3.3.1.cmml" xref="Sx5.E7.m1.3.3.1">ğœ†</ci><ci id="Sx5.E7.m1.3.3.2a.cmml" xref="Sx5.E7.m1.3.3.2"><mtext id="Sx5.E7.m1.3.3.2.cmml" xref="Sx5.E7.m1.3.3.2">argmin</mtext></ci></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1"><times id="Sx5.E7.m1.4.4.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.2"></times><ci id="Sx5.E7.m1.4.4.1.1.1.1.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.3">â„’</ci><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1"><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.2"></sum><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3"><in id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.1"></in><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.2">ğ‘</ci><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.2.3.3">ğƒğŸ</ci></apply></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1"><abs id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.2"></abs><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><minus id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"></minus><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¸</ci><list id="Sx5.E7.m1.2.2.2.3.cmml" xref="Sx5.E7.m1.2.2.2.4"><ci id="Sx5.E7.m1.1.1.1.1.cmml" xref="Sx5.E7.m1.1.1.1.1">DFT</ci><ci id="Sx5.E7.m1.2.2.2.2.cmml" xref="Sx5.E7.m1.2.2.2.2">ğ‘</ci></list></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.2"></sum><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><in id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></in><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2">ğ±</ci><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3">ğ‘–</ci></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2">ğ—</ci><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3">ğ‘</ci></apply></apply></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğœ€</ci><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğœ†</ci></apply><apply id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ±</ci><ci id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply><cn type="integer" id="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="Sx5.E7.m1.4.4.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E7.m1.4c">\underset{\lambda}{\textrm{argmin}}\quad\mathcal{L}\left(\sum_{c\in\mathbf{D0}}\left|E_{{\rm DFT},c}-\sum_{\mathbf{x}_{i}\in\mathbf{X}_{c}}\varepsilon_{\lambda}(\mathbf{x}_{i})\right|^{2}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="Sx5.SSx2.p3.8" class="ltx_p">where <math id="Sx5.SSx2.p3.6.m1.2" class="ltx_Math" alttext="E_{\mathrm{DFT},c}" display="inline"><semantics id="Sx5.SSx2.p3.6.m1.2a"><msub id="Sx5.SSx2.p3.6.m1.2.3" xref="Sx5.SSx2.p3.6.m1.2.3.cmml"><mi id="Sx5.SSx2.p3.6.m1.2.3.2" xref="Sx5.SSx2.p3.6.m1.2.3.2.cmml">E</mi><mrow id="Sx5.SSx2.p3.6.m1.2.2.2.4" xref="Sx5.SSx2.p3.6.m1.2.2.2.3.cmml"><mi id="Sx5.SSx2.p3.6.m1.1.1.1.1" xref="Sx5.SSx2.p3.6.m1.1.1.1.1.cmml">DFT</mi><mo id="Sx5.SSx2.p3.6.m1.2.2.2.4.1" xref="Sx5.SSx2.p3.6.m1.2.2.2.3.cmml">,</mo><mi id="Sx5.SSx2.p3.6.m1.2.2.2.2" xref="Sx5.SSx2.p3.6.m1.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.6.m1.2b"><apply id="Sx5.SSx2.p3.6.m1.2.3.cmml" xref="Sx5.SSx2.p3.6.m1.2.3"><csymbol cd="ambiguous" id="Sx5.SSx2.p3.6.m1.2.3.1.cmml" xref="Sx5.SSx2.p3.6.m1.2.3">subscript</csymbol><ci id="Sx5.SSx2.p3.6.m1.2.3.2.cmml" xref="Sx5.SSx2.p3.6.m1.2.3.2">ğ¸</ci><list id="Sx5.SSx2.p3.6.m1.2.2.2.3.cmml" xref="Sx5.SSx2.p3.6.m1.2.2.2.4"><ci id="Sx5.SSx2.p3.6.m1.1.1.1.1.cmml" xref="Sx5.SSx2.p3.6.m1.1.1.1.1">DFT</ci><ci id="Sx5.SSx2.p3.6.m1.2.2.2.2.cmml" xref="Sx5.SSx2.p3.6.m1.2.2.2.2">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.6.m1.2c">E_{\mathrm{DFT},c}</annotation></semantics></math> is the ground-truth value for cell <math id="Sx5.SSx2.p3.7.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="Sx5.SSx2.p3.7.m2.1a"><mi id="Sx5.SSx2.p3.7.m2.1.1" xref="Sx5.SSx2.p3.7.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.7.m2.1b"><ci id="Sx5.SSx2.p3.7.m2.1.1.cmml" xref="Sx5.SSx2.p3.7.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.7.m2.1c">c</annotation></semantics></math> against which the model parameters <math id="Sx5.SSx2.p3.8.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="Sx5.SSx2.p3.8.m3.1a"><mi id="Sx5.SSx2.p3.8.m3.1.1" xref="Sx5.SSx2.p3.8.m3.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p3.8.m3.1b"><ci id="Sx5.SSx2.p3.8.m3.1.1.cmml" xref="Sx5.SSx2.p3.8.m3.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p3.8.m3.1c">\lambda</annotation></semantics></math> are optimised.</p>
</div>
<div id="Sx5.SSx2.p4" class="ltx_para">
<p id="Sx5.SSx2.p4.3" class="ltx_p">We first describe the control experiment: training a randomly initialised NN model exclusively on per-structure energies, <math id="Sx5.SSx2.p4.1.m1.2" class="ltx_Math" alttext="E_{{\rm DFT,c}}" display="inline"><semantics id="Sx5.SSx2.p4.1.m1.2a"><msub id="Sx5.SSx2.p4.1.m1.2.3" xref="Sx5.SSx2.p4.1.m1.2.3.cmml"><mi id="Sx5.SSx2.p4.1.m1.2.3.2" xref="Sx5.SSx2.p4.1.m1.2.3.2.cmml">E</mi><mrow id="Sx5.SSx2.p4.1.m1.2.2.2.4" xref="Sx5.SSx2.p4.1.m1.2.2.2.3.cmml"><mi id="Sx5.SSx2.p4.1.m1.1.1.1.1" xref="Sx5.SSx2.p4.1.m1.1.1.1.1.cmml">DFT</mi><mo id="Sx5.SSx2.p4.1.m1.2.2.2.4.1" xref="Sx5.SSx2.p4.1.m1.2.2.2.3.cmml">,</mo><mi mathvariant="normal" id="Sx5.SSx2.p4.1.m1.2.2.2.2" xref="Sx5.SSx2.p4.1.m1.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p4.1.m1.2b"><apply id="Sx5.SSx2.p4.1.m1.2.3.cmml" xref="Sx5.SSx2.p4.1.m1.2.3"><csymbol cd="ambiguous" id="Sx5.SSx2.p4.1.m1.2.3.1.cmml" xref="Sx5.SSx2.p4.1.m1.2.3">subscript</csymbol><ci id="Sx5.SSx2.p4.1.m1.2.3.2.cmml" xref="Sx5.SSx2.p4.1.m1.2.3.2">ğ¸</ci><list id="Sx5.SSx2.p4.1.m1.2.2.2.3.cmml" xref="Sx5.SSx2.p4.1.m1.2.2.2.4"><ci id="Sx5.SSx2.p4.1.m1.1.1.1.1.cmml" xref="Sx5.SSx2.p4.1.m1.1.1.1.1">DFT</ci><ci id="Sx5.SSx2.p4.1.m1.2.2.2.2.cmml" xref="Sx5.SSx2.p4.1.m1.2.2.2.2">c</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p4.1.m1.2c">E_{{\rm DFT,c}}</annotation></semantics></math>.
The resulting model (purple in Fig. <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a) learns atomic energies that, when summed over a cell, predict per-cell energies with a test-set RMSE of <math id="Sx5.SSx2.p4.2.m2.1" class="ltx_Math" alttext="52" display="inline"><semantics id="Sx5.SSx2.p4.2.m2.1a"><mn id="Sx5.SSx2.p4.2.m2.1.1" xref="Sx5.SSx2.p4.2.m2.1.1.cmml">52</mn><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p4.2.m2.1b"><cn type="integer" id="Sx5.SSx2.p4.2.m2.1.1.cmml" xref="Sx5.SSx2.p4.2.m2.1.1">52</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p4.2.m2.1c">52</annotation></semantics></math> meV atom<sup id="Sx5.SSx2.p4.3.1" class="ltx_sup"><span id="Sx5.SSx2.p4.3.1.1" class="ltx_text ltx_font_italic">-1</span></sup>.
This is, to within noise, the same as the original C-GAP-17 model (on its own training data!).
Interestingly, the NN model trained in this way learns to partition per-cell energies into atomic contributions in a different manner to C-GAP-17: a parity plot of these (Fig.Â <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b) shows only a loose correlation.
This non-uniqueness of local energies from NN models seems to be in keeping with previous findings.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></p>
</div>
<div id="Sx5.SSx2.p5" class="ltx_para">
<p id="Sx5.SSx2.p5.2" class="ltx_p">Training a new NN solely on C-GAP-17 local energies for the synthetic dataset unsurprisingly leads to a model (orange in Fig. <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) that reproduces these quantities much more closely. Starting from this pretrained model and its set of optimised parameters, and subsequently performing the same per-cell energy optimisation procedure (Eqn. <a href="#Sx5.E7" title="In Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), we obtain an NN (red in Fig. <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) that performs significantly better than direct training from a random initialisation in predicting per-cell energies, with a test-set RMSE of <math id="Sx5.SSx2.p5.1.m1.1" class="ltx_Math" alttext="46.5" display="inline"><semantics id="Sx5.SSx2.p5.1.m1.1a"><mn id="Sx5.SSx2.p5.1.m1.1.1" xref="Sx5.SSx2.p5.1.m1.1.1.cmml">46.5</mn><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.p5.1.m1.1b"><cn type="float" id="Sx5.SSx2.p5.1.m1.1.1.cmml" xref="Sx5.SSx2.p5.1.m1.1.1">46.5</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.p5.1.m1.1c">46.5</annotation></semantics></math> meV atom<sup id="Sx5.SSx2.p5.2.1" class="ltx_sup"><span id="Sx5.SSx2.p5.2.1.1" class="ltx_text ltx_font_italic">-1</span></sup> (Fig.Â <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>d).
The parity plots (Fig. <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b) show that the fine-tuned network, having been originally guided by the C-GAP-17 local energies, partitions local energies in a more similar manner to C-GAP-17 as compared to the direct training approach.</p>
</div>
<div id="Sx5.SSx2.p6" class="ltx_para">
<p id="Sx5.SSx2.p6.1" class="ltx_p">We perform preliminary tests for the role of dataset size in this pre-training procedure.
Figure <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>c suggests that <span id="Sx5.SSx2.p6.1.1" class="ltx_text ltx_font_bold">D1</span> needs to be at least as large as <span id="Sx5.SSx2.p6.1.2" class="ltx_text ltx_font_bold">D0</span> (in terms of number of atomic environments) in order for the pre-training approach to improve upon the accuracy from direct training on <span id="Sx5.SSx2.p6.1.3" class="ltx_text ltx_font_bold">D0</span> (purple dashed line). Note that we use hyperparameters that maximise accuracy when training on the full <span id="Sx5.SSx2.p6.1.4" class="ltx_text ltx_font_bold">D1</span> set (here, using 4 million atomic environments) for all pre-training dataset sizes investigated; thus, while Fig.Â <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>c might seem to suggest that small amounts of pre-training are detrimental, we assume that they actually have no effect in practice.
Figure <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>d shows that, when pre-training on the full <span id="Sx5.SSx2.p6.1.5" class="ltx_text ltx_font_bold">D1</span> set (red), <math id="Sx5.SSx2.p6.1.m1.1" class="ltx_math_unparsed" alttext="\sim 8\times" display="inline"><semantics id="Sx5.SSx2.p6.1.m1.1a"><mrow id="Sx5.SSx2.p6.1.m1.1b"><mo id="Sx5.SSx2.p6.1.m1.1.1">âˆ¼</mo><mn id="Sx5.SSx2.p6.1.m1.1.2">8</mn><mo lspace="0.222em" id="Sx5.SSx2.p6.1.m1.1.3">Ã—</mo></mrow><annotation encoding="application/x-tex" id="Sx5.SSx2.p6.1.m1.1c">\sim 8\times</annotation></semantics></math> fewer QM labels are required to achieve the same final accuracy compared to using the direct approach (purple). Thus we have shown initial evidence that learning to predict synthetic atomic energies can be a useful and meaningful â€œpre-training taskâ€ for chemistry.</p>
</div>
<div id="Sx5.SSx2.p7" class="ltx_para">
<p id="Sx5.SSx2.p7.1" class="ltx_p">We note that our pre-training can be recast as transfer learning from a lower to a higher level of quantum-aware labelling. Transfer learning for atomistic models has been demonstrated by Smith <span id="Sx5.SSx2.p7.1.1" class="ltx_text ltx_font_italic">et al.</span>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> although we are not aware of prior work going from ML-potential- to DFT-level accuracy, or performing this transfer learning <span id="Sx5.SSx2.p7.1.2" class="ltx_text ltx_font_italic">via</span> synthetic atomic energies (rather than per-structure energies or forces).
We also note that the pre-training of NN models is a well-documented approach in the ML literature for various applications and domains, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> and that it has very recently been described in the context of interatomic potential models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
and as a means to learn general-purpose representations for atomistic structure.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite></p>
</div>
<figure id="Sx5.F6" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_06.png" id="Sx5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="397" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
UMAP projections<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> to visualise the configurational space of the synthetic dataset, as described by the original SOAP vectors (aâ€“c), and by the compressed representations learned by an NN trained on synthetic C-GAP-17 atomic energies (dâ€“f).
From left to right, we colour-code by the C-GAP-17 atomic energy relative to graphite, by coordination environment category as determined by a 1.85â€‰Ã… cut-off, and by SOAP similarity to diamond (red) and graphite (blue).
Some clustering exists in the original SOAP space, although many strictly sp<sup id="Sx5.F6.10.1" class="ltx_sup">2</sup> atoms are found within the space predominantly populated by sp<sup id="Sx5.F6.11.2" class="ltx_sup">3</sup> carbon. At a local scale, the gradient in atomic energy is very noisy in this space.
Compare this to the representation learned by the NN: clear clustering occurs that aligns very tightly with carbon coordination environment. Within the sp<sup id="Sx5.F6.12.3" class="ltx_sup">2</sup> region, further sub-clustering exists, each of which has clear meaning as highlighted by the SOAP similarity colour coding. The local gradient in atomic energy is much smoother, as is to be expected given the network has been trained on this quantity.
</figcaption>
</figure>
<figure id="Sx5.F7" class="ltx_figure"><img src="/html/2211.16443/assets/Figure_07.png" id="Sx5.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="873" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
UMAP embeddings showing (a) clusters found in the NN-based map of Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>dâ€“f, and (b) projection of these cluster labels into the original SOAP space.
Clear mixing occurs in the latter, showing that the NN is learning a different representation than the SOAP features with which it was trained, rather than some linear recombination of these.
</figcaption>
</figure>
</section>
<section id="Sx5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Embedding and visualisation</h3>

<div id="Sx5.SSx3.p1" class="ltx_para">
<p id="Sx5.SSx3.p1.2" class="ltx_p">We finally illustrate the usefulness of synthetic atomistic data for the visualisation of structural and chemical space.
This is an increasingly important task in ML for chemistry, leading to what are commonly referred to as â€œstructure mapsâ€.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
Out of the many recipes for creating such a map, a popular one entails (i) selecting a metric to define the (dis-) similarity, or distance, between two atomic environments, (ii) calculating this metric for each pair from a (representative) set of environments to create a distance matrix, and (iii) embedding this matrix onto a low-dimensional (2D or 3D) manifold.
A popular instantiation of this recipe is to use the SOAP kernel as a similarity metric, and to use a non-linear dimensionality reduction technique, such as UMAP or t-SNE, to embed the distance matrix. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
Such SOAP-based maps have been reported for pristine and chemically functionalised forms of carbon<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and can help in understanding local environments â€“ <span id="Sx5.SSx3.p1.2.1" class="ltx_text ltx_font_italic">e.g.</span>, in assigning the chemical character of a given atom beyond the simplified â€œspâ€ / â€œsp<sup id="Sx5.SSx3.p1.2.2" class="ltx_sup"><span id="Sx5.SSx3.p1.2.2.1" class="ltx_text ltx_font_italic">2</span></sup>â€ / â€œsp<sup id="Sx5.SSx3.p1.2.3" class="ltx_sup"><span id="Sx5.SSx3.p1.2.3.1" class="ltx_text ltx_font_italic">3</span></sup>â€ labels. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite></p>
</div>
<div id="Sx5.SSx3.p2" class="ltx_para">
<p id="Sx5.SSx3.p2.1" class="ltx_p">Our present work explores the ability of an NN model to generate analogous 2D maps of chemical structure. Outputs from hidden layers of an atomistic NN can be used to visualise structural space.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> We investigate such visualisation for a model that has been trained on the synthetic dataset introduced above, and draw comparison with earlier work on carbon. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
We adapt the above recipe by using the Euclidean distance between NN penultimate hidden-layer representations as a dissimilarity metric, and embed the resulting matrix using UMAP,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> a general approach not limited to chemistry. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite></p>
</div>
<div id="Sx5.SSx3.p3" class="ltx_para">
<p id="Sx5.SSx3.p3.1" class="ltx_p">Figure <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the resulting structure maps.
We use 30,000 atomic environments selected at random from the dataset presented above.
The upper row (Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>aâ€“c) shows a SOAP-based UMAP embedding, colour-coded by relevant properties, whereas the lower row (Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>dâ€“f) shows a UMAP embedding derived from hidden-layer representations of an NN model. The former confirms observations made before on smaller datasets: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> distinct types of environments, both energetically and structurally, can be discerned in the map by different colour coding. For example, there is a small island of structures with high local energy (yellow, Fig. <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>a), and those correspond to the twofold-bonded â€œspâ€ environments, as seen from Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b. This observation is consistent with the energy distributions shown in Fig.Â <a href="#Sx1.F2" title="Figure 2 â€£ Introduction â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a.</p>
</div>
<div id="Sx5.SSx3.p4" class="ltx_para">
<p id="Sx5.SSx3.p4.5" class="ltx_p">Whilst the SOAP map does therefore capture relevant aspects, the clustering in the map produced from the learned NN representations (Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>dâ€“f) is significantly more intricate, while also aligning more strongly with our understanding (or the chemistry textbook picture) of carbon atom hybridisation. Specifically, compared to the embeddings produced from SOAP descriptors, the NN space shows a much clearer separation of sp<sup id="Sx5.SSx3.p4.5.1" class="ltx_sup">2</sup> <span id="Sx5.SSx3.p4.5.2" class="ltx_text ltx_font_italic">vs</span> sp<sup id="Sx5.SSx3.p4.5.3" class="ltx_sup">3</sup> atoms (dark red <span id="Sx5.SSx3.p4.5.4" class="ltx_text ltx_font_italic">vs</span> light blue in the central panels), and it also shows sub-clustering within the sp<sup id="Sx5.SSx3.p4.5.5" class="ltx_sup">2</sup> region. We can interpret this sub-clustering by colour-coding each datapoint according to the corresponding environmentâ€™s SOAP similarity to both graphite (blue) and diamond (red). These results are shown on the right-hand side of Fig. <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>: some of the formally sp<sup id="Sx5.SSx3.p4.5.6" class="ltx_sup">2</sup> carbons are very similar to diamond-like environments, suggesting that these are in fact dangling-bond sp<sup id="Sx5.SSx3.p4.5.7" class="ltx_sup">3</sup> environments, further corroborated by their high local energies. This kind of structure is not made as obvious in the SOAP map in Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>c.</p>
</div>
<div id="Sx5.SSx3.p5" class="ltx_para">
<p id="Sx5.SSx3.p5.1" class="ltx_p">In Fig. <a href="#Sx5.F7" title="Figure 7 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we show further evidence that the NN has learned a different description of local structure from the original SOAP descriptors.
We performed cluster analysis in the NN-based structure map, using the BIRCH algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> to separate the data into 7 distinct clusters (colour-coded arbitrarily in Fig.Â <a href="#Sx5.F7" title="Figure 7 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>a), and then projected the resulting cluster labels into the space of the original SOAP map (Fig.Â <a href="#Sx5.F7" title="Figure 7 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>b).
Doing so shows that atoms contained within the same cluster in NN space are not necessarily co-located in SOAP space:
while the sp atoms remain isolated in the SOAP map (<span id="Sx5.SSx3.p5.1.1" class="ltx_text ltx_font_italic">cf.</span>Â Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b), the remaining clusters are clearly heavily intermixed, with some (<span id="Sx5.SSx3.p5.1.2" class="ltx_text ltx_font_italic">e.g.</span>, bright green) spanning most of the SOAP space.
Hence, the mapping between SOAP vector and NN representation is complex and highly non-linear, such that the NN is truly learning a new representation.</p>
</div>
<div id="Sx5.SSx3.p6" class="ltx_para">
<p id="Sx5.SSx3.p6.1" class="ltx_p">We therefore argue that maps based on hidden layers of atomistic NN models, trained on synthetic datasets as exemplified here, can capture aspects of both the structure and the energetics of a given material.
This is not merely a consequence of the higher flexibility of NNs â€“ in fact, an analogue to the SOAP-based maps shown herein would be the visualisation of the latent space of an autoencoder model.
Instead, we here take the hidden layer following <span id="Sx5.SSx3.p6.1.1" class="ltx_text ltx_font_italic">supervised</span> learning, thereby automatically incorporating information about the data labels in the structure map (although the question how exactly this information is learned is deliberately left to the network to optimise).
There is some similarity of this approach to principal covariates regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> which has been combined with kernel metrics for use in atomistic ML, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> and the resulting â€œkernel PCovRâ€ was applied to silicon under pressure. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> Here, however, the data labels can enter into the model in nonlinear form, and also the embedding of the structural information itself is more intricate. We suggest that maps of similar type could be explored for different systems and application problems in chemistry.</p>
</div>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Discussion</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">Our study demonstrates that â€œsyntheticâ€ atomic energies, predicted at large scale by a machine-learning model, are themselves learnable and can be used to study the behaviour of different atomistic ML techniques.
In the present work, we have compared the ability of GPR, NN, and DKL models to predict properties of chemical environments in the large-data limit, using atomic energies as a proxy for other quantities.</p>
</div>
<div id="Sx6.p2" class="ltx_para">
<p id="Sx6.p2.1" class="ltx_p">By means of numerical experiments, we showed that network-based models are able to learn useful representations from the original SOAP descriptors, and that these can lead to improved accuracies compared to SOAP-GPR models if the number of training data points is large.
Whereas DKL can substantially outperform stand-alone NNs in some applications,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and has begun to be successfully applied to research questions in chemistry, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> we have found that in the setting of the present work (<span id="Sx6.p2.1.1" class="ltx_text ltx_font_italic">i.e.</span>, regression of large amounts of per-atom energy data), DKL models were only slightly more accurate than NNs whilst being significantly more expensive when making predictions.
In comparison, SOAP-based GPR models, while slower and less accurate in the large-data regime, show better generalisation (and thus accuracies) for small amounts of data. This finding is consistent with the marked success of sparse-GPR-based atomistic ML models on datasets of (relatively) modest size. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></p>
</div>
<div id="Sx6.p3" class="ltx_para">
<p id="Sx6.p3.1" class="ltx_p">In the present work, we have focused on learning ML atomic energies.
These values do not directly correspond to any quantum-mechanical observable, and yet empirically they do appear to correlate well with local topological disorder and distortions,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> and they can be used to drive structural exploration. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
Irrespective of their physical interpretation (or absence thereof), we propose that ML atomic energies are a useful regression target for NN models: the compressed representations of structure learned through this task are imbued with deep, and to some extent interpretable, meaning (Fig.Â <a href="#Sx5.F6" title="Figure 6 â€£ Pre-training â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).
The fact that synthetic labels are quick to generate, and the networks quick to train, suggests that this is a useful auxiliary or pre-training task to create models with â€œknowledgeâ€ about a chemical space.
These network models could then be used to fine-tune on much smaller datasets, using their existing and general chemical knowledge to overcome the relative weakness of NN models in the low-data regime. We have shown initial evidence for this in Fig.Â <a href="#Sx5.F5" title="Figure 5 â€£ GPR insights â€£ Experiments â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and further work is ongoing.</p>
</div>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix: Technical details</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.3" class="ltx_p">When training all NN models, we employed as means of regularisation: early stopping (by measuring performance on a validation set), dropout, and L2 weight decay.
To find optimal parameters for the number of hidden layers, layer width, learning rate, batch size, dropout fraction, and weight decay magnitude, we performed a random (Hammersley) search over a broadly defined hyper-parameter space.
We found that the optimal learning rate in all instances was close to the commonly used <math id="Sx7.p1.1.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="Sx7.p1.1.m1.1a"><mrow id="Sx7.p1.1.m1.1.1" xref="Sx7.p1.1.m1.1.1.cmml"><mn id="Sx7.p1.1.m1.1.1.2" xref="Sx7.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="Sx7.p1.1.m1.1.1.1" xref="Sx7.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="Sx7.p1.1.m1.1.1.3" xref="Sx7.p1.1.m1.1.1.3.cmml"><mn id="Sx7.p1.1.m1.1.1.3.2" xref="Sx7.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="Sx7.p1.1.m1.1.1.3.3" xref="Sx7.p1.1.m1.1.1.3.3.cmml"><mo id="Sx7.p1.1.m1.1.1.3.3a" xref="Sx7.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="Sx7.p1.1.m1.1.1.3.3.2" xref="Sx7.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p1.1.m1.1b"><apply id="Sx7.p1.1.m1.1.1.cmml" xref="Sx7.p1.1.m1.1.1"><times id="Sx7.p1.1.m1.1.1.1.cmml" xref="Sx7.p1.1.m1.1.1.1"></times><cn type="integer" id="Sx7.p1.1.m1.1.1.2.cmml" xref="Sx7.p1.1.m1.1.1.2">3</cn><apply id="Sx7.p1.1.m1.1.1.3.cmml" xref="Sx7.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx7.p1.1.m1.1.1.3.1.cmml" xref="Sx7.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="Sx7.p1.1.m1.1.1.3.2.cmml" xref="Sx7.p1.1.m1.1.1.3.2">10</cn><apply id="Sx7.p1.1.m1.1.1.3.3.cmml" xref="Sx7.p1.1.m1.1.1.3.3"><minus id="Sx7.p1.1.m1.1.1.3.3.1.cmml" xref="Sx7.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="Sx7.p1.1.m1.1.1.3.3.2.cmml" xref="Sx7.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p1.1.m1.1c">3\times 10^{-4}</annotation></semantics></math> for the Adam optimiser.
3 hidden layers, each with <math id="Sx7.p1.2.m2.1" class="ltx_Math" alttext="\approx 800" display="inline"><semantics id="Sx7.p1.2.m2.1a"><mrow id="Sx7.p1.2.m2.1.1" xref="Sx7.p1.2.m2.1.1.cmml"><mi id="Sx7.p1.2.m2.1.1.2" xref="Sx7.p1.2.m2.1.1.2.cmml"></mi><mo id="Sx7.p1.2.m2.1.1.1" xref="Sx7.p1.2.m2.1.1.1.cmml">â‰ˆ</mo><mn id="Sx7.p1.2.m2.1.1.3" xref="Sx7.p1.2.m2.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p1.2.m2.1b"><apply id="Sx7.p1.2.m2.1.1.cmml" xref="Sx7.p1.2.m2.1.1"><approx id="Sx7.p1.2.m2.1.1.1.cmml" xref="Sx7.p1.2.m2.1.1.1"></approx><csymbol cd="latexml" id="Sx7.p1.2.m2.1.1.2.cmml" xref="Sx7.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="Sx7.p1.2.m2.1.1.3.cmml" xref="Sx7.p1.2.m2.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p1.2.m2.1c">\approx 800</annotation></semantics></math> nodes, gave the most accurate models in the limit of large data, while smaller models performed better for smaller training sets, presumably because the model size is acting as further regularisation to avoid extreme overfitting.
In all instances, we found high dropout (<math id="Sx7.p1.3.m3.1" class="ltx_Math" alttext="p\approx 0.5" display="inline"><semantics id="Sx7.p1.3.m3.1a"><mrow id="Sx7.p1.3.m3.1.1" xref="Sx7.p1.3.m3.1.1.cmml"><mi id="Sx7.p1.3.m3.1.1.2" xref="Sx7.p1.3.m3.1.1.2.cmml">p</mi><mo id="Sx7.p1.3.m3.1.1.1" xref="Sx7.p1.3.m3.1.1.1.cmml">â‰ˆ</mo><mn id="Sx7.p1.3.m3.1.1.3" xref="Sx7.p1.3.m3.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p1.3.m3.1b"><apply id="Sx7.p1.3.m3.1.1.cmml" xref="Sx7.p1.3.m3.1.1"><approx id="Sx7.p1.3.m3.1.1.1.cmml" xref="Sx7.p1.3.m3.1.1.1"></approx><ci id="Sx7.p1.3.m3.1.1.2.cmml" xref="Sx7.p1.3.m3.1.1.2">ğ‘</ci><cn type="float" id="Sx7.p1.3.m3.1.1.3.cmml" xref="Sx7.p1.3.m3.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p1.3.m3.1c">p\approx 0.5</annotation></semantics></math>) to be a more effective regulariser than weight decay.</p>
</div>
<div id="Sx7.p2" class="ltx_para">
<p id="Sx7.p2.4" class="ltx_p">For the NN models trained on DFT per-cell energies, we obtained errors on the test and training set amounting to 52.2 and 26.2 meV atom<sup id="Sx7.p2.4.1" class="ltx_sup"><span id="Sx7.p2.4.1.1" class="ltx_text ltx_font_italic">-1</span></sup>, respectively, and for the fine-tuned models we obtained 46.0/21.3 meV atom<sup id="Sx7.p2.4.2" class="ltx_sup"><span id="Sx7.p2.4.2.1" class="ltx_text ltx_font_italic">-1</span></sup> on test/train data.
We note that, despite aggressive regularisation during training, this generalisation gap is large â€“ we attribute this to the small dataset size (1200 labels) used in the fine-tuning experiment. In comparison, the corresponding generalisation gap for the <math id="Sx7.p2.3.m3.1" class="ltx_Math" alttext="N=10^{6}" display="inline"><semantics id="Sx7.p2.3.m3.1a"><mrow id="Sx7.p2.3.m3.1.1" xref="Sx7.p2.3.m3.1.1.cmml"><mi id="Sx7.p2.3.m3.1.1.2" xref="Sx7.p2.3.m3.1.1.2.cmml">N</mi><mo id="Sx7.p2.3.m3.1.1.1" xref="Sx7.p2.3.m3.1.1.1.cmml">=</mo><msup id="Sx7.p2.3.m3.1.1.3" xref="Sx7.p2.3.m3.1.1.3.cmml"><mn id="Sx7.p2.3.m3.1.1.3.2" xref="Sx7.p2.3.m3.1.1.3.2.cmml">10</mn><mn id="Sx7.p2.3.m3.1.1.3.3" xref="Sx7.p2.3.m3.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p2.3.m3.1b"><apply id="Sx7.p2.3.m3.1.1.cmml" xref="Sx7.p2.3.m3.1.1"><eq id="Sx7.p2.3.m3.1.1.1.cmml" xref="Sx7.p2.3.m3.1.1.1"></eq><ci id="Sx7.p2.3.m3.1.1.2.cmml" xref="Sx7.p2.3.m3.1.1.2">ğ‘</ci><apply id="Sx7.p2.3.m3.1.1.3.cmml" xref="Sx7.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="Sx7.p2.3.m3.1.1.3.1.cmml" xref="Sx7.p2.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="Sx7.p2.3.m3.1.1.3.2.cmml" xref="Sx7.p2.3.m3.1.1.3.2">10</cn><cn type="integer" id="Sx7.p2.3.m3.1.1.3.3.cmml" xref="Sx7.p2.3.m3.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p2.3.m3.1c">N=10^{6}</annotation></semantics></math> NN model in Fig.Â <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Synthetic data enable experiments in atomistic machine learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is much smaller, <span id="Sx7.p2.4.3" class="ltx_text ltx_font_italic">viz.</span>Â 23.7/22.0 meV atom<sup id="Sx7.p2.4.4" class="ltx_sup"><span id="Sx7.p2.4.4.1" class="ltx_text ltx_font_italic">-1</span></sup> for test/train data, respectively.</p>
</div>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data availability</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">The dataset supporting the present work is provided at <a target="_blank" href="https://github.com/jla-gardner/carbon-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jla-gardner/carbon-data</a>. Each trajectory is supplied as a standalone â€œextendedâ€ XYZ file, with local energies provided as a per-atom quantity. These files can be read and processed, for example, by the Atomic Simulation Environment (ASE). <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank J.Â N.Â Foerster, A.Â L.Â Goodwin, and T.Â C.Â Nicholas for useful discussions.
J.L.A.G. acknowledges a UKRI Linacre - The EPA Cephalosporin Scholarship, support from an EPSRC DTP award (EP/T517811/1), and from the Department of Chemistry, University of Oxford.
V.L.D. acknowledges a UK Research and Innovation Frontier Research grant [grant number EP/X016188/1] and support from the John Fell OUP Research Fund.
The authors acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (http://dx.doi.org/10.5281/zenodo.22558).

</div>
</section>
<section id="Sx9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ReymondÂ andÂ Awale [2012]</span>
<span class="ltx_bibblock">J.-L.Â ReymondÂ andÂ M.Â Awale,Â â€œExploring
Chemical Space for Drug Discovery Using the Chemical Universe
Database,â€Â <a target="_blank" href="https://doi.org/10.1021/cn3000422" title="" class="ltx_ref ltx_href">ACS Chem. Neurosci.Â <span id="bib.bib1.1.1.1" class="ltx_text ltx_font_bold">3</span>,Â 649â€“657 (2012)</a>. 
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polishchuk, Madzhidov,Â andÂ Varnek [2013]</span>
<span class="ltx_bibblock">P.Â G.Â Polishchuk, T.Â I.Â Madzhidov,Â andÂ A.Â Varnek,Â â€œEstimation of
the size of drug-like chemical space based on GDB-17 data,â€Â <a target="_blank" href="https://doi.org/10.1007/s10822-013-9672-4" title="" class="ltx_ref ltx_href">J. Comput. Aided Mol. Des.Â <span id="bib.bib2.1.1.1" class="ltx_text ltx_font_bold">27</span>,Â 675â€“679 (2013)</a>. 
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Restrepo [2022]</span>
<span class="ltx_bibblock">G.Â Restrepo,Â â€œChemical
space: Limits, evolution and modelling of an object bigger than our universal
library,â€Â <a target="_blank" href="https://doi.org/10.1039/D2DD00030J" title="" class="ltx_ref ltx_href">Digital DiscoveryÂ <span id="bib.bib3.1.1.1" class="ltx_text ltx_font_bold">1</span>,Â 568â€“585 (2022)</a>. 
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CurtaroloÂ <em id="bib.bib4.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2013]</span>
<span class="ltx_bibblock">S.Â Curtarolo, G.Â L.Â W.Â Hart, M.Â B.Â Nardelli,
N.Â Mingo, S.Â Sanvito,Â andÂ O.Â Levy,Â â€œThe high-throughput highway to computational materials
design,â€Â <a target="_blank" href="https://doi.org/10.1038/nmat3568" title="" class="ltx_ref ltx_href">Nat. Mater.Â <span id="bib.bib4.3.1.1" class="ltx_text ltx_font_bold">12</span>,Â 191â€“201 (2013)</a>. 
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coley, Eyke,Â andÂ Jensen [2020a]</span>
<span class="ltx_bibblock">C.Â W.Â Coley, N.Â S.Â Eyke,Â andÂ K.Â F.Â Jensen,Â â€œAutonomous Discovery in
the Chemical Sciences Part I: Progress,â€Â <a target="_blank" href="https://doi.org/10.1002/anie.201909987" title="" class="ltx_ref ltx_href">Angew. Chem. Int. Ed.Â <span id="bib.bib5.1.1.1" class="ltx_text ltx_font_bold">59</span>,Â 22858â€“22893 (2020a)</a>. 
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coley, Eyke,Â andÂ Jensen [2020b]</span>
<span class="ltx_bibblock">C.Â W.Â Coley, N.Â S.Â Eyke,Â andÂ K.Â F.Â Jensen,Â â€œAutonomous Discovery in
the Chemical Sciences Part II: Outlook,â€Â <a target="_blank" href="https://doi.org/10.1002/anie.201909989" title="" class="ltx_ref ltx_href">Angew. Chem. Int. Ed.Â <span id="bib.bib6.1.1.1" class="ltx_text ltx_font_bold">59</span>,Â 23414â€“23436 (2020b)</a>. 
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KauweÂ <em id="bib.bib7.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">S.Â K.Â Kauwe, J.Â Graser,
R.Â Murdock,Â andÂ T.Â D.Â Sparks,Â â€œCan machine learning find extraordinary
materials?â€Â <a target="_blank" href="https://doi.org/10.1016/j.commatsci.2019.109498" title="" class="ltx_ref ltx_href">Comput. Mater. Sci.Â <span id="bib.bib7.3.1.1" class="ltx_text ltx_font_bold">174</span>,Â 109498 (2020)</a>. 
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dybowski [2020]</span>
<span class="ltx_bibblock">R.Â Dybowski,Â â€œInterpretable
machine learning as a tool for scientific discovery in chemistry,â€Â <a target="_blank" href="https://doi.org/10.1039/D0NJ02592E" title="" class="ltx_ref ltx_href">New J. Chem.Â <span id="bib.bib8.1.1.1" class="ltx_text ltx_font_bold">44</span>,Â 20914â€“20920 (2020)</a>. 
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OviedoÂ <em id="bib.bib9.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">F.Â Oviedo, J.Â L.Â Ferres,
T.Â Buonassisi,Â andÂ K.Â T.Â Butler,Â â€œInterpretable and Explainable Machine
Learning for Materials Science and Chemistry,â€Â <a target="_blank" href="https://doi.org/10.1021/accountsmr.1c00244" title="" class="ltx_ref ltx_href">Acc. Mater. Res.Â <span id="bib.bib9.3.1.1" class="ltx_text ltx_font_bold">3</span>,Â 597â€“607 (2022)</a>. 
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ParuzzoÂ <em id="bib.bib10.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">F.Â M.Â Paruzzo, A.Â Hofstetter,
F.Â Musil, S.Â De, M.Â Ceriotti,Â andÂ L.Â Emsley,Â â€œChemical shifts in molecular solids by machine
learning,â€Â <a target="_blank" href="https://doi.org/10.1038/s41467-018-06972-x" title="" class="ltx_ref ltx_href">Nat. Commun.Â <span id="bib.bib10.3.1.1" class="ltx_text ltx_font_bold">9</span>,Â 4501 (2018)</a>. 
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ChakerÂ <em id="bib.bib11.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2019]</span>
<span class="ltx_bibblock">Z.Â Chaker, M.Â Salanne,
J.-M.Â Delaye,Â andÂ T.Â Charpentier,Â â€œNMR shifts in
aluminosilicate glasses via machine learning,â€Â <a target="_blank" href="https://doi.org/10.1039/C9CP02803J" title="" class="ltx_ref ltx_href">Phys. Chem. Chem. Phys.Â <span id="bib.bib11.3.1.1" class="ltx_text ltx_font_bold">21</span>,Â 21709â€“21725 (2019)</a>. 
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VeitÂ <em id="bib.bib12.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">M.Â Veit, D.Â M.Â Wilkins,
Y.Â Yang, R.Â A.Â DiStasio,Â andÂ M.Â Ceriotti,Â â€œPredicting molecular dipole moments by combining atomic
partial charges and atomic dipoles,â€Â <a target="_blank" href="https://doi.org/10.1063/5.0009106" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib12.3.1.1" class="ltx_text ltx_font_bold">153</span>,Â 024113 (2020)</a>. 
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GrisafiÂ <em id="bib.bib13.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">A.Â Grisafi, D.Â M.Â Wilkins, G.Â CsÃ¡nyi,Â andÂ M.Â Ceriotti,Â â€œSymmetry-Adapted Machine Learning for Tensorial Properties of
Atomistic Systems,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevLett.120.036002" title="" class="ltx_ref ltx_href">Phys. Rev. Lett.Â <span id="bib.bib13.3.1.1" class="ltx_text ltx_font_bold">120</span>,Â 036002 (2018)</a>. 
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BehlerÂ andÂ Parrinello [2007]</span>
<span class="ltx_bibblock">J.Â BehlerÂ andÂ M.Â Parrinello,Â â€œGeneralized
Neural-Network Representation of High-Dimensional Potential-Energy
Surfaces,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevLett.98.146401" title="" class="ltx_ref ltx_href">Phys. Rev. Lett.Â <span id="bib.bib14.1.1.1" class="ltx_text ltx_font_bold">98</span>,Â 146401 (2007)</a>. 
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SchÃ¼ttÂ <em id="bib.bib15.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2017]</span>
<span class="ltx_bibblock">K.Â T.Â SchÃ¼tt, P.-J.Â Kindermans, H.Â E.Â Sauceda, S.Â Chmiela,
A.Â Tkatchenko,Â andÂ K.-R.Â MÃ¼ller,Â â€œSchNet: A
continuous-filter convolutional neural network for modeling quantum
interactions,â€Â inÂ <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the 31st International Conference on Neural Information Processing
Systems</em>,Â NIPSâ€™17Â (Red Hook, NY, USA,Â 2017)Â pp.Â 992â€“1002. 
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gasteiger, GroÃŸ,Â andÂ GÃ¼nnemann [2022]</span>
<span class="ltx_bibblock">J.Â Gasteiger, J.Â GroÃŸ,Â andÂ S.Â GÃ¼nnemann,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.2003.03123" title="" class="ltx_ref ltx_href">â€œDirectional Message Passing for Molecular
Graphs,â€Â </a> (2022),Â <a target="_blank" href="https://arxiv.org/abs/2003.03123" title="" class="ltx_ref ltx_href">arXiv:2003.03123 [cs.LG]</a> . 
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HuÂ <em id="bib.bib17.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">W.Â Hu, M.Â Shuaibi,
A.Â Das, S.Â Goyal, A.Â Sriram, J.Â Leskovec, D.Â Parikh,Â andÂ C.Â L.Â Zitnick,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.2103.01436" title="" class="ltx_ref ltx_href">â€œForceNet: A Graph Neural Network for
Large-Scale Quantum Calculations,â€Â </a> (2021),Â <a target="_blank" href="https://arxiv.org/abs/2103.01436" title="" class="ltx_ref ltx_href">arXiv:2103.01436 [cs.LG]</a> . 
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BartÃ³kÂ <em id="bib.bib18.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2010]</span>
<span class="ltx_bibblock">A.Â P.Â BartÃ³k, M.Â C.Â Payne, R.Â Kondor,Â andÂ G.Â CsÃ¡nyi,Â â€œGaussian Approximation Potentials:
The Accuracy of Quantum Mechanics, without the Electrons,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevLett.104.136403" title="" class="ltx_ref ltx_href">Phys. Rev. Lett.Â <span id="bib.bib18.3.1.1" class="ltx_text ltx_font_bold">104</span>,Â 136403 (2010)</a>. 
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ChmielaÂ <em id="bib.bib19.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2017]</span>
<span class="ltx_bibblock">S.Â Chmiela, A.Â Tkatchenko,
H.Â E.Â Sauceda, I.Â Poltavsky, K.Â T.Â SchÃ¼tt,Â andÂ K.-R.Â MÃ¼ller,Â â€œMachine learning of accurate energy-conserving
molecular force fields,â€Â <a target="_blank" href="https://doi.org/10.1126/sciadv.1603015" title="" class="ltx_ref ltx_href">Sci. Adv.Â <span id="bib.bib19.3.1.1" class="ltx_text ltx_font_bold">3</span>,Â e1603015 (2017)</a>. 
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ThompsonÂ <em id="bib.bib20.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2015]</span>
<span class="ltx_bibblock">A.Â P.Â Thompson, L.Â P.Â Swiler, C.Â R.Â Trott,
S.Â M.Â Foiles,Â andÂ G.Â J.Â Tucker,Â â€œSpectral neighbor analysis method for
automated generation of quantum-accurate interatomic potentials,â€Â <a target="_blank" href="https://doi.org/10.1016/j.jcp.2014.12.018" title="" class="ltx_ref ltx_href">J. Comput. Phys.Â <span id="bib.bib20.3.1.1" class="ltx_text ltx_font_bold">285</span>,Â 316â€“330 (2015)</a>. 
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shapeev [2016]</span>
<span class="ltx_bibblock">A.Â V.Â Shapeev,Â â€œMoment Tensor
Potentials: A class of systematically improvable interatomic potentials,â€Â <a target="_blank" href="https://doi.org/10.1137/15M1054183" title="" class="ltx_ref ltx_href">Multiscale Model. Simul.Â <span id="bib.bib21.1.1.1" class="ltx_text ltx_font_bold">14</span>,Â 1153â€“1173 (2016)</a>. 
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PinheiroÂ <em id="bib.bib22.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">M.Â Pinheiro, F.Â Ge,
N.Â FerrÃ©, P.Â O.Â Dral,Â andÂ M.Â Barbatti,Â â€œChoosing the right molecular machine learning
potential,â€Â <a target="_blank" href="https://doi.org/10.1039/D1SC03564A" title="" class="ltx_ref ltx_href">Chem. Sci.Â <span id="bib.bib22.3.1.1" class="ltx_text ltx_font_bold">12</span>,Â 14396â€“14413 (2021)</a>. 
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RamakrishnanÂ <em id="bib.bib23.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2014]</span>
<span class="ltx_bibblock">R.Â Ramakrishnan, P.Â O.Â Dral, M.Â Rupp,Â andÂ O.Â A.Â von Lilienfeld,Â â€œQuantum chemistry structures
and properties of 134 kilo molecules,â€Â <a target="_blank" href="https://doi.org/10.1038/sdata.2014.22" title="" class="ltx_ref ltx_href">Sci DataÂ <span id="bib.bib23.3.1.1" class="ltx_text ltx_font_bold">1</span>,Â 140022 (2014)</a>. 
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lubbers, Smith,Â andÂ Barros [2018]</span>
<span class="ltx_bibblock">N.Â Lubbers, J.Â S.Â Smith,Â andÂ K.Â Barros,Â â€œHierarchical modeling of
molecular energies using a deep neural network,â€Â <a target="_blank" href="https://doi.org/10.1063/1.5011181" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib24.1.1.1" class="ltx_text ltx_font_bold">148</span>,Â 241715 (2018)</a>. 
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SchÃ¼ttÂ <em id="bib.bib25.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">K.Â T.Â SchÃ¼tt, H.Â E.Â Sauceda, P.-J.Â Kindermans, A.Â Tkatchenko,Â andÂ K.-R.Â MÃ¼ller,Â â€œSchNet â€“
a deep learning architecture for molecules and materials,â€Â <a target="_blank" href="https://doi.org/10.1063/1.5019779" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib25.3.1.1" class="ltx_text ltx_font_bold">148</span>,Â 241722 (2018)</a>. 
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">UnkeÂ andÂ Meuwly [2019]</span>
<span class="ltx_bibblock">O.Â T.Â UnkeÂ andÂ M.Â Meuwly,Â â€œPhysNet: A neural
network for predicting energies, forces, dipole moments, and partial
charges,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.jctc.9b00181" title="" class="ltx_ref ltx_href">J. Chem. Theory Comput.Â <span id="bib.bib26.1.1.1" class="ltx_text ltx_font_bold">15</span>,Â 3678â€“3693 (2019)</a>. 
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ChanussotÂ <em id="bib.bib27.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">L.Â Chanussot, A.Â Das,
S.Â Goyal, T.Â Lavril, M.Â Shuaibi, M.Â Riviere, K.Â Tran, J.Â Heras-Domingo, C.Â Ho,
W.Â Hu, A.Â Palizhati, A.Â Sriram, B.Â Wood, J.Â Yoon, D.Â Parikh,
C.Â L.Â Zitnick,Â andÂ Z.Â Ulissi,Â â€œOpen Catalyst 2020 (OC20)
Dataset and Community Challenges,â€Â <a target="_blank" href="https://doi.org/10.1021/acscatal.0c04525" title="" class="ltx_ref ltx_href">ACS Catal.Â <span id="bib.bib27.3.1.1" class="ltx_text ltx_font_bold">11</span>,Â 6059â€“6072 (2021)</a>. 
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeringerÂ andÂ CsÃ¡nyi [2017]</span>
<span class="ltx_bibblock">V.Â L.Â DeringerÂ andÂ G.Â CsÃ¡nyi,Â â€œMachine
learning based interatomic potential for amorphous carbon,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevB.95.094203" title="" class="ltx_ref ltx_href">Phys. Rev. BÂ <span id="bib.bib28.1.1.1" class="ltx_text ltx_font_bold">95</span>,Â 094203 (2017)</a>. 
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BartÃ³k, Kondor,Â andÂ CsÃ¡nyi [2013]</span>
<span class="ltx_bibblock">A.Â P.Â BartÃ³k, R.Â Kondor,Â andÂ G.Â CsÃ¡nyi,Â â€œOn representing chemical
environments,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevB.87.184115" title="" class="ltx_ref ltx_href">Phys. Rev. BÂ <span id="bib.bib29.1.1.1" class="ltx_text ltx_font_bold">87</span>,Â 184115 (2013)</a>. 
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeringerÂ <em id="bib.bib30.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">V.Â L.Â Deringer, A.Â P.Â BartÃ³k, N.Â Bernstein, D.Â M.Â Wilkins, M.Â Ceriotti,Â andÂ G.Â CsÃ¡nyi,Â â€œGaussian Process
Regression for Materials and Molecules,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.chemrev.1c00022" title="" class="ltx_ref ltx_href">Chem. Rev.Â <span id="bib.bib30.3.1.1" class="ltx_text ltx_font_bold">121</span>,Â 10073â€“10141 (2021)</a>. 
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KhaliullinÂ <em id="bib.bib31.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2010]</span>
<span class="ltx_bibblock">R.Â Z.Â Khaliullin, H.Â Eshet,
T.Â D.Â KÃ¼hne, J.Â Behler,Â andÂ M.Â Parrinello,Â â€œGraphite-diamond phase coexistence study
employing a neural-network mapping of the ab initio potential energy
surface,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevB.81.100103" title="" class="ltx_ref ltx_href">Phys. Rev. BÂ <span id="bib.bib31.3.1.1" class="ltx_text ltx_font_bold">81</span>,Â 100103 (2010)</a>. 
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RoweÂ <em id="bib.bib32.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">P.Â Rowe, V.Â L.Â Deringer,
P.Â Gasparotto, G.Â CsÃ¡nyi,Â andÂ A.Â Michaelides,Â â€œAn accurate and transferable machine
learning potential for carbon,â€Â <a target="_blank" href="https://doi.org/10.1063/5.0005084" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib32.3.1.1" class="ltx_text ltx_font_bold">153</span>,Â 034702 (2020)</a>. 
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WillmanÂ <em id="bib.bib33.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">J.Â T.Â Willman, A.Â S.Â Williams, K.Â Nguyen-Cong, A.Â P.Â Thompson, M.Â A.Â Wood,
A.Â B.Â Belonoshko,Â andÂ I.Â I.Â Oleynik,Â â€œQuantum accurate SNAP
carbon potential for MD shock simulations,â€Â <a target="_blank" href="https://doi.org/10.1063/12.0000881" title="" class="ltx_ref ltx_href">AIP Conference ProceedingsÂ <span id="bib.bib33.3.1.1" class="ltx_text ltx_font_bold">2272</span>,Â 070055 (2020)</a>. 
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ShaiduÂ <em id="bib.bib34.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">Y.Â Shaidu, E.Â KÃ¼Ã§Ã¼kbenli, R.Â Lot,
F.Â Pellegrini, E.Â Kaxiras,Â andÂ S.Â de Gironcoli,Â â€œA systematic approach to generating accurate
neural network potentials: The case of carbon,â€Â <a target="_blank" href="https://doi.org/10.1038/s41524-021-00508-6" title="" class="ltx_ref ltx_href">npj Comput. Mater.Â <span id="bib.bib34.3.1.1" class="ltx_text ltx_font_bold">7</span>,Â 1â€“13 (2021)</a>. 
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ThiemannÂ <em id="bib.bib35.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">F.Â L.Â Thiemann, P.Â Rowe,
A.Â Zen, E.Â A.Â MÃ¼ller,Â andÂ A.Â Michaelides,Â â€œDefect-Dependent Corrugation in
Graphene,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.nanolett.1c02585" title="" class="ltx_ref ltx_href">Nano Lett.Â <span id="bib.bib35.3.1.1" class="ltx_text ltx_font_bold">21</span>,Â 8143â€“8150 (2021)</a>. 
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KarasuluÂ <em id="bib.bib36.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">B.Â Karasulu, J.-M.Â Leyssale, P.Â Rowe,
C.Â Weber,Â andÂ C.Â de Tomas,Â â€œAccelerating the prediction of large carbon
clusters via structure search: Evaluation of machine-learning and
classical potentials,â€Â <a target="_blank" href="https://doi.org/10.1016/j.carbon.2022.01.031" title="" class="ltx_ref ltx_href">CarbonÂ <span id="bib.bib36.3.1.1" class="ltx_text ltx_font_bold">191</span>,Â 255â€“266 (2022)</a>. 
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GolzeÂ <em id="bib.bib37.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">D.Â Golze, M.Â Hirvensalo,
P.Â HernÃ¡ndez-LeÃ³n, A.Â Aarva, J.Â Etula, T.Â Susi, P.Â Rinke, T.Â Laurila,Â andÂ M.Â A.Â Caro,Â â€œAccurate Computational
Prediction of Core-Electron Binding Energies in Carbon-Based
Materials: A Machine-Learning Model Combining Density-Functional Theory
and GW,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.chemmater.1c04279" title="" class="ltx_ref ltx_href">Chem. Mater.Â <span id="bib.bib37.3.1.1" class="ltx_text ltx_font_bold">34</span>,Â 6240â€“6254 (2022)</a>. 
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LarsenÂ <em id="bib.bib38.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2017]</span>
<span class="ltx_bibblock">A.Â H.Â Larsen, J.Â J.Â Mortensen, J.Â Blomqvist, I.Â E.Â Castelli, R.Â Christensen, M.Â DuÅ‚ak, J.Â Friis,
M.Â N.Â Groves, B.Â Hammer, C.Â Hargus, E.Â D.Â Hermes, P.Â C.Â Jennings, P.Â B.Â Jensen, J.Â Kermode, J.Â R.Â Kitchin, E.Â L.Â Kolsbjerg, J.Â Kubal,
K.Â Kaasbjerg, S.Â Lysgaard, J.Â B.Â Maronsson, T.Â Maxson, T.Â Olsen, L.Â Pastewka, A.Â Peterson, C.Â Rostgaard, J.Â SchiÃ¸tz, O.Â SchÃ¼tt, M.Â Strange, K.Â S.Â Thygesen, T.Â Vegge, L.Â Vilhelmsen,
M.Â Walter, Z.Â Zeng,Â andÂ K.Â W.Â Jacobsen,Â â€œThe atomic simulation environmentâ€”a
Python library for working with atoms,â€Â <a target="_blank" href="https://doi.org/10.1088/1361-648X/aa680e" title="" class="ltx_ref ltx_href">J. Phys.: Condens. MatterÂ <span id="bib.bib38.3.1.1" class="ltx_text ltx_font_bold">29</span>,Â 273002 (2017)</a>. 
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ThompsonÂ <em id="bib.bib39.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">A.Â P.Â Thompson, H.Â M.Â Aktulga, R.Â Berger,
D.Â S.Â Bolintineanu,
W.Â M.Â Brown, P.Â S.Â Crozier, P.Â J.Â in â€™t Veld, A.Â Kohlmeyer, S.Â G.Â Moore, T.Â D.Â Nguyen, R.Â Shan, M.Â J.Â Stevens, J.Â Tranchida, C.Â Trott,Â andÂ S.Â J.Â Plimpton,Â â€œLAMMPS - a flexible simulation tool for particle-based materials
modeling at the atomic, meso, and continuum scales,â€Â <a target="_blank" href="https://doi.org/10.1016/j.cpc.2021.108171" title="" class="ltx_ref ltx_href">Comput. Phys. Commun.Â <span id="bib.bib39.3.1.1" class="ltx_text ltx_font_bold">271</span>,Â 108171 (2022)</a>. 
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Powles, Marks,Â andÂ Lau [2009]</span>
<span class="ltx_bibblock">R.Â C.Â Powles, N.Â A.Â Marks,Â andÂ D.Â W.Â M.Â Lau,Â â€œSelf-assembly of
sp<sup id="bib.bib40.2.1" class="ltx_sup"><span id="bib.bib40.2.1.1" class="ltx_text ltx_font_italic">2</span></sup>-bonded carbon nanostructures from amorphous precursors,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevB.79.075430" title="" class="ltx_ref ltx_href">Phys. Rev. BÂ <span id="bib.bib40.3.2.1" class="ltx_text ltx_font_bold">79</span>,Â 075430 (2009)</a>. 
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de TomasÂ <em id="bib.bib41.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2017]</span>
<span class="ltx_bibblock">C.Â de
Tomas, I.Â Suarez-Martinez, F.Â Vallejos-Burgos, M.Â J.Â LÃ³pez, K.Â Kaneko,Â andÂ N.Â A.Â Marks,Â â€œStructural prediction of
graphitization and porosity in carbide-derived carbons,â€Â <a target="_blank" href="https://doi.org/10.1016/j.carbon.2017.04.004" title="" class="ltx_ref ltx_href">CarbonÂ <span id="bib.bib41.3.1.1" class="ltx_text ltx_font_bold">119</span>,Â 1â€“9 (2017)</a>. 
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeringerÂ <em id="bib.bib42.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">V.Â L.Â Deringer, C.Â Merlet,
Y.Â Hu, T.Â H.Â Lee, J.Â A.Â Kattirtzi, O.Â Pecher, G.Â CsÃ¡nyi, S.Â R.Â Elliott,Â andÂ C.Â P.Â Grey,Â â€œTowards an atomistic understanding of disordered carbon electrode
materials,â€Â <a target="_blank" href="https://doi.org/10.1039/C8CC01388H" title="" class="ltx_ref ltx_href">Chem. Commun.Â <span id="bib.bib42.3.1.1" class="ltx_text ltx_font_bold">54</span>,Â 5988â€“5991 (2018)</a>. 
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WangÂ <em id="bib.bib43.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">Y.Â Wang, Z.Â Fan, P.Â Qian, T.Â Ala-Nissila,Â andÂ M.Â A.Â Caro,Â â€œStructure and Pore Size Distribution in Nanoporous
Carbon,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.chemmater.1c03279" title="" class="ltx_ref ltx_href">Chem. Mater.Â <span id="bib.bib43.3.1.1" class="ltx_text ltx_font_bold">34</span>,Â 617â€“628 (2022)</a>. 
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocer, Mason,Â andÂ Erturk [2019]</span>
<span class="ltx_bibblock">E.Â Kocer, J.Â K.Â Mason,Â andÂ H.Â Erturk,Â â€œA novel approach to describe
chemical environments in high-dimensional neural network potentials,â€Â <a target="_blank" href="https://doi.org/10.1063/1.5086167" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib44.1.1.1" class="ltx_text ltx_font_bold">150</span>,Â 154102 (2019)</a>. 
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KaramadÂ <em id="bib.bib45.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">M.Â Karamad, R.Â Magar,
Y.Â Shi, S.Â Siahrostami, I.Â D.Â Gates,Â andÂ A.Â BaratiÂ Farimani,Â â€œOrbital graph convolutional neural network for
material property prediction,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevMaterials.4.093801" title="" class="ltx_ref ltx_href">Phys. Rev. Mater.Â <span id="bib.bib45.3.1.1" class="ltx_text ltx_font_bold">4</span>,Â 093801 (2020)</a>. 
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">XiaÂ <em id="bib.bib46.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2021]</span>
<span class="ltx_bibblock">D.Â Xia, N.Â Li, P.Â Ren,Â andÂ X.Â Wen,Â â€œPrediction Of Material Properties By Neural Network
Fusing The Atomic Local Environment And Global Description: Applied To
Organic Molecules And Crystals,â€Â <a target="_blank" href="https://doi.org/10.1051/e3sconf/202126702059" title="" class="ltx_ref ltx_href">E3S Web Conf.Â <span id="bib.bib46.3.1.1" class="ltx_text ltx_font_bold">267</span>,Â 02059 (2021)</a>. 
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RasmussenÂ andÂ Williams [2006]</span>
<span class="ltx_bibblock">C.Â E.Â RasmussenÂ andÂ C.Â K.Â I.Â Williams,Â <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Gaussian Processes for
Machine Learning</em>,Â Adaptive Computation and Machine LearningÂ (Cambridge, Mass,Â 2006). 
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barron [1993]</span>
<span class="ltx_bibblock">A.Â Barron,Â â€œUniversal
approximation bounds for superpositions of a sigmoidal function,â€Â <a target="_blank" href="https://doi.org/10.1109/18.256500" title="" class="ltx_ref ltx_href">IEEE Transactions on Information TheoryÂ <span id="bib.bib48.1.1.1" class="ltx_text ltx_font_bold">39</span>,Â 930â€“945 (1993)</a>. 
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun, Bengio,Â andÂ Hinton [2015]</span>
<span class="ltx_bibblock">Y.Â LeCun, Y.Â Bengio,Â andÂ G.Â Hinton,Â â€œDeep learning,â€Â <a target="_blank" href="https://doi.org/10.1038/nature14539" title="" class="ltx_ref ltx_href">NatureÂ <span id="bib.bib49.1.1.1" class="ltx_text ltx_font_bold">521</span>,Â 436â€“444 (2015)</a>. 
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber [2015]</span>
<span class="ltx_bibblock">J.Â Schmidhuber,Â â€œDeep
learning in neural networks: An overview,â€Â <a target="_blank" href="https://doi.org/10.1016/j.neunet.2014.09.003" title="" class="ltx_ref ltx_href">Neural NetworksÂ <span id="bib.bib50.1.1.1" class="ltx_text ltx_font_bold">61</span>,Â 85â€“117 (2015)</a>. 
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KingmaÂ andÂ Ba [2017]</span>
<span class="ltx_bibblock">D.Â P.Â KingmaÂ andÂ J.Â Ba,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.1412.6980" title="" class="ltx_ref ltx_href">â€œAdam: A Method for Stochastic Optimization,â€Â </a> (2017),Â <a target="_blank" href="https://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_href">arXiv:1412.6980 [cs.LG]</a>
. 
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barron [2017]</span>
<span class="ltx_bibblock">J.Â T.Â Barron,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.1704.07483" title="" class="ltx_ref ltx_href">â€œContinuously Differentiable Exponential Linear
Units,â€Â </a> (2017),Â <a target="_blank" href="https://arxiv.org/abs/1704.07483" title="" class="ltx_ref ltx_href">arXiv:1704.07483 [cs.LG]</a> . 
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PaszkeÂ <em id="bib.bib53.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2017]</span>
<span class="ltx_bibblock">A.Â Paszke, S.Â Gross,
S.Â Chintala, G.Â Chanan, E.Â Yang, Z.Â DeVito, Z.Â Lin, A.Â Desmaison, L.Â Antiga,Â andÂ A.Â Lerer,Â â€œAutomatic differentiation in
PyTorch,â€Â NIPS 2017 Autodiff WorkshopÂ  (2017). 
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WilsonÂ <em id="bib.bib54.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2015]</span>
<span class="ltx_bibblock">A.Â G.Â Wilson, Z.Â Hu, R.Â Salakhutdinov,Â andÂ E.Â P.Â Xing,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.1511.02222" title="" class="ltx_ref ltx_href">â€œDeep
Kernel Learning,â€Â </a> (2015),Â <a target="_blank" href="https://arxiv.org/abs/1511.02222" title="" class="ltx_ref ltx_href">arXiv:1511.02222 [cs.LG]</a> . 
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WilsonÂ <em id="bib.bib55.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2016]</span>
<span class="ltx_bibblock">A.Â G.Â Wilson, Z.Â Hu, R.Â Salakhutdinov,Â andÂ E.Â P.Â Xing,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.1611.00336" title="" class="ltx_ref ltx_href">â€œStochastic Variational Deep Kernel Learning,â€Â </a> (2016),Â <a target="_blank" href="https://arxiv.org/abs/1611.00336" title="" class="ltx_ref ltx_href">arXiv:1611.00336
[stat.ML]</a> . 
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GardnerÂ <em id="bib.bib56.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">J.Â Gardner, G.Â Pleiss,
K.Â Q.Â Weinberger,
D.Â Bindel,Â andÂ A.Â G.Â Wilson,Â â€œGPyTorch: Blackbox Matrix-Matrix Gaussian
Process Inference with GPU Acceleration,â€Â inÂ <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>,Â Vol.Â 31Â (2018). 
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrow, Gardner,Â andÂ Deringer [2022]</span>
<span class="ltx_bibblock">J.Â D.Â Morrow, J.Â L.Â A.Â Gardner,Â andÂ V.Â L.Â Deringer,Â â€œHow to
validate machine-learned interatomic potentials,â€Â <a target="_blank" href="https://doi.org/10.48550/arXiv.2211.12484" title="" class="ltx_ref ltx_href">arXiv preprintÂ ,Â arXiv:2211.12484
[physics.chemâ€“ph] (2022)</a>. 
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MorrowÂ andÂ Deringer [2022]</span>
<span class="ltx_bibblock">J.Â D.Â MorrowÂ andÂ V.Â L.Â Deringer,Â â€œIndirect
learning and physically guided validation of interatomic potential models,â€Â <a target="_blank" href="https://doi.org/10.1063/5.0099929" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib58.1.1.1" class="ltx_text ltx_font_bold">157</span>,Â 104105 (2022)</a>. 
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BartÃ³kÂ <em id="bib.bib59.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">A.Â P.Â BartÃ³k, J.Â Kermode,
N.Â Bernstein,Â andÂ G.Â CsÃ¡nyi,Â â€œMachine Learning a
General-Purpose Interatomic Potential for Silicon,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevX.8.041048" title="" class="ltx_ref ltx_href">Phys. Rev. XÂ <span id="bib.bib59.3.1.1" class="ltx_text ltx_font_bold">8</span>,Â 041048 (2018)</a>. 
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GeorgeÂ <em id="bib.bib60.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">J.Â George, G.Â Hautier,
A.Â P.Â BartÃ³k,
G.Â CsÃ¡nyi,Â andÂ V.Â L.Â Deringer,Â â€œCombining phonon accuracy
with high transferability in Gaussian approximation potential models,â€Â <a target="_blank" href="https://doi.org/10.1063/5.0013826" title="" class="ltx_ref ltx_href">J. Chem. Phys.Â <span id="bib.bib60.3.1.1" class="ltx_text ltx_font_bold">153</span>,Â 044104 (2020)</a>. 
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith, Isayev,Â andÂ Roitberg [2017]</span>
<span class="ltx_bibblock">J.Â S.Â Smith, O.Â Isayev,Â andÂ A.Â E.Â Roitberg,Â â€œANI-1: an extensible
neural network potential with DFT accuracy at force field computational
cost,â€Â <a target="_blank" href="https://doi.org/10.1039/C6SC05720A" title="" class="ltx_ref ltx_href">Chem. Sci.Â <span id="bib.bib61.1.1.1" class="ltx_text ltx_font_bold">8</span>,Â 3192â€“3203 (2017)</a>. 
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZhangÂ <em id="bib.bib62.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">L.Â Zhang, J.Â Han, H.Â Wang, R.Â Car,Â andÂ W.Â E,Â â€œDeep potential molecular dynamics: A scalable model with
the accuracy of quantum mechanics,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevLett.120.143001" title="" class="ltx_ref ltx_href">Phys. Rev. Lett.Â <span id="bib.bib62.3.1.1" class="ltx_text ltx_font_bold">120</span>,Â 143001 (2018)</a>. 
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BatznerÂ <em id="bib.bib63.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">S.Â Batzner, A.Â Musaelian,
L.Â Sun, M.Â Geiger, J.Â P.Â Mailoa, M.Â Kornbluth, N.Â Molinari, T.Â E.Â Smidt,Â andÂ B.Â Kozinsky,Â â€œE(3)-equivariant graph neural networks for data-efficient and
accurate interatomic potentials,â€Â <a target="_blank" href="https://doi.org/10.1038/s41467-022-29939-5" title="" class="ltx_ref ltx_href">Nat. Commun.Â <span id="bib.bib63.3.1.1" class="ltx_text ltx_font_bold">13</span>,Â 2453 (2022)</a>. 
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">EckhoffÂ andÂ Behler [2019]</span>
<span class="ltx_bibblock">M.Â EckhoffÂ andÂ J.Â Behler,Â â€œFrom Molecular
Fragments to the Bulk: Development of a Neural Network
Potential for MOF-5,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.jctc.8b01288" title="" class="ltx_ref ltx_href">J. Chem. Theory Comput.Â <span id="bib.bib64.1.1.1" class="ltx_text ltx_font_bold">15</span>,Â 3793â€“3809 (2019)</a>. 
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SmithÂ <em id="bib.bib65.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2019]</span>
<span class="ltx_bibblock">J.Â S.Â Smith, B.Â T.Â Nebgen,
R.Â Zubatyuk, N.Â Lubbers, C.Â Devereux, K.Â Barros, S.Â Tretiak, O.Â Isayev,Â andÂ A.Â E.Â Roitberg,Â â€œApproaching coupled cluster accuracy with a general-purpose neural network
potential through transfer learning,â€Â <a target="_blank" href="https://doi.org/10.1038/s41467-019-10827-4" title="" class="ltx_ref ltx_href">Nat. Commun.Â <span id="bib.bib65.3.1.1" class="ltx_text ltx_font_bold">10</span>,Â 2903 (2019)</a>. 
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington, Socher,Â andÂ Manning [2014]</span>
<span class="ltx_bibblock">J.Â Pennington, R.Â Socher,Â andÂ C.Â Manning,Â â€œGlove: Global Vectors
for Word Representation,â€Â inÂ <a target="_blank" href="https://doi.org/10.3115/v1/D14-1162" title="" class="ltx_ref ltx_href"><em id="bib.bib66.1.1.1" class="ltx_emph ltx_font_italic">EMNLP</em></a>,Â Vol.Â 14Â (2014)Â pp.Â 1532â€“1543. 
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky, Sutskever,Â andÂ Hinton [2017]</span>
<span class="ltx_bibblock">A.Â Krizhevsky, I.Â Sutskever,Â andÂ G.Â E.Â Hinton,Â â€œImageNet
classification with deep convolutional neural networks,â€Â <a target="_blank" href="https://doi.org/10.1145/3065386" title="" class="ltx_ref ltx_href">Commun. ACMÂ <span id="bib.bib67.1.1.1" class="ltx_text ltx_font_bold">60</span>,Â 84â€“90
(2017)</a>. 
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZhangÂ <em id="bib.bib68.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">D.Â Zhang, H.Â Bi, F.-Z.Â Dai, W.Â Jiang, L.Â Zhang,Â andÂ H.Â Wang,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.2208.08236" title="" class="ltx_ref ltx_href">â€œDPA-1: Pretraining of Attention-based Deep Potential Model for
Molecular Simulation,â€Â </a> (2022),Â <a target="_blank" href="https://arxiv.org/abs/2208.08236" title="" class="ltx_ref ltx_href">arXiv:2208.08236 [physics.chem-ph]</a>
. 
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GaoÂ <em id="bib.bib69.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">X.Â Gao, W.Â Gao, W.Â Xiao, Z.Â Wang, C.Â Wang,Â andÂ L.Â Xiang,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.2211.14429" title="" class="ltx_ref ltx_href">â€œSupervised Pretraining for Molecular Force Fields and Properties
Prediction,â€Â </a> (2022),Â <a target="_blank" href="https://arxiv.org/abs/2211.14429" title="" class="ltx_ref ltx_href">arXiv:2211.14429 [physics.chem-ph]</a>
. 
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McInnes, Healy,Â andÂ Melville [2020]</span>
<span class="ltx_bibblock">L.Â McInnes, J.Â Healy,Â andÂ J.Â Melville,Â <a target="_blank" href="https://doi.org/10.48550/arXiv.1802.03426" title="" class="ltx_ref ltx_href">â€œUMAP: Uniform Manifold Approximation and Projection for
Dimension Reduction,â€Â </a> (2020),Â <a target="_blank" href="https://arxiv.org/abs/1802.03426" title="" class="ltx_ref ltx_href">arXiv:1802.03426 [stat.ML]</a> . 
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ChengÂ <em id="bib.bib71.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">B.Â Cheng, R.-R.Â Griffiths, S.Â Wengert,
C.Â Kunkel, T.Â Stenczel, B.Â Zhu, V.Â L.Â Deringer, N.Â Bernstein, J.Â T.Â Margraf, K.Â Reuter,Â andÂ G.Â Csanyi,Â â€œMapping
Materials and Molecules,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.accounts.0c00403" title="" class="ltx_ref ltx_href">Acc. Chem. Res.Â <span id="bib.bib71.3.1.1" class="ltx_text ltx_font_bold">53</span>,Â 1981â€“1991 (2020)</a>. 
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeÂ <em id="bib.bib72.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2016]</span>
<span class="ltx_bibblock">S.Â De, A.Â P.Â BartÃ³k,
G.Â CsÃ¡nyi,Â andÂ M.Â Ceriotti,Â â€œComparing molecules and solids across
structural and alchemical space,â€Â <a target="_blank" href="https://doi.org/10.1039/C6CP00415F" title="" class="ltx_ref ltx_href">Phys. Chem. Chem. Phys.Â <span id="bib.bib72.3.1.1" class="ltx_text ltx_font_bold">18</span>,Â 13754â€“13769 (2016)</a>. 
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CaroÂ <em id="bib.bib73.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2018]</span>
<span class="ltx_bibblock">M.Â A.Â Caro, A.Â Aarva,
V.Â L.Â Deringer, G.Â CsÃ¡nyi,Â andÂ T.Â Laurila,Â â€œReactivity of Amorphous Carbon Surfaces:
Rationalizing the Role of Structural Motifs in
Functionalization Using Machine Learning,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.chemmater.8b03353" title="" class="ltx_ref ltx_href">Chem. Mater.Â <span id="bib.bib73.3.1.1" class="ltx_text ltx_font_bold">30</span>,Â 7446â€“7455 (2018)</a>. 
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ShiresÂ andÂ Pickard [2021]</span>
<span class="ltx_bibblock">B.Â W.Â B.Â ShiresÂ andÂ C.Â J.Â Pickard,Â â€œVisualizing Energy Landscapes through Manifold Learning,â€Â <a target="_blank" href="https://doi.org/10.1103/PhysRevX.11.041026" title="" class="ltx_ref ltx_href">Phys. Rev. XÂ <span id="bib.bib74.1.1.1" class="ltx_text ltx_font_bold">11</span>,Â 041026 (2021)</a>. 
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WestermayrÂ <em id="bib.bib75.4.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">J.Â Westermayr, F.Â A.Â Faber, A.Â S.Â Christensen, O.Â A.Â von
Lilienfeld,Â andÂ P.Â Marquetand,Â â€œNeural
networks and kernel ridge regression for excited states dynamics of
CH<sub id="bib.bib75.5.1" class="ltx_sub"><span id="bib.bib75.5.1.1" class="ltx_text ltx_font_italic">2</span></sub>NH<math id="bib.bib75.2.m2.1" class="ltx_Math" alttext="{}_{2}^{+}" display="inline"><semantics id="bib.bib75.2.m2.1a"><mmultiscripts id="bib.bib75.2.m2.1.1" xref="bib.bib75.2.m2.1.1.cmml"><mi id="bib.bib75.2.m2.1.1.2.2" xref="bib.bib75.2.m2.1.1.2.2.cmml"></mi><mprescripts id="bib.bib75.2.m2.1.1a" xref="bib.bib75.2.m2.1.1.cmml"></mprescripts><mrow id="bib.bib75.2.m2.1.1b" xref="bib.bib75.2.m2.1.1.cmml"></mrow><mo id="bib.bib75.2.m2.1.1.3" xref="bib.bib75.2.m2.1.1.3.cmml">+</mo><mn id="bib.bib75.2.m2.1.1.2.3" xref="bib.bib75.2.m2.1.1.2.3.cmml">2</mn><mrow id="bib.bib75.2.m2.1.1c" xref="bib.bib75.2.m2.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="bib.bib75.2.m2.1b"><apply id="bib.bib75.2.m2.1.1.cmml" xref="bib.bib75.2.m2.1.1"><csymbol cd="ambiguous" id="bib.bib75.2.m2.1.1.1.cmml" xref="bib.bib75.2.m2.1.1">superscript</csymbol><apply id="bib.bib75.2.m2.1.1.2.cmml" xref="bib.bib75.2.m2.1.1"><csymbol cd="ambiguous" id="bib.bib75.2.m2.1.1.2.1.cmml" xref="bib.bib75.2.m2.1.1">subscript</csymbol><csymbol cd="latexml" id="bib.bib75.2.m2.1.1.2.2.cmml" xref="bib.bib75.2.m2.1.1.2.2">absent</csymbol><cn type="integer" id="bib.bib75.2.m2.1.1.2.3.cmml" xref="bib.bib75.2.m2.1.1.2.3">2</cn></apply><plus id="bib.bib75.2.m2.1.1.3.cmml" xref="bib.bib75.2.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib75.2.m2.1c">{}_{2}^{+}</annotation></semantics></math>: From single-state to multi-state
representations and multi-property machine learning models,â€Â <a target="_blank" href="https://doi.org/10.1088/2632-2153/ab88d0" title="" class="ltx_ref ltx_href">Mach. Learn.: Sci. Technol.Â <span id="bib.bib75.6.2.1" class="ltx_text ltx_font_bold">1</span>,Â 025009 (2020)</a>. 
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DorkenwaldÂ <em id="bib.bib76.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2022]</span>
<span class="ltx_bibblock">S.Â Dorkenwald, P.Â H.Â Li,
M.Â Januszewski, D.Â R.Â Berger, J.Â Maitin-Shepard, A.Â L.Â Bodor, F.Â Collman, C.Â M.Â Schneider-Mizell, N.Â M.Â daÂ Costa, J.Â W.Â Lichtman,Â andÂ V.Â Jain,Â â€œMulti-Layered Maps of Neuropil with Segmentation-Guided
Contrastive Learning,â€Â <a target="_blank" href="https://doi.org/10.1101/2022.03.29.486320" title="" class="ltx_ref ltx_href">bioRxiv preprintÂ ,Â 2022.03.29.486320 (2022)</a>. 
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang, Ramakrishnan,Â andÂ Livny [1996]</span>
<span class="ltx_bibblock">T.Â Zhang, R.Â Ramakrishnan,Â andÂ M.Â Livny,Â â€œBIRCH: An
efficient data clustering method for very large databases,â€Â inÂ <a target="_blank" href="https://doi.org/10.1145/233269.233324" title="" class="ltx_ref ltx_href"><em id="bib.bib77.1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1996 ACM SIGMOD International Conference on
Management of Data</em></a>,Â SIGMOD â€™96Â (New York, NY, USA,Â 1996)Â pp.Â 103â€“114. 
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de JongÂ andÂ Kiers [1992]</span>
<span class="ltx_bibblock">S.Â de
JongÂ andÂ H.Â A.Â L.Â Kiers,Â â€œPrincipal
covariates regression: Part I. Theory,â€Â <a target="_blank" href="https://doi.org/10.1016/0169-7439(92)80100-I" title="" class="ltx_ref ltx_href">Chemometrics and Intelligent Laboratory SystemsÂ Proceedings of the 2nd Scandinavian Symposium on
Chemometrics,Â <span id="bib.bib78.1.1.1" class="ltx_text ltx_font_bold">14</span>,Â 155â€“164 (1992)</a>. 
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HelfrechtÂ <em id="bib.bib79.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">B.Â A.Â Helfrecht, R.Â K.Â Cersonsky, G.Â Fraux,Â andÂ M.Â Ceriotti,Â â€œStructure-property maps with
Kernel principal covariates regression,â€Â <a target="_blank" href="https://doi.org/10.1088/2632-2153/aba9ef" title="" class="ltx_ref ltx_href">Mach. Learn.: Sci. Technol.Â <span id="bib.bib79.3.1.1" class="ltx_text ltx_font_bold">1</span>,Â 045021 (2020)</a>. 
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">YuÂ <em id="bib.bib80.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2020]</span>
<span class="ltx_bibblock">C.Â Yu, M.Â Seslija,
G.Â Brownbridge, S.Â Mosbach, M.Â Kraft, M.Â Parsi, M.Â Davis, V.Â Page,Â andÂ A.Â Bhave,Â â€œDeep kernel learning approach to engine emissions modeling,â€Â <a target="_blank" href="https://doi.org/10.1017/dce.2020.4" title="" class="ltx_ref ltx_href">Data-Centric EngineeringÂ <span id="bib.bib80.3.1.1" class="ltx_text ltx_font_bold">1</span>,Â e4 (2020)</a>. 
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu, Ziatdinov,Â andÂ Kalinin [2022]</span>
<span class="ltx_bibblock">Y.Â Liu, M.Â Ziatdinov,Â andÂ S.Â V.Â Kalinin,Â â€œExploring Causal
Physical Mechanisms via Non-Gaussian Linear Models and Deep
Kernel Learning: Applications for Ferroelectric Domain
Structures,â€Â <a target="_blank" href="https://doi.org/10.1021/acsnano.1c09059" title="" class="ltx_ref ltx_href">ACS NanoÂ <span id="bib.bib81.1.1.1" class="ltx_text ltx_font_bold">16</span>,Â 1250â€“1259 (2022)</a>. 
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SivaramanÂ andÂ Jackson [2022]</span>
<span class="ltx_bibblock">G.Â SivaramanÂ andÂ N.Â E.Â Jackson,Â â€œCoarse-Grained Density Functional Theory Predictions via Deep
Kernel Learning,â€Â <a target="_blank" href="https://doi.org/10.1021/acs.jctc.1c01001" title="" class="ltx_ref ltx_href">J. Chem. Theory Comput.Â <span id="bib.bib82.1.1.1" class="ltx_text ltx_font_bold">18</span>,Â 1129â€“1141 (2022)</a>. 
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BernsteinÂ <em id="bib.bib83.2.2.1" class="ltx_emph ltx_font_italic">etÂ al.</em> [2019]</span>
<span class="ltx_bibblock">N.Â Bernstein, B.Â Bhattarai, G.Â CsÃ¡nyi, D.Â A.Â Drabold, S.Â R.Â Elliott,Â andÂ V.Â L.Â Deringer,Â â€œQuantifying
Chemical Structure and Machine-Learned Atomic Energies in
Amorphous and Liquid Silicon,â€Â <a target="_blank" href="https://doi.org/10.1002/anie.201902625" title="" class="ltx_ref ltx_href">Angew. Chen. Int. Ed.Â <span id="bib.bib83.3.1.1" class="ltx_text ltx_font_bold">58</span>,Â 7057â€“7061 (2019)</a>. 
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El-Machachi, Wilson,Â andÂ Deringer [2022]</span>
<span class="ltx_bibblock">Z.Â El-Machachi, M.Â Wilson,Â andÂ V.Â L.Â Deringer,Â â€œExploring the
configurational space of amorphous graphene with machine-learned atomic
energies,â€Â <a target="_blank" href="https://doi.org/10.1039/D2SC04326B" title="" class="ltx_ref ltx_href">Chem. Sci.â€‹â€‹â€‹Â ,Â Advance Article, DOI: 10.1039/D2SC04326B (2022)</a>. 
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.16442" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.16443" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.16443">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.16443" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.16445" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 18:59:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
