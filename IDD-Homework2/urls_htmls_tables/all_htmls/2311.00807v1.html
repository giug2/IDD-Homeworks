<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.00807] VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization</title><meta property="og:description" content="Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing doma…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.00807">

<!--Generated on Tue Feb 27 21:49:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document" style="font-size:144%;">VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suraj Jyothi Unni
</span><span class="ltx_author_notes">School of Computing and Augmented Intelligence, Arizona State University. Email:{sjyothiu, rmoraffa, huanliu}@asu.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raha Moraffah<sup id="id3.2.id1" class="ltx_sup">∗</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huan Liu<sup id="id4.2.id1" class="ltx_sup">∗</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p"><span id="id5.id1.1" class="ltx_text" style="font-size:90%;">Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.
<br class="ltx_break"><span id="id5.id1.1.1" class="ltx_text ltx_font_bold">Keywords</span>: Domain Generalization, Multi-modal Reasoning, Visual Question Answering, Distribution Shift</span></p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p"><span id="id6.id1.1" class="ltx_text" style="font-size:90%;">In our paper, we propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift-induced pipeline. We showed that state-of-the-art VQA models struggle to generalize when exposed to VQA-GEN dataset. In this supplementary material, we elaborate on the user study experiment to evaluate the relevance of the generated image-question pairs.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) has emerged as a canonical task for evaluating multi-modal intelligence, requiring joint understanding of visual and textual inputs. Significant progress has been made in VQA, with models achieving human-level performance on many standard benchmarks. However, robustness remains a central concern for deploying VQA systems to real-world applications.
<br class="ltx_break">A core assumption in machine learning is that training and test data are drawn from the same underlying distribution. However, with distributions shift across domains model performance often degrades sharply. While domain generalization datasets expose models to varied training distributions, constructing comprehensive benchmarks spanning the full multi-modal distribution shifts remains an open challenge for VQA.
<br class="ltx_break">Existing benchmark Domain Generalization datasets such as VQA-CP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and VQA-Compose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> focus on textual shift but multimodal datasets have shifts in both text and image modalities. As a result, models trained on existing domain generalization data exhibit limited real-world effectiveness due to single modality focuses.
<br class="ltx_break">Currently, no VQA datasets incorporate coordinated multimodal shifts across images and text. Techniques that can systematically produce coherent variations in both modalities are lacking.In this paper, we present VQA-GEN - a novel large-scale dataset for multi-modal distribution shifts.
<br class="ltx_break"></p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.3.4.1" class="ltx_tr">
<th id="S1.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S1.T1.3.4.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S1.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.2.1" class="ltx_text ltx_font_bold">Image Shift</span></th>
<th id="S1.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.3.1" class="ltx_text ltx_font_bold">Question Shift</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.3.5.1" class="ltx_tr">
<th id="S1.T1.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQA-GEN</th>
<td id="S1.T1.3.5.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.3.5.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
</tr>
<tr id="S1.T1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-Compose</th>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center"><math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><times id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<th id="S1.T1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">VQA-CP</th>
<td id="S1.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_b"><math id="S1.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.2.2.1.m1.1a"><mo id="S1.T1.2.2.1.m1.1.1" xref="S1.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.1.m1.1b"><times id="S1.T1.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_b"><math id="S1.T1.3.3.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.3.3.2.m1.1a"><mo id="S1.T1.3.3.2.m1.1.1" xref="S1.T1.3.3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.2.m1.1b"><times id="S1.T1.3.3.2.m1.1.1.cmml" xref="S1.T1.3.3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.2.m1.1c">\times</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.6.2" class="ltx_text" style="font-size:90%;">Domain Generalization Datasets Comparison</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA-GEN is constructed through an intricately designed three-stage data generation pipeline for inducing controlled multi-modal shifts. The first stage introduces comprehensive visual shifts in the input images through techniques like style transfer and image corruptions, generating stylistic and perturbed variants. The second stage elicits linguistic variations in the source questions using methods such as backward translation and persona-based modeling to induce textual shifts synthetically. The final stage recombines the shifted images and questions through a mix-and-match process to create novel paired cross-modal distributions- the essence of VQA-GEN. This automated pipeline ensures that the contextual alignment of images, questions, and answers remains consistent with the original VQA data. A comparative analysis between existing domain generalization datasets and the proposed VQA-GEN dataset is presented in Table 1.
<br class="ltx_break"></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.00807/assets/x1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="610" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Example of VQA-GEN datasets showcasing image shifts of standing bear, accompanied by corresponding question shifts inquiring about its posture (standing or sleeping).</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Experiments demonstrate that the VQA-GEN dataset exposes the limitations of existing methods when faced with joint multimodal distribution shifts. Furthermore, models trained on VQA-GEN substantially outperform those trained on existing domain generalization datasets on out-of-domain target tasks. This highlights the usefulness of VQA-GEN for training robust models under multimodal shifts. Additional in-dataset evaluations validate the efficacy of each shift technique of our pipeline for evaluating generalized VQA models. We also analytically show the quality of the generated images and questions through domain shift analysis and similarity metrics.Figure 1 shows example of VQA-GEN dataset.
<br class="ltx_break"></p>
</div>
<figure id="S1.T2" class="ltx_table">
<table id="S1.T2.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T2.15.16.1" class="ltx_tr">
<th id="S1.T2.15.16.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Models</th>
<th id="S1.T2.15.16.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">VQA-Compose</th>
<th id="S1.T2.15.16.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">VQA-CP</th>
<th id="S1.T2.15.16.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T2.15.16.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.15.16.1.4.1.1" class="ltx_p" style="width:85.4pt;">VQA-GEN(Joint Shifts)</span>
</span>
</th>
<th id="S1.T2.15.16.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T2.15.16.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.15.16.1.5.1.1" class="ltx_p" style="width:85.4pt;">VQA-GEN(Image Shifts)</span>
</span>
</th>
<th id="S1.T2.15.16.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T2.15.16.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.15.16.1.6.1.1" class="ltx_p" style="width:85.4pt;">VQA-GEN(Question Shifts)</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T2.5.5" class="ltx_tr">
<th id="S1.T2.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLT</th>
<th id="S1.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">55.82 <math id="S1.T2.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.1.1.1.m1.1a"><mo id="S1.T2.1.1.1.m1.1.1" xref="S1.T2.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.1.1.1.m1.1.1.cmml" xref="S1.T2.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.1.1.1.m1.1c">\pm</annotation></semantics></math> 1.21</th>
<th id="S1.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">67.91 <math id="S1.T2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.2.2.2.m1.1a"><mo id="S1.T2.2.2.2.m1.1.1" xref="S1.T2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S1.T2.2.2.2.m1.1.1.cmml" xref="S1.T2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.2.2.2.m1.1c">\pm</annotation></semantics></math> 1.37</th>
<td id="S1.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.3.3.3.1.1" class="ltx_p" style="width:85.4pt;">42.26 <math id="S1.T2.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.3.3.3.1.1.m1.1a"><mo id="S1.T2.3.3.3.1.1.m1.1.1" xref="S1.T2.3.3.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.3.3.3.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.3.3.3.1.1.m1.1.1.cmml" xref="S1.T2.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.3.3.3.1.1.m1.1c">\pm</annotation></semantics></math> 0.08</span>
</span>
</td>
<td id="S1.T2.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T2.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.4.4.4.1.1" class="ltx_p" style="width:85.4pt;">50.24 <math id="S1.T2.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.4.4.4.1.1.m1.1a"><mo id="S1.T2.4.4.4.1.1.m1.1.1" xref="S1.T2.4.4.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.4.4.4.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.4.4.4.1.1.m1.1.1.cmml" xref="S1.T2.4.4.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.4.4.4.1.1.m1.1c">\pm</annotation></semantics></math> 2.25</span>
</span>
</td>
<td id="S1.T2.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T2.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.5.5.5.1.1" class="ltx_p" style="width:85.4pt;">47.86 <math id="S1.T2.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.5.5.5.1.1.m1.1a"><mo id="S1.T2.5.5.5.1.1.m1.1.1" xref="S1.T2.5.5.5.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.5.5.5.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.5.5.5.1.1.m1.1.1.cmml" xref="S1.T2.5.5.5.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.5.5.5.1.1.m1.1c">\pm</annotation></semantics></math> 0.85</span>
</span>
</td>
</tr>
<tr id="S1.T2.10.10" class="ltx_tr">
<th id="S1.T2.10.10.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">MAC</th>
<th id="S1.T2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">38.65 <math id="S1.T2.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.6.6.1.m1.1a"><mo id="S1.T2.6.6.1.m1.1.1" xref="S1.T2.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.6.6.1.m1.1b"><csymbol cd="latexml" id="S1.T2.6.6.1.m1.1.1.cmml" xref="S1.T2.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.6.6.1.m1.1c">\pm</annotation></semantics></math> 0.6</th>
<th id="S1.T2.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">50.35 <math id="S1.T2.7.7.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.7.7.2.m1.1a"><mo id="S1.T2.7.7.2.m1.1.1" xref="S1.T2.7.7.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.7.7.2.m1.1b"><csymbol cd="latexml" id="S1.T2.7.7.2.m1.1.1.cmml" xref="S1.T2.7.7.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.7.7.2.m1.1c">\pm</annotation></semantics></math> 0.3</th>
<td id="S1.T2.8.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.T2.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.8.8.3.1.1" class="ltx_p" style="width:85.4pt;">31.95 <math id="S1.T2.8.8.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.8.8.3.1.1.m1.1a"><mo id="S1.T2.8.8.3.1.1.m1.1.1" xref="S1.T2.8.8.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.8.8.3.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.8.8.3.1.1.m1.1.1.cmml" xref="S1.T2.8.8.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.8.8.3.1.1.m1.1c">\pm</annotation></semantics></math> 0.7</span>
</span>
</td>
<td id="S1.T2.9.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.T2.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.9.9.4.1.1" class="ltx_p" style="width:85.4pt;">36.82 <math id="S1.T2.9.9.4.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.9.9.4.1.1.m1.1a"><mo id="S1.T2.9.9.4.1.1.m1.1.1" xref="S1.T2.9.9.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.9.9.4.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.9.9.4.1.1.m1.1.1.cmml" xref="S1.T2.9.9.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.9.9.4.1.1.m1.1c">\pm</annotation></semantics></math> 0.8</span>
</span>
</td>
<td id="S1.T2.10.10.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.T2.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.10.10.5.1.1" class="ltx_p" style="width:85.4pt;">37.63 <math id="S1.T2.10.10.5.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.10.10.5.1.1.m1.1a"><mo id="S1.T2.10.10.5.1.1.m1.1.1" xref="S1.T2.10.10.5.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.10.10.5.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.10.10.5.1.1.m1.1.1.cmml" xref="S1.T2.10.10.5.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.10.10.5.1.1.m1.1c">\pm</annotation></semantics></math> 0.6</span>
</span>
</td>
</tr>
<tr id="S1.T2.15.15" class="ltx_tr">
<th id="S1.T2.15.15.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">RelNet</th>
<th id="S1.T2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">58.82 <math id="S1.T2.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.11.11.1.m1.1a"><mo id="S1.T2.11.11.1.m1.1.1" xref="S1.T2.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.11.11.1.m1.1b"><csymbol cd="latexml" id="S1.T2.11.11.1.m1.1.1.cmml" xref="S1.T2.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.11.11.1.m1.1c">\pm</annotation></semantics></math> 0.8</th>
<th id="S1.T2.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">64.27 <math id="S1.T2.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.12.12.2.m1.1a"><mo id="S1.T2.12.12.2.m1.1.1" xref="S1.T2.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.12.12.2.m1.1b"><csymbol cd="latexml" id="S1.T2.12.12.2.m1.1.1.cmml" xref="S1.T2.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.12.12.2.m1.1c">\pm</annotation></semantics></math> 0.7</th>
<td id="S1.T2.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S1.T2.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.13.13.3.1.1" class="ltx_p" style="width:85.4pt;">37.91 <math id="S1.T2.13.13.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.13.13.3.1.1.m1.1a"><mo id="S1.T2.13.13.3.1.1.m1.1.1" xref="S1.T2.13.13.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.13.13.3.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.13.13.3.1.1.m1.1.1.cmml" xref="S1.T2.13.13.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.13.13.3.1.1.m1.1c">\pm</annotation></semantics></math> 0.9</span>
</span>
</td>
<td id="S1.T2.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S1.T2.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.14.14.4.1.1" class="ltx_p" style="width:85.4pt;">45.62 <math id="S1.T2.14.14.4.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.14.14.4.1.1.m1.1a"><mo id="S1.T2.14.14.4.1.1.m1.1.1" xref="S1.T2.14.14.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.14.14.4.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.14.14.4.1.1.m1.1.1.cmml" xref="S1.T2.14.14.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.14.14.4.1.1.m1.1c">\pm</annotation></semantics></math> 0.6</span>
</span>
</td>
<td id="S1.T2.15.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S1.T2.15.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.15.15.5.1.1" class="ltx_p" style="width:85.4pt;">43.82 <math id="S1.T2.15.15.5.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T2.15.15.5.1.1.m1.1a"><mo id="S1.T2.15.15.5.1.1.m1.1.1" xref="S1.T2.15.15.5.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T2.15.15.5.1.1.m1.1b"><csymbol cd="latexml" id="S1.T2.15.15.5.1.1.m1.1.1.cmml" xref="S1.T2.15.15.5.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.15.15.5.1.1.m1.1c">\pm</annotation></semantics></math> 0.7</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T2.17.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S1.T2.18.2" class="ltx_text" style="font-size:90%;">Comparison of Validation Accuracy (%) among ViLT, MAC, and RelNet models on different VQA datasets (VQA-Compose, VQA-CP, VQA-GEN(Joint Shift), VQA-GEN(Image Shift), VQA-GEN(Question Shift)). The values represent the model’s accuracy.</span></figcaption>
</figure>
<figure id="S1.T3" class="ltx_table">
<table id="S1.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T3.2.1.1" class="ltx_tr">
<th id="S1.T3.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T3.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.1.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T3.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset Splits</span></span>
</span>
</th>
<th id="S1.T3.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.1.1.2.1.1" class="ltx_p" style="width:85.4pt;">Train/Valid/Test (Same as VQA v2)</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T3.2.2.1" class="ltx_tr">
<td id="S1.T3.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T3.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T3.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Question Length</span></span>
</span>
</td>
<td id="S1.T3.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.2.1.2.1.1" class="ltx_p" style="width:85.4pt;">5 to 15 words</span>
</span>
</td>
</tr>
<tr id="S1.T3.2.3.2" class="ltx_tr">
<td id="S1.T3.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T3.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.3.2.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T3.2.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Vocabulary Size</span></span>
</span>
</td>
<td id="S1.T3.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.3.2.2.1.1" class="ltx_p" style="width:85.4pt;">82,000 unique words</span>
</span>
</td>
</tr>
<tr id="S1.T3.2.4.3" class="ltx_tr">
<td id="S1.T3.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T3.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.4.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T3.2.4.3.1.1.1.1" class="ltx_text ltx_font_bold">QA Pairs</span></span>
</span>
</td>
<td id="S1.T3.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T3.2.4.3.2.1.1" class="ltx_p" style="width:85.4pt;">Over twenty-three million</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S1.T3.4.2" class="ltx_text" style="font-size:90%;">Statistics of the VQA-GEN Dataset</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Together, these results validate that aligned multimodal shift is imperative for VQA robustness. VQA-GEN fills this need through controlled visual and textual shifts beyond current domain generalization VQA datasets.
<br class="ltx_break">In summary, this paper contributes in the following ways:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose VQA-GEN, the <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">first</span> comprehensive benchmark dataset for domain generalization in VQA encompassing multi-modal shifts through controlled visual and textual variations.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a multi-modal pipeline for generating coherent shifts that are logically aligned across image and text domains.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We demonstrate that comprehensive multi-modal shift is pivotal for achieving robust VQA generalization.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Domain generalization</span> datasets in the context of Visual Question Answering (VQA) have garnered significant attention in research. While the VQA-CP approach divides data based on the answer distribution of the VQA v2 dataset, it fails to adequately consider crucial factors such as image and question variations. Conversely, the VQA Compose method primarily focuses on logically composed binary questions, overlooking the broader spectrum of question types. Here, we propose VQA-GEN dataset, which aims to address both image and question bias in the domain generalization of VQA.
<br class="ltx_break">
<br class="ltx_break"><span id="S2.p1.1.2" class="ltx_text ltx_font_bold">Text-based shift techniques</span> have been extensively investigated to generate meaningful variations in text.SimpleAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, is one such technique that relies on paraphrasing but lacks diversity within the dataset. Another approach, DoCoGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, utilizes the T5 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to generate text variations. However, studies have shown that the T5 model often deviates from the original syntax, resulting in a small proportion of generated sentences that closely align with the source text. Other approaches such as multitask learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, reinforced learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and augmenting pre-trained models with rewards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> , require targeted text.
<br class="ltx_break">In the realm of unsupervised approaches, techniques like disentangling style and content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, style-word editing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, utilizing generic resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, ChatGPT-based style transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, back-translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
have been investigated. For our pipeline, we employed back-translation and style transfer techniques using ChatGPT with different personas to generate controlled question shifts.
<br class="ltx_break">
<br class="ltx_break"><span id="S2.p1.1.3" class="ltx_text ltx_font_bold">Image-based shift techniques</span> encompass a range of methods used to modify the position and appearance of an image within its background. One commonly used technique is masking, where specific objects or regions within an image are hidden to introduce variations. However, masking can introduce bias and result in information loss, as it overly emphasizes question-based shifts while neglecting other important visual features and variations.
<br class="ltx_break">Another widely studied technique is image style transfer, which allows the generation of artistic paintings without requiring the expertise of a professional painter. A study by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> found that the inner products of feature maps in Convolutional Neural Networks (CNNs) can represent styles, leading to the proposal of a neural style transfer (NST) method through successive optimization iterations. However, this optimization process is time-consuming and challenging to widely apply.
<br class="ltx_break">Furthermore, techniques such as Imagenet-C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and Imagenet-P perturbations are utilized to generate image shifts. Approaches [ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>] focus on aligning the second-order statistics of style and content images to enable arbitrary style transfer. One approach presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> employs adaptive instance normalization (AdaIN) to normalize content features using the mean and variance of style features, facilitating arbitrary style transfer. Another approach, StyTr2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, incorporates separate transformer encoders to produce domain-specific sequences for content and style. Additionally, Contrastive Arbitrary Style Transfer (CAST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> considers style distribution and enables users to learn style representations directly from image attributes.
<br class="ltx_break">In our approach, we manually selected style images that preserve the semantics of the images and utilized the technique proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, along with Imagenet-C transformations, to generate shifts in images.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminary Assessment</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we conduct a preliminary assessment to validate the need for multi-modal shifts in exposing model vulnerabilities toward robustness. We evaluated three state-of-the-art VQA models from different categories: classic two-stream (RelNet), transformer (ViLT), and hybrid neuro-symbolic (MAC). The models are trained on VQA v2 and tested on VQA-CP, VQA-GEN (Image shifts only), VQA-GEN (Question shifts only), and VQA-GEN (Joint Shifts).
<br class="ltx_break">The results from Table 2 yield critical insights. Firstly, while performance on VQA-CP (50.35-67.91%) and VQA-Compose (55.82-58.82%) suggests current VQA models have good generalizability for textual shifts, their accuracy still drops substantially to 43.82-47.86% on VQA-GEN (Question Shifts). This reveals existing models are not fully robust to textual shifts.
<br class="ltx_break">Secondly, the precipitous decline in accuracy from 50.35-67.91% on VQA-CP to 36.82-50.24% on VQA-GEN (Image Shifts) highlights models’ high vulnerability to image shifts. As existing datasets focus solely on textual shifts, models are never exposed to, and thus unequipped to handle, real-world image shifts.
<br class="ltx_break">Most critically, joint shifts in VQA-GEN cause a huge performance dip, with accuracies plummeting from 50.35-67.91% on VQA-CP to 31.95-42.26% on VQA-GEN (Joint Shifts). This exposes critical gaps missed by single shifts. Using this joint shift approach allows VQA-GEN to comprehensively evaluate model robustness by introducing multi-modal
variations that reveal vulnerabilities.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.00807/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">An overview of our pipeline. We first generate image shifts to the image input through Noise and Style shifts and the input question is passed to the question shift module which generates diverse questions using translational and conversational shift techniques. Finally, the generated shifts from each modal are combined using mix and match, resulting in new image question pairs which are shown on the right.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">variations that reveal vulnerabilities.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>VQA-GEN Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Table 3 provides an overview of VQA-GEN dataset statistics. In this section, we discuss about VQA-GEN dataset generation process. Figure 2 shows the data generation pipeline to construct VQA-GEN. The pipeline consists of three main steps: (1) generating image shifts, (2) generating question shifts, and (3) mixing and matching the shifts into new image-question pairs. Although we could have used any VQA dataset, we leverage VQA v2 as the source dataset for its wide acceptance and real-world-based images.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Generating Image Shift</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our aim was to generate image shifts that preserve the essential features like shape, color, etc. required for addressing the given question. Through our exploration, we found three techniques for creating shifts in images: masking, style transfer, and noise perturbations. Out of these three, we found masking to be less effective as there is a higher chance of losing major features associated with the questions using existing masking techniques.
Consequently, we focused on two approaches: (1) Noise Injection Shifts and (2) Artistic Style Shifts.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2311.00807/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Perturbations methods in Noise Injection Shifts</span></figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Noise Injection Shifts:</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In our first technique, we draw inspiration from the ImageNet-C dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This image generation process ensures controlled perturbations that retain critical objects and features. The specific corruptions used for our pipeline, as shown in Figure 3, include:
<span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Gaussian noise</span> adds subtle pixel-level fluctuations generated by sampling from a normal distribution.
<span id="S4.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_bold">Shot noise</span> results in a grainy appearance akin to low-light photography interference via a Poisson distribution.
<span id="S4.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_bold">Impulse noise</span> introducing pixel anomalies through a Bernoulli distribution.
Blur effects of image like <span id="S4.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_bold">defocus and motion blur</span> using convolutional filters.Visual effects simulating atmospheric conditions like <span id="S4.SS1.SSS1.p1.1.5" class="ltx_text ltx_font_bold">snow, frost, and fog</span> based on procedural noise generation.
<span id="S4.SS1.SSS1.p1.1.6" class="ltx_text ltx_font_bold">brightness and contrast shifts</span> adjust image brightness and differences between light and dark areas using uniform distributions. Stylistic blur techniques like <span id="S4.SS1.SSS1.p1.1.7" class="ltx_text ltx_font_bold">glass and zoom blur</span> using FFTs.
<span id="S4.SS1.SSS1.p1.1.8" class="ltx_text ltx_font_bold">Pixelation</span> transform images into blocky forms through spatial binning.
<span id="S4.SS1.SSS1.p1.1.9" class="ltx_text ltx_font_bold">JPEG compression</span> artifacts through encoding.<span id="S4.SS1.SSS1.p1.1.10" class="ltx_text ltx_font_bold">Elastic warping distortions</span> for elastic and non-linear image deformations via spline grids.
Using this technique, we create 15 perturbations for a single input image.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Artistic Style Shifts:</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">In the second technique, we leveraged the technique of Adaptive Instance Normalization (AdaIN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. AdaIN transfers styles by normalizing feature maps from the content image to match the mean and variance statistics of feature maps from the style image. This alignment of feature statistics allows the network to stylize the content image with the artistic style of the style image. Based on our experiments, we found 4 style shifts - dramatic, art, watercolor, and cartoon - that provide significant artistic diversity while avoiding surreal/abstract styles that could distort semantic content needed for VQA.
<br class="ltx_break">The culmination of these noise and artistic style shifts yields a comprehensive set of 19 image cues per original input image.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2311.00807/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Persona-Based Question Shift</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Generating Question Shift</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The majority of text shifts explored in our research were focused on employing paraphrasing techniques. Although reasoning-based questions and logical compositions were found to be effective, they often generated different answers, which did not align with the requirements of our pipeline. Our pipeline, on the other hand, aimed to maintain a fixed answer while introducing variations in the image and question shifts. Through our investigation, we came up with two techniques aligning with controlled shift generation: (1) Translational Shifts (TS) and (2) Conversational Shifts (CS).</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Translational Shifts (TS):</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">In the first technique, we translate questions to French and back to English using MarianNMT, a machine translation framework based on the Transformer architecture for generating novel text cues. We chose French as an intermediary language because its grammatical and structural differences from English can elicit greater linguistic diversity. Through human evaluation of translations on 10 random questions, we found MarianNMT generated 8 semantically similar variations per question on average. This significantly outperformed paraphrasing models like T5, which struggled to preserve semantics. By leveraging the capabilities of MarianNMT translation paired with French’s linguistic divergence, we generate 4 variations per original question.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Conversational Shifts (CS):</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The second technique generate shifts by emulating distinct English writing styles prevalent in India, Britain, and the United States, we further categorized these styles based on personas, such as high school student (HSST), critic, professor, elementary school student (EST), and college student (CS) as shown in Figure 4. To create these diverse styles, we harnessed ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a large language model (LLM) adept at simulating various writing styles. By providing ChatGPT with hard prompts specifying a particular persona, we are able to elicit 15 distinct stylistic variations for each original question. In total, 19 textual cues are generated per input question through the combination of translational and conversational shifts.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Mixing and Matching</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The image shifts and question shifts generated from the previous steps are combined in the final step. Since the generated images and questions share the same context, we randomly associate questions with images, resulting in 19 different distributions for a single input image and question combination. This approach is consistently applied to all image-question pairs of VQA v2 to generate VQA-GEN dataset. The modular architecture of the pipeline enables its independent application to various VQA datasets, facilitating the conversion into a Domain-Generalized VQA (DG VQA) format.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Datasets</th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">VQA-GEN</th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">VQA-CP</th>
<th id="S4.T4.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">VQA-Compose</th>
<th id="S4.T4.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">CLEVR</th>
<th id="S4.T4.2.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">GQA Bal</th>
<th id="S4.T4.2.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">VQA Abs</th>
<th id="S4.T4.2.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Visual Genome</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.1" class="ltx_tr">
<th id="S4.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQA-GEN</th>
<td id="S4.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#BFBFFF;"><span id="S4.T4.2.2.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#BFBFFF;">64.2</span></td>
<td id="S4.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.3.1" class="ltx_text ltx_font_bold">75.6</span></td>
<td id="S4.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.4.1" class="ltx_text ltx_font_bold">68.4</span></td>
<td id="S4.T4.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.5.1" class="ltx_text ltx_font_bold">58.9</span></td>
<td id="S4.T4.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.6.1" class="ltx_text ltx_font_bold">53.9</span></td>
<td id="S4.T4.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.7.1" class="ltx_text ltx_font_bold">52.2</span></td>
<td id="S4.T4.2.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.1.8.1" class="ltx_text ltx_font_bold">54.8</span></td>
</tr>
<tr id="S4.T4.2.3.2" class="ltx_tr">
<th id="S4.T4.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-CP</th>
<td id="S4.T4.2.3.2.2" class="ltx_td ltx_align_center">30.3</td>
<td id="S4.T4.2.3.2.3" class="ltx_td ltx_align_center" style="background-color:#BFBFFF;"><span id="S4.T4.2.3.2.3.1" class="ltx_text" style="background-color:#BFBFFF;">66.4</span></td>
<td id="S4.T4.2.3.2.4" class="ltx_td ltx_align_center">39.4</td>
<td id="S4.T4.2.3.2.5" class="ltx_td ltx_align_center">31.0</td>
<td id="S4.T4.2.3.2.6" class="ltx_td ltx_align_center">33.6</td>
<td id="S4.T4.2.3.2.7" class="ltx_td ltx_align_center">33.1</td>
<td id="S4.T4.2.3.2.8" class="ltx_td ltx_align_center">29.7</td>
</tr>
<tr id="S4.T4.2.4.3" class="ltx_tr">
<th id="S4.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-Compose</th>
<td id="S4.T4.2.4.3.2" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T4.2.4.3.3" class="ltx_td ltx_align_center">63.1</td>
<td id="S4.T4.2.4.3.4" class="ltx_td ltx_align_center" style="background-color:#BFBFFF;"><span id="S4.T4.2.4.3.4.1" class="ltx_text" style="background-color:#BFBFFF;">84.4</span></td>
<td id="S4.T4.2.4.3.5" class="ltx_td ltx_align_center">42.1</td>
<td id="S4.T4.2.4.3.6" class="ltx_td ltx_align_center">46.4</td>
<td id="S4.T4.2.4.3.7" class="ltx_td ltx_align_center">37.2</td>
<td id="S4.T4.2.4.3.8" class="ltx_td ltx_align_center">39.5</td>
</tr>
<tr id="S4.T4.2.5.4" class="ltx_tr">
<th id="S4.T4.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CLEVR</th>
<td id="S4.T4.2.5.4.2" class="ltx_td ltx_align_center">19.6</td>
<td id="S4.T4.2.5.4.3" class="ltx_td ltx_align_center">25.7</td>
<td id="S4.T4.2.5.4.4" class="ltx_td ltx_align_center">21.3</td>
<td id="S4.T4.2.5.4.5" class="ltx_td ltx_align_center" style="background-color:#BFBFFF;"><span id="S4.T4.2.5.4.5.1" class="ltx_text" style="background-color:#BFBFFF;">84.9</span></td>
<td id="S4.T4.2.5.4.6" class="ltx_td ltx_align_center">21.4</td>
<td id="S4.T4.2.5.4.7" class="ltx_td ltx_align_center">21.7</td>
<td id="S4.T4.2.5.4.8" class="ltx_td ltx_align_center">23.1</td>
</tr>
<tr id="S4.T4.2.6.5" class="ltx_tr">
<th id="S4.T4.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GQA Bal</th>
<td id="S4.T4.2.6.5.2" class="ltx_td ltx_align_center">30.2</td>
<td id="S4.T4.2.6.5.3" class="ltx_td ltx_align_center">38.3</td>
<td id="S4.T4.2.6.5.4" class="ltx_td ltx_align_center">32.6</td>
<td id="S4.T4.2.6.5.5" class="ltx_td ltx_align_center">36.8</td>
<td id="S4.T4.2.6.5.6" class="ltx_td ltx_align_center" style="background-color:#BFBFFF;"><span id="S4.T4.2.6.5.6.1" class="ltx_text" style="background-color:#BFBFFF;">58.2</span></td>
<td id="S4.T4.2.6.5.7" class="ltx_td ltx_align_center">39.8</td>
<td id="S4.T4.2.6.5.8" class="ltx_td ltx_align_center">30.6</td>
</tr>
<tr id="S4.T4.2.7.6" class="ltx_tr">
<th id="S4.T4.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA Abs</th>
<td id="S4.T4.2.7.6.2" class="ltx_td ltx_align_center">35.7</td>
<td id="S4.T4.2.7.6.3" class="ltx_td ltx_align_center">47.2</td>
<td id="S4.T4.2.7.6.4" class="ltx_td ltx_align_center">40.5</td>
<td id="S4.T4.2.7.6.5" class="ltx_td ltx_align_center">41.2</td>
<td id="S4.T4.2.7.6.6" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T4.2.7.6.7" class="ltx_td ltx_align_center" style="background-color:#BFBFFF;"><span id="S4.T4.2.7.6.7.1" class="ltx_text" style="background-color:#BFBFFF;">56.3</span></td>
<td id="S4.T4.2.7.6.8" class="ltx_td ltx_align_center">41.3</td>
</tr>
<tr id="S4.T4.2.8.7" class="ltx_tr">
<th id="S4.T4.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Visual Genome</th>
<td id="S4.T4.2.8.7.2" class="ltx_td ltx_align_center ltx_border_b">31.5</td>
<td id="S4.T4.2.8.7.3" class="ltx_td ltx_align_center ltx_border_b">46.5</td>
<td id="S4.T4.2.8.7.4" class="ltx_td ltx_align_center ltx_border_b">39.2</td>
<td id="S4.T4.2.8.7.5" class="ltx_td ltx_align_center ltx_border_b">32.1</td>
<td id="S4.T4.2.8.7.6" class="ltx_td ltx_align_center ltx_border_b">34.4</td>
<td id="S4.T4.2.8.7.7" class="ltx_td ltx_align_center ltx_border_b">35.2</td>
<td id="S4.T4.2.8.7.8" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#BFBFFF;"><span id="S4.T4.2.8.7.8.1" class="ltx_text" style="background-color:#BFBFFF;">41.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Validation accuracy (%) of VQA datasets on ViLT model. Blue shading represents in-domain accuracy, highlighting the diagonal cells where each dataset’s performance on itself is shown for reference.The remaining cells indicate cross-domain accuracy.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section discusses the experimental settings used to validate the need for the VQA-GEN dataset, including the datasets and evaluation metrics used, and the baselines, followed by a detailed analysis of the experiments. We conduct a series of experiments to answer the following research questions.</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p"><span id="S5.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">RQ.1:</span> Does the identified multi-modal dataset improve the in-domain and cross-domain performance of the models?</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p"><span id="S5.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">RQ.2:</span> What is the importance of each VQA-GEN generation pipeline shift technique in improving the generalization performance?</p>
</div>
</li>
<li id="S5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S5.I1.ix3.p1" class="ltx_para">
<p id="S5.I1.ix3.p1.1" class="ltx_p"><span id="S5.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">RQ.3:</span> Does VQA-GEN provides diverse variations while maintaining relevance for improving VQA models?</p>
</div>
</li>
</ol>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baseline Datasets</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">COCO-QA (120k images, 330k QA pairs) focuses on VQA with diverse images and question-answer sets. Visual Genome (108k images, 1.7M QA pairs) provides extensive image annotations for scene understanding. CLEVR (100k synthesized images, 1M QA pairs) evaluates reasoning with complex questions about 3D scenes. Visual7W (47k images, 327k QA pairs) tests natural language understanding and reasoning. VQA Abstract (50k abstract scenes, 150k QA pairs) challenges models with abstract images. VizWiz (31k real images, 160k QA pairs) addresses real-world VQA for blind users. VQA v2 (204k COCO images, 1.1M QA pairs) provides a VQA benchmark with additional data. VQA-CP (204k COCO images, 2.8M QA pairs) tests generalization with counterfactual questions.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Metrics</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We use these metrics for the evaluation of our experiment:
<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Accuracy :</span>
We calculate top-1 accuracy on the validation split for each dataset by matching predictions to ground-truth answers. This evaluates model performance on the variations.
<span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">MMD Loss :</span>
We utilize the Maximum Mean Discrepancy (MMD) metric for domain shift analysis, a statistical test measuring the difference between probability distributions. In this, we use a pre-trained ResNet-101 model to extract visual features, encompassing high and low-level information about the image. In the question space, BERT encodes 10,000 questions to capture semantic content, and we extract 20 syntactical features like question length. By calculating the MMD based on these features, we quantify distribution shift, with lower MMD values indicating greater similarity between distributions.
<span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">Similarity Analysis :</span>
To compare the original dataset with VQA-GEN, we employed similarity metrics like BLEU-1, BLEU-2, BLEU-3, BLEU-4, and METEOR to assess the quality and diversity of the generated questions.
<span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_bold">BLEU</span> (Bilingual Evaluation Understudy) measures similarity based on overlapping n-grams between a candidate and reference text. Higher BLEU scores indicate greater similarity.
<span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_bold">METEOR</span> (Metric for Evaluation of Translation with Explicit Ordering) calculates similarity using explicit word-to-word matches, with recall weighted higher than precision.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Implementation Details</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In our experiments, we utilized the ViLT architecture for our training process. Our models underwent 100 epochs of training on 2 Nvidia Titan X GPUs, employing the Adam optimizer and a batch size of 64.
<br class="ltx_break">We created custom splits for VQA-GEN to evaluate image and question shifts. We had a training split for image shifts with 10 types of noise shifts (Gaussian Noise, Shot Noise, etc.) applied to 400,000 QA pairs. The test split had 5 additional noise shifts (Glass Blur, JPEG artifacts, etc.) applied to 200,000 QA pairs. We also had a training split with 2 style shifts (Dramatic, Art) applied to 400,000 QA pairs, and a test split with 2 additional style shifts (Watercolor, Cartoon) applied to 200,000 QA pairs. For question shifts, the training split contained 400,000 QA pairs with translational (trans.) shifts while the test split had 200,000 QA pairs with translational shifts. Additionally, the conversational(conv.) shift training split had 3 personas (High School Student, Critic, Professor) applied to 400,000 QA pairs, and the test split had 2 more personas (Elementary Student, College Student) applied to 200,000 QA pairs.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.8.9.1" class="ltx_tr">
<th id="S5.T5.8.9.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.8.9.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.8.9.1.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="S5.T5.8.9.1.1.1.1.1" class="ltx_text ltx_font_bold">Training Splits</span></span>
</span>
</th>
<th id="S5.T5.8.9.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.8.9.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.8.9.1.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="S5.T5.8.9.1.2.1.1.1" class="ltx_text ltx_font_bold">Test Splits</span></span>
</span>
</th>
<th id="S5.T5.8.9.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S5.T5.8.9.1.3.1" class="ltx_text ltx_font_bold">Validation Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1" class="ltx_tr">
<td id="S5.T5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.2.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts</span>
</span>
</td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.3.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts Test</span>
</span>
</td>
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="69.3\pm 1.2" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mrow id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml"><mn id="S5.T5.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.m1.1.1.2.cmml">69.3</mn><mo id="S5.T5.1.1.1.m1.1.1.1" xref="S5.T5.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.m1.1.1.3.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.m1.1.1.2">69.3</cn><cn type="float" id="S5.T5.1.1.1.m1.1.1.3.cmml" xref="S5.T5.1.1.1.m1.1.1.3">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">69.3\pm 1.2</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts</span>
</span>
</td>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.3.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts Test</span>
</span>
</td>
<td id="S5.T5.2.2.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.2.2.1.m1.1" class="ltx_Math" alttext="67.1\pm 0.8" display="inline"><semantics id="S5.T5.2.2.1.m1.1a"><mrow id="S5.T5.2.2.1.m1.1.1" xref="S5.T5.2.2.1.m1.1.1.cmml"><mn id="S5.T5.2.2.1.m1.1.1.2" xref="S5.T5.2.2.1.m1.1.1.2.cmml">67.1</mn><mo id="S5.T5.2.2.1.m1.1.1.1" xref="S5.T5.2.2.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.2.2.1.m1.1.1.3" xref="S5.T5.2.2.1.m1.1.1.3.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.m1.1b"><apply id="S5.T5.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.2.2.1.m1.1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.2.2.1.m1.1.1.2.cmml" xref="S5.T5.2.2.1.m1.1.1.2">67.1</cn><cn type="float" id="S5.T5.2.2.1.m1.1.1.3.cmml" xref="S5.T5.2.2.1.m1.1.1.3">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.m1.1c">67.1\pm 0.8</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.3.3" class="ltx_tr">
<td id="S5.T5.3.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.2.1.1" class="ltx_p" style="width:170.7pt;">Trans. Shifts</span>
</span>
</td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.1.1" class="ltx_p" style="width:170.7pt;">Trans. Shifts Test</span>
</span>
</td>
<td id="S5.T5.3.3.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.3.3.1.m1.1" class="ltx_Math" alttext="66.8\pm 1.4" display="inline"><semantics id="S5.T5.3.3.1.m1.1a"><mrow id="S5.T5.3.3.1.m1.1.1" xref="S5.T5.3.3.1.m1.1.1.cmml"><mn id="S5.T5.3.3.1.m1.1.1.2" xref="S5.T5.3.3.1.m1.1.1.2.cmml">66.8</mn><mo id="S5.T5.3.3.1.m1.1.1.1" xref="S5.T5.3.3.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.3.3.1.m1.1.1.3" xref="S5.T5.3.3.1.m1.1.1.3.cmml">1.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.m1.1b"><apply id="S5.T5.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.3.3.1.m1.1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.3.3.1.m1.1.1.2.cmml" xref="S5.T5.3.3.1.m1.1.1.2">66.8</cn><cn type="float" id="S5.T5.3.3.1.m1.1.1.3.cmml" xref="S5.T5.3.3.1.m1.1.1.3">1.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.m1.1c">66.8\pm 1.4</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.4.4" class="ltx_tr">
<td id="S5.T5.4.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.4.4.2.1.1" class="ltx_p" style="width:170.7pt;">Conv. Shifts</span>
</span>
</td>
<td id="S5.T5.4.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.4.4.3.1.1" class="ltx_p" style="width:170.7pt;">Conv. Shifts Test</span>
</span>
</td>
<td id="S5.T5.4.4.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.4.4.1.m1.1" class="ltx_Math" alttext="65.4\pm 2.3" display="inline"><semantics id="S5.T5.4.4.1.m1.1a"><mrow id="S5.T5.4.4.1.m1.1.1" xref="S5.T5.4.4.1.m1.1.1.cmml"><mn id="S5.T5.4.4.1.m1.1.1.2" xref="S5.T5.4.4.1.m1.1.1.2.cmml">65.4</mn><mo id="S5.T5.4.4.1.m1.1.1.1" xref="S5.T5.4.4.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.4.4.1.m1.1.1.3" xref="S5.T5.4.4.1.m1.1.1.3.cmml">2.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.1.m1.1b"><apply id="S5.T5.4.4.1.m1.1.1.cmml" xref="S5.T5.4.4.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.4.4.1.m1.1.1.1.cmml" xref="S5.T5.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.4.4.1.m1.1.1.2.cmml" xref="S5.T5.4.4.1.m1.1.1.2">65.4</cn><cn type="float" id="S5.T5.4.4.1.m1.1.1.3.cmml" xref="S5.T5.4.4.1.m1.1.1.3">2.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.1.m1.1c">65.4\pm 2.3</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.5.5" class="ltx_tr">
<td id="S5.T5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.5.5.2.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts + Trans. Shifts</span>
</span>
</td>
<td id="S5.T5.5.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.5.5.3.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts Test + Trans. Shifts Test</span>
</span>
</td>
<td id="S5.T5.5.5.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.5.5.1.m1.1" class="ltx_Math" alttext="62.5\pm 0.6" display="inline"><semantics id="S5.T5.5.5.1.m1.1a"><mrow id="S5.T5.5.5.1.m1.1.1" xref="S5.T5.5.5.1.m1.1.1.cmml"><mn id="S5.T5.5.5.1.m1.1.1.2" xref="S5.T5.5.5.1.m1.1.1.2.cmml">62.5</mn><mo id="S5.T5.5.5.1.m1.1.1.1" xref="S5.T5.5.5.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.5.5.1.m1.1.1.3" xref="S5.T5.5.5.1.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.1.m1.1b"><apply id="S5.T5.5.5.1.m1.1.1.cmml" xref="S5.T5.5.5.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.5.5.1.m1.1.1.1.cmml" xref="S5.T5.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.5.5.1.m1.1.1.2.cmml" xref="S5.T5.5.5.1.m1.1.1.2">62.5</cn><cn type="float" id="S5.T5.5.5.1.m1.1.1.3.cmml" xref="S5.T5.5.5.1.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.1.m1.1c">62.5\pm 0.6</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.6.6" class="ltx_tr">
<td id="S5.T5.6.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.6.2.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts + Conv. Shifts</span>
</span>
</td>
<td id="S5.T5.6.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.6.3.1.1" class="ltx_p" style="width:170.7pt;">Noise Shifts Test + Conv. Shifts</span>
</span>
</td>
<td id="S5.T5.6.6.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.6.6.1.m1.1" class="ltx_Math" alttext="63.7\pm 1.1" display="inline"><semantics id="S5.T5.6.6.1.m1.1a"><mrow id="S5.T5.6.6.1.m1.1.1" xref="S5.T5.6.6.1.m1.1.1.cmml"><mn id="S5.T5.6.6.1.m1.1.1.2" xref="S5.T5.6.6.1.m1.1.1.2.cmml">63.7</mn><mo id="S5.T5.6.6.1.m1.1.1.1" xref="S5.T5.6.6.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.6.6.1.m1.1.1.3" xref="S5.T5.6.6.1.m1.1.1.3.cmml">1.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.1.m1.1b"><apply id="S5.T5.6.6.1.m1.1.1.cmml" xref="S5.T5.6.6.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.6.6.1.m1.1.1.1.cmml" xref="S5.T5.6.6.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.6.6.1.m1.1.1.2.cmml" xref="S5.T5.6.6.1.m1.1.1.2">63.7</cn><cn type="float" id="S5.T5.6.6.1.m1.1.1.3.cmml" xref="S5.T5.6.6.1.m1.1.1.3">1.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.1.m1.1c">63.7\pm 1.1</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.7.7" class="ltx_tr">
<td id="S5.T5.7.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.7.7.2.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts + Trans. Shifts</span>
</span>
</td>
<td id="S5.T5.7.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.7.7.3.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts Test + Trans. Shifts Test</span>
</span>
</td>
<td id="S5.T5.7.7.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.7.7.1.m1.1" class="ltx_Math" alttext="63.2\pm 0.3" display="inline"><semantics id="S5.T5.7.7.1.m1.1a"><mrow id="S5.T5.7.7.1.m1.1.1" xref="S5.T5.7.7.1.m1.1.1.cmml"><mn id="S5.T5.7.7.1.m1.1.1.2" xref="S5.T5.7.7.1.m1.1.1.2.cmml">63.2</mn><mo id="S5.T5.7.7.1.m1.1.1.1" xref="S5.T5.7.7.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.7.7.1.m1.1.1.3" xref="S5.T5.7.7.1.m1.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.1.m1.1b"><apply id="S5.T5.7.7.1.m1.1.1.cmml" xref="S5.T5.7.7.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.7.7.1.m1.1.1.1.cmml" xref="S5.T5.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.7.7.1.m1.1.1.2.cmml" xref="S5.T5.7.7.1.m1.1.1.2">63.2</cn><cn type="float" id="S5.T5.7.7.1.m1.1.1.3.cmml" xref="S5.T5.7.7.1.m1.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.1.m1.1c">63.2\pm 0.3</annotation></semantics></math></td>
</tr>
<tr id="S5.T5.8.8" class="ltx_tr">
<td id="S5.T5.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.8.8.2.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts + Conv. Shifts</span>
</span>
</td>
<td id="S5.T5.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S5.T5.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.8.8.3.1.1" class="ltx_p" style="width:170.7pt;">Style Shifts Test + Conv. Shifts Test</span>
</span>
</td>
<td id="S5.T5.8.8.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math id="S5.T5.8.8.1.m1.1" class="ltx_Math" alttext="62.9\pm 0.2" display="inline"><semantics id="S5.T5.8.8.1.m1.1a"><mrow id="S5.T5.8.8.1.m1.1.1" xref="S5.T5.8.8.1.m1.1.1.cmml"><mn id="S5.T5.8.8.1.m1.1.1.2" xref="S5.T5.8.8.1.m1.1.1.2.cmml">62.9</mn><mo id="S5.T5.8.8.1.m1.1.1.1" xref="S5.T5.8.8.1.m1.1.1.1.cmml">±</mo><mn id="S5.T5.8.8.1.m1.1.1.3" xref="S5.T5.8.8.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.1.m1.1b"><apply id="S5.T5.8.8.1.m1.1.1.cmml" xref="S5.T5.8.8.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.8.8.1.m1.1.1.1.cmml" xref="S5.T5.8.8.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T5.8.8.1.m1.1.1.2.cmml" xref="S5.T5.8.8.1.m1.1.1.2">62.9</cn><cn type="float" id="S5.T5.8.8.1.m1.1.1.3.cmml" xref="S5.T5.8.8.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.1.m1.1c">62.9\pm 0.2</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.10.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.11.2" class="ltx_text" style="font-size:90%;">Validation Accuracy (%) of ViLT Model on VQA-GEN Custom Splits(refer implementation details section).</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>RQ.1 Performance Comparison</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In this section, we evaluate the cross-domain and in-domain generalization capabilities of the VQA-GEN dataset. In-domain refers to training and evaluating on the same dataset, while cross-domain involves training on one dataset and evaluating on another. Table 4 shows the comparison results of VQA-GEN and the existing VQA datasets. Here, the ViLT model was trained and evaluated on: VQA-GEN, VQA-Compose, VQA-CP, CLEVR, VizWiz, VQA Abstract, and Visual Genome. We observe that VQA-GEN dataset proves to be useful source data when transferring knowledge to other VQA datasets, achieving the best results on VQA-CP with a validation accuracy of 75.6% and least for VQA Abs. which is around 52%. We also noted that existing domain generalization datasets like VQA-CP and VQA-Compose do improve the model’s performance on some cross-domain datasets. For VQA-CP, it scores lower than VQA-GEN on 6 out of 7 cross-domain datasets, with accuracies in the 30-47% range. Meanwhile, VQA-Compose achieves the top accuracy on CLEVR (42.1%) but poorer performance on VQA Abstract (37.2%) and Visual Genome (39.5%). This suggests while some domains transfer well, others exhibit a larger gap. Apart from that, the CLEVR dataset appears to have the least impact on cross-domain tasks, yielding accuracy rates of approximately 20-25% across all datasets.
<br class="ltx_break">Furthermore, we investigated in-domain generalization, represented along the left diagonal in Table 3. VQA-GEN attains 64.2% validation accuracy, indicating that ViLT successfully learns from its training set. However, this is lower than existing domain generalization datasets like VQA-CP (66.4%) and VQA-Compose (84.4%). In-domain validation accuracy is lowest for Visual Genome, at 41.0%, suggesting that the model doesn’t learn as much from its training split. We can see that domain generalization datasets proved to be effective for the model by providing a wide range of shifts that enable the model to adapt effectively. Notably, VQA-GEN distinguishes itself due to its unique composition, encompassing joint shifts in both image and question domains and also validating the RQ.1.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>RQ.2 In-dataset Generalization</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Table 5 shows the effectiveness of each shift technique in the VQA-GEN data generation pipeline which we call In-dataset generalization. Here we determine whether the performance gains of VQA-GEN result from the applied shifts or the expanded scale of the dataset. ViLT model was trained on each training split and evaluated on testing splits of VQA-GEN mentioned in the implementation detail section. We also combine training splits and evaluate combined test splits to analyze the effect of modeling diverse shifts. We find that noise injection shifts achieve the highest accuracy of 69.3%, indicating the model is most robust to visual corruptions. Artistic style transfers (67.1%) also provide relatively effective generalization, likely due to the model’s ability to adapt to stylistic image changes. However, textual shifts like translation (66.8%) and conversational shifts (65.4%) prove more challenging. The model may struggle to handle semantic variations in questions generated through these approaches. Combining training splits leads to lower accuracy than individual shifts. For instance, accuracy drops to 62.5% when jointly modeling noise injection and translational shifts. This highlights the difficulty in adapting to the full diversity of shifts in VQA-GEN. We found that all these shift techniques provide a challenging evaluation dataset to assess the model. Also on the joint shift data, we observe the model’s performance drops significantly to the unimodal shifts proving the usefulness of the dataset for training robust model(RQ2).</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00807/assets/Figure_1.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">High and low level Image feature shifts.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00807/assets/Figure_2.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Syntactic and semantic shifts for question distributions. </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Domain shift analysis between VQA-GEN and other VQA datasets.</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2311.00807/assets/similarity.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="193" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Bar chart illustrating the evaluation of question relevance between VQA-GEN and VQA v2 using metrics BLEU-1, BLEU-2, BLEU-3, BLEU-4 and METEOR</span></figcaption>
</figure>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Quality of VQA-GEN dataset</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">In this section, we analyze the quality of the VQA-GEN dataset using standard metrics and also compare the domain shift gap between other VQA datasets. Figure 5 shows how the image and question of existing VQA datasets differ from VQA-GEN dataset. We compared VQA-GEN with eight well-established benchmark datasets, namely COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, VQA Abs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, VQA v2, and VQA-CP.
The findings demonstrate that although VQA-GEN is derived from VQA v2, it exhibits different distribution patterns in terms of both image (0.025, 0.0038) and text (0.06, 0.11) compared to VQA v2. This indicates that VQA-GEN differs from the original VQA v2 dataset. Regarding image space, VQA Abstract shows the highest distribution gap with VQA-GEN (0.05, 0.07), while VQA v2 and VQA-CP exhibit the lowest gap.
<br class="ltx_break">Similarly, in the question space, the syntactic shift is highest for VizWiz (0.14) and lowest for COCO-QA (0.02). In terms of semantic shift, VQA Abs leads with a score of 0.13, while Visual7W shows the least shift at 0.04. We observe that VQA-GEN introduces a domain gap when compared to other benchmark VQA datasets while maintaining an unbiased nature towards any specific dataset.
<br class="ltx_break">To further validate the quality of VQA-GEN, we performed a human evaluation study (details in supplementary) which showed the generated image-question pairs maintain high relevance based on human judgements.
Moving on to image and question context, Figure 6 presents a comparison of evaluations for questions’ relevance using BLEU and METEOR scores. BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores range from 0.6 to 0.7, indicating substantial context similarity with the original questions. Additionally, the METEOR score (0.533900) suggests that the generated questions capture a broad spectrum of semantic variations. For image diversity, we evaluated the VQA-GEN dataset using two standard metrics - Inception Score and Fréchet Inception Distance (FID). The Inception Score is 111.49 (higher is better), indicating high image diversity. The FID score of 0.50 versus VQA v2 suggests strong contextual consistency between datasets. From the analyses, we observe that VQA-GEN introduces greater question and image diversity while maintaining relevance for VQA, addressing RQ3.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we make an attempt to solve the lack of comprehensive benchmark datasets for VQA and propose VQA-GEN, the first multi-modal dataset for distribution shifts incorporating systematic and aligned shifts across visual and textual domains. The dataset was generated via an automated pipeline incorporating techniques like style transfer and backward translation to introduce visual and textual variations while preserving contextual coherence. Our dataset exposes fragility in existing VQA models reliant on unimodal distributions. Key results confirmed that textual variations alone are insufficient - truly robust VQA requires expansions in both visual and textual domains. By enabling models to learn more flexible joint representations, VQA-GEN represents a useful resource for training adaptable VQA systems that can reliably perform under diverse real-world conditions. Overall, this work took a significant step towards comprehensive domain generalization for multi-modal VQA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, “Don’t just assume; look and
answer: Overcoming priors for visual question answering,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1712.00377, 2017. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1712.00377" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1712.00377</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Gokhale, P. Banerjee, C. Baral, and Y. Yang, “VQA-LOL: visual question
answering under the lens of logic,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2002.08325, 2020.
[Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2002.08325" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2002.08325</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Kil, C. Zhang, D. Xuan, and W. Chao, “Discovering the unknown knowns:
Turning implicit knowledge in the dataset into explicit training examples for
visual question answering,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2109.06122, 2021.
[Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2109.06122" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2109.06122</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
N. Calderon, E. Ben-David, A. Feder, and R. Reichart, “Docogen: Domain
counterfactual generation for low resource domain adaptation,” 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Niu, S. Rao, and M. Carpuat, “Multi-task neural models for translating
between styles within and across languages,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
27th International Conference on Computational Linguistics</em>.   Santa Fe, New Mexico, USA: Association for
Computational Linguistics, Aug. 2018, pp. 1008–1021. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/C18-1086" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/C18-1086</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Sancheti, K. Krishna, B. V. Srinivasan, and A. Natarajan, “Reinforced
rewards framework for text style transfer,” 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Lai, A. Toral, and M. Nissim, “Thank you BART! rewarding pre-trained
models improves formality style transfer,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 2:
Short Papers)</em>.   Online: Association
for Computational Linguistics, Aug. 2021, pp. 484–494. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/2021.acl-short.62" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-short.62</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Shen, T. Lei, R. Barzilay, and T. Jaakkola, “Style transfer from
non-parallel text by cross-alignment,” 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Wu, X. Ren, F. Luo, and X. Sun, “A hierarchical reinforced sequence
operation method for unsupervised text style transfer.”   ACL, Jul. 2019, pp. 4873–4883. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/P19-1482" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1482</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Chawla and D. Yang, “Semi-supervised formality style transfer using
language model discriminator and mutual information maximization,” in
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP
2020</em>.   Online: Association for
Computational Linguistics, Nov. 2020, pp. 2340–2354. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/2020.findings-emnlp.212" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.findings-emnlp.212</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Lai, A. Toral, and M. Nissim, “Multidimensional evaluation for text style
transfer using chatgpt,” 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Junczys-Dowmunt, R. Grundkiewicz, T. Dwojak, H. Hoang, K. Heafield,
T. Neckermann, F. Seide, U. Germann, A. F. Aji, N. Bogoychev, A. F. T.
Martins, and A. Birch, “Marian: Fast neural machine translation in
C++,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL 2018, System Demonstrations</em>.   Melbourne, Australia: Association for
Computational Linguistics, Jul. 2018, pp. 116–121. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/P18-4020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P18-4020</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using
convolutional neural networks,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, 2016, pp. 2414–2423.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Hendrycks and T. Dietterich, “Benchmarking neural network robustness to
common corruptions and perturbations,” 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X. Huang and S. Belongie, “Arbitrary style transfer in real-time with adaptive
instance normalization,” 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal style
transfer via feature transforms,” 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Deng, F. Tang, W. Dong, C. Ma, X. Pan, L. Wang, and C. Xu, “Stytr<sup id="bib.bib18.2.1" class="ltx_sup">2</sup>:
Image style transfer with transformers,” 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Zhang, F. Tang, W. Dong, H. Huang, C. Ma, T.-Y. Lee, and C. Xu, “Domain
enhanced arbitrary image style transfer via contrastive learning,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Special Interest Group on Computer Graphics and Interactive Techniques
Conference Proceedings</em>.   ACM, aug
2022. [Online]. Available: <a target="_blank" href="https://doi.org/10.1145%2F3528233.3530736" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145%2F3528233.3530736</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. Bishop, “A computer wrote this paper: What chatgpt means for education,
research, and writing,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Research, and Writing (January 26, 2023)</em>,
2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Ren, R. Kiros, and R. Zemel, “Exploring models and data for image question
answering,” 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and F.-F. Li,
“Visual genome: Connecting language and vision using crowdsourced dense
image annotations,” 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and
R. Girshick, “Clevr: A diagnostic dataset for compositional language and
elementary visual reasoning,” 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei, “Visual7w: Grounded question
answering in images,” 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Batra, and
D. Parikh, “Vqa: Visual question answering,” 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P.
Bigham, “Vizwiz grand challenge: Answering visual questions from blind
people,” 2018.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text" style="font-size:144%;">Supplementary Material </span></p>
</div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Human Evaluation of Image-Question Relevance</h2>

<section id="S7.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Methods</h3>

<div id="S7.SSx1.p1" class="ltx_para">
<p id="S7.SSx1.p1.1" class="ltx_p">To assess the quality of the generated image-question pairs, we conducted a survey study involving human participants to evaluate relevance. We recruited 10 students from Arizona State University to take part in an online survey consisting of 10 sets, each comprising an image, an original question, a computer-generated question, and an irrelevant question. Participants were asked to rate the relevance of each question in relation to the paired image using a 5-point Likert scale (1=Not Relevant, 5=Highly Relevant).
<br class="ltx_break">The primary objectives of this evaluation were as follows:</p>
</div>
<div id="S7.SSx1.p2" class="ltx_para">
<p id="S7.SSx1.p2.1" class="ltx_p">1) To determine whether the generated image maintains relevance with the original question. This assessment validates that image manipulation alone does not alter the contextual meaning of the question.
2) To assess whether both the generated image and question remain relevant when shifted together. This evaluation aims to confirm that concurrent adjustments to both image and question preserve relevance.
3) To ascertain whether an irrelevant question is evidently irrelevant when paired with the generated image. This test verifies that image manipulation does not introduce new irrelevant elements.</p>
</div>
</section>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Results</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">A repeated measures ANOVA analysis revealed a significant impact of question type on relevance ratings (F(2,28)=239.42, p¡.001). Further planned contrasts disclosed no noteworthy difference between the relevance ratings for the original (M=3.97, see Figure 1) and generated (M=3.82) questions (t(14)=1.59, p=.134). This result supports the notion that relevance is maintained even after joint shifting of image and question. However, it was observed that generated questions (M=3.82, see Figure 2) were consistently rated as significantly more relevant than irrelevant questions (M=1.23, see Figure 3) (t(14)=20.14, p¡.001), thus confirming that image manipulation did not introduce irrelevant features.
<br class="ltx_break">Qualitative feedback provided additional insights, noting that the generated and irrelevant questions exhibited structural similarities, but the irrelevant questions contained random words that altered their intent and meaning. Overall, these findings underscore the effectiveness of our joint image-question generation approach in producing contextually relevant training data that aligns with human judgments of relevance.</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="/html/2311.00807/assets/x5.jpg" id="S7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S7.F7.3.2" class="ltx_text" style="font-size:90%;">Relevance Ratings for Original Questions</span></figcaption>
</figure>
<figure id="S7.F8" class="ltx_figure"><img src="/html/2311.00807/assets/x6.jpg" id="S7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S7.F8.3.2" class="ltx_text" style="font-size:90%;">Relevance Ratings for Generated Questions</span></figcaption>
</figure>
<figure id="S7.F9" class="ltx_figure"><img src="/html/2311.00807/assets/x7.jpg" id="S7.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="597" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S7.F9.3.2" class="ltx_text" style="font-size:90%;">Relevance Ratings for Irrelevant Questions</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.00806" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.00807" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.00807">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.00807" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.00808" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 21:49:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
