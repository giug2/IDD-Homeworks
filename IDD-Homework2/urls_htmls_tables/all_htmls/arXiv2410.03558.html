<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features</title>
<!--Generated on Thu Oct 10 10:50:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03558v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S1" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S2" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S3" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries: Architecture of Diffusion U-Nets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Distinct Properties of Diffusion U-Nets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.SS1" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Asymmetric Diffusion Noises</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.SS2" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>In-Resolution Granularity Changes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.SS3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Locality without Positional Embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.SS4" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Universality of Three Properties</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Enhanced Feature Selection from Diffusion U-Nets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.SS1" title="In 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Qualitative Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.SS2" title="In 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Quantitative Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experimental Validation on Multiple Discriminative Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.SS1" title="In 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Empirical Results on Semantic Correspondence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.SS2" title="In 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Empirical Results on Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.SS3" title="In 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Empirical Results on Label-Scarce Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S7" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Visualization for Distinct Properties of Diffusion U-Nets</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.SS1" title="In Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Visualization of Other Sample Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.SS2" title="In Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Visualization from Other Diffusion Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Details of Our Method</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2.SS1" title="In Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Feature Selection Solution for SDv1.5</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2.SS2" title="In Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Feature Selection Solution for SDXL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2.SS3" title="In Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Feature Selection Solution with Additional Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2.SS4" title="In Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Alternative Feature Selection Solution with Additional Techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3.SS1" title="In Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Semantic Correspondence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3.SS2" title="In Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3.SS3" title="In Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Label-Scarce Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.SS1" title="In Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Generalizability across Different Scenes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.SS2" title="In Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Quantitative Comparison Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Other Properties of Diffusion U-Nets</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.SS1" title="In Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>High-Frequency Noises in Increment Activations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.SS2" title="In Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Detailed In-Resolution Granularity Change</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.SS3" title="In Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.3 </span>Collaboration between Embedded ViTs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A6" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Future Direction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A7" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Computation Resources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A8" title="In Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Asset License</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Not All Diffusion Model Activations
<br class="ltx_break"/>Have Been Evaluated as Discriminative Features</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="id12.12.12" style="width:398.3pt;">
<span class="ltx_p ltx_align_center" id="id6.6.6.6"><span class="ltx_text" id="id5.5.5.5.5" style="font-size:120%;">Benyuan Meng<sup class="ltx_sup" id="id5.5.5.5.5.1"><span class="ltx_text ltx_font_italic" id="id5.5.5.5.5.1.1">1,2</span></sup> ‚ÄÉQianqian Xu<sup class="ltx_sup" id="id5.5.5.5.5.2"><span class="ltx_text ltx_font_italic" id="id5.5.5.5.5.2.1">3,4</span></sup> ‚ÄÉZitai Wang<sup class="ltx_sup" id="id5.5.5.5.5.3"><span class="ltx_text ltx_font_italic" id="id5.5.5.5.5.3.1">3</span></sup>
<br class="ltx_break"/>
Xiaochun Cao<sup class="ltx_sup" id="id5.5.5.5.5.4"><span class="ltx_text ltx_font_italic" id="id5.5.5.5.5.4.1">5</span></sup> ‚ÄÉQingming Huang<sup class="ltx_sup" id="id5.5.5.5.5.5"><span class="ltx_text ltx_font_italic" id="id5.5.5.5.5.5.1">6,3,7‚àó</span></sup>
<br class="ltx_break"/></span>
<sup class="ltx_sup" id="id6.6.6.6.6">1</sup>Institute of Information Engineering, CAS</span>
<span class="ltx_p ltx_align_center" id="id7.7.7.7"><sup class="ltx_sup" id="id7.7.7.7.1">2</sup>School of Cyber Security, University of Chinese Academy of Sciences</span>
<span class="ltx_p ltx_align_center" id="id8.8.8.8"><sup class="ltx_sup" id="id8.8.8.8.1">3</sup>Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS</span>
<span class="ltx_p ltx_align_center" id="id9.9.9.9"><sup class="ltx_sup" id="id9.9.9.9.1">4</sup>Peng Cheng Laboratory</span>
<span class="ltx_p ltx_align_center" id="id10.10.10.10"><sup class="ltx_sup" id="id10.10.10.10.1">5</sup>School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University</span>
<span class="ltx_p ltx_align_center" id="id11.11.11.11"><sup class="ltx_sup" id="id11.11.11.11.1">6</sup>School of Computer Science and Tech., University of Chinese Academy of Sciences</span>
<span class="ltx_p ltx_align_center" id="id12.12.12.12"><sup class="ltx_sup" id="id12.12.12.12.1">7</sup>Key Laboratory of Big Data Mining and Knowledge Management, CAS</span>
<span class="ltx_p ltx_align_center" id="id12.12.12.13"><span class="ltx_text ltx_font_typewriter" id="id12.12.12.13.1">mengbenyuan@iie.ac.cn</span>‚ÄÉ<span class="ltx_text ltx_font_typewriter" id="id12.12.12.13.2">{xuqianqian,wangzitai}@ict.ac.cn
<br class="ltx_break"/>caoxiaochun@mail.sysu.edu.cn</span>‚ÄÉ<span class="ltx_text ltx_font_typewriter" id="id12.12.12.13.3">qmhuang@ucas.ac.cn</span></span>
</span>
</span><span class="ltx_author_notes"><span class="ltx_text" id="id13.13.id1" style="font-size:120%;">Corresponding authors.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Diffusion models are initially designed for image generation.
Recent research shows that the internal signals within their backbones, named activations, can also serve as dense features for various discriminative tasks such as semantic segmentation.
Given numerous activations, selecting a small yet effective subset poses a fundamental problem.
To this end, the early study of this field performs a large-scale quantitative comparison of the discriminative ability of the activations.
However, we find that many potential activations have not been evaluated, such as the queries and keys used to compute attention scores.
Moreover, recent advancements in diffusion architectures bring many new activations, such as those within embedded ViT modules.
Both combined, activation selection remains unresolved but overlooked.
To tackle this issue, this paper takes a further step with a much broader range of activations evaluated.
Considering the significant increase in activations, a full-scale quantitative comparison is no longer operational.
Instead, we seek to understand the properties of these activations, such that the activations that are clearly inferior can be filtered out in advance via simple qualitative evaluation.
After careful analysis, we discover three properties universal among diffusion models, enabling this study to go beyond specific models.
On top of this, we present effective feature selection solutions for several popular diffusion models.
Finally, the experiments across multiple discriminative tasks validate the superiority of our method over the SOTA competitors.
Our code is available at <a class="ltx_ref ltx_href" href="https://github.com/Darkbblue/generic-diffusion-feature" title="">this url</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Diffusion models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib36" title="">36</a>]</cite> are powerful generative models that progressively reconstruct images from Gaussian noises through a series of denoising steps.
Typically, a U-Net¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite> is trained as the noise predictor backbone to perform denoising.
Recently, the impressive generative capability inspires the application to discriminative tasks such as semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>]</cite> or semantic correspondence¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib56" title="">56</a>]</cite>.
In this direction, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">diffusion feature</span> is one simple yet effective approach, where the intermediate signals, named activations, are extracted from the pre-trained diffusion U-Net as dense features¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The complex architecture of the diffusion U-Net provides many activations that can serve as features.
However, these activations inherently vary in quality, inducing significant performance gaps on discrimination.
Hence, selecting a small yet effective subset from these activations has become a fundamental problem.
In the early stage, Baranchuk <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">et. al</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite> perform a large-scale quantitative comparison among activations within Guided Diffusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib10" title="">10</a>]</cite>.
Later, the activations they select are followed by most studies in this field, pursuing other improvements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, we find that this fundamental issue is far from solved. On one hand, Baranchuk <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">et. al</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite> only consider activations between neighboring modules that comprise the main residuals.
This means that many potential activations are excluded from the candidate pool, such as the queries and keys in the self-attention blocks.
Moreover, recent developments of diffusion architecture, such as cross-attention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib37" title="">37</a>]</cite> or embedded deep vision transformers (ViTs)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib36" title="">36</a>]</cite>, have introduced additional types of activations.
Hence, as shown in upper <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S1.F1" title="In 1 Introduction ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, only a small fraction of potential activations in modern diffusion models have been evaluated for their discriminative ability, which might hinder future work in this direction.
For example, lower <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S1.F1" title="In 1 Introduction ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a> shows that Stable Diffusion XL (SDXL)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib36" title="">36</a>]</cite>, which is more advanced than Stable Diffusion v1.5 (SDv1.5)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib37" title="">37</a>]</cite>, fails to achieve better performance with the feature selection solution proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S1.F1.g1" src="extracted/5915702/pics/intro.jpg" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Prior arts only consider a small fraction of potential activations in diffusion models. As a result, more advanced diffusion architecture fails to achieve better performance (<span class="ltx_text" id="S1.F1.3.1" style="color:#0C71BD;">SDXL <span class="ltx_text ltx_font_italic" id="S1.F1.3.1.1">v.s.</span> SDv1.5</span>). In contrast, we consider a broader range of candidate activations. To facilitate the quantitative comparison, we first make a comprehensive and generalizable analysis to qualitatively filter out many candidates in advance. On top of this, our method achieves superior performance (<span class="ltx_text" id="S1.F1.4.2" style="color:#F7A41F;">75.2 PCK@0.1</span>).
</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we revisit the fundamental problem of feature selection, considering a more comprehensive candidate pool of activations.
Due to its large volume, a full-scale quantitative comparison is no longer operational, urging us to modify the previous research methodology in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.
As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S1.F1" title="In 1 Introduction ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, instead of a direct quantitative comparison, we first explore the properties of diffusion U-Nets.
These properties allow us to qualitatively and efficiently filter out many activations that are highly likely to be sub-optimal, shrinking the candidate pool for the quantitative comparison.
More importantly, we find these properties are universal among diffusion models, making it possible to generalize our findings to more models beyond those covered in this paper.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Specifically, the properties we find, which are distinct to existing knowledge of model properties <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite>, exactly correspond to three top-to-bottom levels of the diffusion U-Net:
(i) <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Diffusion noises</span> at the macro level: the diffusion process induces a new type of noise on both low- and high-frequency signals.
(ii) <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">In-resolution granularity changes</span> at the in-resolution level: the changes in information granularity are not only across but also within resolutions.
(iii) <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">Locality without positional embeddings</span> at the sub-module level: the embedded ViTs in diffusion U-Nets present a new type of local information different from that induced by the conventional positional embeddings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib44" title="">44</a>]</cite>.
Based on these insights, we develop effective feature selection solutions for several popular diffusion models. Finally, the experiments on three discriminative tasks, including semantic correspondence, semantic segmentation, and label-scarce segmentation, validate the superiority of our solutions over the SOTA methods.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, the contributions of this work are three-fold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Revisit of a fundamental problem</span>: To the best of our knowledge, we are the first to point out that the fundamental issue of feature selection remains unresolved in the realm of diffusion feature.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Generic insights</span>: The properties we find are universal among different diffusion U-Nets, which can provide valuable insights for future work.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Extensive validation</span>: Extensive experiments on three discriminative tasks validate the effectiveness of the feature selection solutions induced by our insights.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Diffusion models for discrimination.</span>
We discuss three mainstream ways to use diffusion models for discrimination.
(i) Diffusion classifier¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib7" title="">7</a>]</cite> utilizes Bayes‚Äô theorem to transform a pre-trained diffusion model into an image classifier.
This method enjoys a theoretical guarantee and does not need additional training. However, it is limited to image-level tasks.
(ii) The second way is to model discriminative tasks as image-to-image generation tasks with diffusion models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib23" title="">23</a>]</cite>. This method is suitable for various dense vision tasks but requires heavy training.
(iii) Diffusion feature¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>]</cite>, the focus of this paper, follows the traditional practice of feature extraction to pursue the balance between wider applicability and less training.
This makes it adaptable for different tasks and alleviates the training needs for the diffusion model.
Only small downstream models may require training.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The diffusion feature approach has seen various improvements.
Some techniques toggle the input settings of diffusion models, such as seeking better timesteps¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>]</cite>.
The others add trainable parameters outside the diffusion model to refine the outputs or provide an efficient fine-tuning alternative¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib46" title="">46</a>]</cite>.
Additionally, some studies explore completely training-free methods that utilize spatial attention information¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib43" title="">43</a>]</cite>, and some attempts focus on text-free diffusion models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Despite the progress, previous methods only consider activations between neighboring modules, leaving many potential candidates unevaluated.
Our work shows that such an overlook can hinder model performance.
By introducing a more comprehensive feature selection solution, our method could generically enhance both existing and future diffusion feature approaches.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Analysis on model properties.</span> The inner properties of neural networks have consistently received much attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib34" title="">34</a>]</cite>.
For transformers, Geva <span class="ltx_text ltx_font_italic" id="S2.p4.1.2">et al.</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib15" title="">15</a>]</cite> show that the feed-forward layers act as key-value memories and are interpretable for humans.
Amir <span class="ltx_text ltx_font_italic" id="S2.p4.1.3">et al.</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib1" title="">1</a>]</cite> extend this insight to vision transformers and put it into practical applications such as image classification, while Vilas <span class="ltx_text ltx_font_italic" id="S2.p4.1.4">et, al.</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib45" title="">45</a>]</cite> try to make more detailed interpretation of ViT activations.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The methodology of these studies provides valuable guidance for our research.
However, since diffusion models are trained for generation, our study relies more on qualitative analysis and feature visualization compared to previous work.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries: Architecture of Diffusion U-Nets</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.12">Diffusion models typically involve a forward pass and a reverse pass¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib21" title="">21</a>]</cite>.
During the forward pass, noise is gradually added to a clean image <math alttext="\boldsymbol{x}_{0}\in\mathbb{R}^{3\times w\times h}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><msub id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml"><mi id="S3.p1.1.m1.1.1.2.2" xref="S3.p1.1.m1.1.1.2.2.cmml">ùíô</mi><mn id="S3.p1.1.m1.1.1.2.3" xref="S3.p1.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><mn id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml">w</mi><mo id="S3.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.p1.1.m1.1.1.3.3.4" xref="S3.p1.1.m1.1.1.3.3.4.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><apply id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.2.1.cmml" xref="S3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.2.cmml" xref="S3.p1.1.m1.1.1.2.2">ùíô</ci><cn id="S3.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S3.p1.1.m1.1.1.2.3">0</cn></apply><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">‚Ñù</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><cn id="S3.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3">ùë§</ci><ci id="S3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.p1.1.m1.1.1.3.3.4">‚Ñé</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\boldsymbol{x}_{0}\in\mathbb{R}^{3\times w\times h}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT 3 √ó italic_w √ó italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> until the image resembles Gaussian noise, where <math alttext="w" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ùë§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">w</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_w</annotation></semantics></math> and <math alttext="h" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_h</annotation></semantics></math> denote the width and height, respectively. This process can be denoted by <math alttext="\boldsymbol{x}_{t}\sim q(\boldsymbol{x}_{t}|\boldsymbol{x}_{0})" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><msub id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">ùíô</mi><mi id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml">t</mi></msub><mo id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">‚àº</mo><mrow id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml"><mi id="S3.p1.4.m4.1.1.1.3" xref="S3.p1.4.m4.1.1.1.3.cmml">q</mi><mo id="S3.p1.4.m4.1.1.1.2" xref="S3.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.p1.4.m4.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml"><mo id="S3.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.4.m4.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml"><msub id="S3.p1.4.m4.1.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.1.2.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.1.2.2" xref="S3.p1.4.m4.1.1.1.1.1.1.2.2.cmml">ùíô</mi><mi id="S3.p1.4.m4.1.1.1.1.1.1.2.3" xref="S3.p1.4.m4.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.p1.4.m4.1.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.p1.4.m4.1.1.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.1.3.2" xref="S3.p1.4.m4.1.1.1.1.1.1.3.2.cmml">ùíô</mi><mn id="S3.p1.4.m4.1.1.1.1.1.1.3.3" xref="S3.p1.4.m4.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.p1.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">similar-to</csymbol><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">ùíô</ci><ci id="S3.p1.4.m4.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3">ùë°</ci></apply><apply id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"><times id="S3.p1.4.m4.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.2"></times><ci id="S3.p1.4.m4.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.3">ùëû</ci><apply id="S3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1"><csymbol cd="latexml" id="S3.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.2.2">ùíô</ci><ci id="S3.p1.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.2.3">ùë°</ci></apply><apply id="S3.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p1.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.3.2">ùíô</ci><cn id="S3.p1.4.m4.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p1.4.m4.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\boldsymbol{x}_{t}\sim q(\boldsymbol{x}_{t}|\boldsymbol{x}_{0})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àº italic_q ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="q" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_q</annotation></semantics></math> represents the noise posterior and <math alttext="t" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_t</annotation></semantics></math> is the timestep.
In the reverse pass, an end-to-end neural network <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><msub id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">œµ</mi><mi id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml">Œ∏</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">italic-œµ</ci><ci id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT</annotation></semantics></math>, as parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_Œ∏</annotation></semantics></math>, learns to predict the noises and thus reconstruct the image.
One such denoising step can be denoted as <math alttext="\epsilon=\epsilon_{\theta}(\boldsymbol{x}_{t},t,\boldsymbol{c})" class="ltx_Math" display="inline" id="S3.p1.9.m9.3"><semantics id="S3.p1.9.m9.3a"><mrow id="S3.p1.9.m9.3.3" xref="S3.p1.9.m9.3.3.cmml"><mi id="S3.p1.9.m9.3.3.3" xref="S3.p1.9.m9.3.3.3.cmml">œµ</mi><mo id="S3.p1.9.m9.3.3.2" xref="S3.p1.9.m9.3.3.2.cmml">=</mo><mrow id="S3.p1.9.m9.3.3.1" xref="S3.p1.9.m9.3.3.1.cmml"><msub id="S3.p1.9.m9.3.3.1.3" xref="S3.p1.9.m9.3.3.1.3.cmml"><mi id="S3.p1.9.m9.3.3.1.3.2" xref="S3.p1.9.m9.3.3.1.3.2.cmml">œµ</mi><mi id="S3.p1.9.m9.3.3.1.3.3" xref="S3.p1.9.m9.3.3.1.3.3.cmml">Œ∏</mi></msub><mo id="S3.p1.9.m9.3.3.1.2" xref="S3.p1.9.m9.3.3.1.2.cmml">‚Å¢</mo><mrow id="S3.p1.9.m9.3.3.1.1.1" xref="S3.p1.9.m9.3.3.1.1.2.cmml"><mo id="S3.p1.9.m9.3.3.1.1.1.2" stretchy="false" xref="S3.p1.9.m9.3.3.1.1.2.cmml">(</mo><msub id="S3.p1.9.m9.3.3.1.1.1.1" xref="S3.p1.9.m9.3.3.1.1.1.1.cmml"><mi id="S3.p1.9.m9.3.3.1.1.1.1.2" xref="S3.p1.9.m9.3.3.1.1.1.1.2.cmml">ùíô</mi><mi id="S3.p1.9.m9.3.3.1.1.1.1.3" xref="S3.p1.9.m9.3.3.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.p1.9.m9.3.3.1.1.1.3" xref="S3.p1.9.m9.3.3.1.1.2.cmml">,</mo><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">t</mi><mo id="S3.p1.9.m9.3.3.1.1.1.4" xref="S3.p1.9.m9.3.3.1.1.2.cmml">,</mo><mi id="S3.p1.9.m9.2.2" xref="S3.p1.9.m9.2.2.cmml">ùíÑ</mi><mo id="S3.p1.9.m9.3.3.1.1.1.5" stretchy="false" xref="S3.p1.9.m9.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.3b"><apply id="S3.p1.9.m9.3.3.cmml" xref="S3.p1.9.m9.3.3"><eq id="S3.p1.9.m9.3.3.2.cmml" xref="S3.p1.9.m9.3.3.2"></eq><ci id="S3.p1.9.m9.3.3.3.cmml" xref="S3.p1.9.m9.3.3.3">italic-œµ</ci><apply id="S3.p1.9.m9.3.3.1.cmml" xref="S3.p1.9.m9.3.3.1"><times id="S3.p1.9.m9.3.3.1.2.cmml" xref="S3.p1.9.m9.3.3.1.2"></times><apply id="S3.p1.9.m9.3.3.1.3.cmml" xref="S3.p1.9.m9.3.3.1.3"><csymbol cd="ambiguous" id="S3.p1.9.m9.3.3.1.3.1.cmml" xref="S3.p1.9.m9.3.3.1.3">subscript</csymbol><ci id="S3.p1.9.m9.3.3.1.3.2.cmml" xref="S3.p1.9.m9.3.3.1.3.2">italic-œµ</ci><ci id="S3.p1.9.m9.3.3.1.3.3.cmml" xref="S3.p1.9.m9.3.3.1.3.3">ùúÉ</ci></apply><vector id="S3.p1.9.m9.3.3.1.1.2.cmml" xref="S3.p1.9.m9.3.3.1.1.1"><apply id="S3.p1.9.m9.3.3.1.1.1.1.cmml" xref="S3.p1.9.m9.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.3.3.1.1.1.1.1.cmml" xref="S3.p1.9.m9.3.3.1.1.1.1">subscript</csymbol><ci id="S3.p1.9.m9.3.3.1.1.1.1.2.cmml" xref="S3.p1.9.m9.3.3.1.1.1.1.2">ùíô</ci><ci id="S3.p1.9.m9.3.3.1.1.1.1.3.cmml" xref="S3.p1.9.m9.3.3.1.1.1.1.3">ùë°</ci></apply><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">ùë°</ci><ci id="S3.p1.9.m9.2.2.cmml" xref="S3.p1.9.m9.2.2">ùíÑ</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.3c">\epsilon=\epsilon_{\theta}(\boldsymbol{x}_{t},t,\boldsymbol{c})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.3d">italic_œµ = italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_c )</annotation></semantics></math>,
where <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.p1.10.m10.1"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m10.1d">italic_œµ</annotation></semantics></math> is the predicted noise, and <math alttext="\boldsymbol{c}" class="ltx_Math" display="inline" id="S3.p1.11.m11.1"><semantics id="S3.p1.11.m11.1a"><mi id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml">ùíÑ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><ci id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1">ùíÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">\boldsymbol{c}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m11.1d">bold_italic_c</annotation></semantics></math> is the condition that describes the expected image content.
Although there are alternative formulations for diffusion models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib40" title="">40</a>]</cite>, they all rely on this neural network backbone <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.p1.12.m12.1"><semantics id="S3.p1.12.m12.1a"><msub id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml"><mi id="S3.p1.12.m12.1.1.2" xref="S3.p1.12.m12.1.1.2.cmml">œµ</mi><mi id="S3.p1.12.m12.1.1.3" xref="S3.p1.12.m12.1.1.3.cmml">Œ∏</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><apply id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.p1.12.m12.1.1.1.cmml" xref="S3.p1.12.m12.1.1">subscript</csymbol><ci id="S3.p1.12.m12.1.1.2.cmml" xref="S3.p1.12.m12.1.1.2">italic-œµ</ci><ci id="S3.p1.12.m12.1.1.3.cmml" xref="S3.p1.12.m12.1.1.3">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m12.1d">italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT</annotation></semantics></math>.
This study focuses on this backbone, typically implemented as U-Net¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite>, rather than other components of diffusion models.
Next, we will detail the architecture of the diffusion U-Net and standardize the <span class="ltx_text ltx_font_italic" id="S3.p1.12.1">terminology</span> referring to different parts of the model, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S3.F2" title="In 3 Preliminaries: Architecture of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We start with an overview.
Following the initial U-Net design¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite>, the U-Net has three main <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">stages</span>: down-stage, mid-stage, and up-stage.
The down-stage reduces the resolution of activations, while the up-stage increases it.
Both stages contain multiple <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">resolutions</span>, in each of which the activations share the same resolution.
Furthermore, each resolution includes several <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">modules</span>, including <span class="ltx_text ltx_font_italic" id="S3.p2.1.4" style="background-color:#FBE3D6;">ResModule</span> (convolutional ResNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib20" title="">20</a>]</cite> structures), <span class="ltx_text ltx_font_italic" id="S3.p2.1.5" style="background-color:#CAEEFB;">ViT Module</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib12" title="">12</a>]</cite>, and <span class="ltx_text ltx_font_italic" id="S3.p2.1.6" style="background-color:#D9F2D0;">Downsampler/Upsampler</span> (simple convolutional layers).
Previous diffusion feature approaches only consider activations between these adjacent modules, <span class="ltx_text ltx_font_italic" id="S3.p2.1.7">i.e.</span>, inter-module activations.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="S3.F2.g1" src="extracted/5915702/pics/unet.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>U-Net architecture (upper) and the ViT module (lower), taking SDXL as an example.</figcaption>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We next dive into the details below the module level.
Among these modules, ResModule and ViTs adopt <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">residual connection</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib20" title="">20</a>]</cite>, where an increment activation is added element-wise to the residual activation to refine it.
Specifically, ResModule uses simple convolutional layers to produce increments, whereas ViTs use complex attention mechanisms, which will be further explained next.
Typically, ViT operates as a standalone model followed by a decoder that produces the output predictions for visual tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib18" title="">18</a>]</cite>.
However, in the diffusion U-Net, multiple ViTs are embedded into the network, and their outputs serve as increment activations.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Furthermore, each ViT module consists of several stacked <span class="ltx_text ltx_font_italic" id="S3.p4.1.1">basic blocks</span>.
A basic block typically has a <span class="ltx_text ltx_font_italic" id="S3.p4.1.2" style="background-color:#99FFCC;">self-attention layer</span> to perform attention on the image itself and a <span class="ltx_text ltx_font_italic" id="S3.p4.1.3" style="background-color:#5FC8F0;">feed-forward layer</span>, essentially a two-layer MLP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib15" title="">15</a>]</cite>.
Modern diffusion U-Net introduces an additional <span class="ltx_text ltx_font_italic" id="S3.p4.1.4" style="background-color:#FAC8C8;">cross-attention layer</span> between the two layers, enabling the fusion of the image and additional textual prompts.
Besides, each layer includes a residual connection, meaning that the increment activation added to the outer residual is also the internal residual.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">In a nutshell, the architecture described above provides abundant activations that can serve as dense features. Given the massive activations, it is no longer operational to perform a full-scale quantitative comparison.
Hence, we next make a comprehensive analysis of model properties to better understand these activations, which can help the qualitative filtering.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Distinct Properties of Diffusion U-Nets</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The diffusion U-Net has many interesting properties, but now we only focus on those distinct from the knowledge of traditional U-Net¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite> or ViTs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib44" title="">44</a>]</cite>.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>, this section highlights three noticeable properties, each of which corresponds to a different level of the diffusion U-Net architecture described in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S3" title="3 Preliminaries: Architecture of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>.
Notably, <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">these properties can be universally observed in different samples and diffusion models, though all visualization in the main content is conducted on the same image and SDXL model for consistency</span>.
Additional visualization is provided in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1" title="Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">A</span></a>.
Besides, the omitted properties are available in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5" title="Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="373" id="S4.F3.g1" src="extracted/5915702/pics/mech-large.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>We highlight three properties of diffusion U-Nets that <span class="ltx_text ltx_font_bold" id="S4.F3.3.1">are distinct from existing knowledge about other models</span>:
(a) Asymmetric diffusion noises.
(b) In-resolution granularity changes.
(c) Locality without positional embeddings: pixels within the <span class="ltx_text" id="S4.F3.4.2" style="color:#F3960E;">orange circle</span> resemble nearby background pixels more than distant pixels on the horse‚Äôs neck that are semantically closer.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Asymmetric Diffusion Noises</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The first property is at the macro level and closely related to the overall diffusion process.
It is common that high-frequency signals are typically noisy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib34" title="">34</a>]</cite>.
This phenomenon can also be observed in diffusion U-Nets, especially within the increment activations of residual connection.
However, this does not mean that low-frequency signal activations in diffusion U-Nets are free from noises.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>(a), the diffusion process introduces a new type of noise that also impacts low-frequency signals.
This is not surprising since the diffusion process requires the backbone to process noisy inputs and predict noises as outputs.
As a result, activations near the inputs or outputs, regardless of their frequency, also suffer from such noises.
Considering the special cause, we name such noises <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">diffusion noises</span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">How does the influence of diffusion noises spread across diffusion U-Nets?
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>(a), diffusion noises exist throughout the entire down-stage, with a decreasing magnitude.
Remarkably, during the early half of the up-stage, activations are rather clean, with no perceivable diffusion noises.
Only in the later half do diffusion noises start to resurface.
The existence of this asymmetric behavior can also be indirectly supported by the ablation curves in many fellow studies such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib33" title="">33</a>]</cite>.
Furthermore, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1" title="Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">A</span></a>, such an asymmetric pattern is consistent across activations in different diffusion models.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Similar to common high-frequency noises, diffusion noises can degenerate feature quality.
Hence, this property can serve as a criterion for identifying and filtering out sub-optimal activations, which we will delve into in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5" title="5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>In-Resolution Granularity Changes</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The second property is at the in-resolution level and closely related to recent advances in diffusion architectures.
Specifically, the design of U-Net follows the idea of resolution hierarchy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib38" title="">38</a>]</cite>.
Consequently, the overall architecture displays a fine-coarse-fine granularity trend, looking like the alphabet ‚ÄúU‚Äù.
Traditional U-Nets implement this architecture with a relatively large number of resolutions, while each resolution is typically small, equipped with two or three simple convolutional layers.
Hence, the understanding of traditional U-Net focuses on the granularity changes across resolutions, implicitly assuming that the change within a single resolution is negligible.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">However, diffusion U-Nets become much ‚Äúfatter‚Äù.
In other words, modern diffusion U-Nets typically have much fewer resolutions, but each resolution is significantly enlarged. For example, SDv1.5 only has four resolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib37" title="">37</a>]</cite>, and SDXL further decreases this number to three <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib36" title="">36</a>]</cite>, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S3.F2" title="In 3 Preliminaries: Architecture of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>. Meanwhile, each resolution can produce much more activations, primarily thanks to the embedded ViT modules.
This architectural evolution makes the granularity change within a single resolution more significant, as depicted in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>(b).</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Different granularity carries varied information and quality, resulting in different discriminative performance on downstream tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>]</cite>.
Hence, this discovery of <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">in-resolution granularity changes</span> highlights the necessity to evaluate more activations, especially those within the embedded ViTs, as they are of different granularity.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Locality without Positional Embeddings</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For the third distinct property, we delve into the sub-module level, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">i.e.</span>, the blocks in embedded ViT modules.
Positional embeddings, which are widely used in language transformers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib44" title="">44</a>]</cite> and ViTs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib12" title="">12</a>]</cite>, aim to provide spatial information for each input token. Consequently, the activations of traditional ViTs display strong positional information, where the latent pixel resembles nearby pixels more than those that are semantically similar but far away¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib1" title="">1</a>]</cite>. This phenomenon is significant for the layers close to the inputs. When the layer goes deeper, the tokens are refined with semantic information, making the activations display less positional information. However, only in the last few layers, such positional information becomes negligible.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In contrast, ViT modules in diffusion U-Nets do not use positional embeddings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib36" title="">36</a>]</cite>, perhaps because the other U-Net components have provided sufficient spatial cues.
This change results in distinct properties of the activations.
On one hand, positional information is negligible for most activations despite how near they are to the inputs.
For example, even in the first basic block, cross-attention query activations contain no perceivable positional information.
On the other hand, the queries and keys of self-attention still display non-negligible positional information, <span class="ltx_text" id="S4.SS3.p2.1.1" style="color:#F3960E;"> marked with orange circles</span> in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>(c).
Specifically, the latent pixel on the horse‚Äôs neck is a light blue color, similar to the pixels to its left that actually represent the background.
In contrast, the pixels above the circle are in purple color, though they also represent the horse‚Äôs neck.
Such comparison shows that a latent pixel is more similar to other pixels that are spatially near it than those semantically closer to it.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We name this phenomenon <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">locality</span> since it has a different mechanism from that induced by positional embeddings.
As pointed out in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib12" title="">12</a>]</cite>, self-attention allows ViT to integrate global and local information even in the shallow layers, and the attention scope enlarges <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.2">w.r.t.</span> layer depth.
Even without positional embeddings, self-attention activations are generally consistent with this insight, leading to the existence of locality.
Nevertheless, the magnitude has indeed greatly reduced, compared to the visualization of conventional ViT activations in ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib1" title="">1</a>]</cite>.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>(c), in shallow activations, locality exists but is inherently weaker, as much semantic information is still reserved.
In addition, locality quickly diminishes as the layer goes deeper.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Since positional information can degrade the quality of activations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib1" title="">1</a>]</cite>, its absence has the potential to enhance the activations in ViT modules.
Moreover, locality can play a special role in activation filtering, as presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5" title="5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Universality of Three Properties</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Although the visualization in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4.F3" title="In 4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a> is conducted only on SDXL, the scope of these properties is not limited to the specific architecture. Further supporting evidence is available in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1" title="Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">A</span></a>.</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Diffusion noises directly arise from the diffusion process. Hence, it is promising to extend this property to other diffusion backbones.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">In-resolution granularity changes come from the ‚Äúfatness‚Äù of U-Nets, making it potentially applicable to more traditional U-Nets.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Locality originates from the self-attention mechanism in ViT architectures, so it is broadly applicable to standalone or embedded ViTs where positional embeddings are absent.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Enhanced Feature Selection from Diffusion U-Nets</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">So far, we have had a more comprehensive understating of the properties of diffusion U-Nets.
All these properties, especially in-resolution granularity changes, encourage us to reconsider the feature selection solution, with a special emphasis on the activations in ViT modules.
With these properties, we are also able to filter out many low-quality activations qualitatively, followed by a thus simplified quantitative comparison.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="285" id="S5.F4.g1" src="extracted/5915702/pics/evidence.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>(a) Diffusion noises result in a significant performance degeneration (<span class="ltx_text" id="S5.F4.5.1" style="color:#0470BA;"> Resolution#1</span>).
(b) Locality degrades the quality of self-attention activations (Block#0 and Block#5).
(c) Locality in self-attention activations can suppress diffusion noises, leading to better quality than noisy activations (41.41 <span class="ltx_text ltx_font_italic" id="S5.F4.6.2">v.s.</span> 34.58).
All <math alttext="\text{PCK@0.1}_{\text{img}}(\uparrow)" class="ltx_Math" display="inline" id="S5.F4.2.m1.1"><semantics id="S5.F4.2.m1.1b"><mrow id="S5.F4.2.m1.1.2" xref="S5.F4.2.m1.1.2.cmml"><msub id="S5.F4.2.m1.1.2.2" xref="S5.F4.2.m1.1.2.2.cmml"><mtext id="S5.F4.2.m1.1.2.2.2" xref="S5.F4.2.m1.1.2.2.2a.cmml">PCK@0.1</mtext><mtext id="S5.F4.2.m1.1.2.2.3" xref="S5.F4.2.m1.1.2.2.3a.cmml">img</mtext></msub><mo id="S5.F4.2.m1.1.2.1" xref="S5.F4.2.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S5.F4.2.m1.1.2.3.2" xref="S5.F4.2.m1.1.2.cmml"><mo id="S5.F4.2.m1.1.2.3.2.1" stretchy="false" xref="S5.F4.2.m1.1.2.cmml">(</mo><mo id="S5.F4.2.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.F4.2.m1.1.1.cmml">‚Üë</mo><mo id="S5.F4.2.m1.1.2.3.2.2" stretchy="false" xref="S5.F4.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><apply id="S5.F4.2.m1.1.2.cmml" xref="S5.F4.2.m1.1.2"><times id="S5.F4.2.m1.1.2.1.cmml" xref="S5.F4.2.m1.1.2.1"></times><apply id="S5.F4.2.m1.1.2.2.cmml" xref="S5.F4.2.m1.1.2.2"><csymbol cd="ambiguous" id="S5.F4.2.m1.1.2.2.1.cmml" xref="S5.F4.2.m1.1.2.2">subscript</csymbol><ci id="S5.F4.2.m1.1.2.2.2a.cmml" xref="S5.F4.2.m1.1.2.2.2"><mtext id="S5.F4.2.m1.1.2.2.2.cmml" xref="S5.F4.2.m1.1.2.2.2">PCK@0.1</mtext></ci><ci id="S5.F4.2.m1.1.2.2.3a.cmml" xref="S5.F4.2.m1.1.2.2.3"><mtext id="S5.F4.2.m1.1.2.2.3.cmml" mathsize="70%" xref="S5.F4.2.m1.1.2.2.3">img</mtext></ci></apply><ci id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1">‚Üë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">\text{PCK@0.1}_{\text{img}}(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.m1.1e">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT ( ‚Üë )</annotation></semantics></math> results are evaluated on the semantic correspondence task.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Qualitative Filtering</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Avoiding Diffusion Noises.</span>
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.F4" title="In 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>(a), diffusion noises tend to degrade the quality of activations.
Hence, it is natural to filter out the activations severely affected by diffusion noises from the candidate pool.
Specifically, according to the asymmetric trend of diffusion noises, <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.2">we only consider activations in the early half of the up-stage</span>, which are rather clean.
This approach will significantly reduce the number of candidate activations and simplify the quantitative comparison.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Avoiding Self-Attention Locality.</span>
The locality in self-attention modules is another important factor that can degrade the activations.
The empirical evidence in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.F4" title="In 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>(b) demonstrates that these activations are generally inferior to the others, such as those from cross-attention layers or the outputs of ViT basic blocks.
Consequently, it is rather safe to <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.2">filter out most activations in self-attention modules</span> from our candidate pool.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Using Locality to Suppress Diffusion Noises.</span>
So far, the activations in the candidate pool are clean and free from locality.
However, all these activations are low-resolution ones since high-resolution activations are generally noisy and thus filtered out.
This is unfavorable since some detailed information might only exist in high-resolution activations.
To address this issue, we exploit a side effect of self-attention locality.
Specifically, as indicated in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.F4" title="In 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>(c), locality can help suppress diffusion noises via its focus on spatial structures.
Although locality is sub-optimal, it is still superior to severe diffusion noises. In view of this, the candidate pool <span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.2">reserves self-attention activations extracted from the later half of the up-stage</span>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">We have filtered out many candidate activations based on the distinct properties of diffusion U-Nets. Additionally, all increment activations in residual connection can be further filtered out since they introduce high-frequency noises <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib34" title="">34</a>]</cite>.
After such qualitative filtering, a small but high-quality candidate pool is available. Taking SDXL as an example, the number of candidates decreases from 279 to 63, <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.1">i.e.</span>, a 78% reduction.
Next, we explain how to conduct this quantitative comparison briefly.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Quantitative Comparison</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.4">The quantitative comparison follows the protocol of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>. Specifically, given an input image <math alttext="\boldsymbol{x}_{0}" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">ùíô</mi><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">ùíô</ci><cn id="S5.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS2.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\boldsymbol{x}_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, we first perform the forward pass with a pre-defined timestep <math alttext="t" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_t</annotation></semantics></math> to generate the noisy image <math alttext="\boldsymbol{x}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><msub id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">ùíô</mi><mi id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">ùíô</ci><ci id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\boldsymbol{x}_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Then, the U-Net backbone conducts one denoising step. Instead of collecting the model output <math alttext="\epsilon" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">italic_œµ</annotation></semantics></math>, we gather U-Net activations and consolidate them to the candidate pool, as described in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5.SS1" title="5.1 Qualitative Filtering ‚Ä£ 5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">5.1</span></a>.
Afterward, each activation is individually fed to a downstream model to evaluate its discriminative potential, with the details of the model described in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3" title="Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">C</span></a>.
Such comparison is made fair by using the same dataset for all activations.
Finally, we can obtain the capability ranking of activations from each resolution, according to which features can be selected wisely.
Notably, we conduct such comparisons on multiple datasets to guarantee generalizability, since the best activations may differ among different scenes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.
Thanks to qualitative filtering, <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.4.1">the time cost for each (model, dataset) pair has been reduced by more than one week</span>, equipped with Nvidia(R) RTX 3090 GPUs.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Given the capability ranking, feature selection is in fact flexible, as it is possible to combine multiple activations and specifically tune the choice for a task.
For practicality, we provide off-the-shelf feature combinations for SDv1.5 and SDXL that are likely to generically perform well in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2" title="Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">B</span></a>, according to the results detailed in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4" title="Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">D</span></a>.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Each of them consists of four activations mainly from ViT modules rather than inter-module positions</span>.
Taking SDv1.5 as an example, one of the four selected features is from one self-attention layer in the highest resolution, which utilizes our observation in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4" title="4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental Validation on Multiple Discriminative Tasks</h2>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results on the semantic correspondence task. The best results are in <span class="ltx_text ltx_font_bold" id="S6.T1.5.1" style="color:#FF8000;">bold</span> font and the runner-up is <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.6.2" style="color:#036FC1;">underlined</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S6.T1.2.2.3">Category</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S6.T1.2.2.4">Method</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T1.1.1.1"><math alttext="\text{PCK@0.1}_{\text{img}}\uparrow" class="ltx_Math" display="inline" id="S6.T1.1.1.1.m1.1"><semantics id="S6.T1.1.1.1.m1.1a"><mrow id="S6.T1.1.1.1.m1.1.1" xref="S6.T1.1.1.1.m1.1.1.cmml"><msub id="S6.T1.1.1.1.m1.1.1.2" xref="S6.T1.1.1.1.m1.1.1.2.cmml"><mtext id="S6.T1.1.1.1.m1.1.1.2.2" xref="S6.T1.1.1.1.m1.1.1.2.2a.cmml">PCK@0.1</mtext><mtext id="S6.T1.1.1.1.m1.1.1.2.3" xref="S6.T1.1.1.1.m1.1.1.2.3a.cmml">img</mtext></msub><mo id="S6.T1.1.1.1.m1.1.1.1" stretchy="false" xref="S6.T1.1.1.1.m1.1.1.1.cmml">‚Üë</mo><mi id="S6.T1.1.1.1.m1.1.1.3" xref="S6.T1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.1.1.1.m1.1b"><apply id="S6.T1.1.1.1.m1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1"><ci id="S6.T1.1.1.1.m1.1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1.1">‚Üë</ci><apply id="S6.T1.1.1.1.m1.1.1.2.cmml" xref="S6.T1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T1.1.1.1.m1.1.1.2.1.cmml" xref="S6.T1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S6.T1.1.1.1.m1.1.1.2.2a.cmml" xref="S6.T1.1.1.1.m1.1.1.2.2"><mtext id="S6.T1.1.1.1.m1.1.1.2.2.cmml" xref="S6.T1.1.1.1.m1.1.1.2.2">PCK@0.1</mtext></ci><ci id="S6.T1.1.1.1.m1.1.1.2.3a.cmml" xref="S6.T1.1.1.1.m1.1.1.2.3"><mtext id="S6.T1.1.1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S6.T1.1.1.1.m1.1.1.2.3">img</mtext></ci></apply><csymbol cd="latexml" id="S6.T1.1.1.1.m1.1.1.3.cmml" xref="S6.T1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.1.1.1.m1.1c">\text{PCK@0.1}_{\text{img}}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T1.1.1.1.m1.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT ‚Üë</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S6.T1.2.2.2"><math alttext="\text{PCK@0.1}_{\text{bbox}}\uparrow" class="ltx_Math" display="inline" id="S6.T1.2.2.2.m1.1"><semantics id="S6.T1.2.2.2.m1.1a"><mrow id="S6.T1.2.2.2.m1.1.1" xref="S6.T1.2.2.2.m1.1.1.cmml"><msub id="S6.T1.2.2.2.m1.1.1.2" xref="S6.T1.2.2.2.m1.1.1.2.cmml"><mtext id="S6.T1.2.2.2.m1.1.1.2.2" xref="S6.T1.2.2.2.m1.1.1.2.2a.cmml">PCK@0.1</mtext><mtext id="S6.T1.2.2.2.m1.1.1.2.3" xref="S6.T1.2.2.2.m1.1.1.2.3a.cmml">bbox</mtext></msub><mo id="S6.T1.2.2.2.m1.1.1.1" stretchy="false" xref="S6.T1.2.2.2.m1.1.1.1.cmml">‚Üë</mo><mi id="S6.T1.2.2.2.m1.1.1.3" xref="S6.T1.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.2.2.2.m1.1b"><apply id="S6.T1.2.2.2.m1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1"><ci id="S6.T1.2.2.2.m1.1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1.1">‚Üë</ci><apply id="S6.T1.2.2.2.m1.1.1.2.cmml" xref="S6.T1.2.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T1.2.2.2.m1.1.1.2.1.cmml" xref="S6.T1.2.2.2.m1.1.1.2">subscript</csymbol><ci id="S6.T1.2.2.2.m1.1.1.2.2a.cmml" xref="S6.T1.2.2.2.m1.1.1.2.2"><mtext id="S6.T1.2.2.2.m1.1.1.2.2.cmml" xref="S6.T1.2.2.2.m1.1.1.2.2">PCK@0.1</mtext></ci><ci id="S6.T1.2.2.2.m1.1.1.2.3a.cmml" xref="S6.T1.2.2.2.m1.1.1.2.3"><mtext id="S6.T1.2.2.2.m1.1.1.2.3.cmml" mathsize="70%" xref="S6.T1.2.2.2.m1.1.1.2.3">bbox</mtext></ci></apply><csymbol cd="latexml" id="S6.T1.2.2.2.m1.1.1.3.cmml" xref="S6.T1.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.2.2.m1.1c">\text{PCK@0.1}_{\text{bbox}}\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T1.2.2.2.m1.1d">PCK@0.1 start_POSTSUBSCRIPT bbox end_POSTSUBSCRIPT ‚Üë</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.2.3.1.1" rowspan="4"><span class="ltx_text" id="S6.T1.2.3.1.1.1">SOTA</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.2.3.1.2">DINO</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.2.3.1.3">51.68</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T1.2.3.1.4">41.04</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.2.4.2.1">DHPF</th>
<td class="ltx_td ltx_align_center" id="S6.T1.2.4.2.2">55.28</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T1.2.4.2.3">42.63</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.2.5.3.1">DIFT</th>
<td class="ltx_td ltx_align_center" id="S6.T1.2.5.3.2">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T1.2.5.3.3">52.90</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.2.6.4.1">DHF</th>
<td class="ltx_td ltx_align_center" id="S6.T1.2.6.4.2">72.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T1.2.6.4.3">64.61</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.2.7.5.1" rowspan="2"><span class="ltx_text" id="S6.T1.2.7.5.1.1">Baseline</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.2.7.5.2">Legacy-v1.5</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.2.7.5.3">75.14</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T1.2.7.5.4">66.73</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.2.8.6.1">Legacy-XL</th>
<td class="ltx_td ltx_align_center" id="S6.T1.2.8.6.2">66.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T1.2.8.6.3">59.16</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S6.T1.2.9.7.1" rowspan="3"><span class="ltx_text" id="S6.T1.2.9.7.1.1">Ours</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.2.9.7.2">Ours-v1.5</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.2.9.7.3">77.78</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T1.2.9.7.4">69.83</td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.2.10.8.1">Ours-XL</th>
<td class="ltx_td ltx_align_center" id="S6.T1.2.10.8.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.2.10.8.2.1" style="color:#036FC1;">81.72</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T1.2.10.8.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.2.10.8.3.1" style="color:#036FC1;">75.18</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T1.2.11.9.1">Ours-XL-t</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.2.11.9.2"><span class="ltx_text ltx_font_bold" id="S6.T1.2.11.9.2.1" style="color:#FF8000;">83.90</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S6.T1.2.11.9.3"><span class="ltx_text ltx_font_bold" id="S6.T1.2.11.9.3.1" style="color:#FF8000;">76.86</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To validate the effectiveness of our feature selection solution, the experiments are conducted on three popular discriminative tasks: semantic correspondence, semantic segmentation, and label-scarce segmentation. The SOTA methods for each task are selected as competitors.
Besides, we also compare with two baselines that select the conventional inter-module activations as features¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>, named <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Legacy-v1.5</span> and <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">Legacy-XL</span>. For our method, we provide the following implementations:</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i1.p1.1.1">Ours-v1.5 &amp; Ours-XL</span>: Features extracted from SDv1.5 and SDXL, respectively.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i2.p1.1.1">Ours-XL-t</span>: For fairness, we further enhance SDXL features with some additional techniques that are also adopted by SOTAs. The techniques we select, <span class="ltx_text ltx_font_italic" id="S6.I1.i2.p1.1.2">i.e.</span>, attention maps¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>]</cite> and feature amalgamation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib60" title="">60</a>]</cite>, are relatively simple and lightweight.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S6.p1.2">More experimental details are in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3" title="Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">C</span></a>, such as task information, evaluation metrics, and implementation details.
Besides, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4" title="Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">D</span></a> presents additional results not covered here.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Empirical Results on Semantic Correspondence</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We present the experimental results for semantic correspondence in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.T1" title="In 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>. From the results, we have the following observations:</p>
<ol class="ltx_enumerate" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p" id="S6.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.1">Ours-v1.5</span> outperforms <span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.2">Legacy-v1.5</span> (77.78 <span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.3">v.s.</span> 75.14 on <math alttext="\text{PCK@0.1}_{\text{img}}" class="ltx_Math" display="inline" id="S6.I2.i1.p1.1.m1.1"><semantics id="S6.I2.i1.p1.1.m1.1a"><msub id="S6.I2.i1.p1.1.m1.1.1" xref="S6.I2.i1.p1.1.m1.1.1.cmml"><mtext id="S6.I2.i1.p1.1.m1.1.1.2" xref="S6.I2.i1.p1.1.m1.1.1.2a.cmml">PCK@0.1</mtext><mtext id="S6.I2.i1.p1.1.m1.1.1.3" xref="S6.I2.i1.p1.1.m1.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.I2.i1.p1.1.m1.1b"><apply id="S6.I2.i1.p1.1.m1.1.1.cmml" xref="S6.I2.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.I2.i1.p1.1.m1.1.1.1.cmml" xref="S6.I2.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S6.I2.i1.p1.1.m1.1.1.2a.cmml" xref="S6.I2.i1.p1.1.m1.1.1.2"><mtext id="S6.I2.i1.p1.1.m1.1.1.2.cmml" xref="S6.I2.i1.p1.1.m1.1.1.2">PCK@0.1</mtext></ci><ci id="S6.I2.i1.p1.1.m1.1.1.3a.cmml" xref="S6.I2.i1.p1.1.m1.1.1.3"><mtext id="S6.I2.i1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S6.I2.i1.p1.1.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i1.p1.1.m1.1c">\text{PCK@0.1}_{\text{img}}</annotation><annotation encoding="application/x-llamapun" id="S6.I2.i1.p1.1.m1.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT</annotation></semantics></math>).
The main reason is that <span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.4">Legacy-v1.5</span> fails to effectively handle the diffusion noises in high-resolution activations.
Previous approaches either reserve the noisy activations and thus suffer from performance degradation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib52" title="">52</a>]</cite>, or simply discard high-resolution activations and thus suffer from information loss¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib46" title="">46</a>]</cite>.
In contrast, our approach uses self-attention locality to suppress diffusion noises and harvest better high-resolution activations.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p" id="S6.I2.i2.p1.2"><span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.1">Legacy-XL</span> is inferior to <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.2">Legacy-v1.5</span> (66.00 <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.3">v.s.</span> 75.14 on <math alttext="\text{PCK@0.1}_{\text{img}}" class="ltx_Math" display="inline" id="S6.I2.i2.p1.1.m1.1"><semantics id="S6.I2.i2.p1.1.m1.1a"><msub id="S6.I2.i2.p1.1.m1.1.1" xref="S6.I2.i2.p1.1.m1.1.1.cmml"><mtext id="S6.I2.i2.p1.1.m1.1.1.2" xref="S6.I2.i2.p1.1.m1.1.1.2a.cmml">PCK@0.1</mtext><mtext id="S6.I2.i2.p1.1.m1.1.1.3" xref="S6.I2.i2.p1.1.m1.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.I2.i2.p1.1.m1.1b"><apply id="S6.I2.i2.p1.1.m1.1.1.cmml" xref="S6.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.I2.i2.p1.1.m1.1.1.1.cmml" xref="S6.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S6.I2.i2.p1.1.m1.1.1.2a.cmml" xref="S6.I2.i2.p1.1.m1.1.1.2"><mtext id="S6.I2.i2.p1.1.m1.1.1.2.cmml" xref="S6.I2.i2.p1.1.m1.1.1.2">PCK@0.1</mtext></ci><ci id="S6.I2.i2.p1.1.m1.1.1.3a.cmml" xref="S6.I2.i2.p1.1.m1.1.1.3"><mtext id="S6.I2.i2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S6.I2.i2.p1.1.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i2.p1.1.m1.1c">\text{PCK@0.1}_{\text{img}}</annotation><annotation encoding="application/x-llamapun" id="S6.I2.i2.p1.1.m1.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT</annotation></semantics></math>). At first glance, this result is counter-intuitive since SDXL is more advanced than SDv1.5. However, the analysis in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S4" title="4 Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> can unravel the mystery.
Specifically, since SDXL has more ViT modules, the more valuable activations shift from inter-module positions to these embedded ViTs. Since the baseline does not consider ViT modules, <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.4">Legacy-XL</span> fails to achieve better performance.
In contrast, <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.5">Ours-XL</span> shows improvement over <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.6">Ours-v1.5</span> (<span class="ltx_text ltx_framed ltx_framed_underline" id="S6.I2.i2.p1.2.7" style="color:#036FC1;">81.72</span> <span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.2.8">v.s.</span> 77.78 on <math alttext="\text{PCK@0.1}_{\text{img}}" class="ltx_Math" display="inline" id="S6.I2.i2.p1.2.m2.1"><semantics id="S6.I2.i2.p1.2.m2.1a"><msub id="S6.I2.i2.p1.2.m2.1.1" xref="S6.I2.i2.p1.2.m2.1.1.cmml"><mtext id="S6.I2.i2.p1.2.m2.1.1.2" xref="S6.I2.i2.p1.2.m2.1.1.2a.cmml">PCK@0.1</mtext><mtext id="S6.I2.i2.p1.2.m2.1.1.3" xref="S6.I2.i2.p1.2.m2.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.I2.i2.p1.2.m2.1b"><apply id="S6.I2.i2.p1.2.m2.1.1.cmml" xref="S6.I2.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.I2.i2.p1.2.m2.1.1.1.cmml" xref="S6.I2.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S6.I2.i2.p1.2.m2.1.1.2a.cmml" xref="S6.I2.i2.p1.2.m2.1.1.2"><mtext id="S6.I2.i2.p1.2.m2.1.1.2.cmml" xref="S6.I2.i2.p1.2.m2.1.1.2">PCK@0.1</mtext></ci><ci id="S6.I2.i2.p1.2.m2.1.1.3a.cmml" xref="S6.I2.i2.p1.2.m2.1.1.3"><mtext id="S6.I2.i2.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S6.I2.i2.p1.2.m2.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i2.p1.2.m2.1c">\text{PCK@0.1}_{\text{img}}</annotation><annotation encoding="application/x-llamapun" id="S6.I2.i2.p1.2.m2.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT</annotation></semantics></math>).
This is consistent with the advance in model architecture and again validates our analysis.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="S6.I2.i3.p1">
<p class="ltx_p" id="S6.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i3.p1.1.1">Ours-XL-t</span> significantly outperforms the SOTA method with a similar amalgamation technique, <span class="ltx_text ltx_font_italic" id="S6.I2.i3.p1.1.2">i.e.</span>, DHF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>]</cite> (<span class="ltx_text ltx_font_bold" id="S6.I2.i3.p1.1.3" style="color:#FF8000;">83.90</span> <span class="ltx_text ltx_font_italic" id="S6.I2.i3.p1.1.4">v.s.</span> 72.56 on <math alttext="\text{PCK@0.1}_{\text{img}}" class="ltx_Math" display="inline" id="S6.I2.i3.p1.1.m1.1"><semantics id="S6.I2.i3.p1.1.m1.1a"><msub id="S6.I2.i3.p1.1.m1.1.1" xref="S6.I2.i3.p1.1.m1.1.1.cmml"><mtext id="S6.I2.i3.p1.1.m1.1.1.2" xref="S6.I2.i3.p1.1.m1.1.1.2a.cmml">PCK@0.1</mtext><mtext id="S6.I2.i3.p1.1.m1.1.1.3" xref="S6.I2.i3.p1.1.m1.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.I2.i3.p1.1.m1.1b"><apply id="S6.I2.i3.p1.1.m1.1.1.cmml" xref="S6.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.I2.i3.p1.1.m1.1.1.1.cmml" xref="S6.I2.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S6.I2.i3.p1.1.m1.1.1.2a.cmml" xref="S6.I2.i3.p1.1.m1.1.1.2"><mtext id="S6.I2.i3.p1.1.m1.1.1.2.cmml" xref="S6.I2.i3.p1.1.m1.1.1.2">PCK@0.1</mtext></ci><ci id="S6.I2.i3.p1.1.m1.1.1.3a.cmml" xref="S6.I2.i3.p1.1.m1.1.1.3"><mtext id="S6.I2.i3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S6.I2.i3.p1.1.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I2.i3.p1.1.m1.1c">\text{PCK@0.1}_{\text{img}}</annotation><annotation encoding="application/x-llamapun" id="S6.I2.i3.p1.1.m1.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT</annotation></semantics></math>). This performance gain again validates the effectiveness of our method.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results on semantic segmentation and its altered version with scarce labeled data, evaluated using mIoU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T2.2.m1.1"><semantics id="S6.T2.2.m1.1b"><mo id="S6.T2.2.m1.1.1" stretchy="false" xref="S6.T2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S6.T2.2.m1.1c"><ci id="S6.T2.2.m1.1.1.cmml" xref="S6.T2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T2.2.m1.1e">‚Üë</annotation></semantics></math> metric. The best results are in <span class="ltx_text ltx_font_bold" id="S6.T2.5.1" style="color:#FF8000;">bold</span> font and the runner-up is <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.6.2" style="color:#036FC1;">underlined</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T2.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.7.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T2.7.1.1.1" rowspan="2"><span class="ltx_text" id="S6.T2.7.1.1.1.1">Category</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T2.7.1.1.2" rowspan="2"><span class="ltx_text" id="S6.T2.7.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S6.T2.7.1.1.3">Standard Setting</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T2.7.1.1.4" rowspan="2"><span class="ltx_text" id="S6.T2.7.1.1.4.1">Method</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S6.T2.7.1.1.5">Label-Scarce Setting</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.2.2">
<td class="ltx_td ltx_align_center" id="S6.T2.7.2.2.1">ADE20K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.2.2.2">CityScapes</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.2.2.3">Horse-21</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T2.7.3.3.1" rowspan="4"><span class="ltx_text" id="S6.T2.7.3.3.1.1">SOTA</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.3.2">MaskCLIP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.7.3.3.3">23.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.7.3.3.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.3.5">SwAVw2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T2.7.3.3.6">54.0 ¬± 0.9</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.4.4">
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.4.1">ODISE</td>
<td class="ltx_td ltx_align_center" id="S6.T2.7.4.4.2">29.90</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.4.4.3">-</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.4.4">MAE</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.4.4.5">63.4 ¬± 1.4</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.5.5">
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.5.1">VPD</td>
<td class="ltx_td ltx_align_center" id="S6.T2.7.5.5.2">37.63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.5.5.3">55.06</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.5.4">DatasetDDPM</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.5.5.5">60.8 ¬± 1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.6.6">
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.6.1">Meta Prompts</td>
<td class="ltx_td ltx_align_center" id="S6.T2.7.6.6.2">40.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.6.6.3">71.94</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.6.4">DDPM</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.6.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.7.6.6.5.1" style="color:#036FC1;">65.0 ¬± 0.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T2.7.7.7.1" rowspan="2"><span class="ltx_text" id="S6.T2.7.7.7.1.1">Baseline</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.7.7.2">Legacy-v1.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.7.7.7.3">40.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.7.7.7.4">64.01</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.7.7.5">Legacy-v1.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T2.7.7.7.6">59.4 ¬± 1.3</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.8.8">
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.8.1">Legacy-XL</td>
<td class="ltx_td ltx_align_center" id="S6.T2.7.8.8.2">27.78</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.8.8.3">71.67</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.8.4">Legacy-XL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.8.8.5">53.0 ¬± 0.9</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.9.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S6.T2.7.9.9.1" rowspan="3"><span class="ltx_text" id="S6.T2.7.9.9.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.9.9.2">Ours-v1.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.7.9.9.3">41.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.7.9.9.4">64.10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.9.9.5">Ours-v1.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T2.7.9.9.6">60.2 ¬± 0.9</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.10.10">
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.10.1">Ours-XL</td>
<td class="ltx_td ltx_align_center" id="S6.T2.7.10.10.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.7.10.10.2.1" style="color:#036FC1;">43.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.7.10.10.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.7.10.10.3.1" style="color:#036FC1;">74.47</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.10.4">Ours-XL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.7.10.10.5">62.7 ¬± 0.7</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.11.11.1">Ours-XL-t</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.7.11.11.2"><span class="ltx_text ltx_font_bold" id="S6.T2.7.11.11.2.1" style="color:#FF8000;">45.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.7.11.11.3"><span class="ltx_text ltx_font_bold" id="S6.T2.7.11.11.3.1" style="color:#FF8000;">75.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.11.11.4">Ours-XL-t</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S6.T2.7.11.11.5"><span class="ltx_text ltx_font_bold" id="S6.T2.7.11.11.5.1" style="color:#FF8000;">66.3 ¬± 0.9</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Empirical Results on Semantic Segmentation</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">As shown in the left part of <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.T2" title="In 6.1 Empirical Results on Semantic Correspondence ‚Ä£ 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">Ours-XL-t</span> and <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">Ours-XL</span> achieve state-of-the-art performance on the semantic segmentation task (<span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.3" style="color:#FF8000;">45.71</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.SS2.p1.1.4" style="color:#036FC1;">43.45</span> <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.5">v.s.</span> 40.89 on ADE20K), demonstrating its effectiveness and generalizability.
Furthermore, the most competitive SOTA, Meta Prompts¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib46" title="">46</a>]</cite>, introduces a large number of trainable parameters and uses the diffusion U-Net recurrently, which is rather time-consuming.
In contrast, our method delivers superior results with efficiency maintained.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Unlike the results in semantic correspondence, <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">Legacy-XL</span> outperforms <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">Legacy-v1.5</span> on CityScapes (71.67 <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.3">v.s.</span> 64.01), and the performance gap between <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.4">Legacy-v1.5</span> and <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.5">Ours-v1.5</span> is narrow (64.01 <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.6">v.s.</span> 64.10).
This is because this task utilizes a relatively large-scale downstream model, which can significantly refine the input features and thus reduce the gap in feature quality.
Nevertheless, <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.7">Ours-XL</span> still achieves a significant improvement over <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.8">Legacy-XL</span> (<span class="ltx_text ltx_framed ltx_framed_underline" id="S6.SS2.p2.1.9" style="color:#036FC1;">74.47</span> <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.10">v.s.</span> 71.67).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Empirical Results on Label-Scarce Segmentation</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">One advantage of diffusion features is the applicability to label-scarce scenarios¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.
For validation under such conditions, we experiment on the label-scarce segmentation task, with results presented in the right part of <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.T2" title="In 6.1 Empirical Results on Semantic Correspondence ‚Ä£ 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>.
The observations are generally similar to those on the semantic correspondence task.
For example, <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">Ours-v1.5</span> outperforms <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.2">Legacy-v1.5</span> (60.2 <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.3">v.s.</span> 59.4),
<span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.4">Legacy-XL</span> is inferior to <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.5">Legacy-v1.5</span> (53.0 <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.6">v.s.</span> 59.4),
and <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.7">Ours-XL</span> is better than <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.8">Ours-v1.5</span> (62.7 <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.9">v.s.</span> 60.2).</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">Next, we focus on the comparison with the SOTA method, DDPM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.
Although DDPM is a relatively early study, it outperforms the other competitors and our most implementations.
This result has two reasons.
On one hand, DDPM performs the diffusion process directly in the image space rather than the currently common practice of compressed latent space.
Although inefficient, such an implementation yields better discriminative features.
On the other hand, DDPM utilizes diffusion models specifically trained on each dataset.
In comparison, we use pre-trained general-purposed SDv1.5 or SDXL to be more efficient and thus consistent with the motivation of diffusion feature.
Hence, it is rather challenging to surpass this SOTA method.
Fortunately, our best implementation, <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.1">Ours-XL-t</span>, achieves this goal with the help of additional lightweight techniques (<span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.2" style="color:#FF8000;">66.3</span> <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.3">v.s.</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.SS3.p2.1.4" style="color:#036FC1;">65.0</span>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we revisit the fundamental problem of feature selection from diffusion U-Nets.
We point out that prior arts only consider a limited range of potential activations.
In contrast, we consider a much wider range of activations as candidates, especially those extracted from the embedded ViT modules.
Given the large volume of the candidate pool, we first analyze the properties of diffusion U-Nets.
The properties we find are universal such that our observations are not limited to the specific diffusion architecture.
Based on these properties, we qualitatively filter out many activations with low quality, facilitating the following quantitative comparison.
On top of this, concrete feature selection solutions are proposed for two popular diffusion models, <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">i.e.</span>, SDv1.5 and SDXL.
Finally, extensive experiments on three discriminative tasks validate the effectiveness of our method.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">However, we are not sure whether our observations can generalize well to recently-developed DiT models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib35" title="">35</a>]</cite> since they have a markedly different architecture from U-Net-based diffusion models.
Thus, analyzing DiT models is a promising topic for future research.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported in part by the National Key R&amp;D Program of China under Grant 2018AAA0102000, in part by National Natural Science Foundation of China: 62236008, U21B2038, U23B2051, 61931008, 62122075 and 62025604, in part by Youth Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of the Chinese Academy of Sciences, Grant No. XDB0680000, in part by the Innovation Funding of ICT, CAS under Grant No.E000000, in part by the China National Postdoctoral Program for Innovative Talents under Grant BX20240384.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.

</span>
<span class="ltx_bibblock">Deep vit features as dense visual descriptors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">ECCV Workshop on What is Motion For</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.

</span>
<span class="ltx_bibblock">Label-efficient semantic segmentation with diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">The Tenth International Conference on Learning Representations</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Unsupervised learning of visual features by contrasting cluster assignments.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Annual Conference on Neural Information Processing Systems</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">IEEE/CVF International Conference on Computer Vision, ICCV</span>, pages 9630‚Äì9640, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah.

</span>
<span class="ltx_bibblock">Activation atlas.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Distill</span>, 4(3):e15, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.

</span>
<span class="ltx_bibblock">Videocrafter2: Overcoming data limitations for high-quality video diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">CoRR</span>, abs/2401.09047, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, and Jun Zhu.

</span>
<span class="ltx_bibblock">Your diffusion model is secretly a certifiably robust classifier.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">CoRR</span>, abs/2402.02316, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.

</span>
<span class="ltx_bibblock">The cityscapes dataset for semantic urban scene understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 3213‚Äì3223, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li¬†Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</span>, pages 248‚Äì255, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander¬†Quinn Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Annual Conference on Neural Information Processing Systems</span>, pages 8780‚Äì8794, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu¬†Yuan, Dong Chen, Fang Wen, and Nenghai Yu.

</span>
<span class="ltx_bibblock">Maskclip: Masked self-distillation advances contrastive language-image pretraining.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 10995‚Äì11005, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">9th International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Niladri¬†Shekhar Dutt, Sanjeev Muralikrishnan, and Niloy¬†J. Mitra.

</span>
<span class="ltx_bibblock">Diffusion 3d features (diff3f) decorating untextured shapes with distilled semantic features.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 4494‚Äì4504, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Raphael¬†Antonius Frick and Martin Steinebach.

</span>
<span class="ltx_bibblock">Diffseg: Towards detecting diffusion-based inpainting attacks using multi-feature segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 3802‚Äì3808, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.

</span>
<span class="ltx_bibblock">Transformer feed-forward layers are key-value memories.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 5484‚Äì5495, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Rui Gong, Martin Danelljan, Han Sun, Julio¬†Delgado Mangas, and Luc¬†Van Gool.

</span>
<span class="ltx_bibblock">Prompting diffusion representations for cross-domain semantic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">CoRR</span>, abs/2307.02138, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Boyu Han, Qianqian Xu, Zhiyong Yang, Shilong Bao, Peisong Wen, Yangbangyan Jiang, and Qingming Huang.

</span>
<span class="ltx_bibblock">Aucseg: Auc-oriented pixel-level long-tail semantic segmentation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Shijie Hao, Yuan Zhou, and Yanrong Guo.

</span>
<span class="ltx_bibblock">A brief survey on semantic segmentation with deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Neurocomputing</span>, 406:302‚Äì321, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross¬†B. Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 15979‚Äì15988, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 770‚Äì778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Annual Conference on Neural Information Processing Systems</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo.

</span>
<span class="ltx_bibblock">DDP: diffusion model for dense visual prediction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">IEEE/CVF International Conference on Computer Vision</span>, pages 21684‚Äì21695, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo¬†Caye Daudt, and Konrad Schindler.

</span>
<span class="ltx_bibblock">Repurposing diffusion-based image generators for monocular depth estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">CoRR</span>, abs/2312.02145, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Neehar Kondapaneni, Markus Marks, Manuel Knott, Rog√©rio Guimar√£es, and Pietro Perona.

</span>
<span class="ltx_bibblock">Text-image alignment for diffusion-based perception.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">CoRR</span>, abs/2310.00031, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et¬†al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Alexander¬†C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak.

</span>
<span class="ltx_bibblock">Your diffusion model is secretly a zero-shot classifier.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE/CVF International Conference on Computer Vision</span>, pages 2206‚Äì2217, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Xinghui Li, Jingyi Lu, Kai Han, and Victor Prisacariu.

</span>
<span class="ltx_bibblock">Sd4match: Learning to prompt stable diffusion model for semantic matching.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">CoRR</span>, abs/2310.17569, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yaron Lipman, Ricky T.¬†Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Grace Luo, Lisa Dunlap, Dong¬†Huk Park, Aleksander Holynski, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Diffusion hyperfeatures: Searching through time and space for semantic correspondence.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Annual Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.

</span>
<span class="ltx_bibblock">Spair-71k: A large-scale benchmark for semantic correspondence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">CoRR</span>, abs/1908.10543, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.

</span>
<span class="ltx_bibblock">Learning to compose hypercolumns for visual correspondence.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Computer Vision - ECCV 2020 - 16th European Conference</span>, volume 12360, pages 346‚Äì363. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">CoRR</span>, abs/2307.08702, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Soumik Mukhopadhyay, Matthew Gwilliam, Yosuke Yamaguchi, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Tianyi Zhou, and Abhinav Shrivastava.

</span>
<span class="ltx_bibblock">Do text-free diffusion models learn discriminative visual representations?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">CoRR</span>, abs/2311.17921, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.

</span>
<span class="ltx_bibblock">Feature visualization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Distill</span>, 2(11):e7, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">IEEE/CVF International Conference on Computer Vision</span>, pages 4172‚Äì4182, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">SDXL: improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">CoRR</span>, abs/2307.01952, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 10674‚Äì10685, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Medical Image Computing and Computer-Assisted Intervention</span>, volume 9351, pages 234‚Äì241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yang Song, Jascha Sohl-Dickstein, Diederik¬†P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.

</span>
<span class="ltx_bibblock">Score-based generative modeling through stochastic differential equations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">9th International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Dual diffusion implicit bridges for image-to-image translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Luming Tang, Menglin Jia, Qianqian Wang, Cheng¬†Perng Phoo, and Bharath Hariharan.

</span>
<span class="ltx_bibblock">Emergent correspondence from image diffusion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Annual Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Kashvi Taunk, Sanjukta De, Srishti Verma, and Aleena Swetapadma.

</span>
<span class="ltx_bibblock">A brief review of nearest neighbor algorithm for learning and classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">2019 international conference on intelligent computing and control systems (ICCS)</span>, pages 1255‚Äì1260. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonz√°lez-Franco.

</span>
<span class="ltx_bibblock">Diffuse, attend, and segment: Unsupervised zero-shot segmentation using stable diffusion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">CoRR</span>, abs/2308.12469, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Annual Conference on Neural Information Processing Systems</span>, pages 5998‚Äì6008, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Martina¬†G. Vilas, Timothy Schauml√∂ffel, and Gemma Roig.

</span>
<span class="ltx_bibblock">Analyzing vision transformers for image classification in class embedding space.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Annual Conference on Neural Information Processing Systems 2023</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Qiang Wan, Zilong Huang, Bingyi Kang, Jiashi Feng, and Li¬†Zhang.

</span>
<span class="ltx_bibblock">Harnessing diffusion models for visual perception with meta prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">CoRR</span>, abs/2312.14733, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang.

</span>
<span class="ltx_bibblock">Openauc: Towards auc-oriented open-set recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Annual Conference on Neural Information Processing Systems</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang.

</span>
<span class="ltx_bibblock">Optimizing partial area under the top-k curve: Theory and practice.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 45(4):5053‚Äì5069, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang.

</span>
<span class="ltx_bibblock">A unified generalization analysis of re-weighting and logit-adjustment for imbalanced learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Annual Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike¬†Zheng Shou, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Datasetdm: Synthesizing data with perception annotations using diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Annual Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Changming Xiao, Qi¬†Yang, Feng Zhou, and Changshui Zhang.

</span>
<span class="ltx_bibblock">From text to mask: Localizing entities using the attention of text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">CoRR</span>, abs/2309.04109, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini¬†De Mello.

</span>
<span class="ltx_bibblock">Open-vocabulary panoptic segmentation with text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 2955‚Äì2966, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Xingyi Yang and Xinchao Wang.

</span>
<span class="ltx_bibblock">Diffusion model as representation learner.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">IEEE/CVF International Conference on Computer Vision</span>, pages 18892‚Äì18903, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.

</span>
<span class="ltx_bibblock">LSUN: construction of a large-scale image dataset using deep learning with humans in the loop.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">CoRR</span>, abs/1506.03365, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
David¬†Junhao Zhang, Mutian Xu, Chuhui Xue, Wenqing Zhang, Xiaoguang Han, Song Bai, and Mike¬†Zheng Shou.

</span>
<span class="ltx_bibblock">Free-atm: Exploring unsupervised learning on diffusion-generated images with free attention masks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">CoRR</span>, abs/2308.06739, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa¬†Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang.

</span>
<span class="ltx_bibblock">A tale of two features: Stable diffusion complements DINO for zero-shot semantic correspondence.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Annual Conference on Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhengxin Zhang, Qingjie Liu, and Yunhong Wang.

</span>
<span class="ltx_bibblock">Road extraction by deep residual u-net.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">IEEE Geosci. Remote. Sens. Lett.</span>, 15(5):749‚Äì753, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu.

</span>
<span class="ltx_bibblock">Unleashing text-to-image diffusion models for visual perception.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">IEEE/CVF International Conference on Computer Vision</span>, pages 5706‚Äì5716, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Scene parsing through ADE20K dataset.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 5122‚Äì5130, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Jingyi Zhou, Jiamu Sheng, Jiayuan Fan, Peng Ye, Tong He, Bin Wang, and Tao Chen.

</span>
<span class="ltx_bibblock">When hyperspectral image classification meets diffusion models: An unsupervised feature learning framework.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">CoRR</span>, abs/2306.08964, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">Contents</h2>
<div class="ltx_para" id="Ax1.p1">
<span class="ltx_ERROR undefined" id="Ax1.p1.1">\startcontents</span>
<p class="ltx_p" id="Ax1.p1.2">[appendices]
<span class="ltx_ERROR undefined" id="Ax1.p1.2.1">\printcontents</span>[appendices]l1</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Visualization for Distinct Properties of Diffusion U-Nets</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Visualization of Other Sample Images</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We have provided activation visualization from SDXL in a simple outdoor scene presenting a horse in the wild.
Next, we will show visualization from the same model in different scenes to demonstrate the universality of the observed properties.
Another simple outdoor scene presenting a cat is in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F5" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.
Two simple indoor scenes are visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F6" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F7" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>.
Two complex outdoor scenes of urban streets are visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F8" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F9" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">9</span></a>.
Two complex indoor scenes are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F10" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">10</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F11" title="In A.1 Visualization of Other Sample Images ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="A1.F5.g1" src="extracted/5915702/pics/app-sdxl-cat.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visualization of SDXL activations on a simple outdoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="335" id="A1.F6.g1" src="extracted/5915702/pics/app-sdxl-chair.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of SDXL activations on a simple indoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="A1.F7.g1" src="extracted/5915702/pics/app-sdxl-bottle.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of SDXL activations on a simple indoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="A1.F8.g1" src="extracted/5915702/pics/app-sdxl-city.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Visualization of SDXL activations on a complex outdoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="A1.F9.g1" src="extracted/5915702/pics/app-sdxl-city2.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Visualization of SDXL activations on a complex outdoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="A1.F10.g1" src="extracted/5915702/pics/app-sdxl-ade.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Visualization of SDXL activations on a complex indoor scene.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="A1.F11.g1" src="extracted/5915702/pics/app-sdxl-ade2.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Visualization of SDXL activations on a complex indoor scene.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Visualization from Other Diffusion Models</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p1.1.1">Playground v2 Activations.</span>
Playground v2<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic" title="">https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic</a></span></span></span> shares the same architecture as SDXL but is trained independently, and it is claimed to be more powerful in generation.
In <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F12" title="In A.2 Visualization from Other Diffusion Models ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">12</span></a>, we present its activation visualization using the same horse image as the primary SDXL visualization.
Compared to SDXL, Playground v2 activations are less noisy, particularly in the down-stage.
This supports the claim that Playground v2 is a stronger model.</p>
</div>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="A1.F12.g1" src="extracted/5915702/pics/app-pgv2-horse.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Visualization of Playground v2 activations on a simple outdoor scene.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">SDv1.5 Activations.</span>
SDv1.5, as an older model, has a slightly different architecture from SDXL.
Specifically, SDv1.5 has four resolutions instead of three, but its ViTs contain only one layer.
Moreover, in SDXL, only the highest resolution lacks ViT due to efficiency concerns, while in SDv1.5, only the lowest resolution lacks ViT due to its low resolution.
Despite the architectural changes, the three unique properties still apply to SDv1.5, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F13" title="In A.2 Visualization from Other Diffusion Models ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="A1.F13.g1" src="extracted/5915702/pics/app-sd15-horse.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Visualization of SDv1.5 activations on a simple outdoor scene.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.1.1">Video Diffusion Activations.</span>
To further demonstrate the universality, we even select a diffusion model for video generation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib6" title="">6</a>]</cite> for visualization.
This model is based on the SDv1.5 architecture, with an additional temporal attention layer inserted after cross-attention to enable sequential generation.
Although this model is designed for a different task, we can still observe the three unique properties in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F14" title="In A.2 Visualization from Other Diffusion Models ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="A1.F14.g1" src="extracted/5915702/pics/app-video.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Visualization of the activations of a video diffusion model. The input is a short video, and we visualize one frame of it.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p4.1.1">Conventional U-Net Activations.</span>
As comparison to diffusion U-Net, we also visualize some activations from a conventional U-Net in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A1.F15" title="In A.2 Visualization from Other Diffusion Models ‚Ä£ Appendix A Additional Visualization for Distinct Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">15</span></a>, where the three properties of diffusion U-Net can hardly be observed.</p>
</div>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="286" id="A1.F15.g1" src="extracted/5915702/pics/rebuttal-unet.jpg" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Activation visualization from a traditional U-Net for semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib57" title="">57</a>]</cite>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of Our Method</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we index all diffusion U-Net components in the order in which they are activated during a network forward run.
Besides descriptions in natural language, we also index the activations using the same notations as used in our code implementation.
These notations denote stages using <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">down/mid/up</span>, resolutions as <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">level</span>, and module index as <span class="ltx_text ltx_font_italic" id="A2.p1.1.3">repeat</span>.
Index starts to count from 0 instead of 1, following the convention in coding.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Feature Selection Solution for SDv1.5</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.3">We select four activations from SDv1.5, maintaining the same total feature channels as the conventional way to extract all output activations from each resolution.</p>
<ol class="ltx_enumerate" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">The cross-attention query activation from the 2nd ViT, the 2nd resolution. This provides coarse information for simple scenes (up-level1-repeat1-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">The inter-module activation after the 3rd ResModule, the 2nd resolution. This provides coarse information for complex scenes (up-level1-repeat2-res-out).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1">The cross-attention query activation from the 2nd ViT, the 3rd resolution. This provides finer information with a higher resolution (up-level2-repeat1-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iv)</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1">The self-attention key activation from the 1st ViT, the 4th resolution. This extracts features from the highest resolution, harnessing the noise suppression effect of self-attention locality (up-level3-repeat0-vit-block0-self-k).</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A2.SS1.p1.2">We omit the index of basic blocks in ViTs, as SDv1.5 only contains one-layer ViTs.
Additionally, we totally ignore the lowest resolution, as its activations have very low resolution (<math alttext="8\times 8" class="ltx_Math" display="inline" id="A2.SS1.p1.1.m1.1"><semantics id="A2.SS1.p1.1.m1.1a"><mrow id="A2.SS1.p1.1.m1.1.1" xref="A2.SS1.p1.1.m1.1.1.cmml"><mn id="A2.SS1.p1.1.m1.1.1.2" xref="A2.SS1.p1.1.m1.1.1.2.cmml">8</mn><mo id="A2.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="A2.SS1.p1.1.m1.1.1.3" xref="A2.SS1.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.1.m1.1b"><apply id="A2.SS1.p1.1.m1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1"><times id="A2.SS1.p1.1.m1.1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1.1"></times><cn id="A2.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.SS1.p1.1.m1.1.1.2">8</cn><cn id="A2.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="A2.SS1.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.1.m1.1c">8\times 8</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.1.m1.1d">8 √ó 8</annotation></semantics></math> if the input image is <math alttext="512\times 512" class="ltx_Math" display="inline" id="A2.SS1.p1.2.m2.1"><semantics id="A2.SS1.p1.2.m2.1a"><mrow id="A2.SS1.p1.2.m2.1.1" xref="A2.SS1.p1.2.m2.1.1.cmml"><mn id="A2.SS1.p1.2.m2.1.1.2" xref="A2.SS1.p1.2.m2.1.1.2.cmml">512</mn><mo id="A2.SS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS1.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="A2.SS1.p1.2.m2.1.1.3" xref="A2.SS1.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.2.m2.1b"><apply id="A2.SS1.p1.2.m2.1.1.cmml" xref="A2.SS1.p1.2.m2.1.1"><times id="A2.SS1.p1.2.m2.1.1.1.cmml" xref="A2.SS1.p1.2.m2.1.1.1"></times><cn id="A2.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="A2.SS1.p1.2.m2.1.1.2">512</cn><cn id="A2.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="A2.SS1.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.2.m2.1d">512 √ó 512</annotation></semantics></math>), suggesting inferiority, as supported by the quantitative comparison.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Feature Selection Solution for SDXL</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">Four activations are selected from SDXL, trying to get similar total feature channels to the SDv1.5 feature selection solution.</p>
<ol class="ltx_enumerate" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p" id="A2.I2.i1.p1.1">The output activation after the 8th basic block, the 1st ViT, the 1st resolution. This provides relatively coarse information for simple scenes (up-level0-repeat0-vit-block7-out).</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p" id="A2.I2.i2.p1.1">The output activation after the 6th basic block, the 1st ViT, the 1st resolution. This provides relatively coarse information for complex scenes (up-level0-repeat0-vit-block5-out).</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="A2.I2.i3.p1">
<p class="ltx_p" id="A2.I2.i3.p1.1">The cross-attention query activation from the 1st basic block, the 1st ViT, the 2nd resolution. This provides relatively fine information for simple scenes (up-level1-repeat0-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iv)</span>
<div class="ltx_para" id="A2.I2.i4.p1">
<p class="ltx_p" id="A2.I2.i4.p1.1">The output activation after the 1st basic block, the 1st ViT, the 2nd resolution. This provides relatively fine information for complex scenes (up-level1-repeat0-vit-block0-out).</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A2.SS2.p1.2">We ignore the highest resolution since it is affected by diffusion noises and lacks ViTs from which we can extract self-attention activations.
However, if the downstream model is strong enough to learn to suppress noises, it may be possible to additionally extract inter-module activations from this resolution to harness more information.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Feature Selection Solution with Additional Techniques</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">For the setting <span class="ltx_text ltx_font_italic" id="A2.SS3.p1.1.1">Ours-XL-t</span>, we mainly utilize two simple techniques that are also adopted in some SOTA methods:
(i) We additionally extract attention maps, <span class="ltx_text ltx_font_italic" id="A2.SS3.p1.1.2">i.e.</span>, the similarity scores of cross-attention query and key, as dense features¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib52" title="">52</a>]</cite>.
Such attention maps are closely related to the semantics of prompts, thus providing important supplementary information.
Despite its usefulness, this technique only adds a few additional channels to the features.
(ii) We amalgamate features from different models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib60" title="">60</a>]</cite> through simple concatenation.
This technique is a common practice and can be seen as an extension of amalgamating different activations together as a whole feature.
We next explain what activations are selected to implement the two techniquess.</p>
</div>
<div class="ltx_para" id="A2.SS3.p2">
<p class="ltx_p" id="A2.SS3.p2.1">We first extract features according to the feature selection solution for SDXL.
Afterward, we extract additional features from SDv1.5:</p>
<ol class="ltx_enumerate" id="A2.I3">
<li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A2.I3.i1.p1">
<p class="ltx_p" id="A2.I3.i1.p1.1">The cross-attention query activation from the 2nd ViT, the 2nd resolution (up-level1-repeat1-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="A2.I3.i2.p1">
<p class="ltx_p" id="A2.I3.i2.p1.1">The cross-attention query activation from the 2nd ViT, the 3rd resolution (up-level2-repeat1-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="A2.I3.i3.p1">
<p class="ltx_p" id="A2.I3.i3.p1.1">The upsampler output activation from the 3rd resolution, to harness high-resolution information (up-level2-upsampler-out).</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iv)</span>
<div class="ltx_para" id="A2.I3.i4.p1">
<p class="ltx_p" id="A2.I3.i4.p1.1">The self-attention key activation from the 1st ViT, the 4th resolution, to harness high-resolution information (up-level3-repeat0-vit-block0-self-k).</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(v)</span>
<div class="ltx_para" id="A2.I3.i5.p1">
<p class="ltx_p" id="A2.I3.i5.p1.1">The attention maps averaged over all cross-attention layers in the up-stage.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A2.SS3.p2.2">We also extract one feature from Playground v2:
the output activation after the 4th basic block, the 1st ViT, the 1st resolution (up-level0-repeat0-vit-block3-out).</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Alternative Feature Selection Solution with Additional Techniques</h3>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1">Large-scale datasets for semantic segmentation mostly consist of images of complex scenes.
In such cases, we find attention maps can be too noisy to be useful.
Therefore, we discard the attention map technique and the entire SDv1.5 model, as it is weaker compared to the newer SDXL and Playground v2 models.
To compensate for the loss of activations, we select additional activations from SDXL and Playground v2.</p>
</div>
<div class="ltx_para" id="A2.SS4.p2">
<p class="ltx_p" id="A2.SS4.p2.1">From SDXL, we select all the activations as described in the feature selection solution for SDXL and select one additional activation:
the upsampler output activation from the 2nd resolution, to harness high-resolution information (up-level1-upsampler-out).
From Playground v2, we select the following activations:</p>
<ol class="ltx_enumerate" id="A2.I4">
<li class="ltx_item" id="A2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A2.I4.i1.p1">
<p class="ltx_p" id="A2.I4.i1.p1.1">The output activation after the 6th basic block, the 1st ViT, the 1st resolution (up-level0-repeat0-vit-block5-out).</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="A2.I4.i2.p1">
<p class="ltx_p" id="A2.I4.i2.p1.1">The cross-attention query activation from the 1st basic block, the 1st ViT, the 2nd resolution (up-level1-repeat0-vit-block0-cross-q).</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="A2.I4.i3.p1">
<p class="ltx_p" id="A2.I4.i3.p1.1">The upsampler output activation from the 2nd resolution (up-level1-upsampler-out).</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experimental Details</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Semantic Correspondence</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.1">Task and Dataset.</span>
Semantic correspondence¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib18" title="">18</a>]</cite> involves finding a pixel in an image that semantically matches another keypoint pixel in a reference image, such as the hind legs of two different cats.
We conduct experiments on the SPair-71k dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.6"><span class="ltx_text ltx_font_bold" id="A3.SS1.p2.6.1">Evaluation Metric.</span>
<math alttext="\text{PCK@0.1}_{\text{img}}(\uparrow)" class="ltx_Math" display="inline" id="A3.SS1.p2.1.m1.1"><semantics id="A3.SS1.p2.1.m1.1a"><mrow id="A3.SS1.p2.1.m1.1.2" xref="A3.SS1.p2.1.m1.1.2.cmml"><msub id="A3.SS1.p2.1.m1.1.2.2" xref="A3.SS1.p2.1.m1.1.2.2.cmml"><mtext id="A3.SS1.p2.1.m1.1.2.2.2" xref="A3.SS1.p2.1.m1.1.2.2.2a.cmml">PCK@0.1</mtext><mtext id="A3.SS1.p2.1.m1.1.2.2.3" xref="A3.SS1.p2.1.m1.1.2.2.3a.cmml">img</mtext></msub><mo id="A3.SS1.p2.1.m1.1.2.1" xref="A3.SS1.p2.1.m1.1.2.1.cmml">‚Å¢</mo><mrow id="A3.SS1.p2.1.m1.1.2.3.2" xref="A3.SS1.p2.1.m1.1.2.cmml"><mo id="A3.SS1.p2.1.m1.1.2.3.2.1" stretchy="false" xref="A3.SS1.p2.1.m1.1.2.cmml">(</mo><mo id="A3.SS1.p2.1.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="A3.SS1.p2.1.m1.1.1.cmml">‚Üë</mo><mo id="A3.SS1.p2.1.m1.1.2.3.2.2" stretchy="false" xref="A3.SS1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.1.m1.1b"><apply id="A3.SS1.p2.1.m1.1.2.cmml" xref="A3.SS1.p2.1.m1.1.2"><times id="A3.SS1.p2.1.m1.1.2.1.cmml" xref="A3.SS1.p2.1.m1.1.2.1"></times><apply id="A3.SS1.p2.1.m1.1.2.2.cmml" xref="A3.SS1.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="A3.SS1.p2.1.m1.1.2.2.1.cmml" xref="A3.SS1.p2.1.m1.1.2.2">subscript</csymbol><ci id="A3.SS1.p2.1.m1.1.2.2.2a.cmml" xref="A3.SS1.p2.1.m1.1.2.2.2"><mtext id="A3.SS1.p2.1.m1.1.2.2.2.cmml" xref="A3.SS1.p2.1.m1.1.2.2.2">PCK@0.1</mtext></ci><ci id="A3.SS1.p2.1.m1.1.2.2.3a.cmml" xref="A3.SS1.p2.1.m1.1.2.2.3"><mtext id="A3.SS1.p2.1.m1.1.2.2.3.cmml" mathsize="70%" xref="A3.SS1.p2.1.m1.1.2.2.3">img</mtext></ci></apply><ci id="A3.SS1.p2.1.m1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1">‚Üë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.1.m1.1c">\text{PCK@0.1}_{\text{img}}(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.1.m1.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT ( ‚Üë )</annotation></semantics></math> and <math alttext="\text{PCK@0.1}_{\text{bbox}}(\uparrow)" class="ltx_Math" display="inline" id="A3.SS1.p2.2.m2.1"><semantics id="A3.SS1.p2.2.m2.1a"><mrow id="A3.SS1.p2.2.m2.1.2" xref="A3.SS1.p2.2.m2.1.2.cmml"><msub id="A3.SS1.p2.2.m2.1.2.2" xref="A3.SS1.p2.2.m2.1.2.2.cmml"><mtext id="A3.SS1.p2.2.m2.1.2.2.2" xref="A3.SS1.p2.2.m2.1.2.2.2a.cmml">PCK@0.1</mtext><mtext id="A3.SS1.p2.2.m2.1.2.2.3" xref="A3.SS1.p2.2.m2.1.2.2.3a.cmml">bbox</mtext></msub><mo id="A3.SS1.p2.2.m2.1.2.1" xref="A3.SS1.p2.2.m2.1.2.1.cmml">‚Å¢</mo><mrow id="A3.SS1.p2.2.m2.1.2.3.2" xref="A3.SS1.p2.2.m2.1.2.cmml"><mo id="A3.SS1.p2.2.m2.1.2.3.2.1" stretchy="false" xref="A3.SS1.p2.2.m2.1.2.cmml">(</mo><mo id="A3.SS1.p2.2.m2.1.1" lspace="0em" rspace="0em" stretchy="false" xref="A3.SS1.p2.2.m2.1.1.cmml">‚Üë</mo><mo id="A3.SS1.p2.2.m2.1.2.3.2.2" stretchy="false" xref="A3.SS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.2.m2.1b"><apply id="A3.SS1.p2.2.m2.1.2.cmml" xref="A3.SS1.p2.2.m2.1.2"><times id="A3.SS1.p2.2.m2.1.2.1.cmml" xref="A3.SS1.p2.2.m2.1.2.1"></times><apply id="A3.SS1.p2.2.m2.1.2.2.cmml" xref="A3.SS1.p2.2.m2.1.2.2"><csymbol cd="ambiguous" id="A3.SS1.p2.2.m2.1.2.2.1.cmml" xref="A3.SS1.p2.2.m2.1.2.2">subscript</csymbol><ci id="A3.SS1.p2.2.m2.1.2.2.2a.cmml" xref="A3.SS1.p2.2.m2.1.2.2.2"><mtext id="A3.SS1.p2.2.m2.1.2.2.2.cmml" xref="A3.SS1.p2.2.m2.1.2.2.2">PCK@0.1</mtext></ci><ci id="A3.SS1.p2.2.m2.1.2.2.3a.cmml" xref="A3.SS1.p2.2.m2.1.2.2.3"><mtext id="A3.SS1.p2.2.m2.1.2.2.3.cmml" mathsize="70%" xref="A3.SS1.p2.2.m2.1.2.2.3">bbox</mtext></ci></apply><ci id="A3.SS1.p2.2.m2.1.1.cmml" xref="A3.SS1.p2.2.m2.1.1">‚Üë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.2.m2.1c">\text{PCK@0.1}_{\text{bbox}}(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.2.m2.1d">PCK@0.1 start_POSTSUBSCRIPT bbox end_POSTSUBSCRIPT ( ‚Üë )</annotation></semantics></math> are used, following the widely-adopted protocol reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib30" title="">30</a>]</cite>. These two metrics mean the percentage of correctly predicted keypoints, where a predicted keypoint is considered to be correct if it lies within the neighborhood of the corresponding annotation with a radius of <math alttext="0.1\times max(h,w)" class="ltx_Math" display="inline" id="A3.SS1.p2.3.m3.2"><semantics id="A3.SS1.p2.3.m3.2a"><mrow id="A3.SS1.p2.3.m3.2.3" xref="A3.SS1.p2.3.m3.2.3.cmml"><mrow id="A3.SS1.p2.3.m3.2.3.2" xref="A3.SS1.p2.3.m3.2.3.2.cmml"><mn id="A3.SS1.p2.3.m3.2.3.2.2" xref="A3.SS1.p2.3.m3.2.3.2.2.cmml">0.1</mn><mo id="A3.SS1.p2.3.m3.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p2.3.m3.2.3.2.1.cmml">√ó</mo><mi id="A3.SS1.p2.3.m3.2.3.2.3" xref="A3.SS1.p2.3.m3.2.3.2.3.cmml">m</mi></mrow><mo id="A3.SS1.p2.3.m3.2.3.1" xref="A3.SS1.p2.3.m3.2.3.1.cmml">‚Å¢</mo><mi id="A3.SS1.p2.3.m3.2.3.3" xref="A3.SS1.p2.3.m3.2.3.3.cmml">a</mi><mo id="A3.SS1.p2.3.m3.2.3.1a" xref="A3.SS1.p2.3.m3.2.3.1.cmml">‚Å¢</mo><mi id="A3.SS1.p2.3.m3.2.3.4" xref="A3.SS1.p2.3.m3.2.3.4.cmml">x</mi><mo id="A3.SS1.p2.3.m3.2.3.1b" xref="A3.SS1.p2.3.m3.2.3.1.cmml">‚Å¢</mo><mrow id="A3.SS1.p2.3.m3.2.3.5.2" xref="A3.SS1.p2.3.m3.2.3.5.1.cmml"><mo id="A3.SS1.p2.3.m3.2.3.5.2.1" stretchy="false" xref="A3.SS1.p2.3.m3.2.3.5.1.cmml">(</mo><mi id="A3.SS1.p2.3.m3.1.1" xref="A3.SS1.p2.3.m3.1.1.cmml">h</mi><mo id="A3.SS1.p2.3.m3.2.3.5.2.2" xref="A3.SS1.p2.3.m3.2.3.5.1.cmml">,</mo><mi id="A3.SS1.p2.3.m3.2.2" xref="A3.SS1.p2.3.m3.2.2.cmml">w</mi><mo id="A3.SS1.p2.3.m3.2.3.5.2.3" stretchy="false" xref="A3.SS1.p2.3.m3.2.3.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.3.m3.2b"><apply id="A3.SS1.p2.3.m3.2.3.cmml" xref="A3.SS1.p2.3.m3.2.3"><times id="A3.SS1.p2.3.m3.2.3.1.cmml" xref="A3.SS1.p2.3.m3.2.3.1"></times><apply id="A3.SS1.p2.3.m3.2.3.2.cmml" xref="A3.SS1.p2.3.m3.2.3.2"><times id="A3.SS1.p2.3.m3.2.3.2.1.cmml" xref="A3.SS1.p2.3.m3.2.3.2.1"></times><cn id="A3.SS1.p2.3.m3.2.3.2.2.cmml" type="float" xref="A3.SS1.p2.3.m3.2.3.2.2">0.1</cn><ci id="A3.SS1.p2.3.m3.2.3.2.3.cmml" xref="A3.SS1.p2.3.m3.2.3.2.3">ùëö</ci></apply><ci id="A3.SS1.p2.3.m3.2.3.3.cmml" xref="A3.SS1.p2.3.m3.2.3.3">ùëé</ci><ci id="A3.SS1.p2.3.m3.2.3.4.cmml" xref="A3.SS1.p2.3.m3.2.3.4">ùë•</ci><interval closure="open" id="A3.SS1.p2.3.m3.2.3.5.1.cmml" xref="A3.SS1.p2.3.m3.2.3.5.2"><ci id="A3.SS1.p2.3.m3.1.1.cmml" xref="A3.SS1.p2.3.m3.1.1">‚Ñé</ci><ci id="A3.SS1.p2.3.m3.2.2.cmml" xref="A3.SS1.p2.3.m3.2.2">ùë§</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.3.m3.2c">0.1\times max(h,w)</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.3.m3.2d">0.1 √ó italic_m italic_a italic_x ( italic_h , italic_w )</annotation></semantics></math>. For <math alttext="\text{PCK@0.1}_{\text{img}}" class="ltx_Math" display="inline" id="A3.SS1.p2.4.m4.1"><semantics id="A3.SS1.p2.4.m4.1a"><msub id="A3.SS1.p2.4.m4.1.1" xref="A3.SS1.p2.4.m4.1.1.cmml"><mtext id="A3.SS1.p2.4.m4.1.1.2" xref="A3.SS1.p2.4.m4.1.1.2a.cmml">PCK@0.1</mtext><mtext id="A3.SS1.p2.4.m4.1.1.3" xref="A3.SS1.p2.4.m4.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.4.m4.1b"><apply id="A3.SS1.p2.4.m4.1.1.cmml" xref="A3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.4.m4.1.1.1.cmml" xref="A3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="A3.SS1.p2.4.m4.1.1.2a.cmml" xref="A3.SS1.p2.4.m4.1.1.2"><mtext id="A3.SS1.p2.4.m4.1.1.2.cmml" xref="A3.SS1.p2.4.m4.1.1.2">PCK@0.1</mtext></ci><ci id="A3.SS1.p2.4.m4.1.1.3a.cmml" xref="A3.SS1.p2.4.m4.1.1.3"><mtext id="A3.SS1.p2.4.m4.1.1.3.cmml" mathsize="70%" xref="A3.SS1.p2.4.m4.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.4.m4.1c">\text{PCK@0.1}_{\text{img}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.4.m4.1d">PCK@0.1 start_POSTSUBSCRIPT img end_POSTSUBSCRIPT</annotation></semantics></math>/<math alttext="\text{PCK@0.1}_{\text{bbox}}" class="ltx_Math" display="inline" id="A3.SS1.p2.5.m5.1"><semantics id="A3.SS1.p2.5.m5.1a"><msub id="A3.SS1.p2.5.m5.1.1" xref="A3.SS1.p2.5.m5.1.1.cmml"><mtext id="A3.SS1.p2.5.m5.1.1.2" xref="A3.SS1.p2.5.m5.1.1.2a.cmml">PCK@0.1</mtext><mtext id="A3.SS1.p2.5.m5.1.1.3" xref="A3.SS1.p2.5.m5.1.1.3a.cmml">bbox</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.5.m5.1b"><apply id="A3.SS1.p2.5.m5.1.1.cmml" xref="A3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.5.m5.1.1.1.cmml" xref="A3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="A3.SS1.p2.5.m5.1.1.2a.cmml" xref="A3.SS1.p2.5.m5.1.1.2"><mtext id="A3.SS1.p2.5.m5.1.1.2.cmml" xref="A3.SS1.p2.5.m5.1.1.2">PCK@0.1</mtext></ci><ci id="A3.SS1.p2.5.m5.1.1.3a.cmml" xref="A3.SS1.p2.5.m5.1.1.3"><mtext id="A3.SS1.p2.5.m5.1.1.3.cmml" mathsize="70%" xref="A3.SS1.p2.5.m5.1.1.3">bbox</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.5.m5.1c">\text{PCK@0.1}_{\text{bbox}}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.5.m5.1d">PCK@0.1 start_POSTSUBSCRIPT bbox end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="h,w" class="ltx_Math" display="inline" id="A3.SS1.p2.6.m6.2"><semantics id="A3.SS1.p2.6.m6.2a"><mrow id="A3.SS1.p2.6.m6.2.3.2" xref="A3.SS1.p2.6.m6.2.3.1.cmml"><mi id="A3.SS1.p2.6.m6.1.1" xref="A3.SS1.p2.6.m6.1.1.cmml">h</mi><mo id="A3.SS1.p2.6.m6.2.3.2.1" xref="A3.SS1.p2.6.m6.2.3.1.cmml">,</mo><mi id="A3.SS1.p2.6.m6.2.2" xref="A3.SS1.p2.6.m6.2.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.6.m6.2b"><list id="A3.SS1.p2.6.m6.2.3.1.cmml" xref="A3.SS1.p2.6.m6.2.3.2"><ci id="A3.SS1.p2.6.m6.1.1.cmml" xref="A3.SS1.p2.6.m6.1.1">‚Ñé</ci><ci id="A3.SS1.p2.6.m6.2.2.cmml" xref="A3.SS1.p2.6.m6.2.2">ùë§</ci></list></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.6.m6.2c">h,w</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.6.m6.2d">italic_h , italic_w</annotation></semantics></math> denote the dimension of the entire image/object bounding box, respectively.</p>
</div>
<div class="ltx_para" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p3.1.1">SOTA Competitors.</span>
We provide the results from four SOTA methods: DINO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib4" title="">4</a>]</cite> and DHPF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib31" title="">31</a>]</cite> as non-diffusion-feature methods, as well as DIFT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib41" title="">41</a>]</cite> and DHF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib29" title="">29</a>]</cite> as diffusion feature methods.</p>
</div>
<div class="ltx_para" id="A3.SS1.p4">
<p class="ltx_p" id="A3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p4.1.1">Implementation Details.</span>
The semantic correspondence task can be done via the nearest neighbor algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib42" title="">42</a>]</cite>, which is unsupervised and training-free¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib30" title="">30</a>]</cite>.
We add one additional trainable convolutional layer before applying the nearest neighbor algorithm to refine the input features, which is also adopted by some SOTAs including DHF.
The model is trained for two epochs, each containing 5,000 sample pairs, following conventional settings.
Our implementation is derived from DHF, and we keep all hyper-parameters at their default settings.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Semantic Segmentation</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p1.1.1">Task and Dataset.</span>
Semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib18" title="">18</a>]</cite> is essentially pixel-level classification.
For this task, we choose the ADE20K dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib59" title="">59</a>]</cite> with over 20k annotated images of 150 semantic categories, and the CityScapes dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib8" title="">8</a>]</cite>, which contains 5,000 fine annotated images of urban street scenes.</p>
</div>
<div class="ltx_para" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p2.1.1">Evaluation Metric.</span>
We use mIoU metric, which is the mean over the IoU performance across all semantic classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib18" title="">18</a>]</cite>. For each image, IoU (Intersection over Union, <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.SS2.p2.1.m1.1"><semantics id="A3.SS2.p2.1.m1.1a"><mo id="A3.SS2.p2.1.m1.1.1" stretchy="false" xref="A3.SS2.p2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p2.1.m1.1b"><ci id="A3.SS2.p2.1.m1.1.1.cmml" xref="A3.SS2.p2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p2.1.m1.1d">‚Üë</annotation></semantics></math>) is defined by #(overlapped pixels between the prediction and the ground truth) / #(union pixels of them).</p>
</div>
<div class="ltx_para" id="A3.SS2.p3">
<p class="ltx_p" id="A3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p3.1.1">SOTA Competitors.</span>
We choose three diffusion feature SOTA methods as competitors.
ODISE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib52" title="">52</a>]</cite> is an early method with a simple implementation.
VPD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib58" title="">58</a>]</cite> is another early study of this field, which introduces additional text adapter modules for improvement.
Meta Prompts¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib46" title="">46</a>]</cite> is a newer method and shows significant improvements.
We also report the performance of MaskCLIP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib11" title="">11</a>]</cite>, which is included as a competitor in the ODISE study.</p>
</div>
<div class="ltx_para" id="A3.SS2.p4">
<p class="ltx_p" id="A3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p4.1.1">Implementation Details.</span>
Both VPD and Meta Prompts perform full-scale fine-tuning on the diffusion U-Net using feedback from the discriminative task.
This heavy fine-tuning does not entirely comply with the motivation of diffusion feature, which seeks a balance between wider applicability and less training, and is hard to extend to the larger SDXL model.
Therefore, we keep the entire diffusion model frozen instead.
As the setting has been changed, the performance of VPD and Meta Prompts in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S6.T2" title="In 6.1 Empirical Results on Semantic Correspondence ‚Ä£ 6 Experimental Validation on Multiple Discriminative Tasks ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a> is based on our experiments, not the reported results from the original papers.
Our implementation directly uses the hyper-parameters reported in Meta Prompts.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Label-Scarce Segmentation</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS3.p1.1.1">Task and Dataset.</span>
Using features from a pre-trained diffusion model ensures good performance even when labeled training data is scarce¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.
For this setting, we use a dataset collected in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite> and experiment on its Horse-21 subset, the data of which is sourced from LSUN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib54" title="">54</a>]</cite>.
This subset contains only 30 labeled training images to be consistent with the intuition.
The semantic segmentation in the label-scarce scenario also uses the mIoU metric.</p>
</div>
<div class="ltx_para" id="A3.SS3.p2">
<p class="ltx_p" id="A3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.1">SOTA Competitors.</span>
We select the SOTA diffusion feature approach, DDPM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>, as the major competitor.
We also include other representative segmentation methods: DatasetDDPM, MAE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib19" title="">19</a>]</cite>, SwAV¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib3" title="">3</a>]</cite>, which are all reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A3.SS3.p3">
<p class="ltx_p" id="A3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.1">Implementation Details.</span>
Following DDPM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>, the downstream model is an ensemble of ten simple MLP networks, each conducting pixel-wise classification.
The simplicity of the model is intended to demonstrate the innate capability and generalizability of diffusion models.
Our implementation is derived from DDPM with only batch size changed among all hyper-parameters.
We use a larger batch size for faster experiments as a smaller one does not improve performance.
<span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.2">Additionally, this is also the setting for the quantitative comparison, as the compact size of the dataset can enhance efficiency</span>.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Experimental Results</h2>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Generalizability across Different Scenes</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1">As stated in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5" title="5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>, it is preferable to conduct the quantitative comparison across multiple datasets and choose activations that are optimal for each.
This approach can enhance the generalizability of the selected features.
To evaluate the generalizability of our features, we conducted an additional experiment, with results presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.T3" title="In D.1 Generalizability across Different Scenes ‚Ä£ Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="A4.SS1.p2">
<p class="ltx_p" id="A4.SS1.p2.1">In this experiment, we design an alternative feature selection solution for SDXL, based solely on quantitative results from a single dataset consisting of simple scenes.
In this solution, we extract both optimal and slightly sub-optimal activations, maintaining the same total number of feature channels as the standard solution.
The alternative solution achieves higher performance on the simple scene it is based on but performs significantly worse on the other scene.
Therefore, we conclude that our standard feature selection solution achieves generalizability across different scenes, albeit with a slight performance drop compared to features specifically selected for each scene.</p>
</div>
<figure class="ltx_table" id="A4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examination of generalizability across different scenes. <span class="ltx_text ltx_font_italic" id="A4.T3.4.1">Generic Solution</span> refers to our standard feature selection solution for SDXL, while <span class="ltx_text ltx_font_italic" id="A4.T3.5.2">Specific Solution</span> refers to the outcome of considering only a simple scene for the quantitative comparison. The experiment is conducted on the label-scarce segmentation task, where the Horse-21 subset is used for simple scenes and the Bedroom-28 subset is used for complex scenes.
We mark the better results as <span class="ltx_text ltx_font_bold" id="A4.T3.6.3" style="color:#FF8000;">bold</span> font.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T3.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T3.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A4.T3.7.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T3.7.1.1.2">Simple Scene</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T3.7.1.1.3">Complex Scene</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T3.7.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T3.7.2.1.1">Generic Solution</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T3.7.2.1.2">63.34</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T3.7.2.1.3"><span class="ltx_text ltx_font_bold" id="A4.T3.7.2.1.3.1" style="color:#FF8000;">47.55</span></td>
</tr>
<tr class="ltx_tr" id="A4.T3.7.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A4.T3.7.3.2.1">Specific Solution</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T3.7.3.2.2"><span class="ltx_text ltx_font_bold" id="A4.T3.7.3.2.2.1" style="color:#FF8000;">63.70</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A4.T3.7.3.2.3">45.41</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Quantitative Comparison Results</h3>
<figure class="ltx_table" id="A4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Quantitative comparison results of SDXL and Playground v2. This table shows the results from the lowest resolution.
Activation ID indicates the location of each activation in the diffusion U-Net.
The best results are in <span class="ltx_text ltx_font_bold" id="A4.T4.3.1" style="color:#FF8000;">bold</span> font and the runner-up is <span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.4.2" style="color:#036FC1;">underlined</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T4.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T4.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T4.5.1.1.1" rowspan="2"><span class="ltx_text" id="A4.T4.5.1.1.1.1">Activation ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A4.T4.5.1.1.2">SDXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A4.T4.5.1.1.3">Playground v2</th>
</tr>
<tr class="ltx_tr" id="A4.T4.5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T4.5.2.2.1">Simple</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T4.5.2.2.2">Complex</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T4.5.2.2.3">Simple</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="A4.T4.5.2.2.4">Complex</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T4.5.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T4.5.3.1.1">up-level0-repeat0-res-out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T4.5.3.1.2">53.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T4.5.3.1.3">38.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T4.5.3.1.4">54.57</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T4.5.3.1.5">40.73</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.4.2">
<td class="ltx_td ltx_align_left" id="A4.T4.5.4.2.1">up-level0-repeat0-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.4.2.2">56.46</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.4.2.3">42.31</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.4.2.4">55.32</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.4.2.5">41.93</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.5.3">
<td class="ltx_td ltx_align_left" id="A4.T4.5.5.3.1">up-level0-repeat0-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.5.3.2">57.36</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.5.3.3">42.31</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.5.3.4">56.60</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.5.3.5">44.52</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.6.4">
<td class="ltx_td ltx_align_left" id="A4.T4.5.6.4.1">up-level0-repeat0-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.6.4.2">57.82</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.6.4.3">42.72</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.6.4.4">56.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.6.4.5">43.93</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.7.5">
<td class="ltx_td ltx_align_left" id="A4.T4.5.7.5.1">up-level0-repeat0-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.7.5.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.5.7.5.2.1" style="color:#036FC1;">58.73</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.7.5.3">42.62</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.7.5.4">58.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.7.5.5">45.82</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.8.6">
<td class="ltx_td ltx_align_left" id="A4.T4.5.8.6.1">up-level0-repeat0-vit-block3-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.8.6.2">58.27</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.8.6.3">43.09</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.8.6.4">57.79</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.8.6.5">45.90</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.9.7">
<td class="ltx_td ltx_align_left" id="A4.T4.5.9.7.1">up-level0-repeat0-vit-block3-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.9.7.2"><span class="ltx_text ltx_font_bold" id="A4.T4.5.9.7.2.1" style="color:#FF8000;">58.95</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.9.7.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.5.9.7.3.1" style="color:#036FC1;">44.83</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.9.7.4"><span class="ltx_text ltx_font_bold" id="A4.T4.5.9.7.4.1" style="color:#FF8000;">59.04</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.9.7.5">47.81</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.10.8">
<td class="ltx_td ltx_align_left" id="A4.T4.5.10.8.1">up-level0-repeat0-vit-block5-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.10.8.2">57.30</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.10.8.3">44.36</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.10.8.4">56.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.10.8.5">47.38</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.11.9">
<td class="ltx_td ltx_align_left" id="A4.T4.5.11.9.1">up-level0-repeat0-vit-block5-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.11.9.2">58.70</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.11.9.3"><span class="ltx_text ltx_font_bold" id="A4.T4.5.11.9.3.1" style="color:#FF8000;">45.26</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.11.9.4">58.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.11.9.5"><span class="ltx_text ltx_font_bold" id="A4.T4.5.11.9.5.1" style="color:#FF8000;">49.77</span></td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.12.10">
<td class="ltx_td ltx_align_left" id="A4.T4.5.12.10.1">up-level0-repeat0-vit-block7-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.12.10.2">57.24</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.12.10.3">43.00</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.12.10.4">57.69</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.12.10.5">48.51</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.13.11">
<td class="ltx_td ltx_align_left" id="A4.T4.5.13.11.1">up-level0-repeat0-vit-block7-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.13.11.2"><span class="ltx_text ltx_font_bold" id="A4.T4.5.13.11.2.1" style="color:#FF8000;">58.95</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.13.11.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.5.13.11.3.1" style="color:#036FC1;">44.83</span></td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.13.11.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.5.13.11.4.1" style="color:#036FC1;">58.76</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.13.11.5">48.39</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.14.12">
<td class="ltx_td ltx_align_left" id="A4.T4.5.14.12.1">up-level0-repeat0-vit-block9-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.14.12.2">57.06</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.14.12.3">41.73</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.14.12.4">56.03</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.14.12.5">44.68</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.15.13">
<td class="ltx_td ltx_align_left" id="A4.T4.5.15.13.1">up-level0-repeat0-vit-block9-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.15.13.2">58.46</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.15.13.3">43.98</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.15.13.4">56.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.15.13.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T4.5.15.13.5.1" style="color:#036FC1;">48.54</span></td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.16.14">
<td class="ltx_td ltx_align_left" id="A4.T4.5.16.14.1">up-level0-repeat0-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.16.14.2">58.36</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.16.14.3">41.54</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.16.14.4">57.99</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.16.14.5">43.95</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.17.15">
<td class="ltx_td ltx_align_left" id="A4.T4.5.17.15.1">up-level0-repeat1-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.17.15.2">57.28</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.17.15.3">39.27</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.17.15.4">56.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.17.15.5">41.74</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.18.16">
<td class="ltx_td ltx_align_left" id="A4.T4.5.18.16.1">up-level0-repeat1-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.18.16.2">55.72</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.18.16.3">40.96</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.18.16.4">55.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.18.16.5">43.97</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.19.17">
<td class="ltx_td ltx_align_left" id="A4.T4.5.19.17.1">up-level0-repeat1-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.19.17.2">56.70</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.19.17.3">40.97</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.19.17.4">57.51</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.19.17.5">43.35</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.20.18">
<td class="ltx_td ltx_align_left" id="A4.T4.5.20.18.1">up-level0-repeat1-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.20.18.2">57.92</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.20.18.3">41.65</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.20.18.4">56.37</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.20.18.5">42.10</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.21.19">
<td class="ltx_td ltx_align_left" id="A4.T4.5.21.19.1">up-level0-repeat1-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.21.19.2">57.38</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.21.19.3">42.25</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.21.19.4">57.20</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.21.19.5">43.10</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.22.20">
<td class="ltx_td ltx_align_left" id="A4.T4.5.22.20.1">up-level0-repeat1-vit-block3-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.22.20.2">56.55</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.22.20.3">40.50</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.22.20.4">55.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.22.20.5">42.50</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.23.21">
<td class="ltx_td ltx_align_left" id="A4.T4.5.23.21.1">up-level0-repeat1-vit-block3-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.23.21.2">56.41</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.23.21.3">40.97</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.23.21.4">57.54</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.23.21.5">42.92</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.24.22">
<td class="ltx_td ltx_align_left" id="A4.T4.5.24.22.1">up-level0-repeat1-vit-block5-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.24.22.2">54.92</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.24.22.3">39.66</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.24.22.4">54.89</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.24.22.5">40.79</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.25.23">
<td class="ltx_td ltx_align_left" id="A4.T4.5.25.23.1">up-level0-repeat1-vit-block5-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.25.23.2">55.86</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.25.23.3">40.66</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.25.23.4">56.32</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.25.23.5">42.48</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.26.24">
<td class="ltx_td ltx_align_left" id="A4.T4.5.26.24.1">up-level0-repeat1-vit-block7-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.26.24.2">51.63</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.26.24.3">39.01</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.26.24.4">52.72</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.26.24.5">38.37</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.27.25">
<td class="ltx_td ltx_align_left" id="A4.T4.5.27.25.1">up-level0-repeat1-vit-block7-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.27.25.2">53.97</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.27.25.3">39.79</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.27.25.4">55.90</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.27.25.5">41.34</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.28.26">
<td class="ltx_td ltx_align_left" id="A4.T4.5.28.26.1">up-level0-repeat1-vit-block9-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.28.26.2">50.27</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.28.26.3">36.42</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.28.26.4">36.09</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.28.26.5">24.81</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.29.27">
<td class="ltx_td ltx_align_left" id="A4.T4.5.29.27.1">up-level0-repeat1-vit-block9-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.29.27.2">53.31</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.29.27.3">38.58</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.29.27.4">53.19</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.29.27.5">40.30</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.30.28">
<td class="ltx_td ltx_align_left" id="A4.T4.5.30.28.1">up-level0-repeat1-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.30.28.2">57.89</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.30.28.3">38.91</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.30.28.4">57.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.30.28.5">42.87</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.31.29">
<td class="ltx_td ltx_align_left" id="A4.T4.5.31.29.1">up-level0-repeat2-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.31.29.2">56.11</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.31.29.3">36.37</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.31.29.4">55.95</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.31.29.5">40.62</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.32.30">
<td class="ltx_td ltx_align_left" id="A4.T4.5.32.30.1">up-level0-repeat2-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.32.30.2">56.09</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.32.30.3">36.68</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.32.30.4">55.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.32.30.5">41.29</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.33.31">
<td class="ltx_td ltx_align_left" id="A4.T4.5.33.31.1">up-level0-repeat2-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.33.31.2">57.14</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.33.31.3">37.16</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.33.31.4">56.48</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.33.31.5">41.14</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.34.32">
<td class="ltx_td ltx_align_left" id="A4.T4.5.34.32.1">up-level0-repeat2-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.34.32.2">55.38</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.34.32.3">34.87</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.34.32.4">56.15</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.34.32.5">39.78</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.35.33">
<td class="ltx_td ltx_align_left" id="A4.T4.5.35.33.1">up-level0-repeat2-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.35.33.2">56.36</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.35.33.3">35.63</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.35.33.4">56.71</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.35.33.5">40.93</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.36.34">
<td class="ltx_td ltx_align_left" id="A4.T4.5.36.34.1">up-level0-repeat2-vit-block3-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.36.34.2">55.26</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.36.34.3">34.40</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.36.34.4">54.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.36.34.5">38.70</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.37.35">
<td class="ltx_td ltx_align_left" id="A4.T4.5.37.35.1">up-level0-repeat2-vit-block3-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.37.35.2">55.50</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.37.35.3">34.61</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.37.35.4">56.77</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.37.35.5">39.36</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.38.36">
<td class="ltx_td ltx_align_left" id="A4.T4.5.38.36.1">up-level0-repeat2-vit-block5-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.38.36.2">52.70</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.38.36.3">33.16</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.38.36.4">54.70</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.38.36.5">37.93</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.39.37">
<td class="ltx_td ltx_align_left" id="A4.T4.5.39.37.1">up-level0-repeat2-vit-block5-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.39.37.2">54.68</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.39.37.3">33.75</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.39.37.4">55.75</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.39.37.5">38.95</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.40.38">
<td class="ltx_td ltx_align_left" id="A4.T4.5.40.38.1">up-level0-repeat2-vit-block7-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.40.38.2">51.91</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.40.38.3">31.97</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.40.38.4">52.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.40.38.5">35.58</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.41.39">
<td class="ltx_td ltx_align_left" id="A4.T4.5.41.39.1">up-level0-repeat2-vit-block7-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.41.39.2">53.07</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.41.39.3">32.43</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.41.39.4">54.07</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.41.39.5">36.91</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.42.40">
<td class="ltx_td ltx_align_left" id="A4.T4.5.42.40.1">up-level0-repeat2-vit-block9-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.42.40.2">49.22</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.42.40.3">29.34</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.42.40.4">41.07</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.42.40.5">18.48</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.43.41">
<td class="ltx_td ltx_align_left" id="A4.T4.5.43.41.1">up-level0-repeat2-vit-block9-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.43.41.2">51.41</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.43.41.3">30.64</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.43.41.4">51.31</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.43.41.5">35.64</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.44.42">
<td class="ltx_td ltx_align_left" id="A4.T4.5.44.42.1">up-level0-repeat2-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.44.42.2">53.77</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.44.42.3">33.79</td>
<td class="ltx_td ltx_align_center" id="A4.T4.5.44.42.4">54.24</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T4.5.44.42.5">37.53</td>
</tr>
<tr class="ltx_tr" id="A4.T4.5.45.43">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T4.5.45.43.1">up-level0-upsampler-out</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T4.5.45.43.2">53.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T4.5.45.43.3">32.85</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T4.5.45.43.4">53.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A4.T4.5.45.43.5">36.30</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Quantitative comparison results of SDXL and Playground v2. This table shows the results from the middle resolution.
Activation ID indicates the location of each activation in the diffusion U-Net.
The best results are in <span class="ltx_text ltx_font_bold" id="A4.T5.3.1" style="color:#FF8000;">bold</span> font and the runner-up is <span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T5.4.2" style="color:#036FC1;">underlined</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T5.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T5.5.1.1.1" rowspan="2"><span class="ltx_text" id="A4.T5.5.1.1.1.1">Activation ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A4.T5.5.1.1.2">SDXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A4.T5.5.1.1.3">Playground v2</th>
</tr>
<tr class="ltx_tr" id="A4.T5.5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T5.5.2.2.1">Simple</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T5.5.2.2.2">Complex</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T5.5.2.2.3">Simple</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="A4.T5.5.2.2.4">Complex</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T5.5.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T5.5.3.1.1">up-level1-repeat0-res-out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.5.3.1.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T5.5.3.1.2.1" style="color:#036FC1;">47.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.5.3.1.3">30.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.5.3.1.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T5.5.3.1.4.1" style="color:#036FC1;">47.49</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T5.5.3.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T5.5.3.1.5.1" style="color:#036FC1;">31.63</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.4.2">
<td class="ltx_td ltx_align_left" id="A4.T5.5.4.2.1">up-level1-repeat0-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.4.2.2"><span class="ltx_text ltx_font_bold" id="A4.T5.5.4.2.2.1" style="color:#FF8000;">49.08</span></td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.4.2.3"><span class="ltx_text ltx_font_bold" id="A4.T5.5.4.2.3.1" style="color:#FF8000;">31.75</span></td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.4.2.4"><span class="ltx_text ltx_font_bold" id="A4.T5.5.4.2.4.1" style="color:#FF8000;">47.96</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.4.2.5"><span class="ltx_text ltx_font_bold" id="A4.T5.5.4.2.5.1" style="color:#FF8000;">32.53</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.5.3">
<td class="ltx_td ltx_align_left" id="A4.T5.5.5.3.1">up-level1-repeat0-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.5.3.2">46.74</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.5.3.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T5.5.5.3.3.1" style="color:#036FC1;">31.14</span></td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.5.3.4">46.49</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.5.3.5">30.71</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.6.4">
<td class="ltx_td ltx_align_left" id="A4.T5.5.6.4.1">up-level1-repeat0-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.6.4.2">44.78</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.6.4.3">30.56</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.6.4.4">44.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.6.4.5">28.58</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.7.5">
<td class="ltx_td ltx_align_left" id="A4.T5.5.7.5.1">up-level1-repeat0-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.7.5.2">42.20</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.7.5.3">27.32</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.7.5.4">40.90</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.7.5.5">27.13</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.8.6">
<td class="ltx_td ltx_align_left" id="A4.T5.5.8.6.1">up-level1-repeat0-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.8.6.2">46.93</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.8.6.3">29.35</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.8.6.4">46.08</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.8.6.5">30.15</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.9.7">
<td class="ltx_td ltx_align_left" id="A4.T5.5.9.7.1">up-level1-repeat1-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.9.7.2">41.36</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.9.7.3">26.81</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.9.7.4">41.51</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.9.7.5">28.09</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.10.8">
<td class="ltx_td ltx_align_left" id="A4.T5.5.10.8.1">up-level1-repeat1-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.10.8.2">39.25</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.10.8.3">27.02</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.10.8.4">39.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.10.8.5">27.72</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.11.9">
<td class="ltx_td ltx_align_left" id="A4.T5.5.11.9.1">up-level1-repeat1-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.11.9.2">38.64</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.11.9.3">25.68</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.11.9.4">39.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.11.9.5">27.60</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.12.10">
<td class="ltx_td ltx_align_left" id="A4.T5.5.12.10.1">up-level1-repeat1-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.12.10.2">36.76</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.12.10.3">24.57</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.12.10.4">37.20</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.12.10.5">25.82</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.13.11">
<td class="ltx_td ltx_align_left" id="A4.T5.5.13.11.1">up-level1-repeat1-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.13.11.2">34.74</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.13.11.3">23.20</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.13.11.4">34.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.13.11.5">24.32</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.14.12">
<td class="ltx_td ltx_align_left" id="A4.T5.5.14.12.1">up-level1-repeat1-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.14.12.2">39.16</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.14.12.3">25.48</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.14.12.4">39.87</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.14.12.5">26.82</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.15.13">
<td class="ltx_td ltx_align_left" id="A4.T5.5.15.13.1">up-level1-repeat2-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.15.13.2">33.58</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.15.13.3">23.99</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.15.13.4">36.30</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.15.13.5">25.26</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.16.14">
<td class="ltx_td ltx_align_left" id="A4.T5.5.16.14.1">up-level1-repeat2-vit-block0-self-k</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.16.14.2">31.63</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.16.14.3">22.98</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.16.14.4">33.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.16.14.5">23.88</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.17.15">
<td class="ltx_td ltx_align_left" id="A4.T5.5.17.15.1">up-level1-repeat2-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.17.15.2">32.16</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.17.15.3">24.97</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.17.15.4">34.30</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.17.15.5">24.83</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.18.16">
<td class="ltx_td ltx_align_left" id="A4.T5.5.18.16.1">up-level1-repeat2-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.18.16.2">31.31</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.18.16.3">23.51</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.18.16.4">34.81</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.18.16.5">25.27</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.19.17">
<td class="ltx_td ltx_align_left" id="A4.T5.5.19.17.1">up-level1-repeat2-vit-block1-self-k</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.19.17.2">30.39</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.19.17.3">22.99</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.19.17.4">33.01</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.19.17.5">25.44</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.20.18">
<td class="ltx_td ltx_align_left" id="A4.T5.5.20.18.1">up-level1-repeat2-vit-block1-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.20.18.2">27.65</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.20.18.3">22.61</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.20.18.4">32.40</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.20.18.5">24.01</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.21.19">
<td class="ltx_td ltx_align_left" id="A4.T5.5.21.19.1">up-level1-repeat2-vit-block1-out</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.21.19.2">27.66</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.21.19.3">22.35</td>
<td class="ltx_td ltx_align_center" id="A4.T5.5.21.19.4">31.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T5.5.21.19.5">24.24</td>
</tr>
<tr class="ltx_tr" id="A4.T5.5.22.20">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T5.5.22.20.1">up-level1-repeat2-vit-out</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T5.5.22.20.2">27.61</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T5.5.22.20.3">19.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T5.5.22.20.4">26.29</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A4.T5.5.22.20.5">18.61</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Quantitative comparison results of SDv1.5.
Activation ID indicates the location of each activation in the diffusion U-Net.
The best results are in <span class="ltx_text ltx_font_bold" id="A4.T6.3.1" style="color:#FF8000;">bold</span> font and the runner-up is <span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.4.2" style="color:#036FC1;">underlined</span>, both marked per resolution.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.T6.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T6.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A4.T6.5.1.1.1" rowspan="2"><span class="ltx_text" id="A4.T6.5.1.1.1.1">Activation ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A4.T6.5.1.1.2">SDv1.5</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.2.2">
<td class="ltx_td ltx_align_center" id="A4.T6.5.2.2.1">Simple</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.2.2.2">Complex</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T6.5.3.3.1">up-level1-repeat0-res-out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T6.5.3.3.2">47.16</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T6.5.3.3.3">37.86</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.4.4">
<td class="ltx_td ltx_align_left" id="A4.T6.5.4.4.1">up-level1-repeat0-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.4.4.2">47.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.4.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.4.4.3.1" style="color:#036FC1;">41.13</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.5.5">
<td class="ltx_td ltx_align_left" id="A4.T6.5.5.5.1">up-level1-repeat0-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.5.5.2">48.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.5.5.3">39.75</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.6.6">
<td class="ltx_td ltx_align_left" id="A4.T6.5.6.6.1">up-level1-repeat0-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.6.6.2">48.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.6.6.3">39.91</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.7.7">
<td class="ltx_td ltx_align_left" id="A4.T6.5.7.7.1">up-level1-repeat1-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.7.7.2">49.71</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.7.7.3">40.80</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.8.8">
<td class="ltx_td ltx_align_left" id="A4.T6.5.8.8.1">up-level1-repeat1-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.8.8.2">49.04</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.8.8.3"><span class="ltx_text ltx_font_bold" id="A4.T6.5.8.8.3.1" style="color:#FF8000;">42.80</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.9.9">
<td class="ltx_td ltx_align_left" id="A4.T6.5.9.9.1">up-level1-repeat1-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.9.9.2">49.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.9.9.3">39.43</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.10.10">
<td class="ltx_td ltx_align_left" id="A4.T6.5.10.10.1">up-level1-repeat1-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.10.10.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.10.10.2.1" style="color:#036FC1;">50.23</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.10.10.3">40.98</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.11.11">
<td class="ltx_td ltx_align_left" id="A4.T6.5.11.11.1">up-level1-repeat2-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.11.11.2"><span class="ltx_text ltx_font_bold" id="A4.T6.5.11.11.2.1" style="color:#FF8000;">50.61</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.11.11.3">39.47</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.12.12">
<td class="ltx_td ltx_align_left" id="A4.T6.5.12.12.1">up-level1-repeat2-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.12.12.2">50.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.12.12.3">38.80</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.13.13">
<td class="ltx_td ltx_align_left" id="A4.T6.5.13.13.1">up-level1-repeat2-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.13.13.2">49.09</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.13.13.3">37.44</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.14.14">
<td class="ltx_td ltx_align_left" id="A4.T6.5.14.14.1">up-level1-repeat2-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.14.14.2">49.62</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.14.14.3">36.69</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.15.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T6.5.15.15.1">up-level2-repeat0-res-out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T6.5.15.15.2">52.43</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T6.5.15.15.3">37.98</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.16.16">
<td class="ltx_td ltx_align_left" id="A4.T6.5.16.16.1">up-level2-repeat0-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.16.16.2">52.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.16.16.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.16.16.3.1" style="color:#036FC1;">39.63</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.17.17">
<td class="ltx_td ltx_align_left" id="A4.T6.5.17.17.1">up-level2-repeat0-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.17.17.2">51.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.17.17.3">36.62</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.18.18">
<td class="ltx_td ltx_align_left" id="A4.T6.5.18.18.1">up-level2-repeat0-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.18.18.2">52.29</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.18.18.3">38.78</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.19.19">
<td class="ltx_td ltx_align_left" id="A4.T6.5.19.19.1">up-level2-repeat1-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.19.19.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.19.19.2.1" style="color:#036FC1;">53.30</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.19.19.3">36.65</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.20.20">
<td class="ltx_td ltx_align_left" id="A4.T6.5.20.20.1">up-level2-repeat1-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.20.20.2"><span class="ltx_text ltx_font_bold" id="A4.T6.5.20.20.2.1" style="color:#FF8000;">53.58</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.20.20.3"><span class="ltx_text ltx_font_bold" id="A4.T6.5.20.20.3.1" style="color:#FF8000;">39.82</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.21.21">
<td class="ltx_td ltx_align_left" id="A4.T6.5.21.21.1">up-level2-repeat1-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.21.21.2">50.70</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.21.21.3">35.96</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.22.22">
<td class="ltx_td ltx_align_left" id="A4.T6.5.22.22.1">up-level2-repeat1-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.22.22.2">50.50</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.22.22.3">35.59</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.23.23">
<td class="ltx_td ltx_align_left" id="A4.T6.5.23.23.1">up-level2-repeat2-res-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.23.23.2">48.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.23.23.3">33.70</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.24.24">
<td class="ltx_td ltx_align_left" id="A4.T6.5.24.24.1">up-level2-repeat2-vit-block0-self-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.24.24.2">45.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.24.24.3">32.45</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.25.25">
<td class="ltx_td ltx_align_left" id="A4.T6.5.25.25.1">up-level2-repeat2-vit-block0-self-k</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.25.25.2">45.69</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.25.25.3">32.71</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.26.26">
<td class="ltx_td ltx_align_left" id="A4.T6.5.26.26.1">up-level2-repeat2-vit-block0-cross-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.26.26.2">46.24</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.26.26.3">32.95</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.27.27">
<td class="ltx_td ltx_align_left" id="A4.T6.5.27.27.1">up-level2-repeat2-vit-block0-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.27.27.2">45.33</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.27.27.3">32.23</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.28.28">
<td class="ltx_td ltx_align_left" id="A4.T6.5.28.28.1">up-level2-repeat2-vit-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.28.28.2">45.04</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.28.28.3">28.87</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.29.29">
<td class="ltx_td ltx_align_left" id="A4.T6.5.29.29.1">up-level2-upsampler-out</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.29.29.2">45.07</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.29.29.3">29.57</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.30.30">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T6.5.30.30.1">up-level3-repeat0-vit-block0-self-q</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T6.5.30.30.2"><span class="ltx_text ltx_font_bold" id="A4.T6.5.30.30.2.1" style="color:#FF8000;">38.81</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A4.T6.5.30.30.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.30.30.3.1" style="color:#036FC1;">25.72</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.31.31">
<td class="ltx_td ltx_align_left" id="A4.T6.5.31.31.1">up-level3-repeat0-vit-block0-self-k</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.31.31.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="A4.T6.5.31.31.2.1" style="color:#036FC1;">37.78</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.31.31.3"><span class="ltx_text ltx_font_bold" id="A4.T6.5.31.31.3.1" style="color:#FF8000;">26.04</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.32.32">
<td class="ltx_td ltx_align_left" id="A4.T6.5.32.32.1">up-level3-repeat1-vit-block0-self-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.32.32.2">31.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.32.32.3">24.02</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.33.33">
<td class="ltx_td ltx_align_left" id="A4.T6.5.33.33.1">up-level3-repeat1-vit-block0-self-k</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.33.33.2">32.07</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.33.33.3">24.00</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.34.34">
<td class="ltx_td ltx_align_left" id="A4.T6.5.34.34.1">up-level3-repeat2-vit-block0-self-q</td>
<td class="ltx_td ltx_align_center" id="A4.T6.5.34.34.2">29.24</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T6.5.34.34.3">21.27</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.35.35">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T6.5.35.35.1">up-level3-repeat2-vit-block0-self-k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T6.5.35.35.2">29.19</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A4.T6.5.35.35.3">21.39</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">In this part, we present all the quantitative comparison results obtained following the protocol described in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S5" title="5 Enhanced Feature Selection from Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.
These results are from a dataset of simple scenes (Horse-21¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>) and a dataset of complex scenes (Bedroom-28¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>).
Since SDXL and Playground v2 share the same U-Net architecture, their results are shown in the same tables.
We display the results from the lowest resolution in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.T4" title="In D.2 Quantitative Comparison Results ‚Ä£ Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> and the results from the middle resolution in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.T5" title="In D.2 Quantitative Comparison Results ‚Ä£ Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>.
We have also done a quantitative comparison on SDv1.5, and the results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A4.T6" title="In D.2 Quantitative Comparison Results ‚Ä£ Appendix D Additional Experimental Results ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Other Properties of Diffusion U-Nets</h2>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>High-Frequency Noises in Increment Activations</h3>
<div class="ltx_para" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">Typically, high-frequency signals are usually noisy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib34" title="">34</a>]</cite>.
This applies to the diffusion U-Net, and we can further examine what activations are more vulnerable to such noises.
To be specific, the diffusion U-Net contains many residual connection structures, and their increment activations are high-frequency signals prone to noises.
Such increment activations include:</p>
<ol class="ltx_enumerate" id="A5.I1">
<li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A5.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.p1.1">The increment branch of ResModule. These activations are moderately noisy and thus usually less effective than inter-module activations.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="A5.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i2.p1.1">The feed-forward layer activations within ViTs. These activations are also moderately noisy.
However, this results from their significantly more channels compared to other activations, which can reduce their noise magnitude.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="A5.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i3.p1.1">The self-attention value activations within ViTs.
These activations are severely noisy and suffer significant degradation as they are the nested inner increments within embedded ViTs.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A5.SS1.p1.2">These activations are visualized in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.F16" title="In E.1 High-Frequency Noises in Increment Activations ‚Ä£ Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">16</span></a> for a clearer illustration.
Additionally, the residual activations within ViTs are at the same time also increments to the main inter-module residual.
However, these activations are not obviously affected by high-frequency noise, possibly due to their dual role as ViT residuals.</p>
</div>
<figure class="ltx_figure" id="A5.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="A5.F16.g1" src="extracted/5915702/pics/app-increment.jpg" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>This visualization compares the high-frequency noises in various activations, showing three types of increment activations with strong noises.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Detailed In-Resolution Granularity Change</h3>
<div class="ltx_para" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">We have previously described the existence of in-resolution granularity changes, and this section will further detail the pattern of one such change.
In one resolution of the up-stage, activations gradually shift from coarse to fine granularity, which aligns with intuition.
However, the diffusion U-Net tends to overly refine the inter-module activations, resulting in slight noises in the last few inter-module activations due to excessive detail.
This can be observed from the visualization of <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.F17" title="In E.2 Detailed In-Resolution Granularity Change ‚Ä£ Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">17</span></a>.
Consequently, a drop in discriminative performance is often seen near the end of one resolution.</p>
</div>
<figure class="ltx_figure" id="A5.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="108" id="A5.F17.g1" src="extracted/5915702/pics/app-granularity-detail.jpg" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>This visualization shows the granularity change across all inter-module activations of one resolution. At the end, the activations start to contain some slight noises, which is a sign of over-refinement.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span>Collaboration between Embedded ViTs</h3>
<div class="ltx_para" id="A5.SS3.p1">
<p class="ltx_p" id="A5.SS3.p1.1">Multiple ViTs exist within one resolution; for example, one resolution in the up-stage typically contains three ViTs.
These ViTs collaborate in refining the main inter-module residual.
During this process, each ViT exhibits an inner granularity change as it produces the increment activation, and the changes of collaborating ViTs form a certain pattern.
To be specific, the change of one ViT overlaps with the previous one to some extent but also shifts as a whole towards finer granularity, which is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A5.F18" title="In E.3 Collaboration between Embedded ViTs ‚Ä£ Appendix E Other Properties of Diffusion U-Nets ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">18</span></a>.</p>
</div>
<figure class="ltx_figure" id="A5.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="A5.F18.g1" src="extracted/5915702/pics/mech-multi-vit.jpg" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>This visualization shows that the collaborating ViTs within one resolution contain overlapped and shifting granularity changes.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Future Direction</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">It might be a good future direction to focus on more challenging discrimination scenarios.
Specifically, long tail and out-of-distribution are important problems in discrimination, which can greatly hinder the performance of models that work well under i.i.d. settings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib49" title="">49</a>]</cite>.
There have been a few attempts to address this more challenging problem with diffusion models.
For example, the disentanglement property of prompts might grant diffusion features cross-domain capability¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib16" title="">16</a>]</cite>.
It is also possible to utilize diffusion models to synthesize training samples to adjust training data distribution¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib55" title="">55</a>]</cite>.
Given these attempts, more efforts are still required to be put in this direction.
Moreover, there is a recent study¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib17" title="">17</a>]</cite> that might boost long tail and out-of-distribution discrimination using diffusion models.
To be specific, AUC is an evaluation metric as well as a loss function that promotes good performance on both head and tail samples, which is an important tool for long tail and out-of-distribution studies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib48" title="">48</a>]</cite>.
Conventionally, AUC is applicable to image classification but not semantic segmentation and other pixel-level tasks, but the aforementioned study manages to adopt AUC for semantic segmentation.
Given this new tool, it is now made more viable to attempt to enhance diffusion feature on long tail and out-of-distribution problems.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Computation Resources</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">We use Nvidia(R) RTX 3090 and Nvidia(R) RTX 4090 GPUs for the experiments, all with 24GB VRAM.
Most of our experiments, except label-scarce segmentation, require no additional storage besides the necessary space for model checkpoints and datasets.
The label-scarce segmentation task first extracts features and stores them on the disk, and then loads them for the downstream task, which takes about 4GB.</p>
</div>
<div class="ltx_para" id="A7.p2">
<p class="ltx_p" id="A7.p2.1">The codes are designed to be able to run on a single GPU or less than 4 GPUs, while multiple experiments can run simultaneously if more GPUs are provided.
Each experiment on the semantic correspondence task takes about 5 hours.
Each experiment on the large-scale semantic segmentation task takes 2 to 3 days, depending on the dataset.
Each experiment on the label-scarce segmentation task takes about 1 hour, but we repeat on 5 random splits following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib2" title="">2</a>]</cite>, which increases the overall time.
All the experiments in sum can be done within two weeks.</p>
</div>
<div class="ltx_para" id="A7.p3">
<p class="ltx_p" id="A7.p3.1">Our quantitative comparison is based on the label-scarce segmentation task, each run taking about 40 minutes.
The time is shorter because the quantitative comparison evaluates each activation individually.
This comparison uses large storage, which is about 45GB per model per dataset.
With the help of qualitative filtering, we can finish the quantitative comparison of one model on one dataset within 2 days.</p>
</div>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Asset License</h2>
<div class="ltx_para" id="A8.p1">
<ul class="ltx_itemize" id="A8.I1">
<li class="ltx_item" id="A8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A8.I1.i1.p1">
<p class="ltx_p" id="A8.I1.i1.p1.1">SPair-71k: Available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cvlab.postech.ac.kr/research/SPair-71k/" title="">https://cvlab.postech.ac.kr/research/SPair-71k/</a>.</p>
</div>
</li>
<li class="ltx_item" id="A8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A8.I1.i2.p1">
<p class="ltx_p" id="A8.I1.i2.p1.1">Label-scarce segmentation datasets (sourced from LSUN): Available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/fyu/lsun" title="">https://github.com/fyu/lsun</a>.</p>
</div>
</li>
<li class="ltx_item" id="A8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A8.I1.i3.p1">
<p class="ltx_p" id="A8.I1.i3.p1.1">ADE20K: Custom (research-only, non-commercial), at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://groups.csail.mit.edu/vision/datasets/ADE20K/terms/" title="">https://groups.csail.mit.edu/vision/datasets/ADE20K/terms/</a>.</p>
</div>
</li>
<li class="ltx_item" id="A8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A8.I1.i4.p1">
<p class="ltx_p" id="A8.I1.i4.p1.1">CityScapes: Custom, at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cityscapes-dataset.com/license/" title="">https://www.cityscapes-dataset.com/license/</a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="Ax2">
<h2 class="ltx_title ltx_title_appendix">NeurIPS Paper Checklist</h2>
<div class="ltx_para" id="Ax2.p1">
<ol class="ltx_enumerate" id="Ax2.I2">
<li class="ltx_item" id="Ax2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ax2.I2.i1.p1">
<p class="ltx_p" id="Ax2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i1.p1.1.1">Claims</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix1.p1">
<p class="ltx_p" id="Ax2.I2.ix1.p1.1">Question: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contributions and scope?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix2.p1">
<p class="ltx_p" id="Ax2.I2.ix2.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix2.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix3.p1">
<p class="ltx_p" id="Ax2.I2.ix3.p1.1">Justification: We clearly state our contributions in the abstract and introduction.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix4.p1">
<p class="ltx_p" id="Ax2.I2.ix4.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix4.I1">
<li class="ltx_item" id="Ax2.I2.ix4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix4.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix4.I1.i1.p1.1">The answer NA means that the abstract and introduction do not include the claims made in the paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix4.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix4.I1.i2.p1.1">The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix4.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix4.I1.i3.p1.1">The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix4.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix4.I1.i4.p1.1">It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ax2.I2.i2.p1">
<p class="ltx_p" id="Ax2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i2.p1.1.1">Limitations</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix5.p1">
<p class="ltx_p" id="Ax2.I2.ix5.p1.1">Question: Does the paper discuss the limitations of the work performed by the authors?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix6.p1">
<p class="ltx_p" id="Ax2.I2.ix6.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix6.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix7.p1">
<p class="ltx_p" id="Ax2.I2.ix7.p1.1">Justification: We discuss the limitations in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#S7" title="7 Conclusion and Future Work ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>, which are mainly related to DiT models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#bib.bib35" title="">35</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix8.p1">
<p class="ltx_p" id="Ax2.I2.ix8.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix8.I1">
<li class="ltx_item" id="Ax2.I2.ix8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i1.p1.1">The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i2.p1.1">The authors are encouraged to create a separate "Limitations" section in their paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i3.p1.1">The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i4.p1.1">The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i5.p1.1">The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i6.p1.1">The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i7.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i7.p1.1">If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix8.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix8.I1.i8.p1">
<p class="ltx_p" id="Ax2.I2.ix8.I1.i8.p1.1">While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ax2.I2.i3.p1">
<p class="ltx_p" id="Ax2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i3.p1.1.1">Theory Assumptions and Proofs</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix9.p1">
<p class="ltx_p" id="Ax2.I2.ix9.p1.1">Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix10.p1">
<p class="ltx_p" id="Ax2.I2.ix10.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix10.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix11.p1">
<p class="ltx_p" id="Ax2.I2.ix11.p1.1">Justification: We do not make theoretical contributions.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix12.p1">
<p class="ltx_p" id="Ax2.I2.ix12.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix12.I1">
<li class="ltx_item" id="Ax2.I2.ix12.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i1.p1.1">The answer NA means that the paper does not include theoretical results.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i2.p1.1">All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i3.p1.1">All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i4.p1.1">The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i5.p1.1">Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix12.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix12.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix12.I1.i6.p1.1">Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ax2.I2.i4.p1">
<p class="ltx_p" id="Ax2.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i4.p1.1.1">Experimental Result Reproducibility</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix13.p1">
<p class="ltx_p" id="Ax2.I2.ix13.p1.1">Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix14.p1">
<p class="ltx_p" id="Ax2.I2.ix14.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix14.p1.1.1" style="color:#0000FF;">[Yes] </span> </p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix15" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix15.p1">
<p class="ltx_p" id="Ax2.I2.ix15.p1.1">Justification: The most important information to reproduce our results is the exact activations we select as features, and it is detailed in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A2" title="Appendix B Details of Our Method ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix16.p1">
<p class="ltx_p" id="Ax2.I2.ix16.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix16.I1">
<li class="ltx_item" id="Ax2.I2.ix16.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i1.p1.1">The answer NA means that the paper does not include experiments.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i2.p1.1">If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i3.p1.1">If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i4.p1.1">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i5.p1.1">While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example</p>
<ol class="ltx_enumerate" id="Ax2.I2.ix16.I1.i5.I1">
<li class="ltx_item" id="Ax2.I2.ix16.I1.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i5.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i5.I1.i1.p1.1">If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i5.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i5.I1.i2.p1.1">If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i5.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i5.I1.i3.p1.1">If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix16.I1.i5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para" id="Ax2.I2.ix16.I1.i5.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix16.I1.i5.I1.i4.p1.1">We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Ax2.I2.i5.p1">
<p class="ltx_p" id="Ax2.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i5.p1.1.1">Open access to data and code</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix17" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix17.p1">
<p class="ltx_p" id="Ax2.I2.ix17.p1.1">Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix18" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix18.p1">
<p class="ltx_p" id="Ax2.I2.ix18.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix18.p1.1.1" style="color:#FF8000;">[No] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix19" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix19.p1">
<p class="ltx_p" id="Ax2.I2.ix19.p1.1">Justification: We will release the codes after acceptance. Besides, we have provided the necessary information for reproducing the experiments in this manuscript.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix20.p1">
<p class="ltx_p" id="Ax2.I2.ix20.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix20.I1">
<li class="ltx_item" id="Ax2.I2.ix20.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i1.p1.1">The answer NA means that paper does not include experiments requiring code.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i2.p1.1">Please see the NeurIPS code and data submission guidelines (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nips.cc/public/guides/CodeSubmissionPolicy" title="">https://nips.cc/public/guides/CodeSubmissionPolicy</a>) for more details.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i3.p1.1">While we encourage the release of code and data, we understand that this might not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i4.p1.1">The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nips.cc/public/guides/CodeSubmissionPolicy" title="">https://nips.cc/public/guides/CodeSubmissionPolicy</a>) for more details.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i5.p1.1">The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i6.p1.1">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i7.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i7.p1.1">At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix20.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix20.I1.i8.p1">
<p class="ltx_p" id="Ax2.I2.ix20.I1.i8.p1.1">Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Ax2.I2.i6.p1">
<p class="ltx_p" id="Ax2.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i6.p1.1.1">Experimental Setting/Details</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix21" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix21.p1">
<p class="ltx_p" id="Ax2.I2.ix21.p1.1">Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix22" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix22.p1">
<p class="ltx_p" id="Ax2.I2.ix22.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix22.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix23" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix23.p1">
<p class="ltx_p" id="Ax2.I2.ix23.p1.1">Justification: We have included the experimental details in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A3" title="Appendix C Experimental Details ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix24" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix24.p1">
<p class="ltx_p" id="Ax2.I2.ix24.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix24.I1">
<li class="ltx_item" id="Ax2.I2.ix24.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix24.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix24.I1.i1.p1.1">The answer NA means that the paper does not include experiments.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix24.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix24.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix24.I1.i2.p1.1">The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix24.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix24.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix24.I1.i3.p1.1">The full details can be provided either with the code, in appendix, or as supplemental material.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Ax2.I2.i7.p1">
<p class="ltx_p" id="Ax2.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i7.p1.1.1">Experiment Statistical Significance</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix25" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix25.p1">
<p class="ltx_p" id="Ax2.I2.ix25.p1.1">Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix26" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix26.p1">
<p class="ltx_p" id="Ax2.I2.ix26.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix26.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix27" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix27.p1">
<p class="ltx_p" id="Ax2.I2.ix27.p1.1">Justification: We provide error bars for some experimental results. Others are omitted as it can too computationally expensive, such as about two days per run.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix28.p1">
<p class="ltx_p" id="Ax2.I2.ix28.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix28.I1">
<li class="ltx_item" id="Ax2.I2.ix28.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i1.p1.1">The answer NA means that the paper does not include experiments.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i2.p1.1">The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i3.p1.1">The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i4.p1.1">The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i5.p1.1">The assumptions made should be given (e.g., Normally distributed errors).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i6.p1.1">It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i7.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i7.p1.1">It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i8.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i8.p1.1">For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix28.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix28.I1.i9.p1">
<p class="ltx_p" id="Ax2.I2.ix28.I1.i9.p1.1">If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="Ax2.I2.i8.p1">
<p class="ltx_p" id="Ax2.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i8.p1.1.1">Experiments Compute Resources</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix29" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix29.p1">
<p class="ltx_p" id="Ax2.I2.ix29.p1.1">Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix30" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix30.p1">
<p class="ltx_p" id="Ax2.I2.ix30.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix30.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix31" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix31.p1">
<p class="ltx_p" id="Ax2.I2.ix31.p1.1">Justification: We provide information in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A7" title="Appendix G Computation Resources ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix32" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix32.p1">
<p class="ltx_p" id="Ax2.I2.ix32.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix32.I1">
<li class="ltx_item" id="Ax2.I2.ix32.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix32.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix32.I1.i1.p1.1">The answer NA means that the paper does not include experiments.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix32.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix32.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix32.I1.i2.p1.1">The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix32.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix32.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix32.I1.i3.p1.1">The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix32.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix32.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix32.I1.i4.p1.1">The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into the paper).</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="Ax2.I2.i9.p1">
<p class="ltx_p" id="Ax2.I2.i9.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i9.p1.1.1">Code Of Ethics</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix33" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix33.p1">
<p class="ltx_p" id="Ax2.I2.ix33.p1.1">Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://neurips.cc/public/EthicsGuidelines" title="">https://neurips.cc/public/EthicsGuidelines</a>?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix34" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix34.p1">
<p class="ltx_p" id="Ax2.I2.ix34.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix34.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix35" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix35.p1">
<p class="ltx_p" id="Ax2.I2.ix35.p1.1">Justification: We conform with the NeurIPS Code of Ethics.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix36" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix36.p1">
<p class="ltx_p" id="Ax2.I2.ix36.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix36.I1">
<li class="ltx_item" id="Ax2.I2.ix36.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix36.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix36.I1.i1.p1.1">The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix36.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix36.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix36.I1.i2.p1.1">If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix36.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix36.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix36.I1.i3.p1.1">The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="Ax2.I2.i10.p1">
<p class="ltx_p" id="Ax2.I2.i10.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i10.p1.1.1">Broader Impacts</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix37" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix37.p1">
<p class="ltx_p" id="Ax2.I2.ix37.p1.1">Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix38" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix38.p1">
<p class="ltx_p" id="Ax2.I2.ix38.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix38.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix39" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix39.p1">
<p class="ltx_p" id="Ax2.I2.ix39.p1.1">Justification: There is no socetal impact.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix40.p1">
<p class="ltx_p" id="Ax2.I2.ix40.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix40.I1">
<li class="ltx_item" id="Ax2.I2.ix40.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i1.p1.1">The answer NA means that there is no societal impact of the work performed.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i2.p1.1">If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i3.p1.1">Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i4.p1.1">The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i5.p1.1">The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix40.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix40.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix40.I1.i6.p1.1">If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span>
<div class="ltx_para" id="Ax2.I2.i11.p1">
<p class="ltx_p" id="Ax2.I2.i11.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i11.p1.1.1">Safeguards</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix41" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix41.p1">
<p class="ltx_p" id="Ax2.I2.ix41.p1.1">Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix42" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix42.p1">
<p class="ltx_p" id="Ax2.I2.ix42.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix42.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix43" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix43.p1">
<p class="ltx_p" id="Ax2.I2.ix43.p1.1">Justification: The paper poses no such risks.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix44" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix44.p1">
<p class="ltx_p" id="Ax2.I2.ix44.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix44.I1">
<li class="ltx_item" id="Ax2.I2.ix44.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix44.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix44.I1.i1.p1.1">The answer NA means that the paper poses no such risks.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix44.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix44.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix44.I1.i2.p1.1">Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix44.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix44.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix44.I1.i3.p1.1">Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix44.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix44.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix44.I1.i4.p1.1">We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span>
<div class="ltx_para" id="Ax2.I2.i12.p1">
<p class="ltx_p" id="Ax2.I2.i12.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i12.p1.1.1">Licenses for existing assets</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix45" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix45.p1">
<p class="ltx_p" id="Ax2.I2.ix45.p1.1">Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix46" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix46.p1">
<p class="ltx_p" id="Ax2.I2.ix46.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix46.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix47" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix47.p1">
<p class="ltx_p" id="Ax2.I2.ix47.p1.1">Justification: We cite the original papers where related assets are first mentioned and list their license in <a class="ltx_ref" href="https://arxiv.org/html/2410.03558v2#A8" title="Appendix H Asset License ‚Ä£ Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix48.p1">
<p class="ltx_p" id="Ax2.I2.ix48.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix48.I1">
<li class="ltx_item" id="Ax2.I2.ix48.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i1.p1.1">The answer NA means that the paper does not use existing assets.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i2.p1.1">The authors should cite the original paper that produced the code package or dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i3.p1.1">The authors should state which version of the asset is used and, if possible, include a URL.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i4.p1.1">The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i5.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i5.p1.1">For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i6.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i6.p1.1">If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, <a class="ltx_ref ltx_url ltx_font_typewriter" href="paperswithcode.com/datasets" title="">paperswithcode.com/datasets</a> has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i7.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i7.p1.1">For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix48.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix48.I1.i8.p1">
<p class="ltx_p" id="Ax2.I2.ix48.I1.i8.p1.1">If this information is not available online, the authors are encouraged to reach out to the asset‚Äôs creators.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">13.</span>
<div class="ltx_para" id="Ax2.I2.i13.p1">
<p class="ltx_p" id="Ax2.I2.i13.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i13.p1.1.1">New Assets</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix49" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix49.p1">
<p class="ltx_p" id="Ax2.I2.ix49.p1.1">Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix50" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix50.p1">
<p class="ltx_p" id="Ax2.I2.ix50.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix50.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix51" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix51.p1">
<p class="ltx_p" id="Ax2.I2.ix51.p1.1">Justification: We do not introduce new assets.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix52" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix52.p1">
<p class="ltx_p" id="Ax2.I2.ix52.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix52.I1">
<li class="ltx_item" id="Ax2.I2.ix52.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix52.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix52.I1.i1.p1.1">The answer NA means that the paper does not release new assets.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix52.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix52.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix52.I1.i2.p1.1">Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix52.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix52.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix52.I1.i3.p1.1">The paper should discuss whether and how consent was obtained from people whose asset is used.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix52.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix52.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix52.I1.i4.p1.1">At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">14.</span>
<div class="ltx_para" id="Ax2.I2.i14.p1">
<p class="ltx_p" id="Ax2.I2.i14.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i14.p1.1.1">Crowdsourcing and Research with Human Subjects</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix53" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix53.p1">
<p class="ltx_p" id="Ax2.I2.ix53.p1.1">Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix54" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix54.p1">
<p class="ltx_p" id="Ax2.I2.ix54.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix54.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix55" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix55.p1">
<p class="ltx_p" id="Ax2.I2.ix55.p1.1">Justification: The paper does not involve crowdsourcing nor research with human subjects.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix56" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix56.p1">
<p class="ltx_p" id="Ax2.I2.ix56.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix56.I1">
<li class="ltx_item" id="Ax2.I2.ix56.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix56.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix56.I1.i1.p1.1">The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix56.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix56.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix56.I1.i2.p1.1">Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix56.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix56.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix56.I1.i3.p1.1">According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.i15" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">15.</span>
<div class="ltx_para" id="Ax2.I2.i15.p1">
<p class="ltx_p" id="Ax2.I2.i15.p1.1"><span class="ltx_text ltx_font_bold" id="Ax2.I2.i15.p1.1.1">Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix57" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix57.p1">
<p class="ltx_p" id="Ax2.I2.ix57.p1.1">Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix58" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix58.p1">
<p class="ltx_p" id="Ax2.I2.ix58.p1.1">Answer: <span class="ltx_text" id="Ax2.I2.ix58.p1.1.1" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix59" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix59.p1">
<p class="ltx_p" id="Ax2.I2.ix59.p1.1">Justification: The paper does not involve human subjects.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix60" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax2.I2.ix60.p1">
<p class="ltx_p" id="Ax2.I2.ix60.p1.1">Guidelines:</p>
<ul class="ltx_itemize" id="Ax2.I2.ix60.I1">
<li class="ltx_item" id="Ax2.I2.ix60.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix60.I1.i1.p1">
<p class="ltx_p" id="Ax2.I2.ix60.I1.i1.p1.1">The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix60.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix60.I1.i2.p1">
<p class="ltx_p" id="Ax2.I2.ix60.I1.i2.p1.1">Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix60.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix60.I1.i3.p1">
<p class="ltx_p" id="Ax2.I2.ix60.I1.i3.p1.1">We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I2.ix60.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="Ax2.I2.ix60.I1.i4.p1">
<p class="ltx_p" id="Ax2.I2.ix60.I1.i4.p1.1">For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 10:50:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
