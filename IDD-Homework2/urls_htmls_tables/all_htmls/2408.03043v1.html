<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.03043] Targeted Visual Prompting for Medical Visual Question Answering</title><meta property="og:description" content="With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Targeted Visual Prompting for Medical Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Targeted Visual Prompting for Medical Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.03043">

<!--Generated on Thu Sep  5 13:37:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="VQA Localized Questions Multimodal Large Language Model Vision Transformer">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Bern, Bern, Switzerland
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{sergio.tasconmorales, pablo.marquez, raphael.sznitman}@unibe.ch</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Targeted Visual Prompting for 
<br class="ltx_break">Medical Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sergio Tascon-Morales
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Pablo Márquez-Neila
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Raphael Sznitman
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to several baseline models. Our code and data are available at <a target="_blank" href="https://github.com/sergiotasconmorales/locvqallm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/sergiotasconmorales/locvqallm</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>VQA Localized Questions Multimodal Large Language Model Vision Transformer
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is centered on developing models capable of answering questions about specific images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This task is particularly challenging within the medical domain due to factors such as a scarcity of annotated data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, the wide variety of imaging modalities and anatomical regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, as well as the unique characteristics of medical images and terminology, all of which necessitate specialized expertise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Furthermore, approaches that leverage the detection of natural objects, which have significantly improved performance in the analysis of natural images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, are less straightforward when applied to medical imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Historically, models for Medical VQA (Med-VQA) treated visual and textual information independently, later merging these features through various fusion techniques. This composite data would then be input into a classifier to determine the most probable answer. However, recent developments in transformer-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, including advancements in Large Language Models (LLMs), have led to a notable shift in VQA strategies. These advancements have paved the way for the adoption of multimodal LLMs (MLLMs) that integrate both visual and textual data more seamlessly, a trend that is emerging in both general <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and Med-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> applications.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the remarkable adoption of MLLMs, recent research has raised concerns about the quality of their visual capabilities (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This issue primarily arises from the pre-training process of the visual component, which typically relies on models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Surprisingly, MLLMs can perceive certain visually distinct images as similar, a phenomenon that human observers readily recognize as a visual error <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. These visual understanding failures were also observed in VQA models before the widespread adoption of MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.03043/assets/images/examples_gpt_4v.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of visual understanding failures using GPT-4V for the VQA task (Examples taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>).</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To detect such failures and enhance explainability in the visual component of Med-VQA, the work in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> proposes a novel approach using the formulation of <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">localized questions</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. These questions allow fine-grained probing of images by focusing on user-defined regions rather than the entire image and facilitate a <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">compositional evaluation</span>. To enable such localized questions, the region to query is encoded and directly integrated into the attention mechanism of the model. Other proposed strategies include providing the model with a restricted region of the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> or relying on the language component of the VQA model to interpret region coordinates directly included in the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Yet, due to their design focused on traditional architectures, these methods fail to benefit MLLMs in Med-VQA. Other traditional <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and MLLM-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> rely on object detectors, limiting their applicability to medical images.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To overcome these challenges and enable localized questions in MLLMs in Med-VQA, we introduce <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Targeted Visual Prompting</span>. By carefully designing a prompt that provides both global and local visual tokens relative to the region of interest defined by the user, our method allows the full advantage of the MLLM to enhance the performance of the VQA model. To validate the effectiveness of our method, we conduct exhaustive experiments across multiple datasets. Our results demonstrate clear performance benefits compared to previously proposed methods, all achieved without introducing additional parameters to the model.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.6" class="ltx_p">A VQA model with parameters <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\bm{\theta}</annotation></semantics></math> generates an answer <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S2.p1.2.m2.1a"><mover accent="true" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">a</mi><mo id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><ci id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1">^</ci><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\hat{a}</annotation></semantics></math> when given an input image <math id="S2.p1.3.m3.1" class="ltx_math_unparsed" alttext="\mathbf{x}\in{}^{H\times W\times C}" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1b"><mi id="S2.p1.3.m3.1.1">𝐱</mi><mo id="S2.p1.3.m3.1.2">∈</mo><msup id="S2.p1.3.m3.1.3"><mi id="S2.p1.3.m3.1.3a"></mi><mrow id="S2.p1.3.m3.1.3.1"><mi id="S2.p1.3.m3.1.3.1.2">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p1.3.m3.1.3.1.1">×</mo><mi id="S2.p1.3.m3.1.3.1.3">W</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p1.3.m3.1.3.1.1a">×</mo><mi id="S2.p1.3.m3.1.3.1.4">C</mi></mrow></msup></mrow><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{x}\in{}^{H\times W\times C}</annotation></semantics></math> and a related question represented as a sequence of words, <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\mathbf{q}</annotation></semantics></math>. In its most general form, this process can be described as a function <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\Psi_{\text{VQA}}" display="inline"><semantics id="S2.p1.5.m5.1a"><msub id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi mathvariant="normal" id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">Ψ</mi><mtext id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3a.cmml">VQA</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">Ψ</ci><ci id="S2.p1.5.m5.1.1.3a.cmml" xref="S2.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3">VQA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\Psi_{\text{VQA}}</annotation></semantics></math>, parameterized by <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\bm{\theta}</annotation></semantics></math>, that is applied on the image-question pair,</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="\hat{a}=\Psi_{\text{VQA}}(\mathbf{x},\mathbf{q};\bm{\theta})." display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mover accent="true" id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.cmml">a</mi><mo id="S2.E1.m1.4.4.1.1.2.1" xref="S2.E1.m1.4.4.1.1.2.1.cmml">^</mo></mover><mo id="S2.E1.m1.4.4.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><msub id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.E1.m1.4.4.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.3.2.2.cmml">Ψ</mi><mtext id="S2.E1.m1.4.4.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.3.2.3a.cmml">VQA</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S2.E1.m1.4.4.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.1" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">𝐱</mi><mo id="S2.E1.m1.4.4.1.1.3.3.2.2" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">𝐪</mi><mo id="S2.E1.m1.4.4.1.1.3.3.2.3" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml">;</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">𝜽</mi><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.4" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><eq id="S2.E1.m1.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1"></eq><apply id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"><ci id="S2.E1.m1.4.4.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.1">^</ci><ci id="S2.E1.m1.4.4.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2">𝑎</ci></apply><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><times id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2.2">Ψ</ci><ci id="S2.E1.m1.4.4.1.1.3.2.3a.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3"><mtext mathsize="70%" id="S2.E1.m1.4.4.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3">VQA</mtext></ci></apply><vector id="S2.E1.m1.4.4.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝐱</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝐪</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝜽</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\hat{a}=\Psi_{\text{VQA}}(\mathbf{x},\mathbf{q};\bm{\theta}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.8" class="ltx_p">In practice, this model’s output has traditionally been a distribution over a set of <math id="S2.p1.7.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p1.7.m1.1a"><mi id="S2.p1.7.m1.1.1" xref="S2.p1.7.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m1.1b"><ci id="S2.p1.7.m1.1.1.cmml" xref="S2.p1.7.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m1.1c">N</annotation></semantics></math> candidate answers <math id="S2.p1.8.m2.4" class="ltx_Math" alttext="\{a_{1},a_{2},...,a_{N}\}" display="inline"><semantics id="S2.p1.8.m2.4a"><mrow id="S2.p1.8.m2.4.4.3" xref="S2.p1.8.m2.4.4.4.cmml"><mo stretchy="false" id="S2.p1.8.m2.4.4.3.4" xref="S2.p1.8.m2.4.4.4.cmml">{</mo><msub id="S2.p1.8.m2.2.2.1.1" xref="S2.p1.8.m2.2.2.1.1.cmml"><mi id="S2.p1.8.m2.2.2.1.1.2" xref="S2.p1.8.m2.2.2.1.1.2.cmml">a</mi><mn id="S2.p1.8.m2.2.2.1.1.3" xref="S2.p1.8.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.p1.8.m2.4.4.3.5" xref="S2.p1.8.m2.4.4.4.cmml">,</mo><msub id="S2.p1.8.m2.3.3.2.2" xref="S2.p1.8.m2.3.3.2.2.cmml"><mi id="S2.p1.8.m2.3.3.2.2.2" xref="S2.p1.8.m2.3.3.2.2.2.cmml">a</mi><mn id="S2.p1.8.m2.3.3.2.2.3" xref="S2.p1.8.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.p1.8.m2.4.4.3.6" xref="S2.p1.8.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.p1.8.m2.1.1" xref="S2.p1.8.m2.1.1.cmml">…</mi><mo id="S2.p1.8.m2.4.4.3.7" xref="S2.p1.8.m2.4.4.4.cmml">,</mo><msub id="S2.p1.8.m2.4.4.3.3" xref="S2.p1.8.m2.4.4.3.3.cmml"><mi id="S2.p1.8.m2.4.4.3.3.2" xref="S2.p1.8.m2.4.4.3.3.2.cmml">a</mi><mi id="S2.p1.8.m2.4.4.3.3.3" xref="S2.p1.8.m2.4.4.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.p1.8.m2.4.4.3.8" xref="S2.p1.8.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m2.4b"><set id="S2.p1.8.m2.4.4.4.cmml" xref="S2.p1.8.m2.4.4.3"><apply id="S2.p1.8.m2.2.2.1.1.cmml" xref="S2.p1.8.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m2.2.2.1.1.1.cmml" xref="S2.p1.8.m2.2.2.1.1">subscript</csymbol><ci id="S2.p1.8.m2.2.2.1.1.2.cmml" xref="S2.p1.8.m2.2.2.1.1.2">𝑎</ci><cn type="integer" id="S2.p1.8.m2.2.2.1.1.3.cmml" xref="S2.p1.8.m2.2.2.1.1.3">1</cn></apply><apply id="S2.p1.8.m2.3.3.2.2.cmml" xref="S2.p1.8.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.p1.8.m2.3.3.2.2.1.cmml" xref="S2.p1.8.m2.3.3.2.2">subscript</csymbol><ci id="S2.p1.8.m2.3.3.2.2.2.cmml" xref="S2.p1.8.m2.3.3.2.2.2">𝑎</ci><cn type="integer" id="S2.p1.8.m2.3.3.2.2.3.cmml" xref="S2.p1.8.m2.3.3.2.2.3">2</cn></apply><ci id="S2.p1.8.m2.1.1.cmml" xref="S2.p1.8.m2.1.1">…</ci><apply id="S2.p1.8.m2.4.4.3.3.cmml" xref="S2.p1.8.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.p1.8.m2.4.4.3.3.1.cmml" xref="S2.p1.8.m2.4.4.3.3">subscript</csymbol><ci id="S2.p1.8.m2.4.4.3.3.2.cmml" xref="S2.p1.8.m2.4.4.3.3.2">𝑎</ci><ci id="S2.p1.8.m2.4.4.3.3.3.cmml" xref="S2.p1.8.m2.4.4.3.3.3">𝑁</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m2.4c">\{a_{1},a_{2},...,a_{N}\}</annotation></semantics></math> set beforehand.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.4" class="ltx_p">In this work, however, we choose the answer of the VQA to be generated by an LLM in an auto-regressive manner until the end-of-sentence (EOS) token is produced. To make the LLM multimodal, we adopt the widely used approach of projecting visual embeddings onto the input space of the LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and
express this as,</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="\hat{a}=\Psi_{\text{LLM}}(\Psi_{\text{Vis}}(\mathbf{x},\bm{\theta}_{\text{Vis}})\mathbf{W}^{\text{proj}},\mathbf{q};\bm{\theta}_{\text{LLM}})," display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mover accent="true" id="S2.E2.m1.3.3.1.1.4" xref="S2.E2.m1.3.3.1.1.4.cmml"><mi id="S2.E2.m1.3.3.1.1.4.2" xref="S2.E2.m1.3.3.1.1.4.2.cmml">a</mi><mo id="S2.E2.m1.3.3.1.1.4.1" xref="S2.E2.m1.3.3.1.1.4.1.cmml">^</mo></mover><mo id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml"><msub id="S2.E2.m1.3.3.1.1.2.4" xref="S2.E2.m1.3.3.1.1.2.4.cmml"><mi mathvariant="normal" id="S2.E2.m1.3.3.1.1.2.4.2" xref="S2.E2.m1.3.3.1.1.2.4.2.cmml">Ψ</mi><mtext id="S2.E2.m1.3.3.1.1.2.4.3" xref="S2.E2.m1.3.3.1.1.2.4.3a.cmml">LLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2.3" xref="S2.E2.m1.3.3.1.1.2.3.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S2.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml">Ψ</mi><mtext id="S2.E2.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.3a.cmml">Vis</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">𝐱</mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">𝜽</mi><mtext id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3a.cmml">Vis</mtext></msub><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.1.1.2a" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><msup id="S2.E2.m1.3.3.1.1.1.1.1.1.4" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.cmml">𝐖</mi><mtext id="S2.E2.m1.3.3.1.1.1.1.1.1.4.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.3a.cmml">proj</mtext></msup></mrow><mo id="S2.E2.m1.3.3.1.1.2.2.2.4" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">𝐪</mi><mo id="S2.E2.m1.3.3.1.1.2.2.2.5" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">;</mo><msub id="S2.E2.m1.3.3.1.1.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2.cmml">𝜽</mi><mtext id="S2.E2.m1.3.3.1.1.2.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.2.2.3a.cmml">LLM</mtext></msub><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.2.2.6" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1"><eq id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"></eq><apply id="S2.E2.m1.3.3.1.1.4.cmml" xref="S2.E2.m1.3.3.1.1.4"><ci id="S2.E2.m1.3.3.1.1.4.1.cmml" xref="S2.E2.m1.3.3.1.1.4.1">^</ci><ci id="S2.E2.m1.3.3.1.1.4.2.cmml" xref="S2.E2.m1.3.3.1.1.4.2">𝑎</ci></apply><apply id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"><times id="S2.E2.m1.3.3.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.3"></times><apply id="S2.E2.m1.3.3.1.1.2.4.cmml" xref="S2.E2.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.4.1.cmml" xref="S2.E2.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.2.4.2.cmml" xref="S2.E2.m1.3.3.1.1.2.4.2">Ψ</ci><ci id="S2.E2.m1.3.3.1.1.2.4.3a.cmml" xref="S2.E2.m1.3.3.1.1.2.4.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.1.1.2.4.3.cmml" xref="S2.E2.m1.3.3.1.1.2.4.3">LLM</mtext></ci></apply><vector id="S2.E2.m1.3.3.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2"><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.2">Ψ</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.3.3a.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.3">Vis</mtext></ci></apply><interval closure="open" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝐱</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝜽</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3">Vis</mtext></ci></apply></interval><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4">superscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2">𝐖</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.4.3a.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.1.1.1.1.1.1.4.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.3">proj</mtext></ci></apply></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐪</ci><apply id="S2.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.2">𝜽</ci><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.3a.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2.3">LLM</mtext></ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\hat{a}=\Psi_{\text{LLM}}(\Psi_{\text{Vis}}(\mathbf{x},\bm{\theta}_{\text{Vis}})\mathbf{W}^{\text{proj}},\mathbf{q};\bm{\theta}_{\text{LLM}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.3" class="ltx_p">where <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\Psi_{\text{Vis}}" display="inline"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">Ψ</mi><mtext id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3a.cmml">Vis</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">Ψ</ci><ci id="S2.p2.1.m1.1.1.3a.cmml" xref="S2.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">Vis</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\Psi_{\text{Vis}}</annotation></semantics></math> refers to the visual encoder with parameters <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="\bm{\theta}_{\text{Vis}}" display="inline"><semantics id="S2.p2.2.m2.1a"><msub id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">𝜽</mi><mtext id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3a.cmml">Vis</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">𝜽</ci><ci id="S2.p2.2.m2.1.1.3a.cmml" xref="S2.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">Vis</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\bm{\theta}_{\text{Vis}}</annotation></semantics></math>, and <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{W}^{\text{proj}}" display="inline"><semantics id="S2.p2.3.m3.1a"><msup id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">𝐖</mi><mtext id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3a.cmml">proj</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">superscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝐖</ci><ci id="S2.p2.3.m3.1.1.3a.cmml" xref="S2.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">proj</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\mathbf{W}^{\text{proj}}</annotation></semantics></math> denotes the learnable parameters of the projection layer. Although not explicitly formalized, it is implied that the answer is generated in an auto-regressive fashion, meaning that the next word in the answer depends on the previously predicted words.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">To expand the model’s capability to handle localized questions, we propose here a dedicated targeted visual prompt that allows two perspectives of the image to be encoded: one containing only the region of the image and the other containing the region in context.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.7" class="ltx_p">The targeted visual prompt consists of five components: (1) comprises model instruction, denoted as <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{w}_{\text{instr}}" display="inline"><semantics id="S2.p4.1.m1.1a"><msub id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mi id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml">𝐰</mi><mtext id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3a.cmml">instr</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2">𝐰</ci><ci id="S2.p4.1.m1.1.1.3a.cmml" xref="S2.p4.1.m1.1.1.3"><mtext mathsize="70%" id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">instr</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">\mathbf{w}_{\text{instr}}</annotation></semantics></math>; (2) the visual context represented by the image with the region drawn on it, <math id="S2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{x}_{r}" display="inline"><semantics id="S2.p4.2.m2.1a"><msub id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml"><mi id="S2.p4.2.m2.1.1.2" xref="S2.p4.2.m2.1.1.2.cmml">𝐱</mi><mi id="S2.p4.2.m2.1.1.3" xref="S2.p4.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><apply id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p4.2.m2.1.1.1.cmml" xref="S2.p4.2.m2.1.1">subscript</csymbol><ci id="S2.p4.2.m2.1.1.2.cmml" xref="S2.p4.2.m2.1.1.2">𝐱</ci><ci id="S2.p4.2.m2.1.1.3.cmml" xref="S2.p4.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">\mathbf{x}_{r}</annotation></semantics></math>; (3) <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{w}_{\text{det}}" display="inline"><semantics id="S2.p4.3.m3.1a"><msub id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml"><mi id="S2.p4.3.m3.1.1.2" xref="S2.p4.3.m3.1.1.2.cmml">𝐰</mi><mtext id="S2.p4.3.m3.1.1.3" xref="S2.p4.3.m3.1.1.3a.cmml">det</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><apply id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p4.3.m3.1.1.1.cmml" xref="S2.p4.3.m3.1.1">subscript</csymbol><ci id="S2.p4.3.m3.1.1.2.cmml" xref="S2.p4.3.m3.1.1.2">𝐰</ci><ci id="S2.p4.3.m3.1.1.3a.cmml" xref="S2.p4.3.m3.1.1.3"><mtext mathsize="70%" id="S2.p4.3.m3.1.1.3.cmml" xref="S2.p4.3.m3.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">\mathbf{w}_{\text{det}}</annotation></semantics></math> contains a textual prefix for the region; (4) the cropped region <math id="S2.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{r}" display="inline"><semantics id="S2.p4.4.m4.1a"><mi id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml">𝐫</mi><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><ci id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1">𝐫</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">\mathbf{r}</annotation></semantics></math>; and (5) <math id="S2.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{w}_{q}" display="inline"><semantics id="S2.p4.5.m5.1a"><msub id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml"><mi id="S2.p4.5.m5.1.1.2" xref="S2.p4.5.m5.1.1.2.cmml">𝐰</mi><mi id="S2.p4.5.m5.1.1.3" xref="S2.p4.5.m5.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.1b"><apply id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p4.5.m5.1.1.1.cmml" xref="S2.p4.5.m5.1.1">subscript</csymbol><ci id="S2.p4.5.m5.1.1.2.cmml" xref="S2.p4.5.m5.1.1.2">𝐰</ci><ci id="S2.p4.5.m5.1.1.3.cmml" xref="S2.p4.5.m5.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.1c">\mathbf{w}_{q}</annotation></semantics></math> includes the question <math id="S2.p4.6.m6.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S2.p4.6.m6.1a"><mi id="S2.p4.6.m6.1.1" xref="S2.p4.6.m6.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S2.p4.6.m6.1b"><ci id="S2.p4.6.m6.1.1.cmml" xref="S2.p4.6.m6.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m6.1c">\mathbf{q}</annotation></semantics></math>. Text-containing parts of the prompt undergo tokenization and embedding, while the visual components are processed by a visual encoder and then projected into the input space of the LLM. Subsequently, the results are concatenated and processed by the LLM, resulting in the generation of an answer. To handle global questions, the entire image is assigned to <math id="S2.p4.7.m7.1" class="ltx_Math" alttext="\mathbf{r}" display="inline"><semantics id="S2.p4.7.m7.1a"><mi id="S2.p4.7.m7.1.1" xref="S2.p4.7.m7.1.1.cmml">𝐫</mi><annotation-xml encoding="MathML-Content" id="S2.p4.7.m7.1b"><ci id="S2.p4.7.m7.1.1.cmml" xref="S2.p4.7.m7.1.1">𝐫</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m7.1c">\mathbf{r}</annotation></semantics></math>. We illustrate our model in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Method ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and summarize the computation of the answer as,</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="\hat{a}=\Psi_{\text{LLM}}(\mathbf{w}_{\text{instr}},\Psi_{\text{Vis}}(\mathbf{x}_{r},\bm{\theta}_{\text{Vis}})\mathbf{W}_{\mathbf{x}_{r}}^{\text{proj}},\mathbf{w}_{\text{det}},\Psi_{\text{Vis}}(\mathbf{r},\bm{\theta}_{\text{Vis}})\mathbf{W}_{\mathbf{r}}^{\text{proj}},\mathbf{w}_{q};\bm{\theta}_{\text{LLM}})." display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.1.cmml"><mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml"><mover accent="true" id="S2.E3.m1.2.2.1.1.8" xref="S2.E3.m1.2.2.1.1.8.cmml"><mi id="S2.E3.m1.2.2.1.1.8.2" xref="S2.E3.m1.2.2.1.1.8.2.cmml">a</mi><mo id="S2.E3.m1.2.2.1.1.8.1" xref="S2.E3.m1.2.2.1.1.8.1.cmml">^</mo></mover><mo id="S2.E3.m1.2.2.1.1.7" xref="S2.E3.m1.2.2.1.1.7.cmml">=</mo><mrow id="S2.E3.m1.2.2.1.1.6" xref="S2.E3.m1.2.2.1.1.6.cmml"><msub id="S2.E3.m1.2.2.1.1.6.8" xref="S2.E3.m1.2.2.1.1.6.8.cmml"><mi mathvariant="normal" id="S2.E3.m1.2.2.1.1.6.8.2" xref="S2.E3.m1.2.2.1.1.6.8.2.cmml">Ψ</mi><mtext id="S2.E3.m1.2.2.1.1.6.8.3" xref="S2.E3.m1.2.2.1.1.6.8.3a.cmml">LLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.6.7" xref="S2.E3.m1.2.2.1.1.6.7.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.6.6.6" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.6.6.6.7" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">(</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2.cmml">𝐰</mi><mtext id="S2.E3.m1.2.2.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3a.cmml">instr</mtext></msub><mo id="S2.E3.m1.2.2.1.1.6.6.6.8" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">,</mo><mrow id="S2.E3.m1.2.2.1.1.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.cmml"><msub id="S2.E3.m1.2.2.1.1.2.2.2.2.4" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.cmml"><mi mathvariant="normal" id="S2.E3.m1.2.2.1.1.2.2.2.2.4.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.2.cmml">Ψ</mi><mtext id="S2.E3.m1.2.2.1.1.2.2.2.2.4.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.3a.cmml">Vis</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.2.2.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.3.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml">(</mo><msub id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml">𝐱</mi><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml">r</mi></msub><mo id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.4" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml">𝜽</mi><mtext id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3a.cmml">Vis</mtext></msub><mo stretchy="false" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.5" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.2.2.2.2.3a" xref="S2.E3.m1.2.2.1.1.2.2.2.2.3.cmml">​</mo><msubsup id="S2.E3.m1.2.2.1.1.2.2.2.2.5" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.cmml"><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.2.cmml">𝐖</mi><msub id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.cmml"><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.2.cmml">𝐱</mi><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.3.cmml">r</mi></msub><mtext id="S2.E3.m1.2.2.1.1.2.2.2.2.5.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.3a.cmml">proj</mtext></msubsup></mrow><mo id="S2.E3.m1.2.2.1.1.6.6.6.9" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.3.3.3.3" xref="S2.E3.m1.2.2.1.1.3.3.3.3.cmml"><mi id="S2.E3.m1.2.2.1.1.3.3.3.3.2" xref="S2.E3.m1.2.2.1.1.3.3.3.3.2.cmml">𝐰</mi><mtext id="S2.E3.m1.2.2.1.1.3.3.3.3.3" xref="S2.E3.m1.2.2.1.1.3.3.3.3.3a.cmml">det</mtext></msub><mo id="S2.E3.m1.2.2.1.1.6.6.6.10" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">,</mo><mrow id="S2.E3.m1.2.2.1.1.4.4.4.4" xref="S2.E3.m1.2.2.1.1.4.4.4.4.cmml"><msub id="S2.E3.m1.2.2.1.1.4.4.4.4.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.cmml"><mi mathvariant="normal" id="S2.E3.m1.2.2.1.1.4.4.4.4.3.2" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.2.cmml">Ψ</mi><mtext id="S2.E3.m1.2.2.1.1.4.4.4.4.3.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.3a.cmml">Vis</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.4.4.4.4.2" xref="S2.E3.m1.2.2.1.1.4.4.4.4.2.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.2" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.2.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">𝐫</mi><mo id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.2.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.2" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.2.cmml">𝜽</mi><mtext id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3a.cmml">Vis</mtext></msub><mo stretchy="false" id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.4" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.4.4.4.4.2a" xref="S2.E3.m1.2.2.1.1.4.4.4.4.2.cmml">​</mo><msubsup id="S2.E3.m1.2.2.1.1.4.4.4.4.4" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.cmml"><mi id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.2" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.2.cmml">𝐖</mi><mi id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.3.cmml">𝐫</mi><mtext id="S2.E3.m1.2.2.1.1.4.4.4.4.4.3" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.3a.cmml">proj</mtext></msubsup></mrow><mo id="S2.E3.m1.2.2.1.1.6.6.6.11" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.5.5.5.5" xref="S2.E3.m1.2.2.1.1.5.5.5.5.cmml"><mi id="S2.E3.m1.2.2.1.1.5.5.5.5.2" xref="S2.E3.m1.2.2.1.1.5.5.5.5.2.cmml">𝐰</mi><mi id="S2.E3.m1.2.2.1.1.5.5.5.5.3" xref="S2.E3.m1.2.2.1.1.5.5.5.5.3.cmml">q</mi></msub><mo id="S2.E3.m1.2.2.1.1.6.6.6.12" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">;</mo><msub id="S2.E3.m1.2.2.1.1.6.6.6.6" xref="S2.E3.m1.2.2.1.1.6.6.6.6.cmml"><mi id="S2.E3.m1.2.2.1.1.6.6.6.6.2" xref="S2.E3.m1.2.2.1.1.6.6.6.6.2.cmml">𝜽</mi><mtext id="S2.E3.m1.2.2.1.1.6.6.6.6.3" xref="S2.E3.m1.2.2.1.1.6.6.6.6.3a.cmml">LLM</mtext></msub><mo stretchy="false" id="S2.E3.m1.2.2.1.1.6.6.6.13" xref="S2.E3.m1.2.2.1.1.6.6.7.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E3.m1.2.2.1.2" xref="S2.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1"><eq id="S2.E3.m1.2.2.1.1.7.cmml" xref="S2.E3.m1.2.2.1.1.7"></eq><apply id="S2.E3.m1.2.2.1.1.8.cmml" xref="S2.E3.m1.2.2.1.1.8"><ci id="S2.E3.m1.2.2.1.1.8.1.cmml" xref="S2.E3.m1.2.2.1.1.8.1">^</ci><ci id="S2.E3.m1.2.2.1.1.8.2.cmml" xref="S2.E3.m1.2.2.1.1.8.2">𝑎</ci></apply><apply id="S2.E3.m1.2.2.1.1.6.cmml" xref="S2.E3.m1.2.2.1.1.6"><times id="S2.E3.m1.2.2.1.1.6.7.cmml" xref="S2.E3.m1.2.2.1.1.6.7"></times><apply id="S2.E3.m1.2.2.1.1.6.8.cmml" xref="S2.E3.m1.2.2.1.1.6.8"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.6.8.1.cmml" xref="S2.E3.m1.2.2.1.1.6.8">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.6.8.2.cmml" xref="S2.E3.m1.2.2.1.1.6.8.2">Ψ</ci><ci id="S2.E3.m1.2.2.1.1.6.8.3a.cmml" xref="S2.E3.m1.2.2.1.1.6.8.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.6.8.3.cmml" xref="S2.E3.m1.2.2.1.1.6.8.3">LLM</mtext></ci></apply><vector id="S2.E3.m1.2.2.1.1.6.6.7.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6"><apply id="S2.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2">𝐰</ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.3a.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3">instr</mtext></ci></apply><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2"><times id="S2.E3.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.3"></times><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.4.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.4.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.4.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.2">Ψ</ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.4.3a.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.2.2.2.2.4.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.4.3">Vis</mtext></ci></apply><interval closure="open" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2"><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.2">𝐱</ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.1.1.1.3">𝑟</ci></apply><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.2">𝜽</ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3a.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2.2.2.3">Vis</mtext></ci></apply></interval><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.5.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.5.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5">superscript</csymbol><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.2">𝐖</ci><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.2">𝐱</ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.2.3.3">𝑟</ci></apply></apply><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.5.3a.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.2.2.2.2.5.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.5.3">proj</mtext></ci></apply></apply><apply id="S2.E3.m1.2.2.1.1.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.3.3.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.3.3.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.2">𝐰</ci><ci id="S2.E3.m1.2.2.1.1.3.3.3.3.3a.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.3.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.3">det</mtext></ci></apply><apply id="S2.E3.m1.2.2.1.1.4.4.4.4.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4"><times id="S2.E3.m1.2.2.1.1.4.4.4.4.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.2"></times><apply id="S2.E3.m1.2.2.1.1.4.4.4.4.3.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.4.4.4.4.3.1.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.3.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.2">Ψ</ci><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.3.3a.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.4.4.4.4.3.3.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.3.3">Vis</mtext></ci></apply><interval closure="open" id="S2.E3.m1.2.2.1.1.4.4.4.4.1.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">𝐫</ci><apply id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.2">𝜽</ci><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3a.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.1.1.1.3">Vis</mtext></ci></apply></interval><apply id="S2.E3.m1.2.2.1.1.4.4.4.4.4.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.4.4.4.4.4.1.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4">superscript</csymbol><apply id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.1.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.2.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.2">𝐖</ci><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.3.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.2.3">𝐫</ci></apply><ci id="S2.E3.m1.2.2.1.1.4.4.4.4.4.3a.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.4.4.4.4.4.3.cmml" xref="S2.E3.m1.2.2.1.1.4.4.4.4.4.3">proj</mtext></ci></apply></apply><apply id="S2.E3.m1.2.2.1.1.5.5.5.5.cmml" xref="S2.E3.m1.2.2.1.1.5.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.5.5.5.5.1.cmml" xref="S2.E3.m1.2.2.1.1.5.5.5.5">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.5.5.5.5.2.cmml" xref="S2.E3.m1.2.2.1.1.5.5.5.5.2">𝐰</ci><ci id="S2.E3.m1.2.2.1.1.5.5.5.5.3.cmml" xref="S2.E3.m1.2.2.1.1.5.5.5.5.3">𝑞</ci></apply><apply id="S2.E3.m1.2.2.1.1.6.6.6.6.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6.6"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.6.6.6.6.1.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6.6">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.6.6.6.6.2.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6.6.2">𝜽</ci><ci id="S2.E3.m1.2.2.1.1.6.6.6.6.3a.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6.6.3"><mtext mathsize="70%" id="S2.E3.m1.2.2.1.1.6.6.6.6.3.cmml" xref="S2.E3.m1.2.2.1.1.6.6.6.6.3">LLM</mtext></ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\hat{a}=\Psi_{\text{LLM}}(\mathbf{w}_{\text{instr}},\Psi_{\text{Vis}}(\mathbf{x}_{r},\bm{\theta}_{\text{Vis}})\mathbf{W}_{\mathbf{x}_{r}}^{\text{proj}},\mathbf{w}_{\text{det}},\Psi_{\text{Vis}}(\mathbf{r},\bm{\theta}_{\text{Vis}})\mathbf{W}_{\mathbf{r}}^{\text{proj}},\mathbf{w}_{q};\bm{\theta}_{\text{LLM}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.2" class="ltx_p">To handle questions about the entire image, both <math id="S2.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{r}" display="inline"><semantics id="S2.p5.1.m1.1a"><msub id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml"><mi id="S2.p5.1.m1.1.1.2" xref="S2.p5.1.m1.1.1.2.cmml">𝐱</mi><mi id="S2.p5.1.m1.1.1.3" xref="S2.p5.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><apply id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.1.m1.1.1.1.cmml" xref="S2.p5.1.m1.1.1">subscript</csymbol><ci id="S2.p5.1.m1.1.1.2.cmml" xref="S2.p5.1.m1.1.1.2">𝐱</ci><ci id="S2.p5.1.m1.1.1.3.cmml" xref="S2.p5.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">\mathbf{x}_{r}</annotation></semantics></math> and <math id="S2.p5.2.m2.1" class="ltx_Math" alttext="\mathbf{r}" display="inline"><semantics id="S2.p5.2.m2.1a"><mi id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml">𝐫</mi><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><ci id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1">𝐫</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">\mathbf{r}</annotation></semantics></math> correspond to the original image.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.03043/assets/x1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our customized targeted visual prompt is created by providing the model with the region in context, as well as an isolated version of the region. Visual tokens are projected to the input space of the LLM and concatenated with the instruction tokens.</figcaption>
</figure>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Training. </span> As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, our model is trained using the original auto-regressive training loss of the LLM. The loss function is the standard negative log-likelihood accumulated over all time steps for predicting the correct next token. For a ground truth answer of length <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p6.1.m1.1a"><mi id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">T</annotation></semantics></math>, this loss is expressed as,</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.5" class="ltx_Math" alttext="\mathcal{L}(\bm{\theta})=-\sum_{t=1}^{T}\log p_{\theta}(a^{t}|\mathbf{x},\mathbf{w},a^{1:t-1};\bm{\theta})," display="block"><semantics id="S2.E4.m1.5a"><mrow id="S2.E4.m1.5.5.1" xref="S2.E4.m1.5.5.1.1.cmml"><mrow id="S2.E4.m1.5.5.1.1" xref="S2.E4.m1.5.5.1.1.cmml"><mrow id="S2.E4.m1.5.5.1.1.3" xref="S2.E4.m1.5.5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.5.5.1.1.3.2" xref="S2.E4.m1.5.5.1.1.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.1.3.1" xref="S2.E4.m1.5.5.1.1.3.1.cmml">​</mo><mrow id="S2.E4.m1.5.5.1.1.3.3.2" xref="S2.E4.m1.5.5.1.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.5.5.1.1.3.3.2.1" xref="S2.E4.m1.5.5.1.1.3.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">𝜽</mi><mo stretchy="false" id="S2.E4.m1.5.5.1.1.3.3.2.2" xref="S2.E4.m1.5.5.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.5.5.1.1.2" xref="S2.E4.m1.5.5.1.1.2.cmml">=</mo><mrow id="S2.E4.m1.5.5.1.1.1" xref="S2.E4.m1.5.5.1.1.1.cmml"><mo id="S2.E4.m1.5.5.1.1.1a" xref="S2.E4.m1.5.5.1.1.1.cmml">−</mo><mrow id="S2.E4.m1.5.5.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.cmml"><munderover id="S2.E4.m1.5.5.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E4.m1.5.5.1.1.1.1.2.2.2" xref="S2.E4.m1.5.5.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.2.2.3" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.2.2.3.2" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S2.E4.m1.5.5.1.1.1.1.2.2.3.1" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E4.m1.5.5.1.1.1.1.2.2.3.3" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.5.5.1.1.1.1.2.3" xref="S2.E4.m1.5.5.1.1.1.1.2.3.cmml">T</mi></munderover><mrow id="S2.E4.m1.5.5.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.5.5.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.3.1" xref="S2.E4.m1.5.5.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E4.m1.5.5.1.1.1.1.1.3a" xref="S2.E4.m1.5.5.1.1.1.1.1.3.cmml">⁡</mo><msub id="S2.E4.m1.5.5.1.1.1.1.1.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.3.2.2" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S2.E4.m1.5.5.1.1.1.1.1.3.2.3" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.cmml"><msup id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml">a</mi><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msup><mo fence="false" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">𝐱</mi><mo id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml">𝐰</mi><mo id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msup><mo id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml">𝜽</mi></mrow></mrow><mo stretchy="false" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E4.m1.5.5.1.2" xref="S2.E4.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.5b"><apply id="S2.E4.m1.5.5.1.1.cmml" xref="S2.E4.m1.5.5.1"><eq id="S2.E4.m1.5.5.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.2"></eq><apply id="S2.E4.m1.5.5.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.3"><times id="S2.E4.m1.5.5.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.3.1"></times><ci id="S2.E4.m1.5.5.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.3.2">ℒ</ci><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝜽</ci></apply><apply id="S2.E4.m1.5.5.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1"><minus id="S2.E4.m1.5.5.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1"></minus><apply id="S2.E4.m1.5.5.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1"><apply id="S2.E4.m1.5.5.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2">superscript</csymbol><apply id="S2.E4.m1.5.5.1.1.1.1.2.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2">subscript</csymbol><sum id="S2.E4.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2.2"></sum><apply id="S2.E4.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3"><eq id="S2.E4.m1.5.5.1.1.1.1.2.2.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.1"></eq><ci id="S2.E4.m1.5.5.1.1.1.1.2.2.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S2.E4.m1.5.5.1.1.1.1.2.2.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.5.5.1.1.1.1.2.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.3">𝑇</ci></apply><apply id="S2.E4.m1.5.5.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1"><times id="S2.E4.m1.5.5.1.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.2"></times><apply id="S2.E4.m1.5.5.1.1.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3"><log id="S2.E4.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3.1"></log><apply id="S2.E4.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2.2">𝑝</ci><ci id="S2.E4.m1.5.5.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.2">𝑎</ci><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><list id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1"><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝐱</ci><ci id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3">𝐰</ci><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.2">1</cn><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><ci id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4">𝜽</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.5c">\mathcal{L}(\bm{\theta})=-\sum_{t=1}^{T}\log p_{\theta}(a^{t}|\mathbf{x},\mathbf{w},a^{1:t-1};\bm{\theta}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.p6.4" class="ltx_p">where <math id="S2.p6.2.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S2.p6.2.m1.1a"><mi id="S2.p6.2.m1.1.1" xref="S2.p6.2.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S2.p6.2.m1.1b"><ci id="S2.p6.2.m1.1.1.cmml" xref="S2.p6.2.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m1.1c">\mathbf{x}</annotation></semantics></math> and <math id="S2.p6.3.m2.1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><semantics id="S2.p6.3.m2.1a"><mi id="S2.p6.3.m2.1.1" xref="S2.p6.3.m2.1.1.cmml">𝐰</mi><annotation-xml encoding="MathML-Content" id="S2.p6.3.m2.1b"><ci id="S2.p6.3.m2.1.1.cmml" xref="S2.p6.3.m2.1.1">𝐰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m2.1c">\mathbf{w}</annotation></semantics></math> denote the visual and textual elements, respectively, and <math id="S2.p6.4.m3.4" class="ltx_Math" alttext="\mathbf{a}=\{a_{1},a_{2},...,a_{T}\}" display="inline"><semantics id="S2.p6.4.m3.4a"><mrow id="S2.p6.4.m3.4.4" xref="S2.p6.4.m3.4.4.cmml"><mi id="S2.p6.4.m3.4.4.5" xref="S2.p6.4.m3.4.4.5.cmml">𝐚</mi><mo id="S2.p6.4.m3.4.4.4" xref="S2.p6.4.m3.4.4.4.cmml">=</mo><mrow id="S2.p6.4.m3.4.4.3.3" xref="S2.p6.4.m3.4.4.3.4.cmml"><mo stretchy="false" id="S2.p6.4.m3.4.4.3.3.4" xref="S2.p6.4.m3.4.4.3.4.cmml">{</mo><msub id="S2.p6.4.m3.2.2.1.1.1" xref="S2.p6.4.m3.2.2.1.1.1.cmml"><mi id="S2.p6.4.m3.2.2.1.1.1.2" xref="S2.p6.4.m3.2.2.1.1.1.2.cmml">a</mi><mn id="S2.p6.4.m3.2.2.1.1.1.3" xref="S2.p6.4.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p6.4.m3.4.4.3.3.5" xref="S2.p6.4.m3.4.4.3.4.cmml">,</mo><msub id="S2.p6.4.m3.3.3.2.2.2" xref="S2.p6.4.m3.3.3.2.2.2.cmml"><mi id="S2.p6.4.m3.3.3.2.2.2.2" xref="S2.p6.4.m3.3.3.2.2.2.2.cmml">a</mi><mn id="S2.p6.4.m3.3.3.2.2.2.3" xref="S2.p6.4.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p6.4.m3.4.4.3.3.6" xref="S2.p6.4.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.p6.4.m3.1.1" xref="S2.p6.4.m3.1.1.cmml">…</mi><mo id="S2.p6.4.m3.4.4.3.3.7" xref="S2.p6.4.m3.4.4.3.4.cmml">,</mo><msub id="S2.p6.4.m3.4.4.3.3.3" xref="S2.p6.4.m3.4.4.3.3.3.cmml"><mi id="S2.p6.4.m3.4.4.3.3.3.2" xref="S2.p6.4.m3.4.4.3.3.3.2.cmml">a</mi><mi id="S2.p6.4.m3.4.4.3.3.3.3" xref="S2.p6.4.m3.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S2.p6.4.m3.4.4.3.3.8" xref="S2.p6.4.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.4.m3.4b"><apply id="S2.p6.4.m3.4.4.cmml" xref="S2.p6.4.m3.4.4"><eq id="S2.p6.4.m3.4.4.4.cmml" xref="S2.p6.4.m3.4.4.4"></eq><ci id="S2.p6.4.m3.4.4.5.cmml" xref="S2.p6.4.m3.4.4.5">𝐚</ci><set id="S2.p6.4.m3.4.4.3.4.cmml" xref="S2.p6.4.m3.4.4.3.3"><apply id="S2.p6.4.m3.2.2.1.1.1.cmml" xref="S2.p6.4.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p6.4.m3.2.2.1.1.1.1.cmml" xref="S2.p6.4.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.p6.4.m3.2.2.1.1.1.2.cmml" xref="S2.p6.4.m3.2.2.1.1.1.2">𝑎</ci><cn type="integer" id="S2.p6.4.m3.2.2.1.1.1.3.cmml" xref="S2.p6.4.m3.2.2.1.1.1.3">1</cn></apply><apply id="S2.p6.4.m3.3.3.2.2.2.cmml" xref="S2.p6.4.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p6.4.m3.3.3.2.2.2.1.cmml" xref="S2.p6.4.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.p6.4.m3.3.3.2.2.2.2.cmml" xref="S2.p6.4.m3.3.3.2.2.2.2">𝑎</ci><cn type="integer" id="S2.p6.4.m3.3.3.2.2.2.3.cmml" xref="S2.p6.4.m3.3.3.2.2.2.3">2</cn></apply><ci id="S2.p6.4.m3.1.1.cmml" xref="S2.p6.4.m3.1.1">…</ci><apply id="S2.p6.4.m3.4.4.3.3.3.cmml" xref="S2.p6.4.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p6.4.m3.4.4.3.3.3.1.cmml" xref="S2.p6.4.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.p6.4.m3.4.4.3.3.3.2.cmml" xref="S2.p6.4.m3.4.4.3.3.3.2">𝑎</ci><ci id="S2.p6.4.m3.4.4.3.3.3.3.cmml" xref="S2.p6.4.m3.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.m3.4c">\mathbf{a}=\{a_{1},a_{2},...,a_{T}\}</annotation></semantics></math> is the ground truth answer.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold">Datasets: </span> To evaluate our method, we make use of several publically available datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>: (1) DME-VQA: contains questions on diabetic macular edema (DME) risk grade and about the presence of biomarkers in the entire image or specific regions. (2) RIS-VQA: contains images from the DaVinci robot during gastrointestinal surgery and questions related to surgical instruments. (3) INSEGCAT-VQA: contains frames from cataract surgery videos and questions about instruments used in this type of surgery. A summary of these is shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For all datasets, we use the same partitioning as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Modality</td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"># images</td>
<td id="S3.T1.1.1.6" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"># QA-pairs</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">DME-VQA</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Fundus</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">679</td>
<td id="S3.T1.1.2.6" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">13470</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left">RIS-VQA</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_left">Gastrointestinal</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_align_center">2978</td>
<td id="S3.T1.1.3.6" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.1.3.7" class="ltx_td ltx_align_center">32562</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">INSEGCAT-VQA</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_left ltx_border_bb">Cataract surgery</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb">4647</td>
<td id="S3.T1.1.4.6" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb">39008</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset parameters.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.6" class="ltx_p"><span id="S3.p2.6.7" class="ltx_text ltx_font_bold">Baselines:</span> We benchmark our method against multiple baselines, which are exemplified in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In <span id="S3.p2.6.8" class="ltx_text ltx_font_bold">No mask</span>, the model receives no information about the location of the region; in <span id="S3.p2.6.9" class="ltx_text ltx_font_bold">Region in text</span>, the region is specified in the question; in <span id="S3.p2.6.10" class="ltx_text ltx_font_bold">Draw region</span>, the region is marked on top of the image. In <span id="S3.p2.6.11" class="ltx_text ltx_font_bold">Context only</span>, the model only sees the context, but not the contents of the region; in <span id="S3.p2.6.12" class="ltx_text ltx_font_bold">Crop region</span>, the model receives no context; finally, in <span id="S3.p2.6.13" class="ltx_text ltx_font_bold">LocAtt</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the model has access to the image, as well as a binary image representing the region. For these baselines, the visual prompt given to the model is: <span id="S3.p2.6.6" class="ltx_text ltx_font_italic">“Answer the question below using the context below Context: <math id="S3.p2.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.p2.1.1.m1.1a"><mo id="S3.p2.1.1.m1.1.1" xref="S3.p2.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.1.m1.1b"><lt id="S3.p2.1.1.m1.1.1.cmml" xref="S3.p2.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.1.m1.1c">&lt;</annotation></semantics></math>Img<math id="S3.p2.2.2.m2.1" class="ltx_Math" alttext="&gt;&lt;" display="inline"><semantics id="S3.p2.2.2.m2.1a"><mrow id="S3.p2.2.2.m2.1.1" xref="S3.p2.2.2.m2.1.1.cmml"><mi id="S3.p2.2.2.m2.1.1.2" xref="S3.p2.2.2.m2.1.1.2.cmml"></mi><mo rspace="0em" id="S3.p2.2.2.m2.1.1.1" xref="S3.p2.2.2.m2.1.1.1.cmml">&gt;</mo><mo lspace="0em" id="S3.p2.2.2.m2.1.1.3" xref="S3.p2.2.2.m2.1.1.3.cmml">&lt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.2.m2.1b"><apply id="S3.p2.2.2.m2.1.1.cmml" xref="S3.p2.2.2.m2.1.1"><gt id="S3.p2.2.2.m2.1.1.1.cmml" xref="S3.p2.2.2.m2.1.1.1"></gt><csymbol cd="latexml" id="S3.p2.2.2.m2.1.1.2.cmml" xref="S3.p2.2.2.m2.1.1.2">absent</csymbol><lt id="S3.p2.2.2.m2.1.1.3.cmml" xref="S3.p2.2.2.m2.1.1.3"></lt></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.2.m2.1c">&gt;&lt;</annotation></semantics></math>Image<math id="S3.p2.3.3.m3.1" class="ltx_Math" alttext="&gt;&lt;" display="inline"><semantics id="S3.p2.3.3.m3.1a"><mrow id="S3.p2.3.3.m3.1.1" xref="S3.p2.3.3.m3.1.1.cmml"><mi id="S3.p2.3.3.m3.1.1.2" xref="S3.p2.3.3.m3.1.1.2.cmml"></mi><mo rspace="0em" id="S3.p2.3.3.m3.1.1.1" xref="S3.p2.3.3.m3.1.1.1.cmml">&gt;</mo><mo lspace="0em" id="S3.p2.3.3.m3.1.1.3" xref="S3.p2.3.3.m3.1.1.3.cmml">&lt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.3.m3.1b"><apply id="S3.p2.3.3.m3.1.1.cmml" xref="S3.p2.3.3.m3.1.1"><gt id="S3.p2.3.3.m3.1.1.1.cmml" xref="S3.p2.3.3.m3.1.1.1"></gt><csymbol cd="latexml" id="S3.p2.3.3.m3.1.1.2.cmml" xref="S3.p2.3.3.m3.1.1.2">absent</csymbol><lt id="S3.p2.3.3.m3.1.1.3.cmml" xref="S3.p2.3.3.m3.1.1.3"></lt></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.3.m3.1c">&gt;&lt;</annotation></semantics></math>/Img<math id="S3.p2.4.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.p2.4.4.m4.1a"><mo id="S3.p2.4.4.m4.1.1" xref="S3.p2.4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.p2.4.4.m4.1b"><gt id="S3.p2.4.4.m4.1.1.cmml" xref="S3.p2.4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.4.m4.1c">&gt;</annotation></semantics></math>Question:<math id="S3.p2.5.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.p2.5.5.m5.1a"><mo id="S3.p2.5.5.m5.1.1" xref="S3.p2.5.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.p2.5.5.m5.1b"><lt id="S3.p2.5.5.m5.1.1.cmml" xref="S3.p2.5.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.5.m5.1c">&lt;</annotation></semantics></math>Question<math id="S3.p2.6.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.p2.6.6.m6.1a"><mo id="S3.p2.6.6.m6.1.1" xref="S3.p2.6.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.p2.6.6.m6.1b"><gt id="S3.p2.6.6.m6.1.1.cmml" xref="S3.p2.6.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.6.m6.1c">&gt;</annotation></semantics></math>Answer:”</span></p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.03043/assets/x2.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example input images and questions for evaluated baselines. In the baseline “Region in text,” the digits are separated to provide a fair scenario to the LLM.</figcaption>
</figure>
<section id="S3.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.1 </span>Implementation details:</h4>

<div id="S3.SS0.SSS1.p1" class="ltx_para">
<p id="S3.SS0.SSS1.p1.1" class="ltx_p">We use R2GenGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as base MLLM, adapting it from the task of radiology report generation to VQA. We use a pre-trained Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as visual encoder and Llama 2 7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> as LLM, initialized with its official weights. Different from to R2GenGPT, we finetune all modules, including the LLM, end-to-end. We train all our models for 15 epochs, with a batch size of 8 and learning rate of 1e-4, with the AdamW optimizer and a cosine annealing scheduler with minimum learning rate 1e-6. For the text generation, we use a repetition penalty of 2.0 and a length penalty of -1.0. Our implementation uses PyTorch 2.0.1 and two Nvidia A100 cards with 80 GB of memory each.</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Results</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Results ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes our results on the DME-VQA, RIS-VQA, and INSEGCAT-VQA datasets. The accuracy and F1 score are reported for all datasets. Notably, our method consistently outperforms all evaluated baselines across all datasets, underscoring the efficacy of targeted visual prompting in enhancing MLLMs with localized question capabilities.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.3.3.4" class="ltx_tr">
<td id="S3.T2.3.3.4.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S3.T2.3.3.4.2" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T2.3.3.4.3" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="S3.T2.3.3.4.4" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T2.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt">Accuracy (%)</td>
<td id="S3.T2.3.3.4.6" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T2.3.3.4.7" class="ltx_td ltx_align_center ltx_border_tt">F1 score (%)</td>
</tr>
<tr id="S3.T2.3.3.5" class="ltx_tr">
<td id="S3.T2.3.3.5.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="7"><span id="S3.T2.3.3.5.1.1" class="ltx_text">DME-VQA</span></td>
<td id="S3.T2.3.3.5.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.5.3" class="ltx_td ltx_align_left ltx_border_t">No Mask</td>
<td id="S3.T2.3.3.5.4" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t">57.32</td>
<td id="S3.T2.3.3.5.6" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.5.7" class="ltx_td ltx_align_center ltx_border_t">57.32</td>
</tr>
<tr id="S3.T2.3.3.6" class="ltx_tr">
<td id="S3.T2.3.3.6.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.6.2" class="ltx_td ltx_align_left">Region in Text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S3.T2.3.3.6.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.6.4" class="ltx_td ltx_align_center">62.12</td>
<td id="S3.T2.3.3.6.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.6.6" class="ltx_td ltx_align_center">63.59</td>
</tr>
<tr id="S3.T2.3.3.7" class="ltx_tr">
<td id="S3.T2.3.3.7.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.7.2" class="ltx_td ltx_align_left">Crop Region <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.3.3.7.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.7.4" class="ltx_td ltx_align_center">86.52</td>
<td id="S3.T2.3.3.7.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.7.6" class="ltx_td ltx_align_center">87.26</td>
</tr>
<tr id="S3.T2.3.3.8" class="ltx_tr">
<td id="S3.T2.3.3.8.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.8.2" class="ltx_td ltx_align_left">Draw Region</td>
<td id="S3.T2.3.3.8.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.8.4" class="ltx_td ltx_align_center">86.86</td>
<td id="S3.T2.3.3.8.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.8.6" class="ltx_td ltx_align_center">86.85</td>
</tr>
<tr id="S3.T2.3.3.9" class="ltx_tr">
<td id="S3.T2.3.3.9.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.9.2" class="ltx_td ltx_align_left">Context Only</td>
<td id="S3.T2.3.3.9.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.9.4" class="ltx_td ltx_align_center">88.07</td>
<td id="S3.T2.3.3.9.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.9.6" class="ltx_td ltx_align_center">88.45</td>
</tr>
<tr id="S3.T2.3.3.10" class="ltx_tr">
<td id="S3.T2.3.3.10.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.10.2" class="ltx_td ltx_align_left"><span id="S3.T2.3.3.10.2.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S3.T2.3.3.10.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.10.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.10.4.1" class="ltx_text ltx_font_bold">90.30</span></td>
<td id="S3.T2.3.3.10.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.10.6" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.10.6.1" class="ltx_text ltx_font_bold">90.22</span></td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1" class="ltx_text" style="color:#808080;">LocAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><sup id="S3.T2.1.1.1.1.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.4.1" class="ltx_text" style="color:#808080;">84.2</span></td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.6.1" class="ltx_text" style="color:#808080;">85.79</span></td>
</tr>
<tr id="S3.T2.3.3.11" class="ltx_tr">
<td id="S3.T2.3.3.11.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="7"><span id="S3.T2.3.3.11.1.1" class="ltx_text">RIS-VQA</span></td>
<td id="S3.T2.3.3.11.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.11.3" class="ltx_td ltx_align_left ltx_border_t">No Mask</td>
<td id="S3.T2.3.3.11.4" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.11.5" class="ltx_td ltx_align_center ltx_border_t">50.00</td>
<td id="S3.T2.3.3.11.6" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.11.7" class="ltx_td ltx_align_center ltx_border_t">50.00</td>
</tr>
<tr id="S3.T2.3.3.12" class="ltx_tr">
<td id="S3.T2.3.3.12.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.12.2" class="ltx_td ltx_align_left">Region in Text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S3.T2.3.3.12.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.12.4" class="ltx_td ltx_align_center">64.81</td>
<td id="S3.T2.3.3.12.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.12.6" class="ltx_td ltx_align_center">65.39</td>
</tr>
<tr id="S3.T2.3.3.13" class="ltx_tr">
<td id="S3.T2.3.3.13.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.13.2" class="ltx_td ltx_align_left">Crop Region <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.3.3.13.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.13.4" class="ltx_td ltx_align_center">85.50</td>
<td id="S3.T2.3.3.13.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.13.6" class="ltx_td ltx_align_center">85.64</td>
</tr>
<tr id="S3.T2.3.3.14" class="ltx_tr">
<td id="S3.T2.3.3.14.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.14.2" class="ltx_td ltx_align_left">Draw Region</td>
<td id="S3.T2.3.3.14.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.14.4" class="ltx_td ltx_align_center">91.30</td>
<td id="S3.T2.3.3.14.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.14.6" class="ltx_td ltx_align_center">91.43</td>
</tr>
<tr id="S3.T2.3.3.15" class="ltx_tr">
<td id="S3.T2.3.3.15.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.15.2" class="ltx_td ltx_align_left">Context Only</td>
<td id="S3.T2.3.3.15.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.15.4" class="ltx_td ltx_align_center">91.77</td>
<td id="S3.T2.3.3.15.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.15.6" class="ltx_td ltx_align_center">91.81</td>
</tr>
<tr id="S3.T2.3.3.16" class="ltx_tr">
<td id="S3.T2.3.3.16.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.16.2" class="ltx_td ltx_align_left"><span id="S3.T2.3.3.16.2.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S3.T2.3.3.16.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.16.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.16.4.1" class="ltx_text ltx_font_bold">92.60</span></td>
<td id="S3.T2.3.3.16.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.16.6" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.16.6.1" class="ltx_text ltx_font_bold">92.54</span></td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_left"><span id="S3.T2.2.2.2.1.1" class="ltx_text" style="color:#808080;">LocAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><sup id="S3.T2.2.2.2.1.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_align_center"><span id="S3.T2.2.2.2.4.1" class="ltx_text" style="color:#808080;">82.73</span></td>
<td id="S3.T2.2.2.2.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.2.2.2.6" class="ltx_td ltx_align_center"><span id="S3.T2.2.2.2.6.1" class="ltx_text" style="color:#808080;">86.15</span></td>
</tr>
<tr id="S3.T2.3.3.17" class="ltx_tr">
<td id="S3.T2.3.3.17.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="7"><span id="S3.T2.3.3.17.1.1" class="ltx_text">INSEGCAT-VQA</span></td>
<td id="S3.T2.3.3.17.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.17.3" class="ltx_td ltx_align_left ltx_border_t">No Mask</td>
<td id="S3.T2.3.3.17.4" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.17.5" class="ltx_td ltx_align_center ltx_border_t">50.00</td>
<td id="S3.T2.3.3.17.6" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T2.3.3.17.7" class="ltx_td ltx_align_center ltx_border_t">50.00</td>
</tr>
<tr id="S3.T2.3.3.18" class="ltx_tr">
<td id="S3.T2.3.3.18.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.18.2" class="ltx_td ltx_align_left">Region in Text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S3.T2.3.3.18.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.18.4" class="ltx_td ltx_align_center">73.51</td>
<td id="S3.T2.3.3.18.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.18.6" class="ltx_td ltx_align_center">74.55</td>
</tr>
<tr id="S3.T2.3.3.19" class="ltx_tr">
<td id="S3.T2.3.3.19.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.19.2" class="ltx_td ltx_align_left">Crop Region <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.3.3.19.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.19.4" class="ltx_td ltx_align_center">90.91</td>
<td id="S3.T2.3.3.19.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.19.6" class="ltx_td ltx_align_center">90.93</td>
</tr>
<tr id="S3.T2.3.3.20" class="ltx_tr">
<td id="S3.T2.3.3.20.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.20.2" class="ltx_td ltx_align_left">Draw Region</td>
<td id="S3.T2.3.3.20.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.20.4" class="ltx_td ltx_align_center">95.44</td>
<td id="S3.T2.3.3.20.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.20.6" class="ltx_td ltx_align_center">95.43</td>
</tr>
<tr id="S3.T2.3.3.21" class="ltx_tr">
<td id="S3.T2.3.3.21.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.21.2" class="ltx_td ltx_align_left">Context Only</td>
<td id="S3.T2.3.3.21.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.21.4" class="ltx_td ltx_align_center">95.19</td>
<td id="S3.T2.3.3.21.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.21.6" class="ltx_td ltx_align_center">95.17</td>
</tr>
<tr id="S3.T2.3.3.22" class="ltx_tr">
<td id="S3.T2.3.3.22.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.22.2" class="ltx_td ltx_align_left"><span id="S3.T2.3.3.22.2.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S3.T2.3.3.22.3" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.22.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.22.4.1" class="ltx_text ltx_font_bold">95.51</span></td>
<td id="S3.T2.3.3.22.5" class="ltx_td ltx_align_top"></td>
<td id="S3.T2.3.3.22.6" class="ltx_td ltx_align_center"><span id="S3.T2.3.3.22.6.1" class="ltx_text ltx_font_bold">95.47</span></td>
</tr>
<tr id="S3.T2.3.3.3" class="ltx_tr">
<td id="S3.T2.3.3.3.2" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.3.3.3.1.1" class="ltx_text" style="color:#808080;">LocAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><sup id="S3.T2.3.3.3.1.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.3.3.3.4.1" class="ltx_text" style="color:#808080;">88.13</span></td>
<td id="S3.T2.3.3.3.5" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T2.3.3.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.3.3.3.6.1" class="ltx_text" style="color:#808080;">90.14</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy and F1 score comparison to SOTA approaches on the DME-VQA, RIS-VQA and INSEGCAT-VQA datasets. For the DME-VQA dataset only localized questions are considered. <sup id="S3.T2.7.1" class="ltx_sup">∗</sup>This result corresponds to a different architecture, but we include it for completeness. </figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the case of the DME-VQA and RIS-VQA datasets, we observe that the performance of <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">context only</span> surpasses that of <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">crop region</span>. At first glance, this suggests that the context holds more relevance than the specific contents of the region. However, this behavior is likely influenced by spurious correlations between region sizes/locations, and the corresponding answers. For instance, in DME-VQA, images with a high amount of biomarkers often feature smaller regions associated with negative answers. Another reason for this behavior is that in many cases the context provides more evidence, in terms of pixel count, to answer the question, as compared to the region. For instance, in RIS-VQA, the tool can often be determined from its body without considering the tip.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Notably, the <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">region in text</span> baseline exhibits poor performance. Given the use of a powerful LLM in the pipeline, higher performance might be expected. Different variations were explored for this baseline, including not separating the coordinate digits or replacing coordinate digits with words, but performance did not improve. We hypothesize that the model fails to correctly map location information from the text to the image, which can be at least partly attributed to using a ViT to embed the image.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We provide qualitative example results in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.1 Results ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The first column exemplifies cases where our method demonstrates robustness to subtle evidence (small biomarkers), correlations (surgical suture is usually close to the needle driver), and borderline cases (evidence close to the region border). The second column highlights the weaknesses of <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">context only</span> when the context fails to provide enough evidence for the answer. Finally, the third column shows errors made by our model in tricky cases (subtle or ambiguous evidence in the region).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2408.03043/assets/x3.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative examples on the DME-VQA (first row), RIS-VQA (second row), and INSEGCAT-VQA (third row) datasets.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2408.03043/assets/x4.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="333" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Error analysis by region location for the four strongest baselines. The maps are obtained by adding binary masks representing the regions for all QA pairs in each category and then normalizing. <span id="S3.F5.4.1" class="ltx_text ltx_font_bold">Top:</span> DME-VQA dataset. <span id="S3.F5.5.2" class="ltx_text ltx_font_bold">Bottom:</span> INSEGCAT-VQA dataset.</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.1 Results ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows error maps by region location for the DME-VQA and INSEGCAT-VQA datasets and for the four strongest baselines. On the left side of the plot, the locations of actual positives and negatives are illustrated. For the INSEGCAT-VQA dataset, this visualization reveals a location bias that other baselines without access to the region or the context may be exploiting. Due to the nature of the images (cataract surgery) and questions, regions with positive answers tend to cluster in a specific area. This, coupled with the dissimilarity of objects mentioned in the questions, explains why a baseline like <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">crop region</span> achieves relatively high performance on this dataset compared to the other two datasets (see Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Results ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Similarly, in the case of DME-VQA, it becomes evident that the lack of context in <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_italic">crop region</span> results in lower sensitivity, highlighting the significance of context even when the isolated region should theoretically provide sufficient evidence. Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.1 Results ‣ 3 Experiments and results ‣ Targeted Visual Prompting for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> also demonstrates that <span id="S3.SS1.p5.1.3" class="ltx_text ltx_font_italic">draw region</span> and <span id="S3.SS1.p5.1.4" class="ltx_text ltx_font_italic">context only</span> exhibit marked clusters of false positives and negatives in INSEGCAT-VQA, potentially indicating the utilization of location biases. In contrast, our method produces a more evenly distributed location for both types of errors.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we introduced a novel approach to enable localized questions in multimodal LLMs for the tasks of VQA. Our proposed approach involves the utilization of targeted visual prompting, granting the model access not only to the region and its context within the image but also to an isolated version of the region. Doing so allows two perspectives to be encoded in the prompt and more fine-grained information to be leveraged. Our approach demonstrates enhanced performance across all evaluated datasets compared to various baselines. Future works include extending the methodology to accommodate multiple images and enabling the use of comparison questions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Prospect of application:</span> This approach aims to be useful for medical assistants/chatbots that can help doctors assess specific parts of an image that look suspicious. By providing a second opinion, it can improve the accuracy of diagnoses. Additionally, this technology could help medical students learn and reinforce medical concepts by enabling a more modular analysis of medical images.

<span id="S4.p2.1.2" class="ltx_ERROR undefined">{credits}</span></p>
</div>
<section id="S4.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Acknowledgements</h4>

<div id="S4.SS0.SSS1.p1" class="ltx_para">
<p id="S4.SS0.SSS1.p1.1" class="ltx_p">This work was partially funded by the Swiss National Science Foundation through grant 191983.</p>
</div>
</section>
<section id="S4.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span><span id="S4.SS0.SSS2.1.1" class="ltx_ERROR undefined">\discintname</span>
</h4>

<div id="S4.SS0.SSS2.p1" class="ltx_para">
<p id="S4.SS0.SSS2.p1.1" class="ltx_p">The authors have no competing interests to declare that are relevant to the content of this article.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6077–6086 (2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: Vqa: Visual question answering. In: Proceedings of the IEEE international conference on computer vision. pp. 2425–2433 (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chen, C., Qin, R., Luo, F., Mi, X., Li, P., Sun, M., Liu, Y.: Position-enhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437 (2023)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6904–6913 (2017)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gupta, D., Suman, S., Ekbal, A.: Hierarchical deep multi-modal network for medical visual question answering. Expert Systems with Applications <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">164</span>, 113993 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hudson, D.A., Manning, C.D.: Gqa: a new dataset for compositional question answering over real-world images. arXiv preprint arXiv:1902.09506 <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">3</span>(8) (2019)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Liu, F., Peng, Y., Rosen, M.P.: An effective deep transfer learning and information fusion framework for medical visual question answering. In: International Conference of the Cross-Language Evaluation Forum for European Languages. pp. 238–247. Springer (2019)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012–10022 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mani, A., Yoo, N., Hinthorn, W., Russakovsky, O.: Point and ask: Incorporating pointing into visual question answering. arXiv preprint arXiv:2011.13681 (2020)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Nguyen, B.D., Do, T.T., Nguyen, B.X., Do, T., Tjiputra, E., Tran, Q.D.: Overcoming data limitation in medical visual question answering. In: Shen, D., Liu, T., Peters, T.M., Staib, L.H., Essert, C., Zhou, S., Yap, P.T., Khan, A. (eds.) Medical Image Computing and Computer Assisted Intervention – MICCAI 2019. pp. 522–530. Springer International Publishing, Cham (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ribeiro, M.T., Guestrin, C., Singh, S.: Are red roses red? evaluating consistency of question-answering models. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6174–6184 (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Seenivasan, L., Islam, M., Kannan, G., Ren, H.: Surgicalgpt: End-to-end language-vision gpt for visual question answering in surgery. arXiv preprint arXiv:2304.09974 (2023)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Selvaraju, R.R., Tendulkar, P., Parikh, D., Horvitz, E., Ribeiro, M.T., Nushi, B., Kamar, E.: Squinting at vqa models: Introspecting vqa models with sub-questions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10003–10011 (2020)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tascon-Morales, S., Márquez-Neila, P., Sznitman, R.: Consistency-preserving visual question answering in medical imaging. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part VIII. pp. 386–395. Springer (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tascon-Morales, S., Márquez-Neila, P., Sznitman, R.: Localized questions in medical visual question answering. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 361–370. Springer (2023)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">34</span>, 200–212 (2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Vu, M.H., Löfstedt, T., Nyholm, T., Sznitman, R.: A question-centric model for visual question answering in medical imaging. IEEE transactions on medical imaging <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">39</span>(9), 2856–2868 (2020)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Wang, Z., Liu, L., Wang, L., Zhou, L.: R2gengpt: Radiology report generation with frozen llms. Meta-Radiology <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">1</span>(3), 100033 (2023)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., Chen, E.: A survey on multimodal large language models. arXiv preprint arXiv:2306.13549 (2023)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhan, L.M., Liu, B., Fan, L., Chen, J., Wu, X.M.: Medical visual question answering via conditional reasoning. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 2345–2354 (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhang, D., Yu, Y., Li, C., Dong, J., Su, D., Chu, C., Yu, D.: Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601 (2024)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601 (2023)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., Xie, W.: Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.03041" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.03043" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.03043">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.03043" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.03045" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:37:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
