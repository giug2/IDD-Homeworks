<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.09831] Federated Large Language Models for Swarm Intelligence: A Survey</title><meta property="og:description" content="Federated learning (FL) offers a compelling framework for training large language models (LLMs) while addressing data privacy and decentralization challenges. This paper surveys recent advancements in the federated lea…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Large Language Models for Swarm Intelligence: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Large Language Models for Swarm Intelligence: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.09831">

<!--Generated on Fri Jul  5 20:29:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\receiveddate</span>
<p id="p1.2" class="ltx_p">XX Month, XXXX
<span id="p1.2.1" class="ltx_ERROR undefined">\reviseddate</span>XX Month, XXXX
<span id="p1.2.2" class="ltx_ERROR undefined">\accepteddate</span>XX Month, XXXX
<span id="p1.2.3" class="ltx_ERROR undefined">\publisheddate</span>XX Month, XXXX
<span id="p1.2.4" class="ltx_ERROR undefined">\currentdate</span>XX Month, XXXX
<span id="p1.2.5" class="ltx_ERROR undefined">\doiinfo</span>OJIM.2022.1234567</p>
</div>
<h1 class="ltx_title ltx_title_document">Federated Large Language Models for Swarm Intelligence: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Youyang Qu<span id="id1.1.id1" class="ltx_ERROR undefined">\authorrefmark</span>1
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Member
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> IEEE
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated learning (FL) offers a compelling framework for training large language models (LLMs) while addressing data privacy and decentralization challenges. This paper surveys recent advancements in the federated learning of large language models, with a particular focus on machine unlearning—a crucial aspect for complying with privacy regulations like the Right to be Forgotten. Machine unlearning in the context of federated LLMs involves systematically and securely removing individual data contributions from the learned model without retraining from scratch. We explore various strategies that enable effective unlearning, such as perturbation techniques, model decomposition, and incremental learning, highlighting their implications for maintaining model performance and data privacy. Furthermore, we examine case studies and experimental results from recent literature to assess the effectiveness and efficiency of these approaches in real-world scenarios. Our survey reveals a growing interest in developing more robust and scalable federated unlearning methods, suggesting a vital area for future research in the intersection of AI ethics and distributed machine learning technologies.</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{IEEEkeywords}</span>
<p id="p2.2" class="ltx_p">Federated Learning (FL), Large Language Models (LLMs), Swarm Intelligence, Efficiency, Pre-trained Models, Privacy and Security</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para">
<span id="S1.p1.1" class="ltx_ERROR undefined">\IEEEPARstart</span>
<p id="S1.p1.2" class="ltx_p">In the rapid evolution of artificial intelligence has led to significant breakthroughs in various domains, particularly with the advent of Large Language Models (LLMs) like GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. These models have revolutionized the way machines understand and generate human language, enabling applications ranging from automated customer support to advanced natural language processing tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Concurrently, there has been a growing interest in swarm intelligence, a field inspired by the collective behavior of decentralized, self-organized systems found in nature, such as ant colonies or bee swarms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Swarm intelligence leverages the simple rules followed by individual agents interacting locally with their environment to solve complex problems, a concept that is inherently robust and scalable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The convergence of LLMs with the principles of swarm intelligence introduces a novel approach to enhancing decentralized decision-making processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. By integrating these two technologies, it becomes possible to process and generate language-based data under the constraints of decentralized and privacy-preserving environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This integration is especially pertinent in scenarios where centralizing sensitive data is impractical or undesirable, such as in healthcare or financial services, where privacy concerns are paramount.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Federated learning (FL) emerges as a critical enabler in this context. It allows for the collaborative training of LLMs across multiple decentralized nodes without the need to share their actual data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This approach not only preserves the privacy and security of the data but also harnesses the collective intelligence of the swarm. By distributing the learning process across numerous nodes, federated learning enhances the robustness and adaptability of the models, making them more resilient to attacks and capable of generalizing across diverse datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, integrating LLMs with swarm principles in a federated setting introduces unique challenges. These include managing heterogeneous data sources, ensuring consistency in learning outcomes across diverse nodes, and developing efficient communication protocols that adhere to the lightweight nature of swarm agents. Addressing these challenges requires innovative solutions that balance the scalability and flexibility of swarm intelligence with the advanced processing capabilities of LLMs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Motivated by this, we aim to provide a systematic view of the current research status of federated LLMs for swarm intelligence from various perspectives, including architecture, efficiency, scalability, pre-trained models, security, and privacy. The main contributions of this survey are as follows.</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Synthesis of Federated Learning and Swarm Intelligence: We provide a systematic review of the methodologies that integrate federated learning with large language models, specifically tailored for swarm intelligence applications. This includes discussions on the adaptations necessary for handling decentralized, autonomous agents in a swarm.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Analysis of Techniques and Performance: The paper evaluates various strategies and modifications to traditional federated learning algorithms to enhance their applicability and efficiency in swarm-based systems, emphasizing real-world implementations and case studies.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Strategic Insights and Frameworks: We compare a set of criteria and frameworks for designing and deploying federated LLMs within swarm intelligence contexts, offering insights into achieving scalability, robustness, and real-time responsiveness.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of the survey is as follows. Section 2 provides a detailed exploration of the foundations of swarm intelligence and its integration with large language models, highlighting the synergies and potential enhancements. Section 3 examines the specific adaptations and innovations in federated learning algorithms that facilitate efficient and effective training of LLMs across distributed swarm agents. Section 4 presents a critical analysis of current implementations and discusses the technical challenges, practical limitations, and potential solutions. Section 5 concludes the survey with a discussion on future research directions and the broader implications of federated LLMs for swarm intelligence in various applications.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.09831/assets/diagram.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated LLMs for Swarm Intelligence Architecture</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Foundations and Integration of SI and LLM</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Introduction to Swarm Intelligence</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Definition and Historical Context</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Swarm intelligence (SI) refers to the collective behavior of decentralized, self-organized systems, typically composed of a population of simple agents interacting locally with their environment and each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local interactions between such agents lead to the emergence of “intelligent” global behavior, unknown to the individual agents.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">The concept of swarm intelligence originated from biological studies of colonies of ants and swarms of bees in the late 1980s and early 1990s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In nature, these organisms exhibit behaviors that can be extraordinarily complex, such as finding the shortest path to a food source or constructing complex structures, despite the simplicity of individual behaviors and the lack of central control. This paradox, where complex global behavior arises from simple local interactions, has inspired various algorithms and techniques in artificial intelligence.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Key Principles of Swarm Behavior</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Swarm behavior is underpinned by several key principles that enable the robust, scalable, and flexible characteristics of natural swarms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>:</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Decentralization:</span> There is no single controlling entity in swarm systems; instead, control is distributed across the individual agents comprising the swarm. This decentralization contributes to the robustness and fault tolerance of the system, as there is no single point of failure.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p"><span id="S2.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Simple Rules:</span> Each agent in a swarm follows simple rules, and the interaction of these rules across the entire swarm leads to the emergence of complex behavior. These rules are typically based on the local perception of the environment, without a global view.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p"><span id="S2.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Local Interaction:</span> Agents primarily rely on local information or interactions with their neighbors. This limitation is crucial for scalability, as it reduces the communication overhead and computational complexity that would arise from managing global information.</p>
</div>
<div id="S2.SS1.SSS2.p5" class="ltx_para">
<p id="S2.SS1.SSS2.p5.1" class="ltx_p"><span id="S2.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Emergence:</span> The global behavior of the swarm emerges from the interactions of individuals without being explicitly programmed. Emergent behavior is a hallmark of swarm intelligence, providing solutions that are often more adaptive and resilient than those devised through central planning.</p>
</div>
<div id="S2.SS1.SSS2.p6" class="ltx_para">
<p id="S2.SS1.SSS2.p6.1" class="ltx_p"><span id="S2.SS1.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Adaptability and Learning:</span> Swarm systems are adaptive; they can change their behavior based on feedback from the environment. This learning can occur at the level of the individual or the swarm, enhancing the system’s ability to respond to dynamic or uncertain environments.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fundamentals of Large Language Models</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Architecture and Functionality</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Large Language Models (LLMs) are a type of artificial intelligence model designed to understand, generate, and manipulate human language by learning from vast amounts of textual data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These models are based on the architecture of deep neural networks, particularly Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> models, which have revolutionized the field of natural language processing (NLP) since their introduction.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">The core component of a Transformer, and consequently of many LLMs, is the self-attention mechanism. This mechanism allows the models to weigh the importance of different words in a sentence, regardless of their distance from each other in the text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. This feature enables the model to capture complex linguistic structures and contextual nuances, making them highly effective for a wide range of NLP tasks, from machine translation and summarization to question answering and text generation.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">LLMs such as GPT (Generative Pretrained Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and their variants are pretrained on diverse internet-scale datasets using tasks like masked language modeling (where random words are masked out and the model learns to predict them based on the surrounding context) and next-token prediction (predicting the next word in a sequence). This pretraining step allows the models to develop a broad understanding of language and context, which can then be fine-tuned to specific applications with relatively little additional training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Evolution of LLMs in Natural Language Processing</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The evolution of LLMs has marked several milestones in the field of NLP, starting from simpler models like RNNs (Recurrent Neural Networks) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and LSTMs (Long Short-Term Memory networks) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to more sophisticated architectures like the aforementioned Transformers. The ability of LLMs to handle longer sequences of text and their parallel processing capabilities (a significant advantage over RNNs and LSTMs) have dramatically improved efficiency and scalability in training and inference processes.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">The progression from models that could handle only specific tasks (task-specific models) to models that can be applied to virtually any text-based task (general-purpose models) is another crucial development <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This shift has been driven by the realization that a deep, contextual understanding of language can be more effectively achieved through large-scale, unsupervised learning from diverse and extensive textual corpora.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">As LLMs have grown in size—from millions to billions of parameters—their ability to model and generate human-like text has improved. However, this increase in size comes with its own set of challenges, such as higher computational costs, greater energy consumption, and the need for substantial hardware resources, prompting ongoing research into more efficient model architectures and training methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Integrating Swarm Intelligence with LLMs</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Potential Benefits and Synergistic Effects</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">Integrating swarm intelligence (SI) principles with large language models (LLMs) offers unique opportunities to enhance the adaptability and efficiency of distributed computing systems, especially in processing and generating language-based data. This combination leverages the decentralized nature of SI to manage and operate LLMs across multiple nodes, potentially reducing the computational load and data privacy concerns associated with centralized training and inference.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p">One of the main benefits of this integration is the enhancement of robustness and fault tolerance in language processing tasks. By distributing the LLM’s functionalities across a swarm of agents, the system can continue to operate effectively even if some nodes fail or are compromised, as the collective intelligence of the swarm compensates for individual failures.</p>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p">Another significant benefit is scalability. Swarm intelligence inherently supports scaling to large numbers of simple agents or nodes, which is particularly useful when deploying LLMs in environments with resource constraints or where real-time responses are crucial. This scalability also allows for more extensive data processing without the need for proportionally increased computational power at a single point.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Case Studies and Successful Implementations</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">Several case studies highlight the practical applications and benefits of integrating swarm intelligence with large language models:</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">Decentralized Content Moderation: In social media platforms, a swarm of agents each running smaller segments of an LLM can collaboratively monitor and moderate content in real-time, ensuring compliance with community guidelines while maintaining user privacy.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p">Multi-Agent Translation Systems: For multinational corporations, swarm-based LLMs can provide real-time translation services across different regions, where each agent handles language tasks specific to its local region, improving the speed and accuracy of services.</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para">
<p id="S2.SS3.SSS2.p4.1" class="ltx_p">Distributed Sentiment Analysis: In market research, a swarm of LLM-equipped agents can perform sentiment analysis on customer feedback data distributed across various servers or locations, synthesizing insights locally and aggregating them to derive global sentiment trends without transferring sensitive data centrally.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated LLMs for Smarm Intelligence</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>LLM Architectures in Federated Settings</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of Federated Large Language Model Architectures</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Contributions</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Technique</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">LM used</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.2.1.1" class="ltx_p" style="width:113.8pt;">Implementation of N-gram models in federated settings, dealing with distributed data.</span>
</span>
</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;">Federated learning</span>
</span>
</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.4.1.1" class="ltx_p" style="width:56.9pt;">N-gram RNN</span>
</span>
</td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.5.1.1" class="ltx_p" style="width:56.9pt;">Not specified</span>
</span>
</td>
<td id="S3.T1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;">High communication overhead, data privacy concerns.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">Enhances privacy and reduces server communication through localized computations.</span>
</span>
</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.3.1.1" class="ltx_p" style="width:56.9pt;">Federated Reconstruction</span>
</span>
</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.4.1.1" class="ltx_p" style="width:56.9pt;">LSTM-based</span>
</span>
</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.5.1.1" class="ltx_p" style="width:56.9pt;">MovieLens</span>
</span>
</td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">Complexity in local computations, potential performance trade-offs.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">Framework for training and deploying LLMs in federated settings, addressing scalability and efficiency.</span>
</span>
</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">FederatedScope</span>
</span>
</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">LLaMa-7B</span>
</span>
</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;">HumanEval, HELM, etc</span>
</span>
</td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">Scalability issues, computational demands.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4</th>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.2.1.1" class="ltx_p" style="width:113.8pt;">Integration of LLMs with edge computing for autonomous decision-making in connected environments.</span>
</span>
</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.3.1.1" class="ltx_p" style="width:56.9pt;">Edge AI, Federated Learning</span>
</span>
</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">GPT-3</span>
</span>
</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.5.1.1" class="ltx_p" style="width:56.9pt;">Synthetic challenging user request dataset</span>
</span>
</td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">Resource constraints on edge devices, deployment challenges.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">5</th>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.2.1.1" class="ltx_p" style="width:113.8pt;">Techniques for reducing LLM parameter count in federated settings, ensuring efficiency.</span>
</span>
</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.3.1.1" class="ltx_p" style="width:56.9pt;">Model pruning, Knowledge distillation</span>
</span>
</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.4.1.1" class="ltx_p" style="width:56.9pt;">Bert-based</span>
</span>
</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.5.1.1" class="ltx_p" style="width:56.9pt;">IMDP, Yelp</span>
</span>
</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.6.1.1" class="ltx_p" style="width:113.8pt;">Compromise on model performance, complexity in model reduction techniques.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> explore the adaptation of N-gram language models to federated learning environments. It discusses the challenges of maintaining model performance when training data is distributed and not centrally stored. The primary focus is on how to effectively aggregate learned N-grams from multiple nodes while preserving privacy and minimizing communication overhead.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Singhal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> introduce a novel approach named ”Federated Reconstruction,” which enhances privacy in federated learning by allowing more computation to be performed locally. The paper particularly emphasizes its application to large language models, discussing how it can reduce the amount of data required to be shared with the server, thus enhancing data privacy and reducing bandwidth requirements.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Kuang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> present FEDERATEDSCOPE, a framework designed to facilitate the training and deployment of large language models in a federated setting. It offers insights into the architectural adjustments and optimizations necessary to handle the complexities associated with training large models across decentralized data sources. The paper also explores scalability and efficiency challenges, providing solutions to maintain robust model performance.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> discusses the integration of large language models (LLMs) with edge computing devices in a federated learning context. It focuses on how LLMs can empower edge devices for autonomous decision-making, highlighting the potential for LLMs to enhance connected intelligence across various applications. The paper addresses the challenges of deploying complex models on resource-constrained edge devices.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Focusing on the issue of model size, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> propose techniques for reducing the parameter count of large language models in federated settings. It explores methods like model pruning and knowledge distillation to maintain model efficacy with fewer parameters, which is critical for efficient data transmission and quick adaptation in federated networks.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">These papers collectively cover a spectrum of considerations for deploying large language models in federated learning environments. Starting from the basic implementation of N-gram models in a federated manner, the survey transitions to advanced strategies like Federated Reconstruction for enhancing privacy and computational efficiency. It then moves into a discussion on frameworks and architectural adjustments necessary for scaling LLMs across decentralized networks. The integration with edge AI represents an application-focused advancement, providing a practical viewpoint on deploying reduced-parameter models in real-world settings. This logical progression from foundational concepts to practical implementations and optimizations illustrates a comprehensive view of current research in the domain of federated large language models.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The integration of large language models (LLMs) in federated settings, as explored in these papers, significantly advances the field of swarm intelligence by leveraging the collective learning capabilities of decentralized networks. The first paper’s exploration of N-gram models in federated environments lays the foundational understanding of how individual nodes, like agents in a swarm, can contribute localized knowledge to build a cohesive and comprehensive model. The introduction of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> in the second paper enhances this concept by minimizing central coordination, thereby promoting more autonomous local decision-making—akin to the independent yet coordinated behavior seen in natural swarms. The  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> framework further capitalizes on this by scaling up the capabilities of LLMs across diverse and distributed datasets, mirroring how swarms adapt to varied environments. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> brings these concepts into the realm of edge computing, where LLMs empower edge devices to function like intelligent agents that perform tasks and make decisions in real time, enhancing the swarm’s overall responsiveness and flexibility. Lastly, the techniques discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> for reducing the parameter count of LLMs ensure that the computational load is manageable even in resource-constrained environments, which is crucial for maintaining the efficiency and agility of a swarm. Together, these studies demonstrate how federated learning models can emulate and enhance swarm-like intelligence, promoting robust, scalable, and decentralized problem-solving capabilities in artificial systems.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Efficient Fine-tuning of LLM</h3>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of Efficient Fine-Tuning of LLMs in Federated Settings</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Contributions</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Technique</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">LM used</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T2.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T2.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.2.1.1" class="ltx_p" style="width:113.8pt;">Integrates differential privacy efficiently in federated LLM training with minimal performance loss.</span>
</span>
</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;">Differential privacy, noise addition, parameter clipping</span>
</span>
</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.4.1.1" class="ltx_p" style="width:56.9pt;">CIFG 19M, Transformer 21M, etc.</span>
</span>
</td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.5.1.1" class="ltx_p" style="width:56.9pt;">WordPiece</span>
</span>
</td>
<td id="S3.T2.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;">Potential impact on model accuracy, complexity in tuning privacy parameters.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<th id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">Utilizes prompt tuning and adaptive optimization to reduce parameters and adapt learning rates dynamically.</span>
</span>
</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.3.1.1" class="ltx_p" style="width:56.9pt;">Prompt tuning, adaptive optimization</span>
</span>
</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.4.1.1" class="ltx_p" style="width:56.9pt;">GPT-2, LLaMa, etc.</span>
</span>
</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.5.1.1" class="ltx_p" style="width:56.9pt;">MRPC</span>
</span>
</td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">May not reach the full model’s potential, depends heavily on prompt design.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<th id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">Optimizes prompt tuning in federated settings with synchronization for stable and accurate model updates.</span>
</span>
</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">Federated prompt tuning, synchronization mechanisms</span>
</span>
</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">Roberta</span>
</span>
</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;">GLUE</span>
</span>
</td>
<td id="S3.T2.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">Challenges in synchronization across diverse networks, prompt design limitations.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<th id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4</th>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.2.1.1" class="ltx_p" style="width:113.8pt;">Benchmarks federated learning models and systems at scale, identifying and addressing bottlenecks.</span>
</span>
</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.3.1.1" class="ltx_p" style="width:56.9pt;">Benchmarking, model compression, robust aggregation methods</span>
</span>
</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">Albert</span>
</span>
</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.5.1.1" class="ltx_p" style="width:56.9pt;">Reddit</span>
</span>
</td>
<td id="S3.T2.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">Scaling issues, may require significant computational resources.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<th id="S3.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">5</th>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.2.1.1" class="ltx_p" style="width:113.8pt;">Reduces data transmission with forward gradient method, enhancing privacy and efficiency.</span>
</span>
</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.3.1.1" class="ltx_p" style="width:56.9pt;">Forward gradient technique, efficient gradient aggregation</span>
</span>
</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.4.1.1" class="ltx_p" style="width:56.9pt;">LLaMa, Roberta, etc.</span>
</span>
</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.5.1.1" class="ltx_p" style="width:56.9pt;">AGNEWS, Yelp, etc.</span>
</span>
</td>
<td id="S3.T2.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T2.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.6.1.1" class="ltx_p" style="width:113.8pt;">Potential loss of gradient information, requires careful implementation.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Ro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> address the challenges of applying differential privacy in federated settings, particularly when fine-tuning large language models (LLMs). It introduces a novel architecture that balances the trade-offs between privacy, efficiency, and performance. The proposed model uses lightweight mechanisms to ensure differential privacy without significantly degrading the model’s utility. Key techniques include noise addition and parameter clipping during the training process to maintain privacy. The architecture is designed to be modular, allowing easy integration with existing federated learning frameworks. This study is pivotal as it demonstrates how privacy considerations can be seamlessly integrated into the federated fine-tuning of LLMs, ensuring that user data remains confidential while still contributing to the collective learning process.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Che et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> explore the use of prompt tuning as a parameter-efficient method for fine-tuning large language models in federated learning environments. Prompt tuning involves adjusting a small set of parameters (prompts) while keeping the majority of the model fixed, significantly reducing the computational and communication overhead typically associated with training large models. Additionally, the paper introduces an adaptive optimization technique that dynamically adjusts learning rates based on the network conditions and convergence rates of the participating nodes. This approach not only enhances the efficiency of the federated learning process but also ensures that the model adapts optimally across diverse and potentially noisy datasets.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Building on the concept of prompt tuning, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> specifically tailor this technique for federated settings. The paper presents a framework that optimizes the deployment of prompt tuning across a distributed network, ensuring that all nodes contribute effectively to the model’s learning process without overwhelming the network’s bandwidth. The framework also includes a novel synchronization mechanism that aligns the updates from all nodes to improve the overall stability and accuracy of the model. By focusing on the efficient distribution and synchronization of prompts, this study further refines the practical application of prompt tuning in federated environments.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Lai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> offer a comprehensive benchmarking study that evaluates the performance of various federated learning models, including LLMs, across different scales and settings. The paper identifies key bottlenecks in scaling federated learning systems and proposes solutions to overcome them. Among the highlighted solutions are strategies for efficient data sampling and distribution, model compression techniques, and robust aggregation methods that can handle large-scale deployments. This benchmarking is crucial for understanding how different models and systems perform in real-world scenarios, providing a foundation for further optimization of federated learning frameworks.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> introduce a forward gradient technique that optimizes the gradient computation and transmission in federated learning of LLMs. By calculating and sharing forward gradients instead of the full gradients, this method significantly reduces the amount of data that needs to be transmitted between nodes and the central server. This is particularly beneficial in scenarios with limited bandwidth or where data privacy is a concern. The technique also includes mechanisms for aggregating these gradients efficiently, ensuring that the model converges quickly without compromising on performance.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Together, these papers present a comprehensive view of current innovations in the efficient fine-tuning of large language models within federated learning frameworks. Starting from foundational privacy-preserving techniques, the discussion progresses through enhancements in parameter efficiency via prompt tuning, system-wide optimizations for handling scale, and innovative gradient management for improved communication efficiency. Each paper contributes to a layered understanding of how to optimize the training and deployment of LLMs across distributed networks, ensuring both performance and practical viability in real-world applications. This narrative arc not only highlights individual advancements but also illustrates the synergistic potential of combining these techniques to address the multifaceted challenges of federated learning.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">In the realm of swarm intelligence within federated learning environments, the contributions of these papers are pivotal in demonstrating how decentralized systems can collaboratively refine and fine-tune large language models (LLMs) efficiently. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> introduces an architecture that incorporates differential privacy directly into the federated training process, reflecting the swarm intelligence concept of achieving collective goals while maintaining individual privacy. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> further the narrative by focusing on prompt tuning—modifying a small subset of model parameters—thus reducing the complexity and computational load of federating large-scale models. This approach allows each node, akin to an agent in a swarm, to contribute more effectively and efficiently to the collective intelligence, optimizing the system’s overall learning capability.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> provides benchmarking insights that help understand the performance and system constraints when scaling up federated learning—akin to assessing the operational capacity of a swarm over varied environments. Lastly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> enhances the communication efficiency between nodes using a forward gradient method, significantly reducing the bandwidth needed for model updates. This mirrors a swarm’s efficiency in using limited resources to maintain robust communication across the group. Collectively, these studies exemplify how federated learning can harness swarm intelligence principles to achieve decentralized problem-solving and learning, effectively managing resources and maintaining synchronization across distributed agents to optimize collective outcomes in the training and deployment of LLMs.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pre-Training of LLM in Federated Learning</h3>

<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Pre-Training of LLM in Federated Learning</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<td id="S3.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Contributions</span></span>
</span>
</td>
<td id="S3.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Technique</span></span>
</span>
</td>
<td id="S3.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T3.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">LM used</span></span>
</span>
</td>
<td id="S3.T3.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T3.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T3.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T3.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<th id="S3.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T3.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.2.1.1" class="ltx_p" style="width:113.8pt;">Adapts BERT pre-training to federated learning, maintaining data privacy across distributed datasets.</span>
</span>
</td>
<td id="S3.T3.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;">Federated learning adaptations, privacy enhancements</span>
</span>
</td>
<td id="S3.T3.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.4.1.1" class="ltx_p" style="width:56.9pt;">BERT</span>
</span>
</td>
<td id="S3.T3.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.5.1.1" class="ltx_p" style="width:56.9pt;">GLUE</span>
</span>
</td>
<td id="S3.T3.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;">Uneven data distribution, increased complexity in training.</span>
</span>
</td>
</tr>
<tr id="S3.T3.1.3.3" class="ltx_tr">
<th id="S3.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S3.T3.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">Explores practical deployment of pretrained models in federated learning, addressing data heterogeneity.</span>
</span>
</td>
<td id="S3.T3.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.3.3.1.1" class="ltx_p" style="width:56.9pt;">Dynamic update rates, selective parameter updating</span>
</span>
</td>
<td id="S3.T3.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.3.4.1.1" class="ltx_p" style="width:56.9pt;">DistiBERT, BART</span>
</span>
</td>
<td id="S3.T3.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.3.5.1.1" class="ltx_p" style="width:56.9pt;">SST2, OntoNotes, etc</span>
</span>
</td>
<td id="S3.T3.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">Sync issues, managing diverse data distributions.</span>
</span>
</td>
</tr>
<tr id="S3.T3.1.4.4" class="ltx_tr">
<th id="S3.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S3.T3.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">Enhances multilingual understanding in federated learning using pretrained models across diverse languages.</span>
</span>
</td>
<td id="S3.T3.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">Multilingual model adaptation</span>
</span>
</td>
<td id="S3.T3.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">Multilingual BERT</span>
</span>
</td>
<td id="S3.T3.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T3.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;">MTNT</span>
</span>
</td>
<td id="S3.T3.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">Requires careful handling of linguistic diversity, potential bias in language representation.</span>
</span>
</td>
</tr>
<tr id="S3.T3.1.5.5" class="ltx_tr">
<th id="S3.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">4</th>
<td id="S3.T3.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T3.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.5.5.2.1.1" class="ltx_p" style="width:113.8pt;">Reduces communication overhead by employing parameter-efficient fine-tuning techniques in federated settings.</span>
</span>
</td>
<td id="S3.T3.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T3.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.5.5.3.1.1" class="ltx_p" style="width:56.9pt;">Layer-wise relevance propagation, parameter-efficient fine-tuning</span>
</span>
</td>
<td id="S3.T3.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T3.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">Bert-based</span>
</span>
</td>
<td id="S3.T3.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T3.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.5.5.5.1.1" class="ltx_p" style="width:56.9pt;">GLUE</span>
</span>
</td>
<td id="S3.T3.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T3.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">Possible reduction in model accuracy, limited updates to model parameters.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> explore the feasibility of applying federated learning to the pre-training phase of BERT (Bidirectional Encoder Representations from Transformers), a popular language model. Named FedBERT, this approach adapts the pre-training process to work across distributed data sets without compromising data privacy. The study demonstrates how federated learning can be effectively utilized to pre-train BERT with data from multiple decentralized sources, ensuring that the private data remains local while still contributing to the training of a robust model. The paper highlights modifications to the BERT training algorithm to accommodate the federated setting, including adjustments to handle the uneven distribution of data across nodes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Focusing on the practical aspects, Agarwal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> provide a detailed examination of deploying pretrained language models in a federated learning framework. It discusses the challenges and solutions related to synchronization, data heterogeneity, and maintaining model performance when the pretrained model is adapted to new datasets across distributed environments. The paper proposes several optimization techniques to enhance the efficiency and effectiveness of federated learning for pretrained models, such as dynamic update rates and selective parameter updating to cope with the diverse data distributions typically encountered in federated settings.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Addressing the challenge of multilingual contexts, Weller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> introduce techniques for leveraging pretrained language models to enhance language understanding across different languages in a federated learning scenario. It explores the application of multilingual BERT models, adapting them to federated settings to improve model training across linguistically diverse data. This approach not only broadens the applicability of federated learning but also enhances the inclusivity of language technologies, allowing for effective model training without centralizing data from various language communities.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Malaviya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> address one of the significant challenges in federated learning: the high communication overhead. It presents strategies for reducing bandwidth consumption during the pre-training of language models in federated setups. By employing parameter-efficient techniques such as layer-wise relevance propagation and other fine-tuning methods that minimize the number of parameters that need to be updated and transmitted, the paper successfully decreases the network load, which is crucial for practical implementations of federated learning where bandwidth may be limited.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">These papers collectively cover a spectrum of methodologies for integrating pre-training phases of large language models with federated learning principles. Starting from adapting existing models like BERT to federated settings, moving through practical deployment considerations, addressing multilingual training needs, reducing communication overhead, and finally discussing comprehensive lifecycle approaches, these studies showcase a progressive enhancement in the techniques and applications of federated learning. Each paper builds on the notion that federated learning can be effectively scaled and adapted to pre-train language models in a way that respects privacy, manages resources efficiently, and embraces linguistic diversity, thus advancing the field towards more inclusive and technologically feasible solutions.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">The four papers significantly advance swarm intelligence principles in federated learning by demonstrating how large language models can be collaboratively pre-trained across distributed nodes. Each paper introduces a different facet of decentralized intelligence: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> adapts BERT for privacy-preserving collaborative training, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> tackles practical deployment challenges such as dynamic updating to handle data heterogeneity, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> extends federated learning to multilingual contexts enhancing linguistic inclusivity, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> reduces communication overhead with parameter-efficient techniques. Together, these studies exemplify how federated learning can emulate swarm behavior, optimizing collaborative tasks through decentralized interactions and contributing to the development of robust, scalable models without centralized control.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Scalable LLM via Federated Learning</h3>

<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Scalable LLM via Federated Learning</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Contributions</span></span>
</span>
</td>
<td id="S3.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T4.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Technique</span></span>
</span>
</td>
<td id="S3.T4.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T4.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T4.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">LM used</span></span>
</span>
</td>
<td id="S3.T4.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T4.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T4.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T4.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T4.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T4.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.1.2.2" class="ltx_tr">
<th id="S3.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T4.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.2.2.1.1" class="ltx_p" style="width:113.8pt;">Develops FATE-LLM, a framework for scalable federated learning of LLMs, with robustness against node dropout and data discrepancies.</span>
</span>
</td>
<td id="S3.T4.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;">Hierarchical aggregation</span>
</span>
</td>
<td id="S3.T4.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.2.4.1.1" class="ltx_p" style="width:56.9pt;">LLaMa, GPT-2, etc.</span>
</span>
</td>
<td id="S3.T4.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.2.5.1.1" class="ltx_p" style="width:56.9pt;">AdvertiseGen</span>
</span>
</td>
<td id="S3.T4.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T4.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;">May struggle with extremely large datasets or very high node dropout rates.</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.3.3" class="ltx_tr">
<th id="S3.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S3.T4.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">Enhances federated learning from pre-trained models using contrastive learning to maintain model quality with non-IID data.</span>
</span>
</td>
<td id="S3.T4.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.3.3.1.1" class="ltx_p" style="width:56.9pt;">Contrastive learning</span>
</span>
</td>
<td id="S3.T4.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.3.4.1.1" class="ltx_p" style="width:56.9pt;">Not specified</span>
</span>
</td>
<td id="S3.T4.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.3.5.1.1" class="ltx_p" style="width:56.9pt;">DomainNet, Office-10, etc.</span>
</span>
</td>
<td id="S3.T4.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T4.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">Requires careful tuning to avoid negative impacts on convergence.</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.4.4" class="ltx_tr">
<th id="S3.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S3.T4.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">Investigates the impact of large cohorts on federated learning efficiency, proposing dynamic cohort size adjustments.</span>
</span>
</td>
<td id="S3.T4.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">Dynamic cohort sizing</span>
</span>
</td>
<td id="S3.T4.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">Not specified</span>
</span>
</td>
<td id="S3.T4.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;">Stack Overflow, SHAKESPEARE, etc.</span>
</span>
</td>
<td id="S3.T4.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T4.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">Communication overhead can still be significant in very large networks.</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.5.5" class="ltx_tr">
<th id="S3.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4</th>
<td id="S3.T4.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.5.2.1.1" class="ltx_p" style="width:113.8pt;">Presents an architecture for efficiently fine-tuning LLMs in federated settings, reducing resource demands.</span>
</span>
</td>
<td id="S3.T4.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.5.3.1.1" class="ltx_p" style="width:56.9pt;">Selective parameter updating</span>
</span>
</td>
<td id="S3.T4.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">DistilBERT</span>
</span>
</td>
<td id="S3.T4.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T4.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.5.5.1.1" class="ltx_p" style="width:56.9pt;">IMDB, AG News, etc.</span>
</span>
</td>
<td id="S3.T4.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T4.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">Limited by the granularity of parameter selection and update mechanisms.</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.6.6" class="ltx_tr">
<th id="S3.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">5</th>
<td id="S3.T4.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T4.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.6.2.1.1" class="ltx_p" style="width:113.8pt;">Discusses methodologies for training larger models in cross-device federated learning through model splitting and layered updates.</span>
</span>
</td>
<td id="S3.T4.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T4.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.6.3.1.1" class="ltx_p" style="width:56.9pt;">Model splitting, layered updates</span>
</span>
</td>
<td id="S3.T4.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T4.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.6.4.1.1" class="ltx_p" style="width:56.9pt;">LSTM, Transformer, etc.</span>
</span>
</td>
<td id="S3.T4.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T4.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.6.5.1.1" class="ltx_p" style="width:56.9pt;">Stack
Overflow</span>
</span>
</td>
<td id="S3.T4.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T4.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.6.6.1.1" class="ltx_p" style="width:113.8pt;">May face challenges in ensuring model consistency and managing update synchronization.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Fan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> introduce FATE-LLM, a framework designed for training large language models using federated learning across highly distributed and heterogeneous environments. It focuses on overcoming the scalability challenges inherent in coordinating and aggregating updates across numerous devices. FATE-LLM employs a hierarchical aggregation strategy that reduces the communication burden and latency, enabling effective scalability without compromising the learning efficiency or model accuracy. The framework also includes robustness against node dropout and data discrepancies, making it particularly suitable for real-world applications where network conditions and data distributions can be highly variable.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Tan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> explore the use of contrastive learning techniques to enhance the effectiveness of federated learning when starting from pre-trained language models. By leveraging contrastive learning, the approach aims to maximize the relevance of local updates to the global model, thus enhancing the overall model performance even when individual clients have sparse or non-IID (non-independent and identically distributed) data. The paper discusses how this technique can be particularly useful in maintaining model quality while scaling up the number of participants in a federated learning setup, addressing common issues such as catastrophic forgetting and model divergence.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Charles et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> investigate the effects of increasing the cohort size (the number of participating nodes in each training round) on the efficiency and effectiveness of federated learning systems. It proposes a method for dynamically adjusting the cohort size based on real-time assessments of network bandwidth and participant availability, optimizing resource allocation and training speed. The paper provides empirical evidence that larger cohorts can lead to faster convergence and improved model performance, provided that the communication and aggregation protocols are efficiently managed.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Hilmkil et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> address the challenge of fine-tuning large pre-trained language models in a federated setting. It presents a novel architecture that allows for efficient distribution of model parameters and selective updating of those parameters most relevant to specific tasks or data types. The approach helps mitigate the high resource demands typically associated with large model fine-tuning, making it feasible to scale up federated learning to handle large language models across extensive networks of distributed nodes.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">Focusing on cross-device scenarios, Ro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> explore methodologies to scale up the size of language models that can be effectively trained in a federated learning framework involving a wide array of devices. It introduces techniques such as model splitting and layered updates to manage the computational and memory constraints of devices, allowing even those with limited capabilities to participate in the training process. The paper emphasizes the scalability and flexibility of federated learning approaches in accommodating large models without requiring high-end hardware on each client device.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">These papers collectively advance the field of federated learning by addressing various aspects of scalability when training large language models. From hierarchical aggregation and contrastive learning approaches that enhance communication efficiency and model relevance, to dynamic cohort management and architectural innovations for fine-tuning, each paper contributes to overcoming the significant challenges of scaling LLMs in distributed environments. By implementing these methodologies, federated learning can harness the power of swarm intelligence, where many distributed agents (devices) work together to solve complex problems, leading to the development of more robust and scalable language models that can operate effectively across diverse and widespread data sources.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.1" class="ltx_p">These papers provide significant insights into how swarm intelligence principles can be applied to scale large language models (LLMs) in federated learning environments. Each paper introduces innovative techniques that enhance collaborative learning across distributed networks, reflecting key aspects of swarm behavior such as decentralized decision-making and collective problem-solving. The first paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, with its hierarchical aggregation framework, exemplifies how individual nodes can efficiently contribute to a global model without centralized oversight, much like how insects in a swarm interact locally with simple rules that lead to complex group behavior. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>’s use of contrastive learning further leverages local computations to maintain the relevance and accuracy of the global model, ensuring that each node’s update significantly contributes to the collective knowledge. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>’s dynamic cohort sizing adapts to changing network conditions and resources, optimizing the learning process similar to how swarms dynamically adjust to environmental changes. Lastly, the paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> on model splitting and layered updates allows even resource-constrained devices to participate in the learning process, promoting inclusivity and resilience, akin to a swarm’s ability to adapt and thrive despite individual limitations. Collectively, these studies advance the implementation of swarm intelligence in federated settings, demonstrating how decentralized, collaborative efforts can lead to robust, scalable, and efficient outcomes in the training of sophisticated models.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Security and Privacy in Federated LLM</h3>

<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Security and Privacy in Federated LLM</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<td id="S3.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T5.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Contributions</span></span>
</span>
</td>
<td id="S3.T5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T5.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Technique</span></span>
</span>
</td>
<td id="S3.T5.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T5.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T5.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">LM used</span></span>
</span>
</td>
<td id="S3.T5.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T5.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T5.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T5.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T5.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T5.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.2.2" class="ltx_tr">
<th id="S3.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T5.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.2.2.1.1" class="ltx_p" style="width:113.8pt;">Analyzes potential privacy leaks and suggests enhanced cryptographic measures and robust aggregation algorithms.</span>
</span>
</td>
<td id="S3.T5.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;">Cryptographic measures, robust aggregation</span>
</span>
</td>
<td id="S3.T5.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.2.4.1.1" class="ltx_p" style="width:56.9pt;">GPT-2, RoberTa, etc.</span>
</span>
</td>
<td id="S3.T5.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.2.5.1.1" class="ltx_p" style="width:56.9pt;">IMDB, Yelp, etc.</span>
</span>
</td>
<td id="S3.T5.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T5.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;">May involve high computational overhead and complexity in implementation.</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.3.3" class="ltx_tr">
<th id="S3.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S3.T5.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">Introduces an open-source framework incorporating SMPC and homomorphic encryption to enhance security.</span>
</span>
</td>
<td id="S3.T5.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.3.3.1.1" class="ltx_p" style="width:56.9pt;">SMPC, homomorphic encryption</span>
</span>
</td>
<td id="S3.T5.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.3.4.1.1" class="ltx_p" style="width:56.9pt;">GPT-4, LLaMa-2, etc.</span>
</span>
</td>
<td id="S3.T5.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.3.5.1.1" class="ltx_p" style="width:56.9pt;">Alpaca, UltraFeedback, etc.</span>
</span>
</td>
<td id="S3.T5.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T5.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">Potential performance degradation due to encryption overhead.</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.4.4" class="ltx_tr">
<th id="S3.T5.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S3.T5.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">Demonstrates practical attacks for recovering private text and discusses countermeasures.</span>
</span>
</td>
<td id="S3.T5.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.4.3.1.1" class="ltx_p" style="width:56.9pt;">Attack simulation, countermeasure development</span>
</span>
</td>
<td id="S3.T5.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.4.4.1.1" class="ltx_p" style="width:56.9pt;">GPT-2</span>
</span>
</td>
<td id="S3.T5.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T5.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.4.5.1.1" class="ltx_p" style="width:56.9pt;">WikiText-103</span>
</span>
</td>
<td id="S3.T5.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T5.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">Countermeasures may not fully prevent leakage and could affect model efficiency.</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.5.5" class="ltx_tr">
<th id="S3.T5.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">4</th>
<td id="S3.T5.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T5.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.5.2.1.1" class="ltx_p" style="width:113.8pt;">Investigates unintended memorization and proposes best practices to reduce data leakage.</span>
</span>
</td>
<td id="S3.T5.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T5.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.5.3.1.1" class="ltx_p" style="width:56.9pt;">Analysis of memorization, training modifications</span>
</span>
</td>
<td id="S3.T5.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T5.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">Bert-based</span>
</span>
</td>
<td id="S3.T5.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T5.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.5.5.1.1" class="ltx_p" style="width:56.9pt;">Stack Overflow</span>
</span>
</td>
<td id="S3.T5.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T5.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">Best practices may not be universally effective across all model types and scenarios.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In this section, we explore the challenges and solutions associated with maintaining privacy and security while training large language models using federated learning.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Vu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> provide an in-depth analysis of how privacy can be compromised during the federated training of large language models. It identifies specific vulnerabilities where sensitive data could potentially be extracted by adversaries through model inversion attacks and other inference techniques. The study systematically examines the types of information that can be leaked and under what conditions, offering insights into the limitations of current privacy-preserving methods like differential privacy. The paper suggests enhanced cryptographic measures and more robust aggregation algorithms to mitigate these risks, setting a foundational context for the need for advanced privacy-preserving techniques in federated learning.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Ye et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> introduce an open-source framework specifically designed to support the development and deployment of federated learning models with an emphasis on privacy and security. This paper describes the architecture of the framework, which includes built-in support for secure multi-party computation (SMPC) and homomorphic encryption, technologies that allow computations to be performed on encrypted data. The framework aims to provide a practical solution to the privacy concerns highlighted in the first paper, facilitating the secure aggregation of updates from multiple clients without exposing their raw data, thereby enhancing the overall security of the federated learning process.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Building on the concerns about privacy leaks, Gupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> demonstrate a practical attack scenario where sensitive information is extracted from a federated language model. The researchers show how certain training techniques and model configurations can inadvertently lead to the memorization of private text, which can then be recovered by malicious participants. The paper tests various scenarios and configurations, providing empirical evidence of the risks involved. It also discusses potential countermeasures, such as more stringent data sanitization and the use of privacy-preserving architectures, to prevent such vulnerabilities.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.1" class="ltx_p">Thakkar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> complement the third by further investigating how language models, especially those trained under federated learning conditions, may unintentionally memorize and expose sensitive data. It delves into the mechanics of memorization in neural networks, exploring how different factors like model size, dataset diversity, and training duration affect the risk of data leakage. The study proposes a set of best practices and modifications to the training algorithms that can help reduce the likelihood of such memorization without significantly impacting model performance.</p>
</div>
<div id="S3.SS5.p6" class="ltx_para">
<p id="S3.SS5.p6.1" class="ltx_p">Together, these papers create a layered understanding of the privacy and security challenges in federated learning of LLMs. Starting from identifying potential privacy leaks, moving to a framework designed to address these leaks through encryption and secure computation, and finally, demonstrating practical attack vectors and offering solutions to mitigate these issues, the narrative crafted by these studies underscores the complexity of securely training language models in a distributed manner. Each paper builds on the insights of its predecessors, collectively advancing the field towards more secure and private federated learning environments. This progression not only highlights the vulnerabilities but also charts a path toward resolving them, ensuring that federated learning can be safely employed in sensitive applications.</p>
</div>
<div id="S3.SS5.p7" class="ltx_para">
<p id="S3.SS5.p7.1" class="ltx_p">The papers in this section contribute to the principles of swarm intelligence in the context of federated learning for large language models by addressing complex challenges related to security and privacy—key aspects for collaborative and distributed systems. First, by analyzing potential privacy leaks, the initial paper lays the groundwork for understanding vulnerabilities akin to assessing environmental risks in a swarm. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> introduces a robust framework with advanced encryption methods, enhancing collective security without centralized control, much like a swarm’s distributed processing enhances collective resilience. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> showcases practical attack simulations and mitigations, reinforcing the swarm’s adaptability and response to threats. Finally, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>’s investigation into unintended data memorization offers strategies to minimize such risks, promoting safer learning environments. Collectively, these studies embody swarm intelligence by improving the collective learning and decision-making processes in a decentralized, secure, and private manner, ensuring the swarm (network of nodes) remains robust against internal and external adversarial influences.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Learned Lessons and Open Challenges</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we show the learned lessons, open challenges, and future research directions derived from the review and analysis.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Learned Lessons</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Efficiency in Distributed Environments: Implementing LLMs within federated frameworks has demonstrated significant improvements in computational efficiency. Techniques such as model compression and data pruning have proven effective in reducing the computational load on individual nodes, thereby enhancing overall system efficiency.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Enhanced Scalability: The application of swarm principles has shown that LLMs can be effectively scaled across a larger network of distributed nodes. This scalability is crucial in handling vast amounts of decentralized data, ensuring the LLMs remain effective even as the network expands.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Emergence of Swarm Intelligence: The self-organizing nature of swarm intelligence has been successfully harnessed to improve collective decision-making processes in LLMs. This emergence has been pivotal in enhancing the adaptability of models to new, unforeseen conditions without centralized control.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Improved Privacy Protections: Federated learning inherently supports privacy by design, allowing for the training of LLMs without requiring the sharing of raw data. Advanced cryptographic techniques and secure multi-party computation have further bolstered these protections, ensuring data remains confidential even in collaborative environments.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Robustness and Fault Tolerance: The decentralized approach has inherently improved the robustness and fault tolerance of systems. Swarm-based LLMs are less susceptible to single points of failure, maintaining functionality across the network even when individual nodes encounter issues.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Open Challenges and Future Directions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Communication and Coordination Overhead: As the number of nodes increases, so does the complexity of managing communication and coordination among them. Future research needs to focus on developing more efficient communication protocols and algorithms that minimize latency and bandwidth usage while maintaining model accuracy.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Consistency in Learning Across Nodes: Ensuring consistent learning outcomes across heterogeneous nodes remains challenging. Investigating new methods for achieving better model convergence and addressing the non-IID nature of distributed data sources is critical.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Security Against Adversarial Attacks: While privacy is enhanced, security against adversarial attacks in a federated setting continues to be a challenge. Research into more robust defense mechanisms to protect against such attacks, particularly in the context of LLMs, is urgently needed.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Handling of Data Heterogeneity: The diverse and distributed nature of data in swarm-based systems presents significant challenges in training uniform and unbiased models. Advanced strategies for effective data sampling and aggregation are required to address these issues.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Exploration of Hybrid Models: There is potential in exploring hybrid models that combine the strengths of both centralized and decentralized learning frameworks. Such models could potentially leverage the efficiency of centralized processing with the robustness and privacy of decentralized approaches.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSION</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This survey has examined the innovative convergence of swarm intelligence, federated learning, and LLMs, revealing a transformative approach to decentralized AI systems. We have demonstrated that the synergy between these technologies not only enhances the efficiency and scalability of AI models but also significantly bolsters their privacy and security features. While the journey thus far has illuminated various promising strategies and noteworthy achievements, it has also highlighted substantial challenges that remain in communication efficiency, data consistency, and security. The future of integrating LLMs with swarm intelligence in a federated context looks bright but requires continued interdisciplinary efforts to refine the technologies and methodologies. As this field evolves, it holds the potential to revolutionize how AI systems are designed and implemented, promising more robust, adaptive, and privacy-preserving models that are capable of operating autonomously across distributed environments.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">ACKNOWLEDGMENT</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">TBA</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
models are unsupervised multitask learners,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, 2019.
[Online]. Available: https://openai.com/blog/better-language-models/

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep
bidirectional transformers for language understanding,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers)</em>.   Association for
Computational Linguistics, 2019, pp. 4171–4186. [Online]. Available:
https://aclanthology.org/N19-1423

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Shanahan, K. McDonell, and L. Reynolds, “Role play with large language
models,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 623, no. 7987, pp. 493–498, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Sun, Y. Han, Z. Zhao, D. Ma, Z. Shen, B. Chen, L. Chen, and K. Yu,
“Scieval: A multi-level large language model evaluation benchmark for
scientific research,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on
Artificial Intelligence</em>, vol. 38, no. 17, 2024, pp. 19 053–19 061.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan, S. Mukherjee,
V. Garg, R. Sarveswara, K. Händler, P. Pickkers, N. A. Aziz
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Swarm learning for decentralized and confidential clinical
machine learning,” <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Nature</em>, vol. 594, no. 7862, pp. 265–270, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Tang, H. Duan, and S. Lao, “Swarm intelligence algorithms for multiple
unmanned aerial vehicles collaboration: A comprehensive review,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Review</em>, vol. 56, no. 5, pp. 4295–4327, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Alqahtani, H. A. Badreldin, M. Alrashed, A. I. Alshaya, S. S. Alghamdi,
K. bin Saleh, S. A. Alowais, O. A. Alshaya, I. Rahman, M. S. Al Yami
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The emergent role of artificial intelligence, natural
learning processing, and large language models in higher education and
research,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Research in Social and Administrative Pharmacy</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. D. Lund, T. Wang, N. R. Mannuru, B. Nie, S. Shimray, and Z. Wang, “Chatgpt
and a new academic reality: Artificial intelligence-written research papers
and the ethics of the large language models in scholarly publishing,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of the Association for Information Science and Technology</em>,
vol. 74, no. 5, pp. 570–581, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. He, H. Yao, T. Mai, F. Wang, and M. Guizani, “Three-stage stackelberg game
enabled clustered federated learning in heterogeneous uav swarms,”
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Vehicular Technology</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Zhang, Y. Yang, Y. Dai, Q. Wang, Y. Yu, L. Qu, and Z. Xu, “Fedpetuning:
When federated learning meets the parameter-efficient tuning methods of
pre-trained language models,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association of
Computational Linguistics 2023</em>.   Association for Computational Linguistics (ACL), 2023, pp. 9963–9977.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Chai, L. Wang, L. Yang, J. Zhang, K. Chen, and Q. Yang, “A survey for
federated learning evaluations: Goals and measures,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Knowledge and Data Engineering</em>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Tang, G. Liu, and Q. Pan, “A review on representative swarm intelligence
algorithms for solving optimization problems: Applications and trends,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE/CAA Journal of Automatica Sinica</em>, vol. 8, no. 10, pp. 1627–1643,
2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Kennedy, “Swarm intelligence,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Handbook of nature-inspired and
innovative computing: integrating classical models with emerging
technologies</em>.   Springer, 2006, pp.
187–219.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. G. Hinchey, R. Sterritt, and C. Rouff, “Swarms and swarm intelligence,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Computer</em>, vol. 40, no. 4, pp. 111–113, 2007.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Birhane, A. Kasirzadeh, D. Leslie, and S. Wachter, “Science in the age of
large language models,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Nature Reviews Physics</em>, vol. 5, no. 5, pp.
277–280, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer in
transformer,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
vol. 34, pp. 15 908–15 919, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
P. Xu, X. Zhu, and D. A. Clifton, “Multimodal learning with transformers: A
survey,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. S. Liang, Q. V. Le,
T. Ma, and A. W. Yu, “Doremi: Optimizing data mixtures speeds up language
model pretraining,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Z. Ma, H. Zhang, and J. Liu, “Mm-rnn: A multimodal rnn for precipitation
nowcasting,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>,
2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Wang, X. Su, and Z. Ding, “Long-term traffic prediction based on lstm
encoder-decoder architecture,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent
Transportation Systems</em>, vol. 22, no. 10, pp. 6561–6571, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre,
I. Heintz, and D. Roth, “Recent advances in natural language processing via
large pre-trained language models: A survey,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>,
vol. 56, no. 2, pp. 1–40, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Xu, H. Du, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, A. Jamalipour,
D. I. Kim, X. Shen <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Unleashing the power of edge-cloud
generative ai in mobile networks: A survey of aigc services,” <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">IEEE
Communications Surveys &amp; Tutorials</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Chen, A. T. Suresh, R. Mathews, A. Wong, C. Allauzen, F. Beaufays, and
M. Riley, “Federated learning of n-gram language models,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Conference on Computational Natural Language
Learning (CoNLL)</em>, 2019, pp. 121–130.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Singhal, H. Sidahmed, Z. Garrett, S. Wu, J. Rush, and S. Prakash,
“Federated reconstruction: Partially local federated learning,”
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp.
11 220–11 232, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding, and
J. Zhou, “Federatedscope-llm: A comprehensive package for fine-tuning large
language models in federated learning,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2309.00363</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B. Letaief,
“Large language models empowered autonomous edge ai for connected
intelligence,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Jiang, X. Liu, and C. Fan, “Low-parameter federated learning with large
language models,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13896</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. H. Ro, S. Bhojanapalli, Z. Xu, Y. Zhang, and A. T. Suresh, “Efficient
language model architectures for differentially private federated learning,”
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.08100</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T. Che, J. Liu, Y. Zhou, J. Ren, J. Zhou, V. Sheng, H. Dai, and D. Dou,
“Federated learning of large language models with parameter-efficient prompt
tuning and adaptive optimization,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing</em>, 2023, pp.
7871–7888.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
F. Lai, Y. Dai, S. Singapuram, J. Liu, X. Zhu, H. Madhyastha, and M. Chowdhury,
“Fedscale: Benchmarking model and system performance of federated learning
at scale,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2022, pp. 11 814–11 827.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Xu, Y. Wu, D. Cai, X. Li, and S. Wang, “Federated fine-tuning of
billion-sized language models across mobile devices,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2308.13894</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Tian, Y. Wan, L. Lyu, D. Yao, H. Jin, and L. Sun, “Fedbert: When federated
learning meets pre-training,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems
and Technology (TIST)</em>, vol. 13, no. 4, pp. 1–26, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Agarwal, M. Rezagholizadeh, and P. Parthasarathi, “Practical takes on
federated learning with pretrained language models,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Findings of
the Association for Computational Linguistics: EACL 2023</em>, 2023, pp.
454–471.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
O. Weller, M. Marone, V. Braverman, D. Lawrie, and B. Van Durme, “Pretrained
models for multilingual federated learning,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2022 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies</em>, 2022, pp.
1413–1421.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Malaviya, M. Shukla, and S. Lodha, “Reducing communication overhead in
federated learning for pre-trained language models using parameter-efficient
finetuning,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Conference on Lifelong Learning Agents</em>.   PMLR, 2023, pp. 456–469.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Fan, Y. Kang, G. Ma, W. Chen, W. Wei, L. Fan, and Q. Yang, “Fate-llm: A
industrial grade federated learning framework for large language models,”
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10049</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Tan, G. Long, J. Ma, L. Liu, T. Zhou, and J. Jiang, “Federated learning
from pre-trained models: A contrastive learning approach,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in
neural information processing systems</em>, vol. 35, pp. 19 332–19 344, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Z. Charles, Z. Garrett, Z. Huo, S. Shmulyian, and V. Smith, “On large-cohort
training for federated learning,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 34, pp. 20 461–20 475, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Hilmkil, S. Callh, M. Barbieri, L. R. Sütfeld, E. L. Zec, and O. Mogren,
“Scaling federated learning for fine-tuning of large language models,” in
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Applications of Natural Language to
Information Systems</em>.   Springer, 2021,
pp. 15–23.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Ro, T. Breiner, L. McConnaughey, M. Chen, A. Suresh, S. Kumar, and
R. Mathews, “Scaling language model size in cross-device federated
learning,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First Workshop on Federated Learning
for Natural Language Processing (FL4NLP 2022)</em>, 2022, pp. 6–20.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Vu, T. Nguyen, M. T. Thai <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Analysis of privacy leakage in
federated large language models,” in <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">International Conference on
Artificial Intelligence and Statistics</em>.   PMLR, 2024, pp. 1423–1431.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
R. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y. Xu, Y. Du, Y. Wang, and S. Chen,
“Openfedllm: Training large language models on decentralized private data
via federated learning,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06954</em>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S. Gupta, Y. Huang, Z. Zhong, T. Gao, K. Li, and D. Chen, “Recovering private
text in federated learning of language models,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 35, pp. 8130–8143, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
O. D. Thakkar, S. Ramaswamy, R. Mathews, and F. Beaufays, “Understanding
unintended memorization in language models under federated learning,” in
<em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Privacy in Natural Language
Processing</em>, 2021, pp. 1–10.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.09830" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.09831" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.09831">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.09831" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.09832" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:29:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
