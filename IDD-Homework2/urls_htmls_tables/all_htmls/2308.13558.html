<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.13558] Federated Learning for Computer Vision</title><meta property="og:description" content="Computer Vision (CV) is playing a significant role in transforming society by utilizing machine learning (ML) tools for a wide range of tasks. However, the need for large-scale datasets to train ML models creates chall‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning for Computer Vision">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning for Computer Vision">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.13558">

<!--Generated on Wed Feb 28 10:37:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Computer vision (CV),  Federated learning (FL),  Blockchain,  Horizontal federated learning (HFL),  Vertical federated learning (VFL).
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning for Computer Vision</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yassine Himeur1,
Iraklis Varlamis2,
Hamza Kheddar3,
Abbes Amira4,5,
Shadi Atalla1,
Yashbir Singh6,
Faycal Bensaali3 and
Wathiq Mansoor1

<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1
College of Engineering and Information Technology, University of Dubai, Dubai, UAE
</span>
<span class="ltx_contact ltx_role_affiliation">2Department of Informatics and Telematics, Harokopio University of Athens, Greece
</span>
<span class="ltx_contact ltx_role_affiliation">3LSEA Laboratory, Electrical Engineering Department, University of Medea, Algeria
</span>
<span class="ltx_contact ltx_role_affiliation">4Department of Computer Science, University of Sharjah, UAE
</span>
<span class="ltx_contact ltx_role_affiliation">5Institute of Artificial Intelligence, De Montfort University, Leicester, United Kingdom
</span>
<span class="ltx_contact ltx_role_affiliation">6Department of Radiology, Mayo Clinic, Rochester, MN, USA
</span>
<span class="ltx_contact ltx_role_affiliation">6Electrical Engineering Department, Qatar University, Qatar
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Computer Vision (CV) is playing a significant role in transforming society by utilizing machine learning (ML) tools for a wide range of tasks. However, the need for large-scale datasets to train ML models creates challenges for centralized ML algorithms. The massive computation loads required for processing and the potential privacy risks associated with storing and processing data on central cloud servers put these algorithms under severe strain. To address these issues, federated learning (FL) has emerged as a promising solution, allowing privacy preservation by training models locally and exchanging them to improve overall performance. Additionally, the computational load is distributed across multiple clients, reducing the burden on central servers.
This paper presents, to the best of the authors‚Äô knowledge, the first review discussing recent advancements of FL in CV applications, comparing them to conventional centralized training paradigms. It provides an overview of current FL applications in various CV tasks, emphasizing the advantages of FL and the challenges of implementing it in CV. To facilitate this, the paper proposes a taxonomy of FL techniques in CV, outlining their applications and security threats. It also discusses privacy concerns related to implementing blockchain in FL schemes for CV tasks and summarizes existing privacy preservation methods. Moving on, the paper identifies open research challenges and potential future research directions to further exploit the potential of FL and blockchain in CV applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Computer vision (CV), Federated learning (FL), Blockchain, Horizontal federated learning (HFL), Vertical federated learning (VFL).

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the emergence and evolution of machine learning (ML) has carved out a distinct niche for itself within the broader realm of artificial intelligence (AI). Specifically, ML emphasizes the design, development, and training of algorithms and software agents to sift through, interpret, and make data-driven decisions based on patterns hidden within vast datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. These artificial agents are increasingly being equipped with multifaceted sensory capabilities that allow them to interact more intuitively with their environment. Among these, vision stands out as the most profound, granting machines the ability to ‚Äôsee‚Äô and ‚Äôunderstand‚Äô the world around them in ways comparable to human cognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Given the intrinsic importance of visual data, computer vision (CV) has rapidly risen to prominence within the ML community. CV seeks to enable machines to interpret and derive meaningful conclusions from visual data, mimicking the human ability to recognize, process, and respond to visual stimuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This capability not only empowers artificial agents to better grasp their surroundings but also paves the way for more informed decision-making processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. By incorporating CV techniques, artificial agents can move beyond mere data analysis, delving into a more profound comprehension of the visual realm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The integration of such insights holds the potential to revolutionize various domains by enhancing the depth of understanding and the efficacy of decision-making processes in AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Understanding the contents and concepts of an image involves a significant amount of information which is connected with image segmentation, extraction of features and objects, and synthesis of the scene as a whole <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Although the CV and perception task is reflexively performed by humans, grace to their ability for abstraction, it is still quite complex for artificial agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. It is also hard for humans to explain how they perceive the world and develop ML modules that behave in a similar manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
The four main techniques that are employed for training machines to perform CV tasks are either based on statistics (i.e. on patterns learned from large training datasets), on the logic expressed in the form of rules, on deep neural networks (DNNs) that capture the non-linear relations between image features and the final decision or on genetic and evolutionary algorithms that combine multiple decisions in order to find the one that maximizes the overall performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The huge advancements and impressive results in the performance of CV algorithms in tasks such as image classification, image segmentation, object detection, and scene perception, came from a shift from signal processing methods to solutions that rely on deep learning (DL) methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The biggest challenge for DL methods is that they require huge computational resources and energy for training, which in turn makes them hardly applicable in many application setups, where decisions have to be taken on the edge, using low resources and limited power (e.g. in drones, mobile phones, etc.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
For this purpose low-power CV challenges have been established and new energy-efficient algorithms and light-weight DL architectures have been proposed, such as reservoir computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, or random neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another challenge for CV systems is the protection of user privacy.
Collecting millions of people‚Äôs images and videos poses serious privacy risks, which must be seriously considered <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. First of all, the right to be forgotten is constantly violated by companies that collect visual data and retain it permanently or use it for multiple purposes. Also, through numerous flaws, such delicate visual data could be exploited or exposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Secondly, the privacy-preserving analysis of images and videos from video surveillance applications, where CV methods are used to detect violations (e.g. mask-wearing, distance keeping, etc.) in public spaces can be quite challenging too <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Such videos frequently include inadvertently caught private objects including faces, car plates, computer screens, and more. The ability of methods to detect and even identify humans and other objects in unconstrained environments can put at risk the anonymity of people in monitored places and, if not used properly, can become a threat to citizen privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rise of edge computing architectures has introduced a new potential to privacy-preserving CV, since with the proper use of the limited edge resources, it has become possible to perform basic CV tasks without transferring sensitive data to the cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Sharing the trained model rather than releasing the actual data also helps to retain the privacy of the data, without losing on prediction performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Due to their built-in privacy-preserving features, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and split learning (SL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> are the two ML techniques, that perform on visual data on a distributed manner and have attracted the interest of researchers in the CV field. Such approaches assume a client-server (edge-cloud) architecture, where the clients usually have fewer resources than the server. The clients train their models individually, using their own data, and then exchange their models either with the server or among them in order to synchronize what they learned <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The ML models are trained locally on the client (edge) devices and this happens in parallel, as long as the clients receive and process training data. Periodically, they exchange models and aggregate them following simple or more sophisticated strategies, which may involve additional training on the cloud for avoiding the bias introduced in the clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. In the case of SL the training of the model layers is also split between the edge and the cloud. Only a few layers are trained on the edge, in order to facilitate training with little resources, and the remaining layers are trained on the cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">A risk that emerges from this distributed training process for CV models is the exposure to malicious users that intentionally introduce noise or falsified models in order to bias the FL model for their benefit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Adversarial training techniques can be employed to strengthen the FL models, but even adversarial training has leakages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The use of blockchain technologies can help mitigate many of the threats of data or model-sharing methods. All the model-sharing activities of a node that is allowed to share are stored as transactions in the blockchain and all information about the providers‚Äô profiles is also stored in the blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. In this multi-party setup, when a node needs to update its model by using the models of other nodes, first performs a request for models to its neighbouring nodes, then validates the nodes by checking the blockchain and retrieves their models from the blockchain. The resulting FL model is consequently stored on the blockchain in a new transaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.5.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.6.2" class="ltx_text ltx_font_italic">Survey Methodology</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">FL is emerging as a powerful machine learning paradigm that allows for the training of AI models across numerous decentralized devices while maintaining data privacy. In essence, FL enables devices to collaboratively learn a shared model without having to share raw data, a feature that holds immense potential for applications in diverse domains, particularly in CV. In CV, the applications of FL range from object detection and image classification to semantic segmentation, among others, with an array of potential real-world use cases in sectors such as healthcare, autonomous driving, and surveillance systems. However, the adoption of FL in CV is not without its challenges. Issues related to model performance, communication efficiency, data heterogeneity, and privacy preservation need to be addressed and carefully managed. This systematic literature review aims to delve deep into these challenges, exploring the progress, limitations, and future directions of applying FL in the realm of CV. The review will address several research questions that encapsulate the core facets of this intriguing intersection of FL and CV, which can summarized in Table <a href="#S1.T1" title="TABLE I ‚Ä£ I-A Survey Methodology ‚Ä£ I Introduction ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Research questions covered in this review.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1.1" class="ltx_p"><span id="S1.T1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ#</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.2.1.1" class="ltx_p"><span id="S1.T1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Question</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.3.1.1" class="ltx_p"><span id="S1.T1.1.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Objective</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.1.1.1" class="ltx_p"><span id="S1.T1.1.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ1</span></span>
</span>
</td>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.2.1.1" class="ltx_p"><span id="S1.T1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">What is the primary motivation behind conducting a review of FL for CV?</span></span>
</span>
</td>
<td id="S1.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.3.1.1" class="ltx_p"><span id="S1.T1.1.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Present the underlying reasons or driving factors for undertaking a comprehensive review specifically on the application of FL in the domain of CV.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.1.1.1" class="ltx_p"><span id="S1.T1.1.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ2</span></span>
</span>
</td>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.2.1.1" class="ltx_p"><span id="S1.T1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">How is the problem of data privacy handled in FL for CV?</span></span>
</span>
</td>
<td id="S1.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.3.1.1" class="ltx_p"><span id="S1.T1.1.3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">Investigate the mechanisms, strategies, or solutions implemented within the FL framework, specifically for CV tasks, to address and ensure data privacy.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.1.1.1" class="ltx_p"><span id="S1.T1.1.4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ3</span></span>
</span>
</td>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.2.1.1" class="ltx_p"><span id="S1.T1.1.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">How is model performance measured and optimized in FL scenarios?</span></span>
</span>
</td>
<td id="S1.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.3.1.1" class="ltx_p"><span id="S1.T1.1.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">Delve into the methods, metrics, and techniques used to evaluate the efficacy of models trained using FL.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.1.1.1" class="ltx_p"><span id="S1.T1.1.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ4</span></span>
</span>
</td>
<td id="S1.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.2.1.1" class="ltx_p"><span id="S1.T1.1.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">How are issues related to data distribution across nodes (non-IID data) managed?</span></span>
</span>
</td>
<td id="S1.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.3.1.1" class="ltx_p"><span id="S1.T1.1.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">Understand the strategies, techniques, and solutions employed to handle the challenges posed by non-Independent and Identically Distributed (non-IID) data in decentralized or FL environments.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.5.1.1.1" class="ltx_p"><span id="S1.T1.1.6.5.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ5</span></span>
</span>
</td>
<td id="S1.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.5.2.1.1" class="ltx_p"><span id="S1.T1.1.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">What are the strategies to handle communication efficiency in FL?</span></span>
</span>
</td>
<td id="S1.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.5.3.1.1" class="ltx_p"><span id="S1.T1.1.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">Discuss the various techniques and methods implemented to optimize or enhance the communication process between nodes or devices in a FL system.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.6.1.1.1" class="ltx_p"><span id="S1.T1.1.7.6.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ6</span></span>
</span>
</td>
<td id="S1.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.6.2.1.1" class="ltx_p"><span id="S1.T1.1.7.6.2.1.1.1" class="ltx_text" style="font-size:90%;">What are the approaches to ensure robustness and security in FL?</span></span>
</span>
</td>
<td id="S1.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.6.3.1.1" class="ltx_p"><span id="S1.T1.1.7.6.3.1.1.1" class="ltx_text" style="font-size:90%;">Investigate the various methodologies, techniques, or safeguards that are employed to protect FL systems against potential vulnerabilities, attacks, or failures.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.7.1.1.1" class="ltx_p"><span id="S1.T1.1.8.7.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ7</span></span>
</span>
</td>
<td id="S1.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.7.2.1.1" class="ltx_p"><span id="S1.T1.1.8.7.2.1.1.1" class="ltx_text" style="font-size:90%;">How are FL systems for CV deployed in the real world?</span></span>
</span>
</td>
<td id="S1.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.7.3.1.1" class="ltx_p"><span id="S1.T1.1.8.7.3.1.1.1" class="ltx_text" style="font-size:90%;">Explore and understand the practical applications, implementation challenges, infrastructure requirements, and real-world scenarios where FL techniques are used specifically in the domain of CV.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.8.1.1.1" class="ltx_p"><span id="S1.T1.1.9.8.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ8</span></span>
</span>
</td>
<td id="S1.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.8.2.1.1" class="ltx_p"><span id="S1.T1.1.9.8.2.1.1.1" class="ltx_text" style="font-size:90%;">What are the challenges and limitations in applying FL to CV tasks?</span></span>
</span>
</td>
<td id="S1.T1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.8.3.1.1" class="ltx_p"><span id="S1.T1.1.9.8.3.1.1.1" class="ltx_text" style="font-size:90%;">Identify the potential difficulties, drawbacks, and constraints encountered when using FL in the context of CV.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:22.8pt;">
<span id="S1.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.9.1.1.1" class="ltx_p"><span id="S1.T1.1.10.9.1.1.1.1" class="ltx_text" style="font-size:90%;">RQ9</span></span>
</span>
</td>
<td id="S1.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:170.7pt;">
<span id="S1.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.9.2.1.1" class="ltx_p"><span id="S1.T1.1.10.9.2.1.1.1" class="ltx_text" style="font-size:90%;">What are the trends and future directions in FL for CV?</span></span>
</span>
</td>
<td id="S1.T1.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:284.5pt;">
<span id="S1.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.9.3.1.1" class="ltx_p"><span id="S1.T1.1.10.9.3.1.1.1" class="ltx_text" style="font-size:90%;">Explore the current trajectories, innovations, and potential advancements in the application of FL to CV.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">During our systematic review, we initially collected <span id="S1.SS1.p2.1.1" class="ltx_text" style="color:#000000;">912</span> papers. To ensure the relevance of the literature, we applied basic criteria such as title, abstract, and topic alignment with our research question. We then established detailed inclusion and exclusion criteria to streamline the selection process. The inclusion criteria encompassed papers proposing FL solutions, discussing the applications of FL in CV, implementing techniques, or proposing enhanced versions of FL. Conversely, the exclusion criteria were applied to exclude publications that did not specifically use FL, it was only mentioned in the literature review , used for comparison purposes or used other research sectors, etc.
One author took the lead in the selection strategy and conducted the initial screening, ensuring consistency with our research theme.
After removing duplicates, we identified <span id="S1.SS1.p2.1.2" class="ltx_text" style="color:#000000;">385</span> unique articles. We then conducted a thorough assessment of the remaining articles by carefully reviewing titles, abstracts, and conclusions. Based on this assessment, we narrowed down the selection to <span id="S1.SS1.p2.1.3" class="ltx_text" style="color:#000000;">255</span> articles that exhibited relevance based on title and abstract. In the subsequent stage, we applied the specified inclusion and exclusion criteria to the remaining articles, leading to the exclusion of certain studies that did not meet our criteria.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">During the selection process, the following exclusion criteria were adopted: (i) duplicate records, (ii)
papers that did not comment on the performance of FL in CV, (iii) papers related to the implementation of similar but not FL techniques, (iv) papers related to research sectors other than CV, and (v) papers written in languages other than English.
Furthermore, the following inclusion criteria were used to select relevant literature: (i) proposes an improvement an FL-based solution in CV, (ii) addresses the privacy and security issues in CV using FL techniques, (iii) measured and optimized model performance in FL, (iv) discusses the deployment of FL systems in real-world CV applications</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">By applying these exclusion and inclusion criteria, we ensured that the selected articles provided insights, solutions, or advancements specifically related to the filter bubble phenomenon in RSss. This process resulted in a final selection of 28 articles that met our inclusion criteria. In order to ensure a comprehensive review, we conducted a reference scan of the selected articles, which led us to identify an additional 6 relevant papers. Consequently, a total of 34 articles were included in our systematic review on the existence of the filter bubble.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.5.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.6.2" class="ltx_text ltx_font_italic">Related reviews</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">While several studies have recently addressed the topic of FL and its various applications, no work has specifically focused on reviewing the applications of FL in CV. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> discusses various applications of FL in communication and networking systems, including wireless sensor networks, IoT, and vehicular networks, and highlight some challenges that need to be addressed to fully realize the potential of FL, such as security concerns, communication overheads, and model heterogeneity. Moving on, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> provides an extensive examination of FL while reviewing the growing concerns about data privacy and security, (ii) offers design considerations and solutions; (iii) Highlights different privacy-preserving approaches utilized in FL; and (iv) it suggests potential future paths. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> discusses (i) the key enabling technologies and protocols used in FL such as secure aggregation, differential privacy, homomorphic encryption, etc.; and (ii) challenges faced by FL such as communication overheads, heterogeneity of devices and data, privacy concerns, and security issues.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Besides, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> provides a thorough exploration of the increasing applications of FL in IoT networks. It begins by acknowledging the limitations of centralized AI methods reliant on data collection and privacy concerns. FL, as a collaborative and distributed AI approach, offers a solution for intelligent IoT applications without data-sharing needs. The authors showcase various use cases in healthcare, transportation, etc., where FL can be utilized in IoT networks. Additionally, they address the challenges and potential opportunities linked with FL.
Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> discuss IoT-based FL and its applications and summarizes the technical aspects, key contributions, and limitations of each reference work. Moreover, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> introduces a novel classification of FL subjects and research domains and encompasses comprehensive taxonomies that address various challenging aspects, contributions, and trends in the literature. Additionally, the survey delves into significant challenges and potential research paths for developing more robust FL systems.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS3.5.1.1" class="ltx_text">I-C</span> </span><span id="S1.SS3.6.2" class="ltx_text ltx_font_italic">Contribution </span>
</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">By contrast to the above-mentioned reviews, this study presents to the best of the authors‚Äô knowledge the first review of FL-based contributions in CV. Typically, it (i) offers a comparative perspective by presenting related reviews, thereby situating its contributions in the broader scholarly conversation; (ii) provides foundational knowledge about FL, including its definition and problem formulation; (iii) discusses different aggregation methods, covering averaging aggregation, progressive Fourier aggregation, and FedGKT aggregation, (iv) presents a detailed overview of privacy technologies associated with FL, including the Secure MPC model, differential privacy, and homomorphic encryption, (v) distinguishes and elaborates on supervised, unsupervised, and semi-supervised FL and evaluates CV schemes based on FL.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">Moreover, it offers an in-depth look at how FL is employed in various CV tasks such as object detection, video surveillance, healthcare, autonomous driving, and more.
Moving on, it addresses significant challenges and concerns in the FL domain, such as communication overhead, device compatibility, and human bias. Additionally, by pinpointing the specific obstacles that researchers and practitioners might face, the review contributes to the growing body of knowledge about the intersection of FL and CV.
Lastly, beyond its retrospective lens, the review also peers into the future, suggesting potential avenues of research and development in the field.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background of FL</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">FL enables the training of AI models without the sharing of training data.
While most AI has been trained on data gathered and crunched in a unique repository, today‚Äôs ML algorithms are shifting towards a decentralized scheme. ML models can collaboratively be trained on edge devices, e.g., mobile phones, laptops, or private servers.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Definition</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">By overviewing recent studies of FL frameworks proposed in CV applications, FL models can be categorized into
three types: 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Horizontal FL (HFL):</span> refers to training a shared overall model by the clients using their datasets, which are characterized by the same feature space but have different sample spaces, as portrayed in Fig. <a href="#S2.F1" title="Figure 1 ‚Ä£ II-A Definition ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In this regard, local FL participants can adopt
the same AI model (e.g., neural network-NN) for training their datasets. Afterwards, the
server combines the local updates communicated by the local clients to develop the overall update without accessing local data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.22" class="ltx_p">Let us consider a HFL scenario with <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">N</annotation></semantics></math> parties. Each party <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">i</annotation></semantics></math>, where <math id="S2.SS1.p3.3.m3.4" class="ltx_Math" alttext="i\in\{1,2,\dots,N\}" display="inline"><semantics id="S2.SS1.p3.3.m3.4a"><mrow id="S2.SS1.p3.3.m3.4.5" xref="S2.SS1.p3.3.m3.4.5.cmml"><mi id="S2.SS1.p3.3.m3.4.5.2" xref="S2.SS1.p3.3.m3.4.5.2.cmml">i</mi><mo id="S2.SS1.p3.3.m3.4.5.1" xref="S2.SS1.p3.3.m3.4.5.1.cmml">‚àà</mo><mrow id="S2.SS1.p3.3.m3.4.5.3.2" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S2.SS1.p3.3.m3.4.5.3.2.1" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml">{</mo><mn id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">1</mn><mo id="S2.SS1.p3.3.m3.4.5.3.2.2" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mn id="S2.SS1.p3.3.m3.2.2" xref="S2.SS1.p3.3.m3.2.2.cmml">2</mn><mo id="S2.SS1.p3.3.m3.4.5.3.2.3" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.3.m3.3.3" xref="S2.SS1.p3.3.m3.3.3.cmml">‚Ä¶</mi><mo id="S2.SS1.p3.3.m3.4.5.3.2.4" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mi id="S2.SS1.p3.3.m3.4.4" xref="S2.SS1.p3.3.m3.4.4.cmml">N</mi><mo stretchy="false" id="S2.SS1.p3.3.m3.4.5.3.2.5" xref="S2.SS1.p3.3.m3.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.4b"><apply id="S2.SS1.p3.3.m3.4.5.cmml" xref="S2.SS1.p3.3.m3.4.5"><in id="S2.SS1.p3.3.m3.4.5.1.cmml" xref="S2.SS1.p3.3.m3.4.5.1"></in><ci id="S2.SS1.p3.3.m3.4.5.2.cmml" xref="S2.SS1.p3.3.m3.4.5.2">ùëñ</ci><set id="S2.SS1.p3.3.m3.4.5.3.1.cmml" xref="S2.SS1.p3.3.m3.4.5.3.2"><cn type="integer" id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">1</cn><cn type="integer" id="S2.SS1.p3.3.m3.2.2.cmml" xref="S2.SS1.p3.3.m3.2.2">2</cn><ci id="S2.SS1.p3.3.m3.3.3.cmml" xref="S2.SS1.p3.3.m3.3.3">‚Ä¶</ci><ci id="S2.SS1.p3.3.m3.4.4.cmml" xref="S2.SS1.p3.3.m3.4.4">ùëÅ</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.4c">i\in\{1,2,\dots,N\}</annotation></semantics></math>, has a local dataset denoted by <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><msub id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml"><mi id="S2.SS1.p3.4.m4.1.1.2" xref="S2.SS1.p3.4.m4.1.1.2.cmml">D</mi><mi id="S2.SS1.p3.4.m4.1.1.3" xref="S2.SS1.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.2.cmml" xref="S2.SS1.p3.4.m4.1.1.2">ùê∑</ci><ci id="S2.SS1.p3.4.m4.1.1.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">D_{i}</annotation></semantics></math>. Each <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msub id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">D</mi><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">ùê∑</ci><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">D_{i}</annotation></semantics></math> consists of a set of input data points <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">X</mi><mi id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">ùëã</ci><ci id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">X_{i}</annotation></semantics></math> and corresponding labels <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="Y_{i}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><msub id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">Y</mi><mi id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">ùëå</ci><ci id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">Y_{i}</annotation></semantics></math>. The goal is to train a global model denoted by <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><mi id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><ci id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">f</annotation></semantics></math>, which can generalize well on all parties‚Äô datasets. The model <math id="S2.SS1.p3.9.m9.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p3.9.m9.1a"><mi id="S2.SS1.p3.9.m9.1.1" xref="S2.SS1.p3.9.m9.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.1b"><ci id="S2.SS1.p3.9.m9.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.1c">f</annotation></semantics></math> is typically represented as a parameterized function, such as a neural network, with trainable parameters denoted by <math id="S2.SS1.p3.10.m10.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p3.10.m10.1a"><mi id="S2.SS1.p3.10.m10.1.1" xref="S2.SS1.p3.10.m10.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.10.m10.1b"><ci id="S2.SS1.p3.10.m10.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.10.m10.1c">\theta</annotation></semantics></math>. The training process in HFL typically involves the following steps: (1) the process starts with the initialization of local model parameters <math id="S2.SS1.p3.11.m11.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S2.SS1.p3.11.m11.1a"><msub id="S2.SS1.p3.11.m11.1.1" xref="S2.SS1.p3.11.m11.1.1.cmml"><mi id="S2.SS1.p3.11.m11.1.1.2" xref="S2.SS1.p3.11.m11.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p3.11.m11.1.1.3" xref="S2.SS1.p3.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.11.m11.1b"><apply id="S2.SS1.p3.11.m11.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.1.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1">subscript</csymbol><ci id="S2.SS1.p3.11.m11.1.1.2.cmml" xref="S2.SS1.p3.11.m11.1.1.2">ùúÉ</ci><ci id="S2.SS1.p3.11.m11.1.1.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.11.m11.1c">\theta_{i}</annotation></semantics></math> for each participating party <math id="S2.SS1.p3.12.m12.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p3.12.m12.1a"><mi id="S2.SS1.p3.12.m12.1.1" xref="S2.SS1.p3.12.m12.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.12.m12.1b"><ci id="S2.SS1.p3.12.m12.1.1.cmml" xref="S2.SS1.p3.12.m12.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.12.m12.1c">i</annotation></semantics></math>. (2) Then, each party performs local training on its own dataset <math id="S2.SS1.p3.13.m13.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p3.13.m13.1a"><msub id="S2.SS1.p3.13.m13.1.1" xref="S2.SS1.p3.13.m13.1.1.cmml"><mi id="S2.SS1.p3.13.m13.1.1.2" xref="S2.SS1.p3.13.m13.1.1.2.cmml">D</mi><mi id="S2.SS1.p3.13.m13.1.1.3" xref="S2.SS1.p3.13.m13.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.13.m13.1b"><apply id="S2.SS1.p3.13.m13.1.1.cmml" xref="S2.SS1.p3.13.m13.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.13.m13.1.1.1.cmml" xref="S2.SS1.p3.13.m13.1.1">subscript</csymbol><ci id="S2.SS1.p3.13.m13.1.1.2.cmml" xref="S2.SS1.p3.13.m13.1.1.2">ùê∑</ci><ci id="S2.SS1.p3.13.m13.1.1.3.cmml" xref="S2.SS1.p3.13.m13.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.13.m13.1c">D_{i}</annotation></semantics></math> using its local model parameters <math id="S2.SS1.p3.14.m14.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S2.SS1.p3.14.m14.1a"><msub id="S2.SS1.p3.14.m14.1.1" xref="S2.SS1.p3.14.m14.1.1.cmml"><mi id="S2.SS1.p3.14.m14.1.1.2" xref="S2.SS1.p3.14.m14.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p3.14.m14.1.1.3" xref="S2.SS1.p3.14.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.14.m14.1b"><apply id="S2.SS1.p3.14.m14.1.1.cmml" xref="S2.SS1.p3.14.m14.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.14.m14.1.1.1.cmml" xref="S2.SS1.p3.14.m14.1.1">subscript</csymbol><ci id="S2.SS1.p3.14.m14.1.1.2.cmml" xref="S2.SS1.p3.14.m14.1.1.2">ùúÉ</ci><ci id="S2.SS1.p3.14.m14.1.1.3.cmml" xref="S2.SS1.p3.14.m14.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.14.m14.1c">\theta_{i}</annotation></semantics></math>. During local training, a local loss function <math id="S2.SS1.p3.15.m15.1" class="ltx_Math" alttext="L_{i}(\theta_{i})" display="inline"><semantics id="S2.SS1.p3.15.m15.1a"><mrow id="S2.SS1.p3.15.m15.1.1" xref="S2.SS1.p3.15.m15.1.1.cmml"><msub id="S2.SS1.p3.15.m15.1.1.3" xref="S2.SS1.p3.15.m15.1.1.3.cmml"><mi id="S2.SS1.p3.15.m15.1.1.3.2" xref="S2.SS1.p3.15.m15.1.1.3.2.cmml">L</mi><mi id="S2.SS1.p3.15.m15.1.1.3.3" xref="S2.SS1.p3.15.m15.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p3.15.m15.1.1.2" xref="S2.SS1.p3.15.m15.1.1.2.cmml">‚Äã</mo><mrow id="S2.SS1.p3.15.m15.1.1.1.1" xref="S2.SS1.p3.15.m15.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.15.m15.1.1.1.1.2" xref="S2.SS1.p3.15.m15.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p3.15.m15.1.1.1.1.1" xref="S2.SS1.p3.15.m15.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.15.m15.1.1.1.1.1.2" xref="S2.SS1.p3.15.m15.1.1.1.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p3.15.m15.1.1.1.1.1.3" xref="S2.SS1.p3.15.m15.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p3.15.m15.1.1.1.1.3" xref="S2.SS1.p3.15.m15.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.15.m15.1b"><apply id="S2.SS1.p3.15.m15.1.1.cmml" xref="S2.SS1.p3.15.m15.1.1"><times id="S2.SS1.p3.15.m15.1.1.2.cmml" xref="S2.SS1.p3.15.m15.1.1.2"></times><apply id="S2.SS1.p3.15.m15.1.1.3.cmml" xref="S2.SS1.p3.15.m15.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.15.m15.1.1.3.1.cmml" xref="S2.SS1.p3.15.m15.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.15.m15.1.1.3.2.cmml" xref="S2.SS1.p3.15.m15.1.1.3.2">ùêø</ci><ci id="S2.SS1.p3.15.m15.1.1.3.3.cmml" xref="S2.SS1.p3.15.m15.1.1.3.3">ùëñ</ci></apply><apply id="S2.SS1.p3.15.m15.1.1.1.1.1.cmml" xref="S2.SS1.p3.15.m15.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.15.m15.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.15.m15.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.15.m15.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.15.m15.1.1.1.1.1.2">ùúÉ</ci><ci id="S2.SS1.p3.15.m15.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.15.m15.1.1.1.1.1.3">ùëñ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.15.m15.1c">L_{i}(\theta_{i})</annotation></semantics></math> is optimized to minimize the discrepancy between the local model‚Äôs predictions and the corresponding labels in <math id="S2.SS1.p3.16.m16.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p3.16.m16.1a"><msub id="S2.SS1.p3.16.m16.1.1" xref="S2.SS1.p3.16.m16.1.1.cmml"><mi id="S2.SS1.p3.16.m16.1.1.2" xref="S2.SS1.p3.16.m16.1.1.2.cmml">D</mi><mi id="S2.SS1.p3.16.m16.1.1.3" xref="S2.SS1.p3.16.m16.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.16.m16.1b"><apply id="S2.SS1.p3.16.m16.1.1.cmml" xref="S2.SS1.p3.16.m16.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.16.m16.1.1.1.cmml" xref="S2.SS1.p3.16.m16.1.1">subscript</csymbol><ci id="S2.SS1.p3.16.m16.1.1.2.cmml" xref="S2.SS1.p3.16.m16.1.1.2">ùê∑</ci><ci id="S2.SS1.p3.16.m16.1.1.3.cmml" xref="S2.SS1.p3.16.m16.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.16.m16.1c">D_{i}</annotation></semantics></math>. The aim is to update the local model parameters <math id="S2.SS1.p3.17.m17.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S2.SS1.p3.17.m17.1a"><msub id="S2.SS1.p3.17.m17.1.1" xref="S2.SS1.p3.17.m17.1.1.cmml"><mi id="S2.SS1.p3.17.m17.1.1.2" xref="S2.SS1.p3.17.m17.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p3.17.m17.1.1.3" xref="S2.SS1.p3.17.m17.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.17.m17.1b"><apply id="S2.SS1.p3.17.m17.1.1.cmml" xref="S2.SS1.p3.17.m17.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.17.m17.1.1.1.cmml" xref="S2.SS1.p3.17.m17.1.1">subscript</csymbol><ci id="S2.SS1.p3.17.m17.1.1.2.cmml" xref="S2.SS1.p3.17.m17.1.1.2">ùúÉ</ci><ci id="S2.SS1.p3.17.m17.1.1.3.cmml" xref="S2.SS1.p3.17.m17.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.17.m17.1c">\theta_{i}</annotation></semantics></math> and reduce the local loss <math id="S2.SS1.p3.18.m18.1" class="ltx_Math" alttext="L_{i}" display="inline"><semantics id="S2.SS1.p3.18.m18.1a"><msub id="S2.SS1.p3.18.m18.1.1" xref="S2.SS1.p3.18.m18.1.1.cmml"><mi id="S2.SS1.p3.18.m18.1.1.2" xref="S2.SS1.p3.18.m18.1.1.2.cmml">L</mi><mi id="S2.SS1.p3.18.m18.1.1.3" xref="S2.SS1.p3.18.m18.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.18.m18.1b"><apply id="S2.SS1.p3.18.m18.1.1.cmml" xref="S2.SS1.p3.18.m18.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.18.m18.1.1.1.cmml" xref="S2.SS1.p3.18.m18.1.1">subscript</csymbol><ci id="S2.SS1.p3.18.m18.1.1.2.cmml" xref="S2.SS1.p3.18.m18.1.1.2">ùêø</ci><ci id="S2.SS1.p3.18.m18.1.1.3.cmml" xref="S2.SS1.p3.18.m18.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.18.m18.1c">L_{i}</annotation></semantics></math>. (3) After local training, the parties encrypt their gradients calculated with respect to their local loss functions. The encrypted gradients ensure that the parties‚Äô local updates remain private. The encrypted gradients are then sent to a trusted aggregator or a central server for further processing. (4) The trusted aggregator or central server performs secure aggregation on the encrypted gradients received from the parties. Secure aggregation techniques, such as secure multiparty computation (MPC) or homomorphic encryption, are employed to aggregate the gradients while maintaining privacy. The aggregator can compute the average of the encrypted gradients or perform more sophisticated aggregation schemes, such as averaging the parameters, of <math id="S2.SS1.p3.19.m19.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S2.SS1.p3.19.m19.1a"><msub id="S2.SS1.p3.19.m19.1.1" xref="S2.SS1.p3.19.m19.1.1.cmml"><mi id="S2.SS1.p3.19.m19.1.1.2" xref="S2.SS1.p3.19.m19.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p3.19.m19.1.1.3" xref="S2.SS1.p3.19.m19.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.19.m19.1b"><apply id="S2.SS1.p3.19.m19.1.1.cmml" xref="S2.SS1.p3.19.m19.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.19.m19.1.1.1.cmml" xref="S2.SS1.p3.19.m19.1.1">subscript</csymbol><ci id="S2.SS1.p3.19.m19.1.1.2.cmml" xref="S2.SS1.p3.19.m19.1.1.2">ùúÉ</ci><ci id="S2.SS1.p3.19.m19.1.1.3.cmml" xref="S2.SS1.p3.19.m19.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.19.m19.1c">\theta_{i}</annotation></semantics></math> to obtain the global model parameters <math id="S2.SS1.p3.20.m20.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p3.20.m20.1a"><mi id="S2.SS1.p3.20.m20.1.1" xref="S2.SS1.p3.20.m20.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.20.m20.1b"><ci id="S2.SS1.p3.20.m20.1.1.cmml" xref="S2.SS1.p3.20.m20.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.20.m20.1c">\theta</annotation></semantics></math>. (5) Once the secure aggregation is completed, the aggregated gradients are decrypted by the trusted aggregator or central server. The global model parameters <math id="S2.SS1.p3.21.m21.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p3.21.m21.1a"><mi id="S2.SS1.p3.21.m21.1.1" xref="S2.SS1.p3.21.m21.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.21.m21.1b"><ci id="S2.SS1.p3.21.m21.1.1.cmml" xref="S2.SS1.p3.21.m21.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.21.m21.1c">\theta</annotation></semantics></math> are then updated using the decrypted aggregated gradients. This update ensures that the global model benefits from the collective knowledge of all parties while preserving the privacy of individual data.
The mathematical formulation of HFL objective typically involves minimizing a global loss function <math id="S2.SS1.p3.22.m22.1" class="ltx_Math" alttext="L(\theta)" display="inline"><semantics id="S2.SS1.p3.22.m22.1a"><mrow id="S2.SS1.p3.22.m22.1.2" xref="S2.SS1.p3.22.m22.1.2.cmml"><mi id="S2.SS1.p3.22.m22.1.2.2" xref="S2.SS1.p3.22.m22.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.22.m22.1.2.1" xref="S2.SS1.p3.22.m22.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p3.22.m22.1.2.3.2" xref="S2.SS1.p3.22.m22.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.22.m22.1.2.3.2.1" xref="S2.SS1.p3.22.m22.1.2.cmml">(</mo><mi id="S2.SS1.p3.22.m22.1.1" xref="S2.SS1.p3.22.m22.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p3.22.m22.1.2.3.2.2" xref="S2.SS1.p3.22.m22.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.22.m22.1b"><apply id="S2.SS1.p3.22.m22.1.2.cmml" xref="S2.SS1.p3.22.m22.1.2"><times id="S2.SS1.p3.22.m22.1.2.1.cmml" xref="S2.SS1.p3.22.m22.1.2.1"></times><ci id="S2.SS1.p3.22.m22.1.2.2.cmml" xref="S2.SS1.p3.22.m22.1.2.2">ùêø</ci><ci id="S2.SS1.p3.22.m22.1.1.cmml" xref="S2.SS1.p3.22.m22.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.22.m22.1c">L(\theta)</annotation></semantics></math> over all parties‚Äô datasets. This can be expressed as:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="L(\theta)=\sum_{i=1}^{N}L_{i}(\theta_{i})+\lambda.R(\theta)," display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1"><mrow id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.3.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.3.3.2.1" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.3.3.2.2" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.E1.m1.3.3.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><munderover id="S2.E1.m1.3.3.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">L</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.2.cmml">+</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.3.cmml">Œª</mi></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S2.E1.m1.3.3.1.1.2.3" xref="S2.E1.m1.3.3.1.1.3a.cmml">.</mo><mrow id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.2.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.2.2.1" xref="S2.E1.m1.3.3.1.1.2.2.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.2.2.3.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.2.3.2.1" xref="S2.E1.m1.3.3.1.1.2.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Œ∏</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.2.3.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3a.cmml" xref="S2.E1.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1"><eq id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2"></eq><apply id="S2.E1.m1.3.3.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.3"><times id="S2.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.3.1"></times><ci id="S2.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.3.2">ùêø</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ùúÉ</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><plus id="S2.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.2"></plus><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2"></sum><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3"><eq id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2">ùêø</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.3">ùëñ</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">ùúÉ</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">ùëñ</ci></apply></apply></apply><ci id="S2.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3">ùúÜ</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"><times id="S2.E1.m1.3.3.1.1.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1"></times><ci id="S2.E1.m1.3.3.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.2">ùëÖ</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ùúÉ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">L(\theta)=\sum_{i=1}^{N}L_{i}(\theta_{i})+\lambda.R(\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.3" class="ltx_p">where <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="\sum_{i=1}^{N}L_{i}(\theta_{i})" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mrow id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml"><msubsup id="S2.SS1.p5.1.m1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.2.cmml"><mo id="S2.SS1.p5.1.m1.1.1.2.2.2" xref="S2.SS1.p5.1.m1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S2.SS1.p5.1.m1.1.1.2.2.3" xref="S2.SS1.p5.1.m1.1.1.2.2.3.cmml"><mi id="S2.SS1.p5.1.m1.1.1.2.2.3.2" xref="S2.SS1.p5.1.m1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.SS1.p5.1.m1.1.1.2.2.3.1" xref="S2.SS1.p5.1.m1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.SS1.p5.1.m1.1.1.2.2.3.3" xref="S2.SS1.p5.1.m1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p5.1.m1.1.1.2.3" xref="S2.SS1.p5.1.m1.1.1.2.3.cmml">N</mi></msubsup><mrow id="S2.SS1.p5.1.m1.1.1.1" xref="S2.SS1.p5.1.m1.1.1.1.cmml"><msub id="S2.SS1.p5.1.m1.1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.1.3.cmml"><mi id="S2.SS1.p5.1.m1.1.1.1.3.2" xref="S2.SS1.p5.1.m1.1.1.1.3.2.cmml">L</mi><mi id="S2.SS1.p5.1.m1.1.1.1.3.3" xref="S2.SS1.p5.1.m1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.SS1.p5.1.m1.1.1.1.1.1" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p5.1.m1.1.1.1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p5.1.m1.1.1.1.1.1.1" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p5.1.m1.1.1.1.1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p5.1.m1.1.1.1.1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p5.1.m1.1.1.1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1"><apply id="S2.SS1.p5.1.m1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.2.1.cmml" xref="S2.SS1.p5.1.m1.1.1.2">superscript</csymbol><apply id="S2.SS1.p5.1.m1.1.1.2.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.2.2.1.cmml" xref="S2.SS1.p5.1.m1.1.1.2">subscript</csymbol><sum id="S2.SS1.p5.1.m1.1.1.2.2.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2.2"></sum><apply id="S2.SS1.p5.1.m1.1.1.2.2.3.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2.3"><eq id="S2.SS1.p5.1.m1.1.1.2.2.3.1.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2.3.1"></eq><ci id="S2.SS1.p5.1.m1.1.1.2.2.3.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S2.SS1.p5.1.m1.1.1.2.2.3.3.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.SS1.p5.1.m1.1.1.2.3.cmml" xref="S2.SS1.p5.1.m1.1.1.2.3">ùëÅ</ci></apply><apply id="S2.SS1.p5.1.m1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1"><times id="S2.SS1.p5.1.m1.1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.1.2"></times><apply id="S2.SS1.p5.1.m1.1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.1.3.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.1.3.2.cmml" xref="S2.SS1.p5.1.m1.1.1.1.3.2">ùêø</ci><ci id="S2.SS1.p5.1.m1.1.1.1.3.3.cmml" xref="S2.SS1.p5.1.m1.1.1.1.3.3">ùëñ</ci></apply><apply id="S2.SS1.p5.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.2">ùúÉ</ci><ci id="S2.SS1.p5.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.1.1.1.1.3">ùëñ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">\sum_{i=1}^{N}L_{i}(\theta_{i})</annotation></semantics></math> represents the sum of local loss functions for all parties, <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mi id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><ci id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">\lambda</annotation></semantics></math> is a regularization parameter, and <math id="S2.SS1.p5.3.m3.1" class="ltx_Math" alttext="R(\theta)" display="inline"><semantics id="S2.SS1.p5.3.m3.1a"><mrow id="S2.SS1.p5.3.m3.1.2" xref="S2.SS1.p5.3.m3.1.2.cmml"><mi id="S2.SS1.p5.3.m3.1.2.2" xref="S2.SS1.p5.3.m3.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.3.m3.1.2.1" xref="S2.SS1.p5.3.m3.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p5.3.m3.1.2.3.2" xref="S2.SS1.p5.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p5.3.m3.1.2.3.2.1" xref="S2.SS1.p5.3.m3.1.2.cmml">(</mo><mi id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p5.3.m3.1.2.3.2.2" xref="S2.SS1.p5.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><apply id="S2.SS1.p5.3.m3.1.2.cmml" xref="S2.SS1.p5.3.m3.1.2"><times id="S2.SS1.p5.3.m3.1.2.1.cmml" xref="S2.SS1.p5.3.m3.1.2.1"></times><ci id="S2.SS1.p5.3.m3.1.2.2.cmml" xref="S2.SS1.p5.3.m3.1.2.2">ùëÖ</ci><ci id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">R(\theta)</annotation></semantics></math> represents a regularization term that helps prevent overfitting and encourages model simplicity. 
<br class="ltx_break"></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2308.13558/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="298" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of HFL used in CV.</figcaption>
</figure>
<div id="S2.SS1.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.p6.25" class="ltx_p"><span id="S2.SS1.p6.25.1" class="ltx_text ltx_font_bold">Vertical FL (VFL):</span> as illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‚Ä£ II-A Definition ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, VFL trains ML models on datasets having the same sample space, but different feature spaces. In this context, techniques that rely on entity alignment can be used in combination with encryption to overcome the problem of data sample overlapping at distributed clients during the local training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. With VFL different clients can cooperatively train their models without sharing their data (that differ in features), by only sharing their predictions for the samples they have in common. Let us consider a scenario with two parties, party <math id="S2.SS1.p6.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p6.1.m1.1a"><mi id="S2.SS1.p6.1.m1.1.1" xref="S2.SS1.p6.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.1.m1.1b"><ci id="S2.SS1.p6.1.m1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.1.m1.1c">A</annotation></semantics></math> and party <math id="S2.SS1.p6.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p6.2.m2.1a"><mi id="S2.SS1.p6.2.m2.1.1" xref="S2.SS1.p6.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.2.m2.1b"><ci id="S2.SS1.p6.2.m2.1.1.cmml" xref="S2.SS1.p6.2.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.2.m2.1c">B</annotation></semantics></math>. Party <math id="S2.SS1.p6.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p6.3.m3.1a"><mi id="S2.SS1.p6.3.m3.1.1" xref="S2.SS1.p6.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.3.m3.1b"><ci id="S2.SS1.p6.3.m3.1.1.cmml" xref="S2.SS1.p6.3.m3.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.3.m3.1c">A</annotation></semantics></math> holds a dataset with features <math id="S2.SS1.p6.4.m4.1" class="ltx_Math" alttext="X_{A}" display="inline"><semantics id="S2.SS1.p6.4.m4.1a"><msub id="S2.SS1.p6.4.m4.1.1" xref="S2.SS1.p6.4.m4.1.1.cmml"><mi id="S2.SS1.p6.4.m4.1.1.2" xref="S2.SS1.p6.4.m4.1.1.2.cmml">X</mi><mi id="S2.SS1.p6.4.m4.1.1.3" xref="S2.SS1.p6.4.m4.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.4.m4.1b"><apply id="S2.SS1.p6.4.m4.1.1.cmml" xref="S2.SS1.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.4.m4.1.1.1.cmml" xref="S2.SS1.p6.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p6.4.m4.1.1.2.cmml" xref="S2.SS1.p6.4.m4.1.1.2">ùëã</ci><ci id="S2.SS1.p6.4.m4.1.1.3.cmml" xref="S2.SS1.p6.4.m4.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.4.m4.1c">X_{A}</annotation></semantics></math> and labels <math id="S2.SS1.p6.5.m5.1" class="ltx_Math" alttext="Y_{A}" display="inline"><semantics id="S2.SS1.p6.5.m5.1a"><msub id="S2.SS1.p6.5.m5.1.1" xref="S2.SS1.p6.5.m5.1.1.cmml"><mi id="S2.SS1.p6.5.m5.1.1.2" xref="S2.SS1.p6.5.m5.1.1.2.cmml">Y</mi><mi id="S2.SS1.p6.5.m5.1.1.3" xref="S2.SS1.p6.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.5.m5.1b"><apply id="S2.SS1.p6.5.m5.1.1.cmml" xref="S2.SS1.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.5.m5.1.1.1.cmml" xref="S2.SS1.p6.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p6.5.m5.1.1.2.cmml" xref="S2.SS1.p6.5.m5.1.1.2">ùëå</ci><ci id="S2.SS1.p6.5.m5.1.1.3.cmml" xref="S2.SS1.p6.5.m5.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.5.m5.1c">Y_{A}</annotation></semantics></math>, while party <math id="S2.SS1.p6.6.m6.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p6.6.m6.1a"><mi id="S2.SS1.p6.6.m6.1.1" xref="S2.SS1.p6.6.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.6.m6.1b"><ci id="S2.SS1.p6.6.m6.1.1.cmml" xref="S2.SS1.p6.6.m6.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.6.m6.1c">B</annotation></semantics></math> holds a dataset with features <math id="S2.SS1.p6.7.m7.1" class="ltx_Math" alttext="X_{B}" display="inline"><semantics id="S2.SS1.p6.7.m7.1a"><msub id="S2.SS1.p6.7.m7.1.1" xref="S2.SS1.p6.7.m7.1.1.cmml"><mi id="S2.SS1.p6.7.m7.1.1.2" xref="S2.SS1.p6.7.m7.1.1.2.cmml">X</mi><mi id="S2.SS1.p6.7.m7.1.1.3" xref="S2.SS1.p6.7.m7.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.7.m7.1b"><apply id="S2.SS1.p6.7.m7.1.1.cmml" xref="S2.SS1.p6.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.7.m7.1.1.1.cmml" xref="S2.SS1.p6.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p6.7.m7.1.1.2.cmml" xref="S2.SS1.p6.7.m7.1.1.2">ùëã</ci><ci id="S2.SS1.p6.7.m7.1.1.3.cmml" xref="S2.SS1.p6.7.m7.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.7.m7.1c">X_{B}</annotation></semantics></math> and labels <math id="S2.SS1.p6.8.m8.1" class="ltx_Math" alttext="Y_{B}" display="inline"><semantics id="S2.SS1.p6.8.m8.1a"><msub id="S2.SS1.p6.8.m8.1.1" xref="S2.SS1.p6.8.m8.1.1.cmml"><mi id="S2.SS1.p6.8.m8.1.1.2" xref="S2.SS1.p6.8.m8.1.1.2.cmml">Y</mi><mi id="S2.SS1.p6.8.m8.1.1.3" xref="S2.SS1.p6.8.m8.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.8.m8.1b"><apply id="S2.SS1.p6.8.m8.1.1.cmml" xref="S2.SS1.p6.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.8.m8.1.1.1.cmml" xref="S2.SS1.p6.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p6.8.m8.1.1.2.cmml" xref="S2.SS1.p6.8.m8.1.1.2">ùëå</ci><ci id="S2.SS1.p6.8.m8.1.1.3.cmml" xref="S2.SS1.p6.8.m8.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.8.m8.1c">Y_{B}</annotation></semantics></math>. The goal is to jointly train a model that leverages the complementary information from both parties‚Äô datasets without sharing their raw data. The training process in VFL typically involves the following steps: (1) The datasets at party <math id="S2.SS1.p6.9.m9.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p6.9.m9.1a"><mi id="S2.SS1.p6.9.m9.1.1" xref="S2.SS1.p6.9.m9.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.9.m9.1b"><ci id="S2.SS1.p6.9.m9.1.1.cmml" xref="S2.SS1.p6.9.m9.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.9.m9.1c">A</annotation></semantics></math> and party <math id="S2.SS1.p6.10.m10.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p6.10.m10.1a"><mi id="S2.SS1.p6.10.m10.1.1" xref="S2.SS1.p6.10.m10.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.10.m10.1b"><ci id="S2.SS1.p6.10.m10.1.1.cmml" xref="S2.SS1.p6.10.m10.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.10.m10.1c">B</annotation></semantics></math> need to be pre-processed to ensure compatibilities and feature alignment, such as data transformation, feature mapping, etc. (2) Initialize the model parameters <math id="S2.SS1.p6.11.m11.1" class="ltx_Math" alttext="\theta_{A}" display="inline"><semantics id="S2.SS1.p6.11.m11.1a"><msub id="S2.SS1.p6.11.m11.1.1" xref="S2.SS1.p6.11.m11.1.1.cmml"><mi id="S2.SS1.p6.11.m11.1.1.2" xref="S2.SS1.p6.11.m11.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p6.11.m11.1.1.3" xref="S2.SS1.p6.11.m11.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.11.m11.1b"><apply id="S2.SS1.p6.11.m11.1.1.cmml" xref="S2.SS1.p6.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.11.m11.1.1.1.cmml" xref="S2.SS1.p6.11.m11.1.1">subscript</csymbol><ci id="S2.SS1.p6.11.m11.1.1.2.cmml" xref="S2.SS1.p6.11.m11.1.1.2">ùúÉ</ci><ci id="S2.SS1.p6.11.m11.1.1.3.cmml" xref="S2.SS1.p6.11.m11.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.11.m11.1c">\theta_{A}</annotation></semantics></math>, <math id="S2.SS1.p6.12.m12.1" class="ltx_Math" alttext="\theta_{B}" display="inline"><semantics id="S2.SS1.p6.12.m12.1a"><msub id="S2.SS1.p6.12.m12.1.1" xref="S2.SS1.p6.12.m12.1.1.cmml"><mi id="S2.SS1.p6.12.m12.1.1.2" xref="S2.SS1.p6.12.m12.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p6.12.m12.1.1.3" xref="S2.SS1.p6.12.m12.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.12.m12.1b"><apply id="S2.SS1.p6.12.m12.1.1.cmml" xref="S2.SS1.p6.12.m12.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.12.m12.1.1.1.cmml" xref="S2.SS1.p6.12.m12.1.1">subscript</csymbol><ci id="S2.SS1.p6.12.m12.1.1.2.cmml" xref="S2.SS1.p6.12.m12.1.1.2">ùúÉ</ci><ci id="S2.SS1.p6.12.m12.1.1.3.cmml" xref="S2.SS1.p6.12.m12.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.12.m12.1c">\theta_{B}</annotation></semantics></math>, and <math id="S2.SS1.p6.13.m13.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p6.13.m13.1a"><mi id="S2.SS1.p6.13.m13.1.1" xref="S2.SS1.p6.13.m13.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.13.m13.1b"><ci id="S2.SS1.p6.13.m13.1.1.cmml" xref="S2.SS1.p6.13.m13.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.13.m13.1c">\theta</annotation></semantics></math> of the local and global models. (3) Each party independently performs local training on its own dataset using its local model parameters <math id="S2.SS1.p6.14.m14.1" class="ltx_Math" alttext="\theta_{A}" display="inline"><semantics id="S2.SS1.p6.14.m14.1a"><msub id="S2.SS1.p6.14.m14.1.1" xref="S2.SS1.p6.14.m14.1.1.cmml"><mi id="S2.SS1.p6.14.m14.1.1.2" xref="S2.SS1.p6.14.m14.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p6.14.m14.1.1.3" xref="S2.SS1.p6.14.m14.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.14.m14.1b"><apply id="S2.SS1.p6.14.m14.1.1.cmml" xref="S2.SS1.p6.14.m14.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.14.m14.1.1.1.cmml" xref="S2.SS1.p6.14.m14.1.1">subscript</csymbol><ci id="S2.SS1.p6.14.m14.1.1.2.cmml" xref="S2.SS1.p6.14.m14.1.1.2">ùúÉ</ci><ci id="S2.SS1.p6.14.m14.1.1.3.cmml" xref="S2.SS1.p6.14.m14.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.14.m14.1c">\theta_{A}</annotation></semantics></math>, and <math id="S2.SS1.p6.15.m15.1" class="ltx_Math" alttext="\theta_{B}" display="inline"><semantics id="S2.SS1.p6.15.m15.1a"><msub id="S2.SS1.p6.15.m15.1.1" xref="S2.SS1.p6.15.m15.1.1.cmml"><mi id="S2.SS1.p6.15.m15.1.1.2" xref="S2.SS1.p6.15.m15.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p6.15.m15.1.1.3" xref="S2.SS1.p6.15.m15.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.15.m15.1b"><apply id="S2.SS1.p6.15.m15.1.1.cmml" xref="S2.SS1.p6.15.m15.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.15.m15.1.1.1.cmml" xref="S2.SS1.p6.15.m15.1.1">subscript</csymbol><ci id="S2.SS1.p6.15.m15.1.1.2.cmml" xref="S2.SS1.p6.15.m15.1.1.2">ùúÉ</ci><ci id="S2.SS1.p6.15.m15.1.1.3.cmml" xref="S2.SS1.p6.15.m15.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.15.m15.1c">\theta_{B}</annotation></semantics></math>. Party <math id="S2.SS1.p6.16.m16.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p6.16.m16.1a"><mi id="S2.SS1.p6.16.m16.1.1" xref="S2.SS1.p6.16.m16.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.16.m16.1b"><ci id="S2.SS1.p6.16.m16.1.1.cmml" xref="S2.SS1.p6.16.m16.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.16.m16.1c">A</annotation></semantics></math> uses <math id="S2.SS1.p6.17.m17.1" class="ltx_Math" alttext="X_{A}" display="inline"><semantics id="S2.SS1.p6.17.m17.1a"><msub id="S2.SS1.p6.17.m17.1.1" xref="S2.SS1.p6.17.m17.1.1.cmml"><mi id="S2.SS1.p6.17.m17.1.1.2" xref="S2.SS1.p6.17.m17.1.1.2.cmml">X</mi><mi id="S2.SS1.p6.17.m17.1.1.3" xref="S2.SS1.p6.17.m17.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.17.m17.1b"><apply id="S2.SS1.p6.17.m17.1.1.cmml" xref="S2.SS1.p6.17.m17.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.17.m17.1.1.1.cmml" xref="S2.SS1.p6.17.m17.1.1">subscript</csymbol><ci id="S2.SS1.p6.17.m17.1.1.2.cmml" xref="S2.SS1.p6.17.m17.1.1.2">ùëã</ci><ci id="S2.SS1.p6.17.m17.1.1.3.cmml" xref="S2.SS1.p6.17.m17.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.17.m17.1c">X_{A}</annotation></semantics></math> and <math id="S2.SS1.p6.18.m18.1" class="ltx_Math" alttext="Y_{A}" display="inline"><semantics id="S2.SS1.p6.18.m18.1a"><msub id="S2.SS1.p6.18.m18.1.1" xref="S2.SS1.p6.18.m18.1.1.cmml"><mi id="S2.SS1.p6.18.m18.1.1.2" xref="S2.SS1.p6.18.m18.1.1.2.cmml">Y</mi><mi id="S2.SS1.p6.18.m18.1.1.3" xref="S2.SS1.p6.18.m18.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.18.m18.1b"><apply id="S2.SS1.p6.18.m18.1.1.cmml" xref="S2.SS1.p6.18.m18.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.18.m18.1.1.1.cmml" xref="S2.SS1.p6.18.m18.1.1">subscript</csymbol><ci id="S2.SS1.p6.18.m18.1.1.2.cmml" xref="S2.SS1.p6.18.m18.1.1.2">ùëå</ci><ci id="S2.SS1.p6.18.m18.1.1.3.cmml" xref="S2.SS1.p6.18.m18.1.1.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.18.m18.1c">Y_{A}</annotation></semantics></math>, while party <math id="S2.SS1.p6.19.m19.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p6.19.m19.1a"><mi id="S2.SS1.p6.19.m19.1.1" xref="S2.SS1.p6.19.m19.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.19.m19.1b"><ci id="S2.SS1.p6.19.m19.1.1.cmml" xref="S2.SS1.p6.19.m19.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.19.m19.1c">B</annotation></semantics></math> uses <math id="S2.SS1.p6.20.m20.1" class="ltx_Math" alttext="X_{B}" display="inline"><semantics id="S2.SS1.p6.20.m20.1a"><msub id="S2.SS1.p6.20.m20.1.1" xref="S2.SS1.p6.20.m20.1.1.cmml"><mi id="S2.SS1.p6.20.m20.1.1.2" xref="S2.SS1.p6.20.m20.1.1.2.cmml">X</mi><mi id="S2.SS1.p6.20.m20.1.1.3" xref="S2.SS1.p6.20.m20.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.20.m20.1b"><apply id="S2.SS1.p6.20.m20.1.1.cmml" xref="S2.SS1.p6.20.m20.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.20.m20.1.1.1.cmml" xref="S2.SS1.p6.20.m20.1.1">subscript</csymbol><ci id="S2.SS1.p6.20.m20.1.1.2.cmml" xref="S2.SS1.p6.20.m20.1.1.2">ùëã</ci><ci id="S2.SS1.p6.20.m20.1.1.3.cmml" xref="S2.SS1.p6.20.m20.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.20.m20.1c">X_{B}</annotation></semantics></math> and <math id="S2.SS1.p6.21.m21.1" class="ltx_Math" alttext="Y_{B}" display="inline"><semantics id="S2.SS1.p6.21.m21.1a"><msub id="S2.SS1.p6.21.m21.1.1" xref="S2.SS1.p6.21.m21.1.1.cmml"><mi id="S2.SS1.p6.21.m21.1.1.2" xref="S2.SS1.p6.21.m21.1.1.2.cmml">Y</mi><mi id="S2.SS1.p6.21.m21.1.1.3" xref="S2.SS1.p6.21.m21.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.21.m21.1b"><apply id="S2.SS1.p6.21.m21.1.1.cmml" xref="S2.SS1.p6.21.m21.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.21.m21.1.1.1.cmml" xref="S2.SS1.p6.21.m21.1.1">subscript</csymbol><ci id="S2.SS1.p6.21.m21.1.1.2.cmml" xref="S2.SS1.p6.21.m21.1.1.2">ùëå</ci><ci id="S2.SS1.p6.21.m21.1.1.3.cmml" xref="S2.SS1.p6.21.m21.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.21.m21.1c">Y_{B}</annotation></semantics></math>. Local loss functions <math id="S2.SS1.p6.22.m22.1" class="ltx_Math" alttext="L_{A}(\theta)" display="inline"><semantics id="S2.SS1.p6.22.m22.1a"><mrow id="S2.SS1.p6.22.m22.1.2" xref="S2.SS1.p6.22.m22.1.2.cmml"><msub id="S2.SS1.p6.22.m22.1.2.2" xref="S2.SS1.p6.22.m22.1.2.2.cmml"><mi id="S2.SS1.p6.22.m22.1.2.2.2" xref="S2.SS1.p6.22.m22.1.2.2.2.cmml">L</mi><mi id="S2.SS1.p6.22.m22.1.2.2.3" xref="S2.SS1.p6.22.m22.1.2.2.3.cmml">A</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p6.22.m22.1.2.1" xref="S2.SS1.p6.22.m22.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p6.22.m22.1.2.3.2" xref="S2.SS1.p6.22.m22.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.22.m22.1.2.3.2.1" xref="S2.SS1.p6.22.m22.1.2.cmml">(</mo><mi id="S2.SS1.p6.22.m22.1.1" xref="S2.SS1.p6.22.m22.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p6.22.m22.1.2.3.2.2" xref="S2.SS1.p6.22.m22.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.22.m22.1b"><apply id="S2.SS1.p6.22.m22.1.2.cmml" xref="S2.SS1.p6.22.m22.1.2"><times id="S2.SS1.p6.22.m22.1.2.1.cmml" xref="S2.SS1.p6.22.m22.1.2.1"></times><apply id="S2.SS1.p6.22.m22.1.2.2.cmml" xref="S2.SS1.p6.22.m22.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.22.m22.1.2.2.1.cmml" xref="S2.SS1.p6.22.m22.1.2.2">subscript</csymbol><ci id="S2.SS1.p6.22.m22.1.2.2.2.cmml" xref="S2.SS1.p6.22.m22.1.2.2.2">ùêø</ci><ci id="S2.SS1.p6.22.m22.1.2.2.3.cmml" xref="S2.SS1.p6.22.m22.1.2.2.3">ùê¥</ci></apply><ci id="S2.SS1.p6.22.m22.1.1.cmml" xref="S2.SS1.p6.22.m22.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.22.m22.1c">L_{A}(\theta)</annotation></semantics></math> and <math id="S2.SS1.p6.23.m23.1" class="ltx_Math" alttext="L_{B}(\theta)" display="inline"><semantics id="S2.SS1.p6.23.m23.1a"><mrow id="S2.SS1.p6.23.m23.1.2" xref="S2.SS1.p6.23.m23.1.2.cmml"><msub id="S2.SS1.p6.23.m23.1.2.2" xref="S2.SS1.p6.23.m23.1.2.2.cmml"><mi id="S2.SS1.p6.23.m23.1.2.2.2" xref="S2.SS1.p6.23.m23.1.2.2.2.cmml">L</mi><mi id="S2.SS1.p6.23.m23.1.2.2.3" xref="S2.SS1.p6.23.m23.1.2.2.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p6.23.m23.1.2.1" xref="S2.SS1.p6.23.m23.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p6.23.m23.1.2.3.2" xref="S2.SS1.p6.23.m23.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.23.m23.1.2.3.2.1" xref="S2.SS1.p6.23.m23.1.2.cmml">(</mo><mi id="S2.SS1.p6.23.m23.1.1" xref="S2.SS1.p6.23.m23.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p6.23.m23.1.2.3.2.2" xref="S2.SS1.p6.23.m23.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.23.m23.1b"><apply id="S2.SS1.p6.23.m23.1.2.cmml" xref="S2.SS1.p6.23.m23.1.2"><times id="S2.SS1.p6.23.m23.1.2.1.cmml" xref="S2.SS1.p6.23.m23.1.2.1"></times><apply id="S2.SS1.p6.23.m23.1.2.2.cmml" xref="S2.SS1.p6.23.m23.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.23.m23.1.2.2.1.cmml" xref="S2.SS1.p6.23.m23.1.2.2">subscript</csymbol><ci id="S2.SS1.p6.23.m23.1.2.2.2.cmml" xref="S2.SS1.p6.23.m23.1.2.2.2">ùêø</ci><ci id="S2.SS1.p6.23.m23.1.2.2.3.cmml" xref="S2.SS1.p6.23.m23.1.2.2.3">ùêµ</ci></apply><ci id="S2.SS1.p6.23.m23.1.1.cmml" xref="S2.SS1.p6.23.m23.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.23.m23.1c">L_{B}(\theta)</annotation></semantics></math> are optimized to update the local model parameters and reduce the discrepancies between the local model‚Äôs predictions and the corresponding labels in each party‚Äôs dataset. The steps (4), (5), and (6) are similar to steps (3), (4), and (5) of HFL. The mathematical formulation of <math id="S2.SS1.p6.24.m24.1" class="ltx_Math" alttext="L(\theta)" display="inline"><semantics id="S2.SS1.p6.24.m24.1a"><mrow id="S2.SS1.p6.24.m24.1.2" xref="S2.SS1.p6.24.m24.1.2.cmml"><mi id="S2.SS1.p6.24.m24.1.2.2" xref="S2.SS1.p6.24.m24.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.24.m24.1.2.1" xref="S2.SS1.p6.24.m24.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p6.24.m24.1.2.3.2" xref="S2.SS1.p6.24.m24.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.24.m24.1.2.3.2.1" xref="S2.SS1.p6.24.m24.1.2.cmml">(</mo><mi id="S2.SS1.p6.24.m24.1.1" xref="S2.SS1.p6.24.m24.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p6.24.m24.1.2.3.2.2" xref="S2.SS1.p6.24.m24.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.24.m24.1b"><apply id="S2.SS1.p6.24.m24.1.2.cmml" xref="S2.SS1.p6.24.m24.1.2"><times id="S2.SS1.p6.24.m24.1.2.1.cmml" xref="S2.SS1.p6.24.m24.1.2.1"></times><ci id="S2.SS1.p6.24.m24.1.2.2.cmml" xref="S2.SS1.p6.24.m24.1.2.2">ùêø</ci><ci id="S2.SS1.p6.24.m24.1.1.cmml" xref="S2.SS1.p6.24.m24.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.24.m24.1c">L(\theta)</annotation></semantics></math> in VFL, for <math id="S2.SS1.p6.25.m25.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p6.25.m25.1a"><mi id="S2.SS1.p6.25.m25.1.1" xref="S2.SS1.p6.25.m25.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.25.m25.1b"><ci id="S2.SS1.p6.25.m25.1.1.cmml" xref="S2.SS1.p6.25.m25.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.25.m25.1c">N</annotation></semantics></math> parties, remains the same as the HFL case (Equation <a href="#S2.E1" title="In II-A Definition ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). 
<br class="ltx_break"></p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.13558/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="388" height="350" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of VFL used in CV.</figcaption>
</figure>
<div id="S2.SS1.p7" class="ltx_para ltx_noindent">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_bold">Federated transfer learning (FTL):</span> handles datasets
with limited overlap both in the sample space and in the feature spaces, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‚Ä£ II-A Definition ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Unlike VFL systems the number of common samples and thus labels that can be shared across the models is small thus limiting their reusability across the different models. A common transfer space is used for mapping the different feature spaces and exchanging knowledge.
Specifically, adopting transfer learning methods helps calculate feature values from distinct feature spaces, which is utilized for training local datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.13" class="ltx_p">Let us consider a scenario with <math id="S2.SS1.p8.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p8.1.m1.1a"><mi id="S2.SS1.p8.1.m1.1.1" xref="S2.SS1.p8.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.1.m1.1b"><ci id="S2.SS1.p8.1.m1.1.1.cmml" xref="S2.SS1.p8.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.1.m1.1c">N</annotation></semantics></math> parties, where each party <math id="S2.SS1.p8.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p8.2.m2.1a"><mi id="S2.SS1.p8.2.m2.1.1" xref="S2.SS1.p8.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.2.m2.1b"><ci id="S2.SS1.p8.2.m2.1.1.cmml" xref="S2.SS1.p8.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.2.m2.1c">i</annotation></semantics></math>, where <math id="S2.SS1.p8.3.m3.4" class="ltx_Math" alttext="i\in\{1,2,\dots,N\}" display="inline"><semantics id="S2.SS1.p8.3.m3.4a"><mrow id="S2.SS1.p8.3.m3.4.5" xref="S2.SS1.p8.3.m3.4.5.cmml"><mi id="S2.SS1.p8.3.m3.4.5.2" xref="S2.SS1.p8.3.m3.4.5.2.cmml">i</mi><mo id="S2.SS1.p8.3.m3.4.5.1" xref="S2.SS1.p8.3.m3.4.5.1.cmml">‚àà</mo><mrow id="S2.SS1.p8.3.m3.4.5.3.2" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S2.SS1.p8.3.m3.4.5.3.2.1" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml">{</mo><mn id="S2.SS1.p8.3.m3.1.1" xref="S2.SS1.p8.3.m3.1.1.cmml">1</mn><mo id="S2.SS1.p8.3.m3.4.5.3.2.2" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml">,</mo><mn id="S2.SS1.p8.3.m3.2.2" xref="S2.SS1.p8.3.m3.2.2.cmml">2</mn><mo id="S2.SS1.p8.3.m3.4.5.3.2.3" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p8.3.m3.3.3" xref="S2.SS1.p8.3.m3.3.3.cmml">‚Ä¶</mi><mo id="S2.SS1.p8.3.m3.4.5.3.2.4" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml">,</mo><mi id="S2.SS1.p8.3.m3.4.4" xref="S2.SS1.p8.3.m3.4.4.cmml">N</mi><mo stretchy="false" id="S2.SS1.p8.3.m3.4.5.3.2.5" xref="S2.SS1.p8.3.m3.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.3.m3.4b"><apply id="S2.SS1.p8.3.m3.4.5.cmml" xref="S2.SS1.p8.3.m3.4.5"><in id="S2.SS1.p8.3.m3.4.5.1.cmml" xref="S2.SS1.p8.3.m3.4.5.1"></in><ci id="S2.SS1.p8.3.m3.4.5.2.cmml" xref="S2.SS1.p8.3.m3.4.5.2">ùëñ</ci><set id="S2.SS1.p8.3.m3.4.5.3.1.cmml" xref="S2.SS1.p8.3.m3.4.5.3.2"><cn type="integer" id="S2.SS1.p8.3.m3.1.1.cmml" xref="S2.SS1.p8.3.m3.1.1">1</cn><cn type="integer" id="S2.SS1.p8.3.m3.2.2.cmml" xref="S2.SS1.p8.3.m3.2.2">2</cn><ci id="S2.SS1.p8.3.m3.3.3.cmml" xref="S2.SS1.p8.3.m3.3.3">‚Ä¶</ci><ci id="S2.SS1.p8.3.m3.4.4.cmml" xref="S2.SS1.p8.3.m3.4.4">ùëÅ</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.3.m3.4c">i\in\{1,2,\dots,N\}</annotation></semantics></math>, has a local dataset denoted by <math id="S2.SS1.p8.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p8.4.m4.1a"><msub id="S2.SS1.p8.4.m4.1.1" xref="S2.SS1.p8.4.m4.1.1.cmml"><mi id="S2.SS1.p8.4.m4.1.1.2" xref="S2.SS1.p8.4.m4.1.1.2.cmml">D</mi><mi id="S2.SS1.p8.4.m4.1.1.3" xref="S2.SS1.p8.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.4.m4.1b"><apply id="S2.SS1.p8.4.m4.1.1.cmml" xref="S2.SS1.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.4.m4.1.1.1.cmml" xref="S2.SS1.p8.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p8.4.m4.1.1.2.cmml" xref="S2.SS1.p8.4.m4.1.1.2">ùê∑</ci><ci id="S2.SS1.p8.4.m4.1.1.3.cmml" xref="S2.SS1.p8.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.4.m4.1c">D_{i}</annotation></semantics></math> and a task denoted by <math id="S2.SS1.p8.5.m5.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S2.SS1.p8.5.m5.1a"><msub id="S2.SS1.p8.5.m5.1.1" xref="S2.SS1.p8.5.m5.1.1.cmml"><mi id="S2.SS1.p8.5.m5.1.1.2" xref="S2.SS1.p8.5.m5.1.1.2.cmml">T</mi><mi id="S2.SS1.p8.5.m5.1.1.3" xref="S2.SS1.p8.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.5.m5.1b"><apply id="S2.SS1.p8.5.m5.1.1.cmml" xref="S2.SS1.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.5.m5.1.1.1.cmml" xref="S2.SS1.p8.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p8.5.m5.1.1.2.cmml" xref="S2.SS1.p8.5.m5.1.1.2">ùëá</ci><ci id="S2.SS1.p8.5.m5.1.1.3.cmml" xref="S2.SS1.p8.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.5.m5.1c">T_{i}</annotation></semantics></math>. The goal in FTL is to jointly train a global model that leverages the knowledge learned from different tasks. The training process in FTL typically involves the following steps: (1) Each party <math id="S2.SS1.p8.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p8.6.m6.1a"><mi id="S2.SS1.p8.6.m6.1.1" xref="S2.SS1.p8.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.6.m6.1b"><ci id="S2.SS1.p8.6.m6.1.1.cmml" xref="S2.SS1.p8.6.m6.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.6.m6.1c">i</annotation></semantics></math> performs local training on its own <math id="S2.SS1.p8.7.m7.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p8.7.m7.1a"><msub id="S2.SS1.p8.7.m7.1.1" xref="S2.SS1.p8.7.m7.1.1.cmml"><mi id="S2.SS1.p8.7.m7.1.1.2" xref="S2.SS1.p8.7.m7.1.1.2.cmml">D</mi><mi id="S2.SS1.p8.7.m7.1.1.3" xref="S2.SS1.p8.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.7.m7.1b"><apply id="S2.SS1.p8.7.m7.1.1.cmml" xref="S2.SS1.p8.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.7.m7.1.1.1.cmml" xref="S2.SS1.p8.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p8.7.m7.1.1.2.cmml" xref="S2.SS1.p8.7.m7.1.1.2">ùê∑</ci><ci id="S2.SS1.p8.7.m7.1.1.3.cmml" xref="S2.SS1.p8.7.m7.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.7.m7.1c">D_{i}</annotation></semantics></math> and task-specific model parameters <math id="S2.SS1.p8.8.m8.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S2.SS1.p8.8.m8.1a"><msub id="S2.SS1.p8.8.m8.1.1" xref="S2.SS1.p8.8.m8.1.1.cmml"><mi id="S2.SS1.p8.8.m8.1.1.2" xref="S2.SS1.p8.8.m8.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p8.8.m8.1.1.3" xref="S2.SS1.p8.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.8.m8.1b"><apply id="S2.SS1.p8.8.m8.1.1.cmml" xref="S2.SS1.p8.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.8.m8.1.1.1.cmml" xref="S2.SS1.p8.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p8.8.m8.1.1.2.cmml" xref="S2.SS1.p8.8.m8.1.1.2">ùúÉ</ci><ci id="S2.SS1.p8.8.m8.1.1.3.cmml" xref="S2.SS1.p8.8.m8.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.8.m8.1c">\theta_{i}</annotation></semantics></math>. During this step, the local model optimizes a task-specific loss function <math id="S2.SS1.p8.9.m9.1" class="ltx_Math" alttext="L_{i}(\theta_{i})" display="inline"><semantics id="S2.SS1.p8.9.m9.1a"><mrow id="S2.SS1.p8.9.m9.1.1" xref="S2.SS1.p8.9.m9.1.1.cmml"><msub id="S2.SS1.p8.9.m9.1.1.3" xref="S2.SS1.p8.9.m9.1.1.3.cmml"><mi id="S2.SS1.p8.9.m9.1.1.3.2" xref="S2.SS1.p8.9.m9.1.1.3.2.cmml">L</mi><mi id="S2.SS1.p8.9.m9.1.1.3.3" xref="S2.SS1.p8.9.m9.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p8.9.m9.1.1.2" xref="S2.SS1.p8.9.m9.1.1.2.cmml">‚Äã</mo><mrow id="S2.SS1.p8.9.m9.1.1.1.1" xref="S2.SS1.p8.9.m9.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p8.9.m9.1.1.1.1.2" xref="S2.SS1.p8.9.m9.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p8.9.m9.1.1.1.1.1" xref="S2.SS1.p8.9.m9.1.1.1.1.1.cmml"><mi id="S2.SS1.p8.9.m9.1.1.1.1.1.2" xref="S2.SS1.p8.9.m9.1.1.1.1.1.2.cmml">Œ∏</mi><mi id="S2.SS1.p8.9.m9.1.1.1.1.1.3" xref="S2.SS1.p8.9.m9.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p8.9.m9.1.1.1.1.3" xref="S2.SS1.p8.9.m9.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.9.m9.1b"><apply id="S2.SS1.p8.9.m9.1.1.cmml" xref="S2.SS1.p8.9.m9.1.1"><times id="S2.SS1.p8.9.m9.1.1.2.cmml" xref="S2.SS1.p8.9.m9.1.1.2"></times><apply id="S2.SS1.p8.9.m9.1.1.3.cmml" xref="S2.SS1.p8.9.m9.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p8.9.m9.1.1.3.1.cmml" xref="S2.SS1.p8.9.m9.1.1.3">subscript</csymbol><ci id="S2.SS1.p8.9.m9.1.1.3.2.cmml" xref="S2.SS1.p8.9.m9.1.1.3.2">ùêø</ci><ci id="S2.SS1.p8.9.m9.1.1.3.3.cmml" xref="S2.SS1.p8.9.m9.1.1.3.3">ùëñ</ci></apply><apply id="S2.SS1.p8.9.m9.1.1.1.1.1.cmml" xref="S2.SS1.p8.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.9.m9.1.1.1.1.1.1.cmml" xref="S2.SS1.p8.9.m9.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p8.9.m9.1.1.1.1.1.2.cmml" xref="S2.SS1.p8.9.m9.1.1.1.1.1.2">ùúÉ</ci><ci id="S2.SS1.p8.9.m9.1.1.1.1.1.3.cmml" xref="S2.SS1.p8.9.m9.1.1.1.1.1.3">ùëñ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.9.m9.1c">L_{i}(\theta_{i})</annotation></semantics></math> to minimize the discrepancy between the local model‚Äôs predictions and the corresponding labels in <math id="S2.SS1.p8.10.m10.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p8.10.m10.1a"><msub id="S2.SS1.p8.10.m10.1.1" xref="S2.SS1.p8.10.m10.1.1.cmml"><mi id="S2.SS1.p8.10.m10.1.1.2" xref="S2.SS1.p8.10.m10.1.1.2.cmml">D</mi><mi id="S2.SS1.p8.10.m10.1.1.3" xref="S2.SS1.p8.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.10.m10.1b"><apply id="S2.SS1.p8.10.m10.1.1.cmml" xref="S2.SS1.p8.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p8.10.m10.1.1.1.cmml" xref="S2.SS1.p8.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p8.10.m10.1.1.2.cmml" xref="S2.SS1.p8.10.m10.1.1.2">ùê∑</ci><ci id="S2.SS1.p8.10.m10.1.1.3.cmml" xref="S2.SS1.p8.10.m10.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.10.m10.1c">D_{i}</annotation></semantics></math>. (2) The parties communicate and align their models (shared layers or parameters) to create a joint model architecture. (3) This transfer of knowledge, from party <math id="S2.SS1.p8.11.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p8.11.m11.1a"><mi id="S2.SS1.p8.11.m11.1.1" xref="S2.SS1.p8.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.11.m11.1b"><ci id="S2.SS1.p8.11.m11.1.1.cmml" xref="S2.SS1.p8.11.m11.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.11.m11.1c">i</annotation></semantics></math> to the global model noted as <math id="S2.SS1.p8.12.m12.1" class="ltx_Math" alttext="Ki(\theta)" display="inline"><semantics id="S2.SS1.p8.12.m12.1a"><mrow id="S2.SS1.p8.12.m12.1.2" xref="S2.SS1.p8.12.m12.1.2.cmml"><mi id="S2.SS1.p8.12.m12.1.2.2" xref="S2.SS1.p8.12.m12.1.2.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p8.12.m12.1.2.1" xref="S2.SS1.p8.12.m12.1.2.1.cmml">‚Äã</mo><mi id="S2.SS1.p8.12.m12.1.2.3" xref="S2.SS1.p8.12.m12.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p8.12.m12.1.2.1a" xref="S2.SS1.p8.12.m12.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p8.12.m12.1.2.4.2" xref="S2.SS1.p8.12.m12.1.2.cmml"><mo stretchy="false" id="S2.SS1.p8.12.m12.1.2.4.2.1" xref="S2.SS1.p8.12.m12.1.2.cmml">(</mo><mi id="S2.SS1.p8.12.m12.1.1" xref="S2.SS1.p8.12.m12.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p8.12.m12.1.2.4.2.2" xref="S2.SS1.p8.12.m12.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.12.m12.1b"><apply id="S2.SS1.p8.12.m12.1.2.cmml" xref="S2.SS1.p8.12.m12.1.2"><times id="S2.SS1.p8.12.m12.1.2.1.cmml" xref="S2.SS1.p8.12.m12.1.2.1"></times><ci id="S2.SS1.p8.12.m12.1.2.2.cmml" xref="S2.SS1.p8.12.m12.1.2.2">ùêæ</ci><ci id="S2.SS1.p8.12.m12.1.2.3.cmml" xref="S2.SS1.p8.12.m12.1.2.3">ùëñ</ci><ci id="S2.SS1.p8.12.m12.1.1.cmml" xref="S2.SS1.p8.12.m12.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.12.m12.1c">Ki(\theta)</annotation></semantics></math>, can occur through parameter sharing or feature extraction. steps (4) and (5) remain the same as HFL. The global loss function <math id="S2.SS1.p8.13.m13.1" class="ltx_Math" alttext="L(\theta)" display="inline"><semantics id="S2.SS1.p8.13.m13.1a"><mrow id="S2.SS1.p8.13.m13.1.2" xref="S2.SS1.p8.13.m13.1.2.cmml"><mi id="S2.SS1.p8.13.m13.1.2.2" xref="S2.SS1.p8.13.m13.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p8.13.m13.1.2.1" xref="S2.SS1.p8.13.m13.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS1.p8.13.m13.1.2.3.2" xref="S2.SS1.p8.13.m13.1.2.cmml"><mo stretchy="false" id="S2.SS1.p8.13.m13.1.2.3.2.1" xref="S2.SS1.p8.13.m13.1.2.cmml">(</mo><mi id="S2.SS1.p8.13.m13.1.1" xref="S2.SS1.p8.13.m13.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S2.SS1.p8.13.m13.1.2.3.2.2" xref="S2.SS1.p8.13.m13.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.13.m13.1b"><apply id="S2.SS1.p8.13.m13.1.2.cmml" xref="S2.SS1.p8.13.m13.1.2"><times id="S2.SS1.p8.13.m13.1.2.1.cmml" xref="S2.SS1.p8.13.m13.1.2.1"></times><ci id="S2.SS1.p8.13.m13.1.2.2.cmml" xref="S2.SS1.p8.13.m13.1.2.2">ùêø</ci><ci id="S2.SS1.p8.13.m13.1.1.cmml" xref="S2.SS1.p8.13.m13.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.13.m13.1c">L(\theta)</annotation></semantics></math> in FTL typically includes a combination of task-specific loss terms and transfer-related terms.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">In smart healthcare, FTL can support disease diagnosis by collaborating countries with multiple hospitals that
have different patients (sample space) and different monitor and therapeutic programs (feature space). In this way, FTL can enrich the shared AI model output for improving the accuracy of diagnosis.
<br class="ltx_break"></p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.13558/assets/x3.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="388" height="292" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Architecture of FTL used in CV.</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2308.13558/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="404" height="289" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Taxonomy of FL frameworks for CV applications.</figcaption>
</figure>
<div id="S2.SS1.p10" class="ltx_para ltx_noindent">
<p id="S2.SS1.p10.1" class="ltx_p"><span id="S2.SS1.p10.1.1" class="ltx_text ltx_font_bold">Collaborative learning (CL):</span>
CL refers to a cooperative approach in which multiple entities actively participate and share their data and model updates to collectively train a shared model. It involves a collaborative effort where participants collaborate and contribute to the model‚Äôs training by sharing their local knowledge and insights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
CL, within the context of FL, specifically refers to the scenario where participants actively collaborate and exchange data and model updates among themselves to collectively improve the shared model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In this sense, CL represents a specific instance of FL that emphasizes the collaborative nature of model training, where participants engage in direct data and model parameter sharing with each other, while still adhering to the principles of decentralized data ownership and privacy preservation characteristic of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. CL can be justified as a form of FL based on the following key aspects:</p>
</div>
<div id="S2.SS1.p11" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data Distribution:</span> In CL, data is distributed across multiple entities or participants who collaborate to train a shared model. Similarly, FL involves training a model on decentralized data stored across multiple devices or servers, where each device contributes its local data.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Local Model Updates:</span> In both CL and FL, individual participants or devices perform local model updates using their respective data. These updates capture the specific patterns and characteristics present in their local data.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Aggregation and Collaboration:</span> Both approaches involve aggregating the local model updates from multiple participants or devices. In CL, participants typically share their model updates directly with each other, whereas in FL, the updates are aggregated on a central server or coordinator.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Privacy and Security:</span> CL and FL prioritize privacy and security. They aim to protect sensitive data by keeping it decentralized and only exchanging model updates or gradients instead of raw data. This ensures that participants‚Äô data remains private while enabling collaborative model training.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p"><span id="S2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Iterative Improvement:</span> Both methods involve iterative improvement, where the shared model is updated based on the aggregated information from all participants. The process of local updates, aggregation, and model refinement is repeated until convergence is achieved.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p12" class="ltx_para ltx_noindent">
<p id="S2.SS1.p12.1" class="ltx_p">While CL and FL share similarities, there are also some key differences between the two approaches:</p>
</div>
<div id="S2.SS1.p13" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Data Ownership:</span> In CL, all participants typically have access to and ownership of the entire dataset. They actively share and collaborate on the data and model updates. In contrast, FL operates on decentralized data, where each participant retains ownership and control over their local data, which is not directly accessible to other participants.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Centralization vs. Decentralization:</span> CL tends to have a more centralized approach, where participants directly communicate and share their updates with each other. FL, on the other hand, employs a decentralized approach, where local updates are sent to a central server or coordinator for aggregation and model updates.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Communication Overhead:</span> In CL, participants typically need to establish direct communication channels to exchange data and model updates. This can require significant communication overhead, especially when the number of participants is large. FL reduces this communication overhead by relying on a central server or coordinator that facilitates the aggregation process.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p"><span id="S2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Privacy and Security Focus:</span> While both approaches prioritize privacy and security, FL places a stronger emphasis on data privacy. FL minimizes the exposure of raw data by exchanging only model updates or gradients between participants and the central server. CL may involve more direct sharing of data or model parameters, which can introduce potential privacy risks.</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p"><span id="S2.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Scalability:</span> FL is particularly suitable for large-scale distributed environments with a large number of participants or devices. Its decentralized nature allows for scalability and efficient training across a vast network of devices. CL may face challenges in terms of scalability, as direct communication and coordination among all participants become increasingly complex with a larger number of entities.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p14" class="ltx_para ltx_noindent">
<p id="S2.SS1.p14.1" class="ltx_p">These differences highlight the varying degrees of decentralization, data ownership, communication, and scalability between CL and FL. While they share common principles, the specific implementation and focus of each approach can differ based on the context and requirements of the collaborative training scenario.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Problem formulation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Koneƒçn·ª≥ et al.‚Äôs work has made FL well-known, but there are other definitions of the concept available in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. FL can be realized through various topologies and compute plans despite a shared objective of combining knowledge from non-co-located data. This section aims to provide a detailed explanation of what FL is, while also highlighting the significant challenges and technical considerations that arise when using FL in CV.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The goal of FL is generally to build a global statistical model using data from many remote devices, which can range from tens to millions in number. This process involves minimizing an objective function that captures the desired characteristics of the model.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.5" class="ltx_Math" alttext="\underset{w}{\min}F(w),\text{ \ \ \ where \ \ \ }F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w)" display="block"><semantics id="S2.E2.m1.5a"><mrow id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml"><mrow id="S2.E2.m1.5.5.2.2" xref="S2.E2.m1.5.5.2.3.cmml"><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><munder accentunder="true" id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.cmml">min</mi><mo id="S2.E2.m1.4.4.1.1.1.2.1" xref="S2.E2.m1.4.4.1.1.1.2.1.cmml">ùë§</mo></munder><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml">‚Äã</mo><mi id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1a" xref="S2.E2.m1.4.4.1.1.1.1.cmml">‚Äã</mo><mrow id="S2.E2.m1.4.4.1.1.1.4.2" xref="S2.E2.m1.4.4.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.4.2.1" xref="S2.E2.m1.4.4.1.1.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">w</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.4.2.2" xref="S2.E2.m1.4.4.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.5.5.2.2.3" xref="S2.E2.m1.5.5.2.3.cmml">,</mo><mrow id="S2.E2.m1.5.5.2.2.2" xref="S2.E2.m1.5.5.2.2.2.cmml"><mtext id="S2.E2.m1.5.5.2.2.2.2" xref="S2.E2.m1.5.5.2.2.2.2a.cmml">¬†where¬†</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.2.2.2.1" xref="S2.E2.m1.5.5.2.2.2.1.cmml">‚Äã</mo><mi id="S2.E2.m1.5.5.2.2.2.3" xref="S2.E2.m1.5.5.2.2.2.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.2.2.2.1a" xref="S2.E2.m1.5.5.2.2.2.1.cmml">‚Äã</mo><mrow id="S2.E2.m1.5.5.2.2.2.4.2" xref="S2.E2.m1.5.5.2.2.2.cmml"><mo stretchy="false" id="S2.E2.m1.5.5.2.2.2.4.2.1" xref="S2.E2.m1.5.5.2.2.2.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">w</mi><mo rspace="0.278em" stretchy="false" id="S2.E2.m1.5.5.2.2.2.4.2.2" xref="S2.E2.m1.5.5.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.111em" id="S2.E2.m1.5.5.3" xref="S2.E2.m1.5.5.3.cmml">:=</mo><mrow id="S2.E2.m1.5.5.4" xref="S2.E2.m1.5.5.4.cmml"><munderover id="S2.E2.m1.5.5.4.1" xref="S2.E2.m1.5.5.4.1.cmml"><mo movablelimits="false" id="S2.E2.m1.5.5.4.1.2.2" xref="S2.E2.m1.5.5.4.1.2.2.cmml">‚àë</mo><mrow id="S2.E2.m1.5.5.4.1.2.3" xref="S2.E2.m1.5.5.4.1.2.3.cmml"><mi id="S2.E2.m1.5.5.4.1.2.3.2" xref="S2.E2.m1.5.5.4.1.2.3.2.cmml">k</mi><mo id="S2.E2.m1.5.5.4.1.2.3.1" xref="S2.E2.m1.5.5.4.1.2.3.1.cmml">=</mo><mn id="S2.E2.m1.5.5.4.1.2.3.3" xref="S2.E2.m1.5.5.4.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.5.5.4.1.3" xref="S2.E2.m1.5.5.4.1.3.cmml">m</mi></munderover><mrow id="S2.E2.m1.5.5.4.2" xref="S2.E2.m1.5.5.4.2.cmml"><msub id="S2.E2.m1.5.5.4.2.2" xref="S2.E2.m1.5.5.4.2.2.cmml"><mi id="S2.E2.m1.5.5.4.2.2.2" xref="S2.E2.m1.5.5.4.2.2.2.cmml">p</mi><mi id="S2.E2.m1.5.5.4.2.2.3" xref="S2.E2.m1.5.5.4.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.4.2.1" xref="S2.E2.m1.5.5.4.2.1.cmml">‚Äã</mo><msub id="S2.E2.m1.5.5.4.2.3" xref="S2.E2.m1.5.5.4.2.3.cmml"><mi id="S2.E2.m1.5.5.4.2.3.2" xref="S2.E2.m1.5.5.4.2.3.2.cmml">F</mi><mi id="S2.E2.m1.5.5.4.2.3.3" xref="S2.E2.m1.5.5.4.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.4.2.1a" xref="S2.E2.m1.5.5.4.2.1.cmml">‚Äã</mo><mrow id="S2.E2.m1.5.5.4.2.4.2" xref="S2.E2.m1.5.5.4.2.cmml"><mo stretchy="false" id="S2.E2.m1.5.5.4.2.4.2.1" xref="S2.E2.m1.5.5.4.2.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">w</mi><mo stretchy="false" id="S2.E2.m1.5.5.4.2.4.2.2" xref="S2.E2.m1.5.5.4.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.5b"><apply id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5"><csymbol cd="latexml" id="S2.E2.m1.5.5.3.cmml" xref="S2.E2.m1.5.5.3">assign</csymbol><list id="S2.E2.m1.5.5.2.3.cmml" xref="S2.E2.m1.5.5.2.2"><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"></times><apply id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"><ci id="S2.E2.m1.4.4.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.1">ùë§</ci><min id="S2.E2.m1.4.4.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2"></min></apply><ci id="S2.E2.m1.4.4.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.3">ùêπ</ci><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ùë§</ci></apply><apply id="S2.E2.m1.5.5.2.2.2.cmml" xref="S2.E2.m1.5.5.2.2.2"><times id="S2.E2.m1.5.5.2.2.2.1.cmml" xref="S2.E2.m1.5.5.2.2.2.1"></times><ci id="S2.E2.m1.5.5.2.2.2.2a.cmml" xref="S2.E2.m1.5.5.2.2.2.2"><mtext id="S2.E2.m1.5.5.2.2.2.2.cmml" xref="S2.E2.m1.5.5.2.2.2.2">¬†where¬†</mtext></ci><ci id="S2.E2.m1.5.5.2.2.2.3.cmml" xref="S2.E2.m1.5.5.2.2.2.3">ùêπ</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ùë§</ci></apply></list><apply id="S2.E2.m1.5.5.4.cmml" xref="S2.E2.m1.5.5.4"><apply id="S2.E2.m1.5.5.4.1.cmml" xref="S2.E2.m1.5.5.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.4.1.1.cmml" xref="S2.E2.m1.5.5.4.1">superscript</csymbol><apply id="S2.E2.m1.5.5.4.1.2.cmml" xref="S2.E2.m1.5.5.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.4.1.2.1.cmml" xref="S2.E2.m1.5.5.4.1">subscript</csymbol><sum id="S2.E2.m1.5.5.4.1.2.2.cmml" xref="S2.E2.m1.5.5.4.1.2.2"></sum><apply id="S2.E2.m1.5.5.4.1.2.3.cmml" xref="S2.E2.m1.5.5.4.1.2.3"><eq id="S2.E2.m1.5.5.4.1.2.3.1.cmml" xref="S2.E2.m1.5.5.4.1.2.3.1"></eq><ci id="S2.E2.m1.5.5.4.1.2.3.2.cmml" xref="S2.E2.m1.5.5.4.1.2.3.2">ùëò</ci><cn type="integer" id="S2.E2.m1.5.5.4.1.2.3.3.cmml" xref="S2.E2.m1.5.5.4.1.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.5.5.4.1.3.cmml" xref="S2.E2.m1.5.5.4.1.3">ùëö</ci></apply><apply id="S2.E2.m1.5.5.4.2.cmml" xref="S2.E2.m1.5.5.4.2"><times id="S2.E2.m1.5.5.4.2.1.cmml" xref="S2.E2.m1.5.5.4.2.1"></times><apply id="S2.E2.m1.5.5.4.2.2.cmml" xref="S2.E2.m1.5.5.4.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.4.2.2.1.cmml" xref="S2.E2.m1.5.5.4.2.2">subscript</csymbol><ci id="S2.E2.m1.5.5.4.2.2.2.cmml" xref="S2.E2.m1.5.5.4.2.2.2">ùëù</ci><ci id="S2.E2.m1.5.5.4.2.2.3.cmml" xref="S2.E2.m1.5.5.4.2.2.3">ùëò</ci></apply><apply id="S2.E2.m1.5.5.4.2.3.cmml" xref="S2.E2.m1.5.5.4.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.4.2.3.1.cmml" xref="S2.E2.m1.5.5.4.2.3">subscript</csymbol><ci id="S2.E2.m1.5.5.4.2.3.2.cmml" xref="S2.E2.m1.5.5.4.2.3.2">ùêπ</ci><ci id="S2.E2.m1.5.5.4.2.3.3.cmml" xref="S2.E2.m1.5.5.4.2.3.3">ùëò</ci></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ùë§</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.5c">\underset{w}{\min}F(w),\text{ \ \ \ where \ \ \ }F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.6" class="ltx_p">where <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">m</annotation></semantics></math> is the overall number of devices/centers, <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><msub id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.2" xref="S2.SS2.p4.2.m2.1.1.2.cmml">F</mi><mi id="S2.SS2.p4.2.m2.1.1.3" xref="S2.SS2.p4.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.2.m2.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p4.2.m2.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.2">ùêπ</ci><ci id="S2.SS2.p4.2.m2.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">F_{k}</annotation></semantics></math> is the local objective function for the <math id="S2.SS2.p4.3.m3.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.SS2.p4.3.m3.1a"><msup id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml"><mi id="S2.SS2.p4.3.m3.1.1.2" xref="S2.SS2.p4.3.m3.1.1.2.cmml">k</mi><mrow id="S2.SS2.p4.3.m3.1.1.3" xref="S2.SS2.p4.3.m3.1.1.3.cmml"><mi id="S2.SS2.p4.3.m3.1.1.3.2" xref="S2.SS2.p4.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.3.m3.1.1.3.1" xref="S2.SS2.p4.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S2.SS2.p4.3.m3.1.1.3.3" xref="S2.SS2.p4.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><apply id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1">superscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2">ùëò</ci><apply id="S2.SS2.p4.3.m3.1.1.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3"><times id="S2.SS2.p4.3.m3.1.1.3.1.cmml" xref="S2.SS2.p4.3.m3.1.1.3.1"></times><ci id="S2.SS2.p4.3.m3.1.1.3.2.cmml" xref="S2.SS2.p4.3.m3.1.1.3.2">ùë°</ci><ci id="S2.SS2.p4.3.m3.1.1.3.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">k^{th}</annotation></semantics></math> device/center, and <math id="S2.SS2.p4.4.m4.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S2.SS2.p4.4.m4.1a"><msub id="S2.SS2.p4.4.m4.1.1" xref="S2.SS2.p4.4.m4.1.1.cmml"><mi id="S2.SS2.p4.4.m4.1.1.2" xref="S2.SS2.p4.4.m4.1.1.2.cmml">p</mi><mi id="S2.SS2.p4.4.m4.1.1.3" xref="S2.SS2.p4.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.4.m4.1b"><apply id="S2.SS2.p4.4.m4.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.4.m4.1.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p4.4.m4.1.1.2.cmml" xref="S2.SS2.p4.4.m4.1.1.2">ùëù</ci><ci id="S2.SS2.p4.4.m4.1.1.3.cmml" xref="S2.SS2.p4.4.m4.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.4.m4.1c">p_{k}</annotation></semantics></math> represents relative impact of every device/center with <math id="S2.SS2.p4.5.m5.1" class="ltx_Math" alttext="p_{k}\geq 0" display="inline"><semantics id="S2.SS2.p4.5.m5.1a"><mrow id="S2.SS2.p4.5.m5.1.1" xref="S2.SS2.p4.5.m5.1.1.cmml"><msub id="S2.SS2.p4.5.m5.1.1.2" xref="S2.SS2.p4.5.m5.1.1.2.cmml"><mi id="S2.SS2.p4.5.m5.1.1.2.2" xref="S2.SS2.p4.5.m5.1.1.2.2.cmml">p</mi><mi id="S2.SS2.p4.5.m5.1.1.2.3" xref="S2.SS2.p4.5.m5.1.1.2.3.cmml">k</mi></msub><mo id="S2.SS2.p4.5.m5.1.1.1" xref="S2.SS2.p4.5.m5.1.1.1.cmml">‚â•</mo><mn id="S2.SS2.p4.5.m5.1.1.3" xref="S2.SS2.p4.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.5.m5.1b"><apply id="S2.SS2.p4.5.m5.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1"><geq id="S2.SS2.p4.5.m5.1.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1.1"></geq><apply id="S2.SS2.p4.5.m5.1.1.2.cmml" xref="S2.SS2.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p4.5.m5.1.1.2.1.cmml" xref="S2.SS2.p4.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.p4.5.m5.1.1.2.2.cmml" xref="S2.SS2.p4.5.m5.1.1.2.2">ùëù</ci><ci id="S2.SS2.p4.5.m5.1.1.2.3.cmml" xref="S2.SS2.p4.5.m5.1.1.2.3">ùëò</ci></apply><cn type="integer" id="S2.SS2.p4.5.m5.1.1.3.cmml" xref="S2.SS2.p4.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.5.m5.1c">p_{k}\geq 0</annotation></semantics></math> and <math id="S2.SS2.p4.6.m6.1" class="ltx_Math" alttext="\sum\limits_{k=1}^{m}p_{k}=1." display="inline"><semantics id="S2.SS2.p4.6.m6.1a"><mrow id="S2.SS2.p4.6.m6.1.1.1" xref="S2.SS2.p4.6.m6.1.1.1.1.cmml"><mrow id="S2.SS2.p4.6.m6.1.1.1.1" xref="S2.SS2.p4.6.m6.1.1.1.1.cmml"><mrow id="S2.SS2.p4.6.m6.1.1.1.1.2" xref="S2.SS2.p4.6.m6.1.1.1.1.2.cmml"><munderover id="S2.SS2.p4.6.m6.1.1.1.1.2.1" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.cmml"><mo movablelimits="false" id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.2" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.2.cmml">‚àë</mo><mrow id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.cmml"><mi id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.2" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.2.cmml">k</mi><mo id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.1" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.1.cmml">=</mo><mn id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.3" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.3.cmml">1</mn></mrow><mi id="S2.SS2.p4.6.m6.1.1.1.1.2.1.3" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.3.cmml">m</mi></munderover><msub id="S2.SS2.p4.6.m6.1.1.1.1.2.2" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2.cmml"><mi id="S2.SS2.p4.6.m6.1.1.1.1.2.2.2" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2.2.cmml">p</mi><mi id="S2.SS2.p4.6.m6.1.1.1.1.2.2.3" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2.3.cmml">k</mi></msub></mrow><mo id="S2.SS2.p4.6.m6.1.1.1.1.1" xref="S2.SS2.p4.6.m6.1.1.1.1.1.cmml">=</mo><mn id="S2.SS2.p4.6.m6.1.1.1.1.3" xref="S2.SS2.p4.6.m6.1.1.1.1.3.cmml">1</mn></mrow><mo lspace="0em" id="S2.SS2.p4.6.m6.1.1.1.2" xref="S2.SS2.p4.6.m6.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.6.m6.1b"><apply id="S2.SS2.p4.6.m6.1.1.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1"><eq id="S2.SS2.p4.6.m6.1.1.1.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.1"></eq><apply id="S2.SS2.p4.6.m6.1.1.1.1.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2"><apply id="S2.SS2.p4.6.m6.1.1.1.1.2.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S2.SS2.p4.6.m6.1.1.1.1.2.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1">superscript</csymbol><apply id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1">subscript</csymbol><sum id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.2"></sum><apply id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3"><eq id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.1"></eq><ci id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.2">ùëò</ci><cn type="integer" id="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.3.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.2.3.3">1</cn></apply></apply><ci id="S2.SS2.p4.6.m6.1.1.1.1.2.1.3.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.1.3">ùëö</ci></apply><apply id="S2.SS2.p4.6.m6.1.1.1.1.2.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p4.6.m6.1.1.1.1.2.2.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2">subscript</csymbol><ci id="S2.SS2.p4.6.m6.1.1.1.1.2.2.2.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2.2">ùëù</ci><ci id="S2.SS2.p4.6.m6.1.1.1.1.2.2.3.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.2.2.3">ùëò</ci></apply></apply><cn type="integer" id="S2.SS2.p4.6.m6.1.1.1.1.3.cmml" xref="S2.SS2.p4.6.m6.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.6.m6.1c">\sum\limits_{k=1}^{m}p_{k}=1.</annotation></semantics></math></p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.6" class="ltx_p">The empirical risk for a local dataset can be expressed using the local objective function <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><msub id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mi id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml">F</mi><mi id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2">ùêπ</ci><ci id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">F_{k}</annotation></semantics></math>. The relative weight or influence of each device, represented by <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><msub id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><mi id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml">p</mi><mi id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2">ùëù</ci><ci id="S2.SS2.p5.2.m2.1.1.3.cmml" xref="S2.SS2.p5.2.m2.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">p_{k}</annotation></semantics></math>, can be set by the user. There are two common choices for setting <math id="S2.SS2.p5.3.m3.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S2.SS2.p5.3.m3.1a"><msub id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml"><mi id="S2.SS2.p5.3.m3.1.1.2" xref="S2.SS2.p5.3.m3.1.1.2.cmml">p</mi><mi id="S2.SS2.p5.3.m3.1.1.3" xref="S2.SS2.p5.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><apply id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.3.m3.1.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p5.3.m3.1.1.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2">ùëù</ci><ci id="S2.SS2.p5.3.m3.1.1.3.cmml" xref="S2.SS2.p5.3.m3.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">p_{k}</annotation></semantics></math>: <math id="S2.SS2.p5.4.m4.1" class="ltx_Math" alttext="p_{k}=\frac{1}{m}" display="inline"><semantics id="S2.SS2.p5.4.m4.1a"><mrow id="S2.SS2.p5.4.m4.1.1" xref="S2.SS2.p5.4.m4.1.1.cmml"><msub id="S2.SS2.p5.4.m4.1.1.2" xref="S2.SS2.p5.4.m4.1.1.2.cmml"><mi id="S2.SS2.p5.4.m4.1.1.2.2" xref="S2.SS2.p5.4.m4.1.1.2.2.cmml">p</mi><mi id="S2.SS2.p5.4.m4.1.1.2.3" xref="S2.SS2.p5.4.m4.1.1.2.3.cmml">k</mi></msub><mo id="S2.SS2.p5.4.m4.1.1.1" xref="S2.SS2.p5.4.m4.1.1.1.cmml">=</mo><mfrac id="S2.SS2.p5.4.m4.1.1.3" xref="S2.SS2.p5.4.m4.1.1.3.cmml"><mn id="S2.SS2.p5.4.m4.1.1.3.2" xref="S2.SS2.p5.4.m4.1.1.3.2.cmml">1</mn><mi id="S2.SS2.p5.4.m4.1.1.3.3" xref="S2.SS2.p5.4.m4.1.1.3.3.cmml">m</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.4.m4.1b"><apply id="S2.SS2.p5.4.m4.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1"><eq id="S2.SS2.p5.4.m4.1.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1.1"></eq><apply id="S2.SS2.p5.4.m4.1.1.2.cmml" xref="S2.SS2.p5.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.4.m4.1.1.2.1.cmml" xref="S2.SS2.p5.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.4.m4.1.1.2.2.cmml" xref="S2.SS2.p5.4.m4.1.1.2.2">ùëù</ci><ci id="S2.SS2.p5.4.m4.1.1.2.3.cmml" xref="S2.SS2.p5.4.m4.1.1.2.3">ùëò</ci></apply><apply id="S2.SS2.p5.4.m4.1.1.3.cmml" xref="S2.SS2.p5.4.m4.1.1.3"><divide id="S2.SS2.p5.4.m4.1.1.3.1.cmml" xref="S2.SS2.p5.4.m4.1.1.3"></divide><cn type="integer" id="S2.SS2.p5.4.m4.1.1.3.2.cmml" xref="S2.SS2.p5.4.m4.1.1.3.2">1</cn><ci id="S2.SS2.p5.4.m4.1.1.3.3.cmml" xref="S2.SS2.p5.4.m4.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.4.m4.1c">p_{k}=\frac{1}{m}</annotation></semantics></math> or <math id="S2.SS2.p5.5.m5.1" class="ltx_Math" alttext="p_{k}=\frac{nk}{n}" display="inline"><semantics id="S2.SS2.p5.5.m5.1a"><mrow id="S2.SS2.p5.5.m5.1.1" xref="S2.SS2.p5.5.m5.1.1.cmml"><msub id="S2.SS2.p5.5.m5.1.1.2" xref="S2.SS2.p5.5.m5.1.1.2.cmml"><mi id="S2.SS2.p5.5.m5.1.1.2.2" xref="S2.SS2.p5.5.m5.1.1.2.2.cmml">p</mi><mi id="S2.SS2.p5.5.m5.1.1.2.3" xref="S2.SS2.p5.5.m5.1.1.2.3.cmml">k</mi></msub><mo id="S2.SS2.p5.5.m5.1.1.1" xref="S2.SS2.p5.5.m5.1.1.1.cmml">=</mo><mfrac id="S2.SS2.p5.5.m5.1.1.3" xref="S2.SS2.p5.5.m5.1.1.3.cmml"><mrow id="S2.SS2.p5.5.m5.1.1.3.2" xref="S2.SS2.p5.5.m5.1.1.3.2.cmml"><mi id="S2.SS2.p5.5.m5.1.1.3.2.2" xref="S2.SS2.p5.5.m5.1.1.3.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p5.5.m5.1.1.3.2.1" xref="S2.SS2.p5.5.m5.1.1.3.2.1.cmml">‚Äã</mo><mi id="S2.SS2.p5.5.m5.1.1.3.2.3" xref="S2.SS2.p5.5.m5.1.1.3.2.3.cmml">k</mi></mrow><mi id="S2.SS2.p5.5.m5.1.1.3.3" xref="S2.SS2.p5.5.m5.1.1.3.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.5.m5.1b"><apply id="S2.SS2.p5.5.m5.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1"><eq id="S2.SS2.p5.5.m5.1.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1.1"></eq><apply id="S2.SS2.p5.5.m5.1.1.2.cmml" xref="S2.SS2.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.5.m5.1.1.2.1.cmml" xref="S2.SS2.p5.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.5.m5.1.1.2.2.cmml" xref="S2.SS2.p5.5.m5.1.1.2.2">ùëù</ci><ci id="S2.SS2.p5.5.m5.1.1.2.3.cmml" xref="S2.SS2.p5.5.m5.1.1.2.3">ùëò</ci></apply><apply id="S2.SS2.p5.5.m5.1.1.3.cmml" xref="S2.SS2.p5.5.m5.1.1.3"><divide id="S2.SS2.p5.5.m5.1.1.3.1.cmml" xref="S2.SS2.p5.5.m5.1.1.3"></divide><apply id="S2.SS2.p5.5.m5.1.1.3.2.cmml" xref="S2.SS2.p5.5.m5.1.1.3.2"><times id="S2.SS2.p5.5.m5.1.1.3.2.1.cmml" xref="S2.SS2.p5.5.m5.1.1.3.2.1"></times><ci id="S2.SS2.p5.5.m5.1.1.3.2.2.cmml" xref="S2.SS2.p5.5.m5.1.1.3.2.2">ùëõ</ci><ci id="S2.SS2.p5.5.m5.1.1.3.2.3.cmml" xref="S2.SS2.p5.5.m5.1.1.3.2.3">ùëò</ci></apply><ci id="S2.SS2.p5.5.m5.1.1.3.3.cmml" xref="S2.SS2.p5.5.m5.1.1.3.3">ùëõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.5.m5.1c">p_{k}=\frac{nk}{n}</annotation></semantics></math>, where <math id="S2.SS2.p5.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p5.6.m6.1a"><mi id="S2.SS2.p5.6.m6.1.1" xref="S2.SS2.p5.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.6.m6.1b"><ci id="S2.SS2.p5.6.m6.1.1.cmml" xref="S2.SS2.p5.6.m6.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.6.m6.1c">n</annotation></semantics></math> is the total number of samples from all devices.
Although this is a common FL objective, there are other alternatives, such as multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, where related local models are learned simultaneously and each client corresponds to a task. Both multi-task learning and meta-learning allow for personalized or device-specific modeling, which can be a useful approach to handle the statistical heterogeneity of the data in FL. While FL and classical distributed learning both aim to minimize the empirical risk across distributed entities, there are several challenges that must be addressed when implementing this objective in a federated setting.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Aggregation approaches in FL</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.3" class="ltx_p">Following local training, the models are combined using an aggregation algorithm. To be more specific, each model takes an update step in its respective center <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">k</annotation></semantics></math>, utilizing a learning rate <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">Œ∑</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ùúÇ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\eta</annotation></semantics></math> and the gradients <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="g_{k}" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><msub id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">g</mi><mi id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">ùëî</ci><ci id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">g_{k}</annotation></semantics></math>, so that <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="w_{t+1}^{k}=w_{t}-\eta.g_{k},\forall k" display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.3.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><msubsup id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.2.2.2" xref="S2.E3.m1.1.1.1.1.2.2.2.cmml">w</mi><mrow id="S2.E3.m1.1.1.1.1.2.2.3" xref="S2.E3.m1.1.1.1.1.2.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.2.2.3.2" xref="S2.E3.m1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S2.E3.m1.1.1.1.1.2.2.3.1" xref="S2.E3.m1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="S2.E3.m1.1.1.1.1.2.2.3.3" xref="S2.E3.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E3.m1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml"><msub id="S2.E3.m1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.3.2.2.cmml">w</mi><mi id="S2.E3.m1.1.1.1.1.3.2.3" xref="S2.E3.m1.1.1.1.1.3.2.3.cmml">t</mi></msub><mo id="S2.E3.m1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.3.1.cmml">‚àí</mo><mi id="S2.E3.m1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.3.3.cmml">Œ∑</mi></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S2.E3.m1.2.2.2.3" xref="S2.E3.m1.2.2.3a.cmml">.</mo><mrow id="S2.E3.m1.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.3.cmml"><msub id="S2.E3.m1.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.1.1.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.1.1.2.cmml">g</mi><mi id="S2.E3.m1.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.1.1.3.cmml">k</mi></msub><mo id="S2.E3.m1.2.2.2.2.2.3" xref="S2.E3.m1.2.2.2.2.3.cmml">,</mo><mrow id="S2.E3.m1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.cmml"><mo rspace="0.167em" id="S2.E3.m1.2.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.2.1.cmml">‚àÄ</mo><mi id="S2.E3.m1.2.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.2.cmml">k</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.3a.cmml" xref="S2.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"></eq><apply id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.1.1.1.1.2.2.2">ùë§</ci><apply id="S2.E3.m1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.1.1.1.1.2.2.3"><plus id="S2.E3.m1.1.1.1.1.2.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.2.2.3.1"></plus><ci id="S2.E3.m1.1.1.1.1.2.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.2.2.3.2">ùë°</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.2.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E3.m1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.2.3">ùëò</ci></apply><apply id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3"><minus id="S2.E3.m1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3.1"></minus><apply id="S2.E3.m1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2.2">ùë§</ci><ci id="S2.E3.m1.1.1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.3.2.3">ùë°</ci></apply><ci id="S2.E3.m1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3">ùúÇ</ci></apply></apply><list id="S2.E3.m1.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2"><apply id="S2.E3.m1.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2">ùëî</ci><ci id="S2.E3.m1.2.2.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3">ùëò</ci></apply><apply id="S2.E3.m1.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2"><csymbol cd="latexml" id="S2.E3.m1.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.1">for-all</csymbol><ci id="S2.E3.m1.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2">ùëò</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">w_{t+1}^{k}=w_{t}-\eta.g_{k},\forall k</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Subsequently, the weights are gathered into the global model in a manner that is relative to the sample size of each center <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>:</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.1" class="ltx_Math" alttext="W_{t+1}=\sum_{k=1}^{m}\frac{n_{k}}{n}w_{t+1}^{k}" display="block"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><msub id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.2.2" xref="S2.E4.m1.1.1.2.2.cmml">W</mi><mrow id="S2.E4.m1.1.1.2.3" xref="S2.E4.m1.1.1.2.3.cmml"><mi id="S2.E4.m1.1.1.2.3.2" xref="S2.E4.m1.1.1.2.3.2.cmml">t</mi><mo id="S2.E4.m1.1.1.2.3.1" xref="S2.E4.m1.1.1.2.3.1.cmml">+</mo><mn id="S2.E4.m1.1.1.2.3.3" xref="S2.E4.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo rspace="0.111em" id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml">=</mo><mrow id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml"><munderover id="S2.E4.m1.1.1.3.1" xref="S2.E4.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E4.m1.1.1.3.1.2.2" xref="S2.E4.m1.1.1.3.1.2.2.cmml">‚àë</mo><mrow id="S2.E4.m1.1.1.3.1.2.3" xref="S2.E4.m1.1.1.3.1.2.3.cmml"><mi id="S2.E4.m1.1.1.3.1.2.3.2" xref="S2.E4.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S2.E4.m1.1.1.3.1.2.3.1" xref="S2.E4.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E4.m1.1.1.3.1.2.3.3" xref="S2.E4.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.1.1.3.1.3" xref="S2.E4.m1.1.1.3.1.3.cmml">m</mi></munderover><mrow id="S2.E4.m1.1.1.3.2" xref="S2.E4.m1.1.1.3.2.cmml"><mfrac id="S2.E4.m1.1.1.3.2.2" xref="S2.E4.m1.1.1.3.2.2.cmml"><msub id="S2.E4.m1.1.1.3.2.2.2" xref="S2.E4.m1.1.1.3.2.2.2.cmml"><mi id="S2.E4.m1.1.1.3.2.2.2.2" xref="S2.E4.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="S2.E4.m1.1.1.3.2.2.2.3" xref="S2.E4.m1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S2.E4.m1.1.1.3.2.2.3" xref="S2.E4.m1.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.2.1" xref="S2.E4.m1.1.1.3.2.1.cmml">‚Äã</mo><msubsup id="S2.E4.m1.1.1.3.2.3" xref="S2.E4.m1.1.1.3.2.3.cmml"><mi id="S2.E4.m1.1.1.3.2.3.2.2" xref="S2.E4.m1.1.1.3.2.3.2.2.cmml">w</mi><mrow id="S2.E4.m1.1.1.3.2.3.2.3" xref="S2.E4.m1.1.1.3.2.3.2.3.cmml"><mi id="S2.E4.m1.1.1.3.2.3.2.3.2" xref="S2.E4.m1.1.1.3.2.3.2.3.2.cmml">t</mi><mo id="S2.E4.m1.1.1.3.2.3.2.3.1" xref="S2.E4.m1.1.1.3.2.3.2.3.1.cmml">+</mo><mn id="S2.E4.m1.1.1.3.2.3.2.3.3" xref="S2.E4.m1.1.1.3.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.1.1.3.2.3.3" xref="S2.E4.m1.1.1.3.2.3.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><eq id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"></eq><apply id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.2.2">ùëä</ci><apply id="S2.E4.m1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.2.3"><plus id="S2.E4.m1.1.1.2.3.1.cmml" xref="S2.E4.m1.1.1.2.3.1"></plus><ci id="S2.E4.m1.1.1.2.3.2.cmml" xref="S2.E4.m1.1.1.2.3.2">ùë°</ci><cn type="integer" id="S2.E4.m1.1.1.2.3.3.cmml" xref="S2.E4.m1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3"><apply id="S2.E4.m1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.1.1.cmml" xref="S2.E4.m1.1.1.3.1">superscript</csymbol><apply id="S2.E4.m1.1.1.3.1.2.cmml" xref="S2.E4.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.1.2.1.cmml" xref="S2.E4.m1.1.1.3.1">subscript</csymbol><sum id="S2.E4.m1.1.1.3.1.2.2.cmml" xref="S2.E4.m1.1.1.3.1.2.2"></sum><apply id="S2.E4.m1.1.1.3.1.2.3.cmml" xref="S2.E4.m1.1.1.3.1.2.3"><eq id="S2.E4.m1.1.1.3.1.2.3.1.cmml" xref="S2.E4.m1.1.1.3.1.2.3.1"></eq><ci id="S2.E4.m1.1.1.3.1.2.3.2.cmml" xref="S2.E4.m1.1.1.3.1.2.3.2">ùëò</ci><cn type="integer" id="S2.E4.m1.1.1.3.1.2.3.3.cmml" xref="S2.E4.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.1.1.3.1.3.cmml" xref="S2.E4.m1.1.1.3.1.3">ùëö</ci></apply><apply id="S2.E4.m1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.3.2"><times id="S2.E4.m1.1.1.3.2.1.cmml" xref="S2.E4.m1.1.1.3.2.1"></times><apply id="S2.E4.m1.1.1.3.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2"><divide id="S2.E4.m1.1.1.3.2.2.1.cmml" xref="S2.E4.m1.1.1.3.2.2"></divide><apply id="S2.E4.m1.1.1.3.2.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.2.2.1.cmml" xref="S2.E4.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E4.m1.1.1.3.2.2.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2.2.2">ùëõ</ci><ci id="S2.E4.m1.1.1.3.2.2.2.3.cmml" xref="S2.E4.m1.1.1.3.2.2.2.3">ùëò</ci></apply><ci id="S2.E4.m1.1.1.3.2.2.3.cmml" xref="S2.E4.m1.1.1.3.2.2.3">ùëõ</ci></apply><apply id="S2.E4.m1.1.1.3.2.3.cmml" xref="S2.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.3.1.cmml" xref="S2.E4.m1.1.1.3.2.3">superscript</csymbol><apply id="S2.E4.m1.1.1.3.2.3.2.cmml" xref="S2.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.3.2.1.cmml" xref="S2.E4.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.E4.m1.1.1.3.2.3.2.2.cmml" xref="S2.E4.m1.1.1.3.2.3.2.2">ùë§</ci><apply id="S2.E4.m1.1.1.3.2.3.2.3.cmml" xref="S2.E4.m1.1.1.3.2.3.2.3"><plus id="S2.E4.m1.1.1.3.2.3.2.3.1.cmml" xref="S2.E4.m1.1.1.3.2.3.2.3.1"></plus><ci id="S2.E4.m1.1.1.3.2.3.2.3.2.cmml" xref="S2.E4.m1.1.1.3.2.3.2.3.2">ùë°</ci><cn type="integer" id="S2.E4.m1.1.1.3.2.3.2.3.3.cmml" xref="S2.E4.m1.1.1.3.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.1.1.3.2.3.3.cmml" xref="S2.E4.m1.1.1.3.2.3.3">ùëò</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">W_{t+1}=\sum_{k=1}^{m}\frac{n_{k}}{n}w_{t+1}^{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p">There are numerous FL aggregation techniques in the literature. In the following, we will discuss some of the most useful techniques specifically applicable to CV-based FL. These techniques include:
<br class="ltx_break"></p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS1.5.1.1" class="ltx_text">II-C</span>1 </span>Averaging aggregation</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">There are many federated averaging (FedAvg) aggregation strategies in the literature. FedAvg algorithm is thoroughly detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Fig. <a href="#S2.F5" title="Figure 5 ‚Ä£ II-C1 Averaging aggregation ‚Ä£ II-C Aggregation approaches in FL ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a summary of the main differences, characteristics, and principles among the alternatives.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2308.13558/assets/x5.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="380" height="360" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of key federated averaging Aggregation Approaches.</figcaption>
</figure>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS2.5.1.1" class="ltx_text">II-C</span>2 </span>Progressive Fourier aggregation</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> addressed the retrogress and class imbalance problems using a personalized FL approach. Precisely, for better integrating the parameters of client models in the frequency domain, a progressive Fourier aggregation (PFA) is used at the server. Next, a deputy-enhanced transfer (DET) is designed at the clients‚Äô side to easily share overall knowledge with the personalized local model. In particular, the approach involves the development of PFA at the server, which ensures a stable and efficient gathering of global knowledge. This is achieved by gradually integrating client models from low-frequency to high-frequency. Additionally, the authors introduce a deputy model at the client‚Äôs end to receive the aggregated server model. This facilitates the implementation of the DET strategy, which follows three types of decisions (<math id="S2.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="d_{k}" display="inline"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><msub id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p1.1.m1.1.1.2" xref="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml">d</mi><mi id="S2.SS3.SSS2.p1.1.m1.1.1.3" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><apply id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.2">ùëë</ci><ci id="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">d_{k}</annotation></semantics></math>): Recover-Exchange-Sublimate. These steps aim to enhance the personalized local model by smoothly transferring global knowledge. In this process, the advantage of utilizing the Fast Fourier Transform (FFT) is to obtain both the amplitude map and the phase map of the parameters (conversion from parameters to map). The inverse FFT (IFFT) is used for the inverse operation (conversion from map to parameters). Fig. <a href="#S2.F6" title="Figure 6 ‚Ä£ II-C2 Progressive Fourier aggregation ‚Ä£ II-C Aggregation approaches in FL ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the principle of the PFA algorithm, which can be employed as an aggregation technique in FL. 
<br class="ltx_break"></p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2308.13558/assets/x6.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="281" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Principle of PFA algorithm. </figcaption>
</figure>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS3.5.1.1" class="ltx_text">II-C</span>3 </span>FedGKT aggregation</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">FL group knowledge transfer (FedGKT) represents a streamlined FL approach tailored to edge devices with limited resources. Its primary objective is to combine the advantages of FedAvg and split learning (SL) by employing local stochastic gradient descent (SGD) training, similar to FedAvg, while simultaneously ensuring low computational burden at the edge, akin to SL. Notably, FedGKT facilitates the seamless transmission of knowledge from numerous compact edge-trained convolutional neural networks (CNNs) to a significantly larger CNN trained at a cloud server. Fig. <a href="#S2.F7" title="Figure 7 ‚Ä£ II-C3 FedGKT aggregation ‚Ä£ II-C Aggregation approaches in FL ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the overview of FedGKT, (i) at the edge device, a compact CNN is employed, comprising a lightweight feature extractor and classifier, efficiently trained using its private data (local training). (ii) Following local training, consensus is reached among all edge nodes to generate uniform tensor dimensions as output from the feature extractor. Subsequently, the larger server model is trained, wherein the extracted features from the edge-side model serve as inputs. The training process employs a knowledge distillation (KD)-based loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to minimize the disparity between ground truth and soft label predictions, representing probabilistic estimations obtained from the edge-side model ( Periodic transfer). (iii) To enhance the edge model‚Äôs performance, the server conveys its predicted soft labels to the edge, enabling the edge to conduct further training on its local dataset using a KD-based loss function with the server‚Äôs soft labels ( Transfer back). As a result, both the server and edge models mutually benefit from knowledge exchange, leading to performance improvement. (iv) After completion of the training process, the final model is a fusion of the local feature extractor and the shared server model ( Edge-sided model) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. The researchers who proposed FedGKT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> claimed that it enables cost-effective edge training by significantly reducing computational expenses. In comparison to edge training with FedAvg, FedGKT demands only 9 to 17 times less computational power (measured in FLOPs) and requires a substantially smaller number of parameters, ranging from 54 to 105 times fewer.</p>
</div>
<figure id="S2.F7" class="ltx_figure"><img src="/html/2308.13558/assets/x7.png" id="S2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Principle of FedGKT algorithm.</figcaption>
</figure>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Privacy technologies of FL</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">FL places a strong emphasis on privacy management, which involves analyzing security models in order to effectively protect personal information. In this section, various technologies that are currently used to safeguard privacy in the context of FL will be discussed.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS1.5.1.1" class="ltx_text">II-D</span>1 </span>Secure MPC model</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Secure MPC (SMPC) models involve data from multiple parties and use a well-defined simulation framework to provide safety certification. These models are designed to ensure that there is no interaction or sharing of knowledge data between the parties, meaning that users are unaware of the input and output data as well as any other information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. This zero-knowledge model can be considered a form of secure and complex computing protocol that is based on publicly available knowledge. Research has shown that SMPC can be used to create a security model that improves computing efficiency in low-security environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
The MPC protocol enables model training and verification without requiring users to share sensitive data. However, SMPC has certain limitations. It is unable to prevent servers from being curious and may not provide sufficient protection against privacy attacks from other clients. Additionally, the four-round interactive nature of the protocol can lead to wasted data and reduced model accuracy as the server does not have access to the client data until the submission phase is completed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS2.5.1.1" class="ltx_text">II-D</span>2 </span>Differential privacy</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">Differential privacy technology, as indicated by past studies, has been employed as a protective measure for data privacy, where it processes and obscures sensitive attributes. This practice makes it challenging for third parties to identify individual users, thus rendering the data irrecoverable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. However, this method necessitates the transfer of data, which can negatively impact data accuracy, indicating a compromise between accuracy and privacy in such scenarios.
In addition, some researchers have suggested the incorporation of differential privacy within FL to safeguard client data by masking client contributions during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> introduced a differentially private FL framework for analyzing histopathology images, which are considered some of the most intricate medical images. The authors assessed how data distribution, the number of healthcare providers, and individual dataset sizes influenced the framework‚Äôs performance. They used the cancer genome Atlas (TCGA) dataset, an openly accessible repository, to mimic a distributed environment.
Furthermore, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> presented an FL framework capable of generating a global model from distributed health data stored in diverse locations, without the need to move or share raw data. This framework also ensures privacy protection through differential privacy. An evaluation using real electronic health data from a million patients across two healthcare applications indicated the framework‚Äôs success in delivering considerable privacy without compromising the global model‚Äôs utility.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.p2.1" class="ltx_p">Differential privacy can be used in the training of DNN through the use of the differentially private stochastic gradient descent (DP-SGD) algorithm. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> introduces deepee, a free and open-source framework for differentially private DL that is compatible with the PyTorch DL framework for medical image analysis. This study uses parallel processing to calculate and modify the gradients for each sample. This process is made efficient through the use of a data structure that stores shared memory references to the neural network weights in order to save memory. It also provides specialized data loading procedures and privacy budget tracking based on the Gaussian differential privacy framework, as well as the ability to automatically modify the user-provided neural network architecture to ensure that it adheres to DP standards. Besides, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> incorporates FL into the DL of medical image analysis models to enhance the protection of local models and prevent adversaries from inferring private medical data through attacks such as model reconstruction or model inversion. Cryptographic techniques such as masks and homomorphic encryption are utilized. Instead of relying on the size of the datasets, as is commonly done in DL, the contribution rate of each local model to the global model during each training epoch is determined based on the qualities of the datasets owned by the participating entities. Additionally, a dropout-tolerant approach for FL is presented in which the process is not interrupted if the number of online clients is above a predetermined threshold.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS3.5.1.1" class="ltx_text">II-D</span>3 </span>Homomorphic encryption</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">Homomorphic encryption is a type of encryption used in the ML process that uses parameter exchange to protect the privacy of user data. Unlike differential privacy, it does not transmit the data or models themselves and encrypts the data without allowing it to be discovered. This makes it less likely for the original data to be leaked. The additive homomorphic encryption model is the most commonly used version in practice.
Besides, despite the benefits of FL, there is a risk that private or sensitive personal data may be exposed through membership attacks when model parameters or summary statistics are shared with a central site. To address this issue, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> presents a secure FL-based framework that employs fully homomorphic encryption (FHE). In doing so, the Cheon-Kim-Kim-Song (CKKS) construction has been used,
which enables the execution of approximate calculations, on real and floating-point numbers benefiting from ciphertext rescaling and packing. Moving on, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> introduces a homomorphic encryption and blockchain-based privacy-preserving aggregation framework for medical image analysis.
This allows hospitals to collaborate and train encrypted federated models while maintaining data privacy, using blockchain ledger technology to decentralize the FL process without the need for a central server.
Specifically, homomorphic encryption protects the privacy of the model‚Äôs gradients. This framework involves training a local model using a capsule network for the segmentation and classification of COVID-19 images, securing the local model with the homomorphic encryption scheme, and sharing the model over a decentralized platform using a blockchain-based FL algorithm.</p>
</div>
</section>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.5.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.6.2" class="ltx_text ltx_font_italic">Learning process</span>
</h3>

<section id="S2.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS5.SSS1.5.1.1" class="ltx_text">II-E</span>1 </span>Supervised FL</h4>

<div id="S2.SS5.SSS1.p1" class="ltx_para">
<p id="S2.SS5.SSS1.p1.1" class="ltx_p">Supervised FL trains ML models on sensitive labeled data across multiple devices and learns to predict an output based on the input data. Each device runs the model and updates its parameters based on its local data, and then the updates are aggregated to improve the global model. The supervision is ensured through the presence of labeled data in the participating entities‚Äô local datasets, and the learning process can be implemented using support vector machines (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, linear models (LMs), neural networks (NN), or decision trees (DT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Fig. <a href="#S2.F8" title="Figure 8 ‚Ä£ II-E1 Supervised FL ‚Ä£ II-E Learning process ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates an example of a typical supervised FL with three clients.</p>
</div>
<figure id="S2.F8" class="ltx_figure"><img src="/html/2308.13558/assets/x8.png" id="S2.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="323" height="456" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The basic setup of a typical FL system with supervised learning involves several steps. Initially, the server picks three clients and transmits the global model <math id="S2.F8.3.m1.1" class="ltx_Math" alttext="w_{t}^{g}" display="inline"><semantics id="S2.F8.3.m1.1b"><msubsup id="S2.F8.3.m1.1.1" xref="S2.F8.3.m1.1.1.cmml"><mi id="S2.F8.3.m1.1.1.2.2" xref="S2.F8.3.m1.1.1.2.2.cmml">w</mi><mi id="S2.F8.3.m1.1.1.2.3" xref="S2.F8.3.m1.1.1.2.3.cmml">t</mi><mi id="S2.F8.3.m1.1.1.3" xref="S2.F8.3.m1.1.1.3.cmml">g</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F8.3.m1.1c"><apply id="S2.F8.3.m1.1.1.cmml" xref="S2.F8.3.m1.1.1"><csymbol cd="ambiguous" id="S2.F8.3.m1.1.1.1.cmml" xref="S2.F8.3.m1.1.1">superscript</csymbol><apply id="S2.F8.3.m1.1.1.2.cmml" xref="S2.F8.3.m1.1.1"><csymbol cd="ambiguous" id="S2.F8.3.m1.1.1.2.1.cmml" xref="S2.F8.3.m1.1.1">subscript</csymbol><ci id="S2.F8.3.m1.1.1.2.2.cmml" xref="S2.F8.3.m1.1.1.2.2">ùë§</ci><ci id="S2.F8.3.m1.1.1.2.3.cmml" xref="S2.F8.3.m1.1.1.2.3">ùë°</ci></apply><ci id="S2.F8.3.m1.1.1.3.cmml" xref="S2.F8.3.m1.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F8.3.m1.1d">w_{t}^{g}</annotation></semantics></math> to them. Subsequently, these clients employ their labeled data to update the global model <math id="S2.F8.4.m2.1" class="ltx_Math" alttext="w_{t}^{g}" display="inline"><semantics id="S2.F8.4.m2.1b"><msubsup id="S2.F8.4.m2.1.1" xref="S2.F8.4.m2.1.1.cmml"><mi id="S2.F8.4.m2.1.1.2.2" xref="S2.F8.4.m2.1.1.2.2.cmml">w</mi><mi id="S2.F8.4.m2.1.1.2.3" xref="S2.F8.4.m2.1.1.2.3.cmml">t</mi><mi id="S2.F8.4.m2.1.1.3" xref="S2.F8.4.m2.1.1.3.cmml">g</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F8.4.m2.1c"><apply id="S2.F8.4.m2.1.1.cmml" xref="S2.F8.4.m2.1.1"><csymbol cd="ambiguous" id="S2.F8.4.m2.1.1.1.cmml" xref="S2.F8.4.m2.1.1">superscript</csymbol><apply id="S2.F8.4.m2.1.1.2.cmml" xref="S2.F8.4.m2.1.1"><csymbol cd="ambiguous" id="S2.F8.4.m2.1.1.2.1.cmml" xref="S2.F8.4.m2.1.1">subscript</csymbol><ci id="S2.F8.4.m2.1.1.2.2.cmml" xref="S2.F8.4.m2.1.1.2.2">ùë§</ci><ci id="S2.F8.4.m2.1.1.2.3.cmml" xref="S2.F8.4.m2.1.1.2.3">ùë°</ci></apply><ci id="S2.F8.4.m2.1.1.3.cmml" xref="S2.F8.4.m2.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F8.4.m2.1d">w_{t}^{g}</annotation></semantics></math>, generating their respective local models. Once this process is completed, the clients send their updated local models back to the server. The server then aggregates these local models into a new global model using the FedAvg algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S2.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS5.SSS2.5.1.1" class="ltx_text">II-E</span>2 </span>Unsupervised FL</h4>

<div id="S2.SS5.SSS2.p1" class="ltx_para">
<p id="S2.SS5.SSS2.p1.1" class="ltx_p">Unsupervised FL is a variant of FL in which the participating devices do not have access to labeled data. Instead, they must rely on unsupervised learning techniques, such as clustering or dimensionality reduction, to learn useful representations of the data. This can be useful in scenarios where collecting and annotating labeled data is difficult or expensive, or where the data is highly sensitive and cannot be shared with a centralized server. The learning process can be implemented using federated principal component analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, federated k-means <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, etc.</p>
</div>
</section>
<section id="S2.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS5.SSS3.5.1.1" class="ltx_text">II-E</span>3 </span>Semi-supervised FL</h4>

<div id="S2.SS5.SSS3.p1" class="ltx_para">
<p id="S2.SS5.SSS3.p1.1" class="ltx_p">FL allows training ML algorithms with a semi-supervised learning (SSL) process on remote datasets without the need to share the data itself. However, data annotation remains a challenge, especially in fields like medicine and surgery where specialized knowledge is often required. To address this issue, semi-supervised FL has recently been used where the participating devices have access to a dataset that contains both labeled and unlabeled examples. The labeled examples are used to train a model using supervised learning techniques, while the unlabeled examples are used to learn additional useful features using unsupervised techniques such as clustering or dimensionality reduction. Fig. <a href="#S2.F9" title="Figure 9 ‚Ä£ II-E3 Semi-supervised FL ‚Ä£ II-E Learning process ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates an example of a typical semi-supervised FL with three clients.</p>
</div>
<div id="S2.SS5.SSS3.p2" class="ltx_para">
<p id="S2.SS5.SSS3.p2.1" class="ltx_p">In this direction, Kassem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> propose FedCy, a federated SSL (FSSL) system that combines FL with self-supervised learning to improve the performance of surgical phase recognition using a decentralized dataset containing both labeled and unlabeled videos. FedCy uses temporal patterns in the labeled data to guide the unsupervised training of unlabeled data towards task-specific features for phase recognition. This scheme outperforms state-of-the-art (SOTA) FSSL methods on the task of automatically recognizing surgical phases using a multi-institutional dataset of laparoscopic cholecystectomy videos. Additionally, it learns more generalizable features when tested on data from an unseen domain.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> examines the effectiveness of SOTA video SSL techniques when used in a large-scale FL setting, as simulated using the kinetics-400 dataset. The limitations of these techniques in this context are identified before introducing a new federated SSL framework for a video called FedVSSL. FedVSSL incorporates various aggregation strategies and partial weight updates and has been shown through experiments to outperform centralized SOTA by 6.66% on UCF-101 and 5.13% on HMDB-51 in downstream retrieval tasks.</p>
</div>
<figure id="S2.F9" class="ltx_figure"><img src="/html/2308.13558/assets/x9.png" id="S2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The structure of our semi-supervised FL system involves a server and three clients, which adhere to the standard FL process to update a global autoencoder <math id="S2.F9.8.m1.1" class="ltx_Math" alttext="w_{t}^{a_{g}}" display="inline"><semantics id="S2.F9.8.m1.1b"><msubsup id="S2.F9.8.m1.1.1" xref="S2.F9.8.m1.1.1.cmml"><mi id="S2.F9.8.m1.1.1.2.2" xref="S2.F9.8.m1.1.1.2.2.cmml">w</mi><mi id="S2.F9.8.m1.1.1.2.3" xref="S2.F9.8.m1.1.1.2.3.cmml">t</mi><msub id="S2.F9.8.m1.1.1.3" xref="S2.F9.8.m1.1.1.3.cmml"><mi id="S2.F9.8.m1.1.1.3.2" xref="S2.F9.8.m1.1.1.3.2.cmml">a</mi><mi id="S2.F9.8.m1.1.1.3.3" xref="S2.F9.8.m1.1.1.3.3.cmml">g</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F9.8.m1.1c"><apply id="S2.F9.8.m1.1.1.cmml" xref="S2.F9.8.m1.1.1"><csymbol cd="ambiguous" id="S2.F9.8.m1.1.1.1.cmml" xref="S2.F9.8.m1.1.1">superscript</csymbol><apply id="S2.F9.8.m1.1.1.2.cmml" xref="S2.F9.8.m1.1.1"><csymbol cd="ambiguous" id="S2.F9.8.m1.1.1.2.1.cmml" xref="S2.F9.8.m1.1.1">subscript</csymbol><ci id="S2.F9.8.m1.1.1.2.2.cmml" xref="S2.F9.8.m1.1.1.2.2">ùë§</ci><ci id="S2.F9.8.m1.1.1.2.3.cmml" xref="S2.F9.8.m1.1.1.2.3">ùë°</ci></apply><apply id="S2.F9.8.m1.1.1.3.cmml" xref="S2.F9.8.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F9.8.m1.1.1.3.1.cmml" xref="S2.F9.8.m1.1.1.3">subscript</csymbol><ci id="S2.F9.8.m1.1.1.3.2.cmml" xref="S2.F9.8.m1.1.1.3.2">ùëé</ci><ci id="S2.F9.8.m1.1.1.3.3.cmml" xref="S2.F9.8.m1.1.1.3.3">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.8.m1.1d">w_{t}^{a_{g}}</annotation></semantics></math>. Next, the server employs the encoder from <math id="S2.F9.9.m2.1" class="ltx_Math" alttext="w_{t+1}^{a_{g}}" display="inline"><semantics id="S2.F9.9.m2.1b"><msubsup id="S2.F9.9.m2.1.1" xref="S2.F9.9.m2.1.1.cmml"><mi id="S2.F9.9.m2.1.1.2.2" xref="S2.F9.9.m2.1.1.2.2.cmml">w</mi><mrow id="S2.F9.9.m2.1.1.2.3" xref="S2.F9.9.m2.1.1.2.3.cmml"><mi id="S2.F9.9.m2.1.1.2.3.2" xref="S2.F9.9.m2.1.1.2.3.2.cmml">t</mi><mo id="S2.F9.9.m2.1.1.2.3.1" xref="S2.F9.9.m2.1.1.2.3.1.cmml">+</mo><mn id="S2.F9.9.m2.1.1.2.3.3" xref="S2.F9.9.m2.1.1.2.3.3.cmml">1</mn></mrow><msub id="S2.F9.9.m2.1.1.3" xref="S2.F9.9.m2.1.1.3.cmml"><mi id="S2.F9.9.m2.1.1.3.2" xref="S2.F9.9.m2.1.1.3.2.cmml">a</mi><mi id="S2.F9.9.m2.1.1.3.3" xref="S2.F9.9.m2.1.1.3.3.cmml">g</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F9.9.m2.1c"><apply id="S2.F9.9.m2.1.1.cmml" xref="S2.F9.9.m2.1.1"><csymbol cd="ambiguous" id="S2.F9.9.m2.1.1.1.cmml" xref="S2.F9.9.m2.1.1">superscript</csymbol><apply id="S2.F9.9.m2.1.1.2.cmml" xref="S2.F9.9.m2.1.1"><csymbol cd="ambiguous" id="S2.F9.9.m2.1.1.2.1.cmml" xref="S2.F9.9.m2.1.1">subscript</csymbol><ci id="S2.F9.9.m2.1.1.2.2.cmml" xref="S2.F9.9.m2.1.1.2.2">ùë§</ci><apply id="S2.F9.9.m2.1.1.2.3.cmml" xref="S2.F9.9.m2.1.1.2.3"><plus id="S2.F9.9.m2.1.1.2.3.1.cmml" xref="S2.F9.9.m2.1.1.2.3.1"></plus><ci id="S2.F9.9.m2.1.1.2.3.2.cmml" xref="S2.F9.9.m2.1.1.2.3.2">ùë°</ci><cn type="integer" id="S2.F9.9.m2.1.1.2.3.3.cmml" xref="S2.F9.9.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S2.F9.9.m2.1.1.3.cmml" xref="S2.F9.9.m2.1.1.3"><csymbol cd="ambiguous" id="S2.F9.9.m2.1.1.3.1.cmml" xref="S2.F9.9.m2.1.1.3">subscript</csymbol><ci id="S2.F9.9.m2.1.1.3.2.cmml" xref="S2.F9.9.m2.1.1.3.2">ùëé</ci><ci id="S2.F9.9.m2.1.1.3.3.cmml" xref="S2.F9.9.m2.1.1.3.3">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.9.m2.1d">w_{t+1}^{a_{g}}</annotation></semantics></math> to encode the labeled dataset <math id="S2.F9.10.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.F9.10.m3.1b"><mi id="S2.F9.10.m3.1.1" xref="S2.F9.10.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.F9.10.m3.1c"><ci id="S2.F9.10.m3.1.1.cmml" xref="S2.F9.10.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.10.m3.1d">D</annotation></semantics></math> on the server, resulting in a labeled representation dataset <math id="S2.F9.11.m4.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S2.F9.11.m4.1b"><msup id="S2.F9.11.m4.1.1" xref="S2.F9.11.m4.1.1.cmml"><mi id="S2.F9.11.m4.1.1.2" xref="S2.F9.11.m4.1.1.2.cmml">D</mi><mo id="S2.F9.11.m4.1.1.3" xref="S2.F9.11.m4.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F9.11.m4.1c"><apply id="S2.F9.11.m4.1.1.cmml" xref="S2.F9.11.m4.1.1"><csymbol cd="ambiguous" id="S2.F9.11.m4.1.1.1.cmml" xref="S2.F9.11.m4.1.1">superscript</csymbol><ci id="S2.F9.11.m4.1.1.2.cmml" xref="S2.F9.11.m4.1.1.2">ùê∑</ci><ci id="S2.F9.11.m4.1.1.3.cmml" xref="S2.F9.11.m4.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.11.m4.1d">D^{\prime}</annotation></semantics></math>. Afterward, the server utilizes supervised learning with <math id="S2.F9.12.m5.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S2.F9.12.m5.1b"><msup id="S2.F9.12.m5.1.1" xref="S2.F9.12.m5.1.1.cmml"><mi id="S2.F9.12.m5.1.1.2" xref="S2.F9.12.m5.1.1.2.cmml">D</mi><mo id="S2.F9.12.m5.1.1.3" xref="S2.F9.12.m5.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F9.12.m5.1c"><apply id="S2.F9.12.m5.1.1.cmml" xref="S2.F9.12.m5.1.1"><csymbol cd="ambiguous" id="S2.F9.12.m5.1.1.1.cmml" xref="S2.F9.12.m5.1.1">superscript</csymbol><ci id="S2.F9.12.m5.1.1.2.cmml" xref="S2.F9.12.m5.1.1.2">ùê∑</ci><ci id="S2.F9.12.m5.1.1.3.cmml" xref="S2.F9.12.m5.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.12.m5.1d">D^{\prime}</annotation></semantics></math> to train a classifier <math id="S2.F9.13.m6.1" class="ltx_Math" alttext="w_{t}^{s}" display="inline"><semantics id="S2.F9.13.m6.1b"><msubsup id="S2.F9.13.m6.1.1" xref="S2.F9.13.m6.1.1.cmml"><mi id="S2.F9.13.m6.1.1.2.2" xref="S2.F9.13.m6.1.1.2.2.cmml">w</mi><mi id="S2.F9.13.m6.1.1.2.3" xref="S2.F9.13.m6.1.1.2.3.cmml">t</mi><mi id="S2.F9.13.m6.1.1.3" xref="S2.F9.13.m6.1.1.3.cmml">s</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F9.13.m6.1c"><apply id="S2.F9.13.m6.1.1.cmml" xref="S2.F9.13.m6.1.1"><csymbol cd="ambiguous" id="S2.F9.13.m6.1.1.1.cmml" xref="S2.F9.13.m6.1.1">superscript</csymbol><apply id="S2.F9.13.m6.1.1.2.cmml" xref="S2.F9.13.m6.1.1"><csymbol cd="ambiguous" id="S2.F9.13.m6.1.1.2.1.cmml" xref="S2.F9.13.m6.1.1">subscript</csymbol><ci id="S2.F9.13.m6.1.1.2.2.cmml" xref="S2.F9.13.m6.1.1.2.2">ùë§</ci><ci id="S2.F9.13.m6.1.1.2.3.cmml" xref="S2.F9.13.m6.1.1.2.3">ùë°</ci></apply><ci id="S2.F9.13.m6.1.1.3.cmml" xref="S2.F9.13.m6.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.13.m6.1d">w_{t}^{s}</annotation></semantics></math>, producing a new classifier <math id="S2.F9.14.m7.1" class="ltx_Math" alttext="w_{t+1}^{s}" display="inline"><semantics id="S2.F9.14.m7.1b"><msubsup id="S2.F9.14.m7.1.1" xref="S2.F9.14.m7.1.1.cmml"><mi id="S2.F9.14.m7.1.1.2.2" xref="S2.F9.14.m7.1.1.2.2.cmml">w</mi><mrow id="S2.F9.14.m7.1.1.2.3" xref="S2.F9.14.m7.1.1.2.3.cmml"><mi id="S2.F9.14.m7.1.1.2.3.2" xref="S2.F9.14.m7.1.1.2.3.2.cmml">t</mi><mo id="S2.F9.14.m7.1.1.2.3.1" xref="S2.F9.14.m7.1.1.2.3.1.cmml">+</mo><mn id="S2.F9.14.m7.1.1.2.3.3" xref="S2.F9.14.m7.1.1.2.3.3.cmml">1</mn></mrow><mi id="S2.F9.14.m7.1.1.3" xref="S2.F9.14.m7.1.1.3.cmml">s</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F9.14.m7.1c"><apply id="S2.F9.14.m7.1.1.cmml" xref="S2.F9.14.m7.1.1"><csymbol cd="ambiguous" id="S2.F9.14.m7.1.1.1.cmml" xref="S2.F9.14.m7.1.1">superscript</csymbol><apply id="S2.F9.14.m7.1.1.2.cmml" xref="S2.F9.14.m7.1.1"><csymbol cd="ambiguous" id="S2.F9.14.m7.1.1.2.1.cmml" xref="S2.F9.14.m7.1.1">subscript</csymbol><ci id="S2.F9.14.m7.1.1.2.2.cmml" xref="S2.F9.14.m7.1.1.2.2">ùë§</ci><apply id="S2.F9.14.m7.1.1.2.3.cmml" xref="S2.F9.14.m7.1.1.2.3"><plus id="S2.F9.14.m7.1.1.2.3.1.cmml" xref="S2.F9.14.m7.1.1.2.3.1"></plus><ci id="S2.F9.14.m7.1.1.2.3.2.cmml" xref="S2.F9.14.m7.1.1.2.3.2">ùë°</ci><cn type="integer" id="S2.F9.14.m7.1.1.2.3.3.cmml" xref="S2.F9.14.m7.1.1.2.3.3">1</cn></apply></apply><ci id="S2.F9.14.m7.1.1.3.cmml" xref="S2.F9.14.m7.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F9.14.m7.1d">w_{t+1}^{s}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS5.SSS3.p3" class="ltx_para">
<p id="S2.SS5.SSS3.p3.1" class="ltx_p">Moving forward, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> presents a self-supervised privacy preservation framework for action recognition, namely SPAct. It includes three main components: (i) an anonymization function, (ii) a self-supervised privacy removal module, and (iii) an action recognition module. A minimax optimization strategy is used to train this framework, which minimizes the action recognition cost function and maximizes the privacy cost function through a contrastive self-supervised loss. By using existing protocols for known action and privacy attributes, this framework achieves a good balance between action recognition and privacy protection, similar to the current SOTA supervised methods. Additionally, a new protocol to test the generalization of the learned anonymization function to novel action and privacy attributes is introduced.</p>
</div>
<div id="S2.SS5.SSS3.p4" class="ltx_para">
<p id="S2.SS5.SSS3.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, FedUTN is proposed which is an FL allowing each client to train a model that works well on both independently and identically distributed (IID) and non-independent and identically distributed (non-IID) data. In this framework, each party has two networks, a target network, and an online network. FedUTN uses the online network parameters from each terminal to update the target network, which is different from the method used in previous studies. FedUTN also introduces a new control algorithm for training. After testing, it was found that FedUTN‚Äôs method of aggregation is simpler, more effective, and more robust than other FL algorithms, and outperforms the SOTA algorithm by 0.5%-1.6% under normal conditions.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, a few-shot FL framework utilizing a small amount of labeled private facial expression data to train local models on individual devices is proposed. They are then aggregated in a central server to create a globally optimal model. FL is also used to update the feature extractor network on unlabeled private facial data from user devices to learn robust face representations.</p>
</div>
</section>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS6.5.1.1" class="ltx_text">II-F</span> </span><span id="S2.SS6.6.2" class="ltx_text ltx_font_italic">Evaluation of FL-based CV scheme</span>
</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">To assess the efficiency of FL systems in CV, it is essential to employ metrics that are responsive in gauging the performance of implementing FL across various CV applications. Besides the famous metrics that are commonly employed in ML and DL, such as accuracy, F1-score, and area under the curve (AUC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, other metrics are summarized in Table <a href="#S2.T2" title="TABLE II ‚Ä£ II-F Evaluation of FL-based CV scheme ‚Ä£ II Background of FL ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span> The evaluation metrics used in the suggested FL-based CV schemes.</figcaption>
<table id="S2.T2.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.16.17.1" class="ltx_tr">
<th id="S2.T2.16.17.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">Metric</th>
<th id="S2.T2.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">Math description</th>
<th id="S2.T2.16.17.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:284.5pt;padding-top:3pt;padding-bottom:3pt;">
<span id="S2.T2.16.17.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.16.17.1.3.1.1" class="ltx_p">Characteristic</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.6.6" class="ltx_tr">
<td id="S2.T2.6.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">AP</td>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;"><math id="S2.T2.1.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\frac{1}{N}\sum_{k=1}^{N}P(k)\cdot\text{{rel}}(k)" display="inline"><semantics id="S2.T2.1.1.1.m1.2a"><mrow id="S2.T2.1.1.1.m1.2.3" xref="S2.T2.1.1.1.m1.2.3.cmml"><mstyle displaystyle="true" id="S2.T2.1.1.1.m1.2.3.2" xref="S2.T2.1.1.1.m1.2.3.2.cmml"><mfrac id="S2.T2.1.1.1.m1.2.3.2a" xref="S2.T2.1.1.1.m1.2.3.2.cmml"><mn id="S2.T2.1.1.1.m1.2.3.2.2" xref="S2.T2.1.1.1.m1.2.3.2.2.cmml">1</mn><mi id="S2.T2.1.1.1.m1.2.3.2.3" xref="S2.T2.1.1.1.m1.2.3.2.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.2.3.1" xref="S2.T2.1.1.1.m1.2.3.1.cmml">‚Äã</mo><mrow id="S2.T2.1.1.1.m1.2.3.3" xref="S2.T2.1.1.1.m1.2.3.3.cmml"><mstyle displaystyle="true" id="S2.T2.1.1.1.m1.2.3.3.1" xref="S2.T2.1.1.1.m1.2.3.3.1.cmml"><munderover id="S2.T2.1.1.1.m1.2.3.3.1a" xref="S2.T2.1.1.1.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S2.T2.1.1.1.m1.2.3.3.1.2.2" xref="S2.T2.1.1.1.m1.2.3.3.1.2.2.cmml">‚àë</mo><mrow id="S2.T2.1.1.1.m1.2.3.3.1.2.3" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.cmml"><mi id="S2.T2.1.1.1.m1.2.3.3.1.2.3.2" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.2.cmml">k</mi><mo id="S2.T2.1.1.1.m1.2.3.3.1.2.3.1" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S2.T2.1.1.1.m1.2.3.3.1.2.3.3" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.T2.1.1.1.m1.2.3.3.1.3" xref="S2.T2.1.1.1.m1.2.3.3.1.3.cmml">N</mi></munderover></mstyle><mrow id="S2.T2.1.1.1.m1.2.3.3.2" xref="S2.T2.1.1.1.m1.2.3.3.2.cmml"><mrow id="S2.T2.1.1.1.m1.2.3.3.2.2" xref="S2.T2.1.1.1.m1.2.3.3.2.2.cmml"><mrow id="S2.T2.1.1.1.m1.2.3.3.2.2.2" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.cmml"><mi id="S2.T2.1.1.1.m1.2.3.3.2.2.2.2" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.2.3.3.2.2.2.1" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.1.cmml">‚Äã</mo><mrow id="S2.T2.1.1.1.m1.2.3.3.2.2.2.3.2" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.cmml"><mo stretchy="false" id="S2.T2.1.1.1.m1.2.3.3.2.2.2.3.2.1" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.cmml">(</mo><mi id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">k</mi><mo rspace="0.055em" stretchy="false" id="S2.T2.1.1.1.m1.2.3.3.2.2.2.3.2.2" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.T2.1.1.1.m1.2.3.3.2.2.1" xref="S2.T2.1.1.1.m1.2.3.3.2.2.1.cmml">‚ãÖ</mo><mtext id="S2.T2.1.1.1.m1.2.3.3.2.2.3" xref="S2.T2.1.1.1.m1.2.3.3.2.2.3a.cmml">rel</mtext></mrow><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.2.3.3.2.1" xref="S2.T2.1.1.1.m1.2.3.3.2.1.cmml">‚Äã</mo><mrow id="S2.T2.1.1.1.m1.2.3.3.2.3.2" xref="S2.T2.1.1.1.m1.2.3.3.2.cmml"><mo stretchy="false" id="S2.T2.1.1.1.m1.2.3.3.2.3.2.1" xref="S2.T2.1.1.1.m1.2.3.3.2.cmml">(</mo><mi id="S2.T2.1.1.1.m1.2.2" xref="S2.T2.1.1.1.m1.2.2.cmml">k</mi><mo stretchy="false" id="S2.T2.1.1.1.m1.2.3.3.2.3.2.2" xref="S2.T2.1.1.1.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.2b"><apply id="S2.T2.1.1.1.m1.2.3.cmml" xref="S2.T2.1.1.1.m1.2.3"><times id="S2.T2.1.1.1.m1.2.3.1.cmml" xref="S2.T2.1.1.1.m1.2.3.1"></times><apply id="S2.T2.1.1.1.m1.2.3.2.cmml" xref="S2.T2.1.1.1.m1.2.3.2"><divide id="S2.T2.1.1.1.m1.2.3.2.1.cmml" xref="S2.T2.1.1.1.m1.2.3.2"></divide><cn type="integer" id="S2.T2.1.1.1.m1.2.3.2.2.cmml" xref="S2.T2.1.1.1.m1.2.3.2.2">1</cn><ci id="S2.T2.1.1.1.m1.2.3.2.3.cmml" xref="S2.T2.1.1.1.m1.2.3.2.3">ùëÅ</ci></apply><apply id="S2.T2.1.1.1.m1.2.3.3.cmml" xref="S2.T2.1.1.1.m1.2.3.3"><apply id="S2.T2.1.1.1.m1.2.3.3.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.T2.1.1.1.m1.2.3.3.1.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1">superscript</csymbol><apply id="S2.T2.1.1.1.m1.2.3.3.1.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.T2.1.1.1.m1.2.3.3.1.2.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1">subscript</csymbol><sum id="S2.T2.1.1.1.m1.2.3.3.1.2.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.2.2"></sum><apply id="S2.T2.1.1.1.m1.2.3.3.1.2.3.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3"><eq id="S2.T2.1.1.1.m1.2.3.3.1.2.3.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.1"></eq><ci id="S2.T2.1.1.1.m1.2.3.3.1.2.3.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.2">ùëò</ci><cn type="integer" id="S2.T2.1.1.1.m1.2.3.3.1.2.3.3.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.T2.1.1.1.m1.2.3.3.1.3.cmml" xref="S2.T2.1.1.1.m1.2.3.3.1.3">ùëÅ</ci></apply><apply id="S2.T2.1.1.1.m1.2.3.3.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2"><times id="S2.T2.1.1.1.m1.2.3.3.2.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.1"></times><apply id="S2.T2.1.1.1.m1.2.3.3.2.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2"><ci id="S2.T2.1.1.1.m1.2.3.3.2.2.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.1">‚ãÖ</ci><apply id="S2.T2.1.1.1.m1.2.3.3.2.2.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2"><times id="S2.T2.1.1.1.m1.2.3.3.2.2.2.1.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.1"></times><ci id="S2.T2.1.1.1.m1.2.3.3.2.2.2.2.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.2.2">ùëÉ</ci><ci id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">ùëò</ci></apply><ci id="S2.T2.1.1.1.m1.2.3.3.2.2.3a.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.3"><mtext id="S2.T2.1.1.1.m1.2.3.3.2.2.3.cmml" xref="S2.T2.1.1.1.m1.2.3.3.2.2.3">rel</mtext></ci></apply><ci id="S2.T2.1.1.1.m1.2.2.cmml" xref="S2.T2.1.1.1.m1.2.2">ùëò</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.2c">\displaystyle\frac{1}{N}\sum_{k=1}^{N}P(k)\cdot\text{{rel}}(k)</annotation></semantics></math></td>
<td id="S2.T2.6.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:284.5pt;padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">
<span id="S2.T2.6.6.6.5" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.6.6.6.5.5" class="ltx_p">The average precision (AP) is commonly used in information retrieval and object detection tasks. It measures the precision of a model across different levels of recall. <math id="S2.T2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.T2.2.2.2.1.1.m1.1a"><mi id="S2.T2.2.2.2.1.1.m1.1.1" xref="S2.T2.2.2.2.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.1.1.m1.1b"><ci id="S2.T2.2.2.2.1.1.m1.1.1.cmml" xref="S2.T2.2.2.2.1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.1.1.m1.1c">N</annotation></semantics></math> represents the total number of retrieved items, <math id="S2.T2.3.3.3.2.2.m2.1" class="ltx_Math" alttext="P(k)" display="inline"><semantics id="S2.T2.3.3.3.2.2.m2.1a"><mrow id="S2.T2.3.3.3.2.2.m2.1.2" xref="S2.T2.3.3.3.2.2.m2.1.2.cmml"><mi id="S2.T2.3.3.3.2.2.m2.1.2.2" xref="S2.T2.3.3.3.2.2.m2.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.T2.3.3.3.2.2.m2.1.2.1" xref="S2.T2.3.3.3.2.2.m2.1.2.1.cmml">‚Äã</mo><mrow id="S2.T2.3.3.3.2.2.m2.1.2.3.2" xref="S2.T2.3.3.3.2.2.m2.1.2.cmml"><mo stretchy="false" id="S2.T2.3.3.3.2.2.m2.1.2.3.2.1" xref="S2.T2.3.3.3.2.2.m2.1.2.cmml">(</mo><mi id="S2.T2.3.3.3.2.2.m2.1.1" xref="S2.T2.3.3.3.2.2.m2.1.1.cmml">k</mi><mo stretchy="false" id="S2.T2.3.3.3.2.2.m2.1.2.3.2.2" xref="S2.T2.3.3.3.2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.2.2.m2.1b"><apply id="S2.T2.3.3.3.2.2.m2.1.2.cmml" xref="S2.T2.3.3.3.2.2.m2.1.2"><times id="S2.T2.3.3.3.2.2.m2.1.2.1.cmml" xref="S2.T2.3.3.3.2.2.m2.1.2.1"></times><ci id="S2.T2.3.3.3.2.2.m2.1.2.2.cmml" xref="S2.T2.3.3.3.2.2.m2.1.2.2">ùëÉ</ci><ci id="S2.T2.3.3.3.2.2.m2.1.1.cmml" xref="S2.T2.3.3.3.2.2.m2.1.1">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.2.2.m2.1c">P(k)</annotation></semantics></math> denotes the precision at the <math id="S2.T2.4.4.4.3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.T2.4.4.4.3.3.m3.1a"><mi id="S2.T2.4.4.4.3.3.m3.1.1" xref="S2.T2.4.4.4.3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.4.3.3.m3.1b"><ci id="S2.T2.4.4.4.3.3.m3.1.1.cmml" xref="S2.T2.4.4.4.3.3.m3.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.4.3.3.m3.1c">k</annotation></semantics></math>-th retrieved item, and <math id="S2.T2.5.5.5.4.4.m4.1" class="ltx_Math" alttext="\text{{rel}}(k)" display="inline"><semantics id="S2.T2.5.5.5.4.4.m4.1a"><mrow id="S2.T2.5.5.5.4.4.m4.1.2" xref="S2.T2.5.5.5.4.4.m4.1.2.cmml"><mtext id="S2.T2.5.5.5.4.4.m4.1.2.2" xref="S2.T2.5.5.5.4.4.m4.1.2.2a.cmml">rel</mtext><mo lspace="0em" rspace="0em" id="S2.T2.5.5.5.4.4.m4.1.2.1" xref="S2.T2.5.5.5.4.4.m4.1.2.1.cmml">‚Äã</mo><mrow id="S2.T2.5.5.5.4.4.m4.1.2.3.2" xref="S2.T2.5.5.5.4.4.m4.1.2.cmml"><mo stretchy="false" id="S2.T2.5.5.5.4.4.m4.1.2.3.2.1" xref="S2.T2.5.5.5.4.4.m4.1.2.cmml">(</mo><mi id="S2.T2.5.5.5.4.4.m4.1.1" xref="S2.T2.5.5.5.4.4.m4.1.1.cmml">k</mi><mo stretchy="false" id="S2.T2.5.5.5.4.4.m4.1.2.3.2.2" xref="S2.T2.5.5.5.4.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.5.4.4.m4.1b"><apply id="S2.T2.5.5.5.4.4.m4.1.2.cmml" xref="S2.T2.5.5.5.4.4.m4.1.2"><times id="S2.T2.5.5.5.4.4.m4.1.2.1.cmml" xref="S2.T2.5.5.5.4.4.m4.1.2.1"></times><ci id="S2.T2.5.5.5.4.4.m4.1.2.2a.cmml" xref="S2.T2.5.5.5.4.4.m4.1.2.2"><mtext id="S2.T2.5.5.5.4.4.m4.1.2.2.cmml" xref="S2.T2.5.5.5.4.4.m4.1.2.2">rel</mtext></ci><ci id="S2.T2.5.5.5.4.4.m4.1.1.cmml" xref="S2.T2.5.5.5.4.4.m4.1.1">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.5.4.4.m4.1c">\text{{rel}}(k)</annotation></semantics></math> indicates whether the <math id="S2.T2.6.6.6.5.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.T2.6.6.6.5.5.m5.1a"><mi id="S2.T2.6.6.6.5.5.m5.1.1" xref="S2.T2.6.6.6.5.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.6.5.5.m5.1b"><ci id="S2.T2.6.6.6.5.5.m5.1.1.cmml" xref="S2.T2.6.6.6.5.5.m5.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.6.5.5.m5.1c">k</annotation></semantics></math>-th retrieved item is relevant (1 if relevant, 0 if not relevant).</span>
</span>
</td>
</tr>
<tr id="S2.T2.10.10" class="ltx_tr">
<td id="S2.T2.10.10.5" class="ltx_td ltx_align_left" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">mAP</td>
<td id="S2.T2.7.7.1" class="ltx_td ltx_align_center" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;"><math id="S2.T2.7.7.1.m1.1" class="ltx_Math" alttext="\displaystyle\frac{1}{N_{\text{{class}}}}\sum_{c=1}^{N_{\text{{class}}}}\text{{AP}}_{c}" display="inline"><semantics id="S2.T2.7.7.1.m1.1a"><mrow id="S2.T2.7.7.1.m1.1.1" xref="S2.T2.7.7.1.m1.1.1.cmml"><mstyle displaystyle="true" id="S2.T2.7.7.1.m1.1.1.2" xref="S2.T2.7.7.1.m1.1.1.2.cmml"><mfrac id="S2.T2.7.7.1.m1.1.1.2a" xref="S2.T2.7.7.1.m1.1.1.2.cmml"><mn id="S2.T2.7.7.1.m1.1.1.2.2" xref="S2.T2.7.7.1.m1.1.1.2.2.cmml">1</mn><msub id="S2.T2.7.7.1.m1.1.1.2.3" xref="S2.T2.7.7.1.m1.1.1.2.3.cmml"><mi id="S2.T2.7.7.1.m1.1.1.2.3.2" xref="S2.T2.7.7.1.m1.1.1.2.3.2.cmml">N</mi><mtext id="S2.T2.7.7.1.m1.1.1.2.3.3" xref="S2.T2.7.7.1.m1.1.1.2.3.3a.cmml">class</mtext></msub></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.T2.7.7.1.m1.1.1.1" xref="S2.T2.7.7.1.m1.1.1.1.cmml">‚Äã</mo><mrow id="S2.T2.7.7.1.m1.1.1.3" xref="S2.T2.7.7.1.m1.1.1.3.cmml"><mstyle displaystyle="true" id="S2.T2.7.7.1.m1.1.1.3.1" xref="S2.T2.7.7.1.m1.1.1.3.1.cmml"><munderover id="S2.T2.7.7.1.m1.1.1.3.1a" xref="S2.T2.7.7.1.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S2.T2.7.7.1.m1.1.1.3.1.2.2" xref="S2.T2.7.7.1.m1.1.1.3.1.2.2.cmml">‚àë</mo><mrow id="S2.T2.7.7.1.m1.1.1.3.1.2.3" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.cmml"><mi id="S2.T2.7.7.1.m1.1.1.3.1.2.3.2" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.2.cmml">c</mi><mo id="S2.T2.7.7.1.m1.1.1.3.1.2.3.1" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.T2.7.7.1.m1.1.1.3.1.2.3.3" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><msub id="S2.T2.7.7.1.m1.1.1.3.1.3" xref="S2.T2.7.7.1.m1.1.1.3.1.3.cmml"><mi id="S2.T2.7.7.1.m1.1.1.3.1.3.2" xref="S2.T2.7.7.1.m1.1.1.3.1.3.2.cmml">N</mi><mtext id="S2.T2.7.7.1.m1.1.1.3.1.3.3" xref="S2.T2.7.7.1.m1.1.1.3.1.3.3a.cmml">class</mtext></msub></munderover></mstyle><msub id="S2.T2.7.7.1.m1.1.1.3.2" xref="S2.T2.7.7.1.m1.1.1.3.2.cmml"><mtext id="S2.T2.7.7.1.m1.1.1.3.2.2" xref="S2.T2.7.7.1.m1.1.1.3.2.2a.cmml">AP</mtext><mi id="S2.T2.7.7.1.m1.1.1.3.2.3" xref="S2.T2.7.7.1.m1.1.1.3.2.3.cmml">c</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.1.m1.1b"><apply id="S2.T2.7.7.1.m1.1.1.cmml" xref="S2.T2.7.7.1.m1.1.1"><times id="S2.T2.7.7.1.m1.1.1.1.cmml" xref="S2.T2.7.7.1.m1.1.1.1"></times><apply id="S2.T2.7.7.1.m1.1.1.2.cmml" xref="S2.T2.7.7.1.m1.1.1.2"><divide id="S2.T2.7.7.1.m1.1.1.2.1.cmml" xref="S2.T2.7.7.1.m1.1.1.2"></divide><cn type="integer" id="S2.T2.7.7.1.m1.1.1.2.2.cmml" xref="S2.T2.7.7.1.m1.1.1.2.2">1</cn><apply id="S2.T2.7.7.1.m1.1.1.2.3.cmml" xref="S2.T2.7.7.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S2.T2.7.7.1.m1.1.1.2.3.1.cmml" xref="S2.T2.7.7.1.m1.1.1.2.3">subscript</csymbol><ci id="S2.T2.7.7.1.m1.1.1.2.3.2.cmml" xref="S2.T2.7.7.1.m1.1.1.2.3.2">ùëÅ</ci><ci id="S2.T2.7.7.1.m1.1.1.2.3.3a.cmml" xref="S2.T2.7.7.1.m1.1.1.2.3.3"><mtext mathsize="70%" id="S2.T2.7.7.1.m1.1.1.2.3.3.cmml" xref="S2.T2.7.7.1.m1.1.1.2.3.3">class</mtext></ci></apply></apply><apply id="S2.T2.7.7.1.m1.1.1.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3"><apply id="S2.T2.7.7.1.m1.1.1.3.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.T2.7.7.1.m1.1.1.3.1.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1">superscript</csymbol><apply id="S2.T2.7.7.1.m1.1.1.3.1.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.T2.7.7.1.m1.1.1.3.1.2.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1">subscript</csymbol><sum id="S2.T2.7.7.1.m1.1.1.3.1.2.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.2.2"></sum><apply id="S2.T2.7.7.1.m1.1.1.3.1.2.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3"><eq id="S2.T2.7.7.1.m1.1.1.3.1.2.3.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.1"></eq><ci id="S2.T2.7.7.1.m1.1.1.3.1.2.3.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.2">ùëê</ci><cn type="integer" id="S2.T2.7.7.1.m1.1.1.3.1.2.3.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><apply id="S2.T2.7.7.1.m1.1.1.3.1.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.3"><csymbol cd="ambiguous" id="S2.T2.7.7.1.m1.1.1.3.1.3.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.3">subscript</csymbol><ci id="S2.T2.7.7.1.m1.1.1.3.1.3.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.3.2">ùëÅ</ci><ci id="S2.T2.7.7.1.m1.1.1.3.1.3.3a.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.3.3"><mtext mathsize="50%" id="S2.T2.7.7.1.m1.1.1.3.1.3.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3.1.3.3">class</mtext></ci></apply></apply><apply id="S2.T2.7.7.1.m1.1.1.3.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.T2.7.7.1.m1.1.1.3.2.1.cmml" xref="S2.T2.7.7.1.m1.1.1.3.2">subscript</csymbol><ci id="S2.T2.7.7.1.m1.1.1.3.2.2a.cmml" xref="S2.T2.7.7.1.m1.1.1.3.2.2"><mtext id="S2.T2.7.7.1.m1.1.1.3.2.2.cmml" xref="S2.T2.7.7.1.m1.1.1.3.2.2">AP</mtext></ci><ci id="S2.T2.7.7.1.m1.1.1.3.2.3.cmml" xref="S2.T2.7.7.1.m1.1.1.3.2.3">ùëê</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.1.m1.1c">\displaystyle\frac{1}{N_{\text{{class}}}}\sum_{c=1}^{N_{\text{{class}}}}\text{{AP}}_{c}</annotation></semantics></math></td>
<td id="S2.T2.10.10.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:284.5pt;padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">
<span id="S2.T2.10.10.4.3" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.10.10.4.3.3" class="ltx_p">The mean average precision (mAP) is often employed to assess the performance of models that generate multiple predictions or retrieve ranked results. <math id="S2.T2.8.8.2.1.1.m1.1" class="ltx_Math" alttext="N_{\text{{class}}}" display="inline"><semantics id="S2.T2.8.8.2.1.1.m1.1a"><msub id="S2.T2.8.8.2.1.1.m1.1.1" xref="S2.T2.8.8.2.1.1.m1.1.1.cmml"><mi id="S2.T2.8.8.2.1.1.m1.1.1.2" xref="S2.T2.8.8.2.1.1.m1.1.1.2.cmml">N</mi><mtext id="S2.T2.8.8.2.1.1.m1.1.1.3" xref="S2.T2.8.8.2.1.1.m1.1.1.3a.cmml">class</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.2.1.1.m1.1b"><apply id="S2.T2.8.8.2.1.1.m1.1.1.cmml" xref="S2.T2.8.8.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.8.8.2.1.1.m1.1.1.1.cmml" xref="S2.T2.8.8.2.1.1.m1.1.1">subscript</csymbol><ci id="S2.T2.8.8.2.1.1.m1.1.1.2.cmml" xref="S2.T2.8.8.2.1.1.m1.1.1.2">ùëÅ</ci><ci id="S2.T2.8.8.2.1.1.m1.1.1.3a.cmml" xref="S2.T2.8.8.2.1.1.m1.1.1.3"><mtext mathsize="70%" id="S2.T2.8.8.2.1.1.m1.1.1.3.cmml" xref="S2.T2.8.8.2.1.1.m1.1.1.3">class</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.2.1.1.m1.1c">N_{\text{{class}}}</annotation></semantics></math> represents the total number of classes, <math id="S2.T2.9.9.3.2.2.m2.1" class="ltx_Math" alttext="\text{{AP}}_{c}" display="inline"><semantics id="S2.T2.9.9.3.2.2.m2.1a"><msub id="S2.T2.9.9.3.2.2.m2.1.1" xref="S2.T2.9.9.3.2.2.m2.1.1.cmml"><mtext id="S2.T2.9.9.3.2.2.m2.1.1.2" xref="S2.T2.9.9.3.2.2.m2.1.1.2a.cmml">AP</mtext><mi id="S2.T2.9.9.3.2.2.m2.1.1.3" xref="S2.T2.9.9.3.2.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T2.9.9.3.2.2.m2.1b"><apply id="S2.T2.9.9.3.2.2.m2.1.1.cmml" xref="S2.T2.9.9.3.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.T2.9.9.3.2.2.m2.1.1.1.cmml" xref="S2.T2.9.9.3.2.2.m2.1.1">subscript</csymbol><ci id="S2.T2.9.9.3.2.2.m2.1.1.2a.cmml" xref="S2.T2.9.9.3.2.2.m2.1.1.2"><mtext id="S2.T2.9.9.3.2.2.m2.1.1.2.cmml" xref="S2.T2.9.9.3.2.2.m2.1.1.2">AP</mtext></ci><ci id="S2.T2.9.9.3.2.2.m2.1.1.3.cmml" xref="S2.T2.9.9.3.2.2.m2.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.9.3.2.2.m2.1c">\text{{AP}}_{c}</annotation></semantics></math> denotes the AP for class <math id="S2.T2.10.10.4.3.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.T2.10.10.4.3.3.m3.1a"><mi id="S2.T2.10.10.4.3.3.m3.1.1" xref="S2.T2.10.10.4.3.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.T2.10.10.4.3.3.m3.1b"><ci id="S2.T2.10.10.4.3.3.m3.1.1.cmml" xref="S2.T2.10.10.4.3.3.m3.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.10.10.4.3.3.m3.1c">c</annotation></semantics></math>.</span>
</span>
</td>
</tr>
<tr id="S2.T2.15.15" class="ltx_tr">
<td id="S2.T2.15.15.6" class="ltx_td ltx_align_left" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">MAE</td>
<td id="S2.T2.11.11.1" class="ltx_td ltx_align_center" style="padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;"><math id="S2.T2.11.11.1.m1.5" class="ltx_Math" alttext="\displaystyle\frac{1}{H\times W}\sum_{i}^{H}\sum_{j}^{W}(P(i,j)-G(i,j))" display="inline"><semantics id="S2.T2.11.11.1.m1.5a"><mrow id="S2.T2.11.11.1.m1.5.5" xref="S2.T2.11.11.1.m1.5.5.cmml"><mstyle displaystyle="true" id="S2.T2.11.11.1.m1.5.5.3" xref="S2.T2.11.11.1.m1.5.5.3.cmml"><mfrac id="S2.T2.11.11.1.m1.5.5.3a" xref="S2.T2.11.11.1.m1.5.5.3.cmml"><mn id="S2.T2.11.11.1.m1.5.5.3.2" xref="S2.T2.11.11.1.m1.5.5.3.2.cmml">1</mn><mrow id="S2.T2.11.11.1.m1.5.5.3.3" xref="S2.T2.11.11.1.m1.5.5.3.3.cmml"><mi id="S2.T2.11.11.1.m1.5.5.3.3.2" xref="S2.T2.11.11.1.m1.5.5.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.T2.11.11.1.m1.5.5.3.3.1" xref="S2.T2.11.11.1.m1.5.5.3.3.1.cmml">√ó</mo><mi id="S2.T2.11.11.1.m1.5.5.3.3.3" xref="S2.T2.11.11.1.m1.5.5.3.3.3.cmml">W</mi></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.T2.11.11.1.m1.5.5.2" xref="S2.T2.11.11.1.m1.5.5.2.cmml">‚Äã</mo><mrow id="S2.T2.11.11.1.m1.5.5.1" xref="S2.T2.11.11.1.m1.5.5.1.cmml"><mstyle displaystyle="true" id="S2.T2.11.11.1.m1.5.5.1.2" xref="S2.T2.11.11.1.m1.5.5.1.2.cmml"><munderover id="S2.T2.11.11.1.m1.5.5.1.2a" xref="S2.T2.11.11.1.m1.5.5.1.2.cmml"><mo movablelimits="false" id="S2.T2.11.11.1.m1.5.5.1.2.2.2" xref="S2.T2.11.11.1.m1.5.5.1.2.2.2.cmml">‚àë</mo><mi id="S2.T2.11.11.1.m1.5.5.1.2.2.3" xref="S2.T2.11.11.1.m1.5.5.1.2.2.3.cmml">i</mi><mi id="S2.T2.11.11.1.m1.5.5.1.2.3" xref="S2.T2.11.11.1.m1.5.5.1.2.3.cmml">H</mi></munderover></mstyle><mrow id="S2.T2.11.11.1.m1.5.5.1.1" xref="S2.T2.11.11.1.m1.5.5.1.1.cmml"><mstyle displaystyle="true" id="S2.T2.11.11.1.m1.5.5.1.1.2" xref="S2.T2.11.11.1.m1.5.5.1.1.2.cmml"><munderover id="S2.T2.11.11.1.m1.5.5.1.1.2a" xref="S2.T2.11.11.1.m1.5.5.1.1.2.cmml"><mo movablelimits="false" id="S2.T2.11.11.1.m1.5.5.1.1.2.2.2" xref="S2.T2.11.11.1.m1.5.5.1.1.2.2.2.cmml">‚àë</mo><mi id="S2.T2.11.11.1.m1.5.5.1.1.2.2.3" xref="S2.T2.11.11.1.m1.5.5.1.1.2.2.3.cmml">j</mi><mi id="S2.T2.11.11.1.m1.5.5.1.1.2.3" xref="S2.T2.11.11.1.m1.5.5.1.1.2.3.cmml">W</mi></munderover></mstyle><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.cmml"><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.2.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="S2.T2.11.11.1.m1.1.1" xref="S2.T2.11.11.1.m1.1.1.cmml">i</mi><mo id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.2.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.1.cmml">,</mo><mi id="S2.T2.11.11.1.m1.2.2" xref="S2.T2.11.11.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.2.3" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.1.cmml">‚àí</mo><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.cmml"><mi id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.2.1" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.1.cmml">(</mo><mi id="S2.T2.11.11.1.m1.3.3" xref="S2.T2.11.11.1.m1.3.3.cmml">i</mi><mo id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.2.2" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S2.T2.11.11.1.m1.4.4" xref="S2.T2.11.11.1.m1.4.4.cmml">j</mi><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.2.3" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.3" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.11.11.1.m1.5b"><apply id="S2.T2.11.11.1.m1.5.5.cmml" xref="S2.T2.11.11.1.m1.5.5"><times id="S2.T2.11.11.1.m1.5.5.2.cmml" xref="S2.T2.11.11.1.m1.5.5.2"></times><apply id="S2.T2.11.11.1.m1.5.5.3.cmml" xref="S2.T2.11.11.1.m1.5.5.3"><divide id="S2.T2.11.11.1.m1.5.5.3.1.cmml" xref="S2.T2.11.11.1.m1.5.5.3"></divide><cn type="integer" id="S2.T2.11.11.1.m1.5.5.3.2.cmml" xref="S2.T2.11.11.1.m1.5.5.3.2">1</cn><apply id="S2.T2.11.11.1.m1.5.5.3.3.cmml" xref="S2.T2.11.11.1.m1.5.5.3.3"><times id="S2.T2.11.11.1.m1.5.5.3.3.1.cmml" xref="S2.T2.11.11.1.m1.5.5.3.3.1"></times><ci id="S2.T2.11.11.1.m1.5.5.3.3.2.cmml" xref="S2.T2.11.11.1.m1.5.5.3.3.2">ùêª</ci><ci id="S2.T2.11.11.1.m1.5.5.3.3.3.cmml" xref="S2.T2.11.11.1.m1.5.5.3.3.3">ùëä</ci></apply></apply><apply id="S2.T2.11.11.1.m1.5.5.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1"><apply id="S2.T2.11.11.1.m1.5.5.1.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2"><csymbol cd="ambiguous" id="S2.T2.11.11.1.m1.5.5.1.2.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2">superscript</csymbol><apply id="S2.T2.11.11.1.m1.5.5.1.2.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2"><csymbol cd="ambiguous" id="S2.T2.11.11.1.m1.5.5.1.2.2.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2">subscript</csymbol><sum id="S2.T2.11.11.1.m1.5.5.1.2.2.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2.2.2"></sum><ci id="S2.T2.11.11.1.m1.5.5.1.2.2.3.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2.2.3">ùëñ</ci></apply><ci id="S2.T2.11.11.1.m1.5.5.1.2.3.cmml" xref="S2.T2.11.11.1.m1.5.5.1.2.3">ùêª</ci></apply><apply id="S2.T2.11.11.1.m1.5.5.1.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1"><apply id="S2.T2.11.11.1.m1.5.5.1.1.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.T2.11.11.1.m1.5.5.1.1.2.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2">superscript</csymbol><apply id="S2.T2.11.11.1.m1.5.5.1.1.2.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.T2.11.11.1.m1.5.5.1.1.2.2.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2">subscript</csymbol><sum id="S2.T2.11.11.1.m1.5.5.1.1.2.2.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2.2.2"></sum><ci id="S2.T2.11.11.1.m1.5.5.1.1.2.2.3.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2.2.3">ùëó</ci></apply><ci id="S2.T2.11.11.1.m1.5.5.1.1.2.3.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.2.3">ùëä</ci></apply><apply id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1"><minus id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.1"></minus><apply id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2"><times id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.1"></times><ci id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.2">ùëÉ</ci><interval closure="open" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.2.3.2"><ci id="S2.T2.11.11.1.m1.1.1.cmml" xref="S2.T2.11.11.1.m1.1.1">ùëñ</ci><ci id="S2.T2.11.11.1.m1.2.2.cmml" xref="S2.T2.11.11.1.m1.2.2">ùëó</ci></interval></apply><apply id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3"><times id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.1"></times><ci id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.2">ùê∫</ci><interval closure="open" id="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.1.cmml" xref="S2.T2.11.11.1.m1.5.5.1.1.1.1.1.3.3.2"><ci id="S2.T2.11.11.1.m1.3.3.cmml" xref="S2.T2.11.11.1.m1.3.3">ùëñ</ci><ci id="S2.T2.11.11.1.m1.4.4.cmml" xref="S2.T2.11.11.1.m1.4.4">ùëó</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.11.11.1.m1.5c">\displaystyle\frac{1}{H\times W}\sum_{i}^{H}\sum_{j}^{W}(P(i,j)-G(i,j))</annotation></semantics></math></td>
<td id="S2.T2.15.15.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:284.5pt;padding-bottom:14.22636pt;padding-top:3pt;padding-bottom:3pt;">
<span id="S2.T2.15.15.5.4" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.15.15.5.4.4" class="ltx_p">The mean absolute error (MAE) measures the average absolute difference between each pixel in a normalized prediction saliency map <math id="S2.T2.12.12.2.1.1.m1.2" class="ltx_Math" alttext="P(i,j)" display="inline"><semantics id="S2.T2.12.12.2.1.1.m1.2a"><mrow id="S2.T2.12.12.2.1.1.m1.2.3" xref="S2.T2.12.12.2.1.1.m1.2.3.cmml"><mi id="S2.T2.12.12.2.1.1.m1.2.3.2" xref="S2.T2.12.12.2.1.1.m1.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.T2.12.12.2.1.1.m1.2.3.1" xref="S2.T2.12.12.2.1.1.m1.2.3.1.cmml">‚Äã</mo><mrow id="S2.T2.12.12.2.1.1.m1.2.3.3.2" xref="S2.T2.12.12.2.1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.T2.12.12.2.1.1.m1.2.3.3.2.1" xref="S2.T2.12.12.2.1.1.m1.2.3.3.1.cmml">(</mo><mi id="S2.T2.12.12.2.1.1.m1.1.1" xref="S2.T2.12.12.2.1.1.m1.1.1.cmml">i</mi><mo id="S2.T2.12.12.2.1.1.m1.2.3.3.2.2" xref="S2.T2.12.12.2.1.1.m1.2.3.3.1.cmml">,</mo><mi id="S2.T2.12.12.2.1.1.m1.2.2" xref="S2.T2.12.12.2.1.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S2.T2.12.12.2.1.1.m1.2.3.3.2.3" xref="S2.T2.12.12.2.1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.12.12.2.1.1.m1.2b"><apply id="S2.T2.12.12.2.1.1.m1.2.3.cmml" xref="S2.T2.12.12.2.1.1.m1.2.3"><times id="S2.T2.12.12.2.1.1.m1.2.3.1.cmml" xref="S2.T2.12.12.2.1.1.m1.2.3.1"></times><ci id="S2.T2.12.12.2.1.1.m1.2.3.2.cmml" xref="S2.T2.12.12.2.1.1.m1.2.3.2">ùëÉ</ci><interval closure="open" id="S2.T2.12.12.2.1.1.m1.2.3.3.1.cmml" xref="S2.T2.12.12.2.1.1.m1.2.3.3.2"><ci id="S2.T2.12.12.2.1.1.m1.1.1.cmml" xref="S2.T2.12.12.2.1.1.m1.1.1">ùëñ</ci><ci id="S2.T2.12.12.2.1.1.m1.2.2.cmml" xref="S2.T2.12.12.2.1.1.m1.2.2">ùëó</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.12.12.2.1.1.m1.2c">P(i,j)</annotation></semantics></math> and its corresponding pixel in the binary ground truth mask <math id="S2.T2.13.13.3.2.2.m2.2" class="ltx_Math" alttext="G(i,j)" display="inline"><semantics id="S2.T2.13.13.3.2.2.m2.2a"><mrow id="S2.T2.13.13.3.2.2.m2.2.3" xref="S2.T2.13.13.3.2.2.m2.2.3.cmml"><mi id="S2.T2.13.13.3.2.2.m2.2.3.2" xref="S2.T2.13.13.3.2.2.m2.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.T2.13.13.3.2.2.m2.2.3.1" xref="S2.T2.13.13.3.2.2.m2.2.3.1.cmml">‚Äã</mo><mrow id="S2.T2.13.13.3.2.2.m2.2.3.3.2" xref="S2.T2.13.13.3.2.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S2.T2.13.13.3.2.2.m2.2.3.3.2.1" xref="S2.T2.13.13.3.2.2.m2.2.3.3.1.cmml">(</mo><mi id="S2.T2.13.13.3.2.2.m2.1.1" xref="S2.T2.13.13.3.2.2.m2.1.1.cmml">i</mi><mo id="S2.T2.13.13.3.2.2.m2.2.3.3.2.2" xref="S2.T2.13.13.3.2.2.m2.2.3.3.1.cmml">,</mo><mi id="S2.T2.13.13.3.2.2.m2.2.2" xref="S2.T2.13.13.3.2.2.m2.2.2.cmml">j</mi><mo stretchy="false" id="S2.T2.13.13.3.2.2.m2.2.3.3.2.3" xref="S2.T2.13.13.3.2.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.13.13.3.2.2.m2.2b"><apply id="S2.T2.13.13.3.2.2.m2.2.3.cmml" xref="S2.T2.13.13.3.2.2.m2.2.3"><times id="S2.T2.13.13.3.2.2.m2.2.3.1.cmml" xref="S2.T2.13.13.3.2.2.m2.2.3.1"></times><ci id="S2.T2.13.13.3.2.2.m2.2.3.2.cmml" xref="S2.T2.13.13.3.2.2.m2.2.3.2">ùê∫</ci><interval closure="open" id="S2.T2.13.13.3.2.2.m2.2.3.3.1.cmml" xref="S2.T2.13.13.3.2.2.m2.2.3.3.2"><ci id="S2.T2.13.13.3.2.2.m2.1.1.cmml" xref="S2.T2.13.13.3.2.2.m2.1.1">ùëñ</ci><ci id="S2.T2.13.13.3.2.2.m2.2.2.cmml" xref="S2.T2.13.13.3.2.2.m2.2.2">ùëó</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.13.3.2.2.m2.2c">G(i,j)</annotation></semantics></math>. <math id="S2.T2.14.14.4.3.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.T2.14.14.4.3.3.m3.1a"><mi id="S2.T2.14.14.4.3.3.m3.1.1" xref="S2.T2.14.14.4.3.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.T2.14.14.4.3.3.m3.1b"><ci id="S2.T2.14.14.4.3.3.m3.1.1.cmml" xref="S2.T2.14.14.4.3.3.m3.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.14.4.3.3.m3.1c">H</annotation></semantics></math>, and <math id="S2.T2.15.15.5.4.4.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.T2.15.15.5.4.4.m4.1a"><mi id="S2.T2.15.15.5.4.4.m4.1.1" xref="S2.T2.15.15.5.4.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.T2.15.15.5.4.4.m4.1b"><ci id="S2.T2.15.15.5.4.4.m4.1.1.cmml" xref="S2.T2.15.15.5.4.4.m4.1.1">ùëä</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.15.15.5.4.4.m4.1c">W</annotation></semantics></math> represent the image‚Äôs height and width.</span>
</span>
</td>
</tr>
<tr id="S2.T2.16.16" class="ltx_tr">
<td id="S2.T2.16.16.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:3pt;padding-bottom:3pt;">RMSE</td>
<td id="S2.T2.16.16.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3pt;padding-bottom:3pt;"><math id="S2.T2.16.16.1.m1.5" class="ltx_Math" alttext="\displaystyle\sqrt{\frac{1}{{M\times N}}\sum_{i=1}^{M}\sum_{j=1}^{N}(P(i,j)-G(i,j))^{2}}" display="inline"><semantics id="S2.T2.16.16.1.m1.5a"><msqrt id="S2.T2.16.16.1.m1.5.5" xref="S2.T2.16.16.1.m1.5.5.cmml"><mrow id="S2.T2.16.16.1.m1.5.5.5" xref="S2.T2.16.16.1.m1.5.5.5.cmml"><mstyle displaystyle="true" id="S2.T2.16.16.1.m1.5.5.5.7" xref="S2.T2.16.16.1.m1.5.5.5.7.cmml"><mfrac id="S2.T2.16.16.1.m1.5.5.5.7a" xref="S2.T2.16.16.1.m1.5.5.5.7.cmml"><mn id="S2.T2.16.16.1.m1.5.5.5.7.2" xref="S2.T2.16.16.1.m1.5.5.5.7.2.cmml">1</mn><mrow id="S2.T2.16.16.1.m1.5.5.5.7.3" xref="S2.T2.16.16.1.m1.5.5.5.7.3.cmml"><mi id="S2.T2.16.16.1.m1.5.5.5.7.3.2" xref="S2.T2.16.16.1.m1.5.5.5.7.3.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S2.T2.16.16.1.m1.5.5.5.7.3.1" xref="S2.T2.16.16.1.m1.5.5.5.7.3.1.cmml">√ó</mo><mi id="S2.T2.16.16.1.m1.5.5.5.7.3.3" xref="S2.T2.16.16.1.m1.5.5.5.7.3.3.cmml">N</mi></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.T2.16.16.1.m1.5.5.5.6" xref="S2.T2.16.16.1.m1.5.5.5.6.cmml">‚Äã</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5" xref="S2.T2.16.16.1.m1.5.5.5.5.cmml"><mstyle displaystyle="true" id="S2.T2.16.16.1.m1.5.5.5.5.2" xref="S2.T2.16.16.1.m1.5.5.5.5.2.cmml"><munderover id="S2.T2.16.16.1.m1.5.5.5.5.2a" xref="S2.T2.16.16.1.m1.5.5.5.5.2.cmml"><mo movablelimits="false" id="S2.T2.16.16.1.m1.5.5.5.5.2.2.2" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.2.cmml">‚àë</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.cmml"><mi id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.2" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.2.cmml">i</mi><mo id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.1" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.1.cmml">=</mo><mn id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.3" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.3.cmml">1</mn></mrow><mi id="S2.T2.16.16.1.m1.5.5.5.5.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.2.3.cmml">M</mi></munderover></mstyle><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.cmml"><mstyle displaystyle="true" id="S2.T2.16.16.1.m1.5.5.5.5.1.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.cmml"><munderover id="S2.T2.16.16.1.m1.5.5.5.5.1.2a" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.cmml"><mo movablelimits="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.2.cmml">‚àë</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.cmml"><mi id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.2.cmml">j</mi><mo id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.1.cmml">=</mo><mn id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.T2.16.16.1.m1.5.5.5.5.1.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.3.cmml">N</mi></munderover></mstyle><msup id="S2.T2.16.16.1.m1.5.5.5.5.1.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.cmml"><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.cmml"><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.cmml"><mi id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.2.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="S2.T2.16.16.1.m1.1.1.1.1" xref="S2.T2.16.16.1.m1.1.1.1.1.cmml">i</mi><mo id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.2.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.1.cmml">,</mo><mi id="S2.T2.16.16.1.m1.2.2.2.2" xref="S2.T2.16.16.1.m1.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.1.cmml">‚àí</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.cmml"><mi id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.2.1" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.1.cmml">(</mo><mi id="S2.T2.16.16.1.m1.3.3.3.3" xref="S2.T2.16.16.1.m1.3.3.3.3.cmml">i</mi><mo id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.2.2" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S2.T2.16.16.1.m1.4.4.4.4" xref="S2.T2.16.16.1.m1.4.4.4.4.cmml">j</mi><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.2.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.cmml">)</mo></mrow><mn id="S2.T2.16.16.1.m1.5.5.5.5.1.1.3" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></msqrt><annotation-xml encoding="MathML-Content" id="S2.T2.16.16.1.m1.5b"><apply id="S2.T2.16.16.1.m1.5.5.cmml" xref="S2.T2.16.16.1.m1.5.5"><root id="S2.T2.16.16.1.m1.5.5a.cmml" xref="S2.T2.16.16.1.m1.5.5"></root><apply id="S2.T2.16.16.1.m1.5.5.5.cmml" xref="S2.T2.16.16.1.m1.5.5.5"><times id="S2.T2.16.16.1.m1.5.5.5.6.cmml" xref="S2.T2.16.16.1.m1.5.5.5.6"></times><apply id="S2.T2.16.16.1.m1.5.5.5.7.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7"><divide id="S2.T2.16.16.1.m1.5.5.5.7.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7"></divide><cn type="integer" id="S2.T2.16.16.1.m1.5.5.5.7.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7.2">1</cn><apply id="S2.T2.16.16.1.m1.5.5.5.7.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7.3"><times id="S2.T2.16.16.1.m1.5.5.5.7.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7.3.1"></times><ci id="S2.T2.16.16.1.m1.5.5.5.7.3.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7.3.2">ùëÄ</ci><ci id="S2.T2.16.16.1.m1.5.5.5.7.3.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.7.3.3">ùëÅ</ci></apply></apply><apply id="S2.T2.16.16.1.m1.5.5.5.5.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5"><apply id="S2.T2.16.16.1.m1.5.5.5.5.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2"><csymbol cd="ambiguous" id="S2.T2.16.16.1.m1.5.5.5.5.2.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2">superscript</csymbol><apply id="S2.T2.16.16.1.m1.5.5.5.5.2.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2"><csymbol cd="ambiguous" id="S2.T2.16.16.1.m1.5.5.5.5.2.2.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2">subscript</csymbol><sum id="S2.T2.16.16.1.m1.5.5.5.5.2.2.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.2"></sum><apply id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3"><eq id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.1"></eq><ci id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.2">ùëñ</ci><cn type="integer" id="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.2.3.3">1</cn></apply></apply><ci id="S2.T2.16.16.1.m1.5.5.5.5.2.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.2.3">ùëÄ</ci></apply><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1"><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2"><csymbol cd="ambiguous" id="S2.T2.16.16.1.m1.5.5.5.5.1.2.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2">superscript</csymbol><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2"><csymbol cd="ambiguous" id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2">subscript</csymbol><sum id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.2"></sum><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3"><eq id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.1"></eq><ci id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.2">ùëó</ci><cn type="integer" id="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.2.3.3">1</cn></apply></apply><ci id="S2.T2.16.16.1.m1.5.5.5.5.1.2.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.2.3">ùëÅ</ci></apply><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1"><csymbol cd="ambiguous" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1">superscript</csymbol><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1"><minus id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.1"></minus><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2"><times id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.1"></times><ci id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.2">ùëÉ</ci><interval closure="open" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.2.3.2"><ci id="S2.T2.16.16.1.m1.1.1.1.1.cmml" xref="S2.T2.16.16.1.m1.1.1.1.1">ùëñ</ci><ci id="S2.T2.16.16.1.m1.2.2.2.2.cmml" xref="S2.T2.16.16.1.m1.2.2.2.2">ùëó</ci></interval></apply><apply id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3"><times id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.1"></times><ci id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.2.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.2">ùê∫</ci><interval closure="open" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.1.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.1.1.1.3.3.2"><ci id="S2.T2.16.16.1.m1.3.3.3.3.cmml" xref="S2.T2.16.16.1.m1.3.3.3.3">ùëñ</ci><ci id="S2.T2.16.16.1.m1.4.4.4.4.cmml" xref="S2.T2.16.16.1.m1.4.4.4.4">ùëó</ci></interval></apply></apply><cn type="integer" id="S2.T2.16.16.1.m1.5.5.5.5.1.1.3.cmml" xref="S2.T2.16.16.1.m1.5.5.5.5.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.16.1.m1.5c">\displaystyle\sqrt{\frac{1}{{M\times N}}\sum_{i=1}^{M}\sum_{j=1}^{N}(P(i,j)-G(i,j))^{2}}</annotation></semantics></math></td>
<td id="S2.T2.16.16.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:284.5pt;padding-top:3pt;padding-bottom:3pt;">
<span id="S2.T2.16.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.16.16.3.1.1" class="ltx_p">Is a widely used metric to measure the difference between prediction and ground truth. It calculates the average magnitude of squared differences between corresponding elements in matrices, providing a measure of accuracy.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Applications of FL in CV</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">FL has diverse applications. It enables privacy-preserving object detection, face detection, and video surveillance in smart environments. It also facilitates advancements in healthcare and medical AI, and plays a crucial role in autonomous driving within CV. Below, the survey provides detailed information about each of these domains.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Object detection </span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A special CV task is the segmentation of images and more specifically the detection of salient objects in one or multiple images. Salient object detection (SOD) is an important pre-processing step in many CV tasks, such as object detection and tracking, semantic segmentation, and the interaction between robots and humans (e.g. for tracing human hands during imitation learning scenarios).</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>CL for object detection</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">CL techniques that combine the merits of multiple feature learning techniques from images seem to be the most promising technique. The main issue to handle is the separation of the object from its background, any foreground occlusions, or noise. Other important issues related to the size and variety of objects to be detected in the same image and the need to track objects across consecutive or groups of images. For example, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> emphasized on the use of RGB-D methods that combine RGB images with depth images in order to improve the SOD across multiple images. They proposed a CL framework (CoNet), which combines the edge information extracted from low-level features of the images with the merits of a spatial attention map that detects salient features in the images and depth images that better locate salient objects in a scene. The three different collaborators are combined in a knowledge collector module that first concatenates salient and edge features to jointly learn the boundaries and locations of salient objects and then uses the depth information to separate salient regions from their background. Another work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> presents a NN architecture for content-aware segmentation of sets of images that employs co-saliency maps generated from the input images using a group CL framework (GCoNet). The proposed method outperforms standard SOD alternatives and is capable of detecting co-salient objects in real-time. The main criteria employed are the compactness of the extracted objects within the group of images and the separability of objects from other noise objects in the scenes.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Moving on, the research work on hand detection and tracking over long videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> focused on the need for fast and long-term object detection even in the presence of temporal occlusions and changes among frames. In order to tackle these problems, the researchers suggested the use of a hand detection model (Based on faster R-CNN) combined with the projection across an image (frame) sequences in order to detect clusters of bounding boxes that remain almost stable across frames, even if they are occluded in some of the frames. In order to handle the problem of occlusions, aspect changes and articulations that may hinder the proper detection of objects in uncontrolled scenes, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> propose a method that first detects object parts and consequently tries to associate them and detect the object. The detected object parts are called granules, i.e. small areas with simple properties (e.g. color) that separate them from their context. Using a combinatorial optimization method, based on simulated annealing, they learn how to associate the proper neighboring granules that compose the object. The problem of overlapping objects is also discussed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, where the authors propose a new object detection algorithm that improves the localization of the detected objects and suppresses redundant detection boxes, using a better loss function.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Moreover, CL for medical image segmentation and classification has been applied by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. The authors distinguish between the image segmentation and annotation of segments task, and that of detecting the severity of the disease by considering the image as a whole. They propose the use of CL method for disease severity classification that is based on attention over the detected and annotated segments. A weakly supervised framework for CL is proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> for allowing object detection using labels at the image level. The proposed framework combines a weakly supervised learner (i.e. a two-stream CNN built on VGG16) with a strongly supervised learner (i.e. faster-RCNN) and trains the two subnetworks in a collaborative manner. The former subnetwork optimizes the multi-label classification task, whereas the later optimize the prediction consistency of the detected objects‚Äô bounding boxes. Another weakly supervised object detection approach that employs image-level labels in order to learn to detect the accurate location of objects in the training images has been presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. A CL framework has first been trained on the image-level labels aiming to optimize the image-level classifier and to assign higher weights to instances with more confidence (i.e. without much noise, with fewer objects and simpler backgrounds). Then, a curriculum learning component is employed for guiding the learning procedure of object localization and classification. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, the authors also propose an object detection method from remote sensing images, which combines a weakly supervised detector for image-level labels and a strongly supervised detector for object localization. In a similar manner, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> split the salient object detection task into two sub-tasks that are examined in parallel. The first task relates to the estimation of internal semantics and the second to the prediction of the object boundaries. VGG-16 is used as a basis for feature extraction from images and an additional layer is used to define their semantics. Two decoders are combined for the second part. The first detects the broader area of interest in the images and the second performs a fine-grained boundary detection. The CL network joins the two networks in order to efficiently fuse semantics and boundaries and extract the saliency maps of each image. Combining an image enhancement network with the object recognition network in order to be able to recognize objects in images of extremely low resolution is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. The two networks were trained using a CL technique in which the knowledge from the object recognition network is used to enhance the low-resolution images that are given as input to the other network. The enhanced images are then fed to the classifier to improve its performance. Four different losses (reconstruction, perpetual, classification and edge loss) are combined, in order to optimize the object recognition performance. Transferring the knowledge distillation method in a distributed and CL setup has been proven beneficial for online object detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. The student models (one in each node) are trained separately using as their teacher model, an ensemble that comes from the fusion of the student logits. The teacher‚Äôs knowledge is then distilled back to the students in the form of a soft target. This logic can easily be ported to the FL paradigm as an alternative to FL-based averaging (FedAvg) or more proficient averaging techniques.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>FL for object detection</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The learning process for object detection is usually handled as a centralized offline task, but in resource-restricted environments and applications that need mobility, privacy, and security decentralized and distributed approaches, as well as cloud-edge collaborative approaches have been proposed. In an attempt to optimize object detection performance, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> proposed an FL approach that improves the performance of the federated averaging model aggregation over independent and identically distributed (IID) data. The main claim of their work is that SOTA DL CNN models have been trained in controlled, centralized and balanced datasets and cannot perform well on non-independent and identically distributed (non-IID) data. Since in cases that need privacy and security, such as in medical images for example, the data can be non-IID, the use of simple federated algorithms, such as FedAvg, can be problematic. For this purpose, they propose a weighted variant of FedAvg that improves FL performance in non-IID image data. Using collaborative intelligence on the edge, or balancing between the edge and the cloud is an efficient FL paradigm that can improve the performance of object detection tasks. The work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> has shown that splitting the deep NN models between the cloud and the edge, apart from being privacy-preserving and secure can also be more efficient for object detection tasks. Efficiency can be achieved by quantizing and compressing the tensors of the first layers before sending them to the final layers that reside in the cloud. Depending on where the split is performed, different compression techniques (lossless or lossy ones) are preferred. The same network split strategy (splitNN) has been used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> for protecting the privacy of medical data. More specifically, the client nodes train the network up to a certain layer and the outputs are sent to the server to perform the rest of the training. The inverse process takes place during the back-propagation of the gradients. In order to further protect the labels of the training samples from exposure to the server, the authors also propose a u-shaped forward and backward propagation process in which the first and last layers of the network (along with the training labels) are kept in the clients. Finally, they propose a vertically partitioned data model in which multiple clients train the same first layers of their networks in parallel and send the outputs to the server that concatenates them before proceeding with the remaining layers. This way the clients share the model (at least the last layers) without sharing their data.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Self-supervised-based FL</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Self-supervised learning (SSL) and its variants, momentum contrast (MoCo), bootstrap your own latent (BYOL), and simple siamese (SimSiam), are powerful techniques for learning representations from centralized data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. FL has been combined with SSL to address privacy concerns with decentralized data. However, there is a lack of in-depth understanding of the fundamental building blocks for federated self-supervised learning (FedSSL). The reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> introduces a federated hybrid self-supervised learning (FedHSSL) framework that combines VFL and SSL techniques such as SimSiam, addressing data deficiency. FedHSSL utilizes cross-party views and local views of aligned and unaligned samples within each party to enhance representation learning. It also incorporates partial model aggregation using shared generic features among parties. Experimental results show that FedHSSL outperforms baseline methods, particularly when labeled samples are limited. The researchers also analyze privacy protection mechanisms in FedHSSL, demonstrating its effectiveness in thwarting label inference attacks. Moving forward, Zhuang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> propose a generalized FedSSL framework that accommodates existing SSL methods based on Siamese networks and allows for flexibility in future methods. Through empirical study, the authors uncover insights that challenge the conventional wisdom, showing that stop-gradient operation is not always necessary in FedSSL and retaining local knowledge of clients is beneficial for non-IID data. Building on these insights, the authors propose a new model update approach called federated divergence-aware exponential moving average (FedEMA) update, which adaptively updates local models of clients using exponential moving average of the global model with dynamically measured decay rate based on model divergence. Experimental results demonstrate that FedEMA outperforms existing methods on linear evaluation. The authors hope that this work provides valuable insights for future research in the field of FedSSL. Saeed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> propose a self-supervised approach called scalogram-signal correspondence learning based on wavelet transform (WT) to learn representations from unlabeled sensor inputs such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel-state information. The proposed method addresses issues related to privacy, bandwidth limitations, and cost of annotations associated with centralized data repositories for supervised learning. The auxiliary task involves a deep temporal neural network to determine if a given pair of a signal and its complementary view align with each other, optimized through a contrastive objective. The learned features are extensively evaluated on various public datasets, demonstrating strong performance in multiple domains. The proposed methodology achieves competitive performance with fully supervised networks and outperforms autoencoders in both central and federated contexts. Notably, it improves generalization in a semi-supervised setting by reducing the volume of labeled data required through leveraging self-supervised learning.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Yan et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> propose a robust and label-efficient self-supervised FL (LE-SSFL) framework for medical image analysis. The method utilizes a Transformer-based self-supervised pre-training approach on decentralized target task datasets using masked image modeling. This enables robust representation learning and effective knowledge transfer. Experimental results on simulated and real-world medical imaging non-IID federated datasets demonstrate improved model robustness against data heterogeneity. The approach achieves significant enhancements in test accuracy on retinal, dermatology, and chest X-ray classification without additional pre-training data, outperforming a supervised baseline with ImageNet pre-training. The FedSSL pre-training approach demonstrates better generalization to out-of-distribution data and improved performance with limited labeled data compared to existing FL algorithms. Table <a href="#S3.T3" title="TABLE III ‚Ä£ III-B Self-supervised-based FL ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents of summary of FL frameworks proposed for object detection tasks.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>A summary of FL frameworks proposed for object detection tasks. Learning type (LT), best performance (BP), project link availability (PLA).</figcaption>
<table id="S3.T3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.1.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Ref.</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">LT</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.1.1.5.1.1.1" class="ltx_text" style="font-size:90%;">Description</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.1.1.6.1.1.1" class="ltx_text" style="font-size:90%;">BP</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.1.1.7.1.1.1" class="ltx_text" style="font-size:90%;">Limitations</span></span>
</span>
</th>
<th id="S3.T3.3.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.1.1.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.1.1.8.1.1.1" class="ltx_text" style="font-size:90%;">PLA</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.2.1" class="ltx_tr">
<td id="S3.T3.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.2.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a><span id="S3.T3.3.2.1.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">ResNet50, YOLO 3</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.2.1.4.1.1.1" class="ltx_text" style="font-size:90%;">Pascal Voc 2007, MS COCO</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.2.1.5.1.1.1" class="ltx_text" style="font-size:90%;">A new effective loss function handling both overlapping and non-overlapping boxes</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.2.1.6.1.1.1" class="ltx_text" style="font-size:90%;">AP=17.08</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.2.1.7.1.1.1" class="ltx_text" style="font-size:90%;">only applicable to one stage detectors</span></span>
</span>
</td>
<td id="S3.T3.3.2.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.3.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.2.1.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.2.1.8.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/JaryHuang/awesome_SSD_FPN_GIoU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/JaryHuang/awesome_SSD_FPN_GIoU</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.3.2" class="ltx_tr">
<td id="S3.T3.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.3.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="S3.T3.3.3.2.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">WSDNN, Faster R-CNN</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.3.2.4.1.1.1" class="ltx_text" style="font-size:90%;">Pascal 2007, 2012</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.3.2.5.1.1.1" class="ltx_text" style="font-size:90%;">An end-to-end to end weakly supervised collaborative learning framework to improve model accuracy</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.3.2.6.1.1.1" class="ltx_text" style="font-size:90%;">AP=20.3</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.3.2.7.1.1.1" class="ltx_text" style="font-size:90%;">Achieves maximum performance with a high number of iterations.</span></span>
</span>
</td>
<td id="S3.T3.3.3.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.3.2.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.4.3" class="ltx_tr">
<td id="S3.T3.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.4.3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a><span id="S3.T3.3.4.3.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">VGG16, Fast RCNN</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.4.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Pascal Voc 2007, test-2007, test-2010, COCO 2014</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.4.3.5.1.1.1" class="ltx_text" style="font-size:90%;">A novel collaborative self-paced curriculum learning exploiting prior knowledge to enhance the accuracy of weakly object detection</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.4.3.6.1.1.1" class="ltx_text" style="font-size:90%;">mAP=7.2</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.4.3.7.1.1.1" class="ltx_text" style="font-size:90%;">Generalizability needs to be verified with other tasks and datasets.</span></span>
</span>
</td>
<td id="S3.T3.3.4.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.4.3.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.4.3.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.5.4" class="ltx_tr">
<td id="S3.T3.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.5.4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="S3.T3.3.5.4.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">FCCNET</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.5.4.4.1.1.1" class="ltx_text" style="font-size:90%;">TGRS-HRRSD, DIOR</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.5.4.5.1.1.1" class="ltx_text" style="font-size:90%;">A new CNN model based on several mechanisms that is trained by alternating strongly and weakly detectors</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.5.4.6.1.1.1" class="ltx_text" style="font-size:90%;">mAP=0.2</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.5.4.7.1.1.1" class="ltx_text" style="font-size:90%;">The results drastically between classes.</span></span>
</span>
</td>
<td id="S3.T3.3.5.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.5.4.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.5.4.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.6.5" class="ltx_tr">
<td id="S3.T3.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.6.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="S3.T3.3.6.5.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">SDCLNET, VGG, ResNet</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.6.5.4.1.1.1" class="ltx_text" style="font-size:90%;">ECSSD, Pascal-C, DUTS, HKU-IS, DUT-OMRON, SOD</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.6.5.5.1.1.1" class="ltx_text" style="font-size:90%;">A new model leveraging collaborative learning to optimize to sub goals, internal semantic estimation and boundary detail prediction, to improve the accuracy of saliency maps</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.6.5.6.1.1.1" class="ltx_text" style="font-size:90%;">MAE=0.03</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.6.5.7.1.1.1" class="ltx_text" style="font-size:90%;">Considers only the accuracy while disregarding aspects such as network space redundancy.</span></span>
</span>
</td>
<td id="S3.T3.3.6.5.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.6.5.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.6.5.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.7.6" class="ltx_tr">
<td id="S3.T3.3.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.7.6.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S3.T3.3.7.6.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.7.6.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.7.6.3.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.7.6.4.1.1.1" class="ltx_text" style="font-size:90%;">CIFAR-10, CIFAR-100, ImageNet</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.7.6.5.1.1.1" class="ltx_text" style="font-size:90%;">improvement of object detection in low-resolution images through use of two neural networks trained jointly</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.7.6.6.1.1.1" class="ltx_text" style="font-size:90%;">ACC=7.31</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.7.6.7.1.1.1" class="ltx_text" style="font-size:90%;">Acceptable yet unsatisfactory results.</span></span>
</span>
</td>
<td id="S3.T3.3.7.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.7.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.7.6.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.7.6.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.8.7" class="ltx_tr">
<td id="S3.T3.3.8.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.8.7.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="S3.T3.3.8.7.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.8.7.2.1.1.1" class="ltx_text" style="font-size:90%;">CL</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.8.7.3.1.1.1" class="ltx_text" style="font-size:90%;">ResNet</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.8.7.4.1.1.1" class="ltx_text" style="font-size:90%;">MS COCO, CIFAR, ImageNet</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.8.7.5.1.1.1" class="ltx_text" style="font-size:90%;">A new online knowledge dist distillation method leveraging several networks trained in parallel acting all as students</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.8.7.6.1.1.1" class="ltx_text" style="font-size:90%;">ACC=72.9</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.8.7.7.1.1.1" class="ltx_text" style="font-size:90%;">Generalization issues with limited data.</span></span>
</span>
</td>
<td id="S3.T3.3.8.7.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.8.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.8.7.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.8.7.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.9.8" class="ltx_tr">
<td id="S3.T3.3.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.9.8.1.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib84" title="" class="ltx_ref">84</a><span id="S3.T3.3.9.8.1.1.1.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.2.1.1" class="ltx_p" style="width:8.5pt;"><span id="S3.T3.3.9.8.2.1.1.1" class="ltx_text" style="font-size:90%;">FL</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T3.3.9.8.3.1.1.1" class="ltx_text" style="font-size:90%;">SSD300</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T3.3.9.8.4.1.1.1" class="ltx_text" style="font-size:90%;">Pascal Voc 2007</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.5.1.1" class="ltx_p" style="width:128.0pt;"><span id="S3.T3.3.9.8.5.1.1.1" class="ltx_text" style="font-size:90%;">FL is used to train the model with a large number of features. KLD measures weights divergence between non-IID models. The scheme reduces the impact of abnormal weights in FedAvg using SSD.</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S3.T3.3.9.8.6.1.1.1" class="ltx_text" style="font-size:90%;">FedAvg=76.6 (NonIID)</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.7.1.1" class="ltx_p" style="width:113.8pt;"><span id="S3.T3.3.9.8.7.1.1.1" class="ltx_text" style="font-size:90%;">Generalizability needs to be verified with other datasets.</span></span>
</span>
</td>
<td id="S3.T3.3.9.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T3.3.9.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.9.8.8.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T3.3.9.8.8.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Face detection</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The detection of face mask wearing is another CV task that can be significantly benefited by FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. When visual data are collected by surveillance cameras, the monitoring of face mask-wearing can be split into two separate tasks: i) the detection of faces, ii) the detection of proper mask-wearing. The second task poses a risk to user privacy, making FL the preferred approach. The proposed scheme utilizes a dilation retina-Net face location network for <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">detecting</span> faces, including both clear and occluded ones, in a dense crowd. Additionally, an SRNet20 network has been employed to <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">process</span> the detected faces. The second network is trained using multiple client nodes that share their models with a central server, which aggregates the client models.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Furthermore, a related-face detection task is facial expression recognition, which has numerous applications in the field of affective computing. The study conducted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> proposes a few-shot meta-learning-based FL framework that requires only a limited number of labeled images for training. The local devices process numerous unlabeled face images, which are utilized to train an encoder network for learning face representations. Periodically, a central server aggregates the models using the FedAvg algorithm to enhance representation learning. Additionally, a second few-shot learner employs a small number of labeled private facial expressions to train local models, which are subsequently aggregated in the central server.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Video surveillance and smart environments</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">An application that attracts the interest of CV researchers in terms of balancing the workload between centralized (cloud) and distributed (edge-based) architectures is video surveillance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. Edge computing has offered several solutions in the direction of workload balancing. However, it added new challenges concerning the compression and filtering of data that is transmitted over the communication network, and the fragmentation of knowledge across the edge devices. In order to tackle the aforementioned projects, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> propose a multi-layer design in order to minimize the communication between the edge and the cloud and employ FL to update the detection models without sending training data to the cloud. The multi-layer architecture comprises a data acquisition layer on the edge that performs compression and filtering, an object detection and identification layer with edge servers that are specialized in specific tasks (e.g. face recognition, licence plate detection, etc.) and a FL layer that solves tasks on the edge and marks data and models that must communicated to the cloud. The information transferred to the cloud usually comprises model parameters and the respective weights.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">A generic FL architecture that processes sensor data for human activity recognition has been presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. The proposed FL architecture is based on a federated aggregator that is trained using local data on the edge nodes. The aggregator sends the models to a central entity where the Federated Average algorithm is employed to generate a common model that is distributed back to the edge nodes. The federated aggregator in the video surveillance scenario could be a local server that is trained on the edge using local data.
Moving on, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> introduces a FL framework based on a lightweight version of the Dense-MobileNet model for processing aerial images collected by unmanned aerial vehicles (UAV) swarms. The UAV swarms used for vision sensing collect haze images, and are supported by ground sensors that collect information about air quality. Each UAV employs a ML model in order to correlate air quality with the haze images and shares its trained gradient with a central server. The server combines the gradients a learns a global model, which is then used to predict air-quality distribution in the region.
Besides, platforms that supports FL powered applications of CV in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Lia et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> demonstrate FedVision in a fire detection task, using a YOLOv3 model for object detection and they report its use on three pilots concerning safety hazard detection, photovoltaic panel monitoring and suspicious transaction monitoring in ATMs via cameras. Catalfamo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> thoroughly examine the use of edge federation for implementing ML-based solutions. The platform is introduced for deploying and managing complex services on the edge that utilizes micro-services, which are small, independent, and loosely-connected services. This platform enables the management of these services across a network of edge devices by abstracting the physical devices they run on. The the effectiveness of this solution was demonstrated through a case study involving video analysis in the field of morphology.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS1.5.1.1" class="ltx_text">III-D</span>1 </span>Action recognition</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Using knowledge distillation, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> allows client nodes with limited computational resources to execute action recognition by performing model compression at the central server on a large-scale data repository.
In doing so, fine-tuning is used since small datasets are not appropriate for action recognition models to learn complex spatio-temporal features.
Moving forward, an asynchronous federated optimization is adopted and the convergence bound has been shown since the present clients‚Äô computing resources were heterogeneous. In a different paradigm, a driver action recognition system is built by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> using FL. The latter has been used for model training to protect users‚Äô privacy while enabling online model upgrades. Similarly, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, an FL-based driver activity recognition system is implemented by training the detection model in a decentralized fashion.
FedAvg and FedGKT have been implemented and their performance has been demonstrated on the 2022 AI City Challenge. Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> proposed a semi-supervised FL for activity recognition, which helps edge devices conducting unsupervised learning of general representations using autoencoders and non annotated local data. In this case the cloud server performed supervised learning by training activity classifiers on the learned representations, and annotated data.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS2.5.1.1" class="ltx_text">III-D</span>2 </span>Crowd counting</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">When more refined tasks are assigned to CV algorithms in dense crowds, such as the detection of face mask wearing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, the difficulty increases since the target objects (i.e. masks) are in different scales and occlusions. FL is a promising solution that enhances the privacy of individuals. However, scale variations, occlusions and the diversity in crowd distribution are still the open issues that demand efficient detection techniques, such as deep negative correlation learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>, relational attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>, etc.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">Crowd counting is a complex CV task especially when multiple sensors (i.e. cameras) are combined. FL approaches can be helpful since they allow the distributed trainers to exchange their models and improve their performance quickly, especially when a centralized trainer is able to periodically validate and improve the quality of the aggregated model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, when the distributed nodes are not trusted in advance, or when the quality of their data is ambiguous, more control mechanisms and incentives are needed to avoid the deterioration of the resulting model.
When the distributed trainers have to co-operate in order to reach a consensus, either it is for crowd counting or for any other task, it is important to provide them enough incentives in order to become trustful <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>. Blockchain-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> manage to distribute the incentive among the trained models and evaluate their reliability in order to fairly partition any potential profit (or trust). This way, they allow FL algorithms to become more robust, by including trustful trainers and honest reporters that detect misbehaviors and block them from the FL process.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS3.5.1.1" class="ltx_text">III-D</span>3 </span>Anomaly detection</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">An interesting data mining task with many CV applications is anomaly detection. It involves the identification of strange patterns in data, which may indicate a fake or incorrect situation. Several cutting-edge ML and DL algorithms have been developed in the literature in order to detect and prevent such incidents. When it comes to CV various DL architectures, from variational autoencoders (VAE), generative adversarial networks (GANs) or recurrent neural networks (RNNs) can be trained to detect anomalies in video sequences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.
The capitalization of the use of autoencoders and FL for detecting anomalies is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. The bipartite structure of autoencoders, and their ability to reconstruct the input in their output, allows the detection of anomalies based on the errors spotted during the regeneration of the input. The authors train two clients using different parts of the training dataset and they merge the resulting models using FL library called PySyft for secure and private DL. They employ the MSE loss metric to compare the input and output and examine whether it exceeds a threshold in order to decide whether it is an anomaly or not. Bharti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> have proposed an edge-enabled FL approach for the automatic inspection of products using CV. Their main basis is SqueezeNet, a lightweight model pre-trained on the ImageNet dataset, which is able to identify 100 different types of objects. Although the image processing model has been trained using multiple images of normal products the various defects are detected using an anomaly detection algorithm. SqueezeNet acts as a feature extractor from images, which are then fed to a dense layer with as many output neurons as possible anomaly classes, plus one for the normal products. In its federated version, the edge server aggregates the various local models and computes a new global model that is reshared with the local nodes. Table <a href="#S3.T4" title="TABLE IV ‚Ä£ III-D3 Anomaly detection ‚Ä£ III-D Video surveillance and smart environments ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> summarizes some of the FL frameworks proposed for video surveillance and smart environments.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Summary of FL frameworks proposed for video surveillance and smart environments. Best performance (BP), project link availability (PLA).</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.1.1.1" class="ltx_p" style="width:8.5pt;">Ref.</span>
</span>
</th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.2.1.1" class="ltx_p" style="width:51.2pt;">Model</span>
</span>
</th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;">Dataset</span>
</span>
</th>
<th id="S3.T4.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.4.1.1" class="ltx_p" style="width:142.3pt;">Description</span>
</span>
</th>
<th id="S3.T4.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.5.1.1" class="ltx_p" style="width:42.7pt;">BP</span>
</span>
</th>
<th id="S3.T4.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.6.1.1" class="ltx_p" style="width:113.8pt;">Limitations</span>
</span>
</th>
<th id="S3.T4.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.7.1.1" class="ltx_p" style="width:14.2pt;">PLA</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<td id="S3.T4.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.2.1.1" class="ltx_p" style="width:51.2pt;">ResNet 8</span>
</span>
</td>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.3.1.1" class="ltx_p" style="width:56.9pt;">AI City Challenge, StateFarm</span>
</span>
</td>
<td id="S3.T4.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.4.1.1" class="ltx_p" style="width:142.3pt;">Optimise FL for resource-limited devices with FedGKT</span>
</span>
</td>
<td id="S3.T4.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 95%</span>
</span>
</td>
<td id="S3.T4.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.6.1.1" class="ltx_p" style="width:113.8pt;">FL results relatively close to the centralized results but are not superior.</span>
</span>
</td>
<td id="S3.T4.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.2.1.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.2.1.1" class="ltx_p" style="width:51.2pt;">LSTM</span>
</span>
</td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.3.1.1" class="ltx_p" style="width:56.9pt;">OPP, DG, PAMAP2</span>
</span>
</td>
<td id="S3.T4.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.4.1.1" class="ltx_p" style="width:142.3pt;">Addressed the problem of the lack of labeled data on the client‚Äôs level</span>
</span>
</td>
<td id="S3.T4.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 82%</span>
</span>
</td>
<td id="S3.T4.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.6.1.1" class="ltx_p" style="width:113.8pt;">Require labeled data on the central server.</span>
</span>
</td>
<td id="S3.T4.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.3.2.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<td id="S3.T4.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.2.1.1" class="ltx_p" style="width:51.2pt;">MobileNetV2, ResNet50, VGG16, Xception</span>
</span>
</td>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.3.1.1" class="ltx_p" style="width:56.9pt;">StateFarm</span>
</span>
</td>
<td id="S3.T4.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.4.1.1" class="ltx_p" style="width:142.3pt;">Evaluation of FL for driver‚Äôs action recognition</span>
</span>
</td>
<td id="S3.T4.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 85%</span>
</span>
</td>
<td id="S3.T4.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.6.1.1" class="ltx_p" style="width:113.8pt;">Evaluation considered only the accuracy of the model.</span>
</span>
</td>
<td id="S3.T4.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.4.3.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<td id="S3.T4.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.2.1.1" class="ltx_p" style="width:51.2pt;">ReNet</span>
</span>
</td>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.3.1.1" class="ltx_p" style="width:56.9pt;">HMDB51, UCF101</span>
</span>
</td>
<td id="S3.T4.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.4.1.1" class="ltx_p" style="width:142.3pt;">Custom model initialisation through knowledge distillation with asynchronous aggregation</span>
</span>
</td>
<td id="S3.T4.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 89.5%</span>
</span>
</td>
<td id="S3.T4.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.6.1.1" class="ltx_p" style="width:113.8pt;">Limited consideration of non-iid data.</span>
</span>
</td>
<td id="S3.T4.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.5.4.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<td id="S3.T4.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.2.1.1" class="ltx_p" style="width:51.2pt;">CNN</span>
</span>
</td>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.3.1.1" class="ltx_p" style="width:56.9pt;">MNIST, Next-Character-Prediction</span>
</span>
</td>
<td id="S3.T4.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.4.1.1" class="ltx_p" style="width:142.3pt;">Federated crowd sensing with an incentive mechanism to reward
and motivate participants</span>
</span>
</td>
<td id="S3.T4.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 80%</span>
</span>
</td>
<td id="S3.T4.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.6.1.1" class="ltx_p" style="width:113.8pt;">Even though the loss reduced significantly, it is still high.</span>
</span>
</td>
<td id="S3.T4.1.6.5.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.6.5.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<td id="S3.T4.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.2.1.1" class="ltx_p" style="width:51.2pt;">DDCBF</span>
</span>
</td>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.3.1.1" class="ltx_p" style="width:56.9pt;">NA</span>
</span>
</td>
<td id="S3.T4.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.4.1.1" class="ltx_p" style="width:142.3pt;">Suggest an FL framework to distribute trust and incentive among trainers</span>
</span>
</td>
<td id="S3.T4.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.5.1.1" class="ltx_p" style="width:42.7pt;">accuracy = 97%</span>
</span>
</td>
<td id="S3.T4.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.6.1.1" class="ltx_p" style="width:113.8pt;">Require long time for convergence</span>
</span>
</td>
<td id="S3.T4.1.7.6.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.7.6.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.8.7" class="ltx_tr">
<td id="S3.T4.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.2.1.1" class="ltx_p" style="width:51.2pt;">DRFL</span>
</span>
</td>
<td id="S3.T4.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.3.1.1" class="ltx_p" style="width:56.9pt;">Wider Face</span>
</span>
</td>
<td id="S3.T4.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.4.1.1" class="ltx_p" style="width:142.3pt;">Proposes a cascade network with two stages trained with FL</span>
</span>
</td>
<td id="S3.T4.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.5.1.1" class="ltx_p" style="width:42.7pt;">mAP = 98.5%</span>
</span>
</td>
<td id="S3.T4.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.6.1.1" class="ltx_p" style="width:113.8pt;">Can not achieve real-time detection due to computational complexity.</span>
</span>
</td>
<td id="S3.T4.1.8.7.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.8.7.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/shizenglin/Deep-NCL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/shizenglin/Deep-NCL</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T4.1.9.8" class="ltx_tr">
<td id="S3.T4.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.1.1.1" class="ltx_p" style="width:8.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite></span>
</span>
</td>
<td id="S3.T4.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.2.1.1" class="ltx_p" style="width:51.2pt;">CNN</span>
</span>
</td>
<td id="S3.T4.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.3.1.1" class="ltx_p" style="width:56.9pt;">MVTech</span>
</span>
</td>
<td id="S3.T4.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.4.1.1" class="ltx_p" style="width:142.3pt;">FL-based approach to enable visual inspectors to recognise unseen defects in industrial setups</span>
</span>
</td>
<td id="S3.T4.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.5.1.1" class="ltx_p" style="width:42.7pt;">F1-score ¬ø= 90%</span>
</span>
</td>
<td id="S3.T4.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.6.1.1" class="ltx_p" style="width:113.8pt;">Limited evaluation considering only detection performance.</span>
</span>
</td>
<td id="S3.T4.1.9.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T4.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.9.8.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://developer.apple.com/documentation/vision" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.apple.com/documentation/vision</a> , <a target="_blank" href="https://github.com/hollance/coreml-training" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hollance/coreml-training</a></span></span></span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Healthcare and medical AI</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Nowadays, DL methods with large-scale datasets can produce clinically useful models for computer-aided diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>. However, privacy and ethical concerns are increasingly critical, which makes it difficult to collect large quantities of data from multiple institutions. FL provides a promising decentralized solution to CL by exchanging client models instead of private data. Sheller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> performed the first study that investigated the use of FL for multi-institutional collaboration, and enabled the training of DL models without sharing patients‚Äô data. In particular, the aggregation process involves calculating a weighted average of institutional updates, where each institution‚Äôs weight is determined by the proportion of total data instances it holds. This entire process, comprising local training, update aggregation, and distribution of new parameters, is referred to as a federated round. Linardos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> modeled cardiovascular magnetic resonance using a FL scheme that concentrated on the diagnosis of hypertrophic cardiomyopathy. A 3D-CNN model, pre-trained on action recognition, was deployed. Moreover, shape prior information has been integrated into 3D-CNN using two techniques and four data augmentation strategies (Fig. <a href="#S3.F10" title="Figure 10 ‚Ä£ III-E Healthcare and medical AI ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). This approach has then been evaluated on the automatic cardiac diagnosis challenge (ACDC) dataset. The multi-site fMRI classification problem is addressed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> while preserving privacy using a FL model. Accordingly, a decentralized iterative optimization has been deployed before using a randomization mechanism to alter shared local model weights. In the same way, Dayan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> developed and trained a FL model on data from data from 20 institutes worldwide, namely EMR chest X-ray AI model (EXAM). The latter was built based on the COV-2 clinical decision support (CDS) model. This helps predict the future oxygen requirements of COVID-19 patients using chest X-rays, laboratory data, and inputs of vital signs. In the same way, an FL-based solution that screens COVID-19 from chest X-ray (CXR) images is deployed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>.
In addition, a communication-efficient CNN-based FL scheme to multi-chest diseases classification from CXR images is proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>. Yan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> proposed a real-time contribution measurement approach for participants in FL, which is called Fedcm. The latter has been applied to identify COVID-19 based on medical images. Moving forward, Roth et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> deployed a FL approach for building medical imaging breast density classification solutions. Data from seven clinical institutions around the world has been used to train the FL algorithm.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Because of the gaps in fMRI distributions from distinct sites, a mixture of experts domain adaptation (MoE-DA) and adversarial domain alignment (ADA) schemes have been integrated into the FL algorithm. The reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> introduced a variation-aware FL approach, in which the variations between clients have been reduced by transforming the images of all clients onto a common image space. A privacy-preserving generative adversarial network, namely PPWGAN-GP is introduced. Moving on, a modified CycleGAN is deployed for every client to transfer its raw images to the target image space defined by the shared synthesized images. Accordingly, this helps address the cross-client variation problem while preserving privacy. Similarly, for data privacy-preserving, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> used a FL scheme to securely access and meta-analyze biomedical data without sharing individual information. Specifically, brain structural relationships are investigated across clinical cohorts and diseases. Sheller and their colleagues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> deployed a FL scheme to facilitate multi-institutional collaborations without the need to share patients‚Äô data. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>, MR data from multiple institutions is shared with privacy preservation. Moreover, cross-site modeling for MR image reconstruction is introduced to reduce domain shift and improve the generalization of the FL model. Moving on,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> combined differential privacy and weakly-supervised attention multiple instance learning (WS-AMIL) in order to develop a privacy-preserving FL approach for gigapixel whole slide images in computational pathology. Researchers in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> implemented differential-privacy schemes for protecting patients‚Äô data in a FL setup designed for brain tumour segmentation on the BraTS dataset. In the same field, Bercea et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> developed a disentangled FL approach to segment brain pathologies in an unsupervised mode. Cetinkaya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> attempted to improve the performance of FL-based medical image analysis in non IID settings using image augmentation.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2308.13558/assets/x10.png" id="S3.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="260" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The methodology applied for conducting the experiments in this study is outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Leave center out and collaborative cross-validations, both employed collaborative data sharing (CDS) and FL for the training.</figcaption>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">The schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> consider whether the necessity of watermarking is required when using FL. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> introduced a FL-based zero-watermarking technique for security and privacy preservation in teledermatology healthcare frameworks. FL-based autoencoder is employed for extracting image features from dermatology data using two-dimensional discrete cosine transform (2D-DCT). Conversely, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> are two strategies proposed for watermarking-based FL. The first is called WAFFLE proposed to prevent global FL model theft by offering a mechanism for model owners to showcase their ownership of the models, and the second is a client-side backdoor-triggered watermarking is adopted to secure FL model verification. Blockchain is also another means used for data privacy, Polap et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> developed an intelligent medical system based on agent architecture using blockchain and FL technologies. Since FL algorithms do not inherently contain privacy-preserving mechanisms and can be sensitive to privacy-centered attacks that can divulge patients‚Äô data, it is important to augment them with privacy-enhancing technologies, especially in clinical applications that are implemented in a multi-institutional setting. In this context, a differentially private FL solution is suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> to segment multi-site medical images in order to further enhance privacy against attacks. Besides, Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> propose an FL-based approach for distributed data in medical cyber-physical systems. Their approach helps in training DL models for disease diagnosis following three steps which are repeated in cycle: i) training a global model using offline medical images and transferring the global model to the local diagnosis nodes, ii) re-training the local models using local data and iii) sending them back to the central server for federated averaging. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> propose a technique to harmonize local and global drifts in FL models on heterogeneous medical images.
In doing so, the local update drift is first mitigated by normalizing amplitudes of images transformed into the frequency domain and then a client weight perturbation guiding each local model to reach a flat optimum is designed based on harmonized features.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">The problem setting of federated domain generalization (FedDG) is solved in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>. This helps in learning a FL architecture from various distributed SDs, which enables its generalization to unseen TDs. This was made possible by introducing a the episodic learning in continuous frequency space (ELCFS) technique. Moving on, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> propose a partial initialization-based cross-domain personalized FL, namely PartialFed. Their method is based on loading a subset of the global model‚Äôs parameters instead of loading the entire model, as it is done in most of the FL approaches. Thus, it is closer to the split Learning paradigm. More over, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> design an effective communication, and computation efficient FL scheme using progressive training. This helps to reduce computation and two-way communication costs, while preserving almost the same performance of the final models. While FL allows collaboratively training, using a joint model that is trained in multiple medical centers that maintain their data decentralized to preserve privacy, the federated optimizations face the heterogeneity and non-uniformity of data distribution across medical centers. To overcome this issue, an FL scheme with shared label distribution, namely FedSLD, is proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>. This approach can reduce the discrepancy brought by data heterogeneity and adjust the contribution of every sample to the local objective during optimization via the knowledge of clients‚Äô label distributions.
Tbale <a href="#S3.T5" title="TABLE V ‚Ä£ III-E Healthcare and medical AI ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents a Summary of FL frameworks proposed for healthcare and AI applications.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Summary of FL frameworks proposed for healthcare and medical AI applications. Best performance (BP), project link availability (PLA).</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.1.1.1" class="ltx_p" style="width:14.2pt;">Ref.</span>
</span>
</th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.2.1.1" class="ltx_p" style="width:42.7pt;">Model</span>
</span>
</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.3.1.1" class="ltx_p" style="width:71.1pt;">Dataset</span>
</span>
</th>
<th id="S3.T5.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.4.1.1" class="ltx_p" style="width:128.0pt;">Description</span>
</span>
</th>
<th id="S3.T5.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.5.1.1" class="ltx_p" style="width:28.5pt;">BP (%)</span>
</span>
</th>
<th id="S3.T5.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.6.1.1" class="ltx_p" style="width:130.9pt;">Limitations</span>
</span>
</th>
<th id="S3.T5.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.7.1.1" class="ltx_p" style="width:14.2pt;">PLA</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.2.1" class="ltx_tr">
<td id="S3.T5.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.2.1.1" class="ltx_p" style="width:42.7pt;">3D-CNN</span>
</span>
</td>
<td id="S3.T5.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.3.1.1" class="ltx_p" style="width:71.1pt;">M&amp;M and ACDC datasets</span>
</span>
</td>
<td id="S3.T5.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.4.1.1" class="ltx_p" style="width:128.0pt;">Modeling cardiovascular magnetic resonance with focus on diagnosing hypertrophic cardiomyopathy</span>
</span>
</td>
<td id="S3.T5.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.5.1.1" class="ltx_p" style="width:28.5pt;">AUC = 78</span>
</span>
</td>
<td id="S3.T5.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.6.1.1" class="ltx_p" style="width:130.9pt;">Presence of bias against ACDC on the shape and intensity set-up where FL exhibits an AUC performance of about 0.85 to 0.89.</span>
</span>
</td>
<td id="S3.T5.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.2.1.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/Linardos/federated-HCM-diagnosis" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Linardos/federated-HCM-diagnosis</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.3.2" class="ltx_tr">
<td id="S3.T5.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.2.1.1" class="ltx_p" style="width:42.7pt;">PFA</span>
</span>
</td>
<td id="S3.T5.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.3.1.1" class="ltx_p" style="width:71.1pt;">Real-World Dermoscopic FL Dataset <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/CityU-AIM-Group/PRR-FL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CityU-AIM-Group/PRR-FL</a></span></span></span></span>
</span>
</td>
<td id="S3.T5.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.4.1.1" class="ltx_p" style="width:128.0pt;">A personalized retrogress-resilient FL with modification in the clients and server.</span>
</span>
</td>
<td id="S3.T5.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.5.1.1" class="ltx_p" style="width:28.5pt;">AUC= 88.92, F1= 70.75</span>
</span>
</td>
<td id="S3.T5.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.6.1.1" class="ltx_p" style="width:130.9pt;">Generalization with other datasets is not confirmed.</span>
</span>
</td>
<td id="S3.T5.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.3.2.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/CityU-AIM-Group/PRR-FL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CityU-AIM-Group/PRR-FL</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.4.3" class="ltx_tr">
<td id="S3.T5.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.2.1.1" class="ltx_p" style="width:42.7pt;">MoE-DA, ADA</span>
</span>
</td>
<td id="S3.T5.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.3.1.1" class="ltx_p" style="width:71.1pt;">Autism Brain Imaging Data Exchange dataset (ABIDE I)</span>
</span>
</td>
<td id="S3.T5.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.4.1.1" class="ltx_p" style="width:128.0pt;">Privacy-preserving FL and domain adaptation for multi-site fMRI analysis</span>
</span>
</td>
<td id="S3.T5.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.5.1.1" class="ltx_p" style="width:28.5pt;">ACC= 78.9</span>
</span>
</td>
<td id="S3.T5.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.6.1.1" class="ltx_p" style="width:130.9pt;">The model updating strategy is not optima. Additionally,the sensitivity of the mapping function was difficult to estimate.</span>
</span>
</td>
<td id="S3.T5.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.4.3.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/xxlya/Fed_ABIDE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/xxlya/Fed_ABIDE</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.5.4" class="ltx_tr">
<td id="S3.T5.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.2.1.1" class="ltx_p" style="width:42.7pt;">PPWGAN-GP, modified CycleGAN</span>
</span>
</td>
<td id="S3.T5.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.3.1.1" class="ltx_p" style="width:71.1pt;">LocalPCa, PROSTATEx challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.4.1.1" class="ltx_p" style="width:128.0pt;">Address cross-client variation problem among medical image data using VAFL.</span>
</span>
</td>
<td id="S3.T5.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.5.1.1" class="ltx_p" style="width:28.5pt;">AUC= 96.79, ACC= 98.3%</span>
</span>
</td>
<td id="S3.T5.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.6.1.1" class="ltx_p" style="width:130.9pt;">Do no address the inter-observer problem and incomplete image-to-image translation.</span>
</span>
</td>
<td id="S3.T5.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.5.4.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.6.5" class="ltx_tr">
<td id="S3.T5.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.2.1.1" class="ltx_p" style="width:42.7pt;">fPCA</span>
</span>
</td>
<td id="S3.T5.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.3.1.1" class="ltx_p" style="width:71.1pt;">ADNI, PPMI, MIRIAD and UK Biobank</span>
</span>
</td>
<td id="S3.T5.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.4.1.1" class="ltx_p" style="width:128.0pt;">Meta-analysis of large-scale subcortical brain data using FL</span>
</span>
</td>
<td id="S3.T5.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.5.1.1" class="ltx_p" style="width:28.5pt;">N/A</span>
</span>
</td>
<td id="S3.T5.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.6.1.1" class="ltx_p" style="width:130.9pt;">No comparisons with the SOTA have been reported.</span>
</span>
</td>
<td id="S3.T5.1.6.5.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.6.5.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.7.6" class="ltx_tr">
<td id="S3.T5.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.2.1.1" class="ltx_p" style="width:42.7pt;">fPCA</span>
</span>
</td>
<td id="S3.T5.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.3.1.1" class="ltx_p" style="width:71.1pt;">Synthetic and real world private data</span>
</span>
</td>
<td id="S3.T5.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.4.1.1" class="ltx_p" style="width:128.0pt;">Predict the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs</span>
</span>
</td>
<td id="S3.T5.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.5.1.1" class="ltx_p" style="width:28.5pt;">AUC¬ø 92</span>
</span>
</td>
<td id="S3.T5.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.6.1.1" class="ltx_p" style="width:130.9pt;">Normalization techniques to enable the training of AI models in FL were not investigated.</span>
</span>
</td>
<td id="S3.T5.1.7.6.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.7.6.7.1.1" class="ltx_p" style="width:14.2pt;">Yes<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://ngc.nvidia.com/catalog/models?orderBy=scoreDESC&amp;pageNumber=0&amp;query=covid&amp;quickFilter=models&amp;filters" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ngc.nvidia.com/catalog/models?orderBy=scoreDESC&amp;pageNumber=0&amp;query=covid&amp;quickFilter=models&amp;filters</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.8.7" class="ltx_tr">
<td id="S3.T5.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.2.1.1" class="ltx_p" style="width:42.7pt;">Agent-based mod</span>
</span>
</td>
<td id="S3.T5.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.3.1.1" class="ltx_p" style="width:71.1pt;">Private data</span>
</span>
</td>
<td id="S3.T5.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.4.1.1" class="ltx_p" style="width:128.0pt;">Blockchain technology and threaded FL for private sharing of medical images.</span>
</span>
</td>
<td id="S3.T5.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.5.1.1" class="ltx_p" style="width:28.5pt;">ACC= 80</span>
</span>
</td>
<td id="S3.T5.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.6.1.1" class="ltx_p" style="width:130.9pt;">Poor internet connection hinders accessing the data. The initial adaptation of the
classifier for practical use needs more investigation.</span>
</span>
</td>
<td id="S3.T5.1.8.7.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.8.7.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.9.8" class="ltx_tr">
<td id="S3.T5.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.2.1.1" class="ltx_p" style="width:42.7pt;">FedSLD</span>
</span>
</td>
<td id="S3.T5.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.3.1.1" class="ltx_p" style="width:71.1pt;">MNIST, CIFAR10, OrganMNIST(axial), PathMNIST</span>
</span>
</td>
<td id="S3.T5.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.4.1.1" class="ltx_p" style="width:128.0pt;">FL with shared label distribution to classify medical images</span>
</span>
</td>
<td id="S3.T5.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.5.1.1" class="ltx_p" style="width:28.5pt;">ACC= 95.85</span>
</span>
</td>
<td id="S3.T5.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.6.1.1" class="ltx_p" style="width:130.9pt;">Reduce the impact of non-IID data by leveraging the clients‚Äô label distribution.</span>
</span>
</td>
<td id="S3.T5.1.9.8.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.9.8.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.10.9" class="ltx_tr">
<td id="S3.T5.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite></span>
</span>
</td>
<td id="S3.T5.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.2.1.1" class="ltx_p" style="width:42.7pt;">PartialFed, PartialFed-Adaptive</span>
</span>
</td>
<td id="S3.T5.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.3.1.1" class="ltx_p" style="width:71.1pt;">KITTI, WFace, VOC, LISA, DOTA, COCO, WC, CP, CM, Kit, DL</span>
</span>
</td>
<td id="S3.T5.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.4.1.1" class="ltx_p" style="width:128.0pt;">Partial initialization for cross-domain personalized FL</span>
</span>
</td>
<td id="S3.T5.1.10.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.5.1.1" class="ltx_p" style="width:28.5pt;">ACC= 95.92</span>
</span>
</td>
<td id="S3.T5.1.10.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.6.1.1" class="ltx_p" style="width:130.9pt;">Reduce performance degradation caused by extreme distribution heterogeneity.</span>
</span>
</td>
<td id="S3.T5.1.10.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T5.1.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.10.9.7.1.1" class="ltx_p" style="width:14.2pt;">No</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Autonomous driving</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Autonomous driving has recently received increasing interest due to the advance of CV, which is in the core of this technology. Vehicles leverage object detectors to analyze images collected by multiple sensors and cameras, analyze their surroundings in real-time and then recognize different objects, including other vehicles, road signs, barriers, pedestrians, etc., which help them in safely navigating the roads.
While a plethora of studies have focused on improving the accuracy by training DL algorithms on centralized large-scale datasets, few of them have addressed the users‚Äô privacy. To that end, using FL in autonomous driving has recently attracted increasing attention. However, numerous challenges have been raised, including data discrepancy across clients and the server, expensive communication, systems heterogeneity and privacy concerns. Typically, privacy issues include internal and external data, such as the faces of pedestrians, vehicles‚Äô positions, etc.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2308.13558/assets/x11.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>FL-based framework for wheel steering angle prediction in autonomous vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>.</figcaption>
</figure>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">The approach to on-device ML using FL and validates it through a case study on wheel steering angle prediction for autonomous driving vehicles is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>. The results show that FL can significantly improve the quality of local edge models and reach the same accuracy level as centralized ML without negative effects. FL can also accelerate model training speed and reduce communication overhead, making it useful for deploying ML/DL components to various embedded systems. Fig. <a href="#S3.F11" title="Figure 11 ‚Ä£ III-F Autonomous driving ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> presents a FL-based framework for wheel steering angle prediction in autonomous vehicles. Nguyen et al.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> propose a communication-efficient FL to detect fatigue driving behaviors, namely FedSup. It helps in progressively optimizing the sharing model with tailored client‚Äìedge‚Äìcloud architecture and reduces communication overhead by a Bayesian convolutional neural network (BCNN) data selection strategy. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>, a federated autonomous driving network (FADNet) solution is introduced for enhancing models‚Äô stability, ensuring convergence, and handling imbalanced data distribution problems where FL models are trained.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">Zhou and their colleagues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> discuss the need for distributed ML techniques to take advantage of the massive interconnected networks and heterogeneous data generated at the network edge in the upcoming 6G environment. A two-layer FL model is proposed that utilizes the distributed end-edge-cloud architecture to achieve more efficient and accurate learning while ensuring data privacy protection and reducing communication overhead. A novel multi-layer heterogeneous model selection and aggregation scheme is designed to better utilize the local and global contexts of individual vehicles and roadside units (RSUs) in 6 G-supported vehicular networks. This context-aware distributed learning mechanism is then applied to address intelligent object detection in modern intelligent transportation systems with autonomous vehicles. Fig. <a href="#S3.F12" title="Figure 12 ‚Ä£ III-F Autonomous driving ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> presents an overview of the two-layer FL model based on convolutional neural network (TFL-CNN) framework.
Table <a href="#S3.T6" title="TABLE VI ‚Ä£ III-F Autonomous driving ‚Ä£ III Applications of FL in CV ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> presents a summary of FL frameworks proposed for autonomous vehicles.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2308.13558/assets/x12.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Overview of the the TFL-CNN framework introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>.</figcaption>
</figure>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Summary of FL frameworks proposed for autonomous vehicles, including the ML model used, dataset, description of the main contribution,best performance (BP), limitations and project link availability (PLA).</figcaption>
<table id="S3.T6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<th id="S3.T6.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.1.1.1" class="ltx_p" style="width:14.2pt;">Work</span>
</span>
</th>
<th id="S3.T6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.2.1.1" class="ltx_p" style="width:42.7pt;">Model</span>
</span>
</th>
<th id="S3.T6.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.3.1.1" class="ltx_p" style="width:65.4pt;">Dataset</span>
</span>
</th>
<th id="S3.T6.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.4.1.1" class="ltx_p" style="width:128.0pt;">Description</span>
</span>
</th>
<th id="S3.T6.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.5.1.1" class="ltx_p" style="width:51.2pt;">BP</span>
</span>
</th>
<th id="S3.T6.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.6.1.1" class="ltx_p" style="width:118.4pt;">Limitations</span>
</span>
</th>
<th id="S3.T6.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.1.1.7.1.1" class="ltx_p" style="width:17.1pt;">PLA</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.1.2.1" class="ltx_tr">
<td id="S3.T6.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.2.1.1" class="ltx_p" style="width:42.7pt;">FADNet</span>
</span>
</td>
<td id="S3.T6.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.3.1.1" class="ltx_p" style="width:65.4pt;">Udacity, Gazebo, Carla</span>
</span>
</td>
<td id="S3.T6.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.4.1.1" class="ltx_p" style="width:128.0pt;">P2P federated framework for training autonomous driving models</span>
</span>
</td>
<td id="S3.T6.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.5.1.1" class="ltx_p" style="width:51.2pt;">RMSE= 0.07</span>
</span>
</td>
<td id="S3.T6.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.6.1.1" class="ltx_p" style="width:118.4pt;">Limited deployment experiment</span>
</span>
</td>
<td id="S3.T6.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.2.1.7.1.1" class="ltx_p" style="width:17.1pt;">Yes<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://github.com/aioz-ai/FADNet</span></span></span></span>
</span>
</td>
</tr>
<tr id="S3.T6.1.3.2" class="ltx_tr">
<td id="S3.T6.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.2.1.1" class="ltx_p" style="width:42.7pt;">BSUM-based solution</span>
</span>
</td>
<td id="S3.T6.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.3.1.1" class="ltx_p" style="width:65.4pt;">MNIST</span>
</span>
</td>
<td id="S3.T6.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.4.1.1" class="ltx_p" style="width:128.0pt;">Dispersed FL framework to improve the robustness, privacy-awareness and communication constraints</span>
</span>
</td>
<td id="S3.T6.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.5.1.1" class="ltx_p" style="width:51.2pt;">accuracy = 99%</span>
</span>
</td>
<td id="S3.T6.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.6.1.1" class="ltx_p" style="width:118.4pt;">NP-hard and non-convex formulation of the problem</span>
</span>
</td>
<td id="S3.T6.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.3.2.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.4.3" class="ltx_tr">
<td id="S3.T6.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.2.1.1" class="ltx_p" style="width:42.7pt;">CNN</span>
</span>
</td>
<td id="S3.T6.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.3.1.1" class="ltx_p" style="width:65.4pt;">Private dataset</span>
</span>
</td>
<td id="S3.T6.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.4.1.1" class="ltx_p" style="width:128.0pt;">Addresses the case where vehicles and servers are considered honest but curious through blockchain</span>
</span>
</td>
<td id="S3.T6.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.5.1.1" class="ltx_p" style="width:51.2pt;">accuracy= 92.5%</span>
</span>
</td>
<td id="S3.T6.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.6.1.1" class="ltx_p" style="width:118.4pt;">Require long training iterations for small images</span>
</span>
</td>
<td id="S3.T6.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.4.3.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.5.4" class="ltx_tr">
<td id="S3.T6.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.2.1.1" class="ltx_p" style="width:42.7pt;">CNN</span>
</span>
</td>
<td id="S3.T6.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.3.1.1" class="ltx_p" style="width:65.4pt;">MNIST, FMNIST</span>
</span>
</td>
<td id="S3.T6.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.4.1.1" class="ltx_p" style="width:128.0pt;">A reactive method for the allocation of wireless resources, dynamically occurring at each round</span>
</span>
</td>
<td id="S3.T6.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.5.1.1" class="ltx_p" style="width:51.2pt;">accuracy = 88 %</span>
</span>
</td>
<td id="S3.T6.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.6.1.1" class="ltx_p" style="width:118.4pt;">The impact of resource allocation methods is diminished when
strengthening the role of the proximal term</span>
</span>
</td>
<td id="S3.T6.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.5.4.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.6.5" class="ltx_tr">
<td id="S3.T6.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.2.1.1" class="ltx_p" style="width:42.7pt;">CNN</span>
</span>
</td>
<td id="S3.T6.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.3.1.1" class="ltx_p" style="width:65.4pt;">SullyChen</span>
</span>
</td>
<td id="S3.T6.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.4.1.1" class="ltx_p" style="width:128.0pt;">End-to-end ML model for handling real-time generated data</span>
</span>
</td>
<td id="S3.T6.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.5.1.1" class="ltx_p" style="width:51.2pt;">RMSE= 9.2</span>
</span>
</td>
<td id="S3.T6.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.6.1.1" class="ltx_p" style="width:118.4pt;">Synchroneous aggregation</span>
</span>
</td>
<td id="S3.T6.1.6.5.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.6.5.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.7.6" class="ltx_tr">
<td id="S3.T6.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.2.1.1" class="ltx_p" style="width:42.7pt;">U-NET, CNN</span>
</span>
</td>
<td id="S3.T6.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.3.1.1" class="ltx_p" style="width:65.4pt;">Lyft Level 5 AV</span>
</span>
</td>
<td id="S3.T6.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.4.1.1" class="ltx_p" style="width:128.0pt;">Feasibility study of using FL for vehicular applications</span>
</span>
</td>
<td id="S3.T6.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.5.1.1" class="ltx_p" style="width:51.2pt;">accuracy = 95%</span>
</span>
</td>
<td id="S3.T6.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.6.1.1" class="ltx_p" style="width:118.4pt;">Evaluated only the accuracy</span>
</span>
</td>
<td id="S3.T6.1.7.6.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.7.6.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.8.7" class="ltx_tr">
<td id="S3.T6.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.2.1.1" class="ltx_p" style="width:42.7pt;">TFL-CNN</span>
</span>
</td>
<td id="S3.T6.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.3.1.1" class="ltx_p" style="width:65.4pt;">BelgiumTSC</span>
</span>
</td>
<td id="S3.T6.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.4.1.1" class="ltx_p" style="width:128.0pt;">An improved model selection and aggregation algorithm</span>
</span>
</td>
<td id="S3.T6.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.5.1.1" class="ltx_p" style="width:51.2pt;">F1-score = 94%</span>
</span>
</td>
<td id="S3.T6.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.6.1.1" class="ltx_p" style="width:118.4pt;">Limited evaluation of the framework</span>
</span>
</td>
<td id="S3.T6.1.8.7.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.8.7.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.9.8" class="ltx_tr">
<td id="S3.T6.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.2.1.1" class="ltx_p" style="width:42.7pt;">YOLO3</span>
</span>
</td>
<td id="S3.T6.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.3.1.1" class="ltx_p" style="width:65.4pt;">KITTI</span>
</span>
</td>
<td id="S3.T6.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.4.1.1" class="ltx_p" style="width:128.0pt;">Evaluation of real-time object detection in real traffic environments</span>
</span>
</td>
<td id="S3.T6.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.5.1.1" class="ltx_p" style="width:51.2pt;">MAP = 68.5%</span>
</span>
</td>
<td id="S3.T6.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.6.1.1" class="ltx_p" style="width:118.4pt;">Sensitive to changes in labels across clients</span>
</span>
</td>
<td id="S3.T6.1.9.8.7" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.9.8.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
<tr id="S3.T6.1.10.9" class="ltx_tr">
<td id="S3.T6.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.1.1.1" class="ltx_p" style="width:14.2pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite></span>
</span>
</td>
<td id="S3.T6.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.2.1.1" class="ltx_p" style="width:42.7pt;">SNNs</span>
</span>
</td>
<td id="S3.T6.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.3.1.1" class="ltx_p" style="width:65.4pt;">BelgiumTSC</span>
</span>
</td>
<td id="S3.T6.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.4.1.1" class="ltx_p" style="width:128.0pt;">Leverages spike NN to optimize resources required for the training</span>
</span>
</td>
<td id="S3.T6.1.10.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.5.1.1" class="ltx_p" style="width:51.2pt;">accuracy = 95%</span>
</span>
</td>
<td id="S3.T6.1.10.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.6.1.1" class="ltx_p" style="width:118.4pt;">Suffers from security issue</span>
</span>
</td>
<td id="S3.T6.1.10.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S3.T6.1.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T6.1.10.9.7.1.1" class="ltx_p" style="width:17.1pt;">No</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Open issues</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The advances in CV algorithms have increased the number and variety of tasks that range from the detection and tracking of specific objects in static images or controlled image streams to public surveillance and the detection of normal or abnormal behaviors and conditions in the wild, with applications from agricultural crop monitoring to law enforcement and medical diagnostics. The information captured in surveillance cameras‚Äô footage may contain sensitive and private data that must be protected (e.g., the presence of individuals at a place), the CV models trained to detect abnormal behaviors in public may suffer from race, ethnicity or gender bias, and scene perception software in self-driving cars may suffer from the ability to adapt to new environments or conditions (e.g. fog, mist or haze).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">FL is a new method for protecting privacy when developing a DNN model, that uses data from various clients and trains a common model for all clients. This is achieved by fusing distributed ML, encryption and security, and introducing incentive mechanisms based on game theory and economic theory. FL could therefore serve as the cornerstone of next-generation ML that meets societal and technological requirements for ethical AI development and implementation.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Despite their many advantages, FL solutions also have several challenges to address. First of all, since FL is a distributed learning technique it is important to guarantee the efficient communication between the federated network nodes so that they can communicate the learned model parameters. In addition to this, the models may be trained on nodes of different hardware architecture and capabilities (e.g. IoT devices, smartphones, etc.) which constitute a heterogeneous environment for FL. Consequently, it is important to establish the mechanisms that manage heterogeneous nodes in the same network, considering their restrictions and capabilities. The use of central (cloud-based) nodes that carry part of the training process, either using additional training data or by training parts of the learning model (e.g. some layers of the DNN as in the split learning paradigm) can be beneficial for the overall performance. A third issue that must be considered is the statistical heterogeneity of data that arrives in each node. Learning using non-IID data is still an open challenge for FL algorithms, and various alternatives are still under evaluation. Finally, privacy and robustness concerns are still present and methods that preserve privacy and at the same time guarantee the resulting model robustness to any kind of breaches or attacks must be properly designed and developed.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Communication overhead</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">A crucial component of a FL environment is communication between the client nodes and the central server. The ML model is downloaded, uploaded, and trained over the course of several communication rounds. Although transferring models instead of training datasets is significantly more efficient, these communication rounds may be delayed when the device has limited bandwidth, energy, or power. The communication overhead also increases when multiple client devices participate in each communication round thus leading to a bottleneck. Further delays may occur from the need to synchronise the receipt of models from all clients, including those with low bandwidth or unstable connections. An additional burden to the client synchronization overhead can be caused by the non-identical distribution of training data to the nodes, which may delay the training of some models, and consequently the model update process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>.
This delay in cybersecurity can be considered an intrusion if it surpasses a predetermined threshold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In order to mitigate the communication overhead and establish communication-efficient federation strategies, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> have proposed the compression of transferred data. In a different approach, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> have focused on identifying irrelevant models and precluding them from the aggregation, thus significantly reducing the communication cost. This is achieved by sharing the global tendency of model updating between the central server and all the client nodes and asking each client to align its update with this tendency before communicating the model.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In the case of FLchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, the model communication cost between the clients and a central server, which is common in FL, is replaced by the cost of sharing the models with the blockchain ledger. For this reason, it is important to consider the time needed for this process, including local training, model transmission, consensus, and block mining.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Heterogeneity of client nodes</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">One of the major challenges in FL is the heterogeneity of data on different clients, which can hinder effective training. To address this issue, client selection strategies are often used in an attempt to improve the convergence rate of the FL process. While active client selection strategies have been proposed in recent studies, they do not take into account the loss correlations between clients and only offer limited improvement compared to a uniform selection strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>. To overcome this limitation, FedCor, an FL framework that utilizes a correlation-based client selection strategy to improve the convergence rate of FL is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>. The loss correlations between clients is modeled using a Gaussian process (GP) and use this model to select clients in a way that significantly reduces the expected global loss in each round. Moreover, an efficient GP training method is developed with low communication overhead for use in the FL scenario by leveraging the covariance stationarity. The experimental results show that FedCor can improve convergence rates by 34% to 99% on FMNIST and 26% to 51% on CIFAR-10 compared to the SOTA method.
Besides, FL still suffers from significant challenges such as the lack of convergence and the possibility of catastrophic forgetting.
However, it is demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> that the self-attention-based architectures (e.g., Transformers) have shown to be more resistant to changes in data distribution and therefore can improve the effectiveness of FL on heterogeneous devices.
The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> propose an alternative approach, called FedAlign, which aims to address data heterogeneity by focusing on local learning rather than proximal restriction. They conducted a study using second-order indicators to evaluate the effectiveness of different algorithms in FL and found that standard regularization methods performed well in mitigating the effects of data heterogeneity. FedAlign was found to be a simple and effective method for overcoming data heterogeneity, with competitive accuracy compared to SOTA FL methods and minimal computational and memory overhead.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">On the other hand, when FL is limited only to client nodes that share the same model architecture then the FedAvg algorithm and its alternatives (e.g. FedSDG, FedProx, etc.) can be applied to merge the locally trained models in each iteration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>. However, such approaches assume that the underlying nodes share similar hardware architecture, specifications and processing capabilities in general, which is not always the case in FL settings. For example, when smartphones, monitoring cameras and field programmable gate arrays (FPGA) attached cameras are orchestrated in an FL setting, the models have heterogeneous architectures and model parameter sharing is infeasible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>. The barriers of conventional FL are removed when the models share their models instead of their parameters or updates. Algorithms such as the federated model distillation (FedMD) propose a model agnostic federation solution, which sends the predictions of the local models to the central node instead of the models themselves <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Another impact of node heterogeneity applies to the overall performance of the FL process since the federation of heterogeneous nodes with varying training data structure and size and varying processing capabilities usually requires in each round all the local nodes to train their models before proceeding to the next iteration. As a result, slow nodes, with low usability data may degrade the time performance of the federation. The work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite> proposes a reinforcement learning-based central server, which gradually weights the clients based on the quality of their models and their overall response in an attempt to establish a group of clients that are used to achieve almost optimal performance.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Non-IID data</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Although FL offers a promising method to privacy protection, there are significant difficulties when FL is used in the real world as opposed to centralized learning. Numerous studies have shown that the accuracy of FL on non-IID or heterogeneous data would inevitably deteriorate, mainly because of the divergence in the weights of local models that result from non-IID data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>. Either the FL approach is horizontal (i.e. aggregating the local models‚Äô weights on a central server) or vertical (i.e. aggregating the model outputs in the guest client to calculate the loss function) the non-IID data can take various forms (e.g. attribute, label or temporal skew) that can affect the overall performance, and mitigation measures must be taken to avoid it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>. These include data sharing and augmentation, and the fine-tuning of local models using a combination of local and global information. Wang wt al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> propose FAVOR, a reinforcement-based method (deep Q-learning) for choosing the clients that contribute models to the aggregation phase in each round. The proposed method reduces the non-IID data bias and the communication overhead. Since the number of dimensions of the state space equals the number of model weights times the number of client nodes, a dimensionality reduction technique, such as PCA, is applied in order to compress the state space. Another group of approaches tries to balance the bias of non-IID data by clustering the local updates using a hierarchical algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite>. Such approaches result in multiple models that are trained independently and in parallel, leading to a faster and better convergence of each group.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Device compatibility and network issues</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">FL relies on the participation of multiple devices, which may have different hardware and software configurations. This can make it difficult to ensure that the model can be trained on all devices, and may require additional efforts to optimize the model for different device types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>.
FL requires a stable and reliable network connection in order to train the model and exchange model updates between participants and the central server. If the network connection is unreliable or has low bandwidth, this can negatively impact the training process and the performance of the model.
Explicitly, if the parties involved in FL are located in different geographical regions, the time it takes for model updates to be transmitted over the network can vary significantly, leading to delays in the training process.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Human bias</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">FL relies on the participation of multiple parties, which may have different biases or perspectives that can influence the model‚Äôs training and performance. This can be particularly challenging in the context of CV, as the model may be trained on data with biased or inaccurate annotations. It is important to carefully consider and address these issues in order to ensure that the model is fair and unbiased <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Privacy and robustness to attacks</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">The resilience and data privacy of existing FL protocol designs have been shown to be compromised by adversaries both inside and outside the system. The sharing of gradients during training can reveal private information and cause data leakage to the central server or a third party. Similarly, malicious users can try to affect the global model by introducing poisoned local data or gradients (model poisoning) in an attempt to destroy its convergence (i.e. Byzantine attack) or to implant a trigger in the model that constantly predicts adversarial classes, without losing performance on the main task. These two times of attacks either aim to breach user privacy (e.g. infer class representatives, infer class membership or infer user properties) or poison the model in order to control its behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>.
Homomorphic encryption, secure joint computation from multiple parties, and differential privacy are some of the means for mitigating privacy breaches. Respectively, the defenses against poisoning attacks focus on the detection of malicious users based on the anomalies detected in their models (e.g. different distributions of features than the rest of the users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>) focusing on increasing the robustness against Byzantine attacks. They also examine the resulting models to detect whether they are compromised or not, using a combination of backdoor-enabled and clean inputs and examining the behavior of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">Blockchain can be beneficial for preserving privacy in FL settings and can help avoid various attack types, from single point of failure and membership inference to Byzantine, label flipping, and data poisoning ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>. The FL blockchain, or FLchain for short, paradigm can transform mobile-edge computing to a secure and privacy-preserving system, once the proper solutions are found for allocating resources, providing incentives to the client nodes, and protecting the security and privacy of data at an optimal communication cost.
In their proposed architecture, the mobile-edge computing (MEC) servers can act either as learning clients or as miners to establish blockchain consensus and the mobile devices associated with each server focus only on the learning tasks. The mobile devices transmit their models to the MEC server as transaction. Each MEC server stores the local models it collects to the blockchain after verifying them with other MEC servers. The aggregation node or any other local node is then able to retrieve the stored and verified models from the blockchain, aggregate them and use the resulting model in the next iteration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Watermarking is another means of model security preserving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS7.5.1.1" class="ltx_text">IV-G</span> </span><span id="S4.SS7.6.2" class="ltx_text ltx_font_italic">Replication of research results</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">Despite the fact that the performance of FL models requires further improvement compared to that obtained with centralized training, replicating FL research results and conducting fair comparisons are still challenging. This is mainly due to the lack of exploration in different tasks with a unified FL framework. To that end, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> develop an FL benchmarking and library platform for CV applications, called FedCV. This helps in bridging gaps between SOTA algorithms and facilitating the development of FL solutions.
Additionally, three main taks of CV, including image object detection, image segmentation and image classification can be evaluated on this toolkit, as shown in Fig. <a href="#S4.F13" title="Figure 13 ‚Ä£ IV-G Replication of research results ‚Ä£ IV Open issues ‚Ä£ Federated Learning for Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. Moreover, numerous FL algorithms, models, and non-IID benchmarking datasets have been uploaded and still the toolkit can be updated.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">Besides, enhancing FL-based systems‚Äô efficiency is a delicate task due to the per-client memory cost and large number of parameters. Thus, using the FedCV framework, which is an efficient and flexible distributed training toolkit that has easy-to-use APIs, along with benchmarks, and different evaluation settings can be useful for the FL research community to conduct advanced research CV studies.</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2308.13558/assets/x13.png" id="S4.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="412" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Overview of FedCV platform architecture design proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Challenges of using FL in CV</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">FL is a promising approach for solving privacy and data distribution challenges in CV tasks such as object detection, autonomous vehicles, etc. However, there are several challenges associated with the use of FL in these applications. For instance, FL assumes that the data distribution across clients is similar. However, in CV tasks, the data may be heterogeneous, and the models need to be robust to handle variations in lighting, viewpoint, and occlusions. FL relies on communication between clients and the server. For CV tasks, such as object detection, the models can be large, and the communication costs can be high. This can result in slower training times and higher energy consumption.
Moreover, CV tasks require labeled data for training the models while FL requires each client to have labeled data, which can be challenging in scenarios where labeling data is expensive or time-consuming.
Additionally, FL requires clients to share their data with the server, which can raise privacy and security concerns. Malicious clients can manipulate the data or models, compromising the integrity of the system.
Lastly, FL requires aggregating the models from multiple clients to produce a global model. In CV tasks, this can be challenging due to differences in the quality of the models and the data distribution across clients.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">On the other hand, FedCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> is a benchmarking framework for the evaluation of FL in popular CV tasks such as image classification and segmentation and object detection. It comprises non-IID datasets and various models and algorithms for experimentation. The experiments validate the aforementioned open challenges for FL in CV tasks: i) the degradation of model accuracy when non-IID data are used, ii) the complexity of optimising the training in the FL setting, iii) the huge number of parameters of NN models used in CV that affects the FL performance. The same benchmark can be employed to validate all the remaining issues discussed in this section, such as the node and data heterogeneity, the need for robustness and privacy etc.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">FedVision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> is an online platform for developing object detection solutions using FL and a three step workflow that comprises: i) image annotation, ii) horizontal FL model training and iii) model update. Since the platform is generic, it allows users to configure the learning parameters, schedule the communication between the server and clients, allocate tasks and monitor the utilization of resources. The main challenges mentioned in this section for FL also apply in the case of FedVision, which however chooses specific strategies to tackle them. For example, it uses model compression to reduce communication overhead, cloud object storage to store huge amounts of data (model parameters) in the server, and an one-stage approach, based on YOLOv3, to perform end-to-end training of the model that identifies the bounding box and the class of the object. However, the real challenges for FL approaches in CV come with the application in real-world images from surveillance cameras, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>. The large-scale of data collected and the requirement for almost real-time inference raises more design challenges for FL experts.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future directions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The future of FL is very promising, but also challenging for researchers, with the main directions being as follows:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Deployment over heterogeneous environments:</span> Smartphones are becoming the most popular edge devices for ML applications since they allow users to perform a large variety of tasks from face detection to voice recognition. In this direction, FL can be used to support such tasks without exposing private information. On the other side, IoT networks combine wearables, mobile and stable sensors, and smartphones in order to establish smart environments for the end users. All these constitute a diverse and heterogeneous environment in which models of varying complexity have to be communicated in the place of data in order to implement federation while protecting data privacy.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Efficient communication:</span> When creating techniques for federated networks, communication is a crucial bottleneck to take into account. This is due to the fact that federated networks may contain a sizable number of devices (such as millions of smartphones), and communication throughout the network may be much slower than local computing. Reducing the overall number of communication rounds or the quantity of communicated messages at each round are the two main ways to further cut communication. The communication can be more efficient using: i) Local updating techniques that allow to cut down the overall communication rounds, ii) model compression techniques including quantization, subsampling, and sparsification can be used to reduce the size of messages conveyed during each update round, iii) when the connection with the server becomes a bottleneck, decentralized topologies provide an alternative, particularly when working in networks with low bandwidth or excessive latency. Iterative optimization algorithms are parallelized using asynchronous communication. An appealing strategy for reducing stragglers in heterogeneous contexts is the use of asynchronous systems. Moreover, using 5G/6G networks will offer significantly higher speeds and lower latency compared to previous generations. This can enable faster and more efficient FL in CV applications.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Dispersed FL:</span> Concerns about FL‚Äôs robustness exist since it could cease to function if the aggregate server fails (e.g., due to a malicious attack or physical defect). Dispersed FL can be used as a more robust alternative to FL, with groups of nodes with a lot of computing power to collaborate in more sub-global iterations to increase their performance and consequently support the overall performance of the federation.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">New organizational models:</span> The term ‚Äùdevices‚Äù can refer to entire companies or institutions in the context of FL. For applications in predictive healthcare, hospitals are an example of a company with a lot of patient data. Hospitals must adhere to strong privacy laws and may have to maintain local data due to ethical, administrative, or regulatory requirements. Such applications can benefit from FL since they can ease network load and enable private learning amongst many devices and organizations.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p"><span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Large language models and generative chatbots:</span> Advanced language models, such as ChatGPT, can assist in various ways to improve the use of FL in CV by (i) offering explanations, answering questions, and providing tutorials related to FL and CV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>. By helping researchers and practitioners understand the concepts better, it can drive wider adoption and more informed application of FL in CV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>; (ii) simplifying complex algorithms, provide pseudocode, and suggest optimization strategies; assisting with code debugging or suggesting modifications to FL algorithms, thereby improving their efficiency or performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>; (iii) providing insights into the ethical implications and considerations when applying FL in CV, especially in terms of data privacy and usage; (iv) simulating client-server conversations, allowing developers to anticipate challenges and refine their systems; and (v) offering advice on best practices for integration, be it with databases, cloud services, or edge devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Federated Learning (FL) has emerged as a revolutionary paradigm in the realm of Computer Vision (CV), fostering collaborative machine learning without compromising data privacy. This review navigated through the intricate alleys of FL, from its foundational concepts to the myriad applications in CV. The aggregation approaches such as averaging, Progressive Fourier, and FedGKT accentuate FL‚Äôs versatility. Moreover, the inclusion of privacy technologies like the Secure MPC model, differential privacy, and homomorphic encryption underscores its commitment to safeguarding data.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">It is remarkable to note the vast landscape of CV applications benefitting from FL, ranging from object and face detection to innovative domains like healthcare, autonomous driving, and smart environment surveillance. Yet, like any evolving technology, FL in CV is not devoid of challenges. Issues like communication overhead, device heterogeneity, and the conundrums posed by non-IID data offer fertile grounds for future research.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">While current advancements set a promising trajectory, the open issues highlight areas ripe for exploration and innovation. The challenges also underscore the importance of collaboration between researchers, practitioners, and industries to make FL more efficient, inclusive, and robust for CV.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">As we stand on the cusp of a technological evolution, FL offers a beacon of hope, combining the best of collaborative learning and data privacy. The journey ahead is replete with opportunities and challenges, making it an exhilarating era for researchers and enthusiasts alike.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, K.¬†Ghanem, A.¬†Alsalemi, F.¬†Bensaali, and A.¬†Amira, ‚ÄúArtificial
intelligence based anomaly detection of energy consumption in buildings: A
review, current trends and new perspectives,‚Äù <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Applied Energy</em>, vol.
287, p. 116601, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.¬†Sayed, Y.¬†Himeur, F.¬†Bensaali, and A.¬†Amira, ‚ÄúArtificial intelligence with
iot for energy efficiency in buildings,‚Äù <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Emerging Real-World
Applications of Internet of Things</em>, pp. 233‚Äì252, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.¬†Ng, X.¬†Lan, M.¬†M.-S. Yao, W.¬†P. Chan, and M.¬†Feng, ‚ÄúFederated learning: a
collaborative effort to achieve better medical imaging models for individual
sites that have small labelled datasets,‚Äù <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Quantitative Imaging in
Medicine and Surgery</em>, vol.¬†11, no.¬†2, p. 852, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A.¬†Al-Kababji, F.¬†Bensaali, S.¬†P. Dakua, and Y.¬†Himeur, ‚ÄúAutomated liver
tissues delineation techniques: A systematic survey on machine learning
current trends and future orientations,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Engineering Applications of
Artificial Intelligence</em>, vol. 117, p. 105532, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.¬†N. Sayed, Y.¬†Himeur, and F.¬†Bensaali, ‚ÄúFrom time-series to 2d images for
building occupancy prediction using deep transfer learning,‚Äù
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Engineering Applications of Artificial Intelligence</em>, vol. 119, p.
105786, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y.¬†Teng, J.¬†Zhang, and T.¬†Sun, ‚ÄúData-driven decision-making model based on
artificial intelligence in higher education system of colleges and
universities,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Expert Systems</em>, vol.¬†40, no.¬†4, p. e12820, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, S.¬†Al-Maadeed, N.¬†Almaadeed, K.¬†Abualsaud, A.¬†Mohamed, T.¬†Khattab,
and O.¬†Elharrouss, ‚ÄúDeep visual social distancing monitoring to combat
covid-19: A comprehensive survey,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Sustainable cities and society</em>,
vol.¬†85, p. 104064, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.¬†Elmir, Y.¬†Himeur, and A.¬†Amira, ‚ÄúEcg classification using deep cnn and
gramian angular field,‚Äù <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.02395</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.¬†Chouchane, A.¬†Ouamane, Y.¬†Himeur, W.¬†Mansoor, S.¬†Atalla, A.¬†Benzaibak, and
C.¬†Boudellal, ‚ÄúImproving cnn-based person re-identification using score
normalization,‚Äù <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.00397</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H.¬†Kheddar, Y.¬†Himeur, S.¬†Al-Maadeed, A.¬†Amira, and F.¬†Bensaali, ‚ÄúDeep
transfer learning for automatic speech recognition: Towards better
generalization,‚Äù <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14535</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, S.¬†Al-Maadeed, I.¬†Varlamis, N.¬†Al-Maadeed, K.¬†Abualsaud, and
A.¬†Mohamed, ‚ÄúFace mask detection in smart cities using deep and transfer
learning: lessons learned from the covid-19 pandemic,‚Äù <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Systems</em>,
vol.¬†11, no.¬†2, p. 107, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
F.¬†Esposito and D.¬†Malerba, ‚ÄúMachine learning in computer vision,‚Äù
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Applied Artificial Intelligence</em>, vol.¬†15, no.¬†8, pp. 693‚Äì705, 2001.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.¬†Copiaco, Y.¬†Himeur, A.¬†Amira, W.¬†Mansoor, F.¬†Fadli, S.¬†Atalla, and S.¬†S.
Sohail, ‚ÄúAn innovative deep anomaly detection of building energy consumption
using energy time-series images,‚Äù <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Engineering Applications of
Artificial Intelligence</em>, vol. 119, p. 105775, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S.¬†Alyamkin, M.¬†Ardi, A.¬†C. Berg, A.¬†Brighton, B.¬†Chen, Y.¬†Chen, H.-P. Cheng,
Z.¬†Fan, C.¬†Feng, B.¬†Fu <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúLow-power computer vision: Status,
challenges, and opportunities,‚Äù <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">IEEE Journal on Emerging and Selected
Topics in Circuits and Systems</em>, vol.¬†9, no.¬†2, pp. 411‚Äì421, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Z.¬†Tong and G.¬†Tanaka, ‚ÄúReservoir computing with untrained convolutional
neural networks for image recognition,‚Äù in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2018 24th International
Conference on Pattern Recognition (ICPR)</em>.¬†¬†¬†IEEE, 2018, pp. 1289‚Äì1294.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.¬†Mohamed and G.¬†Rubino, ‚ÄúA study of real-time packet video quality using
random neural networks,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on circuits and systems for
video technology</em>, vol.¬†12, no.¬†12, pp. 1071‚Äì1083, 2002.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, S.¬†S. Sohail, F.¬†Bensaali, A.¬†Amira, and M.¬†Alazab, ‚ÄúLatest trends
of security and privacy in recommender systems: a comprehensive review and
future perspectives,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em>, vol. 118, p. 102746,
2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.¬†Quach, P.¬†Thaichon, K.¬†D. Martin, S.¬†Weaven, and R.¬†W. Palmatier, ‚ÄúDigital
technologies: tensions in privacy and data,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of the Academy of
Marketing Science</em>, vol.¬†50, no.¬†6, pp. 1299‚Äì1323, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y.¬†Himeur and K.¬†A. Sadi, ‚ÄúRobust video copy detection based on ring
decomposition based binarized statistical image features and invariant color
descriptor (rbsif-icd),‚Äù <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, vol.¬†77,
pp. 17‚Äâ309‚Äì17‚Äâ331, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D.¬†Patrikar and M.¬†Parate, ‚ÄúAnomaly detection using edge computing in video
surveillance system: review,‚Äù <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Int J Multimed Info Retr</em>, vol.¬†11, pp.
85‚Äì110, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A.¬†Xiang, ‚ÄúBeing‚Äôseen‚Äôvs.‚Äômis-seen‚Äô: Tensions between privacy and fairness in
computer vision,‚Äù <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Harvard Journal of Law &amp; Technology, Forthcoming</em>,
2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.¬†Alsalemi, Y.¬†Himeur, F.¬†Bensaali, and A.¬†Amira, ‚ÄúAn innovative edge-based
internet of energy solution for promoting energy saving in buildings,‚Äù
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Sustainable Cities and Society</em>, vol.¬†78, p. 103571, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A.¬†Sayed, Y.¬†Himeur, A.¬†Alsalemi, F.¬†Bensaali, and A.¬†Amira, ‚ÄúIntelligent
edge-based recommender system for internet of energy applications,‚Äù
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Systems Journal</em>, vol.¬†16, no.¬†3, pp. 5001‚Äì5010, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Q.¬†Yang, Y.¬†Liu, Y.¬†Cheng, Y.¬†Kang, T.¬†Chen, and H.¬†Yu, ‚ÄúFederated learning,‚Äù
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Synthesis Lectures on Artificial Intelligence and Machine Learning</em>,
vol.¬†13, no.¬†3, pp. 1‚Äì207, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
O.¬†Gupta and R.¬†Raskar, ‚ÄúDistributed learning of deep neural network over
multiple agents,‚Äù <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of Network and Computer Applications</em>, vol.
116, pp. 1‚Äì8, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A.¬†R. Khan, A.¬†Zoha, L.¬†Mohjazi, H.¬†Sajid, Q.¬†Abbasi, and M.¬†A. Imran, ‚ÄúWhen
federated learning meets vision: An outlook on opportunities and
challenges,‚Äù in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">EAI International Conference on Body Area
Networks</em>.¬†¬†¬†Springer, 2021, pp.
308‚Äì319.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K.¬†Giorgas and I.¬†Varlamis, ‚ÄúOnline federated learning with imbalanced class
distribution,‚Äù in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">24th Pan-Hellenic Conference on Informatics</em>, 2020,
pp. 91‚Äì95.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S.¬†Abuadbba, K.¬†Kim, M.¬†Kim, C.¬†Thapa, S.¬†A. Camtepe, Y.¬†Gao, H.¬†Kim, and
S.¬†Nepal, ‚ÄúCan we use split learning on 1d cnn models for privacy preserving
training?‚Äù in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th ACM Asia Conference on Computer
and Communications Security</em>, 2020, pp. 305‚Äì318.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M.¬†S. Jere, T.¬†Farnan, and F.¬†Koushanfar, ‚ÄúA taxonomy of attacks on federated
learning,‚Äù <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Security &amp; Privacy</em>, vol.¬†19, no.¬†2, pp. 20‚Äì28,
2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.¬†Zhang, Y.¬†Chen, and H.¬†Li, ‚ÄúPrivacy leakage of adversarial training models
in federated learning systems,‚Äù in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 108‚Äì114.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, A.¬†Sayed, A.¬†Alsalemi, F.¬†Bensaali, A.¬†Amira, I.¬†Varlamis,
M.¬†Eirinaki, C.¬†Sardianos, and G.¬†Dimitrakopoulos, ‚ÄúBlockchain-based
recommender systems: Applications, challenges and future opportunities,‚Äù
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computer Science Review</em>, vol.¬†43, p. 100439, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y.¬†Lu, X.¬†Huang, Y.¬†Dai, S.¬†Maharjan, and Y.¬†Zhang, ‚ÄúBlockchain and federated
learning for privacy-preserved data sharing in industrial iot,‚Äù <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Industrial Informatics</em>, vol.¬†16, no.¬†6, pp. 4177‚Äì4186,
2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
O.¬†A. Wahab, A.¬†Mourad, H.¬†Otrok, and T.¬†Taleb, ‚ÄúFederated machine learning:
Survey, multi-level classification, desirable criteria and future directions
in communication and networking systems,‚Äù <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys
&amp; Tutorials</em>, vol.¬†23, no.¬†2, pp. 1342‚Äì1397, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Q.¬†Li, Z.¬†Wen, Z.¬†Wu, S.¬†Hu, N.¬†Wang, Y.¬†Li, X.¬†Liu, and B.¬†He, ‚ÄúA survey on
federated learning systems: vision, hype and reality for data privacy and
protection,‚Äù <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>,
2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M.¬†Aledhari, R.¬†Razzak, R.¬†M. Parizi, and F.¬†Saeed, ‚ÄúFederated learning: A
survey on enabling technologies, protocols, and applications,‚Äù <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol.¬†8, pp. 140‚Äâ699‚Äì140‚Äâ725, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D.¬†C. Nguyen, M.¬†Ding, P.¬†N. Pathirana, A.¬†Seneviratne, J.¬†Li, and H.¬†V. Poor,
‚ÄúFederated learning for internet of things: A comprehensive survey,‚Äù
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, vol.¬†23, no.¬†3, pp.
1622‚Äì1658, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
S.¬†AbdulRahman, H.¬†Tout, H.¬†Ould-Slimane, A.¬†Mourad, C.¬†Talhi, and M.¬†Guizani,
‚ÄúA survey on federated learning: The journey from centralized to distributed
on-site learning and beyond,‚Äù <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>,
vol.¬†8, no.¬†7, pp. 5476‚Äì5497, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y.¬†Cheng, Y.¬†Liu, T.¬†Chen, and Q.¬†Yang, ‚ÄúFederatfed learning for
privacy-preserving ai,‚Äù <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol.¬†63, no.¬†12,
pp. 33‚Äì36, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S.¬†Yang, B.¬†Ren, X.¬†Zhou, and L.¬†Liu, ‚ÄúParallel distributed logistic
regression for vertical federated learning without third-party coordinator,‚Äù
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.09824</em>, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S.¬†Sharma, C.¬†Xing, Y.¬†Liu, and Y.¬†Kang, ‚ÄúSecure and efficient federated
transfer learning,‚Äù in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data
(Big Data)</em>.¬†¬†¬†IEEE, 2019, pp.
2569‚Äì2576.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
G.¬†Bjelobaba, A.¬†Saviƒá, T.¬†To≈°iƒá, I.¬†Stefanoviƒá, and
B.¬†Kociƒá, ‚ÄúCollaborative learning supported by blockchain technology as
a model for improving the educational process,‚Äù <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Sustainability</em>,
vol.¬†15, no.¬†6, p. 4780, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y.¬†Liu, X.¬†Zhang, Y.¬†Kang, L.¬†Li, T.¬†Chen, M.¬†Hong, and Q.¬†Yang, ‚ÄúFedbcd: A
communication-efficient collaborative learning framework for distributed
features,‚Äù <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, vol.¬†70, pp.
4277‚Äì4290, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
E.¬†Gabrielli, G.¬†Pica, and G.¬†Tolomei, ‚ÄúA survey on decentralized federated
learning,‚Äù <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.04604</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J.¬†Koneƒçn·ª≥, H.¬†B. McMahan, D.¬†Ramage, and P.¬†Richt√°rik,
‚ÄúFederated optimization: Distributed machine learning for on-device
intelligence,‚Äù <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
V.¬†Smith, C.-K. Chiang, M.¬†Sanjabi, and A.¬†S. Talwalkar, ‚ÄúFederated multi-task
learning,‚Äù <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
vol.¬†30, 2017.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A.¬†Linardos, K.¬†Kushibar, S.¬†Walsh, P.¬†Gkontra, and K.¬†Lekadir, ‚ÄúFederated
learning for multi-center imaging diagnostics: A study in cardiovascular
disease,‚Äù <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.03901</em>, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Z.¬†Chen, M.¬†Zhu, C.¬†Yang, and Y.¬†Yuan, ‚ÄúPersonalized retrogress-resilient
framework for real-world medical federated learning,‚Äù in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">International
Conference on Medical Image Computing and Computer-Assisted
Intervention</em>.¬†¬†¬†Springer, 2021, pp.
347‚Äì356.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
K.¬†Doshi and Y.¬†Yilmaz, ‚ÄúFederated learning-based driver activity recognition
for edge devices,‚Äù in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2022, pp. 3338‚Äì3346.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
H.¬†Zhu, ‚ÄúOn the relationship between (secure) multi-party computation and
(secure) federated learning,‚Äù <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.02609</em>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
D.¬†Byrd and A.¬†Polychroniadou, ‚ÄúDifferentially private secure multi-party
computation for federated learning in financial applications,‚Äù in
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First ACM International Conference on AI in
Finance</em>, 2020, pp. 1‚Äì9.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
R.¬†Kanagavelu, Z.¬†Li, J.¬†Samsudin, Y.¬†Yang, F.¬†Yang, R.¬†S.¬†M. Goh, M.¬†Cheah,
P.¬†Wiwatphonthana, K.¬†Akkarajitsakul, and S.¬†Wang, ‚ÄúTwo-phase multi-party
computation enabled privacy-preserving federated learning,‚Äù in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2020
20th IEEE/ACM International Symposium on Cluster, Cloud and Internet
Computing (CCGRID)</em>.¬†¬†¬†IEEE, 2020, pp.
410‚Äì419.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
A.¬†Triastcyn and B.¬†Faltings, ‚ÄúFederated learning with bayesian differential
privacy,‚Äù in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data (Big
Data)</em>.¬†¬†¬†IEEE, 2019, pp. 2587‚Äì2596.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
X.¬†Wu, Y.¬†Zhang, M.¬†Shi, P.¬†Li, R.¬†Li, and N.¬†N. Xiong, ‚ÄúAn adaptive federated
learning scheme with differential privacy preserving,‚Äù <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Future
Generation Computer Systems</em>, vol. 127, pp. 362‚Äì372, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
U.¬†Shah, I.¬†Dave, J.¬†Malde, J.¬†Mehta, and S.¬†Kodeboyina, ‚ÄúMaintaining privacy
in medical imaging with federated learning, deep learning, differential
privacy, and encrypted computation,‚Äù in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">2021 6th International
Conference for Convergence in Technology (I2CT)</em>.¬†¬†¬†IEEE, 2021, pp. 1‚Äì6.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
M.¬†Adnan, S.¬†Kalra, J.¬†C. Cresswell, G.¬†W. Taylor, and H.¬†R. Tizhoosh,
‚ÄúFederated learning and differential privacy for medical image analysis,‚Äù
<em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Scientific reports</em>, vol.¬†12, no.¬†1, pp. 1‚Äì10, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
O.¬†Choudhury, A.¬†Gkoulalas-Divanis, T.¬†Salonidis, I.¬†Sylla, Y.¬†Park, G.¬†Hsu,
and A.¬†Das, ‚ÄúDifferential privacy-enabled federated learning for sensitive
health data,‚Äù <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.02578</em>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A.¬†Ziller, D.¬†Usynin, R.¬†Braren, M.¬†Makowski, D.¬†Rueckert, and G.¬†Kaissis,
‚ÄúMedical imaging deep learning with differential privacy,‚Äù <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Scientific
Reports</em>, vol.¬†11, no.¬†1, pp. 1‚Äì8, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
L.¬†Zhang, J.¬†Xu, P.¬†Vijayakumar, P.¬†K. Sharma, and U.¬†Ghosh, ‚ÄúHomomorphic
encryption-based privacy-preserving federated learning in iot-enabled
healthcare system,‚Äù <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and
Engineering</em>, pp. 1‚Äì17, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
D.¬†Stripelis, H.¬†Saleem, T.¬†Ghai, N.¬†Dhinagar, U.¬†Gupta, C.¬†Anastasiou,
G.¬†Ver¬†Steeg, S.¬†Ravi, M.¬†Naveed, P.¬†M. Thompson <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúSecure
neuroimaging analysis using federated learning with homomorphic encryption,‚Äù
in <em id="bib.bib59.2.2" class="ltx_emph ltx_font_italic">17th International Symposium on Medical Information Processing and
Analysis</em>, vol. 12088.¬†¬†¬†SPIE, 2021, pp.
351‚Äì359.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
R.¬†Kumar, J.¬†Kumar, A.¬†A. Khan, H.¬†Ali, C.¬†M. Bernard, R.¬†U. Khan, S.¬†Zeng
<em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúBlockchain and homomorphic encryption based
privacy-preserving model aggregation for medical images,‚Äù <em id="bib.bib60.2.2" class="ltx_emph ltx_font_italic">Computerized
Medical Imaging and Graphics</em>, vol. 102, p. 102139, 2022.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
T.¬†S. Brisimi, R.¬†Chen, T.¬†Mela, A.¬†Olshevsky, I.¬†C. Paschalidis, and W.¬†Shi,
‚ÄúFederated learning of predictive models from federated electronic health
records,‚Äù <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">International journal of medical informatics</em>, vol. 112, pp.
59‚Äì67, 2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Y.¬†Zhao, H.¬†Liu, H.¬†Li, P.¬†Barnaghi, and H.¬†Haddadi, ‚ÄúSemi-supervised
federated learning for activity recognition,‚Äù <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2011.00851</em>, 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
A.¬†Grammenos, R.¬†Mendoza¬†Smith, J.¬†Crowcroft, and C.¬†Mascolo, ‚ÄúFederated
principal component analysis,‚Äù <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol.¬†33, pp. 6453‚Äì6464, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
H.¬†H. Kumar, V.¬†Karthik, and M.¬†K. Nair, ‚ÄúFederated k-means clustering: A
novel edge ai based approach for privacy preservation,‚Äù in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Cloud Computing in Emerging Markets
(CCEM)</em>.¬†¬†¬†IEEE, 2020, pp. 52‚Äì56.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
H.¬†Kassem, D.¬†Alapatt, P.¬†Mascagni, C.¬†AI4SafeChole, A.¬†Karargyris, and
N.¬†Padoy, ‚ÄúFederated cycling (fedcy): Semi-supervised federated learning of
surgical phases,‚Äù <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>, pp. 1‚Äì1,
2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Y.¬†Rehman, Y.¬†Gao, J.¬†Shen, P.¬†de¬†Gusm√£o, and N.¬†Lane, ‚ÄúFederated
self-supervised learning for video understanding,‚Äù <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics)</em>, vol. 13691 LNCS, pp.
506‚Äì522, 2022, cited By 0.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
I.¬†Dave, C.¬†Chen, and M.¬†Shah, ‚ÄúSpact: Self-supervised privacy preservation
for action recognition,‚Äù in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, vol. 2022-June.¬†¬†¬†IEEE Computer Society, 2022, pp. 20‚Äâ132‚Äì20‚Äâ141,
cited By 3.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
S.¬†Li, Y.¬†Mao, J.¬†Li, Y.¬†Xu, J.¬†Li, X.¬†Chen, S.¬†Liu, and X.¬†Zhao, ‚ÄúFedutn:
federated self-supervised learning with updating target network,‚Äù
<em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Applied Intelligence</em>, 2022, cited By 0.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
D.¬†Shome and T.¬†Kar, ‚ÄúFedaffect: Few-shot federated learning for facial
expression recognition,‚Äù in <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>, 2021, pp. 4168‚Äì4175.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
H.¬†Kheddar, M.¬†Hemis, Y.¬†Himeur, D.¬†Meg√≠as, and A.¬†Amira, ‚ÄúDeep learning
for diverse data types steganalysis: A review,‚Äù <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2308.04522</em>, 2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
W.¬†Ji, J.¬†Li, M.¬†Zhang, Y.¬†Piao, and H.¬†Lu, ‚ÄúAccurate rgb-d salient object
detection via collaborative learning,‚Äù in <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Computer Vision‚ÄìECCV 2020:
16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part
XVIII 16</em>.¬†¬†¬†Springer, 2020, pp. 52‚Äì69.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Q.¬†Fan, D.-P. Fan, H.¬†Fu, C.-K. Tang, L.¬†Shao, and Y.-W. Tai, ‚ÄúGroup
collaborative learning for co-salient object detection,‚Äù in
<em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 12‚Äâ288‚Äì12‚Äâ298.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
S.¬†Teeparthi, V.¬†Jatla, M.¬†S. Pattichis, S.¬†Celed√≥n-Pattichis, and
C.¬†L√≥pezLeiva, ‚ÄúFast hand detection in collaborative learning
environments,‚Äù in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Analysis of
Images and Patterns</em>.¬†¬†¬†Springer, 2021,
pp. 445‚Äì454.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
S.¬†Teeparthi, ‚ÄúLong term object detection and tracking in collaborative
learning environments,‚Äù <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.07556</em>, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
C.¬†Huang and R.¬†Nevatia, ‚ÄúHigh performance object detection by collaborative
learning of joint ranking of granules features,‚Äù in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">2010 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition</em>.¬†¬†¬†IEEE, 2010, pp. 41‚Äì48.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
X.¬†Fang, Z.¬†Kuang, R.¬†Zhang, X.¬†Shao, and H.¬†Wang, ‚ÄúCollaborative learning in
bounding box regression for object detection,‚Äù <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition
Letters</em>, 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Y.¬†Zhou, X.¬†He, L.¬†Huang, L.¬†Liu, F.¬†Zhu, S.¬†Cui, and L.¬†Shao, ‚ÄúCollaborative
learning of semi-supervised segmentation and classification for medical
images,‚Äù in <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2019, pp. 2079‚Äì2088.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
J.¬†Wang, J.¬†Yao, Y.¬†Zhang, and R.¬†Zhang, ‚ÄúCollaborative learning for weakly
supervised object detection,‚Äù <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.03531</em>, 2018.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
D.¬†Zhang, J.¬†Han, L.¬†Zhao, and D.¬†Meng, ‚ÄúLeveraging prior-knowledge for weakly
supervised object detection under a collaborative self-paced curriculum
learning framework,‚Äù <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol.
127, no.¬†4, pp. 363‚Äì380, 2019.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
S.¬†Chen, D.¬†Shao, X.¬†Shu, C.¬†Zhang, and J.¬†Wang, ‚ÄúFCC-Net: A full-coverage
collaborative network for weakly supervised remote sensing object
detection,‚Äù <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, vol.¬†9, no.¬†9, p. 1356, 2020.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Y.¬†Liang, G.¬†Qin, M.¬†Sun, J.¬†Qin, J.¬†Yan, and Z.¬†Zhang, ‚ÄúSemantic and detail
collaborative learning network for salient object detection,‚Äù
<em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 462, pp. 478‚Äì490, 2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
J.¬†Seo and H.¬†Park, ‚ÄúObject recognition in very low resolution images using
deep collaborative learning,‚Äù <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol.¬†7, pp.
134‚Äâ071‚Äì134‚Äâ082, 2019.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Q.¬†Guo, X.¬†Wang, Y.¬†Wu, Z.¬†Yu, D.¬†Liang, X.¬†Hu, and P.¬†Luo, ‚ÄúOnline knowledge
distillation via collaborative learning,‚Äù in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp.
11‚Äâ020‚Äì11‚Äâ029.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
P.¬†Yu and Y.¬†Liu, ‚ÄúFederated object detection: Optimizing object detection
model with federated learning,‚Äù in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd
International Conference on Vision, Image and Signal Processing</em>, 2019, pp.
1‚Äì6.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
H.¬†Choi and I.¬†V. Bajiƒá, ‚ÄúDeep feature compression for collaborative
object detection,‚Äù in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2018 25th IEEE International Conference on Image
Processing (ICIP)</em>.¬†¬†¬†IEEE, 2018, pp.
3743‚Äì3747.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
P.¬†Vepakomma, O.¬†Gupta, T.¬†Swedish, and R.¬†Raskar, ‚ÄúSplit learning for health:
Distributed deep learning without sharing raw patient data,‚Äù <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1812.00564</em>, 2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Y.¬†He, Y.¬†Kang, J.¬†Luo, L.¬†Fan, and Q.¬†Yang, ‚ÄúA hybrid self-supervised
learning framework for vertical federated learning,‚Äù <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2208.08934</em>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
W.¬†Zhuang, Y.¬†Wen, and S.¬†Zhang, ‚ÄúDivergence-aware federated self-supervised
learning,‚Äù <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.04385</em>, 2022.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
A.¬†Saeed, F.¬†D. Salim, T.¬†Ozcelebi, and J.¬†Lukkien, ‚ÄúFederated self-supervised
learning of multisensor representations for embedded intelligence,‚Äù
<em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol.¬†8, no.¬†2, pp. 1030‚Äì1040, 2020.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
R.¬†Yan, L.¬†Qu, Q.¬†Wei, S.-C. Huang, L.¬†Shen, D.¬†Rubin, L.¬†Xing, and Y.¬†Zhou,
‚ÄúLabel-efficient self-supervised federated learning for tackling data
heterogeneity in medical imaging,‚Äù <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical
Imaging</em>, 2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
R.¬†Zhu, K.¬†Yin, H.¬†Xiong, H.¬†Tang, and G.¬†Yin, ‚ÄúMasked face detection
algorithm in the dense crowd based on federated learning,‚Äù <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Wireless
Communications and Mobile Computing</em>, vol. 2021, 2021.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Y.¬†Himeur, S.¬†Al-Maadeed, H.¬†Kheddar, N.¬†Al-Maadeed, K.¬†Abualsaud, A.¬†Mohamed,
and T.¬†Khattab, ‚ÄúVideo surveillance using deep transfer learning and deep
domain adaptation: Towards better generalization,‚Äù <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Engineering
Applications of Artificial Intelligence</em>, vol. 119, p. 105698, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
A.¬†B. Sada, M.¬†A. Bouras, J.¬†Ma, H.¬†Runhe, and H.¬†Ning, ‚ÄúA distributed video
analytics architecture based on edge-computing and federated learning,‚Äù in
<em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl
Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data
Computing, Intl Conf on Cyber Science and Technology Congress
(DASC/PiCom/CBDCom/CyberSciTech)</em>.¬†¬†¬†IEEE, 2019, pp. 215‚Äì220.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
F.¬†Concone, C.¬†Ferdico, G.¬†L. Re, and M.¬†Morana, ‚ÄúA federated learning
approach for distributed human activity recognition,‚Äù in <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">2022 IEEE
International Conference on Smart Computing (SMARTCOMP)</em>.¬†¬†¬†IEEE, 2022, pp. 269‚Äì274.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Y.¬†Liu, J.¬†Nie, X.¬†Li, S.¬†H. Ahmed, W.¬†Y.¬†B. Lim, and C.¬†Miao, ‚ÄúFederated
learning in the sky: Aerial-ground air quality sensing framework with uav
swarms,‚Äù <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2020.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Y.¬†Liu, A.¬†Huang, Y.¬†Luo, H.¬†Huang, Y.¬†Liu, Y.¬†Chen, L.¬†Feng, T.¬†Chen, H.¬†Yu,
and Q.¬†Yang, ‚ÄúFedvision: An online visual object detection platform powered
by federated learning,‚Äù in <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on
Artificial Intelligence</em>, vol.¬†34, 2020, pp. 13‚Äâ172‚Äì13‚Äâ179.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
A.¬†Catalfamo, A.¬†Celesti, M.¬†Fazio, G.¬†Randazzo, and M.¬†Villari, ‚ÄúA platform
for federated learning on the edge: a video analysis use case,‚Äù in
<em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Symposium on Computers and Communications (ISCC)</em>, 2022, pp.
1‚Äì7.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
P.¬†Jain, S.¬†Goenka, S.¬†Bagchi, B.¬†Banerjee, and S.¬†Chaterji, ‚ÄúFederated action
recognition on heterogeneous embedded devices,‚Äù <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2107.12147</em>, 2021.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
B.¬†Zhang, J.¬†Wang, J.¬†Fu, and J.¬†Xia, ‚ÄúDriver action recognition using
federated learning,‚Äù in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">2021 the 7th International Conference on
Communication and Information Processing (ICCIP)</em>, 2021, pp. 74‚Äì77.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Z.¬†Shi, L.¬†Zhang, Y.¬†Liu, X.¬†Cao, Y.¬†Ye, M.-M. Cheng, and G.¬†Zheng, ‚ÄúCrowd
counting with deep negative correlation learning,‚Äù in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE conference on computer vision and pattern recognition</em>, 2018, pp.
5382‚Äì5390.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
A.¬†Zhang, J.¬†Shen, Z.¬†Xiao, F.¬†Zhu, X.¬†Zhen, X.¬†Cao, and L.¬†Shao, ‚ÄúRelational
attention network for crowd counting,‚Äù in <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2019, pp. 6788‚Äì6797.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Y.¬†Jiang, R.¬†Cong, C.¬†Shu, A.¬†Yang, Z.¬†Zhao, and G.¬†Min, ‚ÄúFederated learning
based mobile crowd sensing with unreliable user data,‚Äù in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
22nd International Conference on High Performance Computing and
Communications; IEEE 18th International Conference on Smart City; IEEE 6th
International Conference on Data Science and Systems
(HPCC/SmartCity/DSS)</em>.¬†¬†¬†IEEE, 2020, pp.
320‚Äì327.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
X.¬†Bao, C.¬†Su, Y.¬†Xiong, W.¬†Huang, and Y.¬†Hu, ‚ÄúFlchain: A blockchain for
auditable federated learning with trust and incentive,‚Äù in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM)</em>.¬†¬†¬†IEEE, 2019, pp. 151‚Äì159.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
B.¬†R. Kiran, D.¬†M. Thomas, and R.¬†Parakkal, ‚ÄúAn overview of deep learning
based methods for unsupervised and semi-supervised anomaly detection in
videos,‚Äù <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Journal of Imaging</em>, vol.¬†4, no.¬†2, p.¬†36, 2018.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
S.¬†Singh, S.¬†Bhardwaj, H.¬†Pandey, and G.¬†Beniwal, ‚ÄúAnomaly detection using
federated learning,‚Äù in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of International Conference on
Artificial Intelligence and Applications</em>.¬†¬†¬†Springer, 2021, pp. 141‚Äì148.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
S.¬†Bharti, A.¬†McGibney, and T.¬†O‚ÄôGorman, ‚ÄúEdge-enabled federated learning for
vision based product quality inspection,‚Äù in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">2022 33rd Irish Signals
and Systems Conference (ISSC)</em>.¬†¬†¬†IEEE,
2022, pp. 1‚Äì6.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
A.¬†Esteva, K.¬†Chou, S.¬†Yeung, N.¬†Naik, A.¬†Madani, A.¬†Mottaghi, Y.¬†Liu,
E.¬†Topol, J.¬†Dean, and R.¬†Socher, ‚ÄúDeep learning-enabled medical computer
vision,‚Äù <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, vol.¬†4, no.¬†1, pp. 1‚Äì9, 2021.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
M.¬†J. Sheller, G.¬†A. Reina, B.¬†Edwards, J.¬†Martin, and S.¬†Bakas,
‚ÄúMulti-institutional deep learning modeling without sharing patient data: A
feasibility study on brain tumor segmentation,‚Äù in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">International
MICCAI Brainlesion Workshop</em>.¬†¬†¬†Springer, 2018, pp. 92‚Äì104.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
X.¬†Li, Y.¬†Gu, N.¬†Dvornek, L.¬†H. Staib, P.¬†Ventola, and J.¬†S. Duncan,
‚ÄúMulti-site fmri analysis using privacy-preserving federated learning and
domain adaptation: Abide results,‚Äù <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, vol.¬†65,
p. 101765, 2020.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
I.¬†Dayan, H.¬†R. Roth, A.¬†Zhong, A.¬†Harouni, A.¬†Gentili, A.¬†Z. Abidin, A.¬†Liu,
A.¬†B. Costa, B.¬†J. Wood, C.-S. Tsai <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúFederated learning for
predicting clinical outcomes in patients with covid-19,‚Äù <em id="bib.bib110.2.2" class="ltx_emph ltx_font_italic">Nature
medicine</em>, vol.¬†27, no.¬†10, pp. 1735‚Äì1743, 2021.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
I.¬†Feki, S.¬†Ammar, Y.¬†Kessentini, and K.¬†Muhammad, ‚ÄúFederated learning for
covid-19 screening from chest x-ray images,‚Äù <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Applied Soft Computing</em>,
vol. 106, p. 107330, 2021.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
A.¬†E. Cetinkaya, M.¬†Akin, and S.¬†Sagiroglu, ‚ÄúA communication efficient
federated learning approach to multi chest diseases classification,‚Äù in
<em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">2021 6th International Conference on Computer Science and Engineering
(UBMK)</em>.¬†¬†¬†IEEE, 2021, pp. 429‚Äì434.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
B.¬†Yan, B.¬†Liu, L.¬†Wang, Y.¬†Zhou, Z.¬†Liang, M.¬†Liu, and C.-Z. Xu, ‚ÄúFedcm: A
real-time contribution measurement method for participants in federated
learning,‚Äù in <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">2021 International Joint Conference on Neural Networks
(IJCNN)</em>.¬†¬†¬†IEEE, 2021, pp. 1‚Äì8.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
H.¬†R. Roth, K.¬†Chang, P.¬†Singh, N.¬†Neumark, W.¬†Li, V.¬†Gupta, S.¬†Gupta, L.¬†Qu,
A.¬†Ihsani, B.¬†C. Bizzo <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúFederated learning for breast density
classification: A real-world implementation,‚Äù in <em id="bib.bib114.2.2" class="ltx_emph ltx_font_italic">Domain Adaptation and
Representation Transfer, and Distributed and Collaborative Learning</em>.¬†¬†¬†Springer, 2020, pp. 181‚Äì191.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Z.¬†Yan, J.¬†Wicaksana, Z.¬†Wang, X.¬†Yang, and K.-T. Cheng, ‚ÄúVariation-aware
federated learning with multi-source decentralized medical image data,‚Äù
<em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health Informatics</em>, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
S.¬†Silva, B.¬†A. Gutman, E.¬†Romero, P.¬†M. Thompson, A.¬†Altmann, and M.¬†Lorenzi,
‚ÄúFederated learning in distributed medical databases: Meta-analysis of
large-scale subcortical brain data,‚Äù in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 16th international
symposium on biomedical imaging (ISBI 2019)</em>.¬†¬†¬†IEEE, 2019, pp. 270‚Äì274.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
M.¬†J. Sheller, B.¬†Edwards, G.¬†A. Reina, J.¬†Martin, S.¬†Pati, A.¬†Kotrotsou,
M.¬†Milchenko, W.¬†Xu, D.¬†Marcus, R.¬†R. Colen <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúFederated
learning in medicine: facilitating multi-institutional collaborations without
sharing patient data,‚Äù <em id="bib.bib117.2.2" class="ltx_emph ltx_font_italic">Scientific reports</em>, vol.¬†10, no.¬†1, pp. 1‚Äì12,
2020.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
P.¬†Guo, P.¬†Wang, J.¬†Zhou, S.¬†Jiang, and V.¬†M. Patel, ‚ÄúMulti-institutional
collaborations for improving deep learning-based magnetic resonance image
reconstruction using federated learning,‚Äù in <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp.
2423‚Äì2432.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
M.¬†Y. Lu, R.¬†J. Chen, D.¬†Kong, J.¬†Lipkova, R.¬†Singh, D.¬†F. Williamson, T.¬†Y.
Chen, and F.¬†Mahmood, ‚ÄúFederated learning for computational pathology on
gigapixel whole slide images,‚Äù <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Medical image analysis</em>, vol.¬†76, p.
102298, 2022.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
W.¬†Li, F.¬†Milletar√¨, D.¬†Xu, N.¬†Rieke, J.¬†Hancox, W.¬†Zhu, M.¬†Baust,
Y.¬†Cheng, S.¬†Ourselin, M.¬†J. Cardoso <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúPrivacy-preserving
federated brain tumour segmentation,‚Äù in <em id="bib.bib120.2.2" class="ltx_emph ltx_font_italic">International workshop on
machine learning in medical imaging</em>.¬†¬†¬†Springer, 2019, pp. 133‚Äì141.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
C.¬†I. Bercea, B.¬†Wiestler, D.¬†Rueckert, and S.¬†Albarqouni, ‚ÄúFeddis:
Disentangled federated learning for unsupervised brain pathology
segmentation,‚Äù <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.03705</em>, 2021.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
A.¬†E. Cetinkaya, M.¬†Akin, and S.¬†Sagiroglu, ‚ÄúImproving performance of
federated learning based medical image analysis in non-iid settings using
image augmentation,‚Äù in <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">2021 International Conference on Information
Security and Cryptology (ISCTURKEY)</em>.¬†¬†¬†IEEE, 2021, pp. 69‚Äì74.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
B.¬†Han, R.¬†Jhaveri, H.¬†Wang, D.¬†Qiao, and J.¬†Du, ‚ÄúApplication of robust
zero-watermarking scheme based on federated learning for securing the
healthcare data,‚Äù <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health Informatics</em>,
2021.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
B.¬†G. Tekgul, Y.¬†Xia, S.¬†Marchal, and N.¬†Asokan, ‚ÄúWaffle: Watermarking in
federated learning,‚Äù in <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">2021 40th International Symposium on Reliable
Distributed Systems (SRDS)</em>.¬†¬†¬†IEEE,
2021, pp. 310‚Äì320.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
X.¬†Liu, S.¬†Shao, Y.¬†Yang, K.¬†Wu, W.¬†Yang, and H.¬†Fang, ‚ÄúSecure federated
learning model verification: A client-side backdoor triggered watermarking
scheme,‚Äù in <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Systems, Man, and
Cybernetics (SMC)</em>.¬†¬†¬†IEEE, 2021, pp.
2414‚Äì2419.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
D.¬†Po≈Çap, G.¬†Srivastava, and K.¬†Yu, ‚ÄúAgent architecture of an intelligent
medical system based on federated learning and blockchain technology,‚Äù
<em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">Journal of Information Security and Applications</em>, vol.¬†58, p. 102748,
2021.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
A.¬†Ziller, D.¬†Usynin, N.¬†Remerscheid, M.¬†Knolle, M.¬†Makowski, R.¬†Braren,
D.¬†Rueckert, and G.¬†Kaissis, ‚ÄúDifferentially private federated deep learning
for multi-site medical image segmentation,‚Äù <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2107.02586</em>, 2021.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
K.¬†Guo, N.¬†Li, J.¬†Kang, and J.¬†Zhang, ‚ÄúTowards efficient federated
learning-based scheme in medical cyber-physical systems for distributed
data,‚Äù <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Software: Practice and Experience</em>, vol.¬†51, no.¬†11, pp.
2274‚Äì2289, 2021.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
M.¬†Jiang, Z.¬†Wang, and Q.¬†Dou, ‚ÄúHarmofl: Harmonizing local and global drifts
in federated learning on heterogeneous medical images,‚Äù <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2112.10775</em>, 2021.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Q.¬†Liu, C.¬†Chen, J.¬†Qin, Q.¬†Dou, and P.-A. Heng, ‚ÄúFeddg: Federated domain
generalization on medical image segmentation via episodic learning in
continuous frequency space,‚Äù in <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition</em>, 2021, pp. 1013‚Äì1023.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
B.¬†Sun, H.¬†Huo, Y.¬†Yang, and B.¬†Bai, ‚ÄúPartialfed: Cross-domain personalized
federated learning via partial initialization,‚Äù <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol.¬†34, 2021.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
H.-P. Wang, S.¬†U. Stich, Y.¬†He, and M.¬†Fritz, ‚ÄúProgfed: Effective,
communication, and computation efficient federated learning by progressive
training,‚Äù <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.05323</em>, 2021.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
J.¬†Luo and S.¬†Wu, ‚ÄúFedsld: Federated learning with shared label distribution
for medical image classification,‚Äù <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08378</em>,
2021.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
G.¬†Litjens, O.¬†Debats, J.¬†Barentsz, N.¬†Karssemeijer, and H.¬†Huisman,
‚ÄúComputer-aided detection of prostate cancer in mri,‚Äù <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on medical imaging</em>, vol.¬†33, no.¬†5, pp. 1083‚Äì1092, 2014.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
H.¬†Zhang, J.¬†Bosch, and H.¬†H. Olsson, ‚ÄúEnd-to-end federated learning for
autonomous driving vehicles,‚Äù in <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">2021 International Joint Conference
on Neural Networks (IJCNN)</em>.¬†¬†¬†IEEE,
2021, pp. 1‚Äì8.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
A.¬†Nguyen, T.¬†Do, M.¬†Tran, B.¬†X. Nguyen, C.¬†Duong, T.¬†Phan, E.¬†Tjiputra, and
Q.¬†D. Tran, ‚ÄúDeep federated learning for autonomous driving,‚Äù in <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">2022
IEEE Intelligent Vehicles Symposium (IV)</em>.¬†¬†¬†IEEE, 2022, pp. 1824‚Äì1830.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
X.¬†Zhou, W.¬†Liang, J.¬†She, Z.¬†Yan, I.¬†Kevin, and K.¬†Wang, ‚ÄúTwo-layer federated
learning with heterogeneous model aggregation for 6g supported internet of
vehicles,‚Äù <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Vehicular Technology</em>, vol.¬†70, no.¬†6,
pp. 5308‚Äì5317, 2021.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
L.¬†U. Khan, Y.¬†K. Tun, M.¬†Alsenwi, M.¬†Imran, Z.¬†Han, and C.¬†S. Hong, ‚ÄúA
dispersed federated learning framework for 6g-enabled autonomous driving
cars,‚Äù <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and Engineering</em>, 2022.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Y.¬†Li, X.¬†Tao, X.¬†Zhang, J.¬†Liu, and J.¬†Xu, ‚ÄúPrivacy-preserved federated
learning for autonomous driving,‚Äù <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent
Transportation Systems</em>, vol.¬†23, no.¬†7, pp. 8423‚Äì8434, 2021.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
I.¬†Donevski, J.¬†J. Nielsen, and P.¬†Popovski, ‚ÄúOn addressing heterogeneity in
federated learning for autonomous vehicles connected to a drone
orchestrator,‚Äù <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Frontiers in Communications and Networks</em>, vol.¬†2, p.
709946, 2021.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
A.¬†M. Elbir, B.¬†Soner, S.¬†√á√∂leri, D.¬†G√ºnd√ºz, and M.¬†Bennis,
‚ÄúFederated learning in vehicular networks,‚Äù in <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">2022 IEEE
International Mediterranean Conference on Communications and Networking
(MeditCom)</em>.¬†¬†¬†IEEE, 2022, pp. 72‚Äì77.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
D.¬†Jallepalli, N.¬†C. Ravikumar, P.¬†V. Badarinath, S.¬†Uchil, and M.¬†A. Suresh,
‚ÄúFederated learning for object detection in autonomous vehicles,‚Äù in
<em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Seventh International Conference on Big Data Computing
Service and Applications (BigDataService)</em>.¬†¬†¬†IEEE, 2021, pp. 107‚Äì114.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
K.¬†Xie, Z.¬†Zhang, B.¬†Li, J.¬†Kang, D.¬†Niyato, S.¬†Xie, and Y.¬†Wu, ‚ÄúEfficient
federated learning with spike neural networks for traffic sign recognition,‚Äù
<em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Vehicular Technology</em>, vol.¬†71, no.¬†9, pp.
9980‚Äì9992, 2022.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
F.¬†Sattler, S.¬†Wiedemann, K.-R. M√ºller, and W.¬†Samek, ‚ÄúRobust and
communication-efficient federated learning from non-iid data,‚Äù <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on neural networks and learning systems</em>, vol.¬†31, no.¬†9, pp.
3400‚Äì3413, 2019.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
F.¬†Haddadpour, M.¬†M. Kamani, A.¬†Mokhtari, and M.¬†Mahdavi, ‚ÄúFederated learning
with compression: Unified analysis and sharp guarantees,‚Äù in
<em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</em>.¬†¬†¬†PMLR, 2021, pp.
2350‚Äì2358.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
W.¬†Luping, W.¬†Wei, and L.¬†Bo, ‚ÄúCmfl: Mitigating communication overhead for
federated learning,‚Äù in <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 39th international conference on
distributed computing systems (ICDCS)</em>.¬†¬†¬†IEEE, 2019, pp. 954‚Äì964.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
I.¬†Varlamis, C.¬†Sardianos, C.¬†Chronis, G.¬†Dimitrakopoulos, Y.¬†Himeur,
A.¬†Alsalemi, F.¬†Bensaali, and A.¬†Amira, ‚ÄúUsing big data and federated
learning for generating energy efficiency recommendations,‚Äù
<em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">International Journal of Data Science and Analytics</em>, pp. 1‚Äì17, 2022.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
M.¬†Tang, X.¬†Ning, Y.¬†Wang, J.¬†Sun, Y.¬†Wang, H.¬†Li, and Y.¬†Chen, ‚ÄúFedcor:
Correlation-based active client selection strategy for heterogeneous
federated learning,‚Äù in <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, 2022, pp. 10‚Äâ092‚Äì10‚Äâ101.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
L.¬†Qu, Y.¬†Zhou, P.¬†P. Liang, Y.¬†Xia, F.¬†Wang, E.¬†Adeli, L.¬†Fei-Fei, and
D.¬†Rubin, ‚ÄúRethinking architecture design for tackling data heterogeneity in
federated learning,‚Äù in <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2022, pp. 10‚Äâ061‚Äì10‚Äâ071.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
M.¬†Mendieta, T.¬†Yang, P.¬†Wang, M.¬†Lee, Z.¬†Ding, and C.¬†Chen, ‚ÄúLocal learning
matters: Rethinking data heterogeneity in federated learning,‚Äù in
<em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2022, pp. 8397‚Äì8406.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
H.¬†Bousbiat, R.¬†Bousselidj, Y.¬†Himeur, A.¬†Amira, F.¬†Bensaali, F.¬†Fadli,
W.¬†Mansoor, and W.¬†Elmenreich, ‚ÄúCrossing roads of federated learning and
smart grids: Overview, challenges, and perspectives,‚Äù <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.08602</em>, 2023.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
M.¬†Dom√≠nguez-Morales, J.¬†P. Dom√≠nguez-Morales,
√Å.¬†Jim√©nez-Fern√°ndez, A.¬†Linares-Barranco, and
G.¬†Jim√©nez-Moreno, ‚ÄúStereo matching in address-event-representation
(aer) bio-inspired binocular systems in a field-programmable gate array
(fpga),‚Äù <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, vol.¬†8, no.¬†4, p. 410, 2019.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
O.¬†Azzouzi, M.¬†Anane, M.¬†Koudil, M.¬†Issad, and Y.¬†Himeur, ‚ÄúNovel
area-efficient and flexible architectures for optimal ate pairing on fpga,‚Äù
<em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.04261</em>, 2023.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
D.¬†Li and J.¬†Wang, ‚ÄúFedmd: Heterogenous federated learning via model
distillation,‚Äù <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em>, 2019.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
J.¬†Pang, Y.¬†Huang, Z.¬†Xie, Q.¬†Han, and Z.¬†Cai, ‚ÄúRealizing the heterogeneity: A
self-organized federated learning framework for iot,‚Äù <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of
Things Journal</em>, vol.¬†8, no.¬†5, pp. 3088‚Äì3098, 2020.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
A.¬†Alsalemi, Y.¬†Himeur, F.¬†Bensaali, and A.¬†Amira, ‚ÄúSmart sensing and
end-users‚Äô behavioral change in residential buildings: An edge-based
internet of energy perspective,‚Äù <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol.¬†21,
no.¬†24, pp. 27‚Äâ623‚Äì27‚Äâ631, 2021.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
H.¬†Zhu, J.¬†Xu, S.¬†Liu, and Y.¬†Jin, ‚ÄúFederated learning on non-iid data: A
survey,‚Äù <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 465, pp. 371‚Äì390, 2021.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
H.¬†Wang, Z.¬†Kaplan, D.¬†Niu, and B.¬†Li, ‚ÄúOptimizing federated learning on
non-iid data with reinforcement learning,‚Äù in <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2020-IEEE
Conference on Computer Communications</em>.¬†¬†¬†IEEE, 2020, pp. 1698‚Äì1707.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
C.¬†Briggs, Z.¬†Fan, and P.¬†Andras, ‚ÄúFederated learning with hierarchical
clustering of local updates to improve training on non-iid data,‚Äù in
<em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>.¬†¬†¬†IEEE, 2020, pp. 1‚Äì9.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
H.¬†Bousbiat, Y.¬†Himeur, I.¬†Varlamis, F.¬†Bensaali, and A.¬†Amira, ‚ÄúNeural load
disaggregation: Meta-analysis, federated learning and beyond,‚Äù
<em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Energies</em>, vol.¬†16, no.¬†2, p. 991, 2023.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
S.¬†Pouriyeh, O.¬†Shahid, R.¬†M. Parizi, Q.¬†Z. Sheng, G.¬†Srivastava, L.¬†Zhao, and
M.¬†Nasajpour, ‚ÄúSecure smart communication efficiency in federated learning:
Achievements and challenges,‚Äù <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol.¬†12, no.¬†18, p.
8980, 2022.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
L.¬†Lyu, H.¬†Yu, X.¬†Ma, L.¬†Sun, J.¬†Zhao, Q.¬†Yang, and P.¬†S. Yu, ‚ÄúPrivacy and
robustness in federated learning: Attacks and defenses,‚Äù <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2012.06337</em>, 2020.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
S.¬†Shen, S.¬†Tople, and P.¬†Saxena, ‚ÄúAuror: Defending against poisoning attacks
in collaborative deep learning systems,‚Äù in <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd
Annual Conference on Computer Security Applications</em>, 2016, pp. 508‚Äì519.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
S.¬†Andreina, G.¬†A. Marson, H.¬†M√∂llering, and G.¬†Karame, ‚ÄúBaffle: Backdoor
detection via feedback-based federated learning,‚Äù in <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 41st
International Conference on Distributed Computing Systems (ICDCS)</em>.¬†¬†¬†IEEE, 2021, pp. 852‚Äì863.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Y.¬†Qi, M.¬†S. Hossain, J.¬†Nie, and X.¬†Li, ‚ÄúPrivacy-preserving blockchain-based
federated learning for traffic flow prediction,‚Äù <em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">Future Generation
Computer Systems</em>, vol. 117, pp. 328‚Äì337, 2021.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
C.¬†He, A.¬†D. Shah, Z.¬†Tang, D.¬†F.¬†N. Sivashunmugam, K.¬†Bhogaraju, M.¬†Shimpi,
L.¬†Shen, X.¬†Chu, M.¬†Soltanolkotabi, and S.¬†Avestimehr, ‚ÄúFedcv: a federated
learning framework for diverse computer vision tasks,‚Äù <em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2111.11066</em>, 2021.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
J.¬†Luo, X.¬†Wu, Y.¬†Luo, A.¬†Huang, Y.¬†Huang, Y.¬†Liu, and Q.¬†Yang, ‚ÄúReal-world
image datasets for federated learning,‚Äù <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1910.11089</em>, 2019.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
S.¬†S. Sohail, F.¬†Farhat, Y.¬†Himeur, M.¬†Nadeem, D.¬†√ò. Madsen, Y.¬†Singh,
S.¬†Atalla, and W.¬†Mansoor, ‚ÄúDecoding chatgpt: A taxonomy of existing
research, current challenges, and possible future directions,‚Äù <em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">Journal
of King Saud University-Computer and Information Sciences</em>, p. 101675, 2023.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
F.¬†Farhat, E.¬†S. Silva, H.¬†Hassani, D.¬†√ò. Madsen, S.¬†S. Sohail, Y.¬†Himeur,
M.¬†A. Alam, and A.¬†Zafar, ‚ÄúAnalyzing the scholarly footprint of chatgpt:
mapping the progress and identifying future trends,‚Äù 2023.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
S.¬†S. Sohail, F.¬†Farhat, Y.¬†Himeur, M.¬†Nadeem, D.¬†√ò. Madsen, Y.¬†Singh,
S.¬†Atalla, and W.¬†Mansoor, ‚ÄúThe future of gpt: A taxonomy of existing
chatgpt research, current challenges, and possible future directions,‚Äù
<em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">Current Challenges, and Possible Future Directions (April 8, 2023)</em>,
2023.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
S.¬†S. Sohail, D.¬†√ò. Madsen, Y.¬†Himeur, and M.¬†Ashraf, ‚ÄúUsing chatgpt to
navigate ambivalent and contradictory research findings on artificial
intelligence,‚Äù <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Available at SSRN 4413913</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.13557" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.13558" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.13558">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.13558" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.13559" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 10:37:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
