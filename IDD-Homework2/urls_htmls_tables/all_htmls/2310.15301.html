<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.15301] ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease</title><meta property="og:description" content="Alzheimer’s Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and n…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.15301">

<!--Generated on Tue Feb 27 22:37:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno" lang="en">
<h1 class="ltx_title ltx_title_document">ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaomin Ouyang<sup id="id17.17.id1" class="ltx_sup"><span id="id17.17.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Xian Shuai<sup id="id18.18.id2" class="ltx_sup"><span id="id18.18.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Yang Li<sup id="id19.19.id3" class="ltx_sup"><span id="id19.19.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Li Pan<sup id="id20.20.id4" class="ltx_sup"><span id="id20.20.id4.1" class="ltx_text ltx_font_italic">1</span></sup>, Xifan Zhang<sup id="id21.21.id5" class="ltx_sup"><span id="id21.21.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Heming Fu<sup id="id22.22.id6" class="ltx_sup"><span id="id22.22.id6.1" class="ltx_text ltx_font_italic">1</span></sup>, Xinyan Wang<sup id="id23.23.id7" class="ltx_sup"><span id="id23.23.id7.1" class="ltx_text ltx_font_italic">1</span></sup>, Shihua Cao<sup id="id24.24.id8" class="ltx_sup"><span id="id24.24.id8.1" class="ltx_text ltx_font_italic">1</span></sup>, Jiang Xin<sup id="id25.25.id9" class="ltx_sup"><span id="id25.25.id9.1" class="ltx_text ltx_font_italic">1</span></sup>, Hazel Mok<sup id="id26.26.id10" class="ltx_sup"><span id="id26.26.id10.1" class="ltx_text ltx_font_italic">1</span></sup>, Zhenyu Yan<sup id="id27.27.id11" class="ltx_sup"><span id="id27.27.id11.1" class="ltx_text ltx_font_italic">1</span></sup>, Doris Sau Fung Yu<sup id="id28.28.id12" class="ltx_sup"><span id="id28.28.id12.1" class="ltx_text ltx_font_italic">1</span></sup>, Timothy Kwok<sup id="id29.29.id13" class="ltx_sup"><span id="id29.29.id13.1" class="ltx_text ltx_font_italic">1</span></sup>, Guoliang Xing<sup id="id30.30.id14" class="ltx_sup"><span id="id30.30.id14.1" class="ltx_text ltx_font_italic">1,∗</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id15.15.1" class="ltx_text ltx_affiliation_institution"><sup id="id15.15.1.1" class="ltx_sup"><span id="id15.15.1.1.1" class="ltx_text ltx_font_italic">1</span></sup>The Chinese University of Hong Kong</span><span id="id31.31.id1" class="ltx_text ltx_affiliation_country"></span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id16.16.1" class="ltx_text ltx_affiliation_institution"><sup id="id16.16.1.1" class="ltx_sup"><span id="id16.16.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>Corresponding Author</span><span id="id32.32.id1" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id33.id1" class="ltx_p"><span id="id33.id1.1" class="ltx_text">Alzheimer’s Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that can allow AD clinicians to characterize and track the complex correlation between multidimensional interpretable digital biomarkers, demographic factors of patients, and AD diagnosis in a longitudinal manner.</span></p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span></span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Alzheimer’s Disease (AD) is a progressive neurodegenerative disease that can cause significantly declining cognitive and functional abilities. AD and related dementia is a growing health challenge worldwide because of population aging <cite class="ltx_cite ltx_citemacro_citep">(Livingston et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>; Livingston
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>. In 2010, about 35.6 million people lived with dementia, which is expected to double every 20 years <cite class="ltx_cite ltx_citemacro_citep">(Fiest
et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A major barrier to the treatment of AD is that many patients are either not diagnosed or diagnosed at the late stages of the disease. Studies suggest that 75% of worldwide persons with dementia are undiagnosed <cite class="ltx_cite ltx_citemacro_citep">(ad-, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. This is largely due to the fact that, the standard clinical procedure for AD diagnosis, based on Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) brain scan, or blood biomarkers like amyloid <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="\beta_{1-42}" display="inline"><semantics id="S1.p2.1.m1.1a"><msub id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">β</mi><mrow id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml"><mn id="S1.p2.1.m1.1.1.3.2" xref="S1.p2.1.m1.1.1.3.2.cmml">1</mn><mo id="S1.p2.1.m1.1.1.3.1" xref="S1.p2.1.m1.1.1.3.1.cmml">−</mo><mn id="S1.p2.1.m1.1.1.3.3" xref="S1.p2.1.m1.1.1.3.3.cmml">42</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">subscript</csymbol><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">𝛽</ci><apply id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3"><minus id="S1.p2.1.m1.1.1.3.1.cmml" xref="S1.p2.1.m1.1.1.3.1"></minus><cn type="integer" id="S1.p2.1.m1.1.1.3.2.cmml" xref="S1.p2.1.m1.1.1.3.2">1</cn><cn type="integer" id="S1.p2.1.m1.1.1.3.3.cmml" xref="S1.p2.1.m1.1.1.3.3">42</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\beta_{1-42}</annotation></semantics></math>, is only available in clinical settings. Although various screening tests can identify cognitive impairments, they are usually intrusive and cannot be conducted routinely or in a real-time manner. Therefore, early identification of people at risk of developing AD and timely intervention to slow the onset and progression of AD are crucial.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A recent major advance in early AD diagnosis and intervention is to leverage AI and sensor devices to capture physiological, behavioral, and lifestyle symptoms of AD (e.g., activities of daily living and social interactions) in natural home environments, referred to as <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">digital biomarkers</em> <cite class="ltx_cite ltx_citemacro_citep">(dig, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>; Guoliang Xing, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>; Kourtis
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. The difficulty in performing activities of daily living (ADLs) is a hallmark feature of AD, because ADLs, such as watching TV, cleaning living areas, and taking medicine, involve tasks that require independence, organization, judgment, and sequencing abilities <cite class="ltx_cite ltx_citemacro_citep">(Tekin et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2001</a>)</cite>.
Moreover, social behaviors, such as family meals and phone calls, are shown to be strongly correlated with the risk of early AD <cite class="ltx_cite ltx_citemacro_citep">(Doris
et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>; Arai
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">There are several major challenges that have not been addressed in previous work on AD digital biomarkers. First, existing work is focused on a particular type of digital biomarker, such as motor function <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, speech features <cite class="ltx_cite ltx_citemacro_citep">(Nasreen
et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>; Luz et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>, or driving habits <cite class="ltx_cite ltx_citemacro_citep">(Bayat et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, which lacks generalizability to subjects with various demographic and medical characteristics. Second, most of the studies are based on the black-box approach, where the sensor data/feature is directly used to identify AD, which will not only incur extremely high computing overhead but also result in digital biomarkers that are difficult to interpret by medical professionals.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-266.1pt,31.2pt) scale(0.448965084049942,0.448965084049942) ;">
<table id="S1.T1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<td id="S1.T1.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Approach</td>
<td id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Accuracy</td>
<td id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T1.2.1.1.4.1" class="ltx_text"></span> <span id="S1.T1.2.1.1.4.2" class="ltx_text">
<span id="S1.T1.2.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.2.1.1.4.2.1.1" class="ltx_tr">
<span id="S1.T1.2.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"># of biomarkers</span></span>
</span></span><span id="S1.T1.2.1.1.4.3" class="ltx_text"></span></td>
<td id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T1.2.1.1.5.1" class="ltx_text"></span> <span id="S1.T1.2.1.1.5.2" class="ltx_text">
<span id="S1.T1.2.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.2.1.1.5.2.1.1" class="ltx_tr">
<span id="S1.T1.2.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Sensors</span></span>
</span></span><span id="S1.T1.2.1.1.5.3" class="ltx_text"></span></td>
<td id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T1.2.1.1.6.1" class="ltx_text"></span> <span id="S1.T1.2.1.1.6.2" class="ltx_text">
<span id="S1.T1.2.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.2.1.1.6.2.1.1" class="ltx_tr">
<span id="S1.T1.2.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"># of subjects</span></span>
</span></span><span id="S1.T1.2.1.1.6.3" class="ltx_text"></span></td>
<td id="S1.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T1.2.1.1.7.1" class="ltx_text"></span> <span id="S1.T1.2.1.1.7.2" class="ltx_text">
<span id="S1.T1.2.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.2.1.1.7.2.1.1" class="ltx_tr">
<span id="S1.T1.2.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Duration</span></span>
</span></span><span id="S1.T1.2.1.1.7.3" class="ltx_text"></span></td>
</tr>
<tr id="S1.T1.2.1.2" class="ltx_tr">
<td id="S1.T1.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Tatc <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S1.T1.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Black-box, Centralized</td>
<td id="S1.T1.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">42.3% for MCI</td>
<td id="S1.T1.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1 (sensor data)</td>
<td id="S1.T1.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">Actigraphy</td>
<td id="S1.T1.2.1.2.6" class="ltx_td ltx_align_center ltx_border_t">729 (185 AD, 103 MCI, 441 NC)</td>
<td id="S1.T1.2.1.2.7" class="ltx_td ltx_align_center ltx_border_t">7 days</td>
</tr>
<tr id="S1.T1.2.1.3" class="ltx_tr">
<td id="S1.T1.2.1.3.1" class="ltx_td ltx_align_center">ADReSS <cite class="ltx_cite ltx_citemacro_citep">(Luz et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S1.T1.2.1.3.2" class="ltx_td ltx_align_center">Black-box, Centralized</td>
<td id="S1.T1.2.1.3.3" class="ltx_td ltx_align_center">60.8% for AD</td>
<td id="S1.T1.2.1.3.4" class="ltx_td ltx_align_center">6 acoustic features</td>
<td id="S1.T1.2.1.3.5" class="ltx_td ltx_align_center">Audio</td>
<td id="S1.T1.2.1.3.6" class="ltx_td ltx_align_center">156 (78 AD, 78 non-AD)</td>
<td id="S1.T1.2.1.3.7" class="ltx_td ltx_align_center">10mins (in lab)</td>
</tr>
<tr id="S1.T1.2.1.4" class="ltx_tr">
<td id="S1.T1.2.1.4.1" class="ltx_td ltx_align_center">Bayat et al. <cite class="ltx_cite ltx_citemacro_citep">(Bayat et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S1.T1.2.1.4.2" class="ltx_td ltx_align_center">Black-box, Centralized</td>
<td id="S1.T1.2.1.4.3" class="ltx_td ltx_align_center">82% fro AD</td>
<td id="S1.T1.2.1.4.4" class="ltx_td ltx_align_center">14 GPS driving indicators</td>
<td id="S1.T1.2.1.4.5" class="ltx_td ltx_align_center">In-vehicle GPS</td>
<td id="S1.T1.2.1.4.6" class="ltx_td ltx_align_center">139 (64 AD, 75 non-AD)</td>
<td id="S1.T1.2.1.4.7" class="ltx_td ltx_align_center">One year</td>
</tr>
<tr id="S1.T1.2.1.5" class="ltx_tr">
<td id="S1.T1.2.1.5.1" class="ltx_td ltx_align_center">Alberdi et al. <cite class="ltx_cite ltx_citemacro_citep">(Alberdi et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S1.T1.2.1.5.2" class="ltx_td ltx_align_center">Interpretable, Centralized</td>
<td id="S1.T1.2.1.5.3" class="ltx_td ltx_align_center">No diagnosis results</td>
<td id="S1.T1.2.1.5.4" class="ltx_td ltx_align_center">Features of 5 events</td>
<td id="S1.T1.2.1.5.5" class="ltx_td ltx_align_center">PIR motion sensor</td>
<td id="S1.T1.2.1.5.6" class="ltx_td ltx_align_center">29 (6 AD, 10 MCI, 12 NC)</td>
<td id="S1.T1.2.1.5.7" class="ltx_td ltx_align_center">Two years</td>
</tr>
<tr id="S1.T1.2.1.6" class="ltx_tr">
<td id="S1.T1.2.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">ADMarker (Ours)</td>
<td id="S1.T1.2.1.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.2.1.6.2.1" class="ltx_text ltx_font_bold">Interpretable, Distributed</span></td>
<td id="S1.T1.2.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T1.2.1.6.3.1" class="ltx_text ltx_font_bold">88.9%</span> for MCI</td>
<td id="S1.T1.2.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">Features of <span id="S1.T1.2.1.6.4.1" class="ltx_text ltx_font_bold">22</span> activities</td>
<td id="S1.T1.2.1.6.5" class="ltx_td ltx_align_center ltx_border_bb">Audio, Depth, Radar</td>
<td id="S1.T1.2.1.6.6" class="ltx_td ltx_align_center ltx_border_bb">91 (31 AD, 30 MCI, 30 NC)</td>
<td id="S1.T1.2.1.6.7" class="ltx_td ltx_align_center ltx_border_bb">Four weeks</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of different AD digital biomarker studies. ADMarker is the first solution that detects a comprehensive set of multidimensional digital biomarkers for AD diagnosis in a privacy-preserving manner.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning (FL) algorithms for detecting multidimensional, more than 20 AD digital biomarkers in a privacy-preserving manner. The system features a novel three-stage FL architecture, where the nodes deployed in subjects’ homes leverage the pre-trained model to reduce the compute overhead of online training, and improve the model performance on their own data through multi-modal unsupervised and weakly supervised FL algorithms. This approach collectively addresses several real-world challenges, including limited labeled data, data heterogeneity and variation, and limited computing resources.
We have implemented ADMarker on a compact multi-modality sensor hardware system with three privacy-preserving sensors (i.e., a depth camera, a mmWave radar, and a microphone) to detect a comprehensive set of digital biomarkers in home environments.
To ensure durability and efficiency, the design of ADMarker addresses several practical challenges in the hardware and software system, including long-term multi-modal data recording, improving training and inference efficiency, and private communication networks.
</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We have deployed ADMarker in a four-week clinical trial that involves a total of 91 elderly subjects, including 31 with AD and 30 with mild cognitive impairment (MCI). The results show that, ADMarker can detect more than 20 daily activities in natural home environments with up to 93.8% detection accuracy, using only a very small amount of labeled data. Leveraging the detected digital biomarkers, we can achieve 88.9% accuracy in early AD diagnosis. ADMarker offers a new clinical tool that allows medical researchers and professionals to monitor the progression of AD manifestations and study the complex correlation between multidimensional interpretable digital biomarkers, demographic factors of patients, and AD diagnosis in a longitudinal manner.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Mobile Health Systems.</span>
Numerous mobile health systems have been proposed in recent years <cite class="ltx_cite ltx_citemacro_citep">(Silva et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>; Istepanian et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2007</a>; Chang
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>; Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2012</a>; Bui et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>.
Recently, several mobile systems are developed for the assessment or rehabilitation of neurodegenerative diseases.
For example, NeuralGait <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> captures the gait segments relationship for brain health assessment. PDlens <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite> uses the smartphone data for drug-effectiveness detection of Parkinson’s disease. However, these systems can only monitor very limited symptoms of the disease, e.g., the gait or sound of the subjects. ADMarker is the first system that can detect multi-dimensional AD digital biomarkers in a real-time and privacy-preserving manner.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Federated Learning for Human Activity Recognition.</span>
Machine learning algorithms have been increasingly adopted for Human Activity Recognition (HAR) <cite class="ltx_cite ltx_citemacro_citep">(Stisen et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2015</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>. Most work in this space is based on centralized learning that needs to train the algorithms at a central server, which imposes significant privacy concerns due to the need to share raw user data. Moreover, current HAR studies are focused on classifying a small number of activities in controlled environments.
Federated learning (FL) has been recently applied to HAR to improve the model accuracy without sharing the raw data <cite class="ltx_cite ltx_citemacro_citep">(Ouyang
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>, <a href="#bib.bib49" title="" class="ltx_ref">2022b</a>; Tu
et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite>.
However, most of the existing FL approaches are focused on training <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">unimodal</em> models with a single type of sensor data.
Several multi-modal federated learning schemes <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2022</a>; Ouyang et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> allow model training over distributed multi-modal data on the nodes. However, they have not addressed the unique challenges in real-world multi-modal federated learning, such as limited labels and modality heterogeneity. ADMarker is the first multi-modal FL system that incorporates novel unsupervised and weakly supervised multi-modal FL designs to detect a comprehensive set of human activities in natural living environments.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Digital Biomarkers for Alzheimer’s Disease.</span>
Recently, understanding AD using mobile sensor-based digital biomarkers has attracted tremendous attention <cite class="ltx_cite ltx_citemacro_citep">(Kourtis
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Luz et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>; Bayat et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>.
Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares several representative AD digital biomarker studies. In summary, these studies have two key major issues. First, they are focused on a very small number of activities, such as sitting, sleeping, and eating. As a result, the detected digital biomarkers are only applicable to partial population demographics or environments. Second, most of the studies are based on the black-box approach that directly infers the diagnosis from raw sensor data/features. Such an approach incurs extremely high computing overhead, as the amount of collected data over time can be huge (e.g., about 90T in our four-week clinical deployment). In addition, even if the diagnosis with raw sensor data is accurate, the results are difficult to interpret with respect to AD manifestations. As a result, they are difficult to be adopted in current practices of AD diagnosis and treatment that are largely based on observable cognitive and behavioral symptoms <cite class="ltx_cite ltx_citemacro_citep">(dig, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>; Kourtis
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2310.15301/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of ADMarker. ADMarker consists of three major components, i.e., a multi-modal sensor system, federated learning algorithms for biomarker detection, and AD analysis based on detected digital biomarkers.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>System Overview</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Motivation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Interpretable two-step AD monitoring.</span>
Instead of predicting AD with the raw multi-modal sensor data (i.e., black-box approaches in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), ADMarker disentangles the disease monitoring into two steps, i.e., digital biomarkers (AD symptoms) detection and early disease analysis with the detected biomarkers. As a result, the behavior biomarkers detected by ADMarker are more interpretable for AD diagnosis and can be adopted to design personalized intervention plans for different individuals.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Detecting multi-dimensional biomarkers.</span>
Compared with existing approaches focused on a particular type of digital biomarker <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; Luz et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>; Bayat et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, ADMarker aims to detect a comprehensive set of multi-dimensional AD symptoms, such as various activities of daily living and social interactions. This allows to investigate how the physical and social interactional etiopathogenesis shape AD’s manifestation. Furthermore, the integration of multi-dimensional digital biomarkers enables the precise detection of early AD, encompassing its diverse manifestations, which would be challenging when relying solely on a single type of biomarker <cite class="ltx_cite ltx_citemacro_citep">(Livingston et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>; Livingston
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Federated learning for privacy-preserving biomarker detection.</span> To continuously and accurately detect the biomarkers, ADMarker needs to accumulate data for a certain period of time and use it to train machine learning models for daily activity recognition.
In order to preserve users’ data privacy, ADMarker features a federated learning <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2016</a>; Ouyang
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> paradigm for biomarker detection, which only requires the nodes to upload model weights to avoid exposing users’ raw data during the learning process.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Challenges</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">ADMarker aims to address three major challenges:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The first challenge is to <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">utilize a large set of digital biomarkers for early AD diagnosis and intervention</em>. It is essential to select a comprehensive set of activities that are highly associated with different AD stages yet can be detected accurately during daily life. This requires extensive domain expertise in both medical and engineering areas.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The second challenge is to develop <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">a hardware system that can be rapidly deployed in real-world home environments</em> for longitudinal daily activity monitoring. For instance, in order to capture multi-dimensional digital biomarkers in a privacy-preserving manner, the hardware system should incorporate carefully selected sensors of different modalities.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The third challenge is to design <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">an effective multi-modal federated learning system that can accurately detect digital biomarkers</em> under real-world data and system dynamics. First, there usually exists no labels or only a very limited amount of labeled data in real-world settings because most sensor data is not intuitive for humans to label <cite class="ltx_cite ltx_citemacro_citep">(Shuai
et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2021</a>)</cite>.
Second, due to the significant diversity in behavior patterns and home environments, the data distribution of different subjects is usually non-i.i.d. and highly imbalanced. For instance, compared with cognitively normal subjects, AD patients tend to have a sedentary lifestyle. As a result, they may have very limited data for activities like “exercising” and “cleaning the living area”, while these activities are as important as frequently occurring activities like “sitting” for AD analysis.
Finally, there may be significant training latency in real-world FL systems due to the limited computing resources and dynamic bandwidth of nodes.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>System Architecture</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Our key idea is to leverage multi-modal sensor devices and federated learning algorithms to detect multi-dimensional AD digital biomarkers in natural home environments. Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overview of ADMarker.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We developed a compact multi-modality hardware system that can function for up to months in home environments to detect digital biomarkers of AD. It incorporates three privacy-preserving sensors (a depth camera, an mmWave radar, and a microphone), an NVIDIA single-board edge computer, and a 4G cellular interface that can communicate with the server. We address several practical challenges to make the hardware system durable, power-efficient, and privacy-preserving. On top of the hardware system, we design a new multi-modal federated learning (FL) system for biomarker detection while preserving users’ data privacy. The system features a novel three-stage architecture. At the first stage, we train a multi-modal model on the server using the labeled data from public datasets or the previous participants. Then, during the deployment, the nodes in the subjects’ homes will load the centrally pre-trained model, and improve the model performance on their own data through a two-stage FL process, i.e., multi-modal unsupervised and weakly supervised FL, respectively.
In weakly supervised FL, the nodes will only leverage <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">weak labels</em> generated from sparse activity logs of the participants, without resorting to labor-intensive manual annotation of raw sensor data, for local model training.
Our approach collectively addresses several major real-world challenges, including limited labeled data, data heterogeneity, and limited computing resources. Finally, the digital biomarkers are input into a neural network for AD diagnosis of different patients.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The design of ADMarker is extensively evaluated in a clinical deployment (see Section <a href="#S7" title="7. Clinical Deployment ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). A total of 91 elderly subjects (43 females and 48 males, 61-93 years old) were recruited for the study, including 31 with AD, 30 with mild cognitive impairment, and 30 cognitively normal subjects.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.2.1" class="ltx_tr">
<td id="S3.T2.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:13.0pt;">
<span id="S3.T2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.1.1.1" class="ltx_p"><span id="S3.T2.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Class</span></span>
<span id="S3.T2.2.1.1.1.2" class="ltx_p ltx_align_center"><span id="S3.T2.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Index</span></span>
</span>
</td>
<td id="S3.T2.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:121.4pt;">
<span id="S3.T2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.2.1.1" class="ltx_p"><span id="S3.T2.2.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Activities</span></span>
</span>
</td>
<td id="S3.T2.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:91.1pt;">
<span id="S3.T2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.3.1.1" class="ltx_p"><span id="S3.T2.2.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Captured</span></span>
<span id="S3.T2.2.1.3.1.2" class="ltx_p ltx_align_center"><span id="S3.T2.2.1.3.1.2.1" class="ltx_text" style="font-size:70%;">Sensors</span></span>
</span>
</td>
<td id="S3.T2.2.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:34.7pt;">
<span id="S3.T2.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.4.1.1" class="ltx_p"><span id="S3.T2.2.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Type</span></span>
</span>
</td>
<td id="S3.T2.2.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:86.7pt;">
<span id="S3.T2.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.5.1.1" class="ltx_p"><span id="S3.T2.2.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Decline at</span></span>
<span id="S3.T2.2.1.5.1.2" class="ltx_p ltx_align_center"><span id="S3.T2.2.1.5.1.2.1" class="ltx_text" style="font-size:70%;">which stage</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.1.1.1" class="ltx_p"><span id="S3.T2.2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">1</span></span>
</span>
</td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.2.1.1" class="ltx_p"><span id="S3.T2.2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">Out of Home</span></span>
</span>
</td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.3.1.1" class="ltx_p"><span id="S3.T2.2.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;"></td>
<td id="S3.T2.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.5.1.1" class="ltx_p"><span id="S3.T2.2.2.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild, Moderate</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.3" class="ltx_tr">
<td id="S3.T2.2.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.3.1.1.1" class="ltx_p"><span id="S3.T2.2.3.1.1.1.1" class="ltx_text" style="font-size:70%;">2</span></span>
</span>
</td>
<td id="S3.T2.2.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.3.2.1.1" class="ltx_p"><span id="S3.T2.2.3.2.1.1.1" class="ltx_text" style="font-size:70%;">Other activities</span></span>
</span>
</td>
<td id="S3.T2.2.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.3.3.1.1" class="ltx_p"><span id="S3.T2.2.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;"></td>
<td id="S3.T2.2.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;"></td>
</tr>
<tr id="S3.T2.2.4" class="ltx_tr">
<td id="S3.T2.2.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.4.1.1.1" class="ltx_p"><span id="S3.T2.2.4.1.1.1.1" class="ltx_text" style="font-size:70%;">3</span></span>
</span>
</td>
<td id="S3.T2.2.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.4.2.1.1" class="ltx_p"><span id="S3.T2.2.4.2.1.1.1" class="ltx_text" style="font-size:70%;">Dressing</span></span>
</span>
</td>
<td id="S3.T2.2.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.4.3.1.1" class="ltx_p"><span id="S3.T2.2.4.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.4.4.1.1" class="ltx_p"><span id="S3.T2.2.4.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.4.5.1.1" class="ltx_p"><span id="S3.T2.2.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.5" class="ltx_tr">
<td id="S3.T2.2.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.5.1.1.1" class="ltx_p"><span id="S3.T2.2.5.1.1.1.1" class="ltx_text" style="font-size:70%;">4</span></span>
</span>
</td>
<td id="S3.T2.2.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.5.2.1.1" class="ltx_p"><span id="S3.T2.2.5.2.1.1.1" class="ltx_text" style="font-size:70%;">Take/Put something</span></span>
</span>
</td>
<td id="S3.T2.2.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.5.3.1.1" class="ltx_p"><span id="S3.T2.2.5.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.5.4.1.1" class="ltx_p"><span id="S3.T2.2.5.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.5.5.1.1" class="ltx_p"><span id="S3.T2.2.5.5.1.1.1" class="ltx_text" style="font-size:70%;">Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.6" class="ltx_tr">
<td id="S3.T2.2.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.6.1.1.1" class="ltx_p"><span id="S3.T2.2.6.1.1.1.1" class="ltx_text" style="font-size:70%;">5</span></span>
</span>
</td>
<td id="S3.T2.2.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.6.2.1.1" class="ltx_p"><span id="S3.T2.2.6.2.1.1.1" class="ltx_text" style="font-size:70%;">Cleaning living area</span></span>
</span>
</td>
<td id="S3.T2.2.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.6.3.1.1" class="ltx_p"><span id="S3.T2.2.6.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.6.4.1.1" class="ltx_p"><span id="S3.T2.2.6.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.6.5.1.1" class="ltx_p"><span id="S3.T2.2.6.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.7" class="ltx_tr">
<td id="S3.T2.2.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.7.1.1.1" class="ltx_p"><span id="S3.T2.2.7.1.1.1.1" class="ltx_text" style="font-size:70%;">6</span></span>
</span>
</td>
<td id="S3.T2.2.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.7.2.1.1" class="ltx_p"><span id="S3.T2.2.7.2.1.1.1" class="ltx_text" style="font-size:70%;">Grooming</span></span>
</span>
</td>
<td id="S3.T2.2.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.7.3.1.1" class="ltx_p"><span id="S3.T2.2.7.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.7.4.1.1" class="ltx_p"><span id="S3.T2.2.7.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.7.5.1.1" class="ltx_p"><span id="S3.T2.2.7.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.8" class="ltx_tr">
<td id="S3.T2.2.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.8.1.1.1" class="ltx_p"><span id="S3.T2.2.8.1.1.1.1" class="ltx_text" style="font-size:70%;">7</span></span>
</span>
</td>
<td id="S3.T2.2.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.8.2.1.1" class="ltx_p"><span id="S3.T2.2.8.2.1.1.1" class="ltx_text" style="font-size:70%;">Wiping hands</span></span>
</span>
</td>
<td id="S3.T2.2.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.8.3.1.1" class="ltx_p"><span id="S3.T2.2.8.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.8.4.1.1" class="ltx_p"><span id="S3.T2.2.8.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.8.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.8.5.1.1" class="ltx_p"><span id="S3.T2.2.8.5.1.1.1" class="ltx_text" style="font-size:70%;">Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.9" class="ltx_tr">
<td id="S3.T2.2.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.9.1.1.1" class="ltx_p"><span id="S3.T2.2.9.1.1.1.1" class="ltx_text" style="font-size:70%;">8</span></span>
</span>
</td>
<td id="S3.T2.2.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.9.2.1.1" class="ltx_p"><span id="S3.T2.2.9.2.1.1.1" class="ltx_text" style="font-size:70%;">Drinking</span></span>
</span>
</td>
<td id="S3.T2.2.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.9.3.1.1" class="ltx_p"><span id="S3.T2.2.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.9.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.9.4.1.1" class="ltx_p"><span id="S3.T2.2.9.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.9.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.9.5.1.1" class="ltx_p"><span id="S3.T2.2.9.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.10" class="ltx_tr">
<td id="S3.T2.2.10.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.10.1.1.1" class="ltx_p"><span id="S3.T2.2.10.1.1.1.1" class="ltx_text" style="font-size:70%;">9</span></span>
</span>
</td>
<td id="S3.T2.2.10.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.10.2.1.1" class="ltx_p"><span id="S3.T2.2.10.2.1.1.1" class="ltx_text" style="font-size:70%;">Eating</span></span>
</span>
</td>
<td id="S3.T2.2.10.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.10.3.1.1" class="ltx_p"><span id="S3.T2.2.10.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.10.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.10.4.1.1" class="ltx_p"><span id="S3.T2.2.10.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.10.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.10.5.1.1" class="ltx_p"><span id="S3.T2.2.10.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.11" class="ltx_tr">
<td id="S3.T2.2.11.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.11.1.1.1" class="ltx_p"><span id="S3.T2.2.11.1.1.1.1" class="ltx_text" style="font-size:70%;">10</span></span>
</span>
</td>
<td id="S3.T2.2.11.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.11.2.1.1" class="ltx_p"><span id="S3.T2.2.11.2.1.1.1" class="ltx_text" style="font-size:70%;">Smoking</span></span>
</span>
</td>
<td id="S3.T2.2.11.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.11.3.1.1" class="ltx_p"><span id="S3.T2.2.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.11.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.11.4.1.1" class="ltx_p"><span id="S3.T2.2.11.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.11.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.11.5.1.1" class="ltx_p"><span id="S3.T2.2.11.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.12" class="ltx_tr">
<td id="S3.T2.2.12.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.12.1.1.1" class="ltx_p"><span id="S3.T2.2.12.1.1.1.1" class="ltx_text" style="font-size:70%;">11</span></span>
</span>
</td>
<td id="S3.T2.2.12.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.12.2.1.1" class="ltx_p"><span id="S3.T2.2.12.2.1.1.1" class="ltx_text" style="font-size:70%;">Sneezing/Coughing</span></span>
</span>
</td>
<td id="S3.T2.2.12.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.12.3.1.1" class="ltx_p"><span id="S3.T2.2.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.12.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.12.4.1.1" class="ltx_p"><span id="S3.T2.2.12.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.12.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.12.5.1.1" class="ltx_p"><span id="S3.T2.2.12.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.13" class="ltx_tr">
<td id="S3.T2.2.13.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.13.1.1.1" class="ltx_p"><span id="S3.T2.2.13.1.1.1.1" class="ltx_text" style="font-size:70%;">12</span></span>
</span>
</td>
<td id="S3.T2.2.13.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.13.2.1.1" class="ltx_p"><span id="S3.T2.2.13.2.1.1.1" class="ltx_text" style="font-size:70%;">Writing</span></span>
</span>
</td>
<td id="S3.T2.2.13.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.13.3.1.1" class="ltx_p"><span id="S3.T2.2.13.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.13.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.13.4.1.1" class="ltx_p"><span id="S3.T2.2.13.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.13.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.13.5.1.1" class="ltx_p"><span id="S3.T2.2.13.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.14" class="ltx_tr">
<td id="S3.T2.2.14.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.14.1.1.1" class="ltx_p"><span id="S3.T2.2.14.1.1.1.1" class="ltx_text" style="font-size:70%;">13</span></span>
</span>
</td>
<td id="S3.T2.2.14.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.14.2.1.1" class="ltx_p"><span id="S3.T2.2.14.2.1.1.1" class="ltx_text" style="font-size:70%;">Watching TV</span></span>
</span>
</td>
<td id="S3.T2.2.14.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.14.3.1.1" class="ltx_p"><span id="S3.T2.2.14.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.14.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.14.4.1.1" class="ltx_p"><span id="S3.T2.2.14.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.14.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.14.5.1.1" class="ltx_p"><span id="S3.T2.2.14.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.15" class="ltx_tr">
<td id="S3.T2.2.15.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.15.1.1.1" class="ltx_p"><span id="S3.T2.2.15.1.1.1.1" class="ltx_text" style="font-size:70%;">14</span></span>
</span>
</td>
<td id="S3.T2.2.15.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.15.2.1.1" class="ltx_p"><span id="S3.T2.2.15.2.1.1.1" class="ltx_text" style="font-size:70%;">Phone call/Using phone</span></span>
</span>
</td>
<td id="S3.T2.2.15.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.15.3.1.1" class="ltx_p"><span id="S3.T2.2.15.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.15.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.15.4.1.1" class="ltx_p"><span id="S3.T2.2.15.4.1.1.1" class="ltx_text" style="font-size:70%;">SI</span></span>
</span>
</td>
<td id="S3.T2.2.15.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.15.5.1.1" class="ltx_p"><span id="S3.T2.2.15.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.16" class="ltx_tr">
<td id="S3.T2.2.16.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.16.1.1.1" class="ltx_p"><span id="S3.T2.2.16.1.1.1.1" class="ltx_text" style="font-size:70%;">15</span></span>
</span>
</td>
<td id="S3.T2.2.16.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.16.2.1.1" class="ltx_p"><span id="S3.T2.2.16.2.1.1.1" class="ltx_text" style="font-size:70%;">Exercising</span></span>
</span>
</td>
<td id="S3.T2.2.16.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.16.3.1.1" class="ltx_p"><span id="S3.T2.2.16.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.16.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.16.4.1.1" class="ltx_p"><span id="S3.T2.2.16.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.16.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.16.5.1.1" class="ltx_p"><span id="S3.T2.2.16.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.17" class="ltx_tr">
<td id="S3.T2.2.17.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.17.1.1.1" class="ltx_p"><span id="S3.T2.2.17.1.1.1.1" class="ltx_text" style="font-size:70%;">16</span></span>
</span>
</td>
<td id="S3.T2.2.17.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.17.2.1.1" class="ltx_p"><span id="S3.T2.2.17.2.1.1.1" class="ltx_text" style="font-size:70%;">Talking with others</span></span>
</span>
</td>
<td id="S3.T2.2.17.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.17.3.1.1" class="ltx_p"><span id="S3.T2.2.17.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar, Mic</span></span>
</span>
</td>
<td id="S3.T2.2.17.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.17.4.1.1" class="ltx_p"><span id="S3.T2.2.17.4.1.1.1" class="ltx_text" style="font-size:70%;">SI</span></span>
</span>
</td>
<td id="S3.T2.2.17.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.17.5.1.1" class="ltx_p"><span id="S3.T2.2.17.5.1.1.1" class="ltx_text" style="font-size:70%;">Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.18" class="ltx_tr">
<td id="S3.T2.2.18.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.18.1.1.1" class="ltx_p"><span id="S3.T2.2.18.1.1.1.1" class="ltx_text" style="font-size:70%;">17</span></span>
</span>
</td>
<td id="S3.T2.2.18.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.18.2.1.1" class="ltx_p"><span id="S3.T2.2.18.2.1.1.1" class="ltx_text" style="font-size:70%;">Stretching</span></span>
</span>
</td>
<td id="S3.T2.2.18.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.18.3.1.1" class="ltx_p"><span id="S3.T2.2.18.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.18.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.18.4.1.1" class="ltx_p"><span id="S3.T2.2.18.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.18.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.18.5.1.1" class="ltx_p"><span id="S3.T2.2.18.5.1.1.1" class="ltx_text" style="font-size:70%;">Mild, Moderate</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.19" class="ltx_tr">
<td id="S3.T2.2.19.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.19.1.1.1" class="ltx_p"><span id="S3.T2.2.19.1.1.1.1" class="ltx_text" style="font-size:70%;">18</span></span>
</span>
</td>
<td id="S3.T2.2.19.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.19.2.1.1" class="ltx_p"><span id="S3.T2.2.19.2.1.1.1" class="ltx_text" style="font-size:70%;">Walking</span></span>
</span>
</td>
<td id="S3.T2.2.19.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.19.3.1.1" class="ltx_p"><span id="S3.T2.2.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.19.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.19.4.1.1" class="ltx_p"><span id="S3.T2.2.19.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.19.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.19.5.1.1" class="ltx_p"><span id="S3.T2.2.19.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.20" class="ltx_tr">
<td id="S3.T2.2.20.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.20.1.1.1" class="ltx_p"><span id="S3.T2.2.20.1.1.1.1" class="ltx_text" style="font-size:70%;">19</span></span>
</span>
</td>
<td id="S3.T2.2.20.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.20.2.1.1" class="ltx_p"><span id="S3.T2.2.20.2.1.1.1" class="ltx_text" style="font-size:70%;">Sitting</span></span>
</span>
</td>
<td id="S3.T2.2.20.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.20.3.1.1" class="ltx_p"><span id="S3.T2.2.20.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.20.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.20.4.1.1" class="ltx_p"><span id="S3.T2.2.20.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.20.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.20.5.1.1" class="ltx_p"><span id="S3.T2.2.20.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.21" class="ltx_tr">
<td id="S3.T2.2.21.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.21.1.1.1" class="ltx_p"><span id="S3.T2.2.21.1.1.1.1" class="ltx_text" style="font-size:70%;">20</span></span>
</span>
</td>
<td id="S3.T2.2.21.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.21.2.1.1" class="ltx_p"><span id="S3.T2.2.21.2.1.1.1" class="ltx_text" style="font-size:70%;">Standing</span></span>
</span>
</td>
<td id="S3.T2.2.21.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.21.3.1.1" class="ltx_p"><span id="S3.T2.2.21.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.21.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.21.4.1.1" class="ltx_p"><span id="S3.T2.2.21.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.21.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.21.5.1.1" class="ltx_p"><span id="S3.T2.2.21.5.1.1.1" class="ltx_text" style="font-size:70%;">Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.22" class="ltx_tr">
<td id="S3.T2.2.22.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.22.1.1.1" class="ltx_p"><span id="S3.T2.2.22.1.1.1.1" class="ltx_text" style="font-size:70%;">21</span></span>
</span>
</td>
<td id="S3.T2.2.22.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.22.2.1.1" class="ltx_p"><span id="S3.T2.2.22.2.1.1.1" class="ltx_text" style="font-size:70%;">Lying</span></span>
</span>
</td>
<td id="S3.T2.2.22.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.22.3.1.1" class="ltx_p"><span id="S3.T2.2.22.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.22.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.22.4.1.1" class="ltx_p"><span id="S3.T2.2.22.4.1.1.1" class="ltx_text" style="font-size:70%;">BADL</span></span>
</span>
</td>
<td id="S3.T2.2.22.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.22.5.1.1" class="ltx_p"><span id="S3.T2.2.22.5.1.1.1" class="ltx_text" style="font-size:70%;">Moderate, Severe</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.23" class="ltx_tr">
<td id="S3.T2.2.23.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:13.0pt;">
<span id="S3.T2.2.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.23.1.1.1" class="ltx_p"><span id="S3.T2.2.23.1.1.1.1" class="ltx_text" style="font-size:70%;">22</span></span>
</span>
</td>
<td id="S3.T2.2.23.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:121.4pt;">
<span id="S3.T2.2.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.23.2.1.1" class="ltx_p"><span id="S3.T2.2.23.2.1.1.1" class="ltx_text" style="font-size:70%;">Moving in/out of chair</span></span>
</span>
</td>
<td id="S3.T2.2.23.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:91.1pt;">
<span id="S3.T2.2.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.23.3.1.1" class="ltx_p"><span id="S3.T2.2.23.3.1.1.1" class="ltx_text" style="font-size:70%;">Depth, Radar</span></span>
</span>
</td>
<td id="S3.T2.2.23.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:34.7pt;">
<span id="S3.T2.2.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.23.4.1.1" class="ltx_p"><span id="S3.T2.2.23.4.1.1.1" class="ltx_text" style="font-size:70%;">IADL</span></span>
</span>
</td>
<td id="S3.T2.2.23.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:86.7pt;">
<span id="S3.T2.2.23.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.23.5.1.1" class="ltx_p"><span id="S3.T2.2.23.5.1.1.1" class="ltx_text" style="font-size:70%;">Severe</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text" style="font-size:129%;">Table 2</span>. </span><span id="S3.T2.6.2" class="ltx_text" style="font-size:129%;"> Selected AD digital biomarkers. BADL: Basic Activities of Daily Living; IADL: Instrumental Activities of Daily Living; SI: Social Interaction.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Biomarker Selection</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We select a total of 22 activities of interest that are shown to be highly related to AD from medical literature <cite class="ltx_cite ltx_citemacro_citep">(Berkman
et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2000</a>; Marique
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2005</a>; Marshall et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2006</a>; Altieri
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>. ADMarker will detect these activities and use the duration and frequency of the activities as potential digital biomarkers for diagnostic analysis (see Section <a href="#S8.SS4" title="8.4. Interpretation of Detected Biomarkers ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8.4</span></a>).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.3. System Architecture ‣ 3. System Overview ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the activities in the biomarker set, which can be categorized into three types: <em id="S3.SS4.p2.1.1" class="ltx_emph ltx_font_italic">basic activities of daily living (BADLs)</em>, <em id="S3.SS4.p2.1.2" class="ltx_emph ltx_font_italic">instrumental activities of daily living (IADLs)</em>, and <em id="S3.SS4.p2.1.3" class="ltx_emph ltx_font_italic">social interactions (SI)</em>.
BADLs include basic self-care tasks, such as eating, drinking, and walking, while IADLs encompass complex tasks that allow for independent living, like cleaning the living area and grooming <cite class="ltx_cite ltx_citemacro_citep">(Altieri
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Reed et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2016</a>)</cite>. AD patients who suffer from decreased ability in ADLs and social interactions are more likely to exhibit amyloid plaques and neurofibrillary tangles in several brain regions <cite class="ltx_cite ltx_citemacro_citep">(Marshall et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2006</a>; Marique
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2005</a>)</cite>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Moreover, the last column in Table <a href="#S3.T2" title="Table 2 ‣ 3.3. System Architecture ‣ 3. System Overview ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the stages of the disease where these activities become difficult for the patients. At the mild stage of AD, activities like watching TV <cite class="ltx_cite ltx_citemacro_citep">(Gústafsdóttir, <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite> and using phone <cite class="ltx_cite ltx_citemacro_citep">(Yamada et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2021</a>)</cite> become challenging due to early signs of cognitive decline. When the disease progresses to moderate and severe stages, the patients will suffer comprehensive functional deterioration, and are difficult to perform basic activities like standing <cite class="ltx_cite ltx_citemacro_citep">(Montero-Odasso et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2012</a>)</cite>, exercising<cite class="ltx_cite ltx_citemacro_citep">(Miu
et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2008</a>)</cite>, and talking with others <cite class="ltx_cite ltx_citemacro_citep">(McNamara et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">1992</a>)</cite>.
Therefore, the digital biomarkers selected above are not only interpretable with respect to AD manifestations, but also can be readily applied in intervention plans. For example, by monitoring the subjects’ daily activities during the progression of AD, the duration and intensity of exercise can be prescribed for personalized intervention, which leads to iterative and more effective treatment.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Federated Learning for Biomarker Detection</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>A Motivation Study</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Understanding the real-world challenges.</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">We first analyze the sensor data and system log recorded by our ADMarker testbed (see Section <a href="#S6" title="6. System Implementation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) in a four-week clinical deployment to understand the real-world challenges.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.15301/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Examples of data distributions of participants in a real-world deployment. The data distribution is highly imbalanced among different classes and non-i.i.d across different participants.</span></figcaption>
</figure>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p"><span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Data challenges.</span> Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the examples of data distributions of participants over the four weeks of the clinical deployment (see Section <a href="#S7" title="7. Clinical Deployment ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).
First, the amount of data from different activities is highly imbalanced. For example, in the daily living activities of Subject 3, the ratio of samples from class 1 (Out of Home, 81.78%) is very large, while that from class 15 (Exercising, 1.54%) is small.
The imbalanced activity distribution will lead to a model bias on head classes, and a significant classification accuracy drop on minority tail classes <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2008</a>; Shuai
et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>. Second, the distributions of different subjects’ data are non-i.i.d. For example, the number of occurred activities and their ratios are very different between Subject 1 (cognitively normal) and Subject 3 (AD). The non-i.i.d data distribution across nodes will reduce the accuracy performance during FL <cite class="ltx_cite ltx_citemacro_citep">(Ouyang
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>, <a href="#bib.bib49" title="" class="ltx_ref">2022b</a>)</cite>. For example, the accuracy of federated learning reduces by up to 55% for models trained for highly skewed non-i.i.d data <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2018</a>)</cite>.
Finally, there usually exists no labels or only a very small amount of labeled data in real-world settings because the sensor data (depth images, radar point cloud, and MFCC audio features in ADMarker) is not intuitive for humans to label (as shown in Figure <a href="#S4.F7.sf3" title="In Figure 7 ‣ 4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(c)</span></a>).</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2310.15301/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Times of sensor down and bandwidth of all nodes over four weeks of the real-world deployment. </span></figcaption>
</figure>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p"><span id="S4.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">System dynamics.</span> We then evaluate the long-term system dynamics using the system log recorded during the real-world deployment. The results are shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The upper sub-figure shows the down times of sensors (per day) of the ADMarker prototype over four weeks, where each box shows the statistics of all nodes in the day.
During the 4-week deployment, the sensors may stop data recording occasionally (about 2-6 times per day) due to the system dynamics, such as power surges or unstable sensor connections, resulting in heterogeneous sensor modalities across different nodes in FL. The lower sub-figure shows the upload bandwidth of nodes (with 4G LTE networks) over different days, which fluctuates in a significant dynamic range (e.g., 0-20 Mbps).
Such bandwidth dynamics will result in various communication delays in transmitting model weights in federated learning.
</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:111.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(55.4pt,-14.2pt) scale(1.34282109560689,1.34282109560689) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<td id="S4.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Approach</td>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Pre-trained</td>
<td id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Supervised</td>
<td id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Semi-supervised</td>
</tr>
<tr id="S4.T3.2.1.2" class="ltx_tr">
<td id="S4.T3.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Training data</td>
<td id="S4.T3.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T3.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Label</td>
<td id="S4.T3.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Unlabel+Label</td>
</tr>
<tr id="S4.T3.2.1.3" class="ltx_tr">
<td id="S4.T3.2.1.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.3.1.1" class="ltx_text"></span> <span id="S4.T3.2.1.3.1.2" class="ltx_text">
<span id="S4.T3.2.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.2.1.3.1.2.1.1" class="ltx_tr">
<span id="S4.T3.2.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Testing Accuracy</span></span>
</span></span><span id="S4.T3.2.1.3.1.3" class="ltx_text"></span></td>
<td id="S4.T3.2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">18.75%</td>
<td id="S4.T3.2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">59.37%</td>
<td id="S4.T3.2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">80.62%</td>
</tr>
<tr id="S4.T3.2.1.4" class="ltx_tr">
<td id="S4.T3.2.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T3.2.1.4.1.1" class="ltx_text"></span> <span id="S4.T3.2.1.4.1.2" class="ltx_text">
<span id="S4.T3.2.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.2.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T3.2.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Training Latency (h)</span></span>
</span></span><span id="S4.T3.2.1.4.1.3" class="ltx_text"></span></td>
<td id="S4.T3.2.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0</td>
<td id="S4.T3.2.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4.14</td>
<td id="S4.T3.2.1.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">85.83 + 2.07</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Model performance of three different learning approaches. “Pre-trained” means directly applying the pe-trained model to the target subject. “Supervised” means learning from scratch with limited labeled data from the subject. “Semi-supervised” trains the models with both unlabeled and limited labeled data. The training latency is measured during the on-device model training on the ADMarker prototype.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Performance of different learning approaches.</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We then evaluate the performance of different traditional learning approaches to motivate the design of our federated learning architecture. The task is to classify 22 daily living activities related to Alzheimer’s Disease using audio, depth, and radar data. The testing data is collected from a subject over four weeks, with a total of 500 samples. There are 200 labeled training samples and 4,000 unlabeled training samples from the target subjects. The pre-trained model is trained using labeled data (3,728 samples) from another nine subjects.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the model testing accuracy and on-device training latency of different training schemes. First, directly applying the pre-trained model to the new subject results in a low testing accuracy (i.e., only 18.75%), which shows that there is a huge domain gap among the data of different subjects. Second, when there is only limited labeled training data (the supervised approach), the model accuracy is still unsatisfactory, i.e., 59.37%. And the model accuracy can be improved by unsupervised multi-modal learning that leverages large amounts of unlabeled data from the subject (the semi-supervised approach). However, when learning from scratch, unsupervised model training with large amounts of unlabeled data will incur significant computing overhead on the edge device, i.e., about 85 hours.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Design Overview: A Three-Stage Federated Learning Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Motivated by the case study, we propose a novel three-stage federated learning architecture that integrates a pre-trained model on the cloud and leverages both unlabeled and labeled data on nodes deployed in the subjects’ homes. As shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2. Design Overview: A Three-Stage Federated Learning Architecture ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, at the first stage, we train a multi-modal model using the labeled data on the server.
Such training data can leverage the public data sets that have already been made public. For instance, the data collected from this work will be made public and utilized for model pre-training in future deployments.
Then, during the deployment, the nodes in the subjects’ homes will load the centrally pre-trained model, and improve the model performance on their own data through two-stage federated learning, i.e., unsupervised multi-modal FL and weakly supervised multi-modal FL, respectively.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.15301/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">The three-stage federated learning architecture of ADMarker. Stage 1: Centralized model pre-training; Stage 2: Unsupervised multi-modal FL; Stage 3: Weakly supervised multi-modal FL.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The reasons for the three-stage training design are as follows. First, the model trained on the cloud needs to be retrained to adapt to the local data of each subject. Second, training the multi-modal network from scratch with noisy real-world data is an extremely challenging task in FL settings. The pre-trained model can reduce the computing overhead of online FL training.
Third, there usually exists a very small amount of labeled data, as it is difficult to label multi-modal data in real-world settings. Therefore, the nodes perform unsupervised FL to leverage the large amounts of unlabeled multi-modal data, and weakly supervised FL based on limited weakly labeled data. The weak labels for supervised local training are provided by the participants (see Section <a href="#S4.SS4" title="4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). For example, marking the time of having lunch would automatically label the multi-modal data during lunch.
In the following, we focus on the framework of unsupervised and supervised multi-modal federated learning.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Unsupervised Multi-Modal FL</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this stage, the nodes will download the pre-trained model from the server, and collaboratively train feature encoder networks of different data modalities (i.e., depth images, audio, and radar data) using the collected unlabeled sensor data during deployment. There are two main challenges during the unsupervised federated learning stage. First, the sensor modalities produce highly heterogeneous information about the same events/activities. For example, the audio features and radar data have significantly different dimensions and patterns, making it challenging to extract useful information. Second, the sensor modalities available on different nodes may vary due to the deployment constraints or runtime system dynamics. For example, some families may not be willing to have depth cameras installed in the bedroom, and the sensors may fail dynamically, e.g., due to power surges. Therefore, the design of unsupervised multi-modal FL should adapt to different modality combinations.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To address these challenges, the nodes will run fusion-based contrastive learning that trains the feature encoders by exploring the consistent information of heterogeneous data modalities. The server will aggregate the feature encoders of nodes with heterogeneous data modalities through modality-wise federated averaging. Compared with traditional multi-modal FL approaches, the key advantage of this idea is that it is oblivious to differences of modalities on nodes.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.15301/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Contrastive fusion learning on nodes. Through contrastive learning on the extensively augmented fused features, the uni-modal feature encoders are trained to capture consistent information from unlabeled multi-modal data.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p"><span id="S4.SS3.p3.2.1" class="ltx_text ltx_font_bold">Contrastive fusion learning on nodes.</span> During local training, a fusion-based feature augmentation module <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2022a</a>)</cite> will extensively augment the uni-modal features (extracted by encoders of different modalities) to a group of fused features via weighted sum or concatenation.
As shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3. Unsupervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, each augmented feature represents a different fusion combination of the sensor features and contains some subset of information in the original data sample.
Let <math id="S4.SS3.p3.1.m1.4" class="ltx_Math" alttext="s\in\emph{S}\equiv\{1,2,...,P\times N\}" display="inline"><semantics id="S4.SS3.p3.1.m1.4a"><mrow id="S4.SS3.p3.1.m1.4.4" xref="S4.SS3.p3.1.m1.4.4.cmml"><mi id="S4.SS3.p3.1.m1.4.4.3" xref="S4.SS3.p3.1.m1.4.4.3.cmml">s</mi><mo id="S4.SS3.p3.1.m1.4.4.4" xref="S4.SS3.p3.1.m1.4.4.4.cmml">∈</mo><mtext class="ltx_mathvariant_italic" id="S4.SS3.p3.1.m1.4.4.5" xref="S4.SS3.p3.1.m1.4.4.5b.cmml"><em id="S4.SS3.p3.1.m1.4.4.5.1nest" class="ltx_emph ltx_font_italic">S</em></mtext><mo id="S4.SS3.p3.1.m1.4.4.6" xref="S4.SS3.p3.1.m1.4.4.6.cmml">≡</mo><mrow id="S4.SS3.p3.1.m1.4.4.1.1" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml"><mo stretchy="false" id="S4.SS3.p3.1.m1.4.4.1.1.2" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml">{</mo><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">1</mn><mo id="S4.SS3.p3.1.m1.4.4.1.1.3" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml">,</mo><mn id="S4.SS3.p3.1.m1.2.2" xref="S4.SS3.p3.1.m1.2.2.cmml">2</mn><mo id="S4.SS3.p3.1.m1.4.4.1.1.4" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml">,</mo><mi mathvariant="normal" id="S4.SS3.p3.1.m1.3.3" xref="S4.SS3.p3.1.m1.3.3.cmml">…</mi><mo id="S4.SS3.p3.1.m1.4.4.1.1.5" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml">,</mo><mrow id="S4.SS3.p3.1.m1.4.4.1.1.1" xref="S4.SS3.p3.1.m1.4.4.1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.4.4.1.1.1.2" xref="S4.SS3.p3.1.m1.4.4.1.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p3.1.m1.4.4.1.1.1.1" xref="S4.SS3.p3.1.m1.4.4.1.1.1.1.cmml">×</mo><mi id="S4.SS3.p3.1.m1.4.4.1.1.1.3" xref="S4.SS3.p3.1.m1.4.4.1.1.1.3.cmml">N</mi></mrow><mo stretchy="false" id="S4.SS3.p3.1.m1.4.4.1.1.6" xref="S4.SS3.p3.1.m1.4.4.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.4b"><apply id="S4.SS3.p3.1.m1.4.4.cmml" xref="S4.SS3.p3.1.m1.4.4"><and id="S4.SS3.p3.1.m1.4.4a.cmml" xref="S4.SS3.p3.1.m1.4.4"></and><apply id="S4.SS3.p3.1.m1.4.4b.cmml" xref="S4.SS3.p3.1.m1.4.4"><in id="S4.SS3.p3.1.m1.4.4.4.cmml" xref="S4.SS3.p3.1.m1.4.4.4"></in><ci id="S4.SS3.p3.1.m1.4.4.3.cmml" xref="S4.SS3.p3.1.m1.4.4.3">𝑠</ci><ci id="S4.SS3.p3.1.m1.4.4.5b.cmml" xref="S4.SS3.p3.1.m1.4.4.5"><mtext class="ltx_mathvariant_italic" id="S4.SS3.p3.1.m1.4.4.5.cmml" xref="S4.SS3.p3.1.m1.4.4.5"><em id="S4.SS3.p3.1.m1.4.4.5.1anest" class="ltx_emph ltx_font_italic">S</em></mtext></ci></apply><apply id="S4.SS3.p3.1.m1.4.4c.cmml" xref="S4.SS3.p3.1.m1.4.4"><equivalent id="S4.SS3.p3.1.m1.4.4.6.cmml" xref="S4.SS3.p3.1.m1.4.4.6"></equivalent><share href="#S4.SS3.p3.1.m1.4.4.5.cmml" id="S4.SS3.p3.1.m1.4.4d.cmml" xref="S4.SS3.p3.1.m1.4.4"></share><set id="S4.SS3.p3.1.m1.4.4.1.2.cmml" xref="S4.SS3.p3.1.m1.4.4.1.1"><cn type="integer" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">1</cn><cn type="integer" id="S4.SS3.p3.1.m1.2.2.cmml" xref="S4.SS3.p3.1.m1.2.2">2</cn><ci id="S4.SS3.p3.1.m1.3.3.cmml" xref="S4.SS3.p3.1.m1.3.3">…</ci><apply id="S4.SS3.p3.1.m1.4.4.1.1.1.cmml" xref="S4.SS3.p3.1.m1.4.4.1.1.1"><times id="S4.SS3.p3.1.m1.4.4.1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.4.4.1.1.1.1"></times><ci id="S4.SS3.p3.1.m1.4.4.1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.4.4.1.1.1.2">𝑃</ci><ci id="S4.SS3.p3.1.m1.4.4.1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.4.4.1.1.1.3">𝑁</ci></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.4c">s\in\emph{S}\equiv\{1,2,...,P\times N\}</annotation></semantics></math> be the index of an arbitrary augmented feature, and let <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="p\in P(s)" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.2" xref="S4.SS3.p3.2.m2.1.2.cmml"><mi id="S4.SS3.p3.2.m2.1.2.2" xref="S4.SS3.p3.2.m2.1.2.2.cmml">p</mi><mo id="S4.SS3.p3.2.m2.1.2.1" xref="S4.SS3.p3.2.m2.1.2.1.cmml">∈</mo><mrow id="S4.SS3.p3.2.m2.1.2.3" xref="S4.SS3.p3.2.m2.1.2.3.cmml"><mi id="S4.SS3.p3.2.m2.1.2.3.2" xref="S4.SS3.p3.2.m2.1.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.2.m2.1.2.3.1" xref="S4.SS3.p3.2.m2.1.2.3.1.cmml">​</mo><mrow id="S4.SS3.p3.2.m2.1.2.3.3.2" xref="S4.SS3.p3.2.m2.1.2.3.cmml"><mo stretchy="false" id="S4.SS3.p3.2.m2.1.2.3.3.2.1" xref="S4.SS3.p3.2.m2.1.2.3.cmml">(</mo><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">s</mi><mo stretchy="false" id="S4.SS3.p3.2.m2.1.2.3.3.2.2" xref="S4.SS3.p3.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.2.cmml" xref="S4.SS3.p3.2.m2.1.2"><in id="S4.SS3.p3.2.m2.1.2.1.cmml" xref="S4.SS3.p3.2.m2.1.2.1"></in><ci id="S4.SS3.p3.2.m2.1.2.2.cmml" xref="S4.SS3.p3.2.m2.1.2.2">𝑝</ci><apply id="S4.SS3.p3.2.m2.1.2.3.cmml" xref="S4.SS3.p3.2.m2.1.2.3"><times id="S4.SS3.p3.2.m2.1.2.3.1.cmml" xref="S4.SS3.p3.2.m2.1.2.3.1"></times><ci id="S4.SS3.p3.2.m2.1.2.3.2.cmml" xref="S4.SS3.p3.2.m2.1.2.3.2">𝑃</ci><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">p\in P(s)</annotation></semantics></math> be the index of the other augmented features originating from the same source sample.
The contrastive fusion loss can be defined as:
</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.7" class="ltx_Math" alttext="\mathcal{L}_{conf}=\sum_{s\in\emph{S}}\frac{-1}{|P(s)|}\sum_{p\in P(s)}log\frac{exp(\mathbf{v}_{s}\cdot\mathbf{v}_{p}/\tau)}{\sum_{a\in\emph{S}\backslash\{s\}}exp(\mathbf{v}_{s}\cdot\mathbf{v}_{a}/\tau)}." display="block"><semantics id="S4.E1.m1.7a"><mrow id="S4.E1.m1.7.7.1" xref="S4.E1.m1.7.7.1.1.cmml"><mrow id="S4.E1.m1.7.7.1.1" xref="S4.E1.m1.7.7.1.1.cmml"><msub id="S4.E1.m1.7.7.1.1.2" xref="S4.E1.m1.7.7.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.7.7.1.1.2.2" xref="S4.E1.m1.7.7.1.1.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.7.7.1.1.2.3" xref="S4.E1.m1.7.7.1.1.2.3.cmml"><mi id="S4.E1.m1.7.7.1.1.2.3.2" xref="S4.E1.m1.7.7.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.3.1" xref="S4.E1.m1.7.7.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.7.7.1.1.2.3.3" xref="S4.E1.m1.7.7.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.3.1a" xref="S4.E1.m1.7.7.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.7.7.1.1.2.3.4" xref="S4.E1.m1.7.7.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.3.1b" xref="S4.E1.m1.7.7.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.7.7.1.1.2.3.5" xref="S4.E1.m1.7.7.1.1.2.3.5.cmml">f</mi></mrow></msub><mo rspace="0.111em" id="S4.E1.m1.7.7.1.1.1" xref="S4.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.7.7.1.1.3" xref="S4.E1.m1.7.7.1.1.3.cmml"><munder id="S4.E1.m1.7.7.1.1.3.1" xref="S4.E1.m1.7.7.1.1.3.1.cmml"><mo movablelimits="false" id="S4.E1.m1.7.7.1.1.3.1.2" xref="S4.E1.m1.7.7.1.1.3.1.2.cmml">∑</mo><mrow id="S4.E1.m1.7.7.1.1.3.1.3" xref="S4.E1.m1.7.7.1.1.3.1.3.cmml"><mi id="S4.E1.m1.7.7.1.1.3.1.3.2" xref="S4.E1.m1.7.7.1.1.3.1.3.2.cmml">s</mi><mo id="S4.E1.m1.7.7.1.1.3.1.3.1" xref="S4.E1.m1.7.7.1.1.3.1.3.1.cmml">∈</mo><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.7.7.1.1.3.1.3.3" xref="S4.E1.m1.7.7.1.1.3.1.3.3b.cmml"><em id="S4.E1.m1.7.7.1.1.3.1.3.3.1nest" class="ltx_emph ltx_font_italic" style="font-size:70%;">S</em></mtext></mrow></munder><mrow id="S4.E1.m1.7.7.1.1.3.2" xref="S4.E1.m1.7.7.1.1.3.2.cmml"><mfrac id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mrow id="S4.E1.m1.2.2.4" xref="S4.E1.m1.2.2.4.cmml"><mo id="S4.E1.m1.2.2.4a" xref="S4.E1.m1.2.2.4.cmml">−</mo><mn id="S4.E1.m1.2.2.4.2" xref="S4.E1.m1.2.2.4.2.cmml">1</mn></mrow><mrow id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.2.2" xref="S4.E1.m1.2.2.2.3.1.cmml">|</mo><mrow id="S4.E1.m1.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.1.cmml"><mi id="S4.E1.m1.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.1.1.cmml">​</mo><mrow id="S4.E1.m1.2.2.2.2.1.3.2" xref="S4.E1.m1.2.2.2.2.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.3.2.1" xref="S4.E1.m1.2.2.2.2.1.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.3.2.2" xref="S4.E1.m1.2.2.2.2.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E1.m1.2.2.2.2.3" xref="S4.E1.m1.2.2.2.3.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.3.2.1" xref="S4.E1.m1.7.7.1.1.3.2.1.cmml">​</mo><mrow id="S4.E1.m1.7.7.1.1.3.2.2" xref="S4.E1.m1.7.7.1.1.3.2.2.cmml"><munder id="S4.E1.m1.7.7.1.1.3.2.2.1" xref="S4.E1.m1.7.7.1.1.3.2.2.1.cmml"><mo movablelimits="false" id="S4.E1.m1.7.7.1.1.3.2.2.1.2" xref="S4.E1.m1.7.7.1.1.3.2.2.1.2.cmml">∑</mo><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.cmml"><mi id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.3.3.1.3.cmml">p</mi><mo id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.2.cmml">∈</mo><mrow id="S4.E1.m1.3.3.1.4" xref="S4.E1.m1.3.3.1.4.cmml"><mi id="S4.E1.m1.3.3.1.4.2" xref="S4.E1.m1.3.3.1.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.4.1" xref="S4.E1.m1.3.3.1.4.1.cmml">​</mo><mrow id="S4.E1.m1.3.3.1.4.3.2" xref="S4.E1.m1.3.3.1.4.cmml"><mo stretchy="false" id="S4.E1.m1.3.3.1.4.3.2.1" xref="S4.E1.m1.3.3.1.4.cmml">(</mo><mi id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml">s</mi><mo stretchy="false" id="S4.E1.m1.3.3.1.4.3.2.2" xref="S4.E1.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S4.E1.m1.7.7.1.1.3.2.2.2" xref="S4.E1.m1.7.7.1.1.3.2.2.2.cmml"><mi id="S4.E1.m1.7.7.1.1.3.2.2.2.2" xref="S4.E1.m1.7.7.1.1.3.2.2.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.3.2.2.2.1" xref="S4.E1.m1.7.7.1.1.3.2.2.2.1.cmml">​</mo><mi id="S4.E1.m1.7.7.1.1.3.2.2.2.3" xref="S4.E1.m1.7.7.1.1.3.2.2.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.3.2.2.2.1a" xref="S4.E1.m1.7.7.1.1.3.2.2.2.1.cmml">​</mo><mi id="S4.E1.m1.7.7.1.1.3.2.2.2.4" xref="S4.E1.m1.7.7.1.1.3.2.2.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.3.2.2.2.1b" xref="S4.E1.m1.7.7.1.1.3.2.2.2.1.cmml">​</mo><mfrac id="S4.E1.m1.6.6" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.cmml"><mi id="S4.E1.m1.4.4.1.3" xref="S4.E1.m1.4.4.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.2" xref="S4.E1.m1.4.4.1.2.cmml">​</mo><mi id="S4.E1.m1.4.4.1.4" xref="S4.E1.m1.4.4.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.2a" xref="S4.E1.m1.4.4.1.2.cmml">​</mo><mi id="S4.E1.m1.4.4.1.5" xref="S4.E1.m1.4.4.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.2b" xref="S4.E1.m1.4.4.1.2.cmml">​</mo><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.2.cmml"><msub id="S4.E1.m1.4.4.1.1.1.1.2.2" xref="S4.E1.m1.4.4.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.2.2.2" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml">𝐯</mi><mi id="S4.E1.m1.4.4.1.1.1.1.2.2.3" xref="S4.E1.m1.4.4.1.1.1.1.2.2.3.cmml">s</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.4.4.1.1.1.1.2.1" xref="S4.E1.m1.4.4.1.1.1.1.2.1.cmml">⋅</mo><msub id="S4.E1.m1.4.4.1.1.1.1.2.3" xref="S4.E1.m1.4.4.1.1.1.1.2.3.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.2.3.2" xref="S4.E1.m1.4.4.1.1.1.1.2.3.2.cmml">𝐯</mi><mi id="S4.E1.m1.4.4.1.1.1.1.2.3.3" xref="S4.E1.m1.4.4.1.1.1.1.2.3.3.cmml">p</mi></msub></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.cmml">/</mo><mi id="S4.E1.m1.4.4.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S4.E1.m1.4.4.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S4.E1.m1.6.6.3" xref="S4.E1.m1.6.6.3.cmml"><msub id="S4.E1.m1.6.6.3.3" xref="S4.E1.m1.6.6.3.3.cmml"><mo id="S4.E1.m1.6.6.3.3.2" xref="S4.E1.m1.6.6.3.3.2.cmml">∑</mo><mrow id="S4.E1.m1.5.5.2.1.1" xref="S4.E1.m1.5.5.2.1.1.cmml"><mi id="S4.E1.m1.5.5.2.1.1.3" xref="S4.E1.m1.5.5.2.1.1.3.cmml">a</mi><mo id="S4.E1.m1.5.5.2.1.1.2" xref="S4.E1.m1.5.5.2.1.1.2.cmml">∈</mo><mrow id="S4.E1.m1.5.5.2.1.1.4" xref="S4.E1.m1.5.5.2.1.1.4.cmml"><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.5.5.2.1.1.4.2" xref="S4.E1.m1.5.5.2.1.1.4.2b.cmml"><em id="S4.E1.m1.5.5.2.1.1.4.2.1nest" class="ltx_emph ltx_font_italic" style="font-size:70%;">S</em></mtext><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.5.5.2.1.1.4.1" xref="S4.E1.m1.5.5.2.1.1.4.1.cmml">\</mo><mrow id="S4.E1.m1.5.5.2.1.1.4.3.2" xref="S4.E1.m1.5.5.2.1.1.4.3.1.cmml"><mo stretchy="false" id="S4.E1.m1.5.5.2.1.1.4.3.2.1" xref="S4.E1.m1.5.5.2.1.1.4.3.1.cmml">{</mo><mi id="S4.E1.m1.5.5.2.1.1.1" xref="S4.E1.m1.5.5.2.1.1.1.cmml">s</mi><mo stretchy="false" id="S4.E1.m1.5.5.2.1.1.4.3.2.2" xref="S4.E1.m1.5.5.2.1.1.4.3.1.cmml">}</mo></mrow></mrow></mrow></msub><mrow id="S4.E1.m1.6.6.3.2" xref="S4.E1.m1.6.6.3.2.cmml"><mi id="S4.E1.m1.6.6.3.2.3" xref="S4.E1.m1.6.6.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.3.2.2" xref="S4.E1.m1.6.6.3.2.2.cmml">​</mo><mi id="S4.E1.m1.6.6.3.2.4" xref="S4.E1.m1.6.6.3.2.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.3.2.2a" xref="S4.E1.m1.6.6.3.2.2.cmml">​</mo><mi id="S4.E1.m1.6.6.3.2.5" xref="S4.E1.m1.6.6.3.2.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.3.2.2b" xref="S4.E1.m1.6.6.3.2.2.cmml">​</mo><mrow id="S4.E1.m1.6.6.3.2.1.1" xref="S4.E1.m1.6.6.3.2.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.6.6.3.2.1.1.2" xref="S4.E1.m1.6.6.3.2.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.6.6.3.2.1.1.1" xref="S4.E1.m1.6.6.3.2.1.1.1.cmml"><mrow id="S4.E1.m1.6.6.3.2.1.1.1.2" xref="S4.E1.m1.6.6.3.2.1.1.1.2.cmml"><msub id="S4.E1.m1.6.6.3.2.1.1.1.2.2" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2.cmml"><mi id="S4.E1.m1.6.6.3.2.1.1.1.2.2.2" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2.2.cmml">𝐯</mi><mi id="S4.E1.m1.6.6.3.2.1.1.1.2.2.3" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2.3.cmml">s</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.6.6.3.2.1.1.1.2.1" xref="S4.E1.m1.6.6.3.2.1.1.1.2.1.cmml">⋅</mo><msub id="S4.E1.m1.6.6.3.2.1.1.1.2.3" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3.cmml"><mi id="S4.E1.m1.6.6.3.2.1.1.1.2.3.2" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3.2.cmml">𝐯</mi><mi id="S4.E1.m1.6.6.3.2.1.1.1.2.3.3" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3.3.cmml">a</mi></msub></mrow><mo id="S4.E1.m1.6.6.3.2.1.1.1.1" xref="S4.E1.m1.6.6.3.2.1.1.1.1.cmml">/</mo><mi id="S4.E1.m1.6.6.3.2.1.1.1.3" xref="S4.E1.m1.6.6.3.2.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S4.E1.m1.6.6.3.2.1.1.3" xref="S4.E1.m1.6.6.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.7.7.1.2" xref="S4.E1.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.7b"><apply id="S4.E1.m1.7.7.1.1.cmml" xref="S4.E1.m1.7.7.1"><eq id="S4.E1.m1.7.7.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.1"></eq><apply id="S4.E1.m1.7.7.1.1.2.cmml" xref="S4.E1.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2">subscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2">ℒ</ci><apply id="S4.E1.m1.7.7.1.1.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.3"><times id="S4.E1.m1.7.7.1.1.2.3.1.cmml" xref="S4.E1.m1.7.7.1.1.2.3.1"></times><ci id="S4.E1.m1.7.7.1.1.2.3.2.cmml" xref="S4.E1.m1.7.7.1.1.2.3.2">𝑐</ci><ci id="S4.E1.m1.7.7.1.1.2.3.3.cmml" xref="S4.E1.m1.7.7.1.1.2.3.3">𝑜</ci><ci id="S4.E1.m1.7.7.1.1.2.3.4.cmml" xref="S4.E1.m1.7.7.1.1.2.3.4">𝑛</ci><ci id="S4.E1.m1.7.7.1.1.2.3.5.cmml" xref="S4.E1.m1.7.7.1.1.2.3.5">𝑓</ci></apply></apply><apply id="S4.E1.m1.7.7.1.1.3.cmml" xref="S4.E1.m1.7.7.1.1.3"><apply id="S4.E1.m1.7.7.1.1.3.1.cmml" xref="S4.E1.m1.7.7.1.1.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.3.1.1.cmml" xref="S4.E1.m1.7.7.1.1.3.1">subscript</csymbol><sum id="S4.E1.m1.7.7.1.1.3.1.2.cmml" xref="S4.E1.m1.7.7.1.1.3.1.2"></sum><apply id="S4.E1.m1.7.7.1.1.3.1.3.cmml" xref="S4.E1.m1.7.7.1.1.3.1.3"><in id="S4.E1.m1.7.7.1.1.3.1.3.1.cmml" xref="S4.E1.m1.7.7.1.1.3.1.3.1"></in><ci id="S4.E1.m1.7.7.1.1.3.1.3.2.cmml" xref="S4.E1.m1.7.7.1.1.3.1.3.2">𝑠</ci><ci id="S4.E1.m1.7.7.1.1.3.1.3.3b.cmml" xref="S4.E1.m1.7.7.1.1.3.1.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.E1.m1.7.7.1.1.3.1.3.3.cmml" xref="S4.E1.m1.7.7.1.1.3.1.3.3"><em id="S4.E1.m1.7.7.1.1.3.1.3.3.1anest" class="ltx_emph ltx_font_italic" style="font-size:70%;">S</em></mtext></ci></apply></apply><apply id="S4.E1.m1.7.7.1.1.3.2.cmml" xref="S4.E1.m1.7.7.1.1.3.2"><times id="S4.E1.m1.7.7.1.1.3.2.1.cmml" xref="S4.E1.m1.7.7.1.1.3.2.1"></times><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><divide id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2"></divide><apply id="S4.E1.m1.2.2.4.cmml" xref="S4.E1.m1.2.2.4"><minus id="S4.E1.m1.2.2.4.1.cmml" xref="S4.E1.m1.2.2.4"></minus><cn type="integer" id="S4.E1.m1.2.2.4.2.cmml" xref="S4.E1.m1.2.2.4.2">1</cn></apply><apply id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2"><abs id="S4.E1.m1.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.2"></abs><apply id="S4.E1.m1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1"><times id="S4.E1.m1.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1"></times><ci id="S4.E1.m1.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.2">𝑃</ci><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝑠</ci></apply></apply></apply><apply id="S4.E1.m1.7.7.1.1.3.2.2.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2"><apply id="S4.E1.m1.7.7.1.1.3.2.2.1.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.3.2.2.1.1.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.1">subscript</csymbol><sum id="S4.E1.m1.7.7.1.1.3.2.2.1.2.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.1.2"></sum><apply id="S4.E1.m1.3.3.1.cmml" xref="S4.E1.m1.3.3.1"><in id="S4.E1.m1.3.3.1.2.cmml" xref="S4.E1.m1.3.3.1.2"></in><ci id="S4.E1.m1.3.3.1.3.cmml" xref="S4.E1.m1.3.3.1.3">𝑝</ci><apply id="S4.E1.m1.3.3.1.4.cmml" xref="S4.E1.m1.3.3.1.4"><times id="S4.E1.m1.3.3.1.4.1.cmml" xref="S4.E1.m1.3.3.1.4.1"></times><ci id="S4.E1.m1.3.3.1.4.2.cmml" xref="S4.E1.m1.3.3.1.4.2">𝑃</ci><ci id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1">𝑠</ci></apply></apply></apply><apply id="S4.E1.m1.7.7.1.1.3.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.2"><times id="S4.E1.m1.7.7.1.1.3.2.2.2.1.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.2.1"></times><ci id="S4.E1.m1.7.7.1.1.3.2.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.2.2">𝑙</ci><ci id="S4.E1.m1.7.7.1.1.3.2.2.2.3.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.2.3">𝑜</ci><ci id="S4.E1.m1.7.7.1.1.3.2.2.2.4.cmml" xref="S4.E1.m1.7.7.1.1.3.2.2.2.4">𝑔</ci><apply id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6"><divide id="S4.E1.m1.6.6.4.cmml" xref="S4.E1.m1.6.6"></divide><apply id="S4.E1.m1.4.4.1.cmml" xref="S4.E1.m1.4.4.1"><times id="S4.E1.m1.4.4.1.2.cmml" xref="S4.E1.m1.4.4.1.2"></times><ci id="S4.E1.m1.4.4.1.3.cmml" xref="S4.E1.m1.4.4.1.3">𝑒</ci><ci id="S4.E1.m1.4.4.1.4.cmml" xref="S4.E1.m1.4.4.1.4">𝑥</ci><ci id="S4.E1.m1.4.4.1.5.cmml" xref="S4.E1.m1.4.4.1.5">𝑝</ci><apply id="S4.E1.m1.4.4.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"><divide id="S4.E1.m1.4.4.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1"></divide><apply id="S4.E1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2"><ci id="S4.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.1">⋅</ci><apply id="S4.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.2">𝐯</ci><ci id="S4.E1.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.2.3">𝑠</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.3.2">𝐯</ci><ci id="S4.E1.m1.4.4.1.1.1.1.2.3.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.2.3.3">𝑝</ci></apply></apply><ci id="S4.E1.m1.4.4.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.3">𝜏</ci></apply></apply><apply id="S4.E1.m1.6.6.3.cmml" xref="S4.E1.m1.6.6.3"><apply id="S4.E1.m1.6.6.3.3.cmml" xref="S4.E1.m1.6.6.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.3.3.1.cmml" xref="S4.E1.m1.6.6.3.3">subscript</csymbol><sum id="S4.E1.m1.6.6.3.3.2.cmml" xref="S4.E1.m1.6.6.3.3.2"></sum><apply id="S4.E1.m1.5.5.2.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1"><in id="S4.E1.m1.5.5.2.1.1.2.cmml" xref="S4.E1.m1.5.5.2.1.1.2"></in><ci id="S4.E1.m1.5.5.2.1.1.3.cmml" xref="S4.E1.m1.5.5.2.1.1.3">𝑎</ci><apply id="S4.E1.m1.5.5.2.1.1.4.cmml" xref="S4.E1.m1.5.5.2.1.1.4"><ci id="S4.E1.m1.5.5.2.1.1.4.1.cmml" xref="S4.E1.m1.5.5.2.1.1.4.1">\</ci><ci id="S4.E1.m1.5.5.2.1.1.4.2b.cmml" xref="S4.E1.m1.5.5.2.1.1.4.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.E1.m1.5.5.2.1.1.4.2.cmml" xref="S4.E1.m1.5.5.2.1.1.4.2"><em id="S4.E1.m1.5.5.2.1.1.4.2.1anest" class="ltx_emph ltx_font_italic" style="font-size:70%;">S</em></mtext></ci><set id="S4.E1.m1.5.5.2.1.1.4.3.1.cmml" xref="S4.E1.m1.5.5.2.1.1.4.3.2"><ci id="S4.E1.m1.5.5.2.1.1.1.cmml" xref="S4.E1.m1.5.5.2.1.1.1">𝑠</ci></set></apply></apply></apply><apply id="S4.E1.m1.6.6.3.2.cmml" xref="S4.E1.m1.6.6.3.2"><times id="S4.E1.m1.6.6.3.2.2.cmml" xref="S4.E1.m1.6.6.3.2.2"></times><ci id="S4.E1.m1.6.6.3.2.3.cmml" xref="S4.E1.m1.6.6.3.2.3">𝑒</ci><ci id="S4.E1.m1.6.6.3.2.4.cmml" xref="S4.E1.m1.6.6.3.2.4">𝑥</ci><ci id="S4.E1.m1.6.6.3.2.5.cmml" xref="S4.E1.m1.6.6.3.2.5">𝑝</ci><apply id="S4.E1.m1.6.6.3.2.1.1.1.cmml" xref="S4.E1.m1.6.6.3.2.1.1"><divide id="S4.E1.m1.6.6.3.2.1.1.1.1.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.1"></divide><apply id="S4.E1.m1.6.6.3.2.1.1.1.2.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2"><ci id="S4.E1.m1.6.6.3.2.1.1.1.2.1.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.1">⋅</ci><apply id="S4.E1.m1.6.6.3.2.1.1.1.2.2.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.3.2.1.1.1.2.2.1.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.6.6.3.2.1.1.1.2.2.2.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2.2">𝐯</ci><ci id="S4.E1.m1.6.6.3.2.1.1.1.2.2.3.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.2.3">𝑠</ci></apply><apply id="S4.E1.m1.6.6.3.2.1.1.1.2.3.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.3.2.1.1.1.2.3.1.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3">subscript</csymbol><ci id="S4.E1.m1.6.6.3.2.1.1.1.2.3.2.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3.2">𝐯</ci><ci id="S4.E1.m1.6.6.3.2.1.1.1.2.3.3.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.2.3.3">𝑎</ci></apply></apply><ci id="S4.E1.m1.6.6.3.2.1.1.1.3.cmml" xref="S4.E1.m1.6.6.3.2.1.1.1.3">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.7c">\mathcal{L}_{conf}=\sum_{s\in\emph{S}}\frac{-1}{|P(s)|}\sum_{p\in P(s)}log\frac{exp(\mathbf{v}_{s}\cdot\mathbf{v}_{p}/\tau)}{\sum_{a\in\emph{S}\backslash\{s\}}exp(\mathbf{v}_{s}\cdot\mathbf{v}_{a}/\tau)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS3.p3.9" class="ltx_p">Here <math id="S4.SS3.p3.3.m1.1" class="ltx_Math" alttext="\mathbf{v}_{s}" display="inline"><semantics id="S4.SS3.p3.3.m1.1a"><msub id="S4.SS3.p3.3.m1.1.1" xref="S4.SS3.p3.3.m1.1.1.cmml"><mi id="S4.SS3.p3.3.m1.1.1.2" xref="S4.SS3.p3.3.m1.1.1.2.cmml">𝐯</mi><mi id="S4.SS3.p3.3.m1.1.1.3" xref="S4.SS3.p3.3.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m1.1b"><apply id="S4.SS3.p3.3.m1.1.1.cmml" xref="S4.SS3.p3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m1.1.1.1.cmml" xref="S4.SS3.p3.3.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.3.m1.1.1.2.cmml" xref="S4.SS3.p3.3.m1.1.1.2">𝐯</ci><ci id="S4.SS3.p3.3.m1.1.1.3.cmml" xref="S4.SS3.p3.3.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m1.1c">\mathbf{v}_{s}</annotation></semantics></math> is the feature output of the fusion-based augmentation module, and the symbol <math id="S4.SS3.p3.4.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S4.SS3.p3.4.m2.1a"><mo id="S4.SS3.p3.4.m2.1.1" xref="S4.SS3.p3.4.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m2.1b"><ci id="S4.SS3.p3.4.m2.1.1.cmml" xref="S4.SS3.p3.4.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m2.1c">\cdot</annotation></semantics></math> denotes the inner product of feature vectors. Parameter <math id="S4.SS3.p3.5.m3.1" class="ltx_Math" alttext="\tau\in\mathbb{R}^{+}" display="inline"><semantics id="S4.SS3.p3.5.m3.1a"><mrow id="S4.SS3.p3.5.m3.1.1" xref="S4.SS3.p3.5.m3.1.1.cmml"><mi id="S4.SS3.p3.5.m3.1.1.2" xref="S4.SS3.p3.5.m3.1.1.2.cmml">τ</mi><mo id="S4.SS3.p3.5.m3.1.1.1" xref="S4.SS3.p3.5.m3.1.1.1.cmml">∈</mo><msup id="S4.SS3.p3.5.m3.1.1.3" xref="S4.SS3.p3.5.m3.1.1.3.cmml"><mi id="S4.SS3.p3.5.m3.1.1.3.2" xref="S4.SS3.p3.5.m3.1.1.3.2.cmml">ℝ</mi><mo id="S4.SS3.p3.5.m3.1.1.3.3" xref="S4.SS3.p3.5.m3.1.1.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m3.1b"><apply id="S4.SS3.p3.5.m3.1.1.cmml" xref="S4.SS3.p3.5.m3.1.1"><in id="S4.SS3.p3.5.m3.1.1.1.cmml" xref="S4.SS3.p3.5.m3.1.1.1"></in><ci id="S4.SS3.p3.5.m3.1.1.2.cmml" xref="S4.SS3.p3.5.m3.1.1.2">𝜏</ci><apply id="S4.SS3.p3.5.m3.1.1.3.cmml" xref="S4.SS3.p3.5.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.5.m3.1.1.3.1.cmml" xref="S4.SS3.p3.5.m3.1.1.3">superscript</csymbol><ci id="S4.SS3.p3.5.m3.1.1.3.2.cmml" xref="S4.SS3.p3.5.m3.1.1.3.2">ℝ</ci><plus id="S4.SS3.p3.5.m3.1.1.3.3.cmml" xref="S4.SS3.p3.5.m3.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m3.1c">\tau\in\mathbb{R}^{+}</annotation></semantics></math> represents the temperature used to adjust the impact of different samples <cite class="ltx_cite ltx_citemacro_citep">(Tian et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>.
Therefore, minimizing the contrastive fusion loss will force the fused features from the same multi-modal data sample (positive features, <math id="S4.SS3.p3.6.m4.1" class="ltx_Math" alttext="\mathbf{v}_{s}" display="inline"><semantics id="S4.SS3.p3.6.m4.1a"><msub id="S4.SS3.p3.6.m4.1.1" xref="S4.SS3.p3.6.m4.1.1.cmml"><mi id="S4.SS3.p3.6.m4.1.1.2" xref="S4.SS3.p3.6.m4.1.1.2.cmml">𝐯</mi><mi id="S4.SS3.p3.6.m4.1.1.3" xref="S4.SS3.p3.6.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.6.m4.1b"><apply id="S4.SS3.p3.6.m4.1.1.cmml" xref="S4.SS3.p3.6.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.6.m4.1.1.1.cmml" xref="S4.SS3.p3.6.m4.1.1">subscript</csymbol><ci id="S4.SS3.p3.6.m4.1.1.2.cmml" xref="S4.SS3.p3.6.m4.1.1.2">𝐯</ci><ci id="S4.SS3.p3.6.m4.1.1.3.cmml" xref="S4.SS3.p3.6.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.6.m4.1c">\mathbf{v}_{s}</annotation></semantics></math> and <math id="S4.SS3.p3.7.m5.1" class="ltx_Math" alttext="\mathbf{v}_{p}" display="inline"><semantics id="S4.SS3.p3.7.m5.1a"><msub id="S4.SS3.p3.7.m5.1.1" xref="S4.SS3.p3.7.m5.1.1.cmml"><mi id="S4.SS3.p3.7.m5.1.1.2" xref="S4.SS3.p3.7.m5.1.1.2.cmml">𝐯</mi><mi id="S4.SS3.p3.7.m5.1.1.3" xref="S4.SS3.p3.7.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.7.m5.1b"><apply id="S4.SS3.p3.7.m5.1.1.cmml" xref="S4.SS3.p3.7.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.7.m5.1.1.1.cmml" xref="S4.SS3.p3.7.m5.1.1">subscript</csymbol><ci id="S4.SS3.p3.7.m5.1.1.2.cmml" xref="S4.SS3.p3.7.m5.1.1.2">𝐯</ci><ci id="S4.SS3.p3.7.m5.1.1.3.cmml" xref="S4.SS3.p3.7.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.7.m5.1c">\mathbf{v}_{p}</annotation></semantics></math>) together, while pushing fused features from other data samples (negative features, <math id="S4.SS3.p3.8.m6.1" class="ltx_Math" alttext="\mathbf{v}_{s}" display="inline"><semantics id="S4.SS3.p3.8.m6.1a"><msub id="S4.SS3.p3.8.m6.1.1" xref="S4.SS3.p3.8.m6.1.1.cmml"><mi id="S4.SS3.p3.8.m6.1.1.2" xref="S4.SS3.p3.8.m6.1.1.2.cmml">𝐯</mi><mi id="S4.SS3.p3.8.m6.1.1.3" xref="S4.SS3.p3.8.m6.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.8.m6.1b"><apply id="S4.SS3.p3.8.m6.1.1.cmml" xref="S4.SS3.p3.8.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.8.m6.1.1.1.cmml" xref="S4.SS3.p3.8.m6.1.1">subscript</csymbol><ci id="S4.SS3.p3.8.m6.1.1.2.cmml" xref="S4.SS3.p3.8.m6.1.1.2">𝐯</ci><ci id="S4.SS3.p3.8.m6.1.1.3.cmml" xref="S4.SS3.p3.8.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.8.m6.1c">\mathbf{v}_{s}</annotation></semantics></math> and <math id="S4.SS3.p3.9.m7.1" class="ltx_Math" alttext="\mathbf{v}_{a}" display="inline"><semantics id="S4.SS3.p3.9.m7.1a"><msub id="S4.SS3.p3.9.m7.1.1" xref="S4.SS3.p3.9.m7.1.1.cmml"><mi id="S4.SS3.p3.9.m7.1.1.2" xref="S4.SS3.p3.9.m7.1.1.2.cmml">𝐯</mi><mi id="S4.SS3.p3.9.m7.1.1.3" xref="S4.SS3.p3.9.m7.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.9.m7.1b"><apply id="S4.SS3.p3.9.m7.1.1.cmml" xref="S4.SS3.p3.9.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.9.m7.1.1.1.cmml" xref="S4.SS3.p3.9.m7.1.1">subscript</csymbol><ci id="S4.SS3.p3.9.m7.1.1.2.cmml" xref="S4.SS3.p3.9.m7.1.1.2">𝐯</ci><ci id="S4.SS3.p3.9.m7.1.1.3.cmml" xref="S4.SS3.p3.9.m7.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.9.m7.1c">\mathbf{v}_{a}</annotation></semantics></math>) apart. In this way, the feature encoders are trained to learn consistent information across modalities by maximizing the mutual information of features from different modalities.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Modality-wise federated average.</span> In multi-modal FL systems, the nodes with different data modalities will have different model architectures. For example, the nodes with all data modalities will train models with three multi-modal feature encoders, while the models of nodes with only depth and audio data will train models with two feature encoders.
We propose a modality-wise federated average scheme to address the challenge of modality heterogeneity, where the server will collect and aggregate the encoder networks of the same modality with Fedavg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>. As a result, the nodes with different data modalities can collaborate to improve the performance in unsupervised federated learning. We also apply this model aggregation scheme on the server during the weakly supervised multi-modal FL stage.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Weakly Supervised Multi-Modal FL</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">At the third stage, the nodes will perform weakly supervised multi-modal FL based on the model trained at the second stage. During the real-world deployment, the nodes can leverage weak labels provided by the participants or their caregivers for local model training. For example, marking the time of having lunch would automatically label the sensor data during the period. However, it would be challenging to associate the sparse and noisy labels with collected sensor data for supervised model training. Moreover, different subjects usually have highly heterogeneous and imbalanced data distributions, making it challenging for model aggregation in FL. For example, some activities such as “sitting” incur frequently while others like “writing” appear rarely.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Training with weak labels.</span>
ADMarker leverages sparse activity logs for weakly supervised training. Such logs can be obtained in several different ways. Many patients are routinely suggested by the doctors to keep a journal of activity logs through ADL scales <cite class="ltx_cite ltx_citemacro_citep">(Patterson et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">1992</a>)</cite>. Alternatively, the patients and/or their caregivers may be asked to keep an activity log for the purpose of user training during the initial phase of system deployment. Our results show that ADMarker achieves good performance even if activity logs are available for only several days (see Figure <a href="#S8.F12.sf1" title="In Figure 12 ‣ 8.3.1. Overview of the subjects’ data ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a>). Moreover, only major daily routines like “sleeping”, “having a meal” (see Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a)) are needed, which alleviates the burden of users.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">To train the model with weak labels, we first need to map them to a series of fine-grained activity labels we are interested in (see Table <a href="#S3.T2" title="Table 2 ‣ 3.3. System Architecture ‣ 3. System Overview ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), which requires sophisticated domain knowledge. For example, the weak label “Having a meal” corresponds to the fine-grained labels “eating” and “setting”, while “Household” includes the activities of “standing” and “walking”. Another challenge is to associate the mapped fine-grained labels with collected sensor data, because the mapping from weak to fine-grained labels does not include the order of the events. Therefore, during the weakly supervised training, we split the training data such that the data samples in the same batch are collected successively in the time order. Then, to improve the training performance, we shuffle the training samples in each batch to simulate different orders of the activities, and calculate the cross entropy loss with different label permutations for model training.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2310.15301/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Training with weak labels. The “activity record form” provided by subjects is mapped to fine-grained labels and associated with collected data.</span></figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.4" class="ltx_p"><span id="S4.SS4.p4.4.1" class="ltx_text ltx_font_bold">Local and global balancing on nodes.</span> To address the challenge of local class imbalance, the nodes will perform both self and global balancing during model training. First, the nodes will use balanced cross-entropy loss <cite class="ltx_cite ltx_citemacro_citep">(Lin
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite> <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{balanceCE}" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><msub id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p4.1.m1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.SS4.p4.1.m1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.3.cmml"><mi id="S4.SS4.p4.1.m1.1.1.3.2" xref="S4.SS4.p4.1.m1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.3" xref="S4.SS4.p4.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1a" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.4" xref="S4.SS4.p4.1.m1.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1b" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.5" xref="S4.SS4.p4.1.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1c" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.6" xref="S4.SS4.p4.1.m1.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1d" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.7" xref="S4.SS4.p4.1.m1.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1e" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.8" xref="S4.SS4.p4.1.m1.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1f" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.9" xref="S4.SS4.p4.1.m1.1.1.3.9.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.3.1g" xref="S4.SS4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.1.m1.1.1.3.10" xref="S4.SS4.p4.1.m1.1.1.3.10.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><apply id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p4.1.m1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p4.1.m1.1.1.2.cmml" xref="S4.SS4.p4.1.m1.1.1.2">ℒ</ci><apply id="S4.SS4.p4.1.m1.1.1.3.cmml" xref="S4.SS4.p4.1.m1.1.1.3"><times id="S4.SS4.p4.1.m1.1.1.3.1.cmml" xref="S4.SS4.p4.1.m1.1.1.3.1"></times><ci id="S4.SS4.p4.1.m1.1.1.3.2.cmml" xref="S4.SS4.p4.1.m1.1.1.3.2">𝑏</ci><ci id="S4.SS4.p4.1.m1.1.1.3.3.cmml" xref="S4.SS4.p4.1.m1.1.1.3.3">𝑎</ci><ci id="S4.SS4.p4.1.m1.1.1.3.4.cmml" xref="S4.SS4.p4.1.m1.1.1.3.4">𝑙</ci><ci id="S4.SS4.p4.1.m1.1.1.3.5.cmml" xref="S4.SS4.p4.1.m1.1.1.3.5">𝑎</ci><ci id="S4.SS4.p4.1.m1.1.1.3.6.cmml" xref="S4.SS4.p4.1.m1.1.1.3.6">𝑛</ci><ci id="S4.SS4.p4.1.m1.1.1.3.7.cmml" xref="S4.SS4.p4.1.m1.1.1.3.7">𝑐</ci><ci id="S4.SS4.p4.1.m1.1.1.3.8.cmml" xref="S4.SS4.p4.1.m1.1.1.3.8">𝑒</ci><ci id="S4.SS4.p4.1.m1.1.1.3.9.cmml" xref="S4.SS4.p4.1.m1.1.1.3.9">𝐶</ci><ci id="S4.SS4.p4.1.m1.1.1.3.10.cmml" xref="S4.SS4.p4.1.m1.1.1.3.10">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">\mathcal{L}_{balanceCE}</annotation></semantics></math> to train the local model, which avoids the gradient dominance of the majority classes. In particular, the standard cross entropy loss will be reweighted with the number of samples in each class to down-weigh the loss assigned to head classes.
Moreover, as FL proceeds, the server-side global model more or less accumulates some knowledge on all classes <cite class="ltx_cite ltx_citemacro_citep">(Shuai
et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>. Therefore, the aggregated global model will serve as the teacher model to guide the training of the local model using knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Gou
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> with the loss <math id="S4.SS4.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{KD}" display="inline"><semantics id="S4.SS4.p4.2.m2.1a"><msub id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p4.2.m2.1.1.2" xref="S4.SS4.p4.2.m2.1.1.2.cmml">ℒ</mi><mrow id="S4.SS4.p4.2.m2.1.1.3" xref="S4.SS4.p4.2.m2.1.1.3.cmml"><mi id="S4.SS4.p4.2.m2.1.1.3.2" xref="S4.SS4.p4.2.m2.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.2.m2.1.1.3.1" xref="S4.SS4.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.2.m2.1.1.3.3" xref="S4.SS4.p4.2.m2.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><apply id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p4.2.m2.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p4.2.m2.1.1.2.cmml" xref="S4.SS4.p4.2.m2.1.1.2">ℒ</ci><apply id="S4.SS4.p4.2.m2.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.3"><times id="S4.SS4.p4.2.m2.1.1.3.1.cmml" xref="S4.SS4.p4.2.m2.1.1.3.1"></times><ci id="S4.SS4.p4.2.m2.1.1.3.2.cmml" xref="S4.SS4.p4.2.m2.1.1.3.2">𝐾</ci><ci id="S4.SS4.p4.2.m2.1.1.3.3.cmml" xref="S4.SS4.p4.2.m2.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">\mathcal{L}_{KD}</annotation></semantics></math>.
Therefore, the overall training loss of nodes during the weakly supervised FL stage is added by <math id="S4.SS4.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{balanceCE}" display="inline"><semantics id="S4.SS4.p4.3.m3.1a"><msub id="S4.SS4.p4.3.m3.1.1" xref="S4.SS4.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p4.3.m3.1.1.2" xref="S4.SS4.p4.3.m3.1.1.2.cmml">ℒ</mi><mrow id="S4.SS4.p4.3.m3.1.1.3" xref="S4.SS4.p4.3.m3.1.1.3.cmml"><mi id="S4.SS4.p4.3.m3.1.1.3.2" xref="S4.SS4.p4.3.m3.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.3" xref="S4.SS4.p4.3.m3.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1a" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.4" xref="S4.SS4.p4.3.m3.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1b" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.5" xref="S4.SS4.p4.3.m3.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1c" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.6" xref="S4.SS4.p4.3.m3.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1d" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.7" xref="S4.SS4.p4.3.m3.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1e" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.8" xref="S4.SS4.p4.3.m3.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1f" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.9" xref="S4.SS4.p4.3.m3.1.1.3.9.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.3.m3.1.1.3.1g" xref="S4.SS4.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.3.m3.1.1.3.10" xref="S4.SS4.p4.3.m3.1.1.3.10.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.3.m3.1b"><apply id="S4.SS4.p4.3.m3.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p4.3.m3.1.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p4.3.m3.1.1.2.cmml" xref="S4.SS4.p4.3.m3.1.1.2">ℒ</ci><apply id="S4.SS4.p4.3.m3.1.1.3.cmml" xref="S4.SS4.p4.3.m3.1.1.3"><times id="S4.SS4.p4.3.m3.1.1.3.1.cmml" xref="S4.SS4.p4.3.m3.1.1.3.1"></times><ci id="S4.SS4.p4.3.m3.1.1.3.2.cmml" xref="S4.SS4.p4.3.m3.1.1.3.2">𝑏</ci><ci id="S4.SS4.p4.3.m3.1.1.3.3.cmml" xref="S4.SS4.p4.3.m3.1.1.3.3">𝑎</ci><ci id="S4.SS4.p4.3.m3.1.1.3.4.cmml" xref="S4.SS4.p4.3.m3.1.1.3.4">𝑙</ci><ci id="S4.SS4.p4.3.m3.1.1.3.5.cmml" xref="S4.SS4.p4.3.m3.1.1.3.5">𝑎</ci><ci id="S4.SS4.p4.3.m3.1.1.3.6.cmml" xref="S4.SS4.p4.3.m3.1.1.3.6">𝑛</ci><ci id="S4.SS4.p4.3.m3.1.1.3.7.cmml" xref="S4.SS4.p4.3.m3.1.1.3.7">𝑐</ci><ci id="S4.SS4.p4.3.m3.1.1.3.8.cmml" xref="S4.SS4.p4.3.m3.1.1.3.8">𝑒</ci><ci id="S4.SS4.p4.3.m3.1.1.3.9.cmml" xref="S4.SS4.p4.3.m3.1.1.3.9">𝐶</ci><ci id="S4.SS4.p4.3.m3.1.1.3.10.cmml" xref="S4.SS4.p4.3.m3.1.1.3.10">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.3.m3.1c">\mathcal{L}_{balanceCE}</annotation></semantics></math> and <math id="S4.SS4.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{KD}" display="inline"><semantics id="S4.SS4.p4.4.m4.1a"><msub id="S4.SS4.p4.4.m4.1.1" xref="S4.SS4.p4.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p4.4.m4.1.1.2" xref="S4.SS4.p4.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S4.SS4.p4.4.m4.1.1.3" xref="S4.SS4.p4.4.m4.1.1.3.cmml"><mi id="S4.SS4.p4.4.m4.1.1.3.2" xref="S4.SS4.p4.4.m4.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.4.m4.1.1.3.1" xref="S4.SS4.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.4.m4.1.1.3.3" xref="S4.SS4.p4.4.m4.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.4.m4.1b"><apply id="S4.SS4.p4.4.m4.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p4.4.m4.1.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p4.4.m4.1.1.2.cmml" xref="S4.SS4.p4.4.m4.1.1.2">ℒ</ci><apply id="S4.SS4.p4.4.m4.1.1.3.cmml" xref="S4.SS4.p4.4.m4.1.1.3"><times id="S4.SS4.p4.4.m4.1.1.3.1.cmml" xref="S4.SS4.p4.4.m4.1.1.3.1"></times><ci id="S4.SS4.p4.4.m4.1.1.3.2.cmml" xref="S4.SS4.p4.4.m4.1.1.3.2">𝐾</ci><ci id="S4.SS4.p4.4.m4.1.1.3.3.cmml" xref="S4.SS4.p4.4.m4.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.4.m4.1c">\mathcal{L}_{KD}</annotation></semantics></math>.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x7.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Components and Layout.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x8.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">Typical Home Deployments.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x9.png" id="S4.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F7.sf3.3.2" class="ltx_text" style="font-size:90%;">Examples of recorded multi-modal data.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">The ADMarker prototype. The hardware incorporates three sensor modalities (depth, mmWave radar, and audio) to detect multi-dimensional behavior biomarkers in home environments.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Utilizing Digital Biomarkers for AD Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To evaluate the effectiveness of the detected biomarkers, we use a DNN for AD diagnosis, where the inputs are the features of the digital biomarkers and outputs are the predictions of diagnosis results. Then, the major challenge is how to quantify the digital biomarkers to generate effective features as the input of the disease diagnosis model.
According to medical literature <cite class="ltx_cite ltx_citemacro_citep">(Dawadi
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>, we calculate the duration and frequency of the detected activities over the period of deployment as the features. However, due to the system dynamics (see Section <a href="#S4.SS1.SSS1" title="4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>), the total duration of the recorded sensor data may vary among different subjects. For example, most of the subjects will have four-week data, while some subjects only have one-week or two-week data due to sensor faults. Therefore, we further normalize the duration and frequency of detected activities with the period of the collected data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Moreover, to further select more effective digital biomarkers, we use one-way Analysis of Variance (ANOVA) <cite class="ltx_cite ltx_citemacro_citep">(St
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">1989</a>)</cite> to measure the correlations between each digital marker and the diagnosis results. Before applying ANOVA analysis to the data collected in our clinical deployment, we need to ensure that our data satisfies the following three conditions: independence, normality, and equality. First, the selected features of each individual are independent of others, either in the same or different subject groups (NC, MCI, and AD). Second, we apply the Box-Cox data transform <cite class="ltx_cite ltx_citemacro_citep">(Sakia, <a href="#bib.bib52" title="" class="ltx_ref">1992</a>)</cite> to the selected features, ensuring that the transformed features satisfy the normal distributions.
Third, we use the Levene’s test <cite class="ltx_cite ltx_citemacro_citep">(Brown and
Forsythe, <a href="#bib.bib15" title="" class="ltx_ref">1974</a>)</cite> to access the variances of each feature among the AD, MCI, and NC groups. The mean p-value (0.659) is larger than 0.05, which shows the equality of selected features.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>System Implementation</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Hardware System</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the overview of our ADMarker prototype.
The hardware system incorporates three multi-modal sensors, an NVIDIA single-board edge computer, and a 4G cellular interface that communicates with the server.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Hardware choices.</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">The goal of the hardware design is to capture lots of digital biomarkers in a privacy-preserving manner while ensuring durability and scalability of the system. First, we choose three sensor modalities: a depth camera, a mmWave radar, and a microphone, which collectively capture a wide range of biomarkers while preserving the users’ privacy. In particular, the Time-of-Flight (ToF) depth camera cannot reveal sensitive personal information like faces; the mmWave radar can only detect the motions of the subjects; the ambient microphones run real-time algorithms to extract acoustic features without recording any raw acoustic data. Collectively, the three sensor modalities can detect various activities such as having meals, conversations, watching TV, etc. Second, we choose the NVIDIA Xavier NX <cite class="ltx_cite ltx_citemacro_citep">(xav, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> as the main compute unit as it incorporates powerful NVIDIA GPUs (384-core Volta) and CPUs (6-core @1.9GHZ) for on-device model training. Third, to improve the durability of the system, we choose the NVMe SSD rather than the conventional portable HDD or SSD as the external data storage unit. The reason is that they have a relatively lower read/write speed (about 100MB/s), poor reliability (vulnerable to vibration and power failure), and a larger size/weight, which is unsuitable for long-term operation and mass deployment.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Layout design.</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">The design of our hardware system comprises a number of components (e.g., sensors, accessories, and cables) in a single box, which increases the difficulty of box assembly and heat dissipation. We carefully optimize the cabling and group similar components within the same shelve, making the box compact and lightweight. The size of the hardware box is about 20cm x 20cm x 20cm. Moreover, the sensing coverage of the sensors is limited, e.g., with the range of 0.35m-4.4m and a field-of-view (FOV) of 69°(H) x 51°(V) for the depth camera. In order to capture the main area of a living room and reduce the domain gap of different subjects’ data, we add a tripod at the bottom of the box to adjust the height and angle of the box.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Software System</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We now present the design of several major functions to improve the stability and scalability of the software system.
</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Long-term multi-modal sensor data recording and pre-processing</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">First, ADMarker needs to collect and store the data of multiple sensors continuously for up to months, which requires a large storage space and incurs significant power consumption. In particular, we set the sampling rates of the depth camera, mmWave radar, and microphone as 15 Hz, 20 Hz, and 44,100 Hz, respectively, which will result in around 4TB data during a four-week deployment.
To address this challenge, we save the depth data in an 8-bit format and compress the images into videos with OpenCV MJPG <cite class="ltx_cite ltx_citemacro_citep">(ope, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, bringing about a 75% reduction in data volume without significantly sacrificing the data quality. Second, the sensors may stop data recording occasionally, e.g., due to power surges or unstable sensor connections. We use the <em id="S6.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">systemd</em> service <cite class="ltx_cite ltx_citemacro_citep">(sys, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> of Linux to restart the sensors in case of sensor failures. Finally, to train the multi-modal models, the recorded sensor data is split into 2-second samples, and then converted into a fixed dimension [16,112,112], [20,2,16,32,16], and [20,87] for depth (cropped images), radar (voxels), and audio (Mel-frequency cepstral coefficients), respectively.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Improving training and inference efficiency</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">During online FL, a major challenge of continuous training and inference with all collected sensor data is the significant delay. For example, Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that unsupervised FL with 4,000 samples (i.e., data collected in about 2.2 hours) will incur a training delay of 85 hours (3-4 days). This will not only increase the data storage requirement on the device, but also affect the model accuracy due to the delayed updating of collected data during model training. Therefore, we propose two approaches to improve training and inference efficiency.</p>
</div>
<div id="S6.SS2.SSS2.p2" class="ltx_para">
<p id="S6.SS2.SSS2.p2.1" class="ltx_p">First, we apply the following online data selection strategies to reduce the model training delay while maintaining the effectiveness of AD symptom monitoring. Basically, the sensors will only collect data during a 12-hour period (i.e., 7:00-19:00) that contains fundamental daily activities, such as having meals, house-holding, etc. Moreover, rather than saving all recorded data, we evenly sample the data over time and only choose 1% of the data for model training. The reason is that most activities of interest, such as sitting, walking, or standing, last for a relatively long time (e.g., several minutes).
Finally, we use Yolov5 <cite class="ltx_cite ltx_citemacro_citep">(yol, <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> to detect humans in the depth images and delete the data samples without humans.</p>
</div>
<div id="S6.SS2.SSS2.p3" class="ltx_para">
<p id="S6.SS2.SSS2.p3.1" class="ltx_p">To reduce the on-device inference latency, we design a multi-task scheduling scheme for accelerating the pipeline in end-to-end multi-modal inference, including multi-sensor data collection, data pre-processing, and model inference.
As shown in Figure <a href="#S6.F8" title="Figure 8 ‣ 6.2.2. Improving training and inference efficiency ‣ 6.2. Software System ‣ 6. System Implementation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, ADMarker will run the three major tasks in parallel instead of processing them sequentially. For example, during the inference of previous data frames on GPU, the tasks of recording and pre-processing the incoming data frames are scheduled to utilize the spare resources on CPUs.
Moreover, ADMarker maintains a pool of processes to handle the data pre-possessing tasks of three different modalities in parallel, further reducing the inference latency. As a result, ADMarker can detect biomarkers in real-time, e.g., 9.45 frames per second (of depth data) in the end-to-end multi-modal inference.</p>
</div>
<figure id="S6.F8" class="ltx_figure"><img src="/html/2310.15301/assets/x10.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S6.F8.3.2" class="ltx_text" style="font-size:90%;">Illustration of our multi-process pipeline inference and conventional sequential inference scheme.</span></figcaption>
</figure>
</section>
<section id="S6.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3. </span>Private and stable communication networks</h4>

<div id="S6.SS2.SSS3.p1" class="ltx_para">
<p id="S6.SS2.SSS3.p1.1" class="ltx_p">To enable federated learning, the system should have highly stable and secure Internet connectivity to the server. However, the home WiFi or Ethernet connection of users should not be used due to privacy and cost concerns. Therefore, the ADMarker nodes are incorporated with a cellular interface to communicate with the server using 4G LTE through a Virtual Private Network (VPN). Moreover, to reduce the communication delay, we dynamically select the optimal frequency band on each node according to the runtime demand of FL, e.g., B3 (1800Mhz, 21Mbps) when uploading models to the server and B40 (2300Mhz, 20Mbps) when downloading models.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Clinical Deployment</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">A total of 91 elderly subjects (43 females and 48 males), aged between 61 and 93 years old, have participated in our clinical study<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All the data collection was approved by IRB and the Clinical Research Ethics Committee of the authors’ institution.</span></span></span>. As shown in Table <a href="#S7.T4" title="Table 4 ‣ 7. Clinical Deployment ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the participants were from three groups: 31 with Alzheimer’s Disease, 30 with mild cognitive impairment (MCI), and 30 are cognitively normal. The ADMarker node will be installed at the height of 1.5m-1.8m in the living room of the subject’s home for four weeks, as shown in Figure <a href="#S4.F7.sf2" title="In Figure 7 ‣ 4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a>. The installation process typically takes about ten minutes per home. The subjects and their caregivers were asked to fill in an “activity record form” by ticking the relevant daily activities, which served as the weak labels for behavior analysis (see Section <a href="#S4.SS4" title="4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<figure id="S7.T4" class="ltx_table">
<div id="S7.T4.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:118.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.1pt,-5.2pt) scale(1.09653601869617,1.09653601869617) ;">
<table id="S7.T4.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S7.T4.12.12.13" class="ltx_tr">
<td id="S7.T4.12.12.13.1" class="ltx_td ltx_align_center ltx_border_tt">Group</td>
<td id="S7.T4.12.12.13.2" class="ltx_td ltx_align_center ltx_border_tt">AD</td>
<td id="S7.T4.12.12.13.3" class="ltx_td ltx_align_center ltx_border_tt">MCI</td>
<td id="S7.T4.12.12.13.4" class="ltx_td ltx_align_center ltx_border_tt">NC</td>
</tr>
<tr id="S7.T4.12.12.14" class="ltx_tr">
<td id="S7.T4.12.12.14.1" class="ltx_td ltx_align_center ltx_border_t">No. of subjects</td>
<td id="S7.T4.12.12.14.2" class="ltx_td ltx_align_center ltx_border_t">31 (M/F:10/21)</td>
<td id="S7.T4.12.12.14.3" class="ltx_td ltx_align_center ltx_border_t">30 (M/F:18/12)</td>
<td id="S7.T4.12.12.14.4" class="ltx_td ltx_align_center ltx_border_t">30 (M/F:20/10)</td>
</tr>
<tr id="S7.T4.4.4.4" class="ltx_tr">
<td id="S7.T4.1.1.1.1" class="ltx_td ltx_align_center">Age (Mean<math id="S7.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.1.1.1.1.m1.1a"><mo id="S7.T4.1.1.1.1.m1.1.1" xref="S7.T4.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S7.T4.1.1.1.1.m1.1.1.cmml" xref="S7.T4.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.1.1.1.1.m1.1c">\pm</annotation></semantics></math>STD)</td>
<td id="S7.T4.2.2.2.2" class="ltx_td ltx_align_center">79.53<math id="S7.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.2.2.2.2.m1.1a"><mo id="S7.T4.2.2.2.2.m1.1.1" xref="S7.T4.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S7.T4.2.2.2.2.m1.1.1.cmml" xref="S7.T4.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.2.2.2.2.m1.1c">\pm</annotation></semantics></math>7.11</td>
<td id="S7.T4.3.3.3.3" class="ltx_td ltx_align_center">78.14<math id="S7.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.3.3.3.3.m1.1a"><mo id="S7.T4.3.3.3.3.m1.1.1" xref="S7.T4.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S7.T4.3.3.3.3.m1.1.1.cmml" xref="S7.T4.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.3.3.3.3.m1.1c">\pm</annotation></semantics></math>5.73</td>
<td id="S7.T4.4.4.4.4" class="ltx_td ltx_align_center">70.66<math id="S7.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.4.4.4.4.m1.1a"><mo id="S7.T4.4.4.4.4.m1.1.1" xref="S7.T4.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S7.T4.4.4.4.4.m1.1.1.cmml" xref="S7.T4.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.4.4.4.4.m1.1c">\pm</annotation></semantics></math>6.94</td>
</tr>
<tr id="S7.T4.12.12.15" class="ltx_tr">
<td id="S7.T4.12.12.15.1" class="ltx_td ltx_align_center">Living Mode (A/W-F/W-C)</td>
<td id="S7.T4.12.12.15.2" class="ltx_td ltx_align_center">2/25/4</td>
<td id="S7.T4.12.12.15.3" class="ltx_td ltx_align_center">2/27/1</td>
<td id="S7.T4.12.12.15.4" class="ltx_td ltx_align_center">6/24/0</td>
</tr>
<tr id="S7.T4.8.8.8" class="ltx_tr">
<td id="S7.T4.5.5.5.1" class="ltx_td ltx_align_center">Education Years (Mean<math id="S7.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.5.5.5.1.m1.1a"><mo id="S7.T4.5.5.5.1.m1.1.1" xref="S7.T4.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S7.T4.5.5.5.1.m1.1.1.cmml" xref="S7.T4.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.5.5.5.1.m1.1c">\pm</annotation></semantics></math>STD)</td>
<td id="S7.T4.6.6.6.2" class="ltx_td ltx_align_center">6.16<math id="S7.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.6.6.6.2.m1.1a"><mo id="S7.T4.6.6.6.2.m1.1.1" xref="S7.T4.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S7.T4.6.6.6.2.m1.1.1.cmml" xref="S7.T4.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.6.6.6.2.m1.1c">\pm</annotation></semantics></math>3.82</td>
<td id="S7.T4.7.7.7.3" class="ltx_td ltx_align_center">5.79<math id="S7.T4.7.7.7.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.7.7.7.3.m1.1a"><mo id="S7.T4.7.7.7.3.m1.1.1" xref="S7.T4.7.7.7.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.7.7.7.3.m1.1b"><csymbol cd="latexml" id="S7.T4.7.7.7.3.m1.1.1.cmml" xref="S7.T4.7.7.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.7.7.7.3.m1.1c">\pm</annotation></semantics></math>3.70</td>
<td id="S7.T4.8.8.8.4" class="ltx_td ltx_align_center">10.59<math id="S7.T4.8.8.8.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.8.8.8.4.m1.1a"><mo id="S7.T4.8.8.8.4.m1.1.1" xref="S7.T4.8.8.8.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.8.8.8.4.m1.1b"><csymbol cd="latexml" id="S7.T4.8.8.8.4.m1.1.1.cmml" xref="S7.T4.8.8.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.8.8.8.4.m1.1c">\pm</annotation></semantics></math>4.10</td>
</tr>
<tr id="S7.T4.12.12.12" class="ltx_tr">
<td id="S7.T4.9.9.9.1" class="ltx_td ltx_align_center ltx_border_bb">MoCA Score (Mean<math id="S7.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.9.9.9.1.m1.1a"><mo id="S7.T4.9.9.9.1.m1.1.1" xref="S7.T4.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S7.T4.9.9.9.1.m1.1.1.cmml" xref="S7.T4.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.9.9.9.1.m1.1c">\pm</annotation></semantics></math>STD)</td>
<td id="S7.T4.10.10.10.2" class="ltx_td ltx_align_center ltx_border_bb">11.75<math id="S7.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.10.10.10.2.m1.1a"><mo id="S7.T4.10.10.10.2.m1.1.1" xref="S7.T4.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S7.T4.10.10.10.2.m1.1.1.cmml" xref="S7.T4.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.10.10.10.2.m1.1c">\pm</annotation></semantics></math>6.38</td>
<td id="S7.T4.11.11.11.3" class="ltx_td ltx_align_center ltx_border_bb">19.50<math id="S7.T4.11.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.11.11.11.3.m1.1a"><mo id="S7.T4.11.11.11.3.m1.1.1" xref="S7.T4.11.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.11.11.11.3.m1.1b"><csymbol cd="latexml" id="S7.T4.11.11.11.3.m1.1.1.cmml" xref="S7.T4.11.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.11.11.11.3.m1.1c">\pm</annotation></semantics></math>6.53</td>
<td id="S7.T4.12.12.12.4" class="ltx_td ltx_align_center ltx_border_bb">25.07<math id="S7.T4.12.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T4.12.12.12.4.m1.1a"><mo id="S7.T4.12.12.12.4.m1.1.1" xref="S7.T4.12.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T4.12.12.12.4.m1.1b"><csymbol cd="latexml" id="S7.T4.12.12.12.4.m1.1.1.cmml" xref="S7.T4.12.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T4.12.12.12.4.m1.1c">\pm</annotation></semantics></math>4.41</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T4.14.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S7.T4.15.2" class="ltx_text" style="font-size:90%;">Demographic characteristics of enrolled elderly subjects (N = 91). A: Alone; W-F: With Family; W-C: With Caregiver. MoCA: Montreal Cognitive Assessment <cite class="ltx_cite ltx_citemacro_citep">(Nasreddine
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2005</a>)</cite>.</span></figcaption>
</figure>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Diagnosis of the subjects.</span> Before the study, each elderly subject underwent a screening test (i.e., Montreal Cognitive Assessment <cite class="ltx_cite ltx_citemacro_citep">(Nasreddine
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2005</a>)</cite>, MoCA) to generate a cognitive score. A low MoCA score indicates the risk of cognitive impairment.
The subjects then received physical examinations and face-to-face consultations with a neuropsychologist if they: (1) have no medical record of AD diagnosis but receive low cognitive scores in the screening test; (2) already have a medical record of AD diagnosis before, but their cognitive scores are obviously inconsistent with the medical records. Moreover, patients with AD and MCI who have not received an MRI test within one year are suggested to do a free MRI test, while the MRI test is voluntary for cognitively normal subjects. Then, the neuropsychologist will give the final diagnosis results of the enrolled subjects by combining the results of the face-to-face interview and MRI scan.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Evaluation</h2>

<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>Methodology</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p"><span id="S8.SS1.p1.1.1" class="ltx_text ltx_font_bold">Experiment settings.</span> Our evaluation is based on the clinical study of total 91 subjects. The ADMarker systems deployed in these subjects’ homes collected a total of 61,152 hours of multi-modal sensor data, with a total size of over 91TB. Among these, the data of 31 subjects is used for model pre-training, and 60 deployed nodes run the unsupervised and weakly supervised federated learning algorithms continuously for four weeks.
To evaluate the performance of AD diagnosis using the detected digital biomarkers, we adopt 3-fold cross-validation using the data of 69 subjects (22 AD, 25 MCI, 22 NC), because the remaining 22 subjects either have less than one week of valid sensor data or limited labeled data that contains humans (e.g., tens of samples).
We use the software packages <em id="S8.SS1.p1.1.2" class="ltx_emph ltx_font_italic">jetson-stats</em> from JetPack 4.6.1 <cite class="ltx_cite ltx_citemacro_citep">(jet, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> to record the power, memory/CPU/GPU usage and temperature of the hardware system, and use <em id="S8.SS1.p1.1.3" class="ltx_emph ltx_font_italic">speedtest-cli</em> <cite class="ltx_cite ltx_citemacro_citep">(spe, <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> to measure the network bandwidth and latency.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p"><span id="S8.SS1.p2.1.1" class="ltx_text ltx_font_bold">Configurations of models.</span> We use a CNN to extract deep features and a two-layer RNN to capture the time-series properties in the feature encoders, and use two dense layers for the classifier. Here 2D-CNN is used for audio MFCC features, and 3D-CNN is used for depth and radar data. The learning rate and batch size are 0.01 and 16 for unsupervised FL, and 0.001 and 16 for supervised FL. We use the data collected in the first and last weeks for model training and evaluation, respectively.</p>
</div>
<div id="S8.SS1.p3" class="ltx_para">
<p id="S8.SS1.p3.1" class="ltx_p"><span id="S8.SS1.p3.1.1" class="ltx_text ltx_font_bold">Data annotation</span> As a long-term autonomous monitoring system, ADMarker is expected to rely on as few manual annotations as possible. We use the “activity record form” (see Section <a href="#S4.SS4" title="4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>) provided by the subjects to generate the weak labels for weakly supervised FL. Moreover, the multi-modal data collected by the system are synchronized using the system clock and annotated using depth videos by a professional data labeling company.
The manually labeled data is used as ground truth for evaluating the performance of our system and helps to understand the performance of leveraging weak labels.</p>
</div>
<figure id="S8.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x11.png" id="S8.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F9.sf1.3.2" class="ltx_text" style="font-size:90%;">Training latency.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x12.png" id="S8.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F9.sf2.3.2" class="ltx_text" style="font-size:90%;">Energy consumption.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S8.F9.3.2" class="ltx_text" style="font-size:90%;">System overhead with or without the pre-trained model. “Unsup. Computing” denotes the computing time of nodes in unsupervised FL.</span></figcaption>
</figure>
<figure id="S8.F10" class="ltx_figure"><img src="/html/2310.15301/assets/x13.png" id="S8.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S8.F10.3.2" class="ltx_text" style="font-size:90%;">Round completion time of FL and mean bandwidth in a 24-hour operation.</span></figcaption>
</figure>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>System Overhead</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">In this section, we evaluate the system overhead of ADMaker during federated learning in the presence of various system dynamics (see Section <a href="#S4.SS1.SSS1" title="4.1.1. Understanding the real-world challenges. ‣ 4.1. A Motivation Study ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>).
For example, during the 4-week deployment, the sensors may stop data recording occasionally due to power surges or unstable sensor connections. The bandwidth of nodes also fluctuates over time.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para">
<p id="S8.SS2.p2.1" class="ltx_p">Figure <a href="#S8.F9" title="Figure 9 ‣ 8.1. Methodology ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> compares the mean training latency and energy consumption with or without centralized model pre-training in online FL. Here, the numbers of training samples in unsupervised and unsupervised FL are 300 and 100, respectively. When the models of nodes are initialized with the pre-trained model, the system overhead of both unsupervised and unsupervised FL is significantly reduced. For example, in unsupervised FL, the overall training latency and the energy consumption are reduced by 184.3min and 106KJ, respectively, since the models converge faster than training from scratch. Moreover, the system overhead of supervised FL is much smaller than unsupervised FL, because the feature encoder networks are already trained with large amounts of unlabeled multi-modal data in unsupervised FL.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para">
<p id="S8.SS2.p3.1" class="ltx_p">Figure <a href="#S8.F10" title="Figure 10 ‣ 8.1. Methodology ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> plots the variation of round completion time of FL and mean downlink bandwidth of nodes at different time of the day. The round completion time denotes the latency of finishing one global round of unsupervised FL (i.e., computing time for local training plus communication time). Generally, the training latency of one FL round at nighttime is shorter than at daytime (by about 14%), because of a better 4G LTE network connectivity. Moreover, the latency increases significantly at around 12:00, 17:00, and 19:00, which is probably caused by the increased network traffic from smartphone users during/after mealtimes. Therefore, the nodes in ADMarker could run federated learning at nighttime to reduce the overall training latency. We also note that the network congestion or node disconnectivity will not affect the accuracy of biomarker detection.</p>
</div>
<figure id="S8.F11" class="ltx_figure"><img src="/html/2310.15301/assets/x14.png" id="S8.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="S8.F11.3.2" class="ltx_text" style="font-size:90%;">Examples of activity distribution across nine subjects. The numbers denote the amount of data samples of the activities over four weeks.</span></figcaption>
</figure>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>Accuracy of Biomarker Detection</h3>

<section id="S8.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.1. </span>Overview of the subjects’ data</h4>

<div id="S8.SS3.SSS1.p1" class="ltx_para">
<p id="S8.SS3.SSS1.p1.1" class="ltx_p">We first show the overall characteristics of data collected by ADMarker during the four weeks of real-world deployment. First, Figure <a href="#S8.F11" title="Figure 11 ‣ 8.2. System Overhead ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows examples of the class distribution across nine subjects for daily living activities and basic body movements, respectively. We can observe obvious class imbalance in different subjects’ local data. Moreover, the class distributions of different subjects are highly non-i.i.d.
Second, Figure <a href="#S8.F12.sf1" title="In Figure 12 ‣ 8.3.1. Overview of the subjects’ data ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a> shows that there is only limited labeled sensor data during real-world deployment, due to the significant overhead and cost of data annotation. For example, the average amount of labeled data is less than 10% of unlabeled data. However, the amount of data with weak labels (see Section <a href="#S4.SS4" title="4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>) is much larger than manually annotated data, which can be leveraged to effectively train the models.
Finally, Figure <a href="#S8.F12.sf2" title="In Figure 12 ‣ 8.3.1. Overview of the subjects’ data ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a> presents the number of occurred activities during the deployment. In daily living activities, cognitively normal and MCI subjects exhibit more diverse activities than AD subjects, which shows the decline in cognitive and functional ability during the progression of AD <cite class="ltx_cite ltx_citemacro_citep">(Livingston et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<figure id="S8.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x15.png" id="S8.F12.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F12.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F12.sf1.3.2" class="ltx_text" style="font-size:90%;">Amount of data.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x16.png" id="S8.F12.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F12.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F12.sf2.3.2" class="ltx_text" style="font-size:90%;">Diversity of activities.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>. </span><span id="S8.F12.3.2" class="ltx_text" style="font-size:90%;">Data summary of different subject groups.</span></figcaption>
</figure>
</section>
<section id="S8.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.2. </span>Performance of biomarker detection</h4>

<div id="S8.SS3.SSS2.p1" class="ltx_para">
<p id="S8.SS3.SSS2.p1.1" class="ltx_p">Figure <a href="#S8.F13" title="Figure 13 ‣ 8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the accuracy of biomarker detection under different settings.</p>
</div>
<div id="S8.SS3.SSS2.p2" class="ltx_para">
<p id="S8.SS3.SSS2.p2.1" class="ltx_p"><span id="S8.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Performance at different stages of FL.</span>
Figure <a href="#S8.F13" title="Figure 13 ‣ 8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(a) shows the accuracy at different stages of federated learning. First, directly applying the pre-trained model has a very low detection accuracy on the participants during the deployment due to the large domain gap, e.g., with a mean accuracy of 56.37%.
Second, although the model accuracy after unsupervised FL (Stage 2 of ADMarker) is still low since the classifier layers are not trained, the feature encoders trained using large amounts of unlabeled data can significantly improve the accuracy performance at Stage 3. Finally, full-fledged ADMarker with a three-stage FL design can improve the model accuracy by combining the unlabeled and labeled data, e.g., 89.56% and 93.81% mean accuracy with weak and annotated labels, respectively.</p>
</div>
<figure id="S8.F13" class="ltx_figure"><img src="/html/2310.15301/assets/x17.png" id="S8.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>. </span><span id="S8.F13.3.2" class="ltx_text" style="font-size:90%;">Accuracy of biomarker detection at different stages, on different subject groups and activity classes.</span></figcaption>
</figure>
<div id="S8.SS3.SSS2.p3" class="ltx_para">
<p id="S8.SS3.SSS2.p3.1" class="ltx_p"><span id="S8.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Performance on different subjects.</span> Figure <a href="#S8.F13" title="Figure 13 ‣ 8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(b) shows the accuracy of biomarker detection on different subject groups, which varies among the three subject groups.
In particular, the mean accuracy is over 94.51% for AD subjects, while 87.83% and 91.67% for cognitively normal and MCI subjects, respectively. The reason is that the cognitively normal and MCI subjects generally exhibit significantly more diverse behaviors in daily living (see Figure <a href="#S8.F12.sf2" title="In Figure 12 ‣ 8.3.1. Overview of the subjects’ data ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(b)</span></a>), increasing the difficulty of detection.</p>
</div>
<div id="S8.SS3.SSS2.p4" class="ltx_para">
<p id="S8.SS3.SSS2.p4.3" class="ltx_p"><span id="S8.SS3.SSS2.p4.3.1" class="ltx_text ltx_font_bold">Dealing with class imbalance.</span> Figure <a href="#S8.F13" title="Figure 13 ‣ 8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(c) compares the performance of Fedavg with manually labeled data and ADMarker with weak labels on detecting different classes of activities. The models are initialized with weights trained at the second stage of ADMarker.
We define the behaviors with the most and least four data samples as the head classes (i.e., walking, sitting, standing, and eating) and tail classes (i.e., cleaning the living area, grooming, wiping hands, and exercising), respectively. Compared with Fedavg, the overall accuracy (all classes) is improved by <math id="S8.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S8.SS3.SSS2.p4.1.m1.1a"><mo id="S8.SS3.SSS2.p4.1.m1.1.1" xref="S8.SS3.SSS2.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S8.SS3.SSS2.p4.1.m1.1b"><csymbol cd="latexml" id="S8.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S8.SS3.SSS2.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S8.SS3.SSS2.p4.1.m1.1c">\sim</annotation></semantics></math>9%. However, Fedavg exhibits a very bad accuracy in detecting the tail classes, with <math id="S8.SS3.SSS2.p4.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S8.SS3.SSS2.p4.2.m2.1a"><mo id="S8.SS3.SSS2.p4.2.m2.1.1" xref="S8.SS3.SSS2.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S8.SS3.SSS2.p4.2.m2.1b"><csymbol cd="latexml" id="S8.SS3.SSS2.p4.2.m2.1.1.cmml" xref="S8.SS3.SSS2.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S8.SS3.SSS2.p4.2.m2.1c">\sim</annotation></semantics></math>18.33% on average. Our approach can improve the performance on tail classes by <math id="S8.SS3.SSS2.p4.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S8.SS3.SSS2.p4.3.m3.1a"><mo id="S8.SS3.SSS2.p4.3.m3.1.1" xref="S8.SS3.SSS2.p4.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S8.SS3.SSS2.p4.3.m3.1b"><csymbol cd="latexml" id="S8.SS3.SSS2.p4.3.m3.1.1.cmml" xref="S8.SS3.SSS2.p4.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S8.SS3.SSS2.p4.3.m3.1c">\sim</annotation></semantics></math>48.33% compared with Fedavg.</p>
</div>
<figure id="S8.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x18.png" id="S8.F14.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F14.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F14.sf1.3.2" class="ltx_text" style="font-size:90%;">Amount of annotated data.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x19.png" id="S8.F14.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F14.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F14.sf2.3.2" class="ltx_text" style="font-size:90%;">On different classes.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>. </span><span id="S8.F14.3.2" class="ltx_text" style="font-size:90%;">Understanding the weak labels by comparing with the performance using manually labeled data.</span></figcaption>
</figure>
</section>
<section id="S8.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.3. </span>Understanding the weak labels</h4>

<div id="S8.SS3.SSS3.p1" class="ltx_para">
<p id="S8.SS3.SSS3.p1.1" class="ltx_p">We further study the effectiveness of learning with weak labels in Section <a href="#S4.SS4" title="4.4. Weakly Supervised Multi-Modal FL ‣ 4. Federated Learning for Biomarker Detection ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> by comparing the performance with manually annotated data. To avoid the impact of different subject groups, the results are based on the data of cognitively normal subjects.</p>
</div>
<div id="S8.SS3.SSS3.p2" class="ltx_para">
<p id="S8.SS3.SSS3.p2.1" class="ltx_p"><span id="S8.SS3.SSS3.p2.1.1" class="ltx_text ltx_font_bold">With different amounts of annotated data.</span> Figure <a href="#S8.F14.sf1" title="In Figure 14 ‣ 8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a> compares the performance of two approaches among subjects with different amounts of annotated data. Although the weak labels provided by the subjects are usually noisy and sparse, the amount of associated weakly labeled data is usually larger than manually annotated data (see Figure <a href="#S8.F12.sf1" title="In Figure 12 ‣ 8.3.1. Overview of the subjects’ data ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a>). Therefore, when the subjects have very limited data (# of labels¡100), ADMarker with weak labels even performs better than with annotated labels, e.g., by 1.41% improvement in mean accuracy. However, when the subjects have enough labeled data (# of labels¡100), ADMarker with annotated data will have a better accuracy performance.</p>
</div>
<div id="S8.SS3.SSS3.p3" class="ltx_para">
<p id="S8.SS3.SSS3.p3.1" class="ltx_p"><span id="S8.SS3.SSS3.p3.1.1" class="ltx_text ltx_font_bold">On different classes.</span> We then compare the performance of two approaches on different classes, where the head and tail classes are defined the same as Section <a href="#S8.SS3.SSS2" title="8.3.2. Performance of biomarker detection ‣ 8.3. Accuracy of Biomarker Detection ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8.3.2</span></a>. Compared with ADMarker with annotated labels, the performance with weak labels is very close on the head classes, while worse on the tail classes. The reason is that the weak labels provided by the subjects are more reliable on “persistent” activities like walking, sitting, and eating, while noisy on dynamic activities like cleaning the living area, wiping hands, and exercising. Therefore, the performance of ADMarker with weak labels can be further improved if we combine only a limited amount of annotated labels on the tail classes.</p>
</div>
</section>
</section>
<section id="S8.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.4. </span>Interpretation of Detected Biomarkers</h3>

<section id="S8.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.4.1. </span>Utilizing the biomarkers for AD diagnosis.</h4>

<div id="S8.SS4.SSS1.p1" class="ltx_para">
<p id="S8.SS4.SSS1.p1.1" class="ltx_p">Figure <a href="#S8.F15" title="Figure 15 ‣ 8.4.1. Utilizing the biomarkers for AD diagnosis. ‣ 8.4. Interpretation of Detected Biomarkers ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows the effectiveness of utilizing the detected digital biomarkers for three diagnostic tasks (i.e., normal/MCI, non-AD/AD, and normal/MCI/AD). These tasks are clinically important and are consistent with the current practice of AD diagnosis <cite class="ltx_cite ltx_citemacro_citep">(Marshall et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2011</a>; Livingston
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>.
First, ADMarker achieves about 88.9% accuracy (33.3%+55.6%) in classifying MCI and cognitively normal subjects, meaning that ADMarker is able to identify people at early stage of AD. Second, the accuracy of non-AD/AD and normal/MCI/AD is 71.4% and 64.3%, respectively, which shows that identifying MCI from AD subjects is very difficult for subjects with various demographic and medical characteristics.
Nevertheless, the results are better than the state-of-the-art studies based on a single digital biomarker. For example, Tatc <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> identifies MCI with only 42.3% accuracy, and ADReSS <cite class="ltx_cite ltx_citemacro_citep">(Luz et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> achieves 60.8% for detecting AD.</p>
</div>
<figure id="S8.F15" class="ltx_figure"><img src="/html/2310.15301/assets/x20.png" id="S8.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>. </span><span id="S8.F15.3.2" class="ltx_text" style="font-size:90%;">The confusion matrix of utilizing the digital biomarkers for different AD diagnosis tasks.</span></figcaption>
</figure>
</section>
<section id="S8.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.4.2. </span>Advantages of leveraging multiple biomarkers.</h4>

<div id="S8.SS4.SSS2.p1" class="ltx_para">
<p id="S8.SS4.SSS2.p1.1" class="ltx_p">Figure <a href="#S8.F16.sf1" title="In Figure 16 ‣ 8.4.2. Advantages of leveraging multiple biomarkers. ‣ 8.4. Interpretation of Detected Biomarkers ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16(a)</span></a> compares the accuracy of identifying normal/MCI/AD, with vision-based biomarkers (e.g., walking, standing) using depth and radar data, audio-based biomarkers (e.g., talking) that can be captured with microphones, and all biomarkers (e.g., eating, phone call) that need a combination of all three sensors.
We observe that the diagnosis accuracy with only a single sensor modality is relatively low, e.g., 57.1% and 35.7% for vision and audio-based biomarkers, respectively. When multiple sensor modalities are used, the diagnosis accuracy has more than about 30% improvement over single
modality-based biomarkers. Figure <a href="#S8.F16.sf2" title="In Figure 16 ‣ 8.4.2. Advantages of leveraging multiple biomarkers. ‣ 8.4. Interpretation of Detected Biomarkers ‣ 8. Evaluation ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16(b)</span></a> shows the diagnosis accuracy with different numbers of biomarkers, where the biomarkers are sequentially added with the order of class index in Table <a href="#S3.T2" title="Table 2 ‣ 3.3. System Architecture ‣ 3. System Overview ‣ ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer’s Disease" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The results show that more than a few biomarkers are needed for accurate AD diagnosis, which demonstrates the significance of integrating multiple sensors for detecting multidimensional biomarkers in ADMarker.</p>
</div>
<figure id="S8.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F16.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x21.png" id="S8.F16.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F16.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F16.sf1.3.2" class="ltx_text" style="font-size:90%;">Different modalities.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F16.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.15301/assets/x22.png" id="S8.F16.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="358" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F16.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F16.sf2.3.2" class="ltx_text" style="font-size:90%;">Numbers of biomarkers.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>. </span><span id="S8.F16.3.2" class="ltx_text" style="font-size:90%;">Impact of leveraging multiple biomarkers.</span></figcaption>
</figure>
</section>
<section id="S8.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.4.3. </span>Correlation analysis with the diagnosis results</h4>

<div id="S8.SS4.SSS3.p1" class="ltx_para">
<p id="S8.SS4.SSS3.p1.1" class="ltx_p">Our results not only identify AD, but also shed light on system design, e.g., what digital biomarkers should be monitored more accurately, which reduces the system’s cost and improves the detection accuracy. Specifically, we analyze the correlation between each biomarker and the three diagnostic tasks with ANOVA <cite class="ltx_cite ltx_citemacro_citep">(St
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">1989</a>)</cite>.
We name those biomarkers with a p-value less than 0.05 as <span id="S8.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">cretical digital biomarkers</span>, as they are strongly associated with the diagnostic tasks.
First, the critical digital biomarkers for different diagnostic tasks are different. For example, phone calls, walking, and standing have a strong correlation in differentiating AD/non-AD, yet are not that useful in identifying normal and MCI.
Moreover, it is extremely hard to differentiate cognitively normal subjects and MCI, as only “sitting” is strongly correlated with this diagnosis task.
This also shows that MCI and cognitively normal subjects exhibit similar activity behaviors.
</p>
</div>
<div id="S8.SS4.SSS3.p2" class="ltx_para">
<p id="S8.SS4.SSS3.p2.1" class="ltx_p">In addition, we evaluate the impact of gender and age on diagnosis accuracy using the detected digital biomarkers. The results show that the diagnosis accuracy is similar among males (80.55%) and females (81.82%).
Moreover, the digital biomarkers are more effective
for the subjects aged 60-70 (88.89%) and 80-90 (89.47%) years old, compared with subjects aged 70–80 years old (84.38%).
We believe these results provide insights into further investigation in AD research.</p>
</div>
</section>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion and Discussion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">We propose ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. This study provides key insights into the development of clinically proven digital biomarkers for early AD diagnosis and intervention.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">Here we discuss some limitations of our design and future work. First, due to various human factors, the weak labels extracted from the activity
logs for online supervised FL are usually very sparse and limited. In the future, we will develop new approaches that require fewer weak labels during the deployment, e.g., by associating the weak and manually annotated labels during model pre-training on the cloud. The design of ADMarker can also leverage pseudo-labels automatically generated by running image classification algorithms on high-quality depth data <cite class="ltx_cite ltx_citemacro_citep">(Xia and Aggarwal, <a href="#bib.bib62" title="" class="ltx_ref">2013</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>.
Moreover, we will launch a larger-scale clinical study with an improved hardware design of ADMarker. We will also investigate personalized intervention strategies for different individuals based on the multi-dimensional digital biomarkers identified in this work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ad- (2021)</span>
<span class="ltx_bibblock">
2021.

</span>
<span class="ltx_bibblock">World Alzheimer Report 2021: Journey through the
diagnosis of dementia.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.alzint.org/resource/world-alzheimer-report-2021/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.alzint.org/resource/world-alzheimer-report-2021/</a>.
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">dig (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">ALZHEIMER’S DIGITAL BIOMARKERS.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.alzdiscovery.org/research-and-grants/funding-opportunities/diagnostics-accelerator-digital-biomarkers-program" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.alzdiscovery.org/research-and-grants/funding-opportunities/diagnostics-accelerator-digital-biomarkers-program</a>.
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">xav (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">NVDIA Xavier NX.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-nx/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-nx/</a>.
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ope (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">Opencv MJPG.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html</a>.
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">sys (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">Systemd.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://en.wikipedia.org/wiki/Systemd" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Systemd</a>.
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">yol (2022)</span>
<span class="ltx_bibblock">
2022.

</span>
<span class="ltx_bibblock">Yolov5.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://pytorch.org/hub/ultralytics_yolov5/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/hub/ultralytics_yolov5/</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">jet (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock">jetson-stats.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/rbonghi/jetson_stats" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rbonghi/jetson_stats</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">spe (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock">speedtest-cli.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://pypi.org/project/speedtest-cli/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/speedtest-cli/</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberdi et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Ane Alberdi, Alyssa
Weakley, Maureen Schmitter-Edgecombe,
Diane J Cook, Asier Aztiria,
Adrian Basarab, and Maitane
Barrenechea. 2018.

</span>
<span class="ltx_bibblock">Smart home-based prediction of multidomain symptoms
related to Alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health
informatics</span> 22, 6
(2018), 1720–1731.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altieri
et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Manuela Altieri, Federica
Garramone, and Gabriella Santangelo.
2021.

</span>
<span class="ltx_bibblock">Functional autonomy in dementia of the
Alzheimer’s type, mild cognitive impairment, and healthy aging: a
meta-analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">Neurological Sciences</span>
42 (2021), 1773–1783.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arai
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Asuna Arai, Amartuvshin
Khaltar, Takashi Ozaki, and Yuriko
Katsumata. 2021.

</span>
<span class="ltx_bibblock">Influence of social interaction on behavioral and
psychological symptoms of dementia over 1 year among long-term care facility
residents.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">Geriatric nursing</span> 42,
2 (2021), 509–516.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bayat et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sayeh Bayat, Ganesh M
Babulal, Suzanne E Schindler, Anne M
Fagan, John C Morris, Alex Mihailidis,
and Catherine M Roe. 2021.

</span>
<span class="ltx_bibblock">GPS driving: a digital biomarker for preclinical
Alzheimer disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">Alzheimer’s Research &amp; Therapy</span>
13, 1 (2021),
1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berkman
et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
Lisa F Berkman, Thomas
Glass, Ian Brissette, and Teresa E
Seeman. 2000.

</span>
<span class="ltx_bibblock">From social integration to health: Durkheim in the
new millennium.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">Social science &amp; medicine</span>
51, 6 (2000),
843–857.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown and
Forsythe (1974)</span>
<span class="ltx_bibblock">
Morton B Brown and
Alan B Forsythe. 1974.

</span>
<span class="ltx_bibblock">Robust tests for the equality of variances.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Journal of the American statistical
association</span> 69, 346
(1974), 364–367.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bui et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Nam Bui, Anh Nguyen,
Phuc Nguyen, Hoang Truong,
Ashwin Ashok, Thang Dinh,
Robin Deterding, and Tam Vu.
2017.

</span>
<span class="ltx_bibblock">PhO2: Smartphone Based Blood Oxygen Level
Measurement Systems Using Near-IR and RED Wave-Guided Light. In
<span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Proceedings of the 15th ACM Conference on Embedded
Network Sensor Systems</span> <span id="bib.bib16.4.2" class="ltx_text ltx_font_italic">(SenSys ’17)</span>.
Association for Computing Machinery,
New York, NY, USA, Article 26,
14 pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3131672.3131696" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3131672.3131696</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiangmao Chang, Cheng
Peng, Guoliang Xing, Tian Hao, and
Gang Zhou. 2020.

</span>
<span class="ltx_bibblock">ISleep: A Smartphone System for Unobtrusive Sleep
Quality Monitoring.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">ACM Trans. Sen. Netw.</span>
16, 3, Article 27
(jul 2020), 32 pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3392049" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3392049</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib18.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Richard Chen, Filip
Jankovic, Nikki Marinsek, Luca Foschini,
Lampros Kourtis, Alessio Signorini,
Melissa Pugh, Jie Shen,
Roy Yaari, Vera Maljkovic,
et al<span id="bib.bib18.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Developing measures of cognitive impairment in the
real world from consumer-grade multimodal sensor streams. In
<span id="bib.bib18.5.1" class="ltx_text ltx_font_italic">Proceedings of the 25th ACM SIGKDD international
conference on knowledge discovery &amp; data mining</span>.
2145–2155.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dawadi
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Prafulla Nath Dawadi,
Diane Joyce Cook, and Maureen
Schmitter-Edgecombe. 2015.

</span>
<span class="ltx_bibblock">Automated cognitive health assessment from smart
home-based behavior data.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health
informatics</span> 20, 4
(2015), 1188–1194.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doris
et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
SF Doris, Timothy Kwok,
Jacky Choy, and David J Kavanagh.
2016.

</span>
<span class="ltx_bibblock">Measuring the expressed emotion in Chinese family
caregivers of persons with dementia: Validation of a Chinese version of the
Family Attitude Scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">International journal of nursing studies</span>
55 (2016), 50–59.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fiest
et al<span id="bib.bib21.3.3.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kirsten M Fiest, Nathalie
Jette, Jodie I Roberts, Colleen J
Maxwell, Eric E Smith, Sandra E Black,
Laura Blaikie, Adrienne Cohen,
Lundy Day, Jayna Holroyd-Leduc,
et al<span id="bib.bib21.4.1" class="ltx_text">.</span> 2016.

</span>
<span class="ltx_bibblock">The prevalence and incidence of dementia: a
systematic review and meta-analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text ltx_font_italic">Canadian Journal of Neurological Sciences</span>
43, S1 (2016),
S3–S50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianping Gou, Baosheng
Yu, Stephen J Maybank, and Dacheng
Tao. 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>
129 (2021), 1789–1819.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guoliang Xing (2022)</span>
<span class="ltx_bibblock">
Guoliang Xing.
2022.

</span>
<span class="ltx_bibblock">Machine Learning Technologies for Advancing Digital
Biomarkers for Alzheimer’s Disease, Alzheimer’s Drug Discovery Foundation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.alzdiscovery.org/research-and-grants/portfolio-details/21130887" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.alzdiscovery.org/research-and-grants/portfolio-details/21130887</a>.
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gústafsdóttir (2015)</span>
<span class="ltx_bibblock">
Margrét Gústafsdóttir.
2015.

</span>
<span class="ltx_bibblock">Is watching television a realistic leisure option
for people with dementia.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Dementia and Geriatric Cognitive Disorders
Extra</span> 5, 1 (2015),
116–122.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Istepanian et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Robert Istepanian, Swamy
Laxminarayan, and Constantinos S Pattichis.
2007.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">M-health: Emerging mobile health systems</span>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib26.3.3.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wenjun Jiang, Chenglin
Miao, Fenglong Ma, Shuochao Yao,
Yaqing Wang, Ye Yuan,
Hongfei Xue, Chen Song,
Xin Ma, Dimitrios Koutsonikolas,
et al<span id="bib.bib26.4.1" class="ltx_text">.</span> 2018.

</span>
<span class="ltx_bibblock">Towards environment independent device free human
activity recognition. In <span id="bib.bib26.5.1" class="ltx_text ltx_font_italic">Proceedings of the 24th
Annual International Conference on Mobile Computing and Networking</span>.
289–304.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Daniel Ramage, and
Peter Richtárik. 2016.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine
learning for on-device intelligence.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">arXiv:1610.02527

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kourtis
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Lampros C Kourtis,
Oliver B Regele, Justin M Wright, and
Graham B Jones. 2019.

</span>
<span class="ltx_bibblock">Digital biomarkers for Alzheimer’s disease: the
mobile/wearable devices opportunity.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">NPJ digital medicine</span> 2,
1 (2019), 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Huining Li, Huan Chen,
Chenhan Xu, Zhengxiong Li,
Hanbin Zhang, Xiaoye Qian,
Dongmei Li, Ming-chun Huang, and
Wenyao Xu. 2023.

</span>
<span class="ltx_bibblock">NeuralGait: Assessing Brain Health Using Your
Smartphone.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">Proc. ACM Interact. Mob. Wearable Ubiquitous
Technol.</span> 6, 4, Article
169 (jan 2023),
28 pages.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3569476" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3569476</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jia Li, Yu Rong,
Helen Meng, Zhihui Lu,
Timothy Kwok, and Hong Cheng.
2018.

</span>
<span class="ltx_bibblock">TATC: predicting Alzheimer’s disease with
actigraphy data. In <span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</span>.
509–518.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Priya
Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. 2017.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection. In
<span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on
computer vision</span>. 2980–2988.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Xu-Ying Liu, Jianxin Wu,
and Zhi-Hua Zhou. 2008.

</span>
<span class="ltx_bibblock">Exploratory undersampling for class-imbalance
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Systems, Man, and
Cybernetics, Part B (Cybernetics)</span> 39,
2 (2008), 539–550.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Livingston
et al<span id="bib.bib33.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Gill Livingston, Jonathan
Huntley, Andrew Sommerlad, David Ames,
Clive Ballard, Sube Banerjee,
Carol Brayne, Alistair Burns,
Jiska Cohen-Mansfield, Claudia Cooper,
et al<span id="bib.bib33.4.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Dementia prevention, intervention, and care: 2020
report of the Lancet Commission.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text ltx_font_italic">The Lancet</span> 396,
10248 (2020), 413–446.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Livingston et al<span id="bib.bib34.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gill Livingston, Andrew
Sommerlad, Vasiliki Orgeta, Sergi G
Costafreda, Jonathan Huntley, David
Ames, Clive Ballard, Sube Banerjee,
Alistair Burns, Jiska Cohen-Mansfield,
et al<span id="bib.bib34.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Dementia prevention, intervention, and care.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text ltx_font_italic">The Lancet</span> 390,
10113 (2017), 2673–2734.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Hong Lu, Denise
Frauendorfer, Mashfiqui Rabbi,
Marianne Schmid Mast, Gokul T.
Chittaranjan, Andrew T. Campbell, Daniel
Gatica-Perez, and Tanzeem Choudhury.
2012.

</span>
<span class="ltx_bibblock">StressSense: Detecting Stress in Unconstrained
Acoustic Environments Using Smartphones. In <span id="bib.bib35.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2012 ACM Conference on Ubiquitous Computing</span> <span id="bib.bib35.4.2" class="ltx_text ltx_font_italic">(UbiComp ’12)</span>. Association for
Computing Machinery, New York, NY, USA,
351–360.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/2370216.2370270" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2370216.2370270</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luz et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Saturnino Luz, Fasih
Haider, Sofia de la Fuente, Davida
Fromm, and Brian MacWhinney.
2020.

</span>
<span class="ltx_bibblock">Alzheimer’s Dementia Recognition through
Spontaneous Speech: The ADReSS Challenge. In <span id="bib.bib36.3.1" class="ltx_text ltx_font_italic">Interspeech 2020</span>. ISCA, 2172–2176.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marique
et al<span id="bib.bib37.3.3.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
P Marique, F Peeters,
K Herholz, D Perani, V
Holthoff, E Kalbe, D Anchisi,
S Adam, F Collette, G
Garraux, et al<span id="bib.bib37.4.1" class="ltx_text">.</span> 2005.

</span>
<span class="ltx_bibblock">Cerebral metabolic correlates of four dementia
scales in Alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text ltx_font_italic">Journal of neurology</span>
252, 3 (2005),
283–290.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marshall et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Gad A Marshall, Lynn A
Fairbanks, Sibel Tekin, Harry V Vinters,
and Jeffrey L Cummings. 2006.

</span>
<span class="ltx_bibblock">Neuropathologic correlates of activities of daily
living in Alzheimer disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic">Alzheimer Disease &amp; Associated Disorders</span>
20, 1 (2006),
56–59.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marshall et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Gad A Marshall, Dorene M
Rentz, Meghan T Frey, Joseph J Locascio,
Keith A Johnson, Reisa A Sperling, and
Alzheimer’s Disease Neuroimaging Initiative.
2011.

</span>
<span class="ltx_bibblock">Executive function and instrumental activities of
daily living in mild cognitive impairment and Alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic">Alzheimer’s &amp; Dementia</span>
7, 3 (2011),
300–308.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib40.3.3.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
et al<span id="bib.bib40.4.1" class="ltx_text">.</span> 2016.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.05629</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McNamara et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (1992)</span>
<span class="ltx_bibblock">
Patrick McNamara,
Loraine K Obler, Rhoda Au,
Raymon Durso, and Martin L Albert.
1992.

</span>
<span class="ltx_bibblock">Speech monitoring skills in Alzheimer’s disease,
Parkinson’s disease, and normal aging.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic">Brain and language</span> 42,
1 (1992), 38–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miu
et al<span id="bib.bib42.3.3.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
D Miu, SL Szeto,
YF Mak, et al<span id="bib.bib42.4.1" class="ltx_text">.</span> 2008.

</span>
<span class="ltx_bibblock">A randomised controlled trial on the effect of
exercise on physical, cognitive and affective function in dementia subjects.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text ltx_font_italic">Asian J Gerontol Geriatr</span>
3, 1 (2008),
8–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Montero-Odasso et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Manuel Montero-Odasso, Joe
Verghese, Olivier Beauchet, and
Jeffrey M Hausdorff. 2012.

</span>
<span class="ltx_bibblock">Gait and cognition: a complementary approach to
understanding brain function and the risk of falling.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic">Journal of the American Geriatrics Society</span>
60, 11 (2012),
2127–2136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasreddine
et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
Ziad S Nasreddine,
Natalie A Phillips, Valérie
Bédirian, Simon Charbonneau, Victor
Whitehead, Isabelle Collin, Jeffrey L
Cummings, and Howard Chertkow.
2005.

</span>
<span class="ltx_bibblock">The Montreal Cognitive Assessment, MoCA: a brief
screening tool for mild cognitive impairment.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic">Journal of the American Geriatrics Society</span>
53, 4 (2005),
695–699.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasreen
et al<span id="bib.bib45.3.3.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Shamila Nasreen, Julian
Hough, Matthew Purver, et al<span id="bib.bib45.4.1" class="ltx_text">.</span>
2021.

</span>
<span class="ltx_bibblock">Detecting Alzheimer’s Disease using Interactional
and Acoustic features from Spontaneous Speech. Interspeech.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Xiaomin Ouyang, Xian
Shuai, Jiayu Zhou, Ivy Wang Shi,
Zhiyuan Xie, Guoliang Xing, and
Jianwei Huang. 2022a.

</span>
<span class="ltx_bibblock">Cosmo: contrastive fusion learning with small data
for multimodal human activity recognition. In <span id="bib.bib46.3.1" class="ltx_text ltx_font_italic">Proceedings of the 28th Annual International Conference on Mobile Computing
And Networking</span>. 324–337.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaomin Ouyang, Zhiyuan
Xie, Heming Fu, Sitong Cheng,
Li Pan, Neiwen Ling,
Guoliang Xing, Jiayu Zhou, and
Jianwei Huang. 2023.

</span>
<span class="ltx_bibblock">Harmony: Heterogeneous Multi-Modal Federated
Learning through Disentangled Model Training. In <span id="bib.bib47.3.1" class="ltx_text ltx_font_italic">Proceedings of the 21st Annual International Conference on Mobile Systems,
Applications and Services</span>. 530–543.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang
et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaomin Ouyang, Zhiyuan
Xie, Jiayu Zhou, Jianwei Huang, and
Guoliang Xing. 2021.

</span>
<span class="ltx_bibblock">Clusterfl: a similarity-aware federated learning
system for human activity recognition. In <span id="bib.bib48.3.1" class="ltx_text ltx_font_italic">Proceedings of the 19th Annual International Conference on Mobile Systems,
Applications, and Services</span>. 54–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang
et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Xiaomin Ouyang, Zhiyuan
Xie, Jiayu Zhou, Guoliang Xing, and
Jianwei Huang. 2022b.

</span>
<span class="ltx_bibblock">ClusterFL: A Clustering-based Federated Learning
System for Human Activity Recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic">ACM Transactions on Sensor Networks</span>
19, 1 (2022),
1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (1992)</span>
<span class="ltx_bibblock">
Marian B Patterson,
James L Mack, Marcia M Neundorfer,
Richard J Martin, Kathleen A Smyth, and
Peter J Whitehouse. 1992.

</span>
<span class="ltx_bibblock">Assessment of functional ability in Alzheimer
disease: a review and a preliminary report on the Cleveland Scale for
Activities of Daily Living.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic">Alzheimer Disease &amp; Associated Disorders</span>
6, 3 (1992),
145–163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Catherine Reed, Mark
Belger, Bruno Vellas, Jeffrey Scott
Andrews, Josep M Argimon, Giuseppe
Bruno, Richard Dodel, Roy W Jones,
Anders Wimo, and Josep Maria Haro.
2016.

</span>
<span class="ltx_bibblock">Identifying factors of activities of daily living
important for cost and caregiver outcomes in Alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic">International psychogeriatrics</span>
28, 2 (2016),
247–259.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakia (1992)</span>
<span class="ltx_bibblock">
Remi M Sakia.
1992.

</span>
<span class="ltx_bibblock">The Box-Cox transformation technique: a review.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Journal of the Royal Statistical Society
Series D: The Statistician</span> 41, 2
(1992), 169–178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuai
et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xian Shuai, Yulin Shen,
Siyang Jiang, Zhihe Zhao,
Zhenyu Yan, and Guoliang Xing.
2022.

</span>
<span class="ltx_bibblock">BalanceFL: Addressing class imbalance in long-tail
federated learning. In <span id="bib.bib53.3.1" class="ltx_text ltx_font_italic">2022 21st ACM/IEEE
International Conference on Information Processing in Sensor Networks
(IPSN)</span>. IEEE, 271–284.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuai
et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xian Shuai, Yulin Shen,
Yi Tang, Shuyao Shi,
Luping Ji, and Guoliang Xing.
2021.

</span>
<span class="ltx_bibblock">millieye: A lightweight mmwave radar and camera
fusion system for robust object detection. In <span id="bib.bib54.3.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on Internet-of-Things Design and
Implementation</span>. 145–157.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Bruno MC Silva, Joel JPC
Rodrigues, Isabel de la Torre Díez,
Miguel López-Coronado, and Kashif
Saleem. 2015.

</span>
<span class="ltx_bibblock">Mobile-health: A review of current state in 2015.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic">Journal of biomedical informatics</span>
56 (2015), 265–272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">St
et al<span id="bib.bib56.3.3.1" class="ltx_text">.</span> (1989)</span>
<span class="ltx_bibblock">
Lars St, Svante Wold,
et al<span id="bib.bib56.4.1" class="ltx_text">.</span> 1989.

</span>
<span class="ltx_bibblock">Analysis of variance (ANOVA).

</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text ltx_font_italic">Chemometrics and intelligent laboratory
systems</span> 6, 4 (1989),
259–272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stisen et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Allan Stisen, Henrik
Blunck, Sourav Bhattacharya, Thor Siiger
Prentow, Mikkel Baun Kjærgaard, Anind
Dey, Tobias Sonne, and Mads Møller
Jensen. 2015.

</span>
<span class="ltx_bibblock">Smart devices are different: Assessing and
mitigatingmobile sensing heterogeneities for activity recognition. In
<span id="bib.bib57.3.1" class="ltx_text ltx_font_italic">Proceedings of the 13th ACM conference on embedded
networked sensor systems</span>. 127–140.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2001)</span>
<span class="ltx_bibblock">
Sibel Tekin, Lynn A
Fairbanks, Susan O’Connor, Susan
Rosenberg, and Jeffrey L Cummings.
2001.

</span>
<span class="ltx_bibblock">Activities of daily living in Alzheimer’s disease:
neuropsychiatric, cognitive, and medical illness influences.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic">The American Journal of Geriatric
Psychiatry</span> 9, 1
(2001), 81–86.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yonglong Tian, Chen Sun,
Ben Poole, Dilip Krishnan,
Cordelia Schmid, and Phillip Isola.
2020.

</span>
<span class="ltx_bibblock">What makes for good views for contrastive
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.10243</span>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu
et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Linlin Tu, Xiaomin
Ouyang, Jiayu Zhou, Yuze He, and
Guoliang Xing. 2021.

</span>
<span class="ltx_bibblock">Feddl: Federated learning via dynamic layer sharing
for human activity recognition. In <span id="bib.bib60.3.1" class="ltx_text ltx_font_italic">Proceedings of
the 19th ACM Conference on Embedded Networked Sensor Systems</span>.
15–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yancheng Wang, Yang Xiao,
Fu Xiong, Wenxiang Jiang,
Zhiguo Cao, Joey Tianyi Zhou, and
Junsong Yuan. 2020.

</span>
<span class="ltx_bibblock">3dv: 3d dynamic voxel for action recognition in
depth video. In <span id="bib.bib61.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</span>.
511–520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia and Aggarwal (2013)</span>
<span class="ltx_bibblock">
Lu Xia and JK
Aggarwal. 2013.

</span>
<span class="ltx_bibblock">Spatio-temporal depth cuboid similarity feature for
activity recognition using depth camera. In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</span>. 2834–2841.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Huatao Xu, Pengfei Zhou,
Rui Tan, Mo Li, and
Guobin Shen. 2021.

</span>
<span class="ltx_bibblock">LIMU-BERT: Unleashing the Potential of Unlabeled
Data for IMU Sensing Applications. In <span id="bib.bib63.3.1" class="ltx_text ltx_font_italic">Proceedings
of the 19th ACM Conference on Embedded Networked Sensor Systems</span>.
220–233.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yasunori Yamada, Kaoru
Shinkawa, Masatomo Kobayashi, Vittorio
Caggiano, Miyuki Nemoto, Kiyotaka
Nemoto, and Tetsuaki Arai.
2021.

</span>
<span class="ltx_bibblock">Combining multimodal behavioral data of gait,
speech, and drawing for classification of Alzheimer’s disease and mild
cognitive impairment.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text ltx_font_italic">Journal of Alzheimer’s Disease</span>
84, 1 (2021),
315–327.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hanbin Zhang, Gabriel
Guo, Chen Song, Chenhan Xu,
Kevin Cheung, Jasleen Alexis,
Huining Li, Dongmei Li,
Kun Wang, and Wenyao Xu.
2020.

</span>
<span class="ltx_bibblock">PDLens: Smartphone Knows Drug Effectiveness among
Parkinson’s via Daily-Life Activity Fusion. In <span id="bib.bib65.3.1" class="ltx_text ltx_font_italic">Proceedings of the 26th Annual International Conference on Mobile Computing
and Networking</span> <span id="bib.bib65.4.2" class="ltx_text ltx_font_italic">(MobiCom ’20)</span>.
Association for Computing Machinery,
New York, NY, USA, Article 12,
14 pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3372224.3380889" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3372224.3380889</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuchen Zhao, Payam
Barnaghi, and Hamed Haddadi.
2022.

</span>
<span class="ltx_bibblock">Multimodal Federated Learning on IoT Data. In
<span id="bib.bib66.3.1" class="ltx_text ltx_font_italic">2022 IEEE/ACM Seventh International Conference on
Internet-of-Things Design and Implementation (IoTDI)</span>. IEEE,
43–54.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li,
Liangzhen Lai, Naveen Suda,
Damon Civin, and Vikas Chandra.
2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.00582</span>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.15300" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.15301" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.15301">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.15301" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.15302" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 22:37:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
