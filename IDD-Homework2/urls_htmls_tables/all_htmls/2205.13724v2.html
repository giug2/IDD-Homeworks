<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.13724] V-Doc : Visual questions answers with Documents</title><meta property="og:description" content="We propose V-Doc, a question-answering tool using document images and PDF, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answerin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="V-Doc : Visual questions answers with Documents">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="V-Doc : Visual questions answers with Documents">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.13724">

<!--Generated on Tue Feb 27 05:50:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">V-Doc : Visual questions answers with Documents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yihao Ding<sup id="id1.1.id1" class="ltx_sup">1</sup>,
Zhe Huang<sup id="id2.2.id2" class="ltx_sup">1*</sup>,
Runlin Wang<sup id="id3.3.id3" class="ltx_sup">1</sup>,
YanHang Zhang<sup id="id4.4.id4" class="ltx_sup">1</sup>,
Xianru Chen<sup id="id5.5.id5" class="ltx_sup">1</sup>,
<br class="ltx_break">Yuzhong Ma<sup id="id6.6.id6" class="ltx_sup">1</sup>,
Hyunsuk Chung<sup id="id7.7.id7" class="ltx_sup">2</sup>,
Soyeon Caren Han<sup id="id8.8.id8" class="ltx_sup">1</sup>

<br class="ltx_break"><sup id="id9.9.id9" class="ltx_sup">1</sup>The University of Sydney <sup id="id10.10.id10" class="ltx_sup">2</sup>FortifyEdge 
<br class="ltx_break"><span id="id11.11.id11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yihao.ding, hyunsuk.chung, caren.han}@sydney.edu.au,
<br class="ltx_break">{zhua8534, rwan2687, yzha5308, xche9640, yuma6895}@uni.sydney.edu.au
</span>
</span><span class="ltx_author_notes">
co-first authors

Corresponding author (caren.han@sydney.edu.au)
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">We propose V-Doc, a question-answering tool using document images and PDF, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answering tasks. The V-Doc supports generating and using both extractive and abstractive question-answer pairs using documents images. The extractive QA selects a subset of tokens or phrases from the document contents to predict the answers, while the abstractive QA recognises the language in the content and generates the answer based on the trained model. Both aspects are crucial to understanding the documents, especially in an image format. We include a detailed scenario of question generation for the abstractive QA task. V-Doc supports a wide range of datasets and models, and is highly extensible through a declarative, framework-agnostic platform.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Data and demo video: <a target="_blank" href="https://github.com/usydnlp/vdoc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/usydnlp/vdoc</a></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a multi-modal deep learning to answer text-based questions about an image. There are a set of VQA tasks defined based on various application scenarios, including statistical charts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, daily-life photos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and digital-born documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Among those VQA tasks, Document-VQA, which aims to extract information from documents and answer natural language questions, is more challenging. It requires detecting the scene objects and understanding the meaning and relation of those objects in order to predict reliable answers. Generally, existing QA can be classified into abstractive and extractive tasks. Extractive QA is to predict the answers by selecting a subset of tokens or phrases from document contents, while the answer of abstractive QA is generated by recognizing the document content based on the trained model. Different datasets and state-of-the-art models are introduced to deal with abstractive and extractive VQA problems.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2205.13724/assets/system_architecture.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of V-Doc platform architecture. Users can select sample images from datasets and use the trained models to predict the answer to input questions. Comparing the predicted answer between different models may represent the advantages of user proposed models to target clients.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, there are several Document-VQA datasets proposed by using website <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, textbook slides <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, scanned documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or cross-domain documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Many recent VQA models widely use those datasets, but most datasets use only single page domain documents such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> which ignore the situation of more frequently used multi-page documents in real world. In addition, most of existing abstractive QA task only focus on charts or figures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> not whole document pages.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Transformer and BERT architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> have gained many remarkable achievements in many natural language tasks, including text classification, language generation and text question answering. VQA challenges was solved by different multi-modal models, such as bilinear models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, image-text fusing based transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, etc. Although there are many state-of-the-art models proposed recently, there is no platform that can represent the novel model performance and compare it with some classical baselines.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper proposes a platform, V-Doc, which can allow users to upload their trained abstractive or extractive models on different datasets to compare with other baselines to represent the model improvements and advantages to their clients or targeting users. This platform mainly contains three modules from designing Dataset Storage and Model Storage modules to demonstrate the performance by a web application. Dataset Storage module contains a Compositional Question Generation Engine to generate a new document VQA dataset based on multi-page documents collected from PubMed Open Source Subset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov" title="" class="ltx_ref ltx_href">https://pubmed.ncbi.nlm.nih.gov</a></span></span></span>. It also provides some well pre-processed, commonly used Document-VQA datasets such as FUNSD. User can add and train/test with more dataset to the systems. The Model Storage module provides some trained models on different datasets in our Dataset Storage module, which also provides an environment to support user uploading and testing user trained models in our platform. Our web application can directly use the trained models in the Model Storage module to show the availability and reliability of selected models to target clients.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The contributions of our paper mainly contain:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, Our V-Doc is the first platform for Document-VQA which contains three modules including Dataset Storage, Model Storage and Graph User Interface for users.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce a new dataset generation engine and introduce a new Document-VQA dataset, PubVQA which can be accessed in our Dataset Storage Module for filling the gap of lacking abstractive Document-VQA dataset in this area.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An extendable Data and Model Storage module is provided to support user uploading and comparing their novel models with baselines through a user friendly graph user interface.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System Architecture</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The V-Doc mainly contains three modules: 1) Dataset Storage, 2) Model Storage, and 3) a web application. The Dataset Storage provides existing pre-processed benchmark datasets and a question generation engine to generate QA pairs based on PubMed accessed papers. The Model Storage provides an environment to upload and store various trained Document-VQA models. The proposed web application can call the trained models based on users’ demands.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The Dataset Storage module provides trainable pre-processed datasets and a new question generation engine to generate new QA pairs. The raw PDF documents are collected from PubMed. Each document is split into several images and fed into the pre-trained Mask RCNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> model and Google vision API<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://cloud.google.com/vision" title="" class="ltx_ref ltx_href">https://cloud.google.com/vision</a></span></span></span> to generate the scene graphs of collected documents. The compositional question generation engine processes the scene graph files in order to generate a new abstractive Document-VQA dataset, named PubVQA.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The classical and widely-used deep learning models, MAC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, LayoutLMv2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, are included in the Model Storage module to deal with reading comprehension (abstractive) or information retrieval(extractive) tasks, respectively. Those typical architectures can be used as baselines to compare user uploaded trained models. This system allows user-defined or trained models built on commonly used machine learning frameworks.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The web application provides a graphical user interface for users instead of calling the predict method. The V-Doc web application consists of front-end designing and back-end development. The front-end is built by the JavaScript React framework, which is responsible for rendering the GUI, handling the input from users and passing it to the back-end, finally, rendering the answer that was collected from the back-end. Our back-end is built by the Python Flask restful framework. It loads the models from the Model Storage and calls the pre-defined predict functions via the machine learning framework, including Tensorflow and Pytorch. Each model is deployed in a separate back-end server to avoid the dependencies conflict. Thus, users can get the prediction from any particular model by uploading an image and writing down the question in the user interface.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Storage</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The Dataset Storage module provides some pre-processed Document-VQA datasets. It also contains a question generation module that provides a Compositional Question Generation Engine with well-processed scene graphs of collected PubMed papers to generate a new Document-VQA dataset, PubVQA. The question generation procedure mainly includes data collection, scene graph generation, and question generation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>PubVQA Dataset Generation</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Data Collection</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We collect a set of PDF documents from PubMed Open Access Subset based on PMCID and split the PDF documents into separate document images(.jpg file)<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/Belval/pdf2image" title="" class="ltx_ref ltx_href">https://github.com/Belval/pdf2image</a></span></span></span>. A pre-trained Mask RCNN model provided by IBM based on their proposed large-scale PubLaynet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> Dataset is used to conduct object detection for acquiring bounding box coordinates and the category type of each detected segment. Since the domain of PubLaynet is the same as our dataset, the outputs from this pre-trained Mask-RCNN model are reasonable and acceptable.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Scene Graph Generation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">After acquiring results from Mask-RCNN, we need to generate the scene graph of each image for representing the relation between detected segments. The generated scene graph can provide essential information and be used by the question generation engine to generate QA pairs of PubVQA datasets. The key attributes in the scene graph files contains <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">text content</span>, <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">reading order</span>, <span id="S3.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">parent child relation</span>, <span id="S3.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_italic">relative position</span>, etc. The procedures of acquiring those attributes to generate scene graph files include:</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13724/assets/x1.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="162" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">original image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13724/assets/x2.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="166" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Pre-processed page</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Scene Graph Generation Preprocessing Examples</span></figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Content Extraction:</span>
We apply the Google Cloud Vision API to implement the Optical Character Recognition (OCR) to extract the text content of each PDF segment (bounding box).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.2" class="ltx_p"><span id="S3.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">Reading Order:</span>
We design a method for assigning an index number to each segment according to the reading order that people tend to do. It mainly contains two steps: (1) Ordering segments based on the top-left y-coordinate to generate an initial Reading Order List. (2) If any segment has neighbours on its left side, but the current Reading Order List index is larger than that segment, exchange the location in Reading Order List. Keep doing step 2 to generate the final Reading Order List. Base on the proposed methods, the reading order of segment <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">A</annotation></semantics></math> to <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">F</annotation></semantics></math> in Figure <a href="#S3.F2.sf1" title="Figure 2(a) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> is <span id="S3.I1.i2.p1.2.2" class="ltx_text ltx_font_italic">A(figure), B(text), C(text), E(text), D(title), F(text)</span> and the index of each segment is represented in Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Gap Distance:</span>
We obtain the gap distance by calculating the shortest distance between two bounding boxes. There are three rules:
(1) If two bounding boxes overlap, the gap distance between the boxes is counted as 0.
(2) If both bounding boxes contain the sides that partially intersect on the x-axis or y-axis direction, the gap distance is the vertical or horizontal distance between two sides.
(3) If two bounding boxes have no intersection on the x-axis and y-axis, the gap distance is between the two closest vertices. The line segment <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">(c)</span>, <span id="S3.I1.i3.p1.1.3" class="ltx_text ltx_font_bold">(d)</span> in Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> represent the gap distance between segment <span id="S3.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">D(title)</span> to segment <span id="S3.I1.i3.p1.1.5" class="ltx_text ltx_font_italic">B(text)</span> and <span id="S3.I1.i3.p1.1.6" class="ltx_text ltx_font_italic">E(text)</span>, respectively.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.13" class="ltx_p"><span id="S3.I1.i4.p1.13.1" class="ltx_text ltx_font_bold">Category Refinement:</span>
We have five categories including <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mi id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.1.m1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.1.m1.1.1.1a" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.1.m1.1.1.4" xref="S3.I1.i4.p1.1.m1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.1.m1.1.1.1b" xref="S3.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.1.m1.1.1.5" xref="S3.I1.i4.p1.1.m1.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><times id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1"></times><ci id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3">𝑒</ci><ci id="S3.I1.i4.p1.1.m1.1.1.4.cmml" xref="S3.I1.i4.p1.1.m1.1.1.4">𝑥</ci><ci id="S3.I1.i4.p1.1.m1.1.1.5.cmml" xref="S3.I1.i4.p1.1.m1.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">text</annotation></semantics></math>, <math id="S3.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="title" display="inline"><semantics id="S3.I1.i4.p1.2.m2.1a"><mrow id="S3.I1.i4.p1.2.m2.1.1" xref="S3.I1.i4.p1.2.m2.1.1.cmml"><mi id="S3.I1.i4.p1.2.m2.1.1.2" xref="S3.I1.i4.p1.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.2.m2.1.1.1" xref="S3.I1.i4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.2.m2.1.1.3" xref="S3.I1.i4.p1.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.2.m2.1.1.1a" xref="S3.I1.i4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.2.m2.1.1.4" xref="S3.I1.i4.p1.2.m2.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.2.m2.1.1.1b" xref="S3.I1.i4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.2.m2.1.1.5" xref="S3.I1.i4.p1.2.m2.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.2.m2.1.1.1c" xref="S3.I1.i4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.2.m2.1.1.6" xref="S3.I1.i4.p1.2.m2.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><apply id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1"><times id="S3.I1.i4.p1.2.m2.1.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1.1"></times><ci id="S3.I1.i4.p1.2.m2.1.1.2.cmml" xref="S3.I1.i4.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.2.m2.1.1.3.cmml" xref="S3.I1.i4.p1.2.m2.1.1.3">𝑖</ci><ci id="S3.I1.i4.p1.2.m2.1.1.4.cmml" xref="S3.I1.i4.p1.2.m2.1.1.4">𝑡</ci><ci id="S3.I1.i4.p1.2.m2.1.1.5.cmml" xref="S3.I1.i4.p1.2.m2.1.1.5">𝑙</ci><ci id="S3.I1.i4.p1.2.m2.1.1.6.cmml" xref="S3.I1.i4.p1.2.m2.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">title</annotation></semantics></math>, <math id="S3.I1.i4.p1.3.m3.1" class="ltx_Math" alttext="list" display="inline"><semantics id="S3.I1.i4.p1.3.m3.1a"><mrow id="S3.I1.i4.p1.3.m3.1.1" xref="S3.I1.i4.p1.3.m3.1.1.cmml"><mi id="S3.I1.i4.p1.3.m3.1.1.2" xref="S3.I1.i4.p1.3.m3.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.3.m3.1.1.1" xref="S3.I1.i4.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.3.m3.1.1.3" xref="S3.I1.i4.p1.3.m3.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.3.m3.1.1.1a" xref="S3.I1.i4.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.3.m3.1.1.4" xref="S3.I1.i4.p1.3.m3.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.3.m3.1.1.1b" xref="S3.I1.i4.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.3.m3.1.1.5" xref="S3.I1.i4.p1.3.m3.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.3.m3.1b"><apply id="S3.I1.i4.p1.3.m3.1.1.cmml" xref="S3.I1.i4.p1.3.m3.1.1"><times id="S3.I1.i4.p1.3.m3.1.1.1.cmml" xref="S3.I1.i4.p1.3.m3.1.1.1"></times><ci id="S3.I1.i4.p1.3.m3.1.1.2.cmml" xref="S3.I1.i4.p1.3.m3.1.1.2">𝑙</ci><ci id="S3.I1.i4.p1.3.m3.1.1.3.cmml" xref="S3.I1.i4.p1.3.m3.1.1.3">𝑖</ci><ci id="S3.I1.i4.p1.3.m3.1.1.4.cmml" xref="S3.I1.i4.p1.3.m3.1.1.4">𝑠</ci><ci id="S3.I1.i4.p1.3.m3.1.1.5.cmml" xref="S3.I1.i4.p1.3.m3.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.3.m3.1c">list</annotation></semantics></math>, <math id="S3.I1.i4.p1.4.m4.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.I1.i4.p1.4.m4.1a"><mrow id="S3.I1.i4.p1.4.m4.1.1" xref="S3.I1.i4.p1.4.m4.1.1.cmml"><mi id="S3.I1.i4.p1.4.m4.1.1.2" xref="S3.I1.i4.p1.4.m4.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.4.m4.1.1.1" xref="S3.I1.i4.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.4.m4.1.1.3" xref="S3.I1.i4.p1.4.m4.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.4.m4.1.1.1a" xref="S3.I1.i4.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.4.m4.1.1.4" xref="S3.I1.i4.p1.4.m4.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.4.m4.1.1.1b" xref="S3.I1.i4.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.4.m4.1.1.5" xref="S3.I1.i4.p1.4.m4.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.4.m4.1.1.1c" xref="S3.I1.i4.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.4.m4.1.1.6" xref="S3.I1.i4.p1.4.m4.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.4.m4.1b"><apply id="S3.I1.i4.p1.4.m4.1.1.cmml" xref="S3.I1.i4.p1.4.m4.1.1"><times id="S3.I1.i4.p1.4.m4.1.1.1.cmml" xref="S3.I1.i4.p1.4.m4.1.1.1"></times><ci id="S3.I1.i4.p1.4.m4.1.1.2.cmml" xref="S3.I1.i4.p1.4.m4.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.4.m4.1.1.3.cmml" xref="S3.I1.i4.p1.4.m4.1.1.3">𝑎</ci><ci id="S3.I1.i4.p1.4.m4.1.1.4.cmml" xref="S3.I1.i4.p1.4.m4.1.1.4">𝑏</ci><ci id="S3.I1.i4.p1.4.m4.1.1.5.cmml" xref="S3.I1.i4.p1.4.m4.1.1.5">𝑙</ci><ci id="S3.I1.i4.p1.4.m4.1.1.6.cmml" xref="S3.I1.i4.p1.4.m4.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.4.m4.1c">table</annotation></semantics></math>, <math id="S3.I1.i4.p1.5.m5.1" class="ltx_Math" alttext="figure" display="inline"><semantics id="S3.I1.i4.p1.5.m5.1a"><mrow id="S3.I1.i4.p1.5.m5.1.1" xref="S3.I1.i4.p1.5.m5.1.1.cmml"><mi id="S3.I1.i4.p1.5.m5.1.1.2" xref="S3.I1.i4.p1.5.m5.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.5.m5.1.1.1" xref="S3.I1.i4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.5.m5.1.1.3" xref="S3.I1.i4.p1.5.m5.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.5.m5.1.1.1a" xref="S3.I1.i4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.5.m5.1.1.4" xref="S3.I1.i4.p1.5.m5.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.5.m5.1.1.1b" xref="S3.I1.i4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.5.m5.1.1.5" xref="S3.I1.i4.p1.5.m5.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.5.m5.1.1.1c" xref="S3.I1.i4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.5.m5.1.1.6" xref="S3.I1.i4.p1.5.m5.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.5.m5.1.1.1d" xref="S3.I1.i4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.5.m5.1.1.7" xref="S3.I1.i4.p1.5.m5.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.5.m5.1b"><apply id="S3.I1.i4.p1.5.m5.1.1.cmml" xref="S3.I1.i4.p1.5.m5.1.1"><times id="S3.I1.i4.p1.5.m5.1.1.1.cmml" xref="S3.I1.i4.p1.5.m5.1.1.1"></times><ci id="S3.I1.i4.p1.5.m5.1.1.2.cmml" xref="S3.I1.i4.p1.5.m5.1.1.2">𝑓</ci><ci id="S3.I1.i4.p1.5.m5.1.1.3.cmml" xref="S3.I1.i4.p1.5.m5.1.1.3">𝑖</ci><ci id="S3.I1.i4.p1.5.m5.1.1.4.cmml" xref="S3.I1.i4.p1.5.m5.1.1.4">𝑔</ci><ci id="S3.I1.i4.p1.5.m5.1.1.5.cmml" xref="S3.I1.i4.p1.5.m5.1.1.5">𝑢</ci><ci id="S3.I1.i4.p1.5.m5.1.1.6.cmml" xref="S3.I1.i4.p1.5.m5.1.1.6">𝑟</ci><ci id="S3.I1.i4.p1.5.m5.1.1.7.cmml" xref="S3.I1.i4.p1.5.m5.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.5.m5.1c">figure</annotation></semantics></math> for the outputs from pre-trained Mask-RCNN on Publaynet. However, in order to generate more comprehensive QA pairs, the predicted category of some segments should be updated especially for altering some <math id="S3.I1.i4.p1.6.m6.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i4.p1.6.m6.1a"><mrow id="S3.I1.i4.p1.6.m6.1.1" xref="S3.I1.i4.p1.6.m6.1.1.cmml"><mi id="S3.I1.i4.p1.6.m6.1.1.2" xref="S3.I1.i4.p1.6.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.6.m6.1.1.1" xref="S3.I1.i4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.6.m6.1.1.3" xref="S3.I1.i4.p1.6.m6.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.6.m6.1.1.1a" xref="S3.I1.i4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.6.m6.1.1.4" xref="S3.I1.i4.p1.6.m6.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.6.m6.1.1.1b" xref="S3.I1.i4.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.6.m6.1.1.5" xref="S3.I1.i4.p1.6.m6.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.6.m6.1b"><apply id="S3.I1.i4.p1.6.m6.1.1.cmml" xref="S3.I1.i4.p1.6.m6.1.1"><times id="S3.I1.i4.p1.6.m6.1.1.1.cmml" xref="S3.I1.i4.p1.6.m6.1.1.1"></times><ci id="S3.I1.i4.p1.6.m6.1.1.2.cmml" xref="S3.I1.i4.p1.6.m6.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.6.m6.1.1.3.cmml" xref="S3.I1.i4.p1.6.m6.1.1.3">𝑒</ci><ci id="S3.I1.i4.p1.6.m6.1.1.4.cmml" xref="S3.I1.i4.p1.6.m6.1.1.4">𝑥</ci><ci id="S3.I1.i4.p1.6.m6.1.1.5.cmml" xref="S3.I1.i4.p1.6.m6.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.6.m6.1c">text</annotation></semantics></math> segments to <math id="S3.I1.i4.p1.7.m7.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.I1.i4.p1.7.m7.1a"><mrow id="S3.I1.i4.p1.7.m7.1.1" xref="S3.I1.i4.p1.7.m7.1.1.cmml"><mi id="S3.I1.i4.p1.7.m7.1.1.2" xref="S3.I1.i4.p1.7.m7.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.7.m7.1.1.1" xref="S3.I1.i4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.7.m7.1.1.3" xref="S3.I1.i4.p1.7.m7.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.7.m7.1.1.1a" xref="S3.I1.i4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.7.m7.1.1.4" xref="S3.I1.i4.p1.7.m7.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.7.m7.1.1.1b" xref="S3.I1.i4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.7.m7.1.1.5" xref="S3.I1.i4.p1.7.m7.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.7.m7.1.1.1c" xref="S3.I1.i4.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.7.m7.1.1.6" xref="S3.I1.i4.p1.7.m7.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.7.m7.1b"><apply id="S3.I1.i4.p1.7.m7.1.1.cmml" xref="S3.I1.i4.p1.7.m7.1.1"><times id="S3.I1.i4.p1.7.m7.1.1.1.cmml" xref="S3.I1.i4.p1.7.m7.1.1.1"></times><ci id="S3.I1.i4.p1.7.m7.1.1.2.cmml" xref="S3.I1.i4.p1.7.m7.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.7.m7.1.1.3.cmml" xref="S3.I1.i4.p1.7.m7.1.1.3">𝑎</ci><ci id="S3.I1.i4.p1.7.m7.1.1.4.cmml" xref="S3.I1.i4.p1.7.m7.1.1.4">𝑏</ci><ci id="S3.I1.i4.p1.7.m7.1.1.5.cmml" xref="S3.I1.i4.p1.7.m7.1.1.5">𝑙</ci><ci id="S3.I1.i4.p1.7.m7.1.1.6.cmml" xref="S3.I1.i4.p1.7.m7.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.7.m7.1c">table</annotation></semantics></math> <math id="S3.I1.i4.p1.8.m8.1" class="ltx_Math" alttext="caption" display="inline"><semantics id="S3.I1.i4.p1.8.m8.1a"><mrow id="S3.I1.i4.p1.8.m8.1.1" xref="S3.I1.i4.p1.8.m8.1.1.cmml"><mi id="S3.I1.i4.p1.8.m8.1.1.2" xref="S3.I1.i4.p1.8.m8.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.3" xref="S3.I1.i4.p1.8.m8.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1a" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.4" xref="S3.I1.i4.p1.8.m8.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1b" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.5" xref="S3.I1.i4.p1.8.m8.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1c" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.6" xref="S3.I1.i4.p1.8.m8.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1d" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.7" xref="S3.I1.i4.p1.8.m8.1.1.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.8.m8.1.1.1e" xref="S3.I1.i4.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.8.m8.1.1.8" xref="S3.I1.i4.p1.8.m8.1.1.8.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.8.m8.1b"><apply id="S3.I1.i4.p1.8.m8.1.1.cmml" xref="S3.I1.i4.p1.8.m8.1.1"><times id="S3.I1.i4.p1.8.m8.1.1.1.cmml" xref="S3.I1.i4.p1.8.m8.1.1.1"></times><ci id="S3.I1.i4.p1.8.m8.1.1.2.cmml" xref="S3.I1.i4.p1.8.m8.1.1.2">𝑐</ci><ci id="S3.I1.i4.p1.8.m8.1.1.3.cmml" xref="S3.I1.i4.p1.8.m8.1.1.3">𝑎</ci><ci id="S3.I1.i4.p1.8.m8.1.1.4.cmml" xref="S3.I1.i4.p1.8.m8.1.1.4">𝑝</ci><ci id="S3.I1.i4.p1.8.m8.1.1.5.cmml" xref="S3.I1.i4.p1.8.m8.1.1.5">𝑡</ci><ci id="S3.I1.i4.p1.8.m8.1.1.6.cmml" xref="S3.I1.i4.p1.8.m8.1.1.6">𝑖</ci><ci id="S3.I1.i4.p1.8.m8.1.1.7.cmml" xref="S3.I1.i4.p1.8.m8.1.1.7">𝑜</ci><ci id="S3.I1.i4.p1.8.m8.1.1.8.cmml" xref="S3.I1.i4.p1.8.m8.1.1.8">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.8.m8.1c">caption</annotation></semantics></math> or <span id="S3.I1.i4.p1.13.2" class="ltx_text ltx_font_italic">figure caption</span>. Most of table and figure captions are the segments closest to the table or figure. In this case, based on the calculated gap distance results, it is easy to change the closest <math id="S3.I1.i4.p1.9.m9.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i4.p1.9.m9.1a"><mrow id="S3.I1.i4.p1.9.m9.1.1" xref="S3.I1.i4.p1.9.m9.1.1.cmml"><mi id="S3.I1.i4.p1.9.m9.1.1.2" xref="S3.I1.i4.p1.9.m9.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.9.m9.1.1.1" xref="S3.I1.i4.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.9.m9.1.1.3" xref="S3.I1.i4.p1.9.m9.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.9.m9.1.1.1a" xref="S3.I1.i4.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.9.m9.1.1.4" xref="S3.I1.i4.p1.9.m9.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.9.m9.1.1.1b" xref="S3.I1.i4.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.9.m9.1.1.5" xref="S3.I1.i4.p1.9.m9.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.9.m9.1b"><apply id="S3.I1.i4.p1.9.m9.1.1.cmml" xref="S3.I1.i4.p1.9.m9.1.1"><times id="S3.I1.i4.p1.9.m9.1.1.1.cmml" xref="S3.I1.i4.p1.9.m9.1.1.1"></times><ci id="S3.I1.i4.p1.9.m9.1.1.2.cmml" xref="S3.I1.i4.p1.9.m9.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.9.m9.1.1.3.cmml" xref="S3.I1.i4.p1.9.m9.1.1.3">𝑒</ci><ci id="S3.I1.i4.p1.9.m9.1.1.4.cmml" xref="S3.I1.i4.p1.9.m9.1.1.4">𝑥</ci><ci id="S3.I1.i4.p1.9.m9.1.1.5.cmml" xref="S3.I1.i4.p1.9.m9.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.9.m9.1c">text</annotation></semantics></math> segment to a <math id="S3.I1.i4.p1.10.m10.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.I1.i4.p1.10.m10.1a"><mrow id="S3.I1.i4.p1.10.m10.1.1" xref="S3.I1.i4.p1.10.m10.1.1.cmml"><mi id="S3.I1.i4.p1.10.m10.1.1.2" xref="S3.I1.i4.p1.10.m10.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.10.m10.1.1.1" xref="S3.I1.i4.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.10.m10.1.1.3" xref="S3.I1.i4.p1.10.m10.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.10.m10.1.1.1a" xref="S3.I1.i4.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.10.m10.1.1.4" xref="S3.I1.i4.p1.10.m10.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.10.m10.1.1.1b" xref="S3.I1.i4.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.10.m10.1.1.5" xref="S3.I1.i4.p1.10.m10.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.10.m10.1.1.1c" xref="S3.I1.i4.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.10.m10.1.1.6" xref="S3.I1.i4.p1.10.m10.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.10.m10.1b"><apply id="S3.I1.i4.p1.10.m10.1.1.cmml" xref="S3.I1.i4.p1.10.m10.1.1"><times id="S3.I1.i4.p1.10.m10.1.1.1.cmml" xref="S3.I1.i4.p1.10.m10.1.1.1"></times><ci id="S3.I1.i4.p1.10.m10.1.1.2.cmml" xref="S3.I1.i4.p1.10.m10.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.10.m10.1.1.3.cmml" xref="S3.I1.i4.p1.10.m10.1.1.3">𝑎</ci><ci id="S3.I1.i4.p1.10.m10.1.1.4.cmml" xref="S3.I1.i4.p1.10.m10.1.1.4">𝑏</ci><ci id="S3.I1.i4.p1.10.m10.1.1.5.cmml" xref="S3.I1.i4.p1.10.m10.1.1.5">𝑙</ci><ci id="S3.I1.i4.p1.10.m10.1.1.6.cmml" xref="S3.I1.i4.p1.10.m10.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.10.m10.1c">table</annotation></semantics></math> or <math id="S3.I1.i4.p1.11.m11.1" class="ltx_Math" alttext="figure" display="inline"><semantics id="S3.I1.i4.p1.11.m11.1a"><mrow id="S3.I1.i4.p1.11.m11.1.1" xref="S3.I1.i4.p1.11.m11.1.1.cmml"><mi id="S3.I1.i4.p1.11.m11.1.1.2" xref="S3.I1.i4.p1.11.m11.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.11.m11.1.1.1" xref="S3.I1.i4.p1.11.m11.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.11.m11.1.1.3" xref="S3.I1.i4.p1.11.m11.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.11.m11.1.1.1a" xref="S3.I1.i4.p1.11.m11.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.11.m11.1.1.4" xref="S3.I1.i4.p1.11.m11.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.11.m11.1.1.1b" xref="S3.I1.i4.p1.11.m11.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.11.m11.1.1.5" xref="S3.I1.i4.p1.11.m11.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.11.m11.1.1.1c" xref="S3.I1.i4.p1.11.m11.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.11.m11.1.1.6" xref="S3.I1.i4.p1.11.m11.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.11.m11.1.1.1d" xref="S3.I1.i4.p1.11.m11.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.11.m11.1.1.7" xref="S3.I1.i4.p1.11.m11.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.11.m11.1b"><apply id="S3.I1.i4.p1.11.m11.1.1.cmml" xref="S3.I1.i4.p1.11.m11.1.1"><times id="S3.I1.i4.p1.11.m11.1.1.1.cmml" xref="S3.I1.i4.p1.11.m11.1.1.1"></times><ci id="S3.I1.i4.p1.11.m11.1.1.2.cmml" xref="S3.I1.i4.p1.11.m11.1.1.2">𝑓</ci><ci id="S3.I1.i4.p1.11.m11.1.1.3.cmml" xref="S3.I1.i4.p1.11.m11.1.1.3">𝑖</ci><ci id="S3.I1.i4.p1.11.m11.1.1.4.cmml" xref="S3.I1.i4.p1.11.m11.1.1.4">𝑔</ci><ci id="S3.I1.i4.p1.11.m11.1.1.5.cmml" xref="S3.I1.i4.p1.11.m11.1.1.5">𝑢</ci><ci id="S3.I1.i4.p1.11.m11.1.1.6.cmml" xref="S3.I1.i4.p1.11.m11.1.1.6">𝑟</ci><ci id="S3.I1.i4.p1.11.m11.1.1.7.cmml" xref="S3.I1.i4.p1.11.m11.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.11.m11.1c">figure</annotation></semantics></math> segment to <span id="S3.I1.i4.p1.13.3" class="ltx_text ltx_font_italic">table caption</span> or <span id="S3.I1.i4.p1.13.4" class="ltx_text ltx_font_italic">figure caption</span>. Based on those rules, the category of segment <math id="S3.I1.i4.p1.12.m12.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.I1.i4.p1.12.m12.1a"><mi id="S3.I1.i4.p1.12.m12.1.1" xref="S3.I1.i4.p1.12.m12.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.12.m12.1b"><ci id="S3.I1.i4.p1.12.m12.1.1.cmml" xref="S3.I1.i4.p1.12.m12.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.12.m12.1c">B</annotation></semantics></math> in Figure <a href="#S3.F2.sf1" title="Figure 2(a) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> is updated from <math id="S3.I1.i4.p1.13.m13.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i4.p1.13.m13.1a"><mrow id="S3.I1.i4.p1.13.m13.1.1" xref="S3.I1.i4.p1.13.m13.1.1.cmml"><mi id="S3.I1.i4.p1.13.m13.1.1.2" xref="S3.I1.i4.p1.13.m13.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.13.m13.1.1.1" xref="S3.I1.i4.p1.13.m13.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.13.m13.1.1.3" xref="S3.I1.i4.p1.13.m13.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.13.m13.1.1.1a" xref="S3.I1.i4.p1.13.m13.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.13.m13.1.1.4" xref="S3.I1.i4.p1.13.m13.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.13.m13.1.1.1b" xref="S3.I1.i4.p1.13.m13.1.1.1.cmml">​</mo><mi id="S3.I1.i4.p1.13.m13.1.1.5" xref="S3.I1.i4.p1.13.m13.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.13.m13.1b"><apply id="S3.I1.i4.p1.13.m13.1.1.cmml" xref="S3.I1.i4.p1.13.m13.1.1"><times id="S3.I1.i4.p1.13.m13.1.1.1.cmml" xref="S3.I1.i4.p1.13.m13.1.1.1"></times><ci id="S3.I1.i4.p1.13.m13.1.1.2.cmml" xref="S3.I1.i4.p1.13.m13.1.1.2">𝑡</ci><ci id="S3.I1.i4.p1.13.m13.1.1.3.cmml" xref="S3.I1.i4.p1.13.m13.1.1.3">𝑒</ci><ci id="S3.I1.i4.p1.13.m13.1.1.4.cmml" xref="S3.I1.i4.p1.13.m13.1.1.4">𝑥</ci><ci id="S3.I1.i4.p1.13.m13.1.1.5.cmml" xref="S3.I1.i4.p1.13.m13.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.13.m13.1c">text</annotation></semantics></math> to <span id="S3.I1.i4.p1.13.5" class="ltx_text ltx_font_italic">figure caption</span> in Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.10" class="ltx_p"><span id="S3.I1.i5.p1.10.1" class="ltx_text ltx_font_bold">Parent Child Relation:</span>
We easily generate the parent-child relation between document segments by using the reading order and category refinement results. There are three rules:
(1) If there is not other <math id="S3.I1.i5.p1.1.m1.1" class="ltx_Math" alttext="title" display="inline"><semantics id="S3.I1.i5.p1.1.m1.1a"><mrow id="S3.I1.i5.p1.1.m1.1.1" xref="S3.I1.i5.p1.1.m1.1.1.cmml"><mi id="S3.I1.i5.p1.1.m1.1.1.2" xref="S3.I1.i5.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.1.m1.1.1.1" xref="S3.I1.i5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.1.m1.1.1.3" xref="S3.I1.i5.p1.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.1.m1.1.1.1a" xref="S3.I1.i5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.1.m1.1.1.4" xref="S3.I1.i5.p1.1.m1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.1.m1.1.1.1b" xref="S3.I1.i5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.1.m1.1.1.5" xref="S3.I1.i5.p1.1.m1.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.1.m1.1.1.1c" xref="S3.I1.i5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.1.m1.1.1.6" xref="S3.I1.i5.p1.1.m1.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.1.m1.1b"><apply id="S3.I1.i5.p1.1.m1.1.1.cmml" xref="S3.I1.i5.p1.1.m1.1.1"><times id="S3.I1.i5.p1.1.m1.1.1.1.cmml" xref="S3.I1.i5.p1.1.m1.1.1.1"></times><ci id="S3.I1.i5.p1.1.m1.1.1.2.cmml" xref="S3.I1.i5.p1.1.m1.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.1.m1.1.1.3.cmml" xref="S3.I1.i5.p1.1.m1.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.1.m1.1.1.4.cmml" xref="S3.I1.i5.p1.1.m1.1.1.4">𝑡</ci><ci id="S3.I1.i5.p1.1.m1.1.1.5.cmml" xref="S3.I1.i5.p1.1.m1.1.1.5">𝑙</ci><ci id="S3.I1.i5.p1.1.m1.1.1.6.cmml" xref="S3.I1.i5.p1.1.m1.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.1.m1.1c">title</annotation></semantics></math> objects between two <math id="S3.I1.i5.p1.2.m2.1" class="ltx_Math" alttext="title" display="inline"><semantics id="S3.I1.i5.p1.2.m2.1a"><mrow id="S3.I1.i5.p1.2.m2.1.1" xref="S3.I1.i5.p1.2.m2.1.1.cmml"><mi id="S3.I1.i5.p1.2.m2.1.1.2" xref="S3.I1.i5.p1.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.2.m2.1.1.1" xref="S3.I1.i5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.2.m2.1.1.3" xref="S3.I1.i5.p1.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.2.m2.1.1.1a" xref="S3.I1.i5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.2.m2.1.1.4" xref="S3.I1.i5.p1.2.m2.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.2.m2.1.1.1b" xref="S3.I1.i5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.2.m2.1.1.5" xref="S3.I1.i5.p1.2.m2.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.2.m2.1.1.1c" xref="S3.I1.i5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.2.m2.1.1.6" xref="S3.I1.i5.p1.2.m2.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.2.m2.1b"><apply id="S3.I1.i5.p1.2.m2.1.1.cmml" xref="S3.I1.i5.p1.2.m2.1.1"><times id="S3.I1.i5.p1.2.m2.1.1.1.cmml" xref="S3.I1.i5.p1.2.m2.1.1.1"></times><ci id="S3.I1.i5.p1.2.m2.1.1.2.cmml" xref="S3.I1.i5.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.2.m2.1.1.3.cmml" xref="S3.I1.i5.p1.2.m2.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.2.m2.1.1.4.cmml" xref="S3.I1.i5.p1.2.m2.1.1.4">𝑡</ci><ci id="S3.I1.i5.p1.2.m2.1.1.5.cmml" xref="S3.I1.i5.p1.2.m2.1.1.5">𝑙</ci><ci id="S3.I1.i5.p1.2.m2.1.1.6.cmml" xref="S3.I1.i5.p1.2.m2.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.2.m2.1c">title</annotation></semantics></math> objects based on the Reading Order List, the first title is the parent of all <math id="S3.I1.i5.p1.3.m3.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i5.p1.3.m3.1a"><mrow id="S3.I1.i5.p1.3.m3.1.1" xref="S3.I1.i5.p1.3.m3.1.1.cmml"><mi id="S3.I1.i5.p1.3.m3.1.1.2" xref="S3.I1.i5.p1.3.m3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.3.m3.1.1.1" xref="S3.I1.i5.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.3.m3.1.1.3" xref="S3.I1.i5.p1.3.m3.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.3.m3.1.1.1a" xref="S3.I1.i5.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.3.m3.1.1.4" xref="S3.I1.i5.p1.3.m3.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.3.m3.1.1.1b" xref="S3.I1.i5.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.3.m3.1.1.5" xref="S3.I1.i5.p1.3.m3.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.3.m3.1b"><apply id="S3.I1.i5.p1.3.m3.1.1.cmml" xref="S3.I1.i5.p1.3.m3.1.1"><times id="S3.I1.i5.p1.3.m3.1.1.1.cmml" xref="S3.I1.i5.p1.3.m3.1.1.1"></times><ci id="S3.I1.i5.p1.3.m3.1.1.2.cmml" xref="S3.I1.i5.p1.3.m3.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.3.m3.1.1.3.cmml" xref="S3.I1.i5.p1.3.m3.1.1.3">𝑒</ci><ci id="S3.I1.i5.p1.3.m3.1.1.4.cmml" xref="S3.I1.i5.p1.3.m3.1.1.4">𝑥</ci><ci id="S3.I1.i5.p1.3.m3.1.1.5.cmml" xref="S3.I1.i5.p1.3.m3.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.3.m3.1c">text</annotation></semantics></math> or <math id="S3.I1.i5.p1.4.m4.1" class="ltx_Math" alttext="list" display="inline"><semantics id="S3.I1.i5.p1.4.m4.1a"><mrow id="S3.I1.i5.p1.4.m4.1.1" xref="S3.I1.i5.p1.4.m4.1.1.cmml"><mi id="S3.I1.i5.p1.4.m4.1.1.2" xref="S3.I1.i5.p1.4.m4.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.4.m4.1.1.1" xref="S3.I1.i5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.4.m4.1.1.3" xref="S3.I1.i5.p1.4.m4.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.4.m4.1.1.1a" xref="S3.I1.i5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.4.m4.1.1.4" xref="S3.I1.i5.p1.4.m4.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.4.m4.1.1.1b" xref="S3.I1.i5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.4.m4.1.1.5" xref="S3.I1.i5.p1.4.m4.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.4.m4.1b"><apply id="S3.I1.i5.p1.4.m4.1.1.cmml" xref="S3.I1.i5.p1.4.m4.1.1"><times id="S3.I1.i5.p1.4.m4.1.1.1.cmml" xref="S3.I1.i5.p1.4.m4.1.1.1"></times><ci id="S3.I1.i5.p1.4.m4.1.1.2.cmml" xref="S3.I1.i5.p1.4.m4.1.1.2">𝑙</ci><ci id="S3.I1.i5.p1.4.m4.1.1.3.cmml" xref="S3.I1.i5.p1.4.m4.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.4.m4.1.1.4.cmml" xref="S3.I1.i5.p1.4.m4.1.1.4">𝑠</ci><ci id="S3.I1.i5.p1.4.m4.1.1.5.cmml" xref="S3.I1.i5.p1.4.m4.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.4.m4.1c">list</annotation></semantics></math> segments until the next <math id="S3.I1.i5.p1.5.m5.1" class="ltx_Math" alttext="title" display="inline"><semantics id="S3.I1.i5.p1.5.m5.1a"><mrow id="S3.I1.i5.p1.5.m5.1.1" xref="S3.I1.i5.p1.5.m5.1.1.cmml"><mi id="S3.I1.i5.p1.5.m5.1.1.2" xref="S3.I1.i5.p1.5.m5.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.5.m5.1.1.1" xref="S3.I1.i5.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.5.m5.1.1.3" xref="S3.I1.i5.p1.5.m5.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.5.m5.1.1.1a" xref="S3.I1.i5.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.5.m5.1.1.4" xref="S3.I1.i5.p1.5.m5.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.5.m5.1.1.1b" xref="S3.I1.i5.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.5.m5.1.1.5" xref="S3.I1.i5.p1.5.m5.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.5.m5.1.1.1c" xref="S3.I1.i5.p1.5.m5.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.5.m5.1.1.6" xref="S3.I1.i5.p1.5.m5.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.5.m5.1b"><apply id="S3.I1.i5.p1.5.m5.1.1.cmml" xref="S3.I1.i5.p1.5.m5.1.1"><times id="S3.I1.i5.p1.5.m5.1.1.1.cmml" xref="S3.I1.i5.p1.5.m5.1.1.1"></times><ci id="S3.I1.i5.p1.5.m5.1.1.2.cmml" xref="S3.I1.i5.p1.5.m5.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.5.m5.1.1.3.cmml" xref="S3.I1.i5.p1.5.m5.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.5.m5.1.1.4.cmml" xref="S3.I1.i5.p1.5.m5.1.1.4">𝑡</ci><ci id="S3.I1.i5.p1.5.m5.1.1.5.cmml" xref="S3.I1.i5.p1.5.m5.1.1.5">𝑙</ci><ci id="S3.I1.i5.p1.5.m5.1.1.6.cmml" xref="S3.I1.i5.p1.5.m5.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.5.m5.1c">title</annotation></semantics></math> appearing such as relation <span id="S3.I1.i5.p1.10.2" class="ltx_text ltx_font_bold">(b)</span> in Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> .
(2) If there is no other types of segments between them based on reading order sequence, <math id="S3.I1.i5.p1.6.m6.1" class="ltx_Math" alttext="text" display="inline"><semantics id="S3.I1.i5.p1.6.m6.1a"><mrow id="S3.I1.i5.p1.6.m6.1.1" xref="S3.I1.i5.p1.6.m6.1.1.cmml"><mi id="S3.I1.i5.p1.6.m6.1.1.2" xref="S3.I1.i5.p1.6.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.6.m6.1.1.1" xref="S3.I1.i5.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.6.m6.1.1.3" xref="S3.I1.i5.p1.6.m6.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.6.m6.1.1.1a" xref="S3.I1.i5.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.6.m6.1.1.4" xref="S3.I1.i5.p1.6.m6.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.6.m6.1.1.1b" xref="S3.I1.i5.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.6.m6.1.1.5" xref="S3.I1.i5.p1.6.m6.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.6.m6.1b"><apply id="S3.I1.i5.p1.6.m6.1.1.cmml" xref="S3.I1.i5.p1.6.m6.1.1"><times id="S3.I1.i5.p1.6.m6.1.1.1.cmml" xref="S3.I1.i5.p1.6.m6.1.1.1"></times><ci id="S3.I1.i5.p1.6.m6.1.1.2.cmml" xref="S3.I1.i5.p1.6.m6.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.6.m6.1.1.3.cmml" xref="S3.I1.i5.p1.6.m6.1.1.3">𝑒</ci><ci id="S3.I1.i5.p1.6.m6.1.1.4.cmml" xref="S3.I1.i5.p1.6.m6.1.1.4">𝑥</ci><ci id="S3.I1.i5.p1.6.m6.1.1.5.cmml" xref="S3.I1.i5.p1.6.m6.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.6.m6.1c">text</annotation></semantics></math> or <math id="S3.I1.i5.p1.7.m7.1" class="ltx_Math" alttext="title" display="inline"><semantics id="S3.I1.i5.p1.7.m7.1a"><mrow id="S3.I1.i5.p1.7.m7.1.1" xref="S3.I1.i5.p1.7.m7.1.1.cmml"><mi id="S3.I1.i5.p1.7.m7.1.1.2" xref="S3.I1.i5.p1.7.m7.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.7.m7.1.1.1" xref="S3.I1.i5.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.7.m7.1.1.3" xref="S3.I1.i5.p1.7.m7.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.7.m7.1.1.1a" xref="S3.I1.i5.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.7.m7.1.1.4" xref="S3.I1.i5.p1.7.m7.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.7.m7.1.1.1b" xref="S3.I1.i5.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.7.m7.1.1.5" xref="S3.I1.i5.p1.7.m7.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.7.m7.1.1.1c" xref="S3.I1.i5.p1.7.m7.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.7.m7.1.1.6" xref="S3.I1.i5.p1.7.m7.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.7.m7.1b"><apply id="S3.I1.i5.p1.7.m7.1.1.cmml" xref="S3.I1.i5.p1.7.m7.1.1"><times id="S3.I1.i5.p1.7.m7.1.1.1.cmml" xref="S3.I1.i5.p1.7.m7.1.1.1"></times><ci id="S3.I1.i5.p1.7.m7.1.1.2.cmml" xref="S3.I1.i5.p1.7.m7.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.7.m7.1.1.3.cmml" xref="S3.I1.i5.p1.7.m7.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.7.m7.1.1.4.cmml" xref="S3.I1.i5.p1.7.m7.1.1.4">𝑡</ci><ci id="S3.I1.i5.p1.7.m7.1.1.5.cmml" xref="S3.I1.i5.p1.7.m7.1.1.5">𝑙</ci><ci id="S3.I1.i5.p1.7.m7.1.1.6.cmml" xref="S3.I1.i5.p1.7.m7.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.7.m7.1c">title</annotation></semantics></math> segment is the parent of the <math id="S3.I1.i5.p1.8.m8.1" class="ltx_Math" alttext="list" display="inline"><semantics id="S3.I1.i5.p1.8.m8.1a"><mrow id="S3.I1.i5.p1.8.m8.1.1" xref="S3.I1.i5.p1.8.m8.1.1.cmml"><mi id="S3.I1.i5.p1.8.m8.1.1.2" xref="S3.I1.i5.p1.8.m8.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.8.m8.1.1.1" xref="S3.I1.i5.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.8.m8.1.1.3" xref="S3.I1.i5.p1.8.m8.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.8.m8.1.1.1a" xref="S3.I1.i5.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.8.m8.1.1.4" xref="S3.I1.i5.p1.8.m8.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.8.m8.1.1.1b" xref="S3.I1.i5.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.8.m8.1.1.5" xref="S3.I1.i5.p1.8.m8.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.8.m8.1b"><apply id="S3.I1.i5.p1.8.m8.1.1.cmml" xref="S3.I1.i5.p1.8.m8.1.1"><times id="S3.I1.i5.p1.8.m8.1.1.1.cmml" xref="S3.I1.i5.p1.8.m8.1.1.1"></times><ci id="S3.I1.i5.p1.8.m8.1.1.2.cmml" xref="S3.I1.i5.p1.8.m8.1.1.2">𝑙</ci><ci id="S3.I1.i5.p1.8.m8.1.1.3.cmml" xref="S3.I1.i5.p1.8.m8.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.8.m8.1.1.4.cmml" xref="S3.I1.i5.p1.8.m8.1.1.4">𝑠</ci><ci id="S3.I1.i5.p1.8.m8.1.1.5.cmml" xref="S3.I1.i5.p1.8.m8.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.8.m8.1c">list</annotation></semantics></math> segments.
(3) If there is a <span id="S3.I1.i5.p1.10.3" class="ltx_text ltx_font_italic">table caption</span> or <span id="S3.I1.i5.p1.10.4" class="ltx_text ltx_font_italic">figure caption</span>, <math id="S3.I1.i5.p1.9.m9.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.I1.i5.p1.9.m9.1a"><mrow id="S3.I1.i5.p1.9.m9.1.1" xref="S3.I1.i5.p1.9.m9.1.1.cmml"><mi id="S3.I1.i5.p1.9.m9.1.1.2" xref="S3.I1.i5.p1.9.m9.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.9.m9.1.1.1" xref="S3.I1.i5.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.9.m9.1.1.3" xref="S3.I1.i5.p1.9.m9.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.9.m9.1.1.1a" xref="S3.I1.i5.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.9.m9.1.1.4" xref="S3.I1.i5.p1.9.m9.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.9.m9.1.1.1b" xref="S3.I1.i5.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.9.m9.1.1.5" xref="S3.I1.i5.p1.9.m9.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.9.m9.1.1.1c" xref="S3.I1.i5.p1.9.m9.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.9.m9.1.1.6" xref="S3.I1.i5.p1.9.m9.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.9.m9.1b"><apply id="S3.I1.i5.p1.9.m9.1.1.cmml" xref="S3.I1.i5.p1.9.m9.1.1"><times id="S3.I1.i5.p1.9.m9.1.1.1.cmml" xref="S3.I1.i5.p1.9.m9.1.1.1"></times><ci id="S3.I1.i5.p1.9.m9.1.1.2.cmml" xref="S3.I1.i5.p1.9.m9.1.1.2">𝑡</ci><ci id="S3.I1.i5.p1.9.m9.1.1.3.cmml" xref="S3.I1.i5.p1.9.m9.1.1.3">𝑎</ci><ci id="S3.I1.i5.p1.9.m9.1.1.4.cmml" xref="S3.I1.i5.p1.9.m9.1.1.4">𝑏</ci><ci id="S3.I1.i5.p1.9.m9.1.1.5.cmml" xref="S3.I1.i5.p1.9.m9.1.1.5">𝑙</ci><ci id="S3.I1.i5.p1.9.m9.1.1.6.cmml" xref="S3.I1.i5.p1.9.m9.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.9.m9.1c">table</annotation></semantics></math> or <math id="S3.I1.i5.p1.10.m10.1" class="ltx_Math" alttext="figure" display="inline"><semantics id="S3.I1.i5.p1.10.m10.1a"><mrow id="S3.I1.i5.p1.10.m10.1.1" xref="S3.I1.i5.p1.10.m10.1.1.cmml"><mi id="S3.I1.i5.p1.10.m10.1.1.2" xref="S3.I1.i5.p1.10.m10.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.10.m10.1.1.1" xref="S3.I1.i5.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.10.m10.1.1.3" xref="S3.I1.i5.p1.10.m10.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.10.m10.1.1.1a" xref="S3.I1.i5.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.10.m10.1.1.4" xref="S3.I1.i5.p1.10.m10.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.10.m10.1.1.1b" xref="S3.I1.i5.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.10.m10.1.1.5" xref="S3.I1.i5.p1.10.m10.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.10.m10.1.1.1c" xref="S3.I1.i5.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.10.m10.1.1.6" xref="S3.I1.i5.p1.10.m10.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.I1.i5.p1.10.m10.1.1.1d" xref="S3.I1.i5.p1.10.m10.1.1.1.cmml">​</mo><mi id="S3.I1.i5.p1.10.m10.1.1.7" xref="S3.I1.i5.p1.10.m10.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i5.p1.10.m10.1b"><apply id="S3.I1.i5.p1.10.m10.1.1.cmml" xref="S3.I1.i5.p1.10.m10.1.1"><times id="S3.I1.i5.p1.10.m10.1.1.1.cmml" xref="S3.I1.i5.p1.10.m10.1.1.1"></times><ci id="S3.I1.i5.p1.10.m10.1.1.2.cmml" xref="S3.I1.i5.p1.10.m10.1.1.2">𝑓</ci><ci id="S3.I1.i5.p1.10.m10.1.1.3.cmml" xref="S3.I1.i5.p1.10.m10.1.1.3">𝑖</ci><ci id="S3.I1.i5.p1.10.m10.1.1.4.cmml" xref="S3.I1.i5.p1.10.m10.1.1.4">𝑔</ci><ci id="S3.I1.i5.p1.10.m10.1.1.5.cmml" xref="S3.I1.i5.p1.10.m10.1.1.5">𝑢</ci><ci id="S3.I1.i5.p1.10.m10.1.1.6.cmml" xref="S3.I1.i5.p1.10.m10.1.1.6">𝑟</ci><ci id="S3.I1.i5.p1.10.m10.1.1.7.cmml" xref="S3.I1.i5.p1.10.m10.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.p1.10.m10.1c">figure</annotation></semantics></math> is the parent of the nearest caption segment. For example, based on Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> relation <span id="S3.I1.i5.p1.10.5" class="ltx_text ltx_font_bold">(a)</span>, segment <span id="S3.I1.i5.p1.10.6" class="ltx_text ltx_font_italic">A(figure)</span> is the parent of segment <span id="S3.I1.i5.p1.10.7" class="ltx_text ltx_font_italic">B(figure caption)</span>.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.12" class="ltx_p"><span id="S3.I1.i6.p1.12.1" class="ltx_text ltx_font_bold">Relative Position:</span>
We assign the relative position between two segments is assigned based on their bounding box coordinates. For example, in Figure <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, if two segments coincide in the y-axis direction, but not in the x-axis like relative position between element <math id="S3.I1.i6.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i6.p1.1.m1.1a"><mi id="S3.I1.i6.p1.1.m1.1.1" xref="S3.I1.i6.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.1.m1.1b"><ci id="S3.I1.i6.p1.1.m1.1.1.cmml" xref="S3.I1.i6.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.1.m1.1c">C</annotation></semantics></math> and <math id="S3.I1.i6.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.2.m2.1a"><mi id="S3.I1.i6.p1.2.m2.1.1" xref="S3.I1.i6.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.2.m2.1b"><ci id="S3.I1.i6.p1.2.m2.1.1.cmml" xref="S3.I1.i6.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.2.m2.1c">D</annotation></semantics></math>. We can define <math id="S3.I1.i6.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i6.p1.3.m3.1a"><mi id="S3.I1.i6.p1.3.m3.1.1" xref="S3.I1.i6.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.3.m3.1b"><ci id="S3.I1.i6.p1.3.m3.1.1.cmml" xref="S3.I1.i6.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.3.m3.1c">C</annotation></semantics></math> as on the left of <math id="S3.I1.i6.p1.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.4.m4.1a"><mi id="S3.I1.i6.p1.4.m4.1.1" xref="S3.I1.i6.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.4.m4.1b"><ci id="S3.I1.i6.p1.4.m4.1.1.cmml" xref="S3.I1.i6.p1.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.4.m4.1c">D</annotation></semantics></math>, and <math id="S3.I1.i6.p1.5.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.5.m5.1a"><mi id="S3.I1.i6.p1.5.m5.1.1" xref="S3.I1.i6.p1.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.5.m5.1b"><ci id="S3.I1.i6.p1.5.m5.1.1.cmml" xref="S3.I1.i6.p1.5.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.5.m5.1c">D</annotation></semantics></math> is on the right of <math id="S3.I1.i6.p1.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i6.p1.6.m6.1a"><mi id="S3.I1.i6.p1.6.m6.1.1" xref="S3.I1.i6.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.6.m6.1b"><ci id="S3.I1.i6.p1.6.m6.1.1.cmml" xref="S3.I1.i6.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.6.m6.1c">C</annotation></semantics></math>. If two segments do not coincide in both x and y-axis direction (e.g. between segments <math id="S3.I1.i6.p1.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I1.i6.p1.7.m7.1a"><mi id="S3.I1.i6.p1.7.m7.1.1" xref="S3.I1.i6.p1.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.7.m7.1b"><ci id="S3.I1.i6.p1.7.m7.1.1.cmml" xref="S3.I1.i6.p1.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.7.m7.1c">E</annotation></semantics></math> and <math id="S3.I1.i6.p1.8.m8.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.8.m8.1a"><mi id="S3.I1.i6.p1.8.m8.1.1" xref="S3.I1.i6.p1.8.m8.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.8.m8.1b"><ci id="S3.I1.i6.p1.8.m8.1.1.cmml" xref="S3.I1.i6.p1.8.m8.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.8.m8.1c">D</annotation></semantics></math>), we can define that <math id="S3.I1.i6.p1.9.m9.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.9.m9.1a"><mi id="S3.I1.i6.p1.9.m9.1.1" xref="S3.I1.i6.p1.9.m9.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.9.m9.1b"><ci id="S3.I1.i6.p1.9.m9.1.1.cmml" xref="S3.I1.i6.p1.9.m9.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.9.m9.1c">D</annotation></semantics></math> is located on the right-top of <math id="S3.I1.i6.p1.10.m10.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I1.i6.p1.10.m10.1a"><mi id="S3.I1.i6.p1.10.m10.1.1" xref="S3.I1.i6.p1.10.m10.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.10.m10.1b"><ci id="S3.I1.i6.p1.10.m10.1.1.cmml" xref="S3.I1.i6.p1.10.m10.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.10.m10.1c">E</annotation></semantics></math> and <math id="S3.I1.i6.p1.11.m11.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I1.i6.p1.11.m11.1a"><mi id="S3.I1.i6.p1.11.m11.1.1" xref="S3.I1.i6.p1.11.m11.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.11.m11.1b"><ci id="S3.I1.i6.p1.11.m11.1.1.cmml" xref="S3.I1.i6.p1.11.m11.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.11.m11.1c">E</annotation></semantics></math> is on the left-bottom of <math id="S3.I1.i6.p1.12.m12.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i6.p1.12.m12.1a"><mi id="S3.I1.i6.p1.12.m12.1.1" xref="S3.I1.i6.p1.12.m12.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i6.p1.12.m12.1b"><ci id="S3.I1.i6.p1.12.m12.1.1.cmml" xref="S3.I1.i6.p1.12.m12.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.p1.12.m12.1c">D</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.18.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.19.2" class="ltx_text" style="font-size:90%;">Example of questions and answers for each template</span></figcaption>
<table id="S3.T1.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.16.17.1" class="ltx_tr">
<th id="S3.T1.16.17.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.16.17.1.1.1" class="ltx_text ltx_font_bold">Question Type</span></th>
<th id="S3.T1.16.17.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.16.17.1.2.1" class="ltx_text ltx_font_bold">Sample Question Template</span></th>
<th id="S3.T1.16.17.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.16.17.1.3.1" class="ltx_text ltx_font_bold">Answers Type</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2" class="ltx_tr">
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S3.T1.2.2.3.1" class="ltx_text">Position Related</span></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Where is the caption of the <math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><lt id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">&lt;</annotation></semantics></math>E<math id="S3.T1.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.2.2.2.m2.1a"><mo id="S3.T1.2.2.2.m2.1.1" xref="S3.T1.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m2.1b"><gt id="S3.T1.2.2.2.m2.1.1.cmml" xref="S3.T1.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m2.1c">&gt;</annotation></semantics></math> located at?</td>
<td id="S3.T1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">top/bottom</td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_left">The caption is on which side of the <math id="S3.T1.3.3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.3.3.1.m1.1a"><mo id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><lt id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">&lt;</annotation></semantics></math>E<math id="S3.T1.4.4.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.4.4.2.m2.1a"><mo id="S3.T1.4.4.2.m2.1.1" xref="S3.T1.4.4.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.2.m2.1b"><gt id="S3.T1.4.4.2.m2.1.1.cmml" xref="S3.T1.4.4.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.2.m2.1c">&gt;</annotation></semantics></math>?</td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_left">top/bottom</td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<td id="S3.T1.10.10.7" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S3.T1.10.10.7.1" class="ltx_text">Counting Based</span></td>
<td id="S3.T1.10.10.6" class="ltx_td ltx_align_left ltx_border_t">How many <math id="S3.T1.5.5.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.5.5.1.m1.1a"><mo id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><lt id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">&lt;</annotation></semantics></math>E1<math id="S3.T1.6.6.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.6.6.2.m2.1a"><mo id="S3.T1.6.6.2.m2.1.1" xref="S3.T1.6.6.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.m2.1b"><gt id="S3.T1.6.6.2.m2.1.1.cmml" xref="S3.T1.6.6.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.m2.1c">&gt;</annotation></semantics></math> objects are located at the <math id="S3.T1.7.7.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.7.7.3.m3.1a"><mo id="S3.T1.7.7.3.m3.1.1" xref="S3.T1.7.7.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.3.m3.1b"><lt id="S3.T1.7.7.3.m3.1.1.cmml" xref="S3.T1.7.7.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.3.m3.1c">&lt;</annotation></semantics></math>R<math id="S3.T1.8.8.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.8.8.4.m4.1a"><mo id="S3.T1.8.8.4.m4.1.1" xref="S3.T1.8.8.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.4.m4.1b"><gt id="S3.T1.8.8.4.m4.1.1.cmml" xref="S3.T1.8.8.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.4.m4.1c">&gt;</annotation></semantics></math> side of <math id="S3.T1.9.9.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.9.9.5.m5.1a"><mo id="S3.T1.9.9.5.m5.1.1" xref="S3.T1.9.9.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.5.m5.1b"><lt id="S3.T1.9.9.5.m5.1.1.cmml" xref="S3.T1.9.9.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.5.m5.1c">&lt;</annotation></semantics></math>E2<math id="S3.T1.10.10.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.10.10.6.m6.1a"><mo id="S3.T1.10.10.6.m6.1.1" xref="S3.T1.10.10.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.6.m6.1b"><gt id="S3.T1.10.10.6.m6.1.1.cmml" xref="S3.T1.10.10.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.6.m6.1c">&gt;</annotation></semantics></math>?</td>
<td id="S3.T1.10.10.8" class="ltx_td ltx_align_left ltx_border_t">number</td>
</tr>
<tr id="S3.T1.16.16" class="ltx_tr">
<td id="S3.T1.16.16.6" class="ltx_td ltx_align_left">For <math id="S3.T1.11.11.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.11.11.1.m1.1a"><mo id="S3.T1.11.11.1.m1.1.1" xref="S3.T1.11.11.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.1.m1.1b"><lt id="S3.T1.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.1.m1.1c">&lt;</annotation></semantics></math>E2<math id="S3.T1.12.12.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.12.12.2.m2.1a"><mo id="S3.T1.12.12.2.m2.1.1" xref="S3.T1.12.12.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.2.m2.1b"><gt id="S3.T1.12.12.2.m2.1.1.cmml" xref="S3.T1.12.12.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.2.m2.1c">&gt;</annotation></semantics></math>, how many <math id="S3.T1.13.13.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.13.13.3.m3.1a"><mo id="S3.T1.13.13.3.m3.1.1" xref="S3.T1.13.13.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.3.m3.1b"><lt id="S3.T1.13.13.3.m3.1.1.cmml" xref="S3.T1.13.13.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.3.m3.1c">&lt;</annotation></semantics></math>E1<math id="S3.T1.14.14.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.14.14.4.m4.1a"><mo id="S3.T1.14.14.4.m4.1.1" xref="S3.T1.14.14.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.4.m4.1b"><gt id="S3.T1.14.14.4.m4.1.1.cmml" xref="S3.T1.14.14.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.4.m4.1c">&gt;</annotation></semantics></math> objects are located at its <math id="S3.T1.15.15.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.15.15.5.m5.1a"><mo id="S3.T1.15.15.5.m5.1.1" xref="S3.T1.15.15.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.5.m5.1b"><lt id="S3.T1.15.15.5.m5.1.1.cmml" xref="S3.T1.15.15.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.5.m5.1c">&lt;</annotation></semantics></math>R<math id="S3.T1.16.16.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.16.16.6.m6.1a"><mo id="S3.T1.16.16.6.m6.1.1" xref="S3.T1.16.16.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.16.16.6.m6.1b"><gt id="S3.T1.16.16.6.m6.1.1.cmml" xref="S3.T1.16.16.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.16.6.m6.1c">&gt;</annotation></semantics></math> side?</td>
<td id="S3.T1.16.16.7" class="ltx_td ltx_align_left">number</td>
</tr>
<tr id="S3.T1.16.18.1" class="ltx_tr">
<td id="S3.T1.16.18.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S3.T1.16.18.1.1.1" class="ltx_text">Existence Based</span></td>
<td id="S3.T1.16.18.1.2" class="ltx_td ltx_align_left ltx_border_t">Do title objects exist on this page?</td>
<td id="S3.T1.16.18.1.3" class="ltx_td ltx_align_left ltx_border_t">yes/no</td>
</tr>
<tr id="S3.T1.16.19.2" class="ltx_tr">
<td id="S3.T1.16.19.2.1" class="ltx_td ltx_align_left ltx_border_bb">Are there any titles that exist?</td>
<td id="S3.T1.16.19.2.2" class="ltx_td ltx_align_left ltx_border_bb">yes/no</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Question generation</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Inspired by the Clevr<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposed question generation producer on synthetic images, we designed a Compositional Question Generation Engine for real-world PDF documents by using generated scene graph files of each document image. Similar to Clevr question generation methods, some templates and functional programs are defined to generate QA pairs. Table <a href="#S3.T1" title="Table 1 ‣ 3.1.2 Scene Graph Generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the three different groups of templates, including position related, counting based and existence based with expected answer type.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.8" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1.3 Question generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> represents the workflows of the sequences of pre-defined functions to generate the corresponding QA pairs. For example, the position-related question workflow mainly consists of three pre-defined functions. To generate a question, <span id="S3.SS1.SSS3.p2.2.2" class="ltx_text ltx_font_italic">’Where is the caption of the <math id="S3.SS1.SSS3.p2.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.SSS3.p2.1.1.m1.1a"><mo id="S3.SS1.SSS3.p2.1.1.m1.1.1" xref="S3.SS1.SSS3.p2.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.1.1.m1.1b"><lt id="S3.SS1.SSS3.p2.1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.1.1.m1.1c">&lt;</annotation></semantics></math>E<math id="S3.SS1.SSS3.p2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS1.SSS3.p2.2.2.m2.1a"><mo id="S3.SS1.SSS3.p2.2.2.m2.1.1" xref="S3.SS1.SSS3.p2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.2.2.m2.1b"><gt id="S3.SS1.SSS3.p2.2.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.2.2.m2.1c">&gt;</annotation></semantics></math> located at ?’</span>, the engine needs to filter the target segments by using <math id="S3.SS1.SSS3.p2.3.m1.1" class="ltx_Math" alttext="Filter" display="inline"><semantics id="S3.SS1.SSS3.p2.3.m1.1a"><mrow id="S3.SS1.SSS3.p2.3.m1.1.1" xref="S3.SS1.SSS3.p2.3.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p2.3.m1.1.1.2" xref="S3.SS1.SSS3.p2.3.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.3.m1.1.1.1" xref="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.3.m1.1.1.3" xref="S3.SS1.SSS3.p2.3.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.3.m1.1.1.1a" xref="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.3.m1.1.1.4" xref="S3.SS1.SSS3.p2.3.m1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.3.m1.1.1.1b" xref="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.3.m1.1.1.5" xref="S3.SS1.SSS3.p2.3.m1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.3.m1.1.1.1c" xref="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.3.m1.1.1.6" xref="S3.SS1.SSS3.p2.3.m1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.3.m1.1.1.1d" xref="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.3.m1.1.1.7" xref="S3.SS1.SSS3.p2.3.m1.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.3.m1.1b"><apply id="S3.SS1.SSS3.p2.3.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1"><times id="S3.SS1.SSS3.p2.3.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.1"></times><ci id="S3.SS1.SSS3.p2.3.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.2">𝐹</ci><ci id="S3.SS1.SSS3.p2.3.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.3">𝑖</ci><ci id="S3.SS1.SSS3.p2.3.m1.1.1.4.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.4">𝑙</ci><ci id="S3.SS1.SSS3.p2.3.m1.1.1.5.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.5">𝑡</ci><ci id="S3.SS1.SSS3.p2.3.m1.1.1.6.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.6">𝑒</ci><ci id="S3.SS1.SSS3.p2.3.m1.1.1.7.cmml" xref="S3.SS1.SSS3.p2.3.m1.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.3.m1.1c">Filter</annotation></semantics></math> <math id="S3.SS1.SSS3.p2.4.m2.1" class="ltx_Math" alttext="category" display="inline"><semantics id="S3.SS1.SSS3.p2.4.m2.1a"><mrow id="S3.SS1.SSS3.p2.4.m2.1.1" xref="S3.SS1.SSS3.p2.4.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p2.4.m2.1.1.2" xref="S3.SS1.SSS3.p2.4.m2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.3" xref="S3.SS1.SSS3.p2.4.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1a" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.4" xref="S3.SS1.SSS3.p2.4.m2.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1b" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.5" xref="S3.SS1.SSS3.p2.4.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1c" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.6" xref="S3.SS1.SSS3.p2.4.m2.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1d" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.7" xref="S3.SS1.SSS3.p2.4.m2.1.1.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1e" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.8" xref="S3.SS1.SSS3.p2.4.m2.1.1.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.4.m2.1.1.1f" xref="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.4.m2.1.1.9" xref="S3.SS1.SSS3.p2.4.m2.1.1.9.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.4.m2.1b"><apply id="S3.SS1.SSS3.p2.4.m2.1.1.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1"><times id="S3.SS1.SSS3.p2.4.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.1"></times><ci id="S3.SS1.SSS3.p2.4.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.2">𝑐</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.3">𝑎</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.4.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.4">𝑡</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.5.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.5">𝑒</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.6.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.6">𝑔</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.7.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.7">𝑜</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.8.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.8">𝑟</ci><ci id="S3.SS1.SSS3.p2.4.m2.1.1.9.cmml" xref="S3.SS1.SSS3.p2.4.m2.1.1.9">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.4.m2.1c">category</annotation></semantics></math> function and then pass through a <math id="S3.SS1.SSS3.p2.5.m3.1" class="ltx_Math" alttext="Unique" display="inline"><semantics id="S3.SS1.SSS3.p2.5.m3.1a"><mrow id="S3.SS1.SSS3.p2.5.m3.1.1" xref="S3.SS1.SSS3.p2.5.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p2.5.m3.1.1.2" xref="S3.SS1.SSS3.p2.5.m3.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.5.m3.1.1.1" xref="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.5.m3.1.1.3" xref="S3.SS1.SSS3.p2.5.m3.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.5.m3.1.1.1a" xref="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.5.m3.1.1.4" xref="S3.SS1.SSS3.p2.5.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.5.m3.1.1.1b" xref="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.5.m3.1.1.5" xref="S3.SS1.SSS3.p2.5.m3.1.1.5.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.5.m3.1.1.1c" xref="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.5.m3.1.1.6" xref="S3.SS1.SSS3.p2.5.m3.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.5.m3.1.1.1d" xref="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.5.m3.1.1.7" xref="S3.SS1.SSS3.p2.5.m3.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.5.m3.1b"><apply id="S3.SS1.SSS3.p2.5.m3.1.1.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1"><times id="S3.SS1.SSS3.p2.5.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.1"></times><ci id="S3.SS1.SSS3.p2.5.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.2">𝑈</ci><ci id="S3.SS1.SSS3.p2.5.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.3">𝑛</ci><ci id="S3.SS1.SSS3.p2.5.m3.1.1.4.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.4">𝑖</ci><ci id="S3.SS1.SSS3.p2.5.m3.1.1.5.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.5">𝑞</ci><ci id="S3.SS1.SSS3.p2.5.m3.1.1.6.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.6">𝑢</ci><ci id="S3.SS1.SSS3.p2.5.m3.1.1.7.cmml" xref="S3.SS1.SSS3.p2.5.m3.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.5.m3.1c">Unique</annotation></semantics></math> function to select one target <math id="S3.SS1.SSS3.p2.6.m4.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.SS1.SSS3.p2.6.m4.1a"><mrow id="S3.SS1.SSS3.p2.6.m4.1.1" xref="S3.SS1.SSS3.p2.6.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p2.6.m4.1.1.2" xref="S3.SS1.SSS3.p2.6.m4.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.6.m4.1.1.1" xref="S3.SS1.SSS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.6.m4.1.1.3" xref="S3.SS1.SSS3.p2.6.m4.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.6.m4.1.1.1a" xref="S3.SS1.SSS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.6.m4.1.1.4" xref="S3.SS1.SSS3.p2.6.m4.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.6.m4.1.1.1b" xref="S3.SS1.SSS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.6.m4.1.1.5" xref="S3.SS1.SSS3.p2.6.m4.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.6.m4.1.1.1c" xref="S3.SS1.SSS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.6.m4.1.1.6" xref="S3.SS1.SSS3.p2.6.m4.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.6.m4.1b"><apply id="S3.SS1.SSS3.p2.6.m4.1.1.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1"><times id="S3.SS1.SSS3.p2.6.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.1"></times><ci id="S3.SS1.SSS3.p2.6.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.2">𝑡</ci><ci id="S3.SS1.SSS3.p2.6.m4.1.1.3.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.3">𝑎</ci><ci id="S3.SS1.SSS3.p2.6.m4.1.1.4.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.4">𝑏</ci><ci id="S3.SS1.SSS3.p2.6.m4.1.1.5.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.5">𝑙</ci><ci id="S3.SS1.SSS3.p2.6.m4.1.1.6.cmml" xref="S3.SS1.SSS3.p2.6.m4.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.6.m4.1c">table</annotation></semantics></math> or <math id="S3.SS1.SSS3.p2.7.m5.1" class="ltx_Math" alttext="figure" display="inline"><semantics id="S3.SS1.SSS3.p2.7.m5.1a"><mrow id="S3.SS1.SSS3.p2.7.m5.1.1" xref="S3.SS1.SSS3.p2.7.m5.1.1.cmml"><mi id="S3.SS1.SSS3.p2.7.m5.1.1.2" xref="S3.SS1.SSS3.p2.7.m5.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.7.m5.1.1.1" xref="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.7.m5.1.1.3" xref="S3.SS1.SSS3.p2.7.m5.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.7.m5.1.1.1a" xref="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.7.m5.1.1.4" xref="S3.SS1.SSS3.p2.7.m5.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.7.m5.1.1.1b" xref="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.7.m5.1.1.5" xref="S3.SS1.SSS3.p2.7.m5.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.7.m5.1.1.1c" xref="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.7.m5.1.1.6" xref="S3.SS1.SSS3.p2.7.m5.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.7.m5.1.1.1d" xref="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.7.m5.1.1.7" xref="S3.SS1.SSS3.p2.7.m5.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.7.m5.1b"><apply id="S3.SS1.SSS3.p2.7.m5.1.1.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1"><times id="S3.SS1.SSS3.p2.7.m5.1.1.1.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.1"></times><ci id="S3.SS1.SSS3.p2.7.m5.1.1.2.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.2">𝑓</ci><ci id="S3.SS1.SSS3.p2.7.m5.1.1.3.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.3">𝑖</ci><ci id="S3.SS1.SSS3.p2.7.m5.1.1.4.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.4">𝑔</ci><ci id="S3.SS1.SSS3.p2.7.m5.1.1.5.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.5">𝑢</ci><ci id="S3.SS1.SSS3.p2.7.m5.1.1.6.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.6">𝑟</ci><ci id="S3.SS1.SSS3.p2.7.m5.1.1.7.cmml" xref="S3.SS1.SSS3.p2.7.m5.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.7.m5.1c">figure</annotation></semantics></math> segment randomly. Then, the <span id="S3.SS1.SSS3.p2.8.3" class="ltx_text ltx_font_italic">Caption position</span> function can extract the relative location between target <math id="S3.SS1.SSS3.p2.8.m6.1" class="ltx_Math" alttext="table" display="inline"><semantics id="S3.SS1.SSS3.p2.8.m6.1a"><mrow id="S3.SS1.SSS3.p2.8.m6.1.1" xref="S3.SS1.SSS3.p2.8.m6.1.1.cmml"><mi id="S3.SS1.SSS3.p2.8.m6.1.1.2" xref="S3.SS1.SSS3.p2.8.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.8.m6.1.1.1" xref="S3.SS1.SSS3.p2.8.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.8.m6.1.1.3" xref="S3.SS1.SSS3.p2.8.m6.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.8.m6.1.1.1a" xref="S3.SS1.SSS3.p2.8.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.8.m6.1.1.4" xref="S3.SS1.SSS3.p2.8.m6.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.8.m6.1.1.1b" xref="S3.SS1.SSS3.p2.8.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.8.m6.1.1.5" xref="S3.SS1.SSS3.p2.8.m6.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p2.8.m6.1.1.1c" xref="S3.SS1.SSS3.p2.8.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p2.8.m6.1.1.6" xref="S3.SS1.SSS3.p2.8.m6.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.8.m6.1b"><apply id="S3.SS1.SSS3.p2.8.m6.1.1.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1"><times id="S3.SS1.SSS3.p2.8.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.1"></times><ci id="S3.SS1.SSS3.p2.8.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.2">𝑡</ci><ci id="S3.SS1.SSS3.p2.8.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.3">𝑎</ci><ci id="S3.SS1.SSS3.p2.8.m6.1.1.4.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.4">𝑏</ci><ci id="S3.SS1.SSS3.p2.8.m6.1.1.5.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.5">𝑙</ci><ci id="S3.SS1.SSS3.p2.8.m6.1.1.6.cmml" xref="S3.SS1.SSS3.p2.8.m6.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.8.m6.1c">table</annotation></semantics></math> and its caption.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2205.13724/assets/generation_step.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="593" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Position related, counting based and existence based question generation pre-defined function sequences</span></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">Existing Dataset size (question-answer pairs)</span></figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.4.1.1" class="ltx_tr">
<th id="S3.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Dataset</th>
<th id="S3.T2.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Training</th>
<th id="S3.T2.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Testing</th>
<th id="S3.T2.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.4.2.1" class="ltx_tr">
<th id="S3.T2.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PubVQA</th>
<td id="S3.T2.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">1260</td>
<td id="S3.T2.4.2.1.3" class="ltx_td ltx_align_left ltx_border_t">360</td>
<td id="S3.T2.4.2.1.4" class="ltx_td ltx_align_left ltx_border_t">180</td>
</tr>
<tr id="S3.T2.4.3.2" class="ltx_tr">
<th id="S3.T2.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FUNSD-QA</th>
<td id="S3.T2.4.3.2.2" class="ltx_td ltx_align_left ltx_border_bb">1000</td>
<td id="S3.T2.4.3.2.3" class="ltx_td ltx_align_left ltx_border_bb">300</td>
<td id="S3.T2.4.3.2.4" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Existing Dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The V-Doc have two following default datasets, which are publicly available and widely used datasets in the Document VQA domain.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">FUNSD-QA:</span>
It is a form understanding dataset selected from RVL-CDIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> dataset. It is created for scanned document-related tasks, including text detection, OCR, layout analysis. It contains 199 PDF images and provides the category of layout component, bounding box coordinates, text content and links between entities. Based on the pre-defined four category types (’Question’, ’Answer’, ’Header’, ’Other’) and the links between ’Question’ and ’Answer’, we can easily generate a span classification based extractive QA dataset named FUNSD-QA.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">PubVQA:</span>
We introduce/create a new abstractive Document-VQA dataset, named PubVQA. The dataset split information is list in Table <a href="#S3.T2" title="Table 2 ‣ 3.1.3 Question generation ‣ 3.1 PubVQA Dataset Generation ‣ 3 Dataset Storage ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Some classical abstractive QA models are used on this dataset to generate trained models uploaded to our Model Storage module.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Model Storage</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The V-Doc provides a Model Storage Module to manage existed or user-uploaded trained models. It provides the environment to support those models built on commonly used machine learning frameworks. Similar to Dataset Storage module, the existing trained model in current Model Storage module also can be classified into extractive and abstractive types. Many pre-trained language models can be fine-tuned on spanning based question answering tasks to extract answer from the input texts by predicting the start and end index such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, LayoutLMv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Those models are trained on extractive datasets provided by the Dataset Storage module which can be used as baselines and compared with novel proposed models. In addition, some widely used abstractive baselines are also provided in Model Storage module which can be directly used to generate the predicted answers and compared with user uploaded models.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Extractive QA Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The V-Doc has two extractive QA Models, included in the model storage.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">BERT</span>:
It is a widely used multi-layer bi-directional transformer-based language model. It can be easily applied to solve span classification based question answering models and trained on QA datasets, such as SQuAD (The Standford Question Answering Dataset). The linear layers are placed on the top of pre-trained BERT to predict the start and end logits for extracting the content from inputs. We fine-tune BERT-based QA models on the FUNSD-QA dataset to acquire a trained model uploaded to the Model Storage module.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LayoutLMv2</span>:
It is a multi-modal transformer-based architecture that not only is pre-trained by masked language modelling task but also some new text-image alignment and matching tasks are applied to learn the cross-modality interaction. It also provides a LayoutLMv2FeatureExtractor to extract the image features for pre-training and downstream tasks. Similar to BERT for extractive QA, a linear layer is placed on the top of hidden state outputs to predict the start and end index. A trained LayoutLMv2 model on the FUNSD dataset is provided in our Model Storage module too.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Abstractive QA Models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The V-Doc has two abstractive QA Models, included in the model storage.</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">LSTM+CNN</span>: It is a baseline model architecture adopted by MAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. It uses Long Short Term Memory (LSTM) network to encode textual information of question, and uses pre-trained convolution neural network to learn visual features of document image. Then, the concatenated question and visual embeddings are fed into a linear layer to get the predicted answer logits.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">MAC</span>: It is an end-to-end differentiable architecture which contains sequence of MAC cells to perform the multi-stage reasoning process. Each MAC recurrent cell contains three units including control unit, read unit and write unit to manage, extract and integrate question and image features separately. MAC network has achieved state-of-the-art performance on Clevr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, which is the reason why we choose this model as one our provided baseline model in Model Storage module.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Web Application</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Graphical User Interface</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The user interface of the V-Doc system is built as a single page Web Application by using Nodejs and React framework. All components and icons are imported from Semi design<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_ref_self">https://semi.design/en-US/start/introduction</span></span></span></span>. Mainly, this Web Application sends the PDF images and questions to the server with different prediction models and shows the result in the table below. The GUI of the V-Doc system mainly consists of four sections: model selection, image selection, input question and answer representation.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2205.13724/assets/interface_sample.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="591" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Interface of Web Application. The key subsections are highlighted including Model Selection, Question Input, Dataset Selection and Predicted Answer Representation</span></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Model selection: </span> It allows users to select trained models provided by Model Storage module.
<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_bold">Dataset selection: </span> It provides a image selector that normally contains nine PDF images, including six sample images from PubVQA dataset and three images from FUNSD-QA dataset. Users can select a sample image, and the system can directly apply the chosen model and file it into the input stage. Each sample indicates the template and a recommended input question. Users are allowed to use other images stored in Dataset Storage module in this Dataset selection section.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Question Input: </span>As shown in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.1 Graphical User Interface ‣ 5 Web Application ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our V-Doc system accepts both pre-defined templates and custom questions. When a pre-defined template is chosen, question will be composed by skeleton and parameters. Each model has several particular templates, as the suggested questions. After the question is decided, selected trained model, sample image and question will be sent into back-end to generate predicted answer.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Answer Representation: </span>When the result returned from back-end, the front-end will rendering the file name, question and predicted answer into result table. The results will be saved until user leaves the page.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2205.13724/assets/user_oriented_flow.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">User oriented workflow from model and dataset uploading, trained model and dataset selection, question inputting to get predicted answer.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Backend Dependencies</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.9" class="ltx_p">The dependencies of the V-Doc web application inherit from both provided MAC and LayoutLMv2. Both <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="torch1.8.0" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.1.m1.1.1.1a" xref="S5.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.1.m1.1.1.4" xref="S5.SS2.p1.1.m1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.1.m1.1.1.1b" xref="S5.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.1.m1.1.1.5" xref="S5.SS2.p1.1.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.1.m1.1.1.1c" xref="S5.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.1.m1.1.1.6" xref="S5.SS2.p1.1.m1.1.1.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.1.m1.1.1.1d" xref="S5.SS2.p1.1.m1.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.1.m1.1.1.7" xref="S5.SS2.p1.1.m1.1.1.7.cmml">1.8.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><times id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></times><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑡</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑜</ci><ci id="S5.SS2.p1.1.m1.1.1.4.cmml" xref="S5.SS2.p1.1.m1.1.1.4">𝑟</ci><ci id="S5.SS2.p1.1.m1.1.1.5.cmml" xref="S5.SS2.p1.1.m1.1.1.5">𝑐</ci><ci id="S5.SS2.p1.1.m1.1.1.6.cmml" xref="S5.SS2.p1.1.m1.1.1.6">ℎ</ci><cn type="float" id="S5.SS2.p1.1.m1.1.1.7.cmml" xref="S5.SS2.p1.1.m1.1.1.7">1.8.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">torch1.8.0</annotation></semantics></math> and <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="tensorflow1.15" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1a" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.4" xref="S5.SS2.p1.2.m2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1b" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.5" xref="S5.SS2.p1.2.m2.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1c" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.6" xref="S5.SS2.p1.2.m2.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1d" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.7" xref="S5.SS2.p1.2.m2.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1e" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.8" xref="S5.SS2.p1.2.m2.1.1.8.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1f" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.9" xref="S5.SS2.p1.2.m2.1.1.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1g" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.10" xref="S5.SS2.p1.2.m2.1.1.10.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1h" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.2.m2.1.1.11" xref="S5.SS2.p1.2.m2.1.1.11.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1i" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.2.m2.1.1.12" xref="S5.SS2.p1.2.m2.1.1.12.cmml">1.15</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><times id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></times><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝑡</ci><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">𝑒</ci><ci id="S5.SS2.p1.2.m2.1.1.4.cmml" xref="S5.SS2.p1.2.m2.1.1.4">𝑛</ci><ci id="S5.SS2.p1.2.m2.1.1.5.cmml" xref="S5.SS2.p1.2.m2.1.1.5">𝑠</ci><ci id="S5.SS2.p1.2.m2.1.1.6.cmml" xref="S5.SS2.p1.2.m2.1.1.6">𝑜</ci><ci id="S5.SS2.p1.2.m2.1.1.7.cmml" xref="S5.SS2.p1.2.m2.1.1.7">𝑟</ci><ci id="S5.SS2.p1.2.m2.1.1.8.cmml" xref="S5.SS2.p1.2.m2.1.1.8">𝑓</ci><ci id="S5.SS2.p1.2.m2.1.1.9.cmml" xref="S5.SS2.p1.2.m2.1.1.9">𝑙</ci><ci id="S5.SS2.p1.2.m2.1.1.10.cmml" xref="S5.SS2.p1.2.m2.1.1.10">𝑜</ci><ci id="S5.SS2.p1.2.m2.1.1.11.cmml" xref="S5.SS2.p1.2.m2.1.1.11">𝑤</ci><cn type="float" id="S5.SS2.p1.2.m2.1.1.12.cmml" xref="S5.SS2.p1.2.m2.1.1.12">1.15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">tensorflow1.15</annotation></semantics></math> are implemented to adapt various models. The packages <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="imageio2.9.0" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1a" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.4" xref="S5.SS2.p1.3.m3.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1b" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.5" xref="S5.SS2.p1.3.m3.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1c" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.6" xref="S5.SS2.p1.3.m3.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1d" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.7" xref="S5.SS2.p1.3.m3.1.1.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1e" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.3.m3.1.1.8" xref="S5.SS2.p1.3.m3.1.1.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.3.m3.1.1.1f" xref="S5.SS2.p1.3.m3.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.3.m3.1.1.9" xref="S5.SS2.p1.3.m3.1.1.9.cmml">2.9.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><times id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></times><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">𝑖</ci><ci id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">𝑚</ci><ci id="S5.SS2.p1.3.m3.1.1.4.cmml" xref="S5.SS2.p1.3.m3.1.1.4">𝑎</ci><ci id="S5.SS2.p1.3.m3.1.1.5.cmml" xref="S5.SS2.p1.3.m3.1.1.5">𝑔</ci><ci id="S5.SS2.p1.3.m3.1.1.6.cmml" xref="S5.SS2.p1.3.m3.1.1.6">𝑒</ci><ci id="S5.SS2.p1.3.m3.1.1.7.cmml" xref="S5.SS2.p1.3.m3.1.1.7">𝑖</ci><ci id="S5.SS2.p1.3.m3.1.1.8.cmml" xref="S5.SS2.p1.3.m3.1.1.8">𝑜</ci><cn type="float" id="S5.SS2.p1.3.m3.1.1.9.cmml" xref="S5.SS2.p1.3.m3.1.1.9">2.9.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">imageio2.9.0</annotation></semantics></math> and <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="PIL8.4.0" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.4.m4.1.1.1a" xref="S5.SS2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.4.m4.1.1.4" xref="S5.SS2.p1.4.m4.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.4.m4.1.1.1b" xref="S5.SS2.p1.4.m4.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.4.m4.1.1.5" xref="S5.SS2.p1.4.m4.1.1.5.cmml">8.4.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><times id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></times><ci id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">𝑃</ci><ci id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">𝐼</ci><ci id="S5.SS2.p1.4.m4.1.1.4.cmml" xref="S5.SS2.p1.4.m4.1.1.4">𝐿</ci><cn type="float" id="S5.SS2.p1.4.m4.1.1.5.cmml" xref="S5.SS2.p1.4.m4.1.1.5">8.4.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">PIL8.4.0</annotation></semantics></math> are called to process the uploaded image, as a binary data. Then, <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="dataset1.12.1" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mrow id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1a" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.4" xref="S5.SS2.p1.5.m5.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1b" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.5" xref="S5.SS2.p1.5.m5.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1c" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.6" xref="S5.SS2.p1.5.m5.1.1.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1d" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.7" xref="S5.SS2.p1.5.m5.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1e" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.5.m5.1.1.8" xref="S5.SS2.p1.5.m5.1.1.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.5.m5.1.1.1f" xref="S5.SS2.p1.5.m5.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.5.m5.1.1.9" xref="S5.SS2.p1.5.m5.1.1.9.cmml">1.12.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><times id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1.1"></times><ci id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">𝑑</ci><ci id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3">𝑎</ci><ci id="S5.SS2.p1.5.m5.1.1.4.cmml" xref="S5.SS2.p1.5.m5.1.1.4">𝑡</ci><ci id="S5.SS2.p1.5.m5.1.1.5.cmml" xref="S5.SS2.p1.5.m5.1.1.5">𝑎</ci><ci id="S5.SS2.p1.5.m5.1.1.6.cmml" xref="S5.SS2.p1.5.m5.1.1.6">𝑠</ci><ci id="S5.SS2.p1.5.m5.1.1.7.cmml" xref="S5.SS2.p1.5.m5.1.1.7">𝑒</ci><ci id="S5.SS2.p1.5.m5.1.1.8.cmml" xref="S5.SS2.p1.5.m5.1.1.8">𝑡</ci><cn type="float" id="S5.SS2.p1.5.m5.1.1.9.cmml" xref="S5.SS2.p1.5.m5.1.1.9">1.12.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">dataset1.12.1</annotation></semantics></math> and <math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="pandas1.3.4" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><mrow id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml"><mi id="S5.SS2.p1.6.m6.1.1.2" xref="S5.SS2.p1.6.m6.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.6.m6.1.1.3" xref="S5.SS2.p1.6.m6.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1a" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.6.m6.1.1.4" xref="S5.SS2.p1.6.m6.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1b" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.6.m6.1.1.5" xref="S5.SS2.p1.6.m6.1.1.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1c" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.6.m6.1.1.6" xref="S5.SS2.p1.6.m6.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1d" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.6.m6.1.1.7" xref="S5.SS2.p1.6.m6.1.1.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.6.m6.1.1.1e" xref="S5.SS2.p1.6.m6.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.6.m6.1.1.8" xref="S5.SS2.p1.6.m6.1.1.8.cmml">1.3.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><apply id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1"><times id="S5.SS2.p1.6.m6.1.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1.1"></times><ci id="S5.SS2.p1.6.m6.1.1.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2">𝑝</ci><ci id="S5.SS2.p1.6.m6.1.1.3.cmml" xref="S5.SS2.p1.6.m6.1.1.3">𝑎</ci><ci id="S5.SS2.p1.6.m6.1.1.4.cmml" xref="S5.SS2.p1.6.m6.1.1.4">𝑛</ci><ci id="S5.SS2.p1.6.m6.1.1.5.cmml" xref="S5.SS2.p1.6.m6.1.1.5">𝑑</ci><ci id="S5.SS2.p1.6.m6.1.1.6.cmml" xref="S5.SS2.p1.6.m6.1.1.6">𝑎</ci><ci id="S5.SS2.p1.6.m6.1.1.7.cmml" xref="S5.SS2.p1.6.m6.1.1.7">𝑠</ci><cn type="float" id="S5.SS2.p1.6.m6.1.1.8.cmml" xref="S5.SS2.p1.6.m6.1.1.8">1.3.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">pandas1.3.4</annotation></semantics></math> are used to collect and process both image data and uploaded question, preparing for the model pre-processing step. We applied <math id="S5.SS2.p1.7.m7.1" class="ltx_Math" alttext="transformer4.12.3" display="inline"><semantics id="S5.SS2.p1.7.m7.1a"><mrow id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml"><mi id="S5.SS2.p1.7.m7.1.1.2" xref="S5.SS2.p1.7.m7.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.3" xref="S5.SS2.p1.7.m7.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1a" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.4" xref="S5.SS2.p1.7.m7.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1b" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.5" xref="S5.SS2.p1.7.m7.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1c" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.6" xref="S5.SS2.p1.7.m7.1.1.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1d" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.7" xref="S5.SS2.p1.7.m7.1.1.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1e" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.8" xref="S5.SS2.p1.7.m7.1.1.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1f" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.9" xref="S5.SS2.p1.7.m7.1.1.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1g" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.10" xref="S5.SS2.p1.7.m7.1.1.10.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1h" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.11" xref="S5.SS2.p1.7.m7.1.1.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1i" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.7.m7.1.1.12" xref="S5.SS2.p1.7.m7.1.1.12.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.7.m7.1.1.1j" xref="S5.SS2.p1.7.m7.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.7.m7.1.1.13" xref="S5.SS2.p1.7.m7.1.1.13.cmml">4.12.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><apply id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1"><times id="S5.SS2.p1.7.m7.1.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1.1"></times><ci id="S5.SS2.p1.7.m7.1.1.2.cmml" xref="S5.SS2.p1.7.m7.1.1.2">𝑡</ci><ci id="S5.SS2.p1.7.m7.1.1.3.cmml" xref="S5.SS2.p1.7.m7.1.1.3">𝑟</ci><ci id="S5.SS2.p1.7.m7.1.1.4.cmml" xref="S5.SS2.p1.7.m7.1.1.4">𝑎</ci><ci id="S5.SS2.p1.7.m7.1.1.5.cmml" xref="S5.SS2.p1.7.m7.1.1.5">𝑛</ci><ci id="S5.SS2.p1.7.m7.1.1.6.cmml" xref="S5.SS2.p1.7.m7.1.1.6">𝑠</ci><ci id="S5.SS2.p1.7.m7.1.1.7.cmml" xref="S5.SS2.p1.7.m7.1.1.7">𝑓</ci><ci id="S5.SS2.p1.7.m7.1.1.8.cmml" xref="S5.SS2.p1.7.m7.1.1.8">𝑜</ci><ci id="S5.SS2.p1.7.m7.1.1.9.cmml" xref="S5.SS2.p1.7.m7.1.1.9">𝑟</ci><ci id="S5.SS2.p1.7.m7.1.1.10.cmml" xref="S5.SS2.p1.7.m7.1.1.10">𝑚</ci><ci id="S5.SS2.p1.7.m7.1.1.11.cmml" xref="S5.SS2.p1.7.m7.1.1.11">𝑒</ci><ci id="S5.SS2.p1.7.m7.1.1.12.cmml" xref="S5.SS2.p1.7.m7.1.1.12">𝑟</ci><cn type="float" id="S5.SS2.p1.7.m7.1.1.13.cmml" xref="S5.SS2.p1.7.m7.1.1.13">4.12.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">transformer4.12.3</annotation></semantics></math> build-in methods to pre-process and predict answer by trained models. Finally, <math id="S5.SS2.p1.8.m8.1" class="ltx_Math" alttext="flask2.0.2" display="inline"><semantics id="S5.SS2.p1.8.m8.1a"><mrow id="S5.SS2.p1.8.m8.1.1" xref="S5.SS2.p1.8.m8.1.1.cmml"><mi id="S5.SS2.p1.8.m8.1.1.2" xref="S5.SS2.p1.8.m8.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.8.m8.1.1.1" xref="S5.SS2.p1.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.8.m8.1.1.3" xref="S5.SS2.p1.8.m8.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.8.m8.1.1.1a" xref="S5.SS2.p1.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.8.m8.1.1.4" xref="S5.SS2.p1.8.m8.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.8.m8.1.1.1b" xref="S5.SS2.p1.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.8.m8.1.1.5" xref="S5.SS2.p1.8.m8.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.8.m8.1.1.1c" xref="S5.SS2.p1.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS2.p1.8.m8.1.1.6" xref="S5.SS2.p1.8.m8.1.1.6.cmml">k</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.8.m8.1.1.1d" xref="S5.SS2.p1.8.m8.1.1.1.cmml">​</mo><mn id="S5.SS2.p1.8.m8.1.1.7" xref="S5.SS2.p1.8.m8.1.1.7.cmml">2.0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.8.m8.1b"><apply id="S5.SS2.p1.8.m8.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1"><times id="S5.SS2.p1.8.m8.1.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1.1"></times><ci id="S5.SS2.p1.8.m8.1.1.2.cmml" xref="S5.SS2.p1.8.m8.1.1.2">𝑓</ci><ci id="S5.SS2.p1.8.m8.1.1.3.cmml" xref="S5.SS2.p1.8.m8.1.1.3">𝑙</ci><ci id="S5.SS2.p1.8.m8.1.1.4.cmml" xref="S5.SS2.p1.8.m8.1.1.4">𝑎</ci><ci id="S5.SS2.p1.8.m8.1.1.5.cmml" xref="S5.SS2.p1.8.m8.1.1.5">𝑠</ci><ci id="S5.SS2.p1.8.m8.1.1.6.cmml" xref="S5.SS2.p1.8.m8.1.1.6">𝑘</ci><cn type="float" id="S5.SS2.p1.8.m8.1.1.7.cmml" xref="S5.SS2.p1.8.m8.1.1.7">2.0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.8.m8.1c">flask2.0.2</annotation></semantics></math>, <math id="S5.SS2.p1.9.m9.1" class="ltx_Math" alttext="flask-cors3.0.10" display="inline"><semantics id="S5.SS2.p1.9.m9.1a"><mrow id="S5.SS2.p1.9.m9.1.1" xref="S5.SS2.p1.9.m9.1.1.cmml"><mrow id="S5.SS2.p1.9.m9.1.1.2" xref="S5.SS2.p1.9.m9.1.1.2.cmml"><mi id="S5.SS2.p1.9.m9.1.1.2.2" xref="S5.SS2.p1.9.m9.1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.2.1" xref="S5.SS2.p1.9.m9.1.1.2.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.2.3" xref="S5.SS2.p1.9.m9.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.2.1a" xref="S5.SS2.p1.9.m9.1.1.2.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.2.4" xref="S5.SS2.p1.9.m9.1.1.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.2.1b" xref="S5.SS2.p1.9.m9.1.1.2.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.2.5" xref="S5.SS2.p1.9.m9.1.1.2.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.2.1c" xref="S5.SS2.p1.9.m9.1.1.2.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.2.6" xref="S5.SS2.p1.9.m9.1.1.2.6.cmml">k</mi></mrow><mo id="S5.SS2.p1.9.m9.1.1.1" xref="S5.SS2.p1.9.m9.1.1.1.cmml">−</mo><mrow id="S5.SS2.p1.9.m9.1.1.3" xref="S5.SS2.p1.9.m9.1.1.3.cmml"><mi id="S5.SS2.p1.9.m9.1.1.3.2" xref="S5.SS2.p1.9.m9.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.3.1" xref="S5.SS2.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.3.3" xref="S5.SS2.p1.9.m9.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.3.1a" xref="S5.SS2.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.3.4" xref="S5.SS2.p1.9.m9.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.3.1b" xref="S5.SS2.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p1.9.m9.1.1.3.5" xref="S5.SS2.p1.9.m9.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.9.m9.1.1.3.1c" xref="S5.SS2.p1.9.m9.1.1.3.1.cmml">​</mo><mn id="S5.SS2.p1.9.m9.1.1.3.6" xref="S5.SS2.p1.9.m9.1.1.3.6.cmml">3.0.10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.9.m9.1b"><apply id="S5.SS2.p1.9.m9.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1"><minus id="S5.SS2.p1.9.m9.1.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1.1"></minus><apply id="S5.SS2.p1.9.m9.1.1.2.cmml" xref="S5.SS2.p1.9.m9.1.1.2"><times id="S5.SS2.p1.9.m9.1.1.2.1.cmml" xref="S5.SS2.p1.9.m9.1.1.2.1"></times><ci id="S5.SS2.p1.9.m9.1.1.2.2.cmml" xref="S5.SS2.p1.9.m9.1.1.2.2">𝑓</ci><ci id="S5.SS2.p1.9.m9.1.1.2.3.cmml" xref="S5.SS2.p1.9.m9.1.1.2.3">𝑙</ci><ci id="S5.SS2.p1.9.m9.1.1.2.4.cmml" xref="S5.SS2.p1.9.m9.1.1.2.4">𝑎</ci><ci id="S5.SS2.p1.9.m9.1.1.2.5.cmml" xref="S5.SS2.p1.9.m9.1.1.2.5">𝑠</ci><ci id="S5.SS2.p1.9.m9.1.1.2.6.cmml" xref="S5.SS2.p1.9.m9.1.1.2.6">𝑘</ci></apply><apply id="S5.SS2.p1.9.m9.1.1.3.cmml" xref="S5.SS2.p1.9.m9.1.1.3"><times id="S5.SS2.p1.9.m9.1.1.3.1.cmml" xref="S5.SS2.p1.9.m9.1.1.3.1"></times><ci id="S5.SS2.p1.9.m9.1.1.3.2.cmml" xref="S5.SS2.p1.9.m9.1.1.3.2">𝑐</ci><ci id="S5.SS2.p1.9.m9.1.1.3.3.cmml" xref="S5.SS2.p1.9.m9.1.1.3.3">𝑜</ci><ci id="S5.SS2.p1.9.m9.1.1.3.4.cmml" xref="S5.SS2.p1.9.m9.1.1.3.4">𝑟</ci><ci id="S5.SS2.p1.9.m9.1.1.3.5.cmml" xref="S5.SS2.p1.9.m9.1.1.3.5">𝑠</ci><cn type="float" id="S5.SS2.p1.9.m9.1.1.3.6.cmml" xref="S5.SS2.p1.9.m9.1.1.3.6">3.0.10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.9.m9.1c">flask-cors3.0.10</annotation></semantics></math> are used for building the backend to receive and response the request from front-end.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>User Instruction</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The interaction between users and the V-Doc GUI mainly contains three key steps. The first step is an optional one is to upload user trained model and dataset to Dataset Storage module or Model Storage module base on user demands. For uploading new dataset to Dataset Storage module, user should upload document images and other required embedding sequentially. Regarding to upload trained model to Model Storage module, user need to check whether the dataset existed in Dataset Storage before uploading their trained model. V-Doc GUI also allow users to select the existing trained models on provided datasets in Model Storage and Dataset Storage modules and send them to back-end of Web application. Then, users can input question and get the predicted answers based on selected dataset and model.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Extractive QA performance</h3>

<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S6.T3.3.2" class="ltx_text" style="font-size:90%;">Extractive QA model performance on FUNSD-QA</span></figcaption>
<table id="S6.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.4.1.1" class="ltx_tr">
<th id="S6.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S6.T3.4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T3.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S6.T3.4.1.1.2.1" class="ltx_text ltx_font_bold">BLUE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.4.2.1" class="ltx_tr">
<th id="S6.T3.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S6.T3.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">9.37</td>
</tr>
<tr id="S6.T3.4.3.2" class="ltx_tr">
<th id="S6.T3.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LayoutLMv2</th>
<td id="S6.T3.4.3.2.2" class="ltx_td ltx_align_left ltx_border_bb">11</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For solving extractive problems, we provide two trained model on FUNSD-QA dataset to predict the start index and end index of input documents. After getting the start and end index, we extract the actual tokens and calculate the average sentence-level BLUE score between ground-truth and predicted answers. The first provided Model is fine-tuned on pre-trained ”bert-base-uncased” ignoring the visual aspect features of document images to provide a text-feature only baseline for comparing with other models. In addition, we also provided another multi-modal baseline which is fine-tuned on ”microsoft/layoutlmv2-base-uncased”. We use LayoutLMv2 provided Processor to encode multi-aspect features including text, bounding box, visual features. The BLUE score of two trained models are 9.37 and 11, respectively, which represents multi-aspect features can effectively improve the extractive QA model performance.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.3.2" class="ltx_text" style="font-size:90%;">Abstractive QA model performance on PubVQA dataset</span></figcaption>
<table id="S6.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.4.1.1" class="ltx_tr">
<th id="S6.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T4.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.1.2.1" class="ltx_text ltx_font_bold">Val-Acc</span></th>
<th id="S6.T4.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.1.3.1" class="ltx_text ltx_font_bold">Test-Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.4.2.1" class="ltx_tr">
<td id="S6.T4.4.2.1.1" class="ltx_td ltx_align_left ltx_border_t">LSTM+CNN</td>
<td id="S6.T4.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">45.69</td>
<td id="S6.T4.4.2.1.3" class="ltx_td ltx_align_left ltx_border_t">48.78</td>
</tr>
<tr id="S6.T4.4.3.2" class="ltx_tr">
<td id="S6.T4.4.3.2.1" class="ltx_td ltx_align_left ltx_border_bb">MAC</td>
<td id="S6.T4.4.3.2.2" class="ltx_td ltx_align_left ltx_border_bb">54.32</td>
<td id="S6.T4.4.3.2.3" class="ltx_td ltx_align_left ltx_border_bb">55.44</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Abstractive QA performance</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">For anstractive task, we selected two commonly used baselines and tested on our PubVQA dataset including LSTM+CNN, MAC. We use one bi-directional LSTM layer (dim = 128) and extract the last hidden state, which is concatenated with the image visual feature (dim=1024) extracted from ResNet101 (same as MAC model adopted). For the MAC network, the only change is the output layer dimension is change to 9 which is equal to the length of PubVQA answer set.From Table <a href="#S6.T4" title="Table 4 ‣ 6.1 Extractive QA performance ‣ 6 Evaluation ‣ V-Doc : Visual questions answers with Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, MAC model can have better performance 55.44, than LSTM+CNN structure 48.78 on test dataset.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper introduces a new document visual question answering platform, V-Doc, that provides a web application to predict corresponding answers of input questions by using an uploaded trained model in the model storage. Currently, the extendable model storage contains both abstractive and extractive trained document related VQA baseline models on different datasets. The model storage can also allow a user to upload other trained models to demonstrate the performance in the web application. In addition, we also introduced a new PubVQA dataset generated by real-world medical journals by an extendable question generation engine. In summary, our system provides a platform for deep learning researcher to represent their VQA model results to fill the gaps in this area. In the future, we will update our model storage to provide more trained baseline models, and more question generation templates will be provided to improve the quality of our dataset.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 4171–4186, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, abs/1606.01847, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Peng Gao, Hongsheng Li, Haoxuan You, Zhengkai Jiang, Pan Lu, Steven C. H. Hoi,
and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Dynamic fusion with intra- and inter- modality attention flow for
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Evaluation of deep convolutional nets for document image
classification and retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 13th International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 991–995. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Compositional attention networks for machine reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 6700–6709, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Hazim Kemal Ekenel Jaume, Guillaume and Jean-Philippe Thiran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Funsd: A dataset for form understanding in noisy scanned documents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition Workshops (ICDARW)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, volume 2, pages 1–6. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C
Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 2901–2910, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos
Kádár, Adam Trischler, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Figureqa: An annotated figure dataset for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.07300</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi,
and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Are you smarter than a sixth grader? textbook question answering for
multimodal machine comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4999–5007, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Visualbert: A simple and performant baseline for vision and language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.03557</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Roberta: A robustly optimized bert pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.11692</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Docvqa: A dataset for vqa on document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 2200–2209, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Plotqa: Reasoning over scientific plots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 1527–1536, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Visualmrc: Machine reading comprehension on document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, volume 35, pages 13878–13888, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,
Dinei Florencio, Cha Zhang, Wanxiang Che, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Layoutlmv2: Multi-modal pre-training for visually-rich document
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 2579–2591, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Publaynet: largest dataset ever for document layout analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 1015–1022. IEEE, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.13723" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.13724" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.13724">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.13724" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.13725" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 05:50:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
