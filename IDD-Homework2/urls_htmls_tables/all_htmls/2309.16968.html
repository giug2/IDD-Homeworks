<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.16968] Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.</title><meta property="og:description" content="This research uses deep learning to estimate the topology of manifolds represented by sparse, unordered point cloud scenes in 3D.
A new labelled dataset was synthesised to train neural networks and evaluate their abiliâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.16968">

<!--Generated on Wed Feb 28 03:57:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
persistent homology,  deep learning,  3D pattern analysis,  topological data analysis,  semantic segmentation,  synthetic data generation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government.
Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dylan Peek
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">School of Information and Physical Sciences</span>
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_italic">The University of Newcastle
<br class="ltx_break"></span>Callaghan NSW 2308, Australia 
<br class="ltx_break">Dylan.Peek@uon.edu.au
<br class="ltx_break">OrcidID: 0000-0003-3568-7853
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthew P. Skerritt
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">Mathematical Sciences</span>
<br class="ltx_break"><span id="id5.2.id2" class="ltx_text ltx_font_italic">RMIT University
<br class="ltx_break"></span>Melbourne VIC 3000, Australia 
<br class="ltx_break">Matt.Skerritt@rmit.edu.au
<br class="ltx_break">OrcidID: 0000-0003-2211-7616
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stephan Chalup
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_font_italic">School of Information and Physical Sciences</span>
<br class="ltx_break"><span id="id7.2.id2" class="ltx_text ltx_font_italic">The University of Newcastle
<br class="ltx_break"></span>Callaghan NSW 2308, Australia 
<br class="ltx_break">Stephan.Chalup@newcastle.edu.au
<br class="ltx_break">OrcidID: 0000-0002-7886-3653
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">This research uses deep learning to estimate the topology of manifolds represented by sparse, unordered point cloud scenes in 3D.
A new labelled dataset was synthesised to train neural networks and evaluate their ability to estimate the genus of these manifolds. This data used random homeomorphic deformations to provoke the learning of visual topological features.
We demonstrate that deep learning models could extract these features and discuss some advantages over existing topological data analysis tools that are based on persistent homology. Semantic segmentation was used to provide additional geometric information in conjunction with topological labels. Common point cloud multi-layer perceptron and transformer networks were both used to compare the viability of these methods.
The experimental results of this pilot study support the hypothesis that, with the aid of sophisticated synthetic data generation, neural networks can perform segmentation-based topological data analysis. While our study focused on simulated data, the accuracy achieved suggests a potential for future applications using real data.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
persistent homology, deep learning, 3D pattern analysis, topological data analysis, semantic segmentation, synthetic data generation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Topology is the mathematical field that studies topological shape which is invariant to scale and homeomorphic deformationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. In its applied form Topological Data Analysis (TDA) can offer insight into topological features of data such as the amount of voids or n-dimensional holesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Recent theoretical and computational advancements have allowed a rapid uptake in real-world use. It has been successfully used in Chemical EngineeringÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, EEG ProcessingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Medical DiagnosisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, AviationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Nuclear PhysicsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Heart Rate Variability AnalysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Network AnalysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Persistent homology is a powerful tool for TDA that can be used to produce persistence intervals of topological features across different dimensional scalesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. These intervals can be expressed in forms such as persistence diagrams, persistence images and persistence barcodesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Analysis of these intervals can offer insight into the topological features of data, this data may be expressed in n-dimensional volumetric, mesh or point cloud structures.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose that neural networks can be used as an alternate strategy to some of the topological aspects of persistent homology where it may offer advantages that have yet to be leveraged in research spaces.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">One limitation when extracting topological signatures such as Betti numbers via interval analysis, as in persistent homology, is that the conversion of raw data into intervals may be a lossy processÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
An extraction of n-dimensional holes and voids from intervals can induce some ambiguity in the original configuration.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.F1.2.2" class="ltx_tr">
<th id="S1.F1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S1.F1.1.1.1.1" class="ltx_text" style="position:relative; bottom:-31.3pt;"><img src="/html/2309.16968/assets/x1.png" id="S1.F1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="133" height="100" alt="Refer to caption"></span></th>
<th id="S1.F1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.F1.2.2.2.1" class="ltx_text" style="position:relative; bottom:-31.3pt;"><img src="/html/2309.16968/assets/x2.png" id="S1.F1.2.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="133" height="100" alt="Refer to caption"></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.F1.4.4" class="ltx_tr">
<td id="S1.F1.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><img src="/html/2309.16968/assets/x3.png" id="S1.F1.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="133" height="100" alt="Refer to caption"></td>
<td id="S1.F1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><img src="/html/2309.16968/assets/x4.png" id="S1.F1.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="133" height="100" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Left and right are two scenes demonstrating potential ambiguities when assessing the global topology of a scene using persistent homology. Both scenes have the Betti numbers <math id="S1.F1.9.m1.1" class="ltx_Math" alttext="\beta_{0}=2" display="inline"><semantics id="S1.F1.9.m1.1b"><mrow id="S1.F1.9.m1.1.1" xref="S1.F1.9.m1.1.1.cmml"><msub id="S1.F1.9.m1.1.1.2" xref="S1.F1.9.m1.1.1.2.cmml"><mi id="S1.F1.9.m1.1.1.2.2" xref="S1.F1.9.m1.1.1.2.2.cmml">Î²</mi><mn id="S1.F1.9.m1.1.1.2.3" xref="S1.F1.9.m1.1.1.2.3.cmml">0</mn></msub><mo id="S1.F1.9.m1.1.1.1" xref="S1.F1.9.m1.1.1.1.cmml">=</mo><mn id="S1.F1.9.m1.1.1.3" xref="S1.F1.9.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.9.m1.1c"><apply id="S1.F1.9.m1.1.1.cmml" xref="S1.F1.9.m1.1.1"><eq id="S1.F1.9.m1.1.1.1.cmml" xref="S1.F1.9.m1.1.1.1"></eq><apply id="S1.F1.9.m1.1.1.2.cmml" xref="S1.F1.9.m1.1.1.2"><csymbol cd="ambiguous" id="S1.F1.9.m1.1.1.2.1.cmml" xref="S1.F1.9.m1.1.1.2">subscript</csymbol><ci id="S1.F1.9.m1.1.1.2.2.cmml" xref="S1.F1.9.m1.1.1.2.2">ğ›½</ci><cn type="integer" id="S1.F1.9.m1.1.1.2.3.cmml" xref="S1.F1.9.m1.1.1.2.3">0</cn></apply><cn type="integer" id="S1.F1.9.m1.1.1.3.cmml" xref="S1.F1.9.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.9.m1.1d">\beta_{0}=2</annotation></semantics></math>, <math id="S1.F1.10.m2.1" class="ltx_Math" alttext="\beta_{1}=6" display="inline"><semantics id="S1.F1.10.m2.1b"><mrow id="S1.F1.10.m2.1.1" xref="S1.F1.10.m2.1.1.cmml"><msub id="S1.F1.10.m2.1.1.2" xref="S1.F1.10.m2.1.1.2.cmml"><mi id="S1.F1.10.m2.1.1.2.2" xref="S1.F1.10.m2.1.1.2.2.cmml">Î²</mi><mn id="S1.F1.10.m2.1.1.2.3" xref="S1.F1.10.m2.1.1.2.3.cmml">1</mn></msub><mo id="S1.F1.10.m2.1.1.1" xref="S1.F1.10.m2.1.1.1.cmml">=</mo><mn id="S1.F1.10.m2.1.1.3" xref="S1.F1.10.m2.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.10.m2.1c"><apply id="S1.F1.10.m2.1.1.cmml" xref="S1.F1.10.m2.1.1"><eq id="S1.F1.10.m2.1.1.1.cmml" xref="S1.F1.10.m2.1.1.1"></eq><apply id="S1.F1.10.m2.1.1.2.cmml" xref="S1.F1.10.m2.1.1.2"><csymbol cd="ambiguous" id="S1.F1.10.m2.1.1.2.1.cmml" xref="S1.F1.10.m2.1.1.2">subscript</csymbol><ci id="S1.F1.10.m2.1.1.2.2.cmml" xref="S1.F1.10.m2.1.1.2.2">ğ›½</ci><cn type="integer" id="S1.F1.10.m2.1.1.2.3.cmml" xref="S1.F1.10.m2.1.1.2.3">1</cn></apply><cn type="integer" id="S1.F1.10.m2.1.1.3.cmml" xref="S1.F1.10.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.10.m2.1d">\beta_{1}=6</annotation></semantics></math>, and <math id="S1.F1.11.m3.1" class="ltx_Math" alttext="\beta_{2}=2" display="inline"><semantics id="S1.F1.11.m3.1b"><mrow id="S1.F1.11.m3.1.1" xref="S1.F1.11.m3.1.1.cmml"><msub id="S1.F1.11.m3.1.1.2" xref="S1.F1.11.m3.1.1.2.cmml"><mi id="S1.F1.11.m3.1.1.2.2" xref="S1.F1.11.m3.1.1.2.2.cmml">Î²</mi><mn id="S1.F1.11.m3.1.1.2.3" xref="S1.F1.11.m3.1.1.2.3.cmml">2</mn></msub><mo id="S1.F1.11.m3.1.1.1" xref="S1.F1.11.m3.1.1.1.cmml">=</mo><mn id="S1.F1.11.m3.1.1.3" xref="S1.F1.11.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.11.m3.1c"><apply id="S1.F1.11.m3.1.1.cmml" xref="S1.F1.11.m3.1.1"><eq id="S1.F1.11.m3.1.1.1.cmml" xref="S1.F1.11.m3.1.1.1"></eq><apply id="S1.F1.11.m3.1.1.2.cmml" xref="S1.F1.11.m3.1.1.2"><csymbol cd="ambiguous" id="S1.F1.11.m3.1.1.2.1.cmml" xref="S1.F1.11.m3.1.1.2">subscript</csymbol><ci id="S1.F1.11.m3.1.1.2.2.cmml" xref="S1.F1.11.m3.1.1.2.2">ğ›½</ci><cn type="integer" id="S1.F1.11.m3.1.1.2.3.cmml" xref="S1.F1.11.m3.1.1.2.3">2</cn></apply><cn type="integer" id="S1.F1.11.m3.1.1.3.cmml" xref="S1.F1.11.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.11.m3.1d">\beta_{2}=2</annotation></semantics></math>.
The genus and Betti numbers for these objects can be seen in Table <a href="#S2.T2" title="TABLE II â€£ II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. 
<br class="ltx_break">The meshes of the four displayed surfaces are examples of genus <math id="S1.F1.12.m4.4" class="ltx_Math" alttext="\left\{0,1,2,3\right\}" display="inline"><semantics id="S1.F1.12.m4.4b"><mrow id="S1.F1.12.m4.4.5.2" xref="S1.F1.12.m4.4.5.1.cmml"><mo id="S1.F1.12.m4.4.5.2.1" xref="S1.F1.12.m4.4.5.1.cmml">{</mo><mn id="S1.F1.12.m4.1.1" xref="S1.F1.12.m4.1.1.cmml">0</mn><mo id="S1.F1.12.m4.4.5.2.2" xref="S1.F1.12.m4.4.5.1.cmml">,</mo><mn id="S1.F1.12.m4.2.2" xref="S1.F1.12.m4.2.2.cmml">1</mn><mo id="S1.F1.12.m4.4.5.2.3" xref="S1.F1.12.m4.4.5.1.cmml">,</mo><mn id="S1.F1.12.m4.3.3" xref="S1.F1.12.m4.3.3.cmml">2</mn><mo id="S1.F1.12.m4.4.5.2.4" xref="S1.F1.12.m4.4.5.1.cmml">,</mo><mn id="S1.F1.12.m4.4.4" xref="S1.F1.12.m4.4.4.cmml">3</mn><mo id="S1.F1.12.m4.4.5.2.5" xref="S1.F1.12.m4.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.12.m4.4c"><set id="S1.F1.12.m4.4.5.1.cmml" xref="S1.F1.12.m4.4.5.2"><cn type="integer" id="S1.F1.12.m4.1.1.cmml" xref="S1.F1.12.m4.1.1">0</cn><cn type="integer" id="S1.F1.12.m4.2.2.cmml" xref="S1.F1.12.m4.2.2">1</cn><cn type="integer" id="S1.F1.12.m4.3.3.cmml" xref="S1.F1.12.m4.3.3">2</cn><cn type="integer" id="S1.F1.12.m4.4.4.cmml" xref="S1.F1.12.m4.4.4">3</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.12.m4.4d">\left\{0,1,2,3\right\}</annotation></semantics></math> manifolds that were used as seeds in the data generation process, see SectionÂ <a href="#S3.SS1" title="III-A Dataset â€£ III Method â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. The visualisations were performed in BlenderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For example, a scene may comprise 2 objects and 3 holes however it is ambiguous whether the scene contained a 0-hole object and a 3-hole object or a 1-hole object and a 2-hole object, see FigÂ <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our approach is based on per-point labelling segmentation which addresses such ambiguities by attempting to isolate these objects into classes.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">There are other potential advantages that neural networks could provide to researchers. Persistent homology can have large memory and CPU requirements and a time complexity that scales with point cloud sample countsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. These computational constraints may prevent utilization in certain applications where the resources or time are not available. Details regarding some key concepts and algorithms for the computation of persistent homology are surveyed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> which compares time, CPU, and memory benchmarks across these methods.
Neural networks may also offer an intelligent interpretation of noise, density, and connectivity.
Persistent homology can be robust to noise that has known characteristics however may incur problems when evaluating data with varied noise, relative object scale, and sample densityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Neural networks may use the local and global contexts of points and surfaces to intelligently extract topological features based on â€˜human-likeâ€™ intuition.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.4" class="ltx_p">In this paper we analyse closed compact orientable 2-dimensional manifolds to estimate the topological â€˜genusâ€™.
Each of these manifolds is homeomorphic to a sphere with a number of handles attached, where the genus <math id="S1.p7.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S1.p7.1.m1.1a"><mi id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">g</annotation></semantics></math> is the number of these handlesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The genus is related to the Euler characteristic <math id="S1.p7.2.m2.1" class="ltx_Math" alttext="\chi" display="inline"><semantics id="S1.p7.2.m2.1a"><mi id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">Ï‡</mi><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">ğœ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">\chi</annotation></semantics></math> and the first Betti number <math id="S1.p7.3.m3.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S1.p7.3.m3.1a"><msub id="S1.p7.3.m3.1.1" xref="S1.p7.3.m3.1.1.cmml"><mi id="S1.p7.3.m3.1.1.2" xref="S1.p7.3.m3.1.1.2.cmml">Î²</mi><mn id="S1.p7.3.m3.1.1.3" xref="S1.p7.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.p7.3.m3.1b"><apply id="S1.p7.3.m3.1.1.cmml" xref="S1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S1.p7.3.m3.1.1.1.cmml" xref="S1.p7.3.m3.1.1">subscript</csymbol><ci id="S1.p7.3.m3.1.1.2.cmml" xref="S1.p7.3.m3.1.1.2">ğ›½</ci><cn type="integer" id="S1.p7.3.m3.1.1.3.cmml" xref="S1.p7.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.3.m3.1c">\beta_{1}</annotation></semantics></math> by the formula: <math id="S1.p7.4.m4.1" class="ltx_Math" alttext="g=\frac{1}{2}\beta_{1}=\frac{3}{2}\chi" display="inline"><semantics id="S1.p7.4.m4.1a"><mrow id="S1.p7.4.m4.1.1" xref="S1.p7.4.m4.1.1.cmml"><mi id="S1.p7.4.m4.1.1.2" xref="S1.p7.4.m4.1.1.2.cmml">g</mi><mo id="S1.p7.4.m4.1.1.3" xref="S1.p7.4.m4.1.1.3.cmml">=</mo><mrow id="S1.p7.4.m4.1.1.4" xref="S1.p7.4.m4.1.1.4.cmml"><mfrac id="S1.p7.4.m4.1.1.4.2" xref="S1.p7.4.m4.1.1.4.2.cmml"><mn id="S1.p7.4.m4.1.1.4.2.2" xref="S1.p7.4.m4.1.1.4.2.2.cmml">1</mn><mn id="S1.p7.4.m4.1.1.4.2.3" xref="S1.p7.4.m4.1.1.4.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S1.p7.4.m4.1.1.4.1" xref="S1.p7.4.m4.1.1.4.1.cmml">â€‹</mo><msub id="S1.p7.4.m4.1.1.4.3" xref="S1.p7.4.m4.1.1.4.3.cmml"><mi id="S1.p7.4.m4.1.1.4.3.2" xref="S1.p7.4.m4.1.1.4.3.2.cmml">Î²</mi><mn id="S1.p7.4.m4.1.1.4.3.3" xref="S1.p7.4.m4.1.1.4.3.3.cmml">1</mn></msub></mrow><mo id="S1.p7.4.m4.1.1.5" xref="S1.p7.4.m4.1.1.5.cmml">=</mo><mrow id="S1.p7.4.m4.1.1.6" xref="S1.p7.4.m4.1.1.6.cmml"><mfrac id="S1.p7.4.m4.1.1.6.2" xref="S1.p7.4.m4.1.1.6.2.cmml"><mn id="S1.p7.4.m4.1.1.6.2.2" xref="S1.p7.4.m4.1.1.6.2.2.cmml">3</mn><mn id="S1.p7.4.m4.1.1.6.2.3" xref="S1.p7.4.m4.1.1.6.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S1.p7.4.m4.1.1.6.1" xref="S1.p7.4.m4.1.1.6.1.cmml">â€‹</mo><mi id="S1.p7.4.m4.1.1.6.3" xref="S1.p7.4.m4.1.1.6.3.cmml">Ï‡</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.4.m4.1b"><apply id="S1.p7.4.m4.1.1.cmml" xref="S1.p7.4.m4.1.1"><and id="S1.p7.4.m4.1.1a.cmml" xref="S1.p7.4.m4.1.1"></and><apply id="S1.p7.4.m4.1.1b.cmml" xref="S1.p7.4.m4.1.1"><eq id="S1.p7.4.m4.1.1.3.cmml" xref="S1.p7.4.m4.1.1.3"></eq><ci id="S1.p7.4.m4.1.1.2.cmml" xref="S1.p7.4.m4.1.1.2">ğ‘”</ci><apply id="S1.p7.4.m4.1.1.4.cmml" xref="S1.p7.4.m4.1.1.4"><times id="S1.p7.4.m4.1.1.4.1.cmml" xref="S1.p7.4.m4.1.1.4.1"></times><apply id="S1.p7.4.m4.1.1.4.2.cmml" xref="S1.p7.4.m4.1.1.4.2"><divide id="S1.p7.4.m4.1.1.4.2.1.cmml" xref="S1.p7.4.m4.1.1.4.2"></divide><cn type="integer" id="S1.p7.4.m4.1.1.4.2.2.cmml" xref="S1.p7.4.m4.1.1.4.2.2">1</cn><cn type="integer" id="S1.p7.4.m4.1.1.4.2.3.cmml" xref="S1.p7.4.m4.1.1.4.2.3">2</cn></apply><apply id="S1.p7.4.m4.1.1.4.3.cmml" xref="S1.p7.4.m4.1.1.4.3"><csymbol cd="ambiguous" id="S1.p7.4.m4.1.1.4.3.1.cmml" xref="S1.p7.4.m4.1.1.4.3">subscript</csymbol><ci id="S1.p7.4.m4.1.1.4.3.2.cmml" xref="S1.p7.4.m4.1.1.4.3.2">ğ›½</ci><cn type="integer" id="S1.p7.4.m4.1.1.4.3.3.cmml" xref="S1.p7.4.m4.1.1.4.3.3">1</cn></apply></apply></apply><apply id="S1.p7.4.m4.1.1c.cmml" xref="S1.p7.4.m4.1.1"><eq id="S1.p7.4.m4.1.1.5.cmml" xref="S1.p7.4.m4.1.1.5"></eq><share href="#S1.p7.4.m4.1.1.4.cmml" id="S1.p7.4.m4.1.1d.cmml" xref="S1.p7.4.m4.1.1"></share><apply id="S1.p7.4.m4.1.1.6.cmml" xref="S1.p7.4.m4.1.1.6"><times id="S1.p7.4.m4.1.1.6.1.cmml" xref="S1.p7.4.m4.1.1.6.1"></times><apply id="S1.p7.4.m4.1.1.6.2.cmml" xref="S1.p7.4.m4.1.1.6.2"><divide id="S1.p7.4.m4.1.1.6.2.1.cmml" xref="S1.p7.4.m4.1.1.6.2"></divide><cn type="integer" id="S1.p7.4.m4.1.1.6.2.2.cmml" xref="S1.p7.4.m4.1.1.6.2.2">3</cn><cn type="integer" id="S1.p7.4.m4.1.1.6.2.3.cmml" xref="S1.p7.4.m4.1.1.6.2.3">2</cn></apply><ci id="S1.p7.4.m4.1.1.6.3.cmml" xref="S1.p7.4.m4.1.1.6.3">ğœ’</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.4.m4.1c">g=\frac{1}{2}\beta_{1}=\frac{3}{2}\chi</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
TableÂ <a href="#S2.T2" title="TABLE II â€£ II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> lists some example surfaces including their genus, Betti numbers, and Euler characteristic.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We propose employing semantic segmentation neural networks to directly extract genus information from raw data, providing a novel tool for TDA. To our knowledge, no research exists to evaluate neural networksâ€™ ability to extract Betti numbers and/or genus from raw 3D point cloud data, nor to apply segmentation labelling for topological signatures. Networks aimed at topological data analysis with different inputs, outputs, and objectives are discussed in SectionÂ <a href="#S1.SS1" title="I-A Related Works â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">I-A</span></span></a>.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">An investigation into existing datasets and related literature found no suitable labeled sets for our application of topological machine learning, some of these datasets are discussed in SectionÂ <a href="#S1.SS1" title="I-A Related Works â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">I-A</span></span></a>.
Our dataset required Betti number or genus labels, scenes that included multiple objects and genus groups, and variability between these objects appearance. Real-world data is valuable for certain applications however we aimed to isolate the â€˜human-likeâ€™ element of visual topological analysis. For this, varied homeomorphic deformation for objects with the same topological signatures was deemed necessary to incentivise the learning of target features.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">To fulfill our criteria we created a new dataset using a combination of the Repulsive Surfaces algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and the Wave Function Collapse algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to produce 3D meshes of known properties. We refer to this dataset as the â€˜Repulseâ€™ dataset. An example of an element in the dataset is shown in FigÂ <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> which demonstrates how visually challenging it may be for humans to topologically analyze data that poses little correlation to our existing experiences.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<table id="S1.F2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F2.2.2" class="ltx_tr">
<td id="S1.F2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/images/hd_compare/bnw.png" id="S1.F2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="138" height="157" alt="Refer to caption"></td>
<td id="S1.F2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/images/hd_compare/coloured.png" id="S1.F2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="138" height="157" alt="Refer to caption"></td>
</tr>
<tr id="S1.F2.2.3.1" class="ltx_tr">
<td id="S1.F2.2.3.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S1.F2.2.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample comprising of 3 close proximity objects of known genus. Image (a) consists of a generated dataset mesh. Image (b) has been coloured strictly for demonstrative purposes; genus 0 (blue), genus 2 (green) and genus 3 (red). Visualisation of data was performed in Blender 3.0.1. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure">
<p id="S1.F3.4" class="ltx_p ltx_align_center"><span id="S1.F3.4.4" class="ltx_text">
<img src="/html/2309.16968/assets/images/hd_sequence/0.png" id="S1.F3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="157" height="197" alt="Refer to caption">
<img src="/html/2309.16968/assets/images/hd_sequence/1.png" id="S1.F3.2.2.g2" class="ltx_graphics ltx_img_portrait" width="157" height="197" alt="Refer to caption">
<img src="/html/2309.16968/assets/images/hd_sequence/2.png" id="S1.F3.3.3.g3" class="ltx_graphics ltx_img_portrait" width="157" height="197" alt="Refer to caption">
<img src="/html/2309.16968/assets/images/hd_sequence/3.png" id="S1.F3.4.4.g4" class="ltx_graphics ltx_img_portrait" width="157" height="197" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sample showing the growth of 2 interlinked objects of genus 1 (blue) and genus 3 (brown). Visualisation of data was performed in Blender 3.0.1.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></figcaption>
</figure>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">The key contributions of this research are:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Generation of a novel labeled dataset with scenes of varying object count and genus that could be used for topological machine learning training and evaluation. This dataset can be used in its original mesh form or sampled in point clouds of various point counts.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Training multiple neural network models to analyze 3D point cloud data and assign each point a genus corresponding to its estimated object grouping.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Discuss the viability of novel topological data semantic segmentation and compare network performance with respect to architectural design.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">The dataset generation process is described in Section <a href="#S3.SS1" title="III-A Dataset â€£ III Method â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> and a brief visual summary can be seen in Fig <a href="#S1.F3" title="Figure 3 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.4.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.5.2" class="ltx_text ltx_font_italic">Related Works</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Recent studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have shown success in Betti number estimation of 2D, 3D, and 4D volumetric data. They used synthetic data comprising a range of fundamental topological shapes without deformation in pixel, voxel, and toxel form and conducted TDA with basic convolutional neural networks.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">In contrast, the present project focuses on increasing the complexity of data in 3D significantly by generating sophisticated datasets with diverse organic structures.
By switching from a classification to a segmentation paradigm and using more advanced deep learning models we can demonstrate that more information can be drawn from the resulting network output.
The long-term vision of this project is to achieve better manageable computational performance and real-world applicability of this approach.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Two networks have emerged to produce estimated persistence images of various input data formats: Pi-Net, and TopologyNet. Persistence images are a representation of persistent diagrams (see SectionÂ <a href="#S2.SS2" title="II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>) in <math id="S1.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbb{R}^{n}" display="inline"><semantics id="S1.SS1.p3.1.m1.1a"><msup id="S1.SS1.p3.1.m1.1.1" xref="S1.SS1.p3.1.m1.1.1.cmml"><mi id="S1.SS1.p3.1.m1.1.1.2" xref="S1.SS1.p3.1.m1.1.1.2.cmml">â„</mi><mi id="S1.SS1.p3.1.m1.1.1.3" xref="S1.SS1.p3.1.m1.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S1.SS1.p3.1.m1.1b"><apply id="S1.SS1.p3.1.m1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS1.p3.1.m1.1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1">superscript</csymbol><ci id="S1.SS1.p3.1.m1.1.1.2.cmml" xref="S1.SS1.p3.1.m1.1.1.2">â„</ci><ci id="S1.SS1.p3.1.m1.1.1.3.cmml" xref="S1.SS1.p3.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p3.1.m1.1c">\mathbb{R}^{n}</annotation></semantics></math> space. Further mathematical and computational detail on the creation of persistence images can be seen inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Pi-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> introduced two convolutional neural networks to produce estimated persistence images of multivariate and multichannel input data. Signal PI-Net can process time series signals and Image PI-Net can process 2D image data.
An output format of persistence images still requires further subjective interpretation for the extraction of additional topological features. In this form, however, raw data information that may be useful in determining these features is lost. A one-hit network for outputting a variety of dimensional features could provide additional information. For these 2D image data experiments the datasets CIFAR10Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, CIFAR100Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and SVHNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> were used. These datasets use a combination of common objects and street-view house numbers. As these datasets were not exposed to homeomorphic deformation there may exist a correlation between the geometric shape classification and topological labels.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">A TopologyNet model was used to fit 3D point cloud data directly to persistence imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The training of this network used ModelNet40 datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> comprising 40 categories for classification such as a chair or table. These classes may have spurious correlations between object type and topological labels making them undesirable for evaluating topological models.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">RipsNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> model, like TopologyNet, also successfully used 3D point cloud data however outputs the persistence diagram instead of a persistence image. The creation of persistence diagrams was identified as the most computationally expensive process in the extraction of features and thus estimating this with neural networks can greatly reduce computational requirements. Ripsnet training used ModelNet10 datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> comprising 10 object categories. This shares the topological limitations identified for ModelNet40.</p>
</div>
<div id="S1.SS1.p7" class="ltx_para">
<p id="S1.SS1.p7.1" class="ltx_p">Further testing on the topological space evaluation abilities of neural networks is performed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
This paper emphasizes the benefit of directly outputting topological features rather than persistence diagrams in certain applications to bypass additional computational processing. This work was successful in estimating binary features and â€œtropical coordinatesâ€ from 2D images.
In many applications, the persistence diagram is nothing more than a necessary intermediary step in the calculation of topological features such as Betti numbers.</p>
</div>
<div id="S1.SS1.p8" class="ltx_para">
<p id="S1.SS1.p8.1" class="ltx_p">Our research also attempts to bypass persistence diagram creation and instead directly outputs topological features as labels for each point in the cloud. This allows for object extraction filtered by genus or Betti numbers to produce shortlisted key data subsets.</p>
</div>
<div id="S1.SS1.p9" class="ltx_para">
<p id="S1.SS1.p9.1" class="ltx_p">The topological property on which we segment the network is genus (or, equivalently, Betti number) which we discuss in SectionÂ <a href="#S2.SS2" title="II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>.</p>
</div>
<div id="S1.SS1.p10" class="ltx_para">
<p id="S1.SS1.p10.1" class="ltx_p">Our motivation for this research extends beyond achieving high accuracy in topological analysis. We aim to explore the inherent capacity of neural networks to analyse topological signatures without leveraging pre-learned assumptions regarding object class.
Pre-existing literature uses machine learning for the interpretation of persistence intervals and the creation of persistence diagrams/images. Much of this work uses datasets with spurious correlations between geometric structure and topological features. Prior knowledge regarding certain object characteristics can make it challenging to isolate the true ability of neural networks to interpret data from a topological perspective. We attempt to mitigate this issue through random homeomorphic deformations when generating the Repulse dataset.</p>
</div>
<div id="S1.SS1.p11" class="ltx_para">
<p id="S1.SS1.p11.1" class="ltx_p">We mention in passing that leveraging domain-specific correlations to enhance accuracy may be desirable in real-world applications. However, for our study it compromises the conceptual purity and generality of our approach.</p>
</div>
<div id="S1.SS1.p12" class="ltx_para">
<p id="S1.SS1.p12.1" class="ltx_p">Furthermore, there is an adjacent line of research where the lifespan of persistence intervals can be analyzed to extract topological signatures such as Betti numbers. This interval analysis can be subjective and case specific as different samples and applications may have different properties and scales. Existing research has successfully shown that neural networks can be beneficial in interpreting persistence intervalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. These networks utilize persistent homology as a pre-processing stage and are subsequently still prone to the resource, time, and ambiguity factors previously discussed.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.4.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.5.2" class="ltx_text ltx_font_italic">Approach, Data, and Networks</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">With no existing research, experiments, or benchmarks for 3D topological segmentation, it was deemed premature to introduce additional architectural changes or propose a new network without pre-existing benchmarks.
Instead, we apply a novel approach to TDA via feature segmentation and introduce a synthetic dataset designed to train and evaluate this approach with selected existing networks.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">The surveyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> explores the leading networks for deep learning on 3D point cloud data.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">PointNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> was pioneering research that introduced a multi-layer perceptron network for 3D point cloud data classification and segmentation. PointNet performs well at extracting global features and works on each point individually making it input permutation invariant. Due to this invariance to point order, PointNet does not perform competitively at extracting features at local scales.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">To increase local feature extraction a hierarchical network PointNet++ was proposed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
PointNet++ expands upon PointNet and uses a 3 abstraction layer approach to analyze multiple feature scales.</p>
</div>
<div id="S1.SS2.p5" class="ltx_para">
<p id="S1.SS2.p5.1" class="ltx_p">Multiple networks have emerged to further improve the accuracy of 3D point cloud deep learning. As there are not sufficient benchmarks for topological analysis we considered top performing and intriguing networks evaluated on the S3DIS datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This dataset contains large, indoor office space data with labelling for structural elements and common furniture and items. Comparing network accuracy on the S3DIS dataset would not directly translate to this new topological task, however, it gave some indication of relative performance for 3D segmentation.</p>
</div>
<div id="S1.SS2.p6" class="ltx_para">
<p id="S1.SS2.p6.1" class="ltx_p">The three networks chosen for our experiments were PointNet++, RepSurf-UÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and Point TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
<div id="S1.SS2.p7" class="ltx_para">
<p id="S1.SS2.p7.1" class="ltx_p">The Umbrella RepSurf (RepSurf-U) networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is built upon PointNet++ and is inspired by umbrella curvatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> concepts from computer graphics to form explicit local connections in raw unordered point cloud data. This curvature construction can offer surface and connectivity context to otherwise sparse, edgeless point sets.</p>
</div>
<div id="S1.SS2.p8" class="ltx_para">
<p id="S1.SS2.p8.1" class="ltx_p">The Point Transformer networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> introduces point transformer blocks with self-attention layers for 3D point cloud processing. Self-attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> attempts to assess the contextual weight of embeddings such as the significance of words in a sentence with respect to a target word. Point Transformer applies self-attention around centroid datapoints within point subsets known as â€˜local neighbourhoodsâ€™ which are formed via k-nearest neighbours.</p>
</div>
<div id="S1.SS2.p9" class="ltx_para">
<p id="S1.SS2.p9.1" class="ltx_p">The accuracy of these networks when evaluated on S3DIS dataset (Area-5) can be seen in TableÂ <a href="#S1.T1" title="TABLE I â€£ I-B Approach, Data, and Networks â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. These results were obtained from the official RepSurf GitHub repositoryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Accuracy of chosen networks performed on S3DIS (Area-5).</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU</th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAcc</th>
<th id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">OA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<th id="S1.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PointNet++</th>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">64.05</td>
<td id="S1.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">71.52</td>
<td id="S1.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">87.92</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<th id="S1.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RepSurf-U</th>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_center">68.86</td>
<td id="S1.T1.1.3.2.3" class="ltx_td ltx_align_center">76.54</td>
<td id="S1.T1.1.3.2.4" class="ltx_td ltx_align_center">90.22</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<th id="S1.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Point Transformer</th>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">70.4</td>
<td id="S1.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">76.5</td>
<td id="S1.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">90.8</td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.SS2.p10" class="ltx_para">
<p id="S1.SS2.p10.1" class="ltx_p">PointNet++ was used in our experiments as it is the backbone and inspiration for many new architectures which served as an accuracy baseline. While it scored lower than RepSurf-U and Point Transformer, it was necessary to evaluate whether the enhancements in newer networks were equally applicable to TDA or if they were specifically optimized for recognition segmentation tasks.
RepSurf-U was chosen as it uses PointNet++ as a backbone with specific design elements for surface construction which seemed conceptually interesting for manifold analysis.
Point Transformer was chosen as there is emerging research interest around transformer networks, especially in the natural language processing field. We found it compelling to assess how such architectures fare in TDA tasks.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Three-Dimensional Point Cloud</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">Three-dimensional point cloud data comprises of points in space with <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">x</annotation></semantics></math>, <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">y</annotation></semantics></math>, and <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">z</annotation></semantics></math> coordinates and may include other channels such as red, green, and blue color (RGB).
Point clouds contain no information about connectivity between points; only the points themselves are present.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Point cloud data is commonly used as many real-world sensors such as depth imaging and lidar construct scans based on distance to the surface of an object. Additionally, it is possible to sample existing mesh or voxel data into a point cloud with varying point counts.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">We note that sometimes network training with point cloud data will also include the normal vector to the surface of the object approximated by the point cloud input. This information was not utilised for experimentation on the Repulse dataset as it offers additional boundary and surface context. For complex scenes with unordered and close proximity points the normal vectors may be hard to determine. Additionally, these vectors are not used in conventional persistent homology.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Topology</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.7" class="ltx_p">Geometric Analysis can describe properties in Euclidean space such as size, shape and volume.
Such geometric properties change when objects are deformed within the space.
Topological Analysis, on the other hand, studies properties that are invariant to homeomorphic deformations and instead describes fundamental structural features such as holes and voids.
Homology is a key concept in topological analysis (see e.g.,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>).
For a manifold, <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">X</annotation></semantics></math>, and every dimension, <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">n</annotation></semantics></math>, there exists a vector space <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="H_{n}(X)" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mrow id="S2.SS2.p1.3.m3.1.2" xref="S2.SS2.p1.3.m3.1.2.cmml"><msub id="S2.SS2.p1.3.m3.1.2.2" xref="S2.SS2.p1.3.m3.1.2.2.cmml"><mi id="S2.SS2.p1.3.m3.1.2.2.2" xref="S2.SS2.p1.3.m3.1.2.2.2.cmml">H</mi><mi id="S2.SS2.p1.3.m3.1.2.2.3" xref="S2.SS2.p1.3.m3.1.2.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m3.1.2.1" xref="S2.SS2.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S2.SS2.p1.3.m3.1.2.3.2" xref="S2.SS2.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m3.1.2.3.2.1" xref="S2.SS2.p1.3.m3.1.2.cmml">(</mo><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">X</mi><mo stretchy="false" id="S2.SS2.p1.3.m3.1.2.3.2.2" xref="S2.SS2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.2.cmml" xref="S2.SS2.p1.3.m3.1.2"><times id="S2.SS2.p1.3.m3.1.2.1.cmml" xref="S2.SS2.p1.3.m3.1.2.1"></times><apply id="S2.SS2.p1.3.m3.1.2.2.cmml" xref="S2.SS2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.2.2.1.cmml" xref="S2.SS2.p1.3.m3.1.2.2">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.2.2.2.cmml" xref="S2.SS2.p1.3.m3.1.2.2.2">ğ»</ci><ci id="S2.SS2.p1.3.m3.1.2.2.3.cmml" xref="S2.SS2.p1.3.m3.1.2.2.3">ğ‘›</ci></apply><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ‘‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">H_{n}(X)</annotation></semantics></math>â€”called the <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">n</annotation></semantics></math><sup id="S2.SS2.p1.7.1" class="ltx_sup">th</sup> Homology group of <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mi id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">X</annotation></semantics></math>â€”which characterizes the topological <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><mi id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">n</annotation></semantics></math>-dimensional components of the space <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">X</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.7" class="ltx_p">A related notion to the homology groups are the Betti Numbers, <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="{\beta_{n}}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">Î²</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ›½</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">{\beta_{n}}</annotation></semantics></math>.
Formally, the <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">n</annotation></semantics></math><sup id="S2.SS2.p2.7.1" class="ltx_sup">th</sup> Betti number of a topological space, <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">X</annotation></semantics></math>, is the dimension of the <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">n</annotation></semantics></math><sup id="S2.SS2.p2.7.2" class="ltx_sup">th</sup> homology group of <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">X</annotation></semantics></math>.
Informally, the <math id="S2.SS2.p2.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.6.m6.1a"><mi id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><ci id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">n</annotation></semantics></math><sup id="S2.SS2.p2.7.3" class="ltx_sup">th</sup> Betti number describes the <math id="S2.SS2.p2.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.7.m7.1a"><mi id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><ci id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">n</annotation></semantics></math>-dimensional â€œholesâ€ within a manifold. In 3-dimensional Euclidean space only the first three Betti numbers are relevant:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="{\beta_{0}}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><msub id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">Î²</mi><mn id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">{\beta_{0}}</annotation></semantics></math> can be conceptualized as the number of disconnected objects or blobs,</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.2" class="ltx_p"><math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="{\beta_{1}}" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">Î²</mi><mn id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">{\beta_{1}}</annotation></semantics></math> can be conceptualized as the number of 1-dimensional holes, i.e., those bounded by <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="S^{1}" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><msup id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml">S</mi><mn id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">superscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">ğ‘†</ci><cn type="integer" id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">S^{1}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="{\beta_{2}}" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><msub id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">Î²</mi><mn id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">{\beta_{2}}</annotation></semantics></math> can be conceptualized as the number of voids or bubbles in 3-dimensional space.</p>
</div>
</li>
</ul>
<p id="S2.SS2.p2.8" class="ltx_p">Through this definition, a coffee mug has the same Betti numbers as a ring, and glasses frames have the same as a figure 8. Examples of Betti numbers for different manifolds can be seen in FigÂ <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We mention in passing that the Euler characteristic of a manifold is related to the Betti numbers through the equation:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.1" class="ltx_Math" alttext="\chi=\beta_{0}-\beta_{1}+\beta_{2}-\beta_{3}+...+\beta_{n}" display="block"><semantics id="S2.Ex1.m1.1a"><mrow id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml"><mi id="S2.Ex1.m1.1.1.2" xref="S2.Ex1.m1.1.1.2.cmml">Ï‡</mi><mo id="S2.Ex1.m1.1.1.1" xref="S2.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.3.cmml"><mrow id="S2.Ex1.m1.1.1.3.2" xref="S2.Ex1.m1.1.1.3.2.cmml"><mrow id="S2.Ex1.m1.1.1.3.2.2" xref="S2.Ex1.m1.1.1.3.2.2.cmml"><mrow id="S2.Ex1.m1.1.1.3.2.2.2" xref="S2.Ex1.m1.1.1.3.2.2.2.cmml"><msub id="S2.Ex1.m1.1.1.3.2.2.2.2" xref="S2.Ex1.m1.1.1.3.2.2.2.2.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2.2.2.2" xref="S2.Ex1.m1.1.1.3.2.2.2.2.2.cmml">Î²</mi><mn id="S2.Ex1.m1.1.1.3.2.2.2.2.3" xref="S2.Ex1.m1.1.1.3.2.2.2.2.3.cmml">0</mn></msub><mo id="S2.Ex1.m1.1.1.3.2.2.2.1" xref="S2.Ex1.m1.1.1.3.2.2.2.1.cmml">âˆ’</mo><msub id="S2.Ex1.m1.1.1.3.2.2.2.3" xref="S2.Ex1.m1.1.1.3.2.2.2.3.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2.2.3.2" xref="S2.Ex1.m1.1.1.3.2.2.2.3.2.cmml">Î²</mi><mn id="S2.Ex1.m1.1.1.3.2.2.2.3.3" xref="S2.Ex1.m1.1.1.3.2.2.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.Ex1.m1.1.1.3.2.2.1" xref="S2.Ex1.m1.1.1.3.2.2.1.cmml">+</mo><msub id="S2.Ex1.m1.1.1.3.2.2.3" xref="S2.Ex1.m1.1.1.3.2.2.3.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2.3.2" xref="S2.Ex1.m1.1.1.3.2.2.3.2.cmml">Î²</mi><mn id="S2.Ex1.m1.1.1.3.2.2.3.3" xref="S2.Ex1.m1.1.1.3.2.2.3.3.cmml">2</mn></msub></mrow><mo id="S2.Ex1.m1.1.1.3.2.1" xref="S2.Ex1.m1.1.1.3.2.1.cmml">âˆ’</mo><msub id="S2.Ex1.m1.1.1.3.2.3" xref="S2.Ex1.m1.1.1.3.2.3.cmml"><mi id="S2.Ex1.m1.1.1.3.2.3.2" xref="S2.Ex1.m1.1.1.3.2.3.2.cmml">Î²</mi><mn id="S2.Ex1.m1.1.1.3.2.3.3" xref="S2.Ex1.m1.1.1.3.2.3.3.cmml">3</mn></msub></mrow><mo id="S2.Ex1.m1.1.1.3.1" xref="S2.Ex1.m1.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S2.Ex1.m1.1.1.3.3" xref="S2.Ex1.m1.1.1.3.3.cmml">â€¦</mi><mo id="S2.Ex1.m1.1.1.3.1a" xref="S2.Ex1.m1.1.1.3.1.cmml">+</mo><msub id="S2.Ex1.m1.1.1.3.4" xref="S2.Ex1.m1.1.1.3.4.cmml"><mi id="S2.Ex1.m1.1.1.3.4.2" xref="S2.Ex1.m1.1.1.3.4.2.cmml">Î²</mi><mi id="S2.Ex1.m1.1.1.3.4.3" xref="S2.Ex1.m1.1.1.3.4.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1"><eq id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1"></eq><ci id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.2">ğœ’</ci><apply id="S2.Ex1.m1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.3"><plus id="S2.Ex1.m1.1.1.3.1.cmml" xref="S2.Ex1.m1.1.1.3.1"></plus><apply id="S2.Ex1.m1.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2"><minus id="S2.Ex1.m1.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2.1"></minus><apply id="S2.Ex1.m1.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2"><plus id="S2.Ex1.m1.1.1.3.2.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2.2.1"></plus><apply id="S2.Ex1.m1.1.1.3.2.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2"><minus id="S2.Ex1.m1.1.1.3.2.2.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.1"></minus><apply id="S2.Ex1.m1.1.1.3.2.2.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.2.2.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.2.2.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.2.2">ğ›½</ci><cn type="integer" id="S2.Ex1.m1.1.1.3.2.2.2.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.2.3">0</cn></apply><apply id="S2.Ex1.m1.1.1.3.2.2.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.2.2.3.1.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.2.2.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.3.2">ğ›½</ci><cn type="integer" id="S2.Ex1.m1.1.1.3.2.2.2.3.3.cmml" xref="S2.Ex1.m1.1.1.3.2.2.2.3.3">1</cn></apply></apply><apply id="S2.Ex1.m1.1.1.3.2.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.2.3.1.cmml" xref="S2.Ex1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.2.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2.3.2">ğ›½</ci><cn type="integer" id="S2.Ex1.m1.1.1.3.2.2.3.3.cmml" xref="S2.Ex1.m1.1.1.3.2.2.3.3">2</cn></apply></apply><apply id="S2.Ex1.m1.1.1.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.3.1.cmml" xref="S2.Ex1.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2.3.2">ğ›½</ci><cn type="integer" id="S2.Ex1.m1.1.1.3.2.3.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3.3">3</cn></apply></apply><ci id="S2.Ex1.m1.1.1.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3">â€¦</ci><apply id="S2.Ex1.m1.1.1.3.4.cmml" xref="S2.Ex1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.4.1.cmml" xref="S2.Ex1.m1.1.1.3.4">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.4.2.cmml" xref="S2.Ex1.m1.1.1.3.4.2">ğ›½</ci><ci id="S2.Ex1.m1.1.1.3.4.3.cmml" xref="S2.Ex1.m1.1.1.3.4.3">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">\chi=\beta_{0}-\beta_{1}+\beta_{2}-\beta_{3}+...+\beta_{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.6" class="ltx_p">Our topological Repulse dataset comprises of compact orientable surfaces which consist solely of spheres, <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="S^{2}" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><msup id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml"><mi id="S2.SS2.p4.1.m1.1.1.2" xref="S2.SS2.p4.1.m1.1.1.2.cmml">S</mi><mn id="S2.SS2.p4.1.m1.1.1.3" xref="S2.SS2.p4.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.1.m1.1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">superscript</csymbol><ci id="S2.SS2.p4.1.m1.1.1.2.cmml" xref="S2.SS2.p4.1.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S2.SS2.p4.1.m1.1.1.3.cmml" xref="S2.SS2.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">S^{2}</annotation></semantics></math> and connected sums of <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">g</annotation></semantics></math> tori, <math id="S2.SS2.p4.3.m3.1" class="ltx_Math" alttext="T^{2}\#...\#T^{2}" display="inline"><semantics id="S2.SS2.p4.3.m3.1a"><mrow id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml"><msup id="S2.SS2.p4.3.m3.1.1.2" xref="S2.SS2.p4.3.m3.1.1.2.cmml"><mi id="S2.SS2.p4.3.m3.1.1.2.2" xref="S2.SS2.p4.3.m3.1.1.2.2.cmml">T</mi><mn id="S2.SS2.p4.3.m3.1.1.2.3" xref="S2.SS2.p4.3.m3.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p4.3.m3.1.1.1" xref="S2.SS2.p4.3.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p4.3.m3.1.1.3" xref="S2.SS2.p4.3.m3.1.1.3.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.3.m3.1.1.1a" xref="S2.SS2.p4.3.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p4.3.m3.1.1.4" xref="S2.SS2.p4.3.m3.1.1.4.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.3.m3.1.1.1b" xref="S2.SS2.p4.3.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p4.3.m3.1.1.5" xref="S2.SS2.p4.3.m3.1.1.5.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.3.m3.1.1.1c" xref="S2.SS2.p4.3.m3.1.1.1.cmml">â€‹</mo><msup id="S2.SS2.p4.3.m3.1.1.6" xref="S2.SS2.p4.3.m3.1.1.6.cmml"><mi id="S2.SS2.p4.3.m3.1.1.6.2" xref="S2.SS2.p4.3.m3.1.1.6.2.cmml">T</mi><mn id="S2.SS2.p4.3.m3.1.1.6.3" xref="S2.SS2.p4.3.m3.1.1.6.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><apply id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1"><times id="S2.SS2.p4.3.m3.1.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1.1"></times><apply id="S2.SS2.p4.3.m3.1.1.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.2.1.cmml" xref="S2.SS2.p4.3.m3.1.1.2">superscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.2.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S2.SS2.p4.3.m3.1.1.2.3.cmml" xref="S2.SS2.p4.3.m3.1.1.2.3">2</cn></apply><ci id="S2.SS2.p4.3.m3.1.1.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3">#</ci><ci id="S2.SS2.p4.3.m3.1.1.4.cmml" xref="S2.SS2.p4.3.m3.1.1.4">â€¦</ci><ci id="S2.SS2.p4.3.m3.1.1.5.cmml" xref="S2.SS2.p4.3.m3.1.1.5">#</ci><apply id="S2.SS2.p4.3.m3.1.1.6.cmml" xref="S2.SS2.p4.3.m3.1.1.6"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.6.1.cmml" xref="S2.SS2.p4.3.m3.1.1.6">superscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.6.2.cmml" xref="S2.SS2.p4.3.m3.1.1.6.2">ğ‘‡</ci><cn type="integer" id="S2.SS2.p4.3.m3.1.1.6.3.cmml" xref="S2.SS2.p4.3.m3.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">T^{2}\#...\#T^{2}</annotation></semantics></math>, where <math id="S2.SS2.p4.4.m4.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.SS2.p4.4.m4.1a"><mi id="S2.SS2.p4.4.m4.1.1" xref="S2.SS2.p4.4.m4.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.4.m4.1b"><ci id="S2.SS2.p4.4.m4.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.4.m4.1c">g</annotation></semantics></math> is the genus, a natural number (i.e., <math id="S2.SS2.p4.5.m5.1" class="ltx_Math" alttext="g\in\mathbb{N}" display="inline"><semantics id="S2.SS2.p4.5.m5.1a"><mrow id="S2.SS2.p4.5.m5.1.1" xref="S2.SS2.p4.5.m5.1.1.cmml"><mi id="S2.SS2.p4.5.m5.1.1.2" xref="S2.SS2.p4.5.m5.1.1.2.cmml">g</mi><mo id="S2.SS2.p4.5.m5.1.1.1" xref="S2.SS2.p4.5.m5.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS2.p4.5.m5.1.1.3" xref="S2.SS2.p4.5.m5.1.1.3.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.5.m5.1b"><apply id="S2.SS2.p4.5.m5.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1"><in id="S2.SS2.p4.5.m5.1.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1.1"></in><ci id="S2.SS2.p4.5.m5.1.1.2.cmml" xref="S2.SS2.p4.5.m5.1.1.2">ğ‘”</ci><ci id="S2.SS2.p4.5.m5.1.1.3.cmml" xref="S2.SS2.p4.5.m5.1.1.3">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.5.m5.1c">g\in\mathbb{N}</annotation></semantics></math>) denoting the number of holes. The Betti numbers and Euler characteristics for <math id="S2.SS2.p4.6.m6.4" class="ltx_Math" alttext="g\in\left\{0,1,2,3\right\}" display="inline"><semantics id="S2.SS2.p4.6.m6.4a"><mrow id="S2.SS2.p4.6.m6.4.5" xref="S2.SS2.p4.6.m6.4.5.cmml"><mi id="S2.SS2.p4.6.m6.4.5.2" xref="S2.SS2.p4.6.m6.4.5.2.cmml">g</mi><mo id="S2.SS2.p4.6.m6.4.5.1" xref="S2.SS2.p4.6.m6.4.5.1.cmml">âˆˆ</mo><mrow id="S2.SS2.p4.6.m6.4.5.3.2" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml"><mo id="S2.SS2.p4.6.m6.4.5.3.2.1" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml">{</mo><mn id="S2.SS2.p4.6.m6.1.1" xref="S2.SS2.p4.6.m6.1.1.cmml">0</mn><mo id="S2.SS2.p4.6.m6.4.5.3.2.2" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml">,</mo><mn id="S2.SS2.p4.6.m6.2.2" xref="S2.SS2.p4.6.m6.2.2.cmml">1</mn><mo id="S2.SS2.p4.6.m6.4.5.3.2.3" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml">,</mo><mn id="S2.SS2.p4.6.m6.3.3" xref="S2.SS2.p4.6.m6.3.3.cmml">2</mn><mo id="S2.SS2.p4.6.m6.4.5.3.2.4" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml">,</mo><mn id="S2.SS2.p4.6.m6.4.4" xref="S2.SS2.p4.6.m6.4.4.cmml">3</mn><mo id="S2.SS2.p4.6.m6.4.5.3.2.5" xref="S2.SS2.p4.6.m6.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.6.m6.4b"><apply id="S2.SS2.p4.6.m6.4.5.cmml" xref="S2.SS2.p4.6.m6.4.5"><in id="S2.SS2.p4.6.m6.4.5.1.cmml" xref="S2.SS2.p4.6.m6.4.5.1"></in><ci id="S2.SS2.p4.6.m6.4.5.2.cmml" xref="S2.SS2.p4.6.m6.4.5.2">ğ‘”</ci><set id="S2.SS2.p4.6.m6.4.5.3.1.cmml" xref="S2.SS2.p4.6.m6.4.5.3.2"><cn type="integer" id="S2.SS2.p4.6.m6.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1">0</cn><cn type="integer" id="S2.SS2.p4.6.m6.2.2.cmml" xref="S2.SS2.p4.6.m6.2.2">1</cn><cn type="integer" id="S2.SS2.p4.6.m6.3.3.cmml" xref="S2.SS2.p4.6.m6.3.3">2</cn><cn type="integer" id="S2.SS2.p4.6.m6.4.4.cmml" xref="S2.SS2.p4.6.m6.4.4">3</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.6.m6.4c">g\in\left\{0,1,2,3\right\}</annotation></semantics></math> are shown in Table <a href="#S2.T2" title="TABLE II â€£ II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a></p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Genus (<math id="S2.T2.4.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.4.m1.1b"><mi id="S2.T2.4.m1.1.1" xref="S2.T2.4.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.4.m1.1c"><ci id="S2.T2.4.m1.1.1.cmml" xref="S2.T2.4.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.m1.1d">g</annotation></semantics></math>), Betti numbers (<math id="S2.T2.5.m2.1" class="ltx_Math" alttext="\beta_{n}" display="inline"><semantics id="S2.T2.5.m2.1b"><msub id="S2.T2.5.m2.1.1" xref="S2.T2.5.m2.1.1.cmml"><mi id="S2.T2.5.m2.1.1.2" xref="S2.T2.5.m2.1.1.2.cmml">Î²</mi><mi id="S2.T2.5.m2.1.1.3" xref="S2.T2.5.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T2.5.m2.1c"><apply id="S2.T2.5.m2.1.1.cmml" xref="S2.T2.5.m2.1.1"><csymbol cd="ambiguous" id="S2.T2.5.m2.1.1.1.cmml" xref="S2.T2.5.m2.1.1">subscript</csymbol><ci id="S2.T2.5.m2.1.1.2.cmml" xref="S2.T2.5.m2.1.1.2">ğ›½</ci><ci id="S2.T2.5.m2.1.1.3.cmml" xref="S2.T2.5.m2.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.m2.1d">\beta_{n}</annotation></semantics></math>) and Euler characteristic (<math id="S2.T2.6.m3.1" class="ltx_Math" alttext="\chi" display="inline"><semantics id="S2.T2.6.m3.1b"><mi id="S2.T2.6.m3.1.1" xref="S2.T2.6.m3.1.1.cmml">Ï‡</mi><annotation-xml encoding="MathML-Content" id="S2.T2.6.m3.1c"><ci id="S2.T2.6.m3.1.1.cmml" xref="S2.T2.6.m3.1.1">ğœ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.m3.1d">\chi</annotation></semantics></math>) of closed compact orientable surfaces.</figcaption>
<table id="S2.T2.21" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.12.6" class="ltx_tr">
<th id="S2.T2.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;">Surface <math id="S2.T2.7.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.T2.7.1.1.m1.1a"><mi id="S2.T2.7.1.1.m1.1.1" xref="S2.T2.7.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.T2.7.1.1.m1.1b"><ci id="S2.T2.7.1.1.m1.1.1.cmml" xref="S2.T2.7.1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.1.1.m1.1c">M</annotation></semantics></math>
</th>
<th id="S2.T2.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;"><math id="S2.T2.8.2.2.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.8.2.2.m1.1a"><mi id="S2.T2.8.2.2.m1.1.1" xref="S2.T2.8.2.2.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.8.2.2.m1.1b"><ci id="S2.T2.8.2.2.m1.1.1.cmml" xref="S2.T2.8.2.2.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.2.2.m1.1c">g</annotation></semantics></math></th>
<th id="S2.T2.9.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;"><math id="S2.T2.9.3.3.m1.1" class="ltx_Math" alttext="\beta_{0}" display="inline"><semantics id="S2.T2.9.3.3.m1.1a"><msub id="S2.T2.9.3.3.m1.1.1" xref="S2.T2.9.3.3.m1.1.1.cmml"><mi id="S2.T2.9.3.3.m1.1.1.2" xref="S2.T2.9.3.3.m1.1.1.2.cmml">Î²</mi><mn id="S2.T2.9.3.3.m1.1.1.3" xref="S2.T2.9.3.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.9.3.3.m1.1b"><apply id="S2.T2.9.3.3.m1.1.1.cmml" xref="S2.T2.9.3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.9.3.3.m1.1.1.1.cmml" xref="S2.T2.9.3.3.m1.1.1">subscript</csymbol><ci id="S2.T2.9.3.3.m1.1.1.2.cmml" xref="S2.T2.9.3.3.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.T2.9.3.3.m1.1.1.3.cmml" xref="S2.T2.9.3.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.3.3.m1.1c">\beta_{0}</annotation></semantics></math></th>
<th id="S2.T2.10.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;"><math id="S2.T2.10.4.4.m1.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S2.T2.10.4.4.m1.1a"><msub id="S2.T2.10.4.4.m1.1.1" xref="S2.T2.10.4.4.m1.1.1.cmml"><mi id="S2.T2.10.4.4.m1.1.1.2" xref="S2.T2.10.4.4.m1.1.1.2.cmml">Î²</mi><mn id="S2.T2.10.4.4.m1.1.1.3" xref="S2.T2.10.4.4.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.10.4.4.m1.1b"><apply id="S2.T2.10.4.4.m1.1.1.cmml" xref="S2.T2.10.4.4.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.10.4.4.m1.1.1.1.cmml" xref="S2.T2.10.4.4.m1.1.1">subscript</csymbol><ci id="S2.T2.10.4.4.m1.1.1.2.cmml" xref="S2.T2.10.4.4.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.T2.10.4.4.m1.1.1.3.cmml" xref="S2.T2.10.4.4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.10.4.4.m1.1c">\beta_{1}</annotation></semantics></math></th>
<th id="S2.T2.11.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;"><math id="S2.T2.11.5.5.m1.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S2.T2.11.5.5.m1.1a"><msub id="S2.T2.11.5.5.m1.1.1" xref="S2.T2.11.5.5.m1.1.1.cmml"><mi id="S2.T2.11.5.5.m1.1.1.2" xref="S2.T2.11.5.5.m1.1.1.2.cmml">Î²</mi><mn id="S2.T2.11.5.5.m1.1.1.3" xref="S2.T2.11.5.5.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.11.5.5.m1.1b"><apply id="S2.T2.11.5.5.m1.1.1.cmml" xref="S2.T2.11.5.5.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.11.5.5.m1.1.1.1.cmml" xref="S2.T2.11.5.5.m1.1.1">subscript</csymbol><ci id="S2.T2.11.5.5.m1.1.1.2.cmml" xref="S2.T2.11.5.5.m1.1.1.2">ğ›½</ci><cn type="integer" id="S2.T2.11.5.5.m1.1.1.3.cmml" xref="S2.T2.11.5.5.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.11.5.5.m1.1c">\beta_{2}</annotation></semantics></math></th>
<th id="S2.T2.12.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:3.0pt;"><math id="S2.T2.12.6.6.m1.1" class="ltx_Math" alttext="\chi" display="inline"><semantics id="S2.T2.12.6.6.m1.1a"><mi id="S2.T2.12.6.6.m1.1.1" xref="S2.T2.12.6.6.m1.1.1.cmml">Ï‡</mi><annotation-xml encoding="MathML-Content" id="S2.T2.12.6.6.m1.1b"><ci id="S2.T2.12.6.6.m1.1.1.cmml" xref="S2.T2.12.6.6.m1.1.1">ğœ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.12.6.6.m1.1c">\chi</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.13.7" class="ltx_tr">
<td id="S2.T2.13.7.1" class="ltx_td ltx_align_left ltx_border_t">Sphere <math id="S2.T2.13.7.1.m1.1" class="ltx_Math" alttext="S^{2}" display="inline"><semantics id="S2.T2.13.7.1.m1.1a"><msup id="S2.T2.13.7.1.m1.1.1" xref="S2.T2.13.7.1.m1.1.1.cmml"><mi id="S2.T2.13.7.1.m1.1.1.2" xref="S2.T2.13.7.1.m1.1.1.2.cmml">S</mi><mn id="S2.T2.13.7.1.m1.1.1.3" xref="S2.T2.13.7.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T2.13.7.1.m1.1b"><apply id="S2.T2.13.7.1.m1.1.1.cmml" xref="S2.T2.13.7.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.13.7.1.m1.1.1.1.cmml" xref="S2.T2.13.7.1.m1.1.1">superscript</csymbol><ci id="S2.T2.13.7.1.m1.1.1.2.cmml" xref="S2.T2.13.7.1.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S2.T2.13.7.1.m1.1.1.3.cmml" xref="S2.T2.13.7.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.7.1.m1.1c">S^{2}</annotation></semantics></math>
</td>
<td id="S2.T2.13.7.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S2.T2.13.7.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S2.T2.13.7.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S2.T2.13.7.5" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S2.T2.13.7.6" class="ltx_td ltx_align_center ltx_border_t">2</td>
</tr>
<tr id="S2.T2.14.8" class="ltx_tr">
<td id="S2.T2.14.8.1" class="ltx_td ltx_align_left">Torus <math id="S2.T2.14.8.1.m1.1" class="ltx_Math" alttext="T^{2}" display="inline"><semantics id="S2.T2.14.8.1.m1.1a"><msup id="S2.T2.14.8.1.m1.1.1" xref="S2.T2.14.8.1.m1.1.1.cmml"><mi id="S2.T2.14.8.1.m1.1.1.2" xref="S2.T2.14.8.1.m1.1.1.2.cmml">T</mi><mn id="S2.T2.14.8.1.m1.1.1.3" xref="S2.T2.14.8.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T2.14.8.1.m1.1b"><apply id="S2.T2.14.8.1.m1.1.1.cmml" xref="S2.T2.14.8.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.14.8.1.m1.1.1.1.cmml" xref="S2.T2.14.8.1.m1.1.1">superscript</csymbol><ci id="S2.T2.14.8.1.m1.1.1.2.cmml" xref="S2.T2.14.8.1.m1.1.1.2">ğ‘‡</ci><cn type="integer" id="S2.T2.14.8.1.m1.1.1.3.cmml" xref="S2.T2.14.8.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.8.1.m1.1c">T^{2}</annotation></semantics></math>
</td>
<td id="S2.T2.14.8.2" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.14.8.3" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.14.8.4" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.14.8.5" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.14.8.6" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S2.T2.15.9" class="ltx_tr">
<td id="S2.T2.15.9.1" class="ltx_td ltx_align_left">2-holed torus <math id="S2.T2.15.9.1.m1.1" class="ltx_Math" alttext="T^{2}\#T^{2}" display="inline"><semantics id="S2.T2.15.9.1.m1.1a"><mrow id="S2.T2.15.9.1.m1.1.1" xref="S2.T2.15.9.1.m1.1.1.cmml"><msup id="S2.T2.15.9.1.m1.1.1.2" xref="S2.T2.15.9.1.m1.1.1.2.cmml"><mi id="S2.T2.15.9.1.m1.1.1.2.2" xref="S2.T2.15.9.1.m1.1.1.2.2.cmml">T</mi><mn id="S2.T2.15.9.1.m1.1.1.2.3" xref="S2.T2.15.9.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.T2.15.9.1.m1.1.1.1" xref="S2.T2.15.9.1.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.15.9.1.m1.1.1.3" xref="S2.T2.15.9.1.m1.1.1.3.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.T2.15.9.1.m1.1.1.1a" xref="S2.T2.15.9.1.m1.1.1.1.cmml">â€‹</mo><msup id="S2.T2.15.9.1.m1.1.1.4" xref="S2.T2.15.9.1.m1.1.1.4.cmml"><mi id="S2.T2.15.9.1.m1.1.1.4.2" xref="S2.T2.15.9.1.m1.1.1.4.2.cmml">T</mi><mn id="S2.T2.15.9.1.m1.1.1.4.3" xref="S2.T2.15.9.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.15.9.1.m1.1b"><apply id="S2.T2.15.9.1.m1.1.1.cmml" xref="S2.T2.15.9.1.m1.1.1"><times id="S2.T2.15.9.1.m1.1.1.1.cmml" xref="S2.T2.15.9.1.m1.1.1.1"></times><apply id="S2.T2.15.9.1.m1.1.1.2.cmml" xref="S2.T2.15.9.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T2.15.9.1.m1.1.1.2.1.cmml" xref="S2.T2.15.9.1.m1.1.1.2">superscript</csymbol><ci id="S2.T2.15.9.1.m1.1.1.2.2.cmml" xref="S2.T2.15.9.1.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S2.T2.15.9.1.m1.1.1.2.3.cmml" xref="S2.T2.15.9.1.m1.1.1.2.3">2</cn></apply><ci id="S2.T2.15.9.1.m1.1.1.3.cmml" xref="S2.T2.15.9.1.m1.1.1.3">#</ci><apply id="S2.T2.15.9.1.m1.1.1.4.cmml" xref="S2.T2.15.9.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.T2.15.9.1.m1.1.1.4.1.cmml" xref="S2.T2.15.9.1.m1.1.1.4">superscript</csymbol><ci id="S2.T2.15.9.1.m1.1.1.4.2.cmml" xref="S2.T2.15.9.1.m1.1.1.4.2">ğ‘‡</ci><cn type="integer" id="S2.T2.15.9.1.m1.1.1.4.3.cmml" xref="S2.T2.15.9.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.15.9.1.m1.1c">T^{2}\#T^{2}</annotation></semantics></math>
</td>
<td id="S2.T2.15.9.2" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.15.9.3" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.15.9.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.15.9.5" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.15.9.6" class="ltx_td ltx_align_center">-2</td>
</tr>
<tr id="S2.T2.16.10" class="ltx_tr">
<td id="S2.T2.16.10.1" class="ltx_td ltx_align_left">3-holed torus <math id="S2.T2.16.10.1.m1.1" class="ltx_Math" alttext="T^{2}\#T^{2}\#T^{2}" display="inline"><semantics id="S2.T2.16.10.1.m1.1a"><mrow id="S2.T2.16.10.1.m1.1.1" xref="S2.T2.16.10.1.m1.1.1.cmml"><msup id="S2.T2.16.10.1.m1.1.1.2" xref="S2.T2.16.10.1.m1.1.1.2.cmml"><mi id="S2.T2.16.10.1.m1.1.1.2.2" xref="S2.T2.16.10.1.m1.1.1.2.2.cmml">T</mi><mn id="S2.T2.16.10.1.m1.1.1.2.3" xref="S2.T2.16.10.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.T2.16.10.1.m1.1.1.1" xref="S2.T2.16.10.1.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.16.10.1.m1.1.1.3" xref="S2.T2.16.10.1.m1.1.1.3.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.T2.16.10.1.m1.1.1.1a" xref="S2.T2.16.10.1.m1.1.1.1.cmml">â€‹</mo><msup id="S2.T2.16.10.1.m1.1.1.4" xref="S2.T2.16.10.1.m1.1.1.4.cmml"><mi id="S2.T2.16.10.1.m1.1.1.4.2" xref="S2.T2.16.10.1.m1.1.1.4.2.cmml">T</mi><mn id="S2.T2.16.10.1.m1.1.1.4.3" xref="S2.T2.16.10.1.m1.1.1.4.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.T2.16.10.1.m1.1.1.1b" xref="S2.T2.16.10.1.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.16.10.1.m1.1.1.5" xref="S2.T2.16.10.1.m1.1.1.5.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.T2.16.10.1.m1.1.1.1c" xref="S2.T2.16.10.1.m1.1.1.1.cmml">â€‹</mo><msup id="S2.T2.16.10.1.m1.1.1.6" xref="S2.T2.16.10.1.m1.1.1.6.cmml"><mi id="S2.T2.16.10.1.m1.1.1.6.2" xref="S2.T2.16.10.1.m1.1.1.6.2.cmml">T</mi><mn id="S2.T2.16.10.1.m1.1.1.6.3" xref="S2.T2.16.10.1.m1.1.1.6.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.16.10.1.m1.1b"><apply id="S2.T2.16.10.1.m1.1.1.cmml" xref="S2.T2.16.10.1.m1.1.1"><times id="S2.T2.16.10.1.m1.1.1.1.cmml" xref="S2.T2.16.10.1.m1.1.1.1"></times><apply id="S2.T2.16.10.1.m1.1.1.2.cmml" xref="S2.T2.16.10.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T2.16.10.1.m1.1.1.2.1.cmml" xref="S2.T2.16.10.1.m1.1.1.2">superscript</csymbol><ci id="S2.T2.16.10.1.m1.1.1.2.2.cmml" xref="S2.T2.16.10.1.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S2.T2.16.10.1.m1.1.1.2.3.cmml" xref="S2.T2.16.10.1.m1.1.1.2.3">2</cn></apply><ci id="S2.T2.16.10.1.m1.1.1.3.cmml" xref="S2.T2.16.10.1.m1.1.1.3">#</ci><apply id="S2.T2.16.10.1.m1.1.1.4.cmml" xref="S2.T2.16.10.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.T2.16.10.1.m1.1.1.4.1.cmml" xref="S2.T2.16.10.1.m1.1.1.4">superscript</csymbol><ci id="S2.T2.16.10.1.m1.1.1.4.2.cmml" xref="S2.T2.16.10.1.m1.1.1.4.2">ğ‘‡</ci><cn type="integer" id="S2.T2.16.10.1.m1.1.1.4.3.cmml" xref="S2.T2.16.10.1.m1.1.1.4.3">2</cn></apply><ci id="S2.T2.16.10.1.m1.1.1.5.cmml" xref="S2.T2.16.10.1.m1.1.1.5">#</ci><apply id="S2.T2.16.10.1.m1.1.1.6.cmml" xref="S2.T2.16.10.1.m1.1.1.6"><csymbol cd="ambiguous" id="S2.T2.16.10.1.m1.1.1.6.1.cmml" xref="S2.T2.16.10.1.m1.1.1.6">superscript</csymbol><ci id="S2.T2.16.10.1.m1.1.1.6.2.cmml" xref="S2.T2.16.10.1.m1.1.1.6.2">ğ‘‡</ci><cn type="integer" id="S2.T2.16.10.1.m1.1.1.6.3.cmml" xref="S2.T2.16.10.1.m1.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.10.1.m1.1c">T^{2}\#T^{2}\#T^{2}</annotation></semantics></math>
</td>
<td id="S2.T2.16.10.2" class="ltx_td ltx_align_center">3</td>
<td id="S2.T2.16.10.3" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.16.10.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.16.10.5" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.16.10.6" class="ltx_td ltx_align_center">-4</td>
</tr>
<tr id="S2.T2.21.15" class="ltx_tr">
<td id="S2.T2.18.12.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-bottom:3.0pt;">
<math id="S2.T2.17.11.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.17.11.1.m1.1a"><mi id="S2.T2.17.11.1.m1.1.1" xref="S2.T2.17.11.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.17.11.1.m1.1b"><ci id="S2.T2.17.11.1.m1.1.1.cmml" xref="S2.T2.17.11.1.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.17.11.1.m1.1c">g</annotation></semantics></math>-holed torus <math id="S2.T2.18.12.2.m2.1" class="ltx_Math" alttext="T^{2}\#\dots\#T^{2}" display="inline"><semantics id="S2.T2.18.12.2.m2.1a"><mrow id="S2.T2.18.12.2.m2.1.1" xref="S2.T2.18.12.2.m2.1.1.cmml"><msup id="S2.T2.18.12.2.m2.1.1.2" xref="S2.T2.18.12.2.m2.1.1.2.cmml"><mi id="S2.T2.18.12.2.m2.1.1.2.2" xref="S2.T2.18.12.2.m2.1.1.2.2.cmml">T</mi><mn id="S2.T2.18.12.2.m2.1.1.2.3" xref="S2.T2.18.12.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.T2.18.12.2.m2.1.1.1" xref="S2.T2.18.12.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.18.12.2.m2.1.1.3" xref="S2.T2.18.12.2.m2.1.1.3.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.T2.18.12.2.m2.1.1.1a" xref="S2.T2.18.12.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.18.12.2.m2.1.1.4" xref="S2.T2.18.12.2.m2.1.1.4.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S2.T2.18.12.2.m2.1.1.1b" xref="S2.T2.18.12.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.T2.18.12.2.m2.1.1.5" xref="S2.T2.18.12.2.m2.1.1.5.cmml">#</mi><mo lspace="0em" rspace="0em" id="S2.T2.18.12.2.m2.1.1.1c" xref="S2.T2.18.12.2.m2.1.1.1.cmml">â€‹</mo><msup id="S2.T2.18.12.2.m2.1.1.6" xref="S2.T2.18.12.2.m2.1.1.6.cmml"><mi id="S2.T2.18.12.2.m2.1.1.6.2" xref="S2.T2.18.12.2.m2.1.1.6.2.cmml">T</mi><mn id="S2.T2.18.12.2.m2.1.1.6.3" xref="S2.T2.18.12.2.m2.1.1.6.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.18.12.2.m2.1b"><apply id="S2.T2.18.12.2.m2.1.1.cmml" xref="S2.T2.18.12.2.m2.1.1"><times id="S2.T2.18.12.2.m2.1.1.1.cmml" xref="S2.T2.18.12.2.m2.1.1.1"></times><apply id="S2.T2.18.12.2.m2.1.1.2.cmml" xref="S2.T2.18.12.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.T2.18.12.2.m2.1.1.2.1.cmml" xref="S2.T2.18.12.2.m2.1.1.2">superscript</csymbol><ci id="S2.T2.18.12.2.m2.1.1.2.2.cmml" xref="S2.T2.18.12.2.m2.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S2.T2.18.12.2.m2.1.1.2.3.cmml" xref="S2.T2.18.12.2.m2.1.1.2.3">2</cn></apply><ci id="S2.T2.18.12.2.m2.1.1.3.cmml" xref="S2.T2.18.12.2.m2.1.1.3">#</ci><ci id="S2.T2.18.12.2.m2.1.1.4.cmml" xref="S2.T2.18.12.2.m2.1.1.4">â€¦</ci><ci id="S2.T2.18.12.2.m2.1.1.5.cmml" xref="S2.T2.18.12.2.m2.1.1.5">#</ci><apply id="S2.T2.18.12.2.m2.1.1.6.cmml" xref="S2.T2.18.12.2.m2.1.1.6"><csymbol cd="ambiguous" id="S2.T2.18.12.2.m2.1.1.6.1.cmml" xref="S2.T2.18.12.2.m2.1.1.6">superscript</csymbol><ci id="S2.T2.18.12.2.m2.1.1.6.2.cmml" xref="S2.T2.18.12.2.m2.1.1.6.2">ğ‘‡</ci><cn type="integer" id="S2.T2.18.12.2.m2.1.1.6.3.cmml" xref="S2.T2.18.12.2.m2.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.18.12.2.m2.1c">T^{2}\#\dots\#T^{2}</annotation></semantics></math>
</td>
<td id="S2.T2.19.13.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-bottom:3.0pt;"><math id="S2.T2.19.13.3.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.19.13.3.m1.1a"><mi id="S2.T2.19.13.3.m1.1.1" xref="S2.T2.19.13.3.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.19.13.3.m1.1b"><ci id="S2.T2.19.13.3.m1.1.1.cmml" xref="S2.T2.19.13.3.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.19.13.3.m1.1c">g</annotation></semantics></math></td>
<td id="S2.T2.21.15.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-bottom:3.0pt;">1</td>
<td id="S2.T2.20.14.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-bottom:3.0pt;">2<math id="S2.T2.20.14.4.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.20.14.4.m1.1a"><mi id="S2.T2.20.14.4.m1.1.1" xref="S2.T2.20.14.4.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.20.14.4.m1.1b"><ci id="S2.T2.20.14.4.m1.1.1.cmml" xref="S2.T2.20.14.4.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.20.14.4.m1.1c">g</annotation></semantics></math>
</td>
<td id="S2.T2.21.15.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-bottom:3.0pt;">1</td>
<td id="S2.T2.21.15.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-bottom:3.0pt;">2-2<math id="S2.T2.21.15.5.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.T2.21.15.5.m1.1a"><mi id="S2.T2.21.15.5.m1.1.1" xref="S2.T2.21.15.5.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.T2.21.15.5.m1.1b"><ci id="S2.T2.21.15.5.m1.1.1.cmml" xref="S2.T2.21.15.5.m1.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.21.15.5.m1.1c">g</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.3" class="ltx_p">The dataset is (as previously stated) restricted to compact orientable surfaces in 3 dimensions. This eliminates non-orientable surfaces such as MÃ¶bius strips and Klein bottles which have holes but no internal closed volumes. Under these restrictions, the Betti numbers can be calculated from the collective genus values of objects in the scene. This relationship is not bijective and subsequently the inverse can not be calculated as different permutations of objects can produce the same Betti numbers. The example scene mentioned in the introduction comprises a torus and a 2-holed torus and consequently has <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="\beta_{0}=2" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><msub id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml"><mi id="S2.SS2.p5.1.m1.1.1.2.2" xref="S2.SS2.p5.1.m1.1.1.2.2.cmml">Î²</mi><mn id="S2.SS2.p5.1.m1.1.1.2.3" xref="S2.SS2.p5.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S2.SS2.p5.1.m1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><eq id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1"></eq><apply id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.2.1.cmml" xref="S2.SS2.p5.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.2.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2.2">ğ›½</ci><cn type="integer" id="S2.SS2.p5.1.m1.1.1.2.3.cmml" xref="S2.SS2.p5.1.m1.1.1.2.3">0</cn></apply><cn type="integer" id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">\beta_{0}=2</annotation></semantics></math>, <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="\beta_{1}=6" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><msub id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml"><mi id="S2.SS2.p5.2.m2.1.1.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.cmml">Î²</mi><mn id="S2.SS2.p5.2.m2.1.1.2.3" xref="S2.SS2.p5.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="S2.SS2.p5.2.m2.1.1.1" xref="S2.SS2.p5.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><eq id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1.1"></eq><apply id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.2.m2.1.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.2.m2.1.1.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2">ğ›½</ci><cn type="integer" id="S2.SS2.p5.2.m2.1.1.2.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3">1</cn></apply><cn type="integer" id="S2.SS2.p5.2.m2.1.1.3.cmml" xref="S2.SS2.p5.2.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">\beta_{1}=6</annotation></semantics></math> and <math id="S2.SS2.p5.3.m3.1" class="ltx_Math" alttext="\beta_{2}=2" display="inline"><semantics id="S2.SS2.p5.3.m3.1a"><mrow id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml"><msub id="S2.SS2.p5.3.m3.1.1.2" xref="S2.SS2.p5.3.m3.1.1.2.cmml"><mi id="S2.SS2.p5.3.m3.1.1.2.2" xref="S2.SS2.p5.3.m3.1.1.2.2.cmml">Î²</mi><mn id="S2.SS2.p5.3.m3.1.1.2.3" xref="S2.SS2.p5.3.m3.1.1.2.3.cmml">2</mn></msub><mo id="S2.SS2.p5.3.m3.1.1.1" xref="S2.SS2.p5.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.p5.3.m3.1.1.3" xref="S2.SS2.p5.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><apply id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1"><eq id="S2.SS2.p5.3.m3.1.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1.1"></eq><apply id="S2.SS2.p5.3.m3.1.1.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.3.m3.1.1.2.1.cmml" xref="S2.SS2.p5.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.3.m3.1.1.2.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2.2">ğ›½</ci><cn type="integer" id="S2.SS2.p5.3.m3.1.1.2.3.cmml" xref="S2.SS2.p5.3.m3.1.1.2.3">2</cn></apply><cn type="integer" id="S2.SS2.p5.3.m3.1.1.3.cmml" xref="S2.SS2.p5.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">\beta_{2}=2</annotation></semantics></math> as does a scene with a sphere and a 3-holed torus. This made per-object genus labeling for the Repulse dataset a more favourable descriptor. It should be noted that the ability to deal with this ambiguity associated with the Betti numbers is an advantage of our proposed approach using neural networks over traditional persistent homology methods.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">For more detailed information on the algebraic topology and statistical concepts of persistent homology, seeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A survey and evaluation of common methods for the computation of persistent homology can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As previously stated, our dataset was created using a combination of the Repulsive Surfaces algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and the Wave Function Collapse algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The Repulsive Surfaces algorithm uses repulsive tangent point energies of mesh data to perform collision avoiding optimizations on 3D objects. This optimization attempts to simplify homeomorphic deformations to represent the object in a topologically equivalent form. The strength of this algorithm for our application in topological data generation is that it provides a barrier to intersections, which prevents the genus from changing throughout the surface deformation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Instead of the typical use in reducing homeomorphic deformation we used the algorithm to <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">induce</em> deformation by increasing the objectsâ€™ surface area within random environmental constraints. Some basic seed objects for <math id="S3.SS1.p3.1.m1.4" class="ltx_Math" alttext="g=0,1,2,3" display="inline"><semantics id="S3.SS1.p3.1.m1.4a"><mrow id="S3.SS1.p3.1.m1.4.5" xref="S3.SS1.p3.1.m1.4.5.cmml"><mi id="S3.SS1.p3.1.m1.4.5.2" xref="S3.SS1.p3.1.m1.4.5.2.cmml">g</mi><mo id="S3.SS1.p3.1.m1.4.5.1" xref="S3.SS1.p3.1.m1.4.5.1.cmml">=</mo><mrow id="S3.SS1.p3.1.m1.4.5.3.2" xref="S3.SS1.p3.1.m1.4.5.3.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">0</mn><mo id="S3.SS1.p3.1.m1.4.5.3.2.1" xref="S3.SS1.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p3.1.m1.2.2" xref="S3.SS1.p3.1.m1.2.2.cmml">1</mn><mo id="S3.SS1.p3.1.m1.4.5.3.2.2" xref="S3.SS1.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p3.1.m1.3.3" xref="S3.SS1.p3.1.m1.3.3.cmml">2</mn><mo id="S3.SS1.p3.1.m1.4.5.3.2.3" xref="S3.SS1.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p3.1.m1.4.4" xref="S3.SS1.p3.1.m1.4.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.4b"><apply id="S3.SS1.p3.1.m1.4.5.cmml" xref="S3.SS1.p3.1.m1.4.5"><eq id="S3.SS1.p3.1.m1.4.5.1.cmml" xref="S3.SS1.p3.1.m1.4.5.1"></eq><ci id="S3.SS1.p3.1.m1.4.5.2.cmml" xref="S3.SS1.p3.1.m1.4.5.2">ğ‘”</ci><list id="S3.SS1.p3.1.m1.4.5.3.1.cmml" xref="S3.SS1.p3.1.m1.4.5.3.2"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">0</cn><cn type="integer" id="S3.SS1.p3.1.m1.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2">1</cn><cn type="integer" id="S3.SS1.p3.1.m1.3.3.cmml" xref="S3.SS1.p3.1.m1.3.3">2</cn><cn type="integer" id="S3.SS1.p3.1.m1.4.4.cmml" xref="S3.SS1.p3.1.m1.4.4">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.4c">g=0,1,2,3</annotation></semantics></math> are displayed in FigÂ <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The wave function collapse algorithm allows for random arrangement of rule-based tiles. This was used to create 3D environments with random shape for the objects to grow in.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/2309.16968/assets/x5.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_square" width="168" height="152" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An environment generated using the wave function collapse algorithm.</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Each 3D tile comprised of 3x3 cells with a boolean value indicating cell presence. The 3x3 cells could form a variety of shapes such as straight lines, 90 degree turns, and intersections that when assembled produced a 3D maze-like structure that acted as a restrictive barrier. An example randomly generated environment is shown in FigÂ <a href="#S3.F4" title="Figure 4 â€£ III-A Dataset â€£ III Method â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Using this method the seed objects could be randomly grown through a series of algorithmic iterations providing unique deformations. See FigÂ <a href="#S1.F3" title="Figure 3 â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for an example of dataset growth. This iterative growth process allowed sampling at various stages to produce a dataset with varied complexity. Growing of seed objects within a confined space is conceptually similar to vines or plants growing around a wooden lattice where weaving and wrapping creates unique shapes.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Certain seed objects started linked which was preserved throughout the growth process (see FigÂ <a href="#S3.F5" title="Figure 5 â€£ III-A Dataset â€£ III Method â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). This was an important dataset feature to add complexity as the labeling of objects should remain unaffected by these links.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<table id="S3.F5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F5.3.3" class="ltx_tr">
<td id="S3.F5.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/images/hd_zoomed/bnw.png" id="S3.F5.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="110" height="157" alt="Refer to caption"></td>
<td id="S3.F5.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/2309.16968/assets/images/hd_zoomed/coloured.png" id="S3.F5.2.2.2.g1" class="ltx_graphics ltx_img_portrait" width="110" height="157" alt="Refer to caption"></td>
<td id="S3.F5.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/images/hd_zoomed/zoomed.png" id="S3.F5.3.3.3.g1" class="ltx_graphics ltx_img_portrait" width="110" height="157" alt="Refer to caption"></td>
</tr>
<tr id="S3.F5.3.4.1" class="ltx_tr">
<td id="S3.F5.3.4.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S3.F5.3.4.1.2" class="ltx_td ltx_nopad_l ltx_align_center">(b)</td>
<td id="S3.F5.3.4.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">(c)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An example of a synthetic repulsive data sample comprising of 3 separate objects. Image (a) shows the generated output mesh. Image (b) shows the mesh coloured with genus 0 (green), genus 2 (red) and genus 3 (blue). Image (c) is a closeup of (b) to assist with link identification. Visualisation of data was performed in Blender 3.0.1. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></figcaption>
</figure>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">The dataset consists of 5725 training samples, 1610 validation samples and 965 test samples. Each sample could contain 1-3 unique objects with each object having a genus of 0-3. To reduce training bias there is equal class representation for the amount of objects in the scene and equal quantities of each genus object.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<table id="S3.F6.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F6.2.2" class="ltx_tr">
<td id="S3.F6.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/x6.png" id="S3.F6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
<td id="S3.F6.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/x7.png" id="S3.F6.2.2.2.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
</tr>
<tr id="S3.F6.2.3.1" class="ltx_tr">
<td id="S3.F6.2.3.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S3.F6.2.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An example of a synthetic repulsive data sample comprising of 3 separate objects sampled at 4096 point count. Image (a) shows the point cloud input information and Image (b) visualises the target genus annotations via colour with genus 0 (red), genus 1 (green) and genus 2 (blue). </figcaption>
</figure>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">While the dataset was generated as a mesh, this was less common for real-world application data and offered additional normal and boundary information that could provide an undesirable advantage. To counter this the training, validation and evaluation was performed on a point cloud variant. The generated meshes were sampled uniformly with a point count of 4096. Each point was labeled with the parent object genus for segmentation. An example sampled at 4096 with labels can be seen in FigÂ <a href="#S3.F6" title="Figure 6 â€£ III-A Dataset â€£ III Method â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Each network was trained on the complete training dataset for 100 epochs. The validation dataset was evaluated at the end of each epoch for best model checkpointing. The test set was used for unbiased evaluation on the best validation model after training had finished.
The initial learning rate was 0.001 which was reduced by a factor of 10 every 25 epochs with a batch size of 64.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In order to enhance the size of the dataset and mitigate overfitting, various data augmentation techniques were applied to the training set. These techniques include:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mn id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">50</mn><mo id="S3.I1.i1.p1.1.m1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">50\%</annotation></semantics></math> chance to mirror each of the 3 axis.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Full <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="2\pi" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mn id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">Ï€</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><times id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">2</cn><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">ğœ‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">2\pi</annotation></semantics></math> rotation around each axis.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Anisotropic scale deformation of each axis in the range <math id="S3.I1.i3.p1.1.m1.2" class="ltx_Math" alttext="\left\{0.5,1.5\right\}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.2a"><mrow id="S3.I1.i3.p1.1.m1.2.3.2" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml"><mo id="S3.I1.i3.p1.1.m1.2.3.2.1" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml">{</mo><mn id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">0.5</mn><mo id="S3.I1.i3.p1.1.m1.2.3.2.2" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.I1.i3.p1.1.m1.2.2" xref="S3.I1.i3.p1.1.m1.2.2.cmml">1.5</mn><mo id="S3.I1.i3.p1.1.m1.2.3.2.3" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><set id="S3.I1.i3.p1.1.m1.2.3.1.cmml" xref="S3.I1.i3.p1.1.m1.2.3.2"><cn type="float" id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">0.5</cn><cn type="float" id="S3.I1.i3.p1.1.m1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2">1.5</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">\left\{0.5,1.5\right\}</annotation></semantics></math>, uniformly distributed.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Shift of <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="\pm 25" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mo id="S3.I1.i4.p1.1.m1.1.1a" xref="S3.I1.i4.p1.1.m1.1.1.cmml">Â±</mo><mn id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\pm 25</annotation></semantics></math> in each axis independently, uniformly distributed.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Gaussian noise jitter applied to each points position with a standard deviation of 0.025.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">These augmentations were applied globally to all of the points within a given scene to prevent class-altering deformations.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">A common problem in machine learning is knowing whether networks are actually learning to extract desired features from a dataset or are instead learning to estimate the output based on other contextual clues or artifacts.
With this in mind, it was important to create a dataset where unique, random homeomorphic deformations could occur to attempt to mitigate these artificial features. This could aid in evaluating neural networksâ€™ true ability to assess topology; particularly addressing the question of whether the labels were extrapolated via artifacts from a succession of similar objects existing in both the train and test sets.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Genus segmentation also poses some unique challenges in that objects with the same topological features can take infinite forms in Euclidean space. This prevents the network from learning global shape alone and requires a deep understanding of the relationship between points.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experiments</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Training and evaluation on the â€˜Repulseâ€™ dataset was conducted with 4096 points per scene.
Three networks for semantic segmentation were used: PointNet++, Point Transformer and RepSurf-U.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Summary metrics of neural network models</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAcc</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">OA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Point Transformer</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">68.80</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">81.18</td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">80.99</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center">RepSurf-U</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center">64.35</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center">77.77</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center">78.03</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">PointNet++</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">57.67</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">72.88</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">72.61</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Class accuracy evaluated on Repulse dataset</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Genus</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IoU</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Acc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S4.T4.1.2.1.1.1" class="ltx_text">Point Transformer</span></th>
<th id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0</th>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">78.80</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">87.89</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center">70.12</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center">81.82</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center">56.25</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center">72.87</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center">70.03</td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center">82.13</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<th id="S4.T4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S4.T4.1.6.5.1.1" class="ltx_text">RepSurf-U</span></th>
<th id="S4.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0</th>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">75.54</td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">83.88</td>
</tr>
<tr id="S4.T4.1.7.6" class="ltx_tr">
<th id="S4.T4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T4.1.7.6.2" class="ltx_td ltx_align_center">58.85</td>
<td id="S4.T4.1.7.6.3" class="ltx_td ltx_align_center">72.89</td>
</tr>
<tr id="S4.T4.1.8.7" class="ltx_tr">
<th id="S4.T4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S4.T4.1.8.7.2" class="ltx_td ltx_align_center">53.97</td>
<td id="S4.T4.1.8.7.3" class="ltx_td ltx_align_center">73.51</td>
</tr>
<tr id="S4.T4.1.9.8" class="ltx_tr">
<th id="S4.T4.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S4.T4.1.9.8.2" class="ltx_td ltx_align_center">69.84</td>
<td id="S4.T4.1.9.8.3" class="ltx_td ltx_align_center">80.80</td>
</tr>
<tr id="S4.T4.1.10.9" class="ltx_tr">
<th id="S4.T4.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T4.1.10.9.1.1" class="ltx_text">PointNet++</span></th>
<th id="S4.T4.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0</th>
<td id="S4.T4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t">67.52</td>
<td id="S4.T4.1.10.9.4" class="ltx_td ltx_align_center ltx_border_t">77.73</td>
</tr>
<tr id="S4.T4.1.11.10" class="ltx_tr">
<th id="S4.T4.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<td id="S4.T4.1.11.10.2" class="ltx_td ltx_align_center">57.07</td>
<td id="S4.T4.1.11.10.3" class="ltx_td ltx_align_center">76.33</td>
</tr>
<tr id="S4.T4.1.12.11" class="ltx_tr">
<th id="S4.T4.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S4.T4.1.12.11.2" class="ltx_td ltx_align_center">45.00</td>
<td id="S4.T4.1.12.11.3" class="ltx_td ltx_align_center">63.51</td>
</tr>
<tr id="S4.T4.1.13.12" class="ltx_tr">
<th id="S4.T4.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">3</th>
<td id="S4.T4.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">61.11</td>
<td id="S4.T4.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">73.94</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Discussion</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The obtained results demonstrated mean IoU and mean accuracy comparable to those achieved on datasets such as S3DIS (see Tables <a href="#S1.T1" title="TABLE I â€£ I-B Approach, Data, and Networks â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and <a href="#S4.T3" title="TABLE III â€£ IV-A Experiments â€£ IV Results and Discussion â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) which is designed for object segmentation in building interior spaces. Interestingly, the overall accuracy metric on S3DIS was higher than on the Repulse set. It is speculated that the complexity of the dataset and unique challenges posed by the nature of the features may contribute to this difference. A tighter grouping between mIoU, mAcc, and OA which was seen in the Repulse experiments can also indicate a closer accuracy grouping between classes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">An observation that the accuracies vastly exceeded random distribution when applied to data with homeomorphic deformation is supportive for neural network TDA. This suggests that these networks are capable of assessing the connectivity and structure of the samples rather than purely assessing the local/global geometric features like the surface shape of known object classes.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Such concepts are challenging to evaluate from existing studies as there were often correlations between object classification and topological features, see SectionÂ <a href="#S1.SS1" title="I-A Related Works â€£ I Introduction â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">I-A</span></span></a>. For example samples of varying coffee cups appearing in both training and evaluation datasets. This can make it unclear whether the networks encode topological labels into different object classifications, adjust learned persistence diagrams or images in response to changes in object shape, or perform a desirable topological analysis.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.3" class="ltx_p">Of the three networks trained Point Transformer achieved the highest accuracy which showed that transformer architectures with self-attention may be suitable for this topological segmentation task. Self-attention adjusts point significance with respect to a centroid point belonging to a local neighbour group. This may prioritise certain manifold properties such as curvature and connectivity, aiding in the topological analysis and increasing accuracy. Segmented output examples from this network are provided in FigÂ <a href="#S4.F7" title="Figure 7 â€£ IV-B Discussion â€£ IV Results and Discussion â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. It is notable that the output in row 3 demonstrated a â€˜cleanâ€™ miss-classification in which the objects were successfully distinguished despite an incorrect hole count for the genus 2 object. A class IoU is determined by both the amount of points correctly labelled and the amount incorrectly labelled; <math id="S4.SS2.p4.1.m1.4" class="ltx_Math" alttext="IoU(A,B)=\frac{|A\cap B|}{|A\cup B|}" display="inline"><semantics id="S4.SS2.p4.1.m1.4a"><mrow id="S4.SS2.p4.1.m1.4.5" xref="S4.SS2.p4.1.m1.4.5.cmml"><mrow id="S4.SS2.p4.1.m1.4.5.2" xref="S4.SS2.p4.1.m1.4.5.2.cmml"><mi id="S4.SS2.p4.1.m1.4.5.2.2" xref="S4.SS2.p4.1.m1.4.5.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.2.1" xref="S4.SS2.p4.1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S4.SS2.p4.1.m1.4.5.2.3" xref="S4.SS2.p4.1.m1.4.5.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.2.1a" xref="S4.SS2.p4.1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S4.SS2.p4.1.m1.4.5.2.4" xref="S4.SS2.p4.1.m1.4.5.2.4.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.4.5.2.1b" xref="S4.SS2.p4.1.m1.4.5.2.1.cmml">â€‹</mo><mrow id="S4.SS2.p4.1.m1.4.5.2.5.2" xref="S4.SS2.p4.1.m1.4.5.2.5.1.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.5.2.1" xref="S4.SS2.p4.1.m1.4.5.2.5.1.cmml">(</mo><mi id="S4.SS2.p4.1.m1.3.3" xref="S4.SS2.p4.1.m1.3.3.cmml">A</mi><mo id="S4.SS2.p4.1.m1.4.5.2.5.2.2" xref="S4.SS2.p4.1.m1.4.5.2.5.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.4.4" xref="S4.SS2.p4.1.m1.4.4.cmml">B</mi><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.5.2.3" xref="S4.SS2.p4.1.m1.4.5.2.5.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.1.m1.4.5.1" xref="S4.SS2.p4.1.m1.4.5.1.cmml">=</mo><mfrac id="S4.SS2.p4.1.m1.2.2" xref="S4.SS2.p4.1.m1.2.2.cmml"><mrow id="S4.SS2.p4.1.m1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.1.1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS2.p4.1.m1.1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml">A</mi><mo id="S4.SS2.p4.1.m1.1.1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.cmml">âˆ©</mo><mi id="S4.SS2.p4.1.m1.1.1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.1.1.3.cmml">B</mi></mrow><mo stretchy="false" id="S4.SS2.p4.1.m1.1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS2.p4.1.m1.2.2.2.1" xref="S4.SS2.p4.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.2.2.2.1.2" xref="S4.SS2.p4.1.m1.2.2.2.2.1.cmml">|</mo><mrow id="S4.SS2.p4.1.m1.2.2.2.1.1" xref="S4.SS2.p4.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS2.p4.1.m1.2.2.2.1.1.2" xref="S4.SS2.p4.1.m1.2.2.2.1.1.2.cmml">A</mi><mo id="S4.SS2.p4.1.m1.2.2.2.1.1.1" xref="S4.SS2.p4.1.m1.2.2.2.1.1.1.cmml">âˆª</mo><mi id="S4.SS2.p4.1.m1.2.2.2.1.1.3" xref="S4.SS2.p4.1.m1.2.2.2.1.1.3.cmml">B</mi></mrow><mo stretchy="false" id="S4.SS2.p4.1.m1.2.2.2.1.3" xref="S4.SS2.p4.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.4b"><apply id="S4.SS2.p4.1.m1.4.5.cmml" xref="S4.SS2.p4.1.m1.4.5"><eq id="S4.SS2.p4.1.m1.4.5.1.cmml" xref="S4.SS2.p4.1.m1.4.5.1"></eq><apply id="S4.SS2.p4.1.m1.4.5.2.cmml" xref="S4.SS2.p4.1.m1.4.5.2"><times id="S4.SS2.p4.1.m1.4.5.2.1.cmml" xref="S4.SS2.p4.1.m1.4.5.2.1"></times><ci id="S4.SS2.p4.1.m1.4.5.2.2.cmml" xref="S4.SS2.p4.1.m1.4.5.2.2">ğ¼</ci><ci id="S4.SS2.p4.1.m1.4.5.2.3.cmml" xref="S4.SS2.p4.1.m1.4.5.2.3">ğ‘œ</ci><ci id="S4.SS2.p4.1.m1.4.5.2.4.cmml" xref="S4.SS2.p4.1.m1.4.5.2.4">ğ‘ˆ</ci><interval closure="open" id="S4.SS2.p4.1.m1.4.5.2.5.1.cmml" xref="S4.SS2.p4.1.m1.4.5.2.5.2"><ci id="S4.SS2.p4.1.m1.3.3.cmml" xref="S4.SS2.p4.1.m1.3.3">ğ´</ci><ci id="S4.SS2.p4.1.m1.4.4.cmml" xref="S4.SS2.p4.1.m1.4.4">ğµ</ci></interval></apply><apply id="S4.SS2.p4.1.m1.2.2.cmml" xref="S4.SS2.p4.1.m1.2.2"><divide id="S4.SS2.p4.1.m1.2.2.3.cmml" xref="S4.SS2.p4.1.m1.2.2"></divide><apply id="S4.SS2.p4.1.m1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1"><abs id="S4.SS2.p4.1.m1.1.1.1.2.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.2"></abs><apply id="S4.SS2.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1"><intersect id="S4.SS2.p4.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1"></intersect><ci id="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.2">ğ´</ci><ci id="S4.SS2.p4.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.3">ğµ</ci></apply></apply><apply id="S4.SS2.p4.1.m1.2.2.2.2.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1"><abs id="S4.SS2.p4.1.m1.2.2.2.2.1.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1.2"></abs><apply id="S4.SS2.p4.1.m1.2.2.2.1.1.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1.1"><union id="S4.SS2.p4.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1.1.1"></union><ci id="S4.SS2.p4.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1.1.2">ğ´</ci><ci id="S4.SS2.p4.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS2.p4.1.m1.2.2.2.1.1.3">ğµ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.4c">IoU(A,B)=\frac{|A\cap B|}{|A\cup B|}</annotation></semantics></math>, where <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mi id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">A</annotation></semantics></math> and <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mi id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><ci id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">B</annotation></semantics></math> in this context represent correct and total classification sets. For this specific example, the lack of â€˜bleedingâ€™ between close proximity objects offers an IoU of 100% for genus 1, and 0% for genus 2 resulting in a theoretical mIoU of 50%.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">RepSurf-U achieved similar accuracy however comprised of significantly less parameters than the Point Transformer network (0.976M vs 7.767M) which may be desirable for certain applications and hardware.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">The discrepancy in accuracy between PointNet++ and RepSurf-U seems to indicate that utilizing the umbrella curvature offered an advantage in topological data analysis as RepSurf-U utilizes PointNet++ as a backbone.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">No explicit conclusion can be drawn from these preliminary experiments with respect to optimal architecture as there are many variants of MLPs and transformers as well as different training methods. Additionally, further adjustments to the hyper-parameters may show improvement to tune to this new topological task. It does however offer some insight into the construction of future TDA networks.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.F7.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F7.2.2" class="ltx_tr">
<td id="S4.F7.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/x8.png" id="S4.F7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
<td id="S4.F7.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/x9.png" id="S4.F7.2.2.2.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.F7.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F7.4.2" class="ltx_tr">
<td id="S4.F7.3.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/x10.png" id="S4.F7.3.1.1.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
<td id="S4.F7.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/x11.png" id="S4.F7.4.2.2.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.F7.6" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F7.6.2" class="ltx_tr">
<td id="S4.F7.5.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/x12.png" id="S4.F7.5.1.1.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
<td id="S4.F7.6.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/x13.png" id="S4.F7.6.2.2.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.F7.8" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F7.8.2" class="ltx_tr">
<td id="S4.F7.7.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.16968/assets/x14.png" id="S4.F7.7.1.1.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
<td id="S4.F7.8.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/2309.16968/assets/x15.png" id="S4.F7.8.2.2.g1" class="ltx_graphics ltx_img_square" width="174" height="174" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Semantic segmentation output annotations for 4096 Repulse dataset. The left column shows the target annotations and the right column visualises the Point Transformer network output. Classes are coloured with genus 0 (red), genusÂ 1 (green), genus 2 (blue) and genus 3 (yellow). </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Overall the pilot results of this study demonstrate that the concept of semantic segmentation for topological data analysis appears feasible. Further studies can be conducted into optimizing network architectures and data generation approaches to increase accuracy and applicability.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The proposed approach is computationally efficient once the neural networks are trained and offers an advantage over persistent homology because it estimates the genus of each individual object in a scene and hence resolves ambiguities that are inherent to the Betti number evaluations of persistent homology as was pointed out in SectionÂ <a href="#S2.SS2" title="II-B Topology â€£ II Background â€£ Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data This research was supported by the Australian Government through the ARCâ€™s Discovery Projects funding scheme (project DP210103304). The first author was supported by a Research Training Program (RTP) Scholarship â€“ Fee Offset by the Commonwealth Government. Copyright 2023 IEEE. Published in the Digital Image Computing: Techniques and Applications, 2023 (DICTA 2023), 28 November â€“ 1 December 2023 in Port Macquarie, NSW, Australia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The new topological 3D image dataset was a crucial component of model training. It had a mix of genus and objects with unique shape and curvature which is a higher level of complexity and closer representation of real-world data than in previous studies.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Limitations of this study include a finite object and genus count. The â€˜Repulseâ€™ dataset had 1-3 objects with a genus of 0-3. In future studies this dataset could be greatly expanded to have larger signature variety. Additionally, this data could be further enhanced with additional visual variety by different environmental generation techniques and realistic post-processing that preserves the underlying structures.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Limitations also include a finite quantity of networks tested. Training more networks with greater variance in augmentation, training scheme, point counts and noise would allow further insight to be drawn regarding the effects of architecture and hyper-parameters on accuracy.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Future research could also address detailed benchmarking and comparisons of persistent homology techniques and deep learning-based approaches on different image data and data formats. Both approaches display a range of advantages and disadvantages and the present study could only discuss some aspects of this emerging field of research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H.Â Adams, T.Â Emerson, M.Â Kirby, R.Â Neville, C.Â Peterson, P.Â Shipman, S.Â Chepushtanova, E.Â Hanson, F.Â Motta, and L.Â Ziegelmeier, â€œPersistence images: A stable vector representation of persistent homology,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, vol.Â 18, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
I.Â Armeni, O.Â Sener, A.Â R. Zamir, H.Â Jiang, I.Â Brilakis, M.Â Fischer, and S.Â Savarese, â€œ3d semantic parsing of large-scale indoor spaces,â€ in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2016, pp. 1534â€“1543.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B.Â D.Â T. BlenderÂ Foundation, â€œBlender 3.0.1,â€ 2021, software. [Online]. Available: https://www.blender.org

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P.Â Bubenik and P.Â T. Kim, â€œA statistical approach to persistent homology,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Homology, Homotopy and Applications</em>, vol.Â 9, no.Â 2, pp. 337â€“362, 2007.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G.Â Carlsson and M.Â Vejdemo-Johansson, <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Topological Data Analysis with Applications</em>.Â Â Â Cambridge University Press, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R.Â Q. Charles, H.Â Su, M.Â Kaichun, and L.Â J. Guibas, â€œPointnet: Deep learning on point sets for 3d classification and segmentation,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.-M. Chung, C.-S. Hu, Y.-L. Lo, and H.-T. Wu, â€œA persistent homology approach to heart rate variability analysis with an application to sleep-wake classification,â€ <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Frontiers in Physiology</em>, vol.Â 12, p. 637684, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T.Â deÂ Surrel, F.Â Hensel, M.Â CarriÃ¨re, T.Â Lacombe, Y.Â Ike, H.Â Kurihara, M.Â Glisse, and F.Â Chazal, â€œRipsnet: a general architecture for fast and robust estimation of the persistent homology of point clouds,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Topological, Algebraic and Geometric Learning Workshops 2022</em>.Â Â Â PMLR, 2022, pp. 96â€“106.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H.Â Edelsbrunner and J.Â Harer, <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Computational Topology: An Introduction</em>, ser. Applied Mathematics.Â Â Â American Mathematical Society, 2010. [Online]. Available: https://books.google.com.au/books?id=MDXa6gFRZuIC

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.Â Foorginejad and K.Â Khalili, â€œUmbrella curvature: A new curvature estimation method for point clouds,â€ <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Procedia Technology</em>, vol.Â 12, pp. 347â€“352, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.Â H. Gallier and D.Â Xu, <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">A Guide to the Classification Theorem for Compact Surfaces</em>.Â Â Â Berlin, Heidelberg: Springer, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M.Â Gumin, â€œWavefunctioncollapse,â€ <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">GitHub repository</em>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y.Â Guo, H.Â Wang, Q.Â Hu, H.Â Liu, L.Â Liu, and M.Â Bennamoun, â€œDeep learning for 3d point clouds: A survey,â€ <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol.Â 43, no.Â 12, pp. 4338â€“4364, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.Â Hajij, G.Â Zamzmi, and F.Â Batayneh, â€œTda-net: Fusion of persistent homology and feep learning features for covid-19 detection from chest x-ray images,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</em>.Â Â Â IEEE, 2021, pp. 4115â€“4119.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G.Â Hamilton, T.Â Dore, and C.Â Plumberg, â€œApplications of persistent homology in nuclear collisions,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Physical Review C</em>, vol. 106, no.Â 6, p. 064912, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K.Â M. Hannouch and S.Â Chalup, â€œLearning to see topological properties in 4d using convolutional neural networks,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2nd Annual Topology, Algebra, and Geometry in Machine Learning Workshop at ICML. Proceedings of Machine Learning Research (PMLR)</em>, 2023, accepted June 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Hatcher, <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Algebraic Topology</em>.Â Â Â Cambridge, UK: Cambridge University Press, 2002.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C.Â Hofer, R.Â Kwitt, M.Â Niethammer, and A.Â Uhl, â€œDeep learning with topological signatures,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A.Â Krizhevsky and G.Â Hinton, â€œLearning multiple layers of features from tiny images,â€ University of Toronto, Toronto, Ontario, Tech. Rep.Â 0, 2009.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.Â Z. Li, M.Â S. Ryerson, and H.Â Balakrishnan, â€œTopological data analysis for aviation applications,â€ <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Transportation Research Part E: Logistics and Transportation Review</em>, vol. 128, pp. 149â€“174, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
N.Â O. Malott, S.Â Chen, and P.Â A. Wilsey, â€œA survey on the high-performance computation of persistent homology,â€ <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, vol.Â 35, no.Â 5, pp. 4466â€“4484, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
G.Â MontÃºfar, N.Â Otter, and Y.Â Wang, â€œCan neural networks learn persistent homology features?â€ <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.14688</em>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y.Â Netzer, T.Â Wang, A.Â Coates, A.Â Bissacco, B.Â Wu, and A.Â Y. Ng, â€œReading digits in natural images with unsupervised feature iearning,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</em>, 2011.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N.Â Otter, M.Â A. Porter, U.Â Tillmann, P.Â Grindrod, and H.Â A. Harrington, â€œA roadmap for the computation of persistent homology,â€ <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">EPJ Data Science</em>, vol.Â 6, pp. 1â€“38, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
R.Â Paul and S.Â Chalup, â€œEstimating betti numbers using deep learning,â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks (IJCNN)</em>.Â Â Â IEEE, 2019, pp. 1â€“7.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C.Â R. Qi, L.Â Yi, H.Â Su, and L.Â J. Guibas, â€œPointnet++: Deep hierarchical feature learning on point sets in a metric space,â€ <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H.Â Ran, â€œRepSurf: Surface Representation for Point Clouds,â€ https://github.com/hancyran/RepSurf, 2022, [CVPR 2022 Oral] Official implementation for â€Surface Representation for Point Cloudsâ€.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H.Â Ran, J.Â Liu, and C.Â Wang, â€œSurface representation for point clouds,â€ in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 18â€‰942â€“18â€‰952.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S.Â Rostami, W.Â Saad, and C.Â S. Hong, â€œDeep learning with persistent homology for orbital angular momentum (oam) decoding,â€ <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Letters</em>, vol.Â 24, no.Â 1, pp. 117â€“121, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M.Â Rucco, L.Â Falsetti, D.Â Herman, T.Â Petrossian, E.Â Merelli, C.Â Nitti, and A.Â Salvi, â€œUsing topological data analysis for diagnosis pulmonary embolism,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.5020</em>, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A.Â D. Smith, P.Â DÅ‚otko, and V.Â M. Zavala, â€œTopological data analysis: Concepts, computation, and applications in chemical engineering,â€ <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Chemical Engineering</em>, vol. 146, p. 107202, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A.Â Som, H.Â Choi, K.Â N. Ramamurthy, M.Â P. Buman, and P.Â Turaga, â€œPi-net: A deep learning approach to extract topological persistence images,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, 2020, pp. 834â€“835.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R.Â TurkeÅ¡, J.Â Nys, T.Â Verdonck, and S.Â LatrÃ©, â€œNoise robustness of persistent homology on greyscale images, across filtrations and signatures,â€ <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Plos one</em>, vol.Â 16, no.Â 9, p. e0257215, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, Å.Â Kaiser, and I.Â Polosukhin, â€œAttention is all you need,â€ <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z.Â Wu, S.Â Song, A.Â Khosla, F.Â Yu, L.Â Zhang, X.Â Tang, and J.Â Xiao, â€œ3d shapenets: A deep representation for volumetric shapes,â€ in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2015, pp. 1912â€“1920.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
X.Â Xu, N.Â Drougard, and R.Â N. Roy, â€œTopological data analysis as a new tool for eeg processing,â€ <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol.Â 15, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T.Â Yamanashi, M.Â Kajitani, M.Â Iwata, K.Â J. Crutchley, P.Â Marra, J.Â R. Malicoat, J.Â C. Williams, L.Â R. Leyden, H.Â Long, D.Â Lo <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œTopological data analysis (tda) enhances bispectral eeg (bseeg) algorithm for detection of delirium,â€ <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">Scientific Reports</em>, vol.Â 11, no.Â 1, pp. 1â€“9, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
C.Â Yu, C.Â Brakensiek, H.Â Schumacher, and K.Â Crane, â€œRepulsive surfaces,â€ <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.01664</em>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H.Â Zhao, L.Â Jiang, J.Â Jia, P.Â H. Torr, and V.Â Koltun, â€œPoint transformer,â€ in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 16â€‰259â€“16â€‰268.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
C.Â Zhou, Z.Â Dong, and H.Â Lin, â€œLearning persistent homology of 3d point clouds,â€ <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Graphics</em>, vol. 102, pp. 269â€“279, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.16967" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.16968" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.16968">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.16968" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.16969" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 03:57:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
