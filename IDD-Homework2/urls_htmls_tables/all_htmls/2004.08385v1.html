<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.08385] Knowledge-Based Visual Question Answering in Videos</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Knowledge-Based Visual Question Answering in Videos">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Knowledge-Based Visual Question Answering in Videos">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.08385">

<!--Generated on Thu Mar  7 08:27:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Knowledge-Based Visual Question Answering in Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noa Garcia
<br class="ltx_break">Osaka University
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mayu Otani
<br class="ltx_break">CyberAgent, Inc.
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenhui Chu
<br class="ltx_break">Osaka University
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuta Nakashima
<br class="ltx_break">Osaka University
<br class="ltx_break">
</span></span>
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In standard visual question answering (VQA), answers are usually inferred by extracting the visual content of the images and generalising the information seen at training time. However, as the space of training question-image pairs is finite, the use of image content as the only source of information to predict answers presents two important limitations. First, image features only capture the spatial information of the picture, leaving temporal coherence in video unattended. Second, visual content by itself does not provide enough insights for answering questions that require reasoning beyond the image. To address these limitations, video question answering (VideoQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and knowledge-based visual question answering (KBVQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have emerged as two independent fields.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This work contributes towards building a general framework in which different types of VQA coexist. To that end, we created the KnowIT VQA dataset with questions that requiere both video understanding and knowledge-based reasoning to be answered (example in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Knowledge-Based Visual Question Answering in Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). We use a popular sitcom as an ideal testbed for modelling knowledge-based questions about the world. We then cast the problem as a multi-choice challenge, and introduce a two-piece model that (i) acquires, processes, and maps specific knowledge into a continuous representation inferring the motivation behind each question, and (ii) fuses video and natural language content together with the acquired knowledge in a multi-modal fashion to predict the correct answer. The complete details about the dataset, model and exhaustive experimental results can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2004.08385/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="243" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>KnowIT VQA sample correctly predicted by our model.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2004.08385/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our proposed model.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>KnowIT VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The dataset is based on 207 episodes from the Big Bang Theory tv show. To generate questions, answers, and annotations we used Amazon Mechanical Turk.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.mturk.com</span></span></span> We required workers to have a high knowledge about the show and instructed them to generate questions that are answerable by people familiar with the show, whereas difficult for new spectators. For each clip, we provided the video and subtitles, along with the summaries of all the episodes. Workers were asked to annotate each clip with a question, its correct answer, and three wrong but relevant answers. We randomly split the episodes into training, validation, and test sets, so that questions and clips from the same episode were assigned to the same set. In total, we gathered 24,282 question and answers pairs over 12,087 video clips.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In order to approximate the knowledge viewers acquire by watching the series, we annotated each video clip with expert information. We asked workers to describe in a short sentence the knowledge that is required to answer the question correctly. For example, for the question <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Why did Leonard invite Penny to lunch?</span>, the explanation <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">Penny has just moved in</span> is key to respond the correct answer, <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">He wanted Penny to feel welcomed into the building</span>. We called this field <span id="S2.p2.1.4" class="ltx_text ltx_font_smallcaps">knowledge</span>. We also annotated each question with four possible questions types: visual-, textual-, temporal-, and knowledge-based.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ROCK Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose ROCK (Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Knowledge-Based Visual Question Answering in Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) a model for KBVQA in videos. ROCK consists of three different modules:</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p"><span id="S3.p2.3.1" class="ltx_text ltx_font_bold">1) Knowledge Base (KB)</span>: we build a specific KB, <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="K=\{w_{j}\}" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">K</mi><mo id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">=</mo><mrow id="S3.p2.1.m1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p2.1.m1.1.1.1.1.2" xref="S3.p2.1.m1.1.1.1.2.cmml">{</mo><msub id="S3.p2.1.m1.1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.1.1.1.2" xref="S3.p2.1.m1.1.1.1.1.1.2.cmml">w</mi><mi id="S3.p2.1.m1.1.1.1.1.1.3" xref="S3.p2.1.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.p2.1.m1.1.1.1.1.3" xref="S3.p2.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><eq id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2"></eq><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">𝐾</ci><set id="S3.p2.1.m1.1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.1.1"><apply id="S3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.1.1.1.2">𝑤</ci><ci id="S3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.1.1.1.3">𝑗</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">K=\{w_{j}\}</annotation></semantics></math>, with the <span id="S3.p2.3.2" class="ltx_text ltx_font_smallcaps">knowledge</span> field. Each instance, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="w_{j}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">w</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝑤</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">w_{j}</annotation></semantics></math> with <math id="S3.p2.3.m3.3" class="ltx_Math" alttext="j\in\{1,...,N\}" display="inline"><semantics id="S3.p2.3.m3.3a"><mrow id="S3.p2.3.m3.3.4" xref="S3.p2.3.m3.3.4.cmml"><mi mathsize="90%" id="S3.p2.3.m3.3.4.2" xref="S3.p2.3.m3.3.4.2.cmml">j</mi><mo mathsize="90%" id="S3.p2.3.m3.3.4.1" xref="S3.p2.3.m3.3.4.1.cmml">∈</mo><mrow id="S3.p2.3.m3.3.4.3.2" xref="S3.p2.3.m3.3.4.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p2.3.m3.3.4.3.2.1" xref="S3.p2.3.m3.3.4.3.1.cmml">{</mo><mn mathsize="90%" id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">1</mn><mo mathsize="90%" id="S3.p2.3.m3.3.4.3.2.2" xref="S3.p2.3.m3.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml">…</mi><mo mathsize="90%" id="S3.p2.3.m3.3.4.3.2.3" xref="S3.p2.3.m3.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="S3.p2.3.m3.3.3" xref="S3.p2.3.m3.3.3.cmml">N</mi><mo maxsize="90%" minsize="90%" id="S3.p2.3.m3.3.4.3.2.4" xref="S3.p2.3.m3.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.3b"><apply id="S3.p2.3.m3.3.4.cmml" xref="S3.p2.3.m3.3.4"><in id="S3.p2.3.m3.3.4.1.cmml" xref="S3.p2.3.m3.3.4.1"></in><ci id="S3.p2.3.m3.3.4.2.cmml" xref="S3.p2.3.m3.3.4.2">𝑗</ci><set id="S3.p2.3.m3.3.4.3.1.cmml" xref="S3.p2.3.m3.3.4.3.2"><cn type="integer" id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">1</cn><ci id="S3.p2.3.m3.2.2.cmml" xref="S3.p2.3.m3.2.2">…</ci><ci id="S3.p2.3.m3.3.3.cmml" xref="S3.p2.3.m3.3.3">𝑁</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.3c">j\in\{1,...,N\}</annotation></semantics></math>, is represented as a natural language sentence.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.6" class="ltx_p"><span id="S3.p3.6.1" class="ltx_text ltx_font_bold">2) Knowledge Retrieval</span>:
We retrieve relevant information to each question in the KB. We input the question, the candidate answers, and a knowledge instance into a BERT network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, namely BERT-scoring. BERT-scoring is trained to estimate a similarity score, <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="s_{ij}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">s</mi><mrow id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml"><mi id="S3.p3.1.m1.1.1.3.2" xref="S3.p3.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p3.1.m1.1.1.3.1" xref="S3.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.p3.1.m1.1.1.3.3" xref="S3.p3.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑠</ci><apply id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3"><times id="S3.p3.1.m1.1.1.3.1.cmml" xref="S3.p3.1.m1.1.1.3.1"></times><ci id="S3.p3.1.m1.1.1.3.2.cmml" xref="S3.p3.1.m1.1.1.3.2">𝑖</ci><ci id="S3.p3.1.m1.1.1.3.3.cmml" xref="S3.p3.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">s_{ij}</annotation></semantics></math>, between a question, <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">q</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑞</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">q_{i}</annotation></semantics></math>, and a knowledge instance <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="w_{j}" display="inline"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">w</mi><mi id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝑤</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">w_{j}</annotation></semantics></math>,using matching (<em id="S3.p3.6.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p3.6.3" class="ltx_text"></span> <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="i=j" display="inline"><semantics id="S3.p3.4.m4.1a"><mrow id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml">i</mi><mo id="S3.p3.4.m4.1.1.1" xref="S3.p3.4.m4.1.1.1.cmml">=</mo><mi id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><eq id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1.1"></eq><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">𝑖</ci><ci id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">i=j</annotation></semantics></math>) and non-matching (<em id="S3.p3.6.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p3.6.5" class="ltx_text"></span> <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="i\neq j" display="inline"><semantics id="S3.p3.5.m5.1a"><mrow id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml"><mi id="S3.p3.5.m5.1.1.2" xref="S3.p3.5.m5.1.1.2.cmml">i</mi><mo id="S3.p3.5.m5.1.1.1" xref="S3.p3.5.m5.1.1.1.cmml">≠</mo><mi id="S3.p3.5.m5.1.1.3" xref="S3.p3.5.m5.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1"><neq id="S3.p3.5.m5.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1"></neq><ci id="S3.p3.5.m5.1.1.2.cmml" xref="S3.p3.5.m5.1.1.2">𝑖</ci><ci id="S3.p3.5.m5.1.1.3.cmml" xref="S3.p3.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">i\neq j</annotation></semantics></math>) question-knowledge pairs and a binary cross-entropy loss. The top <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p3.6.m6.1a"><mi id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><ci id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">k</annotation></semantics></math> scoring knowledge instances for a given questions are retrieved.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">3) Video Reasoning</span>:
the visual and language content of the video clip is extracted and used to predict an answer.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_framed ltx_framed_underline">Visual Representation</span>: We apply four different techniques to describe the visual content of video frames: 1) <span id="S3.p5.1.2" class="ltx_text ltx_font_italic">Image</span>, obtained from concatenating frame Resnet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> features; 2) <span id="S3.p5.1.3" class="ltx_text ltx_font_italic">Concepts</span>, bag-of-words with the objects and their attributes detected with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>; 3) <span id="S3.p5.1.4" class="ltx_text ltx_font_italic">Facial</span>, list of main characters in the clip detected with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>; and 4) <span id="S3.p5.1.5" class="ltx_text ltx_font_italic">Captions</span>, sentences describing the visual content of the frames with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_framed ltx_framed_underline">Language Representation</span>: Textual data is processed using a fine-tuned BERT model, namely BERT-reasoning. We concatenate the captions, subtitles, question, a candidate answer, and the top <math id="S3.p6.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p6.1.m1.1a"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">k</annotation></semantics></math> retrieved knowledge instances and input it into the network. For each candidate answer, the output of the network is used as language representation.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_framed ltx_framed_underline">Answer Prediction</span>: The visual and the language representations are concatenated and fed into a linear layer to obtain a candidate answer score. The answer with the highest score is the predicted answer. The network is trained as a classification task with the multi-class cross-entropy loss.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy for different methods on KnowIt VQA dataset. <math id="S3.T1.3.m1.1" class="ltx_Math" alttext="\diamondsuit" display="inline"><semantics id="S3.T1.3.m1.1b"><mi mathvariant="normal" id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><ci id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">\diamondsuit</annotation></semantics></math> for parts of our model, <math id="S3.T1.4.m2.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.T1.4.m2.1b"><mi mathvariant="normal" id="S3.T1.4.m2.1.1" xref="S3.T1.4.m2.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S3.T1.4.m2.1c"><ci id="S3.T1.4.m2.1.1.cmml" xref="S3.T1.4.m2.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.m2.1d">\bigstar</annotation></semantics></math> for our full model.</figcaption>
<div id="S3.T1.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:165.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.6pt,25.4pt) scale(0.765244602674056,0.765244602674056) ;">
<table id="S3.T1.12.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.12.8.9.1" class="ltx_tr">
<th id="S3.T1.12.8.9.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.12.8.9.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.12.8.9.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S3.T1.12.8.9.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.12.8.9.1.3.1" class="ltx_text ltx_font_bold">Vis.</span></td>
<td id="S3.T1.12.8.9.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.12.8.9.1.4.1" class="ltx_text ltx_font_bold">Text.</span></td>
<td id="S3.T1.12.8.9.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.12.8.9.1.5.1" class="ltx_text ltx_font_bold">Temp.</span></td>
<td id="S3.T1.12.8.9.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.12.8.9.1.6.1" class="ltx_text ltx_font_bold">Know.</span></td>
<td id="S3.T1.12.8.9.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.12.8.9.1.7.1" class="ltx_text ltx_font_bold">All</span></td>
</tr>
<tr id="S3.T1.12.8.10.2" class="ltx_tr">
<th id="S3.T1.12.8.10.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="6">
<span id="S3.T1.12.8.10.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:3.0pt;">
<span id="S3.T1.12.8.10.2.1.1.1" class="ltx_p">
<span id="S3.T1.12.8.10.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:62.4pt;vertical-align:-28.7pt;"><span class="ltx_transformed_inner" style="width:62.3pt;transform:translate(-26.72pt,2.92pt) rotate(-90deg) ;">
<span id="S3.T1.12.8.10.2.1.1.1.1.1" class="ltx_p">Vis, Subs, QA</span>
</span></span></span>
</span>
</th>
<th id="S3.T1.12.8.10.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.12.8.10.2.2.1" class="ltx_text ltx_font_typewriter">TVQA</span></th>
<td id="S3.T1.12.8.10.2.3" class="ltx_td ltx_align_center ltx_border_t">0.612</td>
<td id="S3.T1.12.8.10.2.4" class="ltx_td ltx_align_center ltx_border_t">0.645</td>
<td id="S3.T1.12.8.10.2.5" class="ltx_td ltx_align_center ltx_border_t">0.547</td>
<td id="S3.T1.12.8.10.2.6" class="ltx_td ltx_align_center ltx_border_t">0.466</td>
<td id="S3.T1.12.8.10.2.7" class="ltx_td ltx_align_center ltx_border_t">0.522</td>
</tr>
<tr id="S3.T1.5.1.1" class="ltx_tr">
<th id="S3.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.5.1.1.1.m1.1" class="ltx_Math" alttext="\diamondsuit" display="inline"><semantics id="S3.T1.5.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T1.5.1.1.1.m1.1.1" xref="S3.T1.5.1.1.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="S3.T1.5.1.1.1.m1.1b"><ci id="S3.T1.5.1.1.1.m1.1.1.cmml" xref="S3.T1.5.1.1.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.1.1.1.m1.1c">\diamondsuit</annotation></semantics></math> <span id="S3.T1.5.1.1.1.1" class="ltx_text ltx_font_typewriter">ROCK<sub id="S3.T1.5.1.1.1.1.1" class="ltx_sub">VSQA</sub></span> <span id="S3.T1.5.1.1.1.2" class="ltx_text" style="font-size:70%;">Image</span>
</th>
<td id="S3.T1.5.1.1.2" class="ltx_td ltx_align_center">0.643</td>
<td id="S3.T1.5.1.1.3" class="ltx_td ltx_align_center">0.739</td>
<td id="S3.T1.5.1.1.4" class="ltx_td ltx_align_center">0.581</td>
<td id="S3.T1.5.1.1.5" class="ltx_td ltx_align_center">0.539</td>
<td id="S3.T1.5.1.1.6" class="ltx_td ltx_align_center">0.587</td>
</tr>
<tr id="S3.T1.6.2.2" class="ltx_tr">
<th id="S3.T1.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.6.2.2.1.m1.1" class="ltx_Math" alttext="\diamondsuit" display="inline"><semantics id="S3.T1.6.2.2.1.m1.1a"><mi mathvariant="normal" id="S3.T1.6.2.2.1.m1.1.1" xref="S3.T1.6.2.2.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="S3.T1.6.2.2.1.m1.1b"><ci id="S3.T1.6.2.2.1.m1.1.1.cmml" xref="S3.T1.6.2.2.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.2.2.1.m1.1c">\diamondsuit</annotation></semantics></math> <span id="S3.T1.6.2.2.1.1" class="ltx_text ltx_font_typewriter">ROCK<sub id="S3.T1.6.2.2.1.1.1" class="ltx_sub">VSQA</sub></span> <span id="S3.T1.6.2.2.1.2" class="ltx_text" style="font-size:70%;">Concepts</span>
</th>
<td id="S3.T1.6.2.2.2" class="ltx_td ltx_align_center">0.647</td>
<td id="S3.T1.6.2.2.3" class="ltx_td ltx_align_center">0.743</td>
<td id="S3.T1.6.2.2.4" class="ltx_td ltx_align_center">0.581</td>
<td id="S3.T1.6.2.2.5" class="ltx_td ltx_align_center">0.538</td>
<td id="S3.T1.6.2.2.6" class="ltx_td ltx_align_center">0.587</td>
</tr>
<tr id="S3.T1.7.3.3" class="ltx_tr">
<th id="S3.T1.7.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.7.3.3.1.m1.1" class="ltx_Math" alttext="\diamondsuit" display="inline"><semantics id="S3.T1.7.3.3.1.m1.1a"><mi mathvariant="normal" id="S3.T1.7.3.3.1.m1.1.1" xref="S3.T1.7.3.3.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="S3.T1.7.3.3.1.m1.1b"><ci id="S3.T1.7.3.3.1.m1.1.1.cmml" xref="S3.T1.7.3.3.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.3.3.1.m1.1c">\diamondsuit</annotation></semantics></math> <span id="S3.T1.7.3.3.1.1" class="ltx_text ltx_font_typewriter">ROCK<sub id="S3.T1.7.3.3.1.1.1" class="ltx_sub">VSQA</sub></span> <span id="S3.T1.7.3.3.1.2" class="ltx_text" style="font-size:70%;">Facial</span>
</th>
<td id="S3.T1.7.3.3.2" class="ltx_td ltx_align_center">0.649</td>
<td id="S3.T1.7.3.3.3" class="ltx_td ltx_align_center">0.743</td>
<td id="S3.T1.7.3.3.4" class="ltx_td ltx_align_center">0.581</td>
<td id="S3.T1.7.3.3.5" class="ltx_td ltx_align_center">0.537</td>
<td id="S3.T1.7.3.3.6" class="ltx_td ltx_align_center">0.587</td>
</tr>
<tr id="S3.T1.8.4.4" class="ltx_tr">
<th id="S3.T1.8.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.8.4.4.1.m1.1" class="ltx_Math" alttext="\diamondsuit" display="inline"><semantics id="S3.T1.8.4.4.1.m1.1a"><mi mathvariant="normal" id="S3.T1.8.4.4.1.m1.1.1" xref="S3.T1.8.4.4.1.m1.1.1.cmml">♢</mi><annotation-xml encoding="MathML-Content" id="S3.T1.8.4.4.1.m1.1b"><ci id="S3.T1.8.4.4.1.m1.1.1.cmml" xref="S3.T1.8.4.4.1.m1.1.1">♢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.4.4.1.m1.1c">\diamondsuit</annotation></semantics></math> <span id="S3.T1.8.4.4.1.1" class="ltx_text ltx_font_typewriter">ROCK<sub id="S3.T1.8.4.4.1.1.1" class="ltx_sub">VSQA</sub></span> <span id="S3.T1.8.4.4.1.2" class="ltx_text" style="font-size:70%;">Caption</span>
</th>
<td id="S3.T1.8.4.4.2" class="ltx_td ltx_align_center">0.666</td>
<td id="S3.T1.8.4.4.3" class="ltx_td ltx_align_center">0.772</td>
<td id="S3.T1.8.4.4.4" class="ltx_td ltx_align_center">0.581</td>
<td id="S3.T1.8.4.4.5" class="ltx_td ltx_align_center">0.514</td>
<td id="S3.T1.8.4.4.6" class="ltx_td ltx_align_center">0.580</td>
</tr>
<tr id="S3.T1.12.8.11.3" class="ltx_tr">
<th id="S3.T1.12.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Humans <span id="S3.T1.12.8.11.3.1.1" class="ltx_text" style="font-size:70%;">(Rookies)</span>
</th>
<td id="S3.T1.12.8.11.3.2" class="ltx_td ltx_align_center">0.936</td>
<td id="S3.T1.12.8.11.3.3" class="ltx_td ltx_align_center">0.932</td>
<td id="S3.T1.12.8.11.3.4" class="ltx_td ltx_align_center">0.624</td>
<td id="S3.T1.12.8.11.3.5" class="ltx_td ltx_align_center">0.655</td>
<td id="S3.T1.12.8.11.3.6" class="ltx_td ltx_align_center">0.748</td>
</tr>
<tr id="S3.T1.9.5.5" class="ltx_tr">
<th id="S3.T1.9.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="5">
<span id="S3.T1.9.5.5.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:3.0pt;">
<span id="S3.T1.9.5.5.2.1.1" class="ltx_p">
<span id="S3.T1.9.5.5.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:47.6pt;vertical-align:-21.3pt;"><span class="ltx_transformed_inner" style="width:47.5pt;transform:translate(-19.31pt,2.92pt) rotate(-90deg) ;">
<span id="S3.T1.9.5.5.2.1.1.1.1" class="ltx_p">Knowledge</span>
</span></span></span>
</span>
</th>
<th id="S3.T1.9.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<math id="S3.T1.9.5.5.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.T1.9.5.5.1.m1.1a"><mi mathvariant="normal" id="S3.T1.9.5.5.1.m1.1.1" xref="S3.T1.9.5.5.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S3.T1.9.5.5.1.m1.1b"><ci id="S3.T1.9.5.5.1.m1.1.1.cmml" xref="S3.T1.9.5.5.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.5.5.1.m1.1c">\bigstar</annotation></semantics></math> <span id="S3.T1.9.5.5.1.1" class="ltx_text ltx_font_typewriter">ROCK</span> <span id="S3.T1.9.5.5.1.2" class="ltx_text" style="font-size:70%;">Image</span>
</th>
<td id="S3.T1.9.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.654</td>
<td id="S3.T1.9.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.681</td>
<td id="S3.T1.9.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.628</td>
<td id="S3.T1.9.5.5.6" class="ltx_td ltx_align_center ltx_border_t">0.647</td>
<td id="S3.T1.9.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.652</td>
</tr>
<tr id="S3.T1.10.6.6" class="ltx_tr">
<th id="S3.T1.10.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.10.6.6.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.T1.10.6.6.1.m1.1a"><mi mathvariant="normal" id="S3.T1.10.6.6.1.m1.1.1" xref="S3.T1.10.6.6.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S3.T1.10.6.6.1.m1.1b"><ci id="S3.T1.10.6.6.1.m1.1.1.cmml" xref="S3.T1.10.6.6.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.6.6.1.m1.1c">\bigstar</annotation></semantics></math> <span id="S3.T1.10.6.6.1.1" class="ltx_text ltx_font_typewriter">ROCK</span> <span id="S3.T1.10.6.6.1.2" class="ltx_text" style="font-size:70%;">Concepts</span>
</th>
<td id="S3.T1.10.6.6.2" class="ltx_td ltx_align_center">0.654</td>
<td id="S3.T1.10.6.6.3" class="ltx_td ltx_align_center">0.685</td>
<td id="S3.T1.10.6.6.4" class="ltx_td ltx_align_center">0.628</td>
<td id="S3.T1.10.6.6.5" class="ltx_td ltx_align_center">0.646</td>
<td id="S3.T1.10.6.6.6" class="ltx_td ltx_align_center">0.652</td>
</tr>
<tr id="S3.T1.11.7.7" class="ltx_tr">
<th id="S3.T1.11.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.11.7.7.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.T1.11.7.7.1.m1.1a"><mi mathvariant="normal" id="S3.T1.11.7.7.1.m1.1.1" xref="S3.T1.11.7.7.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.7.7.1.m1.1b"><ci id="S3.T1.11.7.7.1.m1.1.1.cmml" xref="S3.T1.11.7.7.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.7.7.1.m1.1c">\bigstar</annotation></semantics></math> <span id="S3.T1.11.7.7.1.1" class="ltx_text ltx_font_typewriter">ROCK</span> <span id="S3.T1.11.7.7.1.2" class="ltx_text" style="font-size:70%;">Facial</span>
</th>
<td id="S3.T1.11.7.7.2" class="ltx_td ltx_align_center">0.654</td>
<td id="S3.T1.11.7.7.3" class="ltx_td ltx_align_center">0.688</td>
<td id="S3.T1.11.7.7.4" class="ltx_td ltx_align_center">0.628</td>
<td id="S3.T1.11.7.7.5" class="ltx_td ltx_align_center">0.646</td>
<td id="S3.T1.11.7.7.6" class="ltx_td ltx_align_center">0.652</td>
</tr>
<tr id="S3.T1.12.8.8" class="ltx_tr">
<th id="S3.T1.12.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.12.8.8.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.T1.12.8.8.1.m1.1a"><mi mathvariant="normal" id="S3.T1.12.8.8.1.m1.1.1" xref="S3.T1.12.8.8.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S3.T1.12.8.8.1.m1.1b"><ci id="S3.T1.12.8.8.1.m1.1.1.cmml" xref="S3.T1.12.8.8.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.8.8.1.m1.1c">\bigstar</annotation></semantics></math> <span id="S3.T1.12.8.8.1.1" class="ltx_text ltx_font_typewriter">ROCK</span> <span id="S3.T1.12.8.8.1.2" class="ltx_text" style="font-size:70%;">Caption</span>
</th>
<td id="S3.T1.12.8.8.2" class="ltx_td ltx_align_center">0.647</td>
<td id="S3.T1.12.8.8.3" class="ltx_td ltx_align_center">0.678</td>
<td id="S3.T1.12.8.8.4" class="ltx_td ltx_align_center">0.593</td>
<td id="S3.T1.12.8.8.5" class="ltx_td ltx_align_center">0.643</td>
<td id="S3.T1.12.8.8.6" class="ltx_td ltx_align_center">0.646</td>
</tr>
<tr id="S3.T1.12.8.12.4" class="ltx_tr">
<th id="S3.T1.12.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Humans <span id="S3.T1.12.8.12.4.1.1" class="ltx_text" style="font-size:70%;">(Masters)</span>
</th>
<td id="S3.T1.12.8.12.4.2" class="ltx_td ltx_align_center ltx_border_b">0.961</td>
<td id="S3.T1.12.8.12.4.3" class="ltx_td ltx_align_center ltx_border_b">0.936</td>
<td id="S3.T1.12.8.12.4.4" class="ltx_td ltx_align_center ltx_border_b">0.857</td>
<td id="S3.T1.12.8.12.4.5" class="ltx_td ltx_align_center ltx_border_b">0.867</td>
<td id="S3.T1.12.8.12.4.6" class="ltx_td ltx_align_center ltx_border_b">0.896</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We train our models with stochastic gradient descent with 0.9 momentum and 0.001 learning rate. For BERT, we used the uncased base model with pre-trained initialisation. We compare ROCK against two categories of models:</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">1) Vis, Sub, QA.</span> Models using language and visual representations, but not knowledge, including (i) <span id="S4.p2.1.2" class="ltx_text ltx_font_typewriter">TVQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> a state-of-the-art VideoQA method in which language is encoded with a LSTM layer, whereas visual data is encoded into visual concepts; (ii) <span id="S4.p2.1.3" class="ltx_text ltx_font_typewriter">ROCK<sub id="S4.p2.1.3.1" class="ltx_sub">VSQA</sub></span> our model without knowledge; and (iii) Humans <span id="S4.p2.1.4" class="ltx_text" style="font-size:70%;">(Rookies)</span> evaluators who have never watched any episode. Our model outperforms <span id="S4.p2.1.5" class="ltx_text ltx_font_typewriter">TVQA</span> by 6.6% but still lags well behind human accuracy</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">2) Knowledge.</span> Models that exploit <span id="S4.p3.1.2" class="ltx_text ltx_font_smallcaps">knowledge</span> to predict the correct answer, i.e. our <span id="S4.p3.1.3" class="ltx_text ltx_font_typewriter">ROCK</span> model in its full version and Humans <span id="S4.p3.1.4" class="ltx_text" style="font-size:70%;">(Rookies)</span> evaluators that have watched the show. Compared to the non-knowledge methods, the inclusion of the knowledge retrieval module increases the accuracy by 6.5%, showing the great potential of knowledge-based approaches in our dataset. Among the visual representations, Image, Concepts, and Facial perform the same. However, the gap between ROCK and humans increases when knowledge is used, suggesting potential room for improvement in both video modeling and knowledge representation. An example result can be seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Knowledge-Based Visual Question Answering in Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented a novel dataset for knowledge-based visual question answering in videos and proposed a video reasoning model in which multi-modal video information was combined together with specific knowledge about the task. Our evaluation showed the great potential of knowledge-based models in video understanding problems. However, there is still a big gap with respect to human performance.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was partly supported by JSPS KAKENHI No. 18H03264 and JST ACT-I.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for
language understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">NAACL</span>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima.

</span>
<span class="ltx_bibblock">KnowIT VQA: Answering knowledge-based questions about videos.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">AAAI</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.

</span>
<span class="ltx_bibblock">TVQA: Localized, compositional video question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.

</span>
<span class="ltx_bibblock">Deep face recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">BMVC</span>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
Urtasun, and Sanja Fidler.

</span>
<span class="ltx_bibblock">Movieqa: Understanding stories in movies through question-answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4631–4640, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton van den Hengel.

</span>
<span class="ltx_bibblock">FVQA: Fact-based visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 40(10), 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4622–4630, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.08384" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.08385" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.08385">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.08385" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.08386" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 08:27:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
