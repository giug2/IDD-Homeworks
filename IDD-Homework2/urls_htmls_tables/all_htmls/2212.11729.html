<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.11729] Federated Learning Methods, Applications and Beyond</title><meta property="og:description" content="In recent years the applications of machine learning models have increased rapidly, due to the large amount of available data and technological progress.
While some domains like web analysis can benefit from this with ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning Methods, Applications and Beyond">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning Methods, Applications and Beyond">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.11729">

<!--Generated on Fri Mar  1 09:44:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning
<br class="ltx_break">Methods, Applications and Beyond</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Moritz Heusinger<sup id="id5.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Christoph Raab<sup id="id6.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fabrice Rossi<sup id="id7.3.id1" class="ltx_sup">2</sup> and Frank-Michael Schleif<sup id="id8.4.id2" class="ltx_sup">1</sup>

<br class="ltx_break">1- University of Applied Science W√ºrzburg-Schweinfurt - Department 
<br class="ltx_break">of Computer Science
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> W√ºrzburg - Germany 
<br class="ltx_break">2- CEREMADE
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> University Paris Dauphine PSL
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> France
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">In recent years the applications of machine learning models have increased rapidly, due to the large amount of available data and technological progress.
While some domains like web analysis can benefit from this with only minor restrictions, other fields like medicine with patient data are stronger
regulated. In particular <em id="id9.id1.1" class="ltx_emph ltx_font_italic">data privacy</em> plays an important role as recently highlighted by the trustworthy AI initiative of the EU or general privacy regulations in legislation. Another major challenge is, that the required training <em id="id9.id1.2" class="ltx_emph ltx_font_italic">data is</em> often <em id="id9.id1.3" class="ltx_emph ltx_font_italic">distributed</em> in terms of features or samples and unavailable for classical
batch learning approaches. In 2016 Google came up with a framework, called <em id="id9.id1.4" class="ltx_emph ltx_font_italic">Federated Learning</em> to solve both of these problems. We provide a brief overview on existing Methods and Applications in the field of vertical and horizontal <em id="id9.id1.5" class="ltx_emph ltx_font_italic">Federated Learning</em>, as well as <em id="id9.id1.6" class="ltx_emph ltx_font_italic">Federated Transfer Learning</em>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a novel concept for learning distributed data, which was first introduced by Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in 2016.</p>
</div>
<div id="Thmdefinition1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinition1.1.1.1" class="ltx_text ltx_font_bold">Definition 1</span></span><span id="Thmdefinition1.2.2" class="ltx_text ltx_font_bold"> </span>(Federated Learning)<span id="Thmdefinition1.3.3" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmdefinition1.p1" class="ltx_para">
<p id="Thmdefinition1.p1.7" class="ltx_p"><span id="Thmdefinition1.p1.7.7" class="ltx_text ltx_font_italic">Given a large number of <math id="Thmdefinition1.p1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Thmdefinition1.p1.1.1.m1.1a"><mi id="Thmdefinition1.p1.1.1.m1.1.1" xref="Thmdefinition1.p1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.1.1.m1.1b"><ci id="Thmdefinition1.p1.1.1.m1.1.1.cmml" xref="Thmdefinition1.p1.1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.1.1.m1.1c">N</annotation></semantics></math> clients and a particular data analysis task,
each client <math id="Thmdefinition1.p1.2.2.m2.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="Thmdefinition1.p1.2.2.m2.1a"><msub id="Thmdefinition1.p1.2.2.m2.1.1" xref="Thmdefinition1.p1.2.2.m2.1.1.cmml"><mi id="Thmdefinition1.p1.2.2.m2.1.1.2" xref="Thmdefinition1.p1.2.2.m2.1.1.2.cmml">C</mi><mi id="Thmdefinition1.p1.2.2.m2.1.1.3" xref="Thmdefinition1.p1.2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.2.2.m2.1b"><apply id="Thmdefinition1.p1.2.2.m2.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.2.2.m2.1.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1">subscript</csymbol><ci id="Thmdefinition1.p1.2.2.m2.1.1.2.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1.2">ùê∂</ci><ci id="Thmdefinition1.p1.2.2.m2.1.1.3.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.2.2.m2.1c">C_{i}</annotation></semantics></math> has its own data addressing the task, without direct
access to the other clients data.
The objective in FL is to learn a predictive model <math id="Thmdefinition1.p1.3.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Thmdefinition1.p1.3.3.m3.1a"><mi id="Thmdefinition1.p1.3.3.m3.1.1" xref="Thmdefinition1.p1.3.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.3.3.m3.1b"><ci id="Thmdefinition1.p1.3.3.m3.1.1.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.3.3.m3.1c">M</annotation></semantics></math> such that
the error on the objective function <math id="Thmdefinition1.p1.4.4.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="Thmdefinition1.p1.4.4.m4.1a"><mi id="Thmdefinition1.p1.4.4.m4.1.1" xref="Thmdefinition1.p1.4.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.4.4.m4.1b"><ci id="Thmdefinition1.p1.4.4.m4.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1">ùê∏</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.4.4.m4.1c">E</annotation></semantics></math> is minimized, in a distributed way. In particular
various data processing clients are involved. The communication
takes place by a distributed protocol where in general a master is
identified to aggregate the prediction model.
FL has three steps (1) an initial model is distributed to the
clients (2) the local model of <math id="Thmdefinition1.p1.5.5.m5.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="Thmdefinition1.p1.5.5.m5.1a"><msub id="Thmdefinition1.p1.5.5.m5.1.1" xref="Thmdefinition1.p1.5.5.m5.1.1.cmml"><mi id="Thmdefinition1.p1.5.5.m5.1.1.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.cmml">C</mi><mi id="Thmdefinition1.p1.5.5.m5.1.1.3" xref="Thmdefinition1.p1.5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.5.5.m5.1b"><apply id="Thmdefinition1.p1.5.5.m5.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.5.5.m5.1.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1">subscript</csymbol><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2">ùê∂</ci><ci id="Thmdefinition1.p1.5.5.m5.1.1.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.5.5.m5.1c">C_{i}</annotation></semantics></math> is trained on its local data by
taking the model information of the master into account
(3) the master aggregates the local models <math id="Thmdefinition1.p1.6.6.m6.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="Thmdefinition1.p1.6.6.m6.1a"><msub id="Thmdefinition1.p1.6.6.m6.1.1" xref="Thmdefinition1.p1.6.6.m6.1.1.cmml"><mi id="Thmdefinition1.p1.6.6.m6.1.1.2" xref="Thmdefinition1.p1.6.6.m6.1.1.2.cmml">M</mi><mi id="Thmdefinition1.p1.6.6.m6.1.1.3" xref="Thmdefinition1.p1.6.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.6.6.m6.1b"><apply id="Thmdefinition1.p1.6.6.m6.1.1.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.6.6.m6.1.1.1.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1">subscript</csymbol><ci id="Thmdefinition1.p1.6.6.m6.1.1.2.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1.2">ùëÄ</ci><ci id="Thmdefinition1.p1.6.6.m6.1.1.3.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.6.6.m6.1c">M_{i}</annotation></semantics></math> to the global model
<math id="Thmdefinition1.p1.7.7.m7.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Thmdefinition1.p1.7.7.m7.1a"><mi id="Thmdefinition1.p1.7.7.m7.1.1" xref="Thmdefinition1.p1.7.7.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.7.7.m7.1b"><ci id="Thmdefinition1.p1.7.7.m7.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.7.7.m7.1c">M</annotation></semantics></math> and communicates the global model back to the clients.
The steps are visualized in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></p>
</div>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL has gained substantial interest in the machine learning (ML) community with different frameworks implementing the main concept <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, in particular <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">flower</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which is
considered very mature. Applications of FL are more and more frequent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The research field is also very active with new communication protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, encryption concepts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
and particular optimization algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">FL has numerous advantages over classical ML. While in classical machine learning training data are submitted to a central instance and a model is learned in batch processing, FL shifts the actual learning to the data source.
This allows one to employ the power of distributed client machines, keeps the user data private, and permits to use information that is otherwise inaccessible and spread over different clients.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this view FL perfectly aligns with recent trends on machine learning on large community data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and the increasing set of constraints due to privacy regulations like the GDPR<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">https://gdprinfo.eu/</span></span></span></span>,
trustworthy AI<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</span></span></span></span> but also objectives covered in various AI manifests<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">https://nouvelles.umontreal.ca/en/article/2018/12/04/developing-ai-in-a-responsible-way/</span></span></span></span>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2212.11729/assets/fig/fl-flow.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Diagram of FL. To preserve data privacy, local model gradients are only sent to one trusted server (or a primary coordinator) and not directly to other clients. The local instances are training their own model with the local data and after multiple iterations the gradient is sent to the primary coordinator. The primary coordinator aggregates all local gradients to a central global update, which is used to update the global model. Finally, the global model is sent back to the clients to replace their previous models.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Broadly speaking, Federated Learning methods belong to three different categories, which depend on how comparable are the local data of the clients, as detailed in Sec. <a href="#S2" title="2 Types of federated learning ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Types of federated learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.5" class="ltx_p">We denote the data held by client <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">C</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ùê∂</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">C_{i}</annotation></semantics></math> as <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">ùíü</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ùíü</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathcal{D}_{i}</annotation></semantics></math>. In FL a data set consists of the features <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">ùêó</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ùêó</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{X}</annotation></semantics></math>, the sample id space <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{i}" display="inline"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">ùê¢</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ùê¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\mathbf{i}</annotation></semantics></math> and an optional label space <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathbf{y}</annotation></semantics></math>. The sample space, as well as the feature space, may not be identical over different data owners. By this characteristic, FL is categorized into horizontal FL (HFL), vertical FL (VFL), and Federated Transfer Learning (FTL), as detailed below.</p>
</div>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/vertical-a.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="553" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Client A</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/vertical-b.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="546" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Client B</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Vertical Federated Learning (inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>)</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/horizontal-a.png" id="S2.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="598" height="572" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Client A</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/horizontal-b.png" id="S2.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="598" height="591" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S2.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Client B</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F4.5.2" class="ltx_text" style="font-size:90%;">Horizontal Federated Learning (inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>)</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/transfer-a.png" id="S2.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="540" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Client A</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.11729/assets/fig/transfer-b.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="551" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Client B</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.6.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.7.2" class="ltx_text" style="font-size:90%;">Federated Transfer Learning (inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>)</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Horizontal FL</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The scenario of HFL is depicted in Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2 Types of federated learning ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and shows that the users are split across various clients, while the feature space is always the same.
A typical scenario is given by a global business model which is implemented on smartphones or other IoT devices. Thereby the user generates the same data, like interaction events or shopping activities but may be located in very different geographical regions. HFL is beneficial by calculating a model which employs the information from a
large (distributed) user group instead of focusing on a centralized approach with a rather limited amount of training data. In HFL it is common to calculate and upload local gradients calculated from the objective function, which are aggregated by a central master client. The data transmissions can be encrypted to improve the level of privacy using homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> or secure aggregation as discussed in Sec. <a href="#S4" title="4 Privacy methods ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vertical FL</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Vertical federated learning considers the case where we have multiple different feature sets on a common basis of users. A typical case for this scenario (depicted in Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2 Types of federated learning ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)
is an analysis task where the user is using at least two ways of interaction and is generating data on two channels, like an online store and in a brick and mortar business.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Transfer learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">As described above, in FL, it is generally assumed that every user provides a sufficient set of labeled data for a model to learn a specified task. In general, it is also assumed that the analyzed data are given in common feature spaces, although potentially split across various clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. If the first assumption does not hold, the model cannot capture the whole data set characteristic leading to poor prediction performance. Further, the provided gradients are biased towards the present data characteristics, which eventually deteriorates the global learning model. If the second assumption does not hold, the entities and the corresponding feature spaces are disjoint. This prevents the model from discovering the feature space characteristics, and therefore, the FL system cannot extract useful information for the specific user model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.5" class="ltx_p">In the cases just described, methods of Federated Transfer Learning need to be applied. The FTL setup is illustrated in Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2 Types of federated learning ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. FTL can be seen as the cross-section of horizontal and vertical transfer learning. In general, Transfer Learning is an algorithmic technique to improve a model trained on one data set, by using related information from another data set. The just mentioned data sets are usually denoted as source and target domain. FTL helps to improve the model of user <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><msub id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml">U</mi><mi id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2">ùëà</ci><ci id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">U_{i}</annotation></semantics></math> on its data by using data or model information from one (or more) users <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="U_{j}" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><msub id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml"><mi id="S2.SS3.p2.2.m2.1.1.2" xref="S2.SS3.p2.2.m2.1.1.2.cmml">U</mi><mi id="S2.SS3.p2.2.m2.1.1.3" xref="S2.SS3.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><apply id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.2.m2.1.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p2.2.m2.1.1.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2">ùëà</ci><ci id="S2.SS3.p2.2.m2.1.1.3.cmml" xref="S2.SS3.p2.2.m2.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">U_{j}</annotation></semantics></math>, where <math id="S2.SS3.p2.3.m3.1" class="ltx_Math" alttext="i\neq j" display="inline"><semantics id="S2.SS3.p2.3.m3.1a"><mrow id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml"><mi id="S2.SS3.p2.3.m3.1.1.2" xref="S2.SS3.p2.3.m3.1.1.2.cmml">i</mi><mo id="S2.SS3.p2.3.m3.1.1.1" xref="S2.SS3.p2.3.m3.1.1.1.cmml">‚â†</mo><mi id="S2.SS3.p2.3.m3.1.1.3" xref="S2.SS3.p2.3.m3.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><apply id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1"><neq id="S2.SS3.p2.3.m3.1.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1.1"></neq><ci id="S2.SS3.p2.3.m3.1.1.2.cmml" xref="S2.SS3.p2.3.m3.1.1.2">ùëñ</ci><ci id="S2.SS3.p2.3.m3.1.1.3.cmml" xref="S2.SS3.p2.3.m3.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">i\neq j</annotation></semantics></math>. Hence, the learning environment of <math id="S2.SS3.p2.4.m4.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S2.SS3.p2.4.m4.1a"><msub id="S2.SS3.p2.4.m4.1.1" xref="S2.SS3.p2.4.m4.1.1.cmml"><mi id="S2.SS3.p2.4.m4.1.1.2" xref="S2.SS3.p2.4.m4.1.1.2.cmml">U</mi><mi id="S2.SS3.p2.4.m4.1.1.3" xref="S2.SS3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.4.m4.1b"><apply id="S2.SS3.p2.4.m4.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.4.m4.1.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS3.p2.4.m4.1.1.2.cmml" xref="S2.SS3.p2.4.m4.1.1.2">ùëà</ci><ci id="S2.SS3.p2.4.m4.1.1.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.1c">U_{i}</annotation></semantics></math> is called target, and <math id="S2.SS3.p2.5.m5.1" class="ltx_Math" alttext="U_{j}" display="inline"><semantics id="S2.SS3.p2.5.m5.1a"><msub id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml"><mi id="S2.SS3.p2.5.m5.1.1.2" xref="S2.SS3.p2.5.m5.1.1.2.cmml">U</mi><mi id="S2.SS3.p2.5.m5.1.1.3" xref="S2.SS3.p2.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2">ùëà</ci><ci id="S2.SS3.p2.5.m5.1.1.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">U_{j}</annotation></semantics></math> is source <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Note, that there can be small intersections regarding feature space or sample space as displayed in Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2 Types of federated learning ‚Ä£ Federated Learning Methods, Applications and Beyond" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. A model improvement on the target by means of FTL is achieved by creating a shared representation keeping the federation of data. For example via manifold alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> or domain adversarial learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Alternatively, FTL methods improve the target model leveraging the source model by instance reweighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> or by receiving gradients from source <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Note that some approaches also consider different data distributions between source and target <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Frameworks and Algorithms</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In FL the communication architecture and protocols are of particular importance. FL has to deal with non i.i.d. data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and due to the FL learning concept substantial communication costs can occur. Considering the typical use cases of FL in the field of mobile devices, IoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, unmanned vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or large distributed server systems additional compression techniques, averaging strategies, and sparsity constraints are applied to obtain real time-efficient systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
One can also employ quantization approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or information about the transmitted data to control the communication load <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Only recently some FL frameworks have been proposed which simplify the implementation of own FL models. In particular, for HFL the BlockFL framework
was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which makes use of a blockchain during the updates of the model parameters. A framework for VFL is provided
by SecureBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. And in the context of Federated Transfer Learning the framework in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is suggested. The framework <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">Flower</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
scales well to a large number of clients.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The machine learning community has also recently started to design dedicated learning algorithms for FL. One example is given in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> where hyperparameter
learning on distributed systems is considered. Also information theoretic strategies have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, matrix factorization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>,
spectral clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, particular designed gradient descend techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or multi-objective solvers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Furthermore, the Learning Vector Quantization (LVQ) concept has been adopted, to fit in a VFL environment by training separate LVQ models locally and using the relevance matrix to update a global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Privacy methods</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">The key element of all FL approaches is to keep the data on the user side and in particular to avoid any disclosures. Three techniques
are most common to ensure this goal and are frequently combined:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">homomorphic encryption</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">differential privacy</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">(secure) model aggregation</p>
</div>
</li>
</ol>
<p id="S4.p1.1" class="ltx_p">The most common is model aggregation which trains the global model by summarizing the model parameters from all clients to avoid disclosure of original data.
Many optimization concepts which are roughly based on a kind of iterative stochastic gradient descent on <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ùê∏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">E</annotation></semantics></math> perform a natural averaging of various update steps which is also used in batch online learning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. A prominent approach for deep learning falling into this category is given in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
An alternative view is to train local models in a multi-task setting which are subsequently combined as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The various
local model parameters can also be safely transferred in an aggregated form by blockchain techniques as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. One may also directly provide privacy-preserving data representations as shown for kernels in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Homomorphic encryption allows to apply calculations on encrypted data without the need of decoding. One approach following this idea is additive homomorphism
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Differential privacy as detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> is a technique to limit information disclosure during learning. Thereby the training procedure is
designed such that small modifications of the training database have no substantial impact on the model outcome. The attacker can not obtain accurate
individual information, but only a controlled piece of information, which still obeys privacy constraints. The particular strategies to implement this
concept can be very simple by adding some noise contributions to the output during training or by more complex compression techniques as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> a privacy-preserving method to compute dot-product kernels in VFL is proposed. The technique uses multi-party computation to provide theoretical guarantees on security and privacy.
For a more detailed analysis, we refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">FL can train a united model on data from distributed sources while preserving data privacy and security and thus can play an important role in many industrial sectors, like sales, health, insurance, and others. In general, in every sector, where data cannot be directly aggregated due to privacy protection, data security, or even property rights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">FL has been successfully used to improve the quality of keyboard search suggestions on the Google Gboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Gboard is a virtual keyboard for mobile devices and has several features, like auto-completion and next-word prediction. The application needs to protect the privacy of users and make latency-free predictions. To avoid high data usage and battery consumption due to the FL optimization, the authors had to design mechanics to only send data to a centralized server, if the device is inside a wireless network and actively charging. The server provides every client with a training task as soon as enough clients are connected. In a related context, FL has also been used to perform mobile keyboard predictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Recently, a project has been started, which uses FL to deploy an auction for intralogistic autonomous drone transportation<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>mFUND program of the BMVI, project FlowPro, grant number 19F2128B</span></span></span>. The goal of the project is, to create a system where drone owners can bid on transport jobs and execute them in case of winning. The bidding model is based on FL, which enables the knowledge incorporation of every drone in the global model of the system. The distribution of the global model also leads to the fact, that drones with fewer executed jobs will have the same chance to win an auction as more experienced drones.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, a hybrid continual learning strategy is used to address the real-world constraints like computational and memory limits in a real-time on-device personalization task, running on a native Android application.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Other applications of FL include ranking browser history suggestions based on user-interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, visual object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, patient clustering to predict hospital stay time as well as mortality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, drug discovery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and brain tumor segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
The application fields of FL are quickly increasing and are a promising research direction of ML. For a more comprehensive review, we refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this tutorial, we briefly discussed the evolving field of federated learning and outlined recent achievements and approaches. A detailed analysis of recent trends and problems in FL is also provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Due to additional privacy constraints in ML and a variety of distributed user groups of ML methods, it can be expected that FL will become even more important in the future</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p ltx_align_left"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acknowledgment</span></p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p ltx_align_left"><span id="S6.p3.1.1" class="ltx_text" style="font-size:80%;">MH and CR are thankful for support in the FuE program Informations- und Kommunikationstechnik of the StMWi, project OBerA, grant number IUK-1709-0011// IUK530/010 and for support in the mFUND program of the BMVI, project FlowPro, grant number 19F2128B.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
Jakub Koneƒçn·ª≥, H¬†Brendan McMahan, Felix¬†X Yu, Peter Richt√°rik,
Ananda¬†Theertha Suresh, and Dave Bacon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:80%;">Federated learning: Strategies for improving communication
efficiency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1610.05492</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:80%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
Jakub Koneƒçn·ª≥, H¬†Brendan McMahan, Daniel Ramage, and Peter
Richt√°rik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:80%;">Federated optimization: Distributed machine learning for on-device
intelligence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:1610.02527</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:80%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
H.¬†B. McMahan, Eider Moore, D.¬†Ramage, and B.¬†A.¬†Y. Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:80%;">Federated learning of deep networks using model averaging.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">ArXiv</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:80%;">, abs/1602.05629, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
Christian Schneebeli, Saikishore Kalloori, and Severin Klingler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:80%;">A practical federated learning framework for small number of
stakeholders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 14th ACM International Conference on Web
Search and Data Mining</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:80%;">, WSDM 2021, pages 910‚Äì913, New York, NY, USA, 2021.
Association for Computing Machinery.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
Daniel¬†J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet,
Pedro P.¬†B. de¬†Gusm√£o, and Nicholas¬†D. Lane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:80%;">Flower: A friendly federated learning research framework, 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:80%;">https://flower.dev/</span><span id="bib.bib5.3.1" class="ltx_text" style="font-size:80%;">.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
Theodora¬†S. Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis¬†Ch.
Paschalidis, and Wei Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:80%;">Federated learning of predictive models from federated electronic
health records.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">International Journal of Medical Informatics</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:80%;">, 112:59‚Äì67, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
Li¬†Li, Yuxi Fan, Mike Tse, and Kuo-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:80%;">A review of applications in federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Computers and Industrial Engineering</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:80%;">, 149:106854, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
Abbas Yazdinejad, Reza¬†M. Parizi, Ali Dehghantanha, and Hadis Karimipour.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:80%;">Federated learning for drone authentication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Ad Hoc Networks</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:80%;">, 120:102574, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
Q.¬†Yang, Y.¬†Liu, T.¬†Chen, and Y.¬†Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:80%;">Federated machine learning: Concept and applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">ACM Transactions on Intelligent Systems and Technology</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:80%;">, 10(2),
2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
Viraaji Mothukuri, Reza¬†M. Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
Dehghantanha, and Gautam Srivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:80%;">A survey on security and privacy of federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Future Generation Computer Systems</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:80%;">, 115:619‚Äì640, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
J.¬†Shi, J.¬†Bian, J.¬†Richter, K.¬†Chen, J.¬†Rahnenf√ºhrer, H.¬†Xiong, and
J.¬†Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:80%;">Modes: model-based optimization on distributed embedded systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Machine Learning</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:80%;">, 110(6):1527‚Äì1547, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
Hangyu Zhu and Yaochu Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:80%;">Multi-objective evolutionary federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:80%;">,
31(4):1310‚Äì1322, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
Enyue Yang, Yunfeng Huang, Feng Liang, Weike Pan, and Zhong Ming.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:80%;">Fcmf: Federated collective matrix factorization for heterogeneous
collaborative filtering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Knowledge-Based Systems</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:80%;">, 220:106946, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
Shaoqi Chen, Dongyu Xue, Guohui Chuai, Qiang Yang, and Qi¬†Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:80%;">Fl-qsar: a federated learning based qsar prototype for collaborative
drug discovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">bioRxiv</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
Haokun Fang and Quan Qian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:80%;">Privacy preserving machine learning with homomorphic encryption and
federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Future Internet</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:80%;">, 13(4), 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
Nuria Rodr√≠guez-Barroso and Goran¬†Stipcich et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:80%;">Federated learning and differential privacy: Software tools analysis,
the sherpa.ai fl framework and methodological guidelines for preserving data
privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Information Fusion</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:80%;">, 64:270‚Äì292, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:80%;">A Secure Federated Transfer Learning Framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Intelligent Systems</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:80%;">, 1672(c):1‚Äì1, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
Ce¬†Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, and Cuntai Guan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:80%;">Federated transfer learning for eeg signal classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">2020 42nd Annual International Conference of the IEEE
Engineering in Medicine Biology Society (EMBC)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:80%;">, pages 3040‚Äì3045, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:80%;">Federated adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:80%;">. OpenReview.net, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
Dashan Gao, Yang Liu, Anbu Huang, Ce¬†Ju, Han Yu, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:80%;">Privacy-preserving heterogeneous federated transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:80%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">2019 IEEE International Conference on Big Data (Big Data)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:80%;">,
pages 2552‚Äì2559, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Felix Sattler, Simon Wiedemann, Klaus-Robert M√ºller, and Wojciech Samek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:80%;">Robust and communication-efficient federated learning from non-i.i.d.
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:80%;">,
31(9):3400‚Äì3413, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
Yang Zhao, Jun Zhao, Mengmeng Yang, Teng Wang, Ning Wang, Lingjuan Lyu, Dusit
Niyato, and Kwok-Yan Lam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:80%;">Local differential privacy-based federated learning for internet of
things.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Internet of Things Journal</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:80%;">, 8(11):8836‚Äì8853, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
Mingzhe Chen, Nir Shlezinger, H.¬†Vincent Poor, Yonina¬†C. Eldar, and Shuguang
Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:80%;">Communication-efficient federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the National Academy of Sciences</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:80%;">, 118(17), 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
Nicola Tonellotto, Alberto Gotta, Franco¬†Maria Nardini, Daniele Gadler, and
Fabrizio Silvestri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:80%;">Neural network quantization in federated learning at the edge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Information Sciences</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:80%;">, 575:417‚Äì436, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
Xueyu Wu, Xin Yao, and Cho-Li Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:80%;">Fedscr: Structure-based communication reduction for federated
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Parallel and Distributed Systems</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:80%;">,
32(7):1565‚Äì1577, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:80%;">Blockchained on-device federated learning, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
K.¬†Cheng, T.¬†Fan, Y.¬†Jin, Y.¬†Liu, T.¬†Chen, D.¬†Papadopoulos, and Q.¬†Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:80%;">Secureboost: A lossless federated learning framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Intelligent Systems</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:80%;">, pages 1‚Äì1, may 5555.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:80%;">A secure federated transfer learning framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Intelligent Systems</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:80%;">, 35(4):70‚Äì82, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
Md¬†Palash Uddin, Yong Xiang, Xuequan Lu, John Yearwood, and Longxiang Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:80%;">Mutual information driven federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Parallel and Distributed Systems</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:80%;">,
32(7):1526‚Äì1538, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
Hongtao Wang, Ang Li, Bolin Shen, Yuyan Sun, and Hongmei Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:80%;">Federated multi-view spectral clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Access</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:80%;">, 8:202249‚Äì202259, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
Wei Liu, Li¬†Chen, Yunfei Chen, and Wenyi Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:80%;">Accelerating federated learning via momentum gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Parallel and Distributed Systems</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:80%;">,
31(8):1754‚Äì1766, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
M.¬†Fernandes, C.¬†Silva, J.¬†Arrais¬†A. Cardoso, and B.¬†Ribeiro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:80%;">Decay momentum for improving federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:80%;">In Michel Verleysen, editor, </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 29. European
Symposium on Artificial Neural Networks ESANN 2021</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:80%;">, page numbers to be
obtained from ToC of this proceedings book, Evere, Belgium, 2021. D-Side
Publications.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
J.¬†Brinkrolf and B.¬†Hammer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:80%;">Federated learning vector quantization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:80%;">In Michel Verleysen, editor, </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 29. European
Symposium on Artificial Neural Networks ESANN 2021</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:80%;">, page numbers to be
obtained from ToC of this proceedings book, Evere, Belgium, 2021. D-Side
Publications.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
C.M. Bishop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Neural networks for pattern recognition</span><span id="bib.bib34.3.2" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:80%;">Oxford University Press, USA, 1995.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan¬†H. Greenewald,
Trong¬†Nghia Hoang, and Yasaman Khazaeni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:80%;">Bayesian nonparametric federated learning of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:80%;">In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:80%;">, volume¬†97 of </span><span id="bib.bib35.6.4" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of Machine Learning Research</span><span id="bib.bib35.7.5" class="ltx_text" style="font-size:80%;">, pages 7252‚Äì7261. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
M.¬†Polato, A.¬†Gallinaro, and F.¬†Aiolli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:80%;">Privacy-preserving kernel computation for vertically partitioned
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:80%;">In Michel Verleysen, editor, </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 29. European
Symposium on Artificial Neural Networks ESANN 2021</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:80%;">, page numbers to be
obtained from ToC of this proceedings book, Evere, Belgium, 2021. D-Side
Publications.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:80%;">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:80%;">
Cynthia Dwork.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:80%;">Differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:80%;">In Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo
Wegener, editors, </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Automata, Languages and Programming, 33rd
International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006,
Proceedings, Part II</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:80%;">, volume 4052 of </span><span id="bib.bib38.6.4" class="ltx_text ltx_font_italic" style="font-size:80%;">Lecture Notes in Computer
Science</span><span id="bib.bib38.7.5" class="ltx_text" style="font-size:80%;">, pages 1‚Äì12. Springer, 2006.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:80%;">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard¬†H. Yang, Farhad Farokhi, Shi Jin,
Tony Q.¬†S. Quek, and H.¬†Vincent Poor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:80%;">Federated learning with differential privacy: Algorithms and
performance analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:80%;">,
15:3454‚Äì3469, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:80%;">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Fran√ßoise Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:80%;">Applied federated learning: Improving google keyboard query
suggestions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:80%;">, abs/1812.02903, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:80%;">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
Dureau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:80%;">Federated learning for keyword spotting, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:80%;">
L.¬†Pellegrini, V.¬†Lomonaco, G.¬†Graffieti, and D.¬†Maltoni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:80%;">Continual learning at the edge: Real-time training on smartphone
devices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:80%;">In Michel Verleysen, editor, </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Proceedings of the 29. European
Symposium on Artificial Neural Networks ESANN 2021</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:80%;">, page numbers to be
obtained from ToC of this proceedings book, Evere, Belgium, 2021. D-Side
Publications.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:80%;">
Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim¬†D. Smith, and Ilana
Segall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:80%;">Federated learning for ranking browser history suggestions, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:80%;">
Yang Liu, Anbu Huang, Yun Luo, He¬†Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng,
Tianjian Chen, Han Yu, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:80%;">Fedvision: An online visual object detection platform powered by
federated learning, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:80%;">
Li¬†Huang, Andrew¬†L. Shea, Huining Qian, Aditya Masurkar, Hao Deng, and Dianbo
Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:80%;">Patient clustering improves efficiency of federated machine learning
to predict mortality and hospital stay time using distributed electronic
medical records.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Journal of Biomedical Informatics</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:80%;">, 99:103291, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:80%;">
Zhaoping Xiong, Ziqiang Cheng, Chi Xu, Xinyuan Lin, Xiaohong Liu, Dingyan Wang,
Xiaomin Luo, Y.¬†Zhang, Nan Qiao, M.¬†Zheng, and Hualiang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:80%;">Facing small and biased data dilemma in drug discovery with federated
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">bioRxiv</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:80%;">
Shaoqi Chen, Dongyu Xue, Guohui Chuai, Qiang Yang, and Qi¬†Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:80%;">Fl-qsar: a federated learning based qsar prototype for collaborative
drug discovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">bioRxiv</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:80%;">
Suyi Li, Yong Cheng, Y.¬†Liu, Wei Wang, and Tianjian Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:80%;">Abnormal client behavior detection in federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">ArXiv</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:80%;">, abs/1910.09933, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:80%;">
Micah¬†J. Sheller, G.¬†Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon
Bakas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:80%;">Multi-institutional deep learning modeling without sharing patient
data: A feasibility study on brain tumor segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CoRR</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:80%;">, abs/1810.04304, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:80%;">
Wenqi Li, Fausto Milletar√¨, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao
Zhu, Maximilian Baust, Yan Cheng, S√©bastien Ourselin, M.¬†Jorge Cardoso,
and Andrew Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:80%;">Privacy-preserving federated brain tumour segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:80%;">In Heung-Il Suk, Mingxia Liu, Pingkun Yan, and Chunfeng Lian,
editors, </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:80%;">Machine Learning in Medical Imaging</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:80%;">, pages 133‚Äì141, Cham,
2019. Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:80%;">
P.¬†Kairouz and H.¬†B.¬†McMahan et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:80%;">Advances and open problems in federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Foundations and Trends in Machine Learning</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:80%;">, 14(1-2):1‚Äì210,
2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:80%;">
C.¬†Zhang, Y.¬†Xie, H.¬†Bai, B.¬†Yu, W.¬†Li, and Y.¬†Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:80%;">A survey on federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Knowledge-Based Systems</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:80%;">, 216, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:80%;">
Mohammed Aledhari, Rehma Razzak, Reza¬†M. Parizi, and Fahad Saeed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:80%;">Federated learning: A survey on enabling technologies, protocols, and
applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Access</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:80%;">, 8:140699‚Äì140725, 2020.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:80%;">
Tian Li, Anit¬†Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:80%;">Federated learning: Challenges, methods, and future directions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:80%;">IEEE Signal Processing Magazine</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:80%;">, 37(3):50‚Äì60, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.11728" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.11729" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.11729">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.11729" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.11730" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 09:44:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
