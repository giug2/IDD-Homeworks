<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.01516] FedFly: Towards Migration in Edge-based Distributed Federated Learning</title><meta property="og:description" content="Federated learning (FL) is a privacy-preserving distributed machine learning technique that trains models while keeping all the original data generated on devices locally.
Since devices may be resource constrained, off…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedFly: Towards Migration in Edge-based Distributed Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedFly: Towards Migration in Edge-based Distributed Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.01516">

<!--Generated on Tue Mar 19 15:57:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  Edge computing,  Deep neural networks,  Distributed machine learning,  Internet-of-Things.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_typewriter">FedFly</span>: Towards Migration in Edge-based Distributed Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Rehmat Ullah,
Di Wu,
Paul Harvey,
Peter Kilpatrick,
Ivor Spence,
and Blesson Varghese
</span><span class="ltx_author_notes">
Copyright © 20xx IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
This work was supported by funds from Rakuten Mobile, Japan. The last author was also supported by a Royal Society Short Industry Fellowship.
R. Ullah, D. Wu and B. Varghese are with the School of Computer Science, University of St Andrews, UK.
P. Kilpatrick and I. Spence are with the School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, UK.
P. Harvey is with Autonomous Networking Research &amp; Innovation Department, Rakuten Mobile, Japan.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated learning (FL) is a privacy-preserving distributed machine learning technique that trains models while keeping all the original data generated on devices locally.
Since devices may be resource constrained, offloading can be used to improve FL performance by transferring computational workload from devices to edge servers. However, due to mobility, devices participating in FL may leave the network during training and need to connect to a different edge server. This is challenging because the offloaded computations from edge server need to be migrated. In line with this assertion, we present <span id="id2.id1.1" class="ltx_text ltx_font_typewriter">FedFly</span>, which is, to the best of our knowledge, the first work to migrate a deep neural network (DNN) when devices move between edge servers during FL training. Our empirical results on the CIFAR-10 dataset, with both balanced and imbalanced data distribution, support our claims that <span id="id2.id1.2" class="ltx_text ltx_font_typewriter">FedFly</span> can reduce training time by up to 33% when a device moves after 50% of the training is completed, and by up to 45% when 90% of the training is completed when compared to state-of-the-art offloading approach in FL. <span id="id2.id1.3" class="ltx_text ltx_font_typewriter">FedFly</span> has negligible overhead of up to two seconds and does not compromise accuracy. Finally, we highlight a number of open research issues for further investigation. <span id="id2.id1.4" class="ltx_text ltx_font_typewriter">FedFly</span> can be downloaded from <a target="_blank" href="https://github.com/qub-blesson/FedFly" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/qub-blesson/FedFly</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, Edge computing, Deep neural networks, Distributed machine learning, Internet-of-Things.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Internet applications that rely on classic machine learning (ML) techniques gather data from mobile and Internet-of-Things (IoT) devices and process them on servers in cloud data centres. Limited uplink network bandwidth, latency sensitivity of applications and data privacy concerns are key challenges in streaming large volumes of data generated by devices to geographically distant clouds. The concept of Federated Learning (FL) provides privacy by design in an ML technique that collaboratively learns across multiple distributed devices without sending raw data to a central server while processing data locally on devices.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, given the limited availability of resources on many devices, performing FL on such devices is impractical due to increased training times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. One approach is to leverage the computational resources offered by edge servers (located at the edge of the network) for training. The concept of offloading computations of the ML model that may be a Deep Neural Network (DNN) from a device to an edge server for FL by splitting the ML model has been introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (this concept is referred to as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">edge-based FL</span>).
However, a major challenge that has not been considered within the context of edge-based FL is <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">device mobility</span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Mobile devices participating in edge-based FL may need to move from one edge server to another (for example, a smartphone or a drone moving from the connectivity of one edge node to another). This will in turn affect the performance of edge-based FL and result in large training times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Moving a device without migrating the accompanying training data from an edge server to the destination will result in training for the device having to start all over again on the destination server. This would be inefficient resulting in an increased overall training time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Therefore, there is a need for developing techniques that can move devices while accounting for migrating partially trained FL models of a device from one edge server to another.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Research on device mobility has been considered in the context of migration. Migration on the edge has been investigated in the literature, more specifically by exploring VM migration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and container migration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, migration in edge-based FL is minimally considered. This paper presents <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> that addresses the mobility challenge of devices in edge-based FL, and the <span id="S1.p4.1.2" class="ltx_text ltx_font_bold ltx_font_italic">key research contributions</span> are:</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">(1) The <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">technique for migrating DNNs in edge-based FL</span>, which to the best of our knowledge is the first time to be considered in the context of edge-based FL. When a device moves from an edge server to a destination server after 50% of FL training is completed, then the training time using the <span id="S1.p5.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> migration technique is reduced by up to 33% compared to the training time when restarting training on the destination server. Similarly, 45% reduction is obtained when a device moves to a destination server after 90% FL training is completed. It is noted that the original accuracy is maintained.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="color:#000000;">(2) The <span id="S1.p6.1.1.1" class="ltx_text ltx_font_italic">implementation and evaluation of <span id="S1.p6.1.1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> in a hierarchical cloud-edge-device architecture</span> that validates the migration technique of edge-based FL on a lab-based testbed. The experimental results are obtained from a lab-based testbed that includes four IoT devices, two edge servers, and one central server (cloud-like) running the VGG-5 DNN model. The evaluation is done on both a balanced (equal data distribution) and an imbalanced dataset (unequal data distribution). The empirical findings show that <span id="S1.p6.1.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> has a negligible overhead of up to two seconds on the testbed. It is further noted that the accuracy is preserved even when data on devices is imbalanced and the most significant node(s) (i.e., nodes with majority of data) move across edge servers.</span></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The rest of this paper is organized as follows: Section <a href="#S2" title="II Background ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces the concepts of FL and offloading in FL. Section <a href="#S3" title="III Impact of Device Mobility on Edge-based Federated Learning ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the motivation for <span id="S1.p7.1.1" class="ltx_text ltx_font_typewriter">FedFly</span>. Section <a href="#S4" title="IV FedFly for Migration in Hierarchical Edge-based Federated Learning ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> proposes the migration technique for edge-based FL. Section <a href="#S5" title="V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the performance analysis of <span id="S1.p7.1.2" class="ltx_text ltx_font_typewriter">FedFly</span>. Section <a href="#S6" title="VI Conclusion and Future Research Directions ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the paper and highlights directions for future research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides an overview of FL and highlights the benefits of offloading ML computations on to edge servers.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is a privacy-preserving technique in which an ML model is collaboratively trained across several participating distributed devices. All data generated by a device that is used for training resides on local devices. In an FL system, the server initiates a global model and distributes the model parameters to all connected devices. Then each device trains a local version of the ML model using local data. Instead of sending the raw data to the server, the local model parameter updates are sent up to the server. Subsequently, the server computes a weighted average using the parameter updates on the server using the Federated Averaging (FedAvg) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to obtain a new set of parameters for the global model. The updated global model is then sent back down to each device for the next round of training by the edge server. The entire process is repeated until the model converges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In practice, running FL across resource constrained devices, for example in an IoT environment, will result in large training times. Therefore, the concept of partitioning and offloading the ML model, for example for a DNN has been explored for performance efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Split Learning (SL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is one ML technique that leverages this concept.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="color:#000000;">In SL, a DNN is partitioned across the device and server. The DNN layer after which the model is partitioned is referred to as the split layer. The device trains the model up to the split layer and then sends the split layer activation (referred to as smashed data) to the server. The server trains the remaining layers of the DNN using the smashed data. The server performs back-propagation up to the split layer and sends the gradients of the smashed data to the devices. The devices use the gradients to perform back-propagation on the rest of the DNN.</span></p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">However, when multiple devices participate in SL, the devices are trained in a sequential round robin fashion whereby only one device is connected to the server at a time. This limitation is overcome by SplitFed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and FedAdapt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. SplitFed and FedAdapt allow for simultaneous training of all participating devices and at the same time leverage on partitioning the DNN to alleviate the computational burden of training on the device. In addition to the underlying approaches of SplitFed, FedAdapt incorporates a reinforcement learning approach to dynamically identify the DNN layers that need to be offloaded from the device to the edge based on the operational conditions of the environment. In this paper, SplitFed is considered as the baseline.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">SplitFed reduces the amount of computation carried out on the device and is faster than classic SL. However, it is limited in that the challenge of device mobility during training has not been considered. Currently, there is no research in the literature that considers the migration of edge-based FL when devices move between edge servers. The next section highlights the key challenges when using SplitFed.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2111.01516/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="392" height="391" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>System of <span id="S2.F1.2.1" class="ltx_text ltx_font_typewriter">FedFly</span>.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Impact of Device Mobility on Edge-based Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section considers the impact of <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">device mobility</span> on the training time in edge-based FL.
Three contributing factors, namely model training, imbalanced data distribution and frequency of device mobility are considered.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_italic">Model training:</span> Due to mobility, a device participating in FL may disconnect from one edge server and will need to connect to another server at any stage during training. <span id="S3.p2.1.2" class="ltx_text" style="color:#000000;">For example, in the early stages of training, if a device moves, restarting training on a different edge server may result in a small increase in training time. However, if the device had completed a larger portion of its training on an edge server before the device moved, then the training time would significantly increase.</span> A migration mechanism is required so that mobile devices can resume training on the destination edge server rather than starting over.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_italic">Imbalanced data distribution</span>:
In a real edge-based FL system, some devices may have more data than others due to frequent use of specific services or have more resources such as memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Consequently, these devices will make a significant contribution to the quality (overall accuracy) of the global model. However, devices that generate a large amount of data cannot be removed from contributing to training since the eventual accuracy of the global model will be adversely affected. Furthermore, devices with more data will require more training time. As a result, restarting training for the device after it has moved to a different edge server will increase the training time. A migration mechanism that allows such devices to resume training (rather than restarting from the beginning) when moving between edge servers is required to reduce training time while not compromising the global model accuracy.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_italic">Frequency of device mobility</span>:
The frequency with which devices may move between edge servers can have an impact on training time. If the devices move frequently during training, the overall training time will increase because training will need to be restarted on each device after it has moved to a different edge server.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In this paper, we present <span id="S3.p5.1.1" class="ltx_text ltx_font_typewriter">FedFly</span>, that aims to address the <span id="S3.p5.1.2" class="ltx_text ltx_font_italic">device mobility</span> challenge by taking into account the above factors for reducing the training time and maintaining the accuracy of the global model as close to that in classic FL.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2111.01516/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="392" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sequence diagram of <span id="S3.F2.2.1" class="ltx_text ltx_font_typewriter">FedFly</span>.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_typewriter">FedFly</span><span id="S4.2.2" class="ltx_text ltx_font_smallcaps"> for Migration in Hierarchical Edge-based Federated Learning</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section presents <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> (<a target="_blank" href="https://github.com/qub-blesson/FedFly" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/qub-blesson/FedFly</a>), the edge-based distributed FL system that caters for mobility of devices. A hierarchical structure that comprises three entities, namely devices, edge servers, and a central server (cloud-like) is considered. The <span id="S4.p1.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> system is shown in Figure <a href="#S2.F1" title="Figure 1 ‣ II Background ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The following highlights the steps in relation to distributed FL and the mobility of devices within the <span id="S4.p1.1.3" class="ltx_text ltx_font_typewriter">FedFly</span> system:</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_italic">Central server initialization:</span> When training begins, the central server initializes the global model parameters and distributes them to the edge servers. The model parameters are received by the edge servers and passed to the participating devices (Step 1). The training on the devices begins when the devices receive the model parameters from the servers.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_italic">Splitting Deep Neural Networks:</span> When the model is initialized, the DNN that would in classic FL run on a device is split between device and the edge server.
After all devices and edge servers complete local training on the data generated by the device, i.e., forward and backward propagation (Step 2 and Step 3), the local model updates are sent to the central server for global model aggregation (Step 4). A complete forward and backward propagation corresponds to one local epoch (an epoch refers to one complete cycle of an entire dataset on a device through the neural network) of a device for all local data of that device. The central server aggregates the model (Step 5), and then the updated parameters of the global model are sent back to the edge servers and devices for training for a next round of FL training (Step 6).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.2" class="ltx_p">At any point during training, it is possible for a device to move between edge servers. Figure <a href="#S3.F2" title="Figure 2 ‣ III Impact of Device Mobility on Edge-based Federated Learning ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the sequence of activities initiated by <span id="S4.p4.2.1" class="ltx_text ltx_font_typewriter">FedFly</span> when a device needs to move from source edge server to the destination edge server. Assume that a device disconnects from the source edge server after the <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="50^{th}" display="inline"><semantics id="S4.p4.1.m1.1a"><msup id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mn id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">50</mn><mrow id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mi id="S4.p4.1.m1.1.1.3.2" xref="S4.p4.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p4.1.m1.1.1.3.1" xref="S4.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p4.1.m1.1.1.3.3" xref="S4.p4.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">50</cn><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><times id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3.1"></times><ci id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3.2">𝑡</ci><ci id="S4.p4.1.m1.1.1.3.3.cmml" xref="S4.p4.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">50^{th}</annotation></semantics></math> round of training. When a device connects to the destination server without using a migration mechanism, all the training is lost until the <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="50^{th}" display="inline"><semantics id="S4.p4.2.m2.1a"><msup id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mn id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">50</mn><mrow id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml"><mi id="S4.p4.2.m2.1.1.3.2" xref="S4.p4.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.1.1.3.1" xref="S4.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p4.2.m2.1.1.3.3" xref="S4.p4.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">50</cn><apply id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"><times id="S4.p4.2.m2.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.3.1"></times><ci id="S4.p4.2.m2.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.3.2">𝑡</ci><ci id="S4.p4.2.m2.1.1.3.3.cmml" xref="S4.p4.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">50^{th}</annotation></semantics></math> round, and training is restarted on the destination edge server. This is because the destination edge server does not have a copy of the model that was trained on the source edge server. It is necessary to migrate the model data from the source edge server to the destination edge server before training can resume.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> overcomes the mobility challenge by migrating model data from the source edge server to the destination edge server. There are three steps that are considered in <span id="S4.p5.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> when a device starts moving during FL training.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_italic">Notify edge server:</span> When a device starts to move, it notifies the source edge server to prepare data that needs to be migrated to the destination edge server (Step 6). <span id="S4.p6.1.2" class="ltx_text" style="color:#000000;">In this article it is assumed that the moving device knows when to disconnect from the source edge server.</span></p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_italic">Model data checkpoint:</span> The source edge server creates a data checkpoint that includes the epoch number, gradients, model weights, loss value, and state of optimizer (such as Gradient Descent) (Step 7). <span id="S4.p7.1.2" class="ltx_text" style="color:#000000;">The checkpointed data is transferred via a socket to the destination edge server (Step 8).</span></p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Resume training:<span id="S4.p8.1.1.1" class="ltx_text ltx_font_upright"> At the destination edge server, the checkpointed data is received via a socket. When a device connects to the destination edge server, training is resumed from the point where the device started moving at the source edge server (Step 9). </span></span></p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">There are several possible ways to transfer model data between edge-servers. In <span id="S4.p9.1.1" class="ltx_text ltx_font_typewriter">FedFly</span>, the source edge server transfers data directly to the destination edge server, after which the device resumes training. However, in practice the two-edge servers may not be connected or may not have the permission to share data with each other. In this case, the device can then transfer the checkpointed data between edge servers.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section first describes the experimental setup, including the lab-based testbed used for carrying out experiments, and then substantiates the key claims of <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> by presenting and analysing the results obtained.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The testbed includes four devices, two edge servers and one central server. The devices are: (i) two Raspberry Pi 4 (Pi4_1, and Pi4_2) Model B with 1.5GHz quad-core ARM Cortex-A72 CPU, 4GB RAM and 32GB storage, and (ii) two Raspberry Pi 3 (Pi3_1, and Pi3_2) Model B with 1.2GHz quad-core ARM Cortex-A53 CPU, 1GB RAM and 32GB storage. The edge servers comprise: (i) a 2.3GHz quad-core Intel i5 CPU, 8GB RAM and 256GB storage, and (ii) a 2.3GHz quad-core Intel i7 CPU, 16GB RAM and 500GB storage. The central server has a 2.9GHz quad-core Intel i5 CPU, 16GB RAM and 1TB storage. All Raspberry Pis have the same version of Raspbian GNU/Linux 10 (Buster) operating system, Python version 3.7 and PyTorch version 1.4.0. The edge servers and the central server have the same version of Python and PyTorch using Anaconda. <span id="S5.SS1.p1.1.1" class="ltx_text" style="color:#000000;">All devices are connected to the servers in a Wi-Fi network with an average available bandwidth of 75Mbps</span>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The DNN model used is VGG-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and the CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> dataset is used as input with size <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="3@32\times 32" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mrow id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml"><mn id="S5.SS1.p2.1.m1.1.1.2.2" xref="S5.SS1.p2.1.m1.1.1.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.1.1.2.1" xref="S5.SS1.p2.1.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p2.1.m1.1.1.2.3" xref="S5.SS1.p2.1.m1.1.1.2.3.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.1.1.2.1a" xref="S5.SS1.p2.1.m1.1.1.2.1.cmml">​</mo><mn id="S5.SS1.p2.1.m1.1.1.2.4" xref="S5.SS1.p2.1.m1.1.1.2.4.cmml">32</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><apply id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2"><times id="S5.SS1.p2.1.m1.1.1.2.1.cmml" xref="S5.SS1.p2.1.m1.1.1.2.1"></times><cn type="integer" id="S5.SS1.p2.1.m1.1.1.2.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2.2">3</cn><ci id="S5.SS1.p2.1.m1.1.1.2.3.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3">@</ci><cn type="integer" id="S5.SS1.p2.1.m1.1.1.2.4.cmml" xref="S5.SS1.p2.1.m1.1.1.2.4">32</cn></apply><cn type="integer" id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">3@32\times 32</annotation></semantics></math> and a batch size of 100 is used for all experiments. The CIFAR-10 dataset contains 50K training and 10K testing samples that consist of color images of ten objects (classes), including plane, car, bird, cat, deer, dog, frog, horse, ship, and truck. The standard FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> aggregation method is used, and the model parameters are updated using Stochastic Gradient Descent (SGD), with a learning rate of 0.01 and a momentum of 0.9.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Empirical Results and Discussion</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we demonstrate the performance of <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> by comparing it with SplitFed in terms of device training time and model accuracy. We validate our claims using balanced and imbalanced datasets at various stages (i.e., 50% and 90%) of FL training.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Effect of mobility on device training time:</span>
When a device moves between edge servers, factors such as training stage and the dataset available on the device can affect training time. In this experiment, we validate the training time claim by generating 25% and 50% of the data required for training on a single device (i.e., Pi3_1, Pi3_2, Pi4_1 and Pi4_2) with training stages at 50% and 90% as shown in Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) and Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b).</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.01516/assets/x3.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="216" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.01516/assets/x4.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="216" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.01516/assets/x5.png" id="S5.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="217" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(a) Device training time per round when 25% of the dataset is required for training on a mobile device, (b) Device training time per round when 50% of the dataset is required for training on a mobile device
<span id="S5.F3.2.1" class="ltx_text" style="color:#000000;">(c) Device training time per round by varying SPs with 25% of the dataset on a mobile device and at 90% of the FL training.</span></figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) shows the effects of device mobility on device training time when 25% of the dataset is required for training on a single device, as well as device movement when 50% and 90% of the training is completed. It is evident from Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) that <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> always outperforms SplitFed, in which the training is restarted at the destination edge server. When we move Pi3_1 when 50% of the training is done, the training time is reduced by up to 33% per round. However, when we move Pi3_2 with the same dataset but 90% of the training is completed, the training time is reduced by up to 45% per round. We also move devices (Pi4_1 and Pi4_2) when 50% and 90% of the training is done, and the training time is reduced by up to 33% and 45% per round, respectively.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), shows the effects of device mobility on device training time when 50% of the dataset is required for training on a single device, as well as device movement when 50% and 90% of the training is completed. It can be seen in Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) that training time on devices is longer than on devices in Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a). This is due to the fact that 50% of the dataset is used for training on mobile devices, which is comparably larger than used for devices in Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a). It has been demonstrated from Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) and Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) that <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> can save a significant amount of training time when compared to SplitFed.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text" style="color:#000000;">Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c) highlights the system performance with device mobility by varying the split points (SP). SP1 denotes the first convolutional layer on devices, SP2 denotes the first two convolutional layers on devices, and SP3 denotes the first three convolutional layers on devices, with the remaining layers on edge-servers. It should be noted that in the experiments illustrated in Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) and Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), all devices and edge servers have fixed split points (i.e., SP2). Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c) depicts that SPs impact the system performance, in terms of training time. By changing the SPs from SP1 to SP3, we note a significant increase in training time. This is because as the number of layers (i.e., computation) on devices and servers increases or decreases, the training time on devices or servers increases or decreases accordingly.
In all cases, <span id="S5.SS2.p5.1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> saves a significant amount of training time when compared to SplitFed. The transfer time is still up to two seconds. This is because the VGG-5 model is used in the experiments, and the data that is checkpointed did not change significantly by varying SPs.</span></p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Effect of mobility on global accuracy:</span>
In this experiment, we verify the accuracy of the global model when a device moves frequently between edge servers.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.11" class="ltx_p">We ran this experiment for a total of 100 rounds, with a mobile device holding 20% of the dataset and 50% of the dataset. We move the device at various rounds during 100 rounds of training, such as at the <math id="S5.SS2.p7.1.m1.1" class="ltx_Math" alttext="10^{th}" display="inline"><semantics id="S5.SS2.p7.1.m1.1a"><msup id="S5.SS2.p7.1.m1.1.1" xref="S5.SS2.p7.1.m1.1.1.cmml"><mn id="S5.SS2.p7.1.m1.1.1.2" xref="S5.SS2.p7.1.m1.1.1.2.cmml">10</mn><mrow id="S5.SS2.p7.1.m1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.3.cmml"><mi id="S5.SS2.p7.1.m1.1.1.3.2" xref="S5.SS2.p7.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.1.m1.1.1.3.1" xref="S5.SS2.p7.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.1.m1.1.1.3.3" xref="S5.SS2.p7.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><apply id="S5.SS2.p7.1.m1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.1.m1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.2">10</cn><apply id="S5.SS2.p7.1.m1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3"><times id="S5.SS2.p7.1.m1.1.1.3.1.cmml" xref="S5.SS2.p7.1.m1.1.1.3.1"></times><ci id="S5.SS2.p7.1.m1.1.1.3.2.cmml" xref="S5.SS2.p7.1.m1.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.1.m1.1.1.3.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">10^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.2.m2.1" class="ltx_Math" alttext="20^{th}" display="inline"><semantics id="S5.SS2.p7.2.m2.1a"><msup id="S5.SS2.p7.2.m2.1.1" xref="S5.SS2.p7.2.m2.1.1.cmml"><mn id="S5.SS2.p7.2.m2.1.1.2" xref="S5.SS2.p7.2.m2.1.1.2.cmml">20</mn><mrow id="S5.SS2.p7.2.m2.1.1.3" xref="S5.SS2.p7.2.m2.1.1.3.cmml"><mi id="S5.SS2.p7.2.m2.1.1.3.2" xref="S5.SS2.p7.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.2.m2.1.1.3.1" xref="S5.SS2.p7.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.2.m2.1.1.3.3" xref="S5.SS2.p7.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.2.m2.1b"><apply id="S5.SS2.p7.2.m2.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.2.m2.1.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.2.m2.1.1.2.cmml" xref="S5.SS2.p7.2.m2.1.1.2">20</cn><apply id="S5.SS2.p7.2.m2.1.1.3.cmml" xref="S5.SS2.p7.2.m2.1.1.3"><times id="S5.SS2.p7.2.m2.1.1.3.1.cmml" xref="S5.SS2.p7.2.m2.1.1.3.1"></times><ci id="S5.SS2.p7.2.m2.1.1.3.2.cmml" xref="S5.SS2.p7.2.m2.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.2.m2.1.1.3.3.cmml" xref="S5.SS2.p7.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.2.m2.1c">20^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.3.m3.1" class="ltx_Math" alttext="30^{th}" display="inline"><semantics id="S5.SS2.p7.3.m3.1a"><msup id="S5.SS2.p7.3.m3.1.1" xref="S5.SS2.p7.3.m3.1.1.cmml"><mn id="S5.SS2.p7.3.m3.1.1.2" xref="S5.SS2.p7.3.m3.1.1.2.cmml">30</mn><mrow id="S5.SS2.p7.3.m3.1.1.3" xref="S5.SS2.p7.3.m3.1.1.3.cmml"><mi id="S5.SS2.p7.3.m3.1.1.3.2" xref="S5.SS2.p7.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.3.m3.1.1.3.1" xref="S5.SS2.p7.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.3.m3.1.1.3.3" xref="S5.SS2.p7.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.3.m3.1b"><apply id="S5.SS2.p7.3.m3.1.1.cmml" xref="S5.SS2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.3.m3.1.1.1.cmml" xref="S5.SS2.p7.3.m3.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.3.m3.1.1.2.cmml" xref="S5.SS2.p7.3.m3.1.1.2">30</cn><apply id="S5.SS2.p7.3.m3.1.1.3.cmml" xref="S5.SS2.p7.3.m3.1.1.3"><times id="S5.SS2.p7.3.m3.1.1.3.1.cmml" xref="S5.SS2.p7.3.m3.1.1.3.1"></times><ci id="S5.SS2.p7.3.m3.1.1.3.2.cmml" xref="S5.SS2.p7.3.m3.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.3.m3.1.1.3.3.cmml" xref="S5.SS2.p7.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.3.m3.1c">30^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.4.m4.1" class="ltx_Math" alttext="40^{th}" display="inline"><semantics id="S5.SS2.p7.4.m4.1a"><msup id="S5.SS2.p7.4.m4.1.1" xref="S5.SS2.p7.4.m4.1.1.cmml"><mn id="S5.SS2.p7.4.m4.1.1.2" xref="S5.SS2.p7.4.m4.1.1.2.cmml">40</mn><mrow id="S5.SS2.p7.4.m4.1.1.3" xref="S5.SS2.p7.4.m4.1.1.3.cmml"><mi id="S5.SS2.p7.4.m4.1.1.3.2" xref="S5.SS2.p7.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.4.m4.1.1.3.1" xref="S5.SS2.p7.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.4.m4.1.1.3.3" xref="S5.SS2.p7.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.4.m4.1b"><apply id="S5.SS2.p7.4.m4.1.1.cmml" xref="S5.SS2.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.4.m4.1.1.1.cmml" xref="S5.SS2.p7.4.m4.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.4.m4.1.1.2.cmml" xref="S5.SS2.p7.4.m4.1.1.2">40</cn><apply id="S5.SS2.p7.4.m4.1.1.3.cmml" xref="S5.SS2.p7.4.m4.1.1.3"><times id="S5.SS2.p7.4.m4.1.1.3.1.cmml" xref="S5.SS2.p7.4.m4.1.1.3.1"></times><ci id="S5.SS2.p7.4.m4.1.1.3.2.cmml" xref="S5.SS2.p7.4.m4.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.4.m4.1.1.3.3.cmml" xref="S5.SS2.p7.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.4.m4.1c">40^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.5.m5.1" class="ltx_Math" alttext="50^{th}" display="inline"><semantics id="S5.SS2.p7.5.m5.1a"><msup id="S5.SS2.p7.5.m5.1.1" xref="S5.SS2.p7.5.m5.1.1.cmml"><mn id="S5.SS2.p7.5.m5.1.1.2" xref="S5.SS2.p7.5.m5.1.1.2.cmml">50</mn><mrow id="S5.SS2.p7.5.m5.1.1.3" xref="S5.SS2.p7.5.m5.1.1.3.cmml"><mi id="S5.SS2.p7.5.m5.1.1.3.2" xref="S5.SS2.p7.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.5.m5.1.1.3.1" xref="S5.SS2.p7.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.5.m5.1.1.3.3" xref="S5.SS2.p7.5.m5.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.5.m5.1b"><apply id="S5.SS2.p7.5.m5.1.1.cmml" xref="S5.SS2.p7.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.5.m5.1.1.1.cmml" xref="S5.SS2.p7.5.m5.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.5.m5.1.1.2.cmml" xref="S5.SS2.p7.5.m5.1.1.2">50</cn><apply id="S5.SS2.p7.5.m5.1.1.3.cmml" xref="S5.SS2.p7.5.m5.1.1.3"><times id="S5.SS2.p7.5.m5.1.1.3.1.cmml" xref="S5.SS2.p7.5.m5.1.1.3.1"></times><ci id="S5.SS2.p7.5.m5.1.1.3.2.cmml" xref="S5.SS2.p7.5.m5.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.5.m5.1.1.3.3.cmml" xref="S5.SS2.p7.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.5.m5.1c">50^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.6.m6.1" class="ltx_Math" alttext="60^{th}" display="inline"><semantics id="S5.SS2.p7.6.m6.1a"><msup id="S5.SS2.p7.6.m6.1.1" xref="S5.SS2.p7.6.m6.1.1.cmml"><mn id="S5.SS2.p7.6.m6.1.1.2" xref="S5.SS2.p7.6.m6.1.1.2.cmml">60</mn><mrow id="S5.SS2.p7.6.m6.1.1.3" xref="S5.SS2.p7.6.m6.1.1.3.cmml"><mi id="S5.SS2.p7.6.m6.1.1.3.2" xref="S5.SS2.p7.6.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.6.m6.1.1.3.1" xref="S5.SS2.p7.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.6.m6.1.1.3.3" xref="S5.SS2.p7.6.m6.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.6.m6.1b"><apply id="S5.SS2.p7.6.m6.1.1.cmml" xref="S5.SS2.p7.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.6.m6.1.1.1.cmml" xref="S5.SS2.p7.6.m6.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.6.m6.1.1.2.cmml" xref="S5.SS2.p7.6.m6.1.1.2">60</cn><apply id="S5.SS2.p7.6.m6.1.1.3.cmml" xref="S5.SS2.p7.6.m6.1.1.3"><times id="S5.SS2.p7.6.m6.1.1.3.1.cmml" xref="S5.SS2.p7.6.m6.1.1.3.1"></times><ci id="S5.SS2.p7.6.m6.1.1.3.2.cmml" xref="S5.SS2.p7.6.m6.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.6.m6.1.1.3.3.cmml" xref="S5.SS2.p7.6.m6.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.6.m6.1c">60^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.7.m7.1" class="ltx_Math" alttext="70^{th}" display="inline"><semantics id="S5.SS2.p7.7.m7.1a"><msup id="S5.SS2.p7.7.m7.1.1" xref="S5.SS2.p7.7.m7.1.1.cmml"><mn id="S5.SS2.p7.7.m7.1.1.2" xref="S5.SS2.p7.7.m7.1.1.2.cmml">70</mn><mrow id="S5.SS2.p7.7.m7.1.1.3" xref="S5.SS2.p7.7.m7.1.1.3.cmml"><mi id="S5.SS2.p7.7.m7.1.1.3.2" xref="S5.SS2.p7.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.7.m7.1.1.3.1" xref="S5.SS2.p7.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.7.m7.1.1.3.3" xref="S5.SS2.p7.7.m7.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.7.m7.1b"><apply id="S5.SS2.p7.7.m7.1.1.cmml" xref="S5.SS2.p7.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.7.m7.1.1.1.cmml" xref="S5.SS2.p7.7.m7.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.7.m7.1.1.2.cmml" xref="S5.SS2.p7.7.m7.1.1.2">70</cn><apply id="S5.SS2.p7.7.m7.1.1.3.cmml" xref="S5.SS2.p7.7.m7.1.1.3"><times id="S5.SS2.p7.7.m7.1.1.3.1.cmml" xref="S5.SS2.p7.7.m7.1.1.3.1"></times><ci id="S5.SS2.p7.7.m7.1.1.3.2.cmml" xref="S5.SS2.p7.7.m7.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.7.m7.1.1.3.3.cmml" xref="S5.SS2.p7.7.m7.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.7.m7.1c">70^{th}</annotation></semantics></math>, <math id="S5.SS2.p7.8.m8.1" class="ltx_Math" alttext="80^{th}" display="inline"><semantics id="S5.SS2.p7.8.m8.1a"><msup id="S5.SS2.p7.8.m8.1.1" xref="S5.SS2.p7.8.m8.1.1.cmml"><mn id="S5.SS2.p7.8.m8.1.1.2" xref="S5.SS2.p7.8.m8.1.1.2.cmml">80</mn><mrow id="S5.SS2.p7.8.m8.1.1.3" xref="S5.SS2.p7.8.m8.1.1.3.cmml"><mi id="S5.SS2.p7.8.m8.1.1.3.2" xref="S5.SS2.p7.8.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.8.m8.1.1.3.1" xref="S5.SS2.p7.8.m8.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.8.m8.1.1.3.3" xref="S5.SS2.p7.8.m8.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.8.m8.1b"><apply id="S5.SS2.p7.8.m8.1.1.cmml" xref="S5.SS2.p7.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.8.m8.1.1.1.cmml" xref="S5.SS2.p7.8.m8.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.8.m8.1.1.2.cmml" xref="S5.SS2.p7.8.m8.1.1.2">80</cn><apply id="S5.SS2.p7.8.m8.1.1.3.cmml" xref="S5.SS2.p7.8.m8.1.1.3"><times id="S5.SS2.p7.8.m8.1.1.3.1.cmml" xref="S5.SS2.p7.8.m8.1.1.3.1"></times><ci id="S5.SS2.p7.8.m8.1.1.3.2.cmml" xref="S5.SS2.p7.8.m8.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.8.m8.1.1.3.3.cmml" xref="S5.SS2.p7.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.8.m8.1c">80^{th}</annotation></semantics></math>, and <math id="S5.SS2.p7.9.m9.1" class="ltx_Math" alttext="90^{th}" display="inline"><semantics id="S5.SS2.p7.9.m9.1a"><msup id="S5.SS2.p7.9.m9.1.1" xref="S5.SS2.p7.9.m9.1.1.cmml"><mn id="S5.SS2.p7.9.m9.1.1.2" xref="S5.SS2.p7.9.m9.1.1.2.cmml">90</mn><mrow id="S5.SS2.p7.9.m9.1.1.3" xref="S5.SS2.p7.9.m9.1.1.3.cmml"><mi id="S5.SS2.p7.9.m9.1.1.3.2" xref="S5.SS2.p7.9.m9.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.9.m9.1.1.3.1" xref="S5.SS2.p7.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.9.m9.1.1.3.3" xref="S5.SS2.p7.9.m9.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.9.m9.1b"><apply id="S5.SS2.p7.9.m9.1.1.cmml" xref="S5.SS2.p7.9.m9.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.9.m9.1.1.1.cmml" xref="S5.SS2.p7.9.m9.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.9.m9.1.1.2.cmml" xref="S5.SS2.p7.9.m9.1.1.2">90</cn><apply id="S5.SS2.p7.9.m9.1.1.3.cmml" xref="S5.SS2.p7.9.m9.1.1.3"><times id="S5.SS2.p7.9.m9.1.1.3.1.cmml" xref="S5.SS2.p7.9.m9.1.1.3.1"></times><ci id="S5.SS2.p7.9.m9.1.1.3.2.cmml" xref="S5.SS2.p7.9.m9.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.9.m9.1.1.3.3.cmml" xref="S5.SS2.p7.9.m9.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.9.m9.1c">90^{th}</annotation></semantics></math> rounds.
Figure <a href="#S5.F4" title="Figure 4 ‣ V-B Empirical Results and Discussion ‣ V Evaluation ‣ FedFly: Towards Migration in Edge-based Distributed Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> clearly shows that there is no effect on accuracy. <span id="S5.SS2.p7.11.1" class="ltx_text ltx_font_typewriter">FedFly</span> and SplitFed both maintain accuracy when a device moves between edge servers holding 20% and 50% of the datasets. In the case of SplitFed, the training is restarted at the destination edge server without any accuracy loss. This is because the device obtains the updated model parameters from the central server and restarts training at the destination edge server. For example, if a device moves at the <math id="S5.SS2.p7.10.m10.1" class="ltx_Math" alttext="10^{th}" display="inline"><semantics id="S5.SS2.p7.10.m10.1a"><msup id="S5.SS2.p7.10.m10.1.1" xref="S5.SS2.p7.10.m10.1.1.cmml"><mn id="S5.SS2.p7.10.m10.1.1.2" xref="S5.SS2.p7.10.m10.1.1.2.cmml">10</mn><mrow id="S5.SS2.p7.10.m10.1.1.3" xref="S5.SS2.p7.10.m10.1.1.3.cmml"><mi id="S5.SS2.p7.10.m10.1.1.3.2" xref="S5.SS2.p7.10.m10.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.10.m10.1.1.3.1" xref="S5.SS2.p7.10.m10.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.10.m10.1.1.3.3" xref="S5.SS2.p7.10.m10.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.10.m10.1b"><apply id="S5.SS2.p7.10.m10.1.1.cmml" xref="S5.SS2.p7.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.10.m10.1.1.1.cmml" xref="S5.SS2.p7.10.m10.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.10.m10.1.1.2.cmml" xref="S5.SS2.p7.10.m10.1.1.2">10</cn><apply id="S5.SS2.p7.10.m10.1.1.3.cmml" xref="S5.SS2.p7.10.m10.1.1.3"><times id="S5.SS2.p7.10.m10.1.1.3.1.cmml" xref="S5.SS2.p7.10.m10.1.1.3.1"></times><ci id="S5.SS2.p7.10.m10.1.1.3.2.cmml" xref="S5.SS2.p7.10.m10.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.10.m10.1.1.3.3.cmml" xref="S5.SS2.p7.10.m10.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.10.m10.1c">10^{th}</annotation></semantics></math> round, the central server has the updated model parameters until the <math id="S5.SS2.p7.11.m11.1" class="ltx_Math" alttext="10^{th}" display="inline"><semantics id="S5.SS2.p7.11.m11.1a"><msup id="S5.SS2.p7.11.m11.1.1" xref="S5.SS2.p7.11.m11.1.1.cmml"><mn id="S5.SS2.p7.11.m11.1.1.2" xref="S5.SS2.p7.11.m11.1.1.2.cmml">10</mn><mrow id="S5.SS2.p7.11.m11.1.1.3" xref="S5.SS2.p7.11.m11.1.1.3.cmml"><mi id="S5.SS2.p7.11.m11.1.1.3.2" xref="S5.SS2.p7.11.m11.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p7.11.m11.1.1.3.1" xref="S5.SS2.p7.11.m11.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p7.11.m11.1.1.3.3" xref="S5.SS2.p7.11.m11.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.11.m11.1b"><apply id="S5.SS2.p7.11.m11.1.1.cmml" xref="S5.SS2.p7.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.11.m11.1.1.1.cmml" xref="S5.SS2.p7.11.m11.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p7.11.m11.1.1.2.cmml" xref="S5.SS2.p7.11.m11.1.1.2">10</cn><apply id="S5.SS2.p7.11.m11.1.1.3.cmml" xref="S5.SS2.p7.11.m11.1.1.3"><times id="S5.SS2.p7.11.m11.1.1.3.1.cmml" xref="S5.SS2.p7.11.m11.1.1.3.1"></times><ci id="S5.SS2.p7.11.m11.1.1.3.2.cmml" xref="S5.SS2.p7.11.m11.1.1.3.2">𝑡</ci><ci id="S5.SS2.p7.11.m11.1.1.3.3.cmml" xref="S5.SS2.p7.11.m11.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.11.m11.1c">10^{th}</annotation></semantics></math> round, and when a device connects to the destination edge server, it receives updated parameters from the central server. Only the training is restarted, which increases the training time but has no effect on accuracy.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.1" class="ltx_p"><span id="S5.SS2.p8.1.1" class="ltx_text ltx_font_typewriter">FedFly</span>, on the other hand, transfers the data to the destination edge server, where training is resumed and maintains the same level of accuracy as SplitFed. The training, however, is not repeated at the destination edge server.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2111.01516/assets/x6.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Global accuracy when 20% and 50% of datasets are required for training on a mobile device for 100 rounds of training.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Summary of the evaluation results</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> performance is affected by a number of factors, including i) balanced and imbalanced datasets on devices, <span id="S5.SS3.p1.1.2" class="ltx_text" style="color:#000000;">ii) varying the SPs,</span> iii) the frequency with which devices move; and iv) the model training stages. Our experimental results provide the following insights:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">In comparison to SplitFed, <span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> reduces the training time per round by up to 33% when a device moves after 50% of the training is completed, and by up to 45% when 90% of the training is completed.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> maintains global accuracy as does SplitFed and there is no accuracy loss.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> results in up to two seconds overhead, which is the time it takes to transfer data between edge servers during migration. This overhead is negligible when compared to the device training time when training is restarted at the destination server. <span id="S5.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">The reduction in training time and overhead reported in this paper are based on experiments carried out on the lab-based testbed.</span></p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Research Directions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The FL system is hindered by two major issues: training time and accuracy. This becomes more challenging when a device moves during FL training and especially when a DNN is partitioned between device and edge server. This paper has proposed <span id="S6.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span>, which for the first time addresses the <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">device mobility</span> challenge during FL training, particularly in edge-based FL. We develop a prototype on a lab-based testbed, that upholds and validates our claims in terms of training time and accuracy using balanced and imbalanced datasets when compared to state-of-the-art SL approach called SplitFed. Our empirical results reveal that <span id="S6.p1.1.3" class="ltx_text ltx_font_typewriter">FedFly</span> introduces a negligible overhead but saves a significant amount of training time while maintaining accuracy.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Future Research Directions</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">We develop <span id="S6.SS0.SSSx1.p1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> for migration in edge-based distributed FL —but this is only the tip of the iceberg of the opportunities it makes available. What follows are a few research questions that we may further investigate.</p>
</div>
<div id="S6.SS0.SSSx1.p2" class="ltx_para">
<p id="S6.SS0.SSSx1.p2.1" class="ltx_p"><span id="S6.SS0.SSSx1.p2.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Multiple devices mobility:<span id="S6.SS0.SSSx1.p2.1.1.1" class="ltx_text ltx_font_upright"> Further challenges may occur in the FL setting if multiple devices try to move at the same time with various data distribution at each node. The impact of a large number of devices on training time and accuracy will be investigated further in order to realise migration in practical FL systems.</span></span></p>
</div>
<div id="S6.SS0.SSSx1.p3" class="ltx_para">
<p id="S6.SS0.SSSx1.p3.1" class="ltx_p"><span id="S6.SS0.SSSx1.p3.1.1" class="ltx_text ltx_font_italic">Hardware heterogeneity:</span> In <span id="S6.SS0.SSSx1.p3.1.2" class="ltx_text ltx_font_typewriter">FedFly</span>, we perform migration in a homogeneous environment, i.e., the hardware at the edge servers is of the same instruction set architecture (ISA). However, in practical scenarios, edge servers are often built with CPUs of different ISAs. As a result, a DNN model that has been natively trained for one ISA cannot be moved to another, making migration to the destination edge server difficult. Migration at runtime across edge servers featuring CPUs of different ISAs, such as ARM and x86, requires further investigation.</p>
</div>
<div id="S6.SS0.SSSx1.p4" class="ltx_para">
<p id="S6.SS0.SSSx1.p4.1" class="ltx_p"><span id="S6.SS0.SSSx1.p4.1.1" class="ltx_text ltx_font_italic">Neural network optimization:</span> In practice, the destination edge server may not have enough resources to run the DNN model, meaning that the destination edge server resource is not equivalent to the source edge server resource. How to move DNN on the fly so that the DNN model can run on the destination edge server with limited resources and how to optimise the DNN without impacting its accuracy may be further investigated.</p>
</div>
<div id="S6.SS0.SSSx1.p5" class="ltx_para">
<p id="S6.SS0.SSSx1.p5.1" class="ltx_p"><span id="S6.SS0.SSSx1.p5.1.1" class="ltx_text ltx_font_italic">Asynchronous training:</span> <span id="S6.SS0.SSSx1.p5.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> currently focuses on synchronous training in edge-based distributed FL. However, the practical FL scenario shows significant heterogeneity in terms of computation resources, hardware, dataset distribution, and communication, etc. It would be worthwhile to investigate the migration issues for asynchronous training in edge-based distributed FL.</p>
</div>
<div id="S6.SS0.SSSx1.p6" class="ltx_para">
<p id="S6.SS0.SSSx1.p6.1" class="ltx_p"><span id="S6.SS0.SSSx1.p6.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Communication overhead:<span id="S6.SS0.SSSx1.p6.1.1.1" class="ltx_text ltx_font_upright"> <span id="S6.SS0.SSSx1.p6.1.1.1.1" class="ltx_text ltx_font_typewriter">FedFly</span> does not impose any communication challenges, as training from the source edge server is resumed with a 2 second overhead at the destination edge server. However, communication challenges may arise as a result of the hierarchical cloud-edge-device architecture in which <span id="S6.SS0.SSSx1.p6.1.1.1.2" class="ltx_text ltx_font_typewriter">FedFly</span> operates since the volume of communication between the cloud, edge servers and devices increase. This may result in a higher communication overhead since model parameters are frequently shared between the cloud to edge to device and vice-versa. Efficient mechanisms for reducing communication overhead between devices, edge servers, and the cloud will be considered in the future.</span></span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing
Magazine</em>, vol. 37, no. 3, pp. 50–60, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Thapa, M. A. P. Chamikara, and S. Camtepe, “Splitfed: When federated
learning meets split learning,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.12088</em>,
2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Imteaj, U. Thakker, S. Wang, J. Li, and M. H. Amini, “A survey on federated
learning for resource-constrained iot devices,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of
Things Journal</em>, vol. 9, no. 1, pp. 1–24, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and
H. Vincent Poor, “Federated learning for internet of things: A comprehensive
survey,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys Tutorials</em>, vol. 23, no. 3, pp.
1622–1658, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Xia, W. Ye, Z. Tao, J. Wu, and Q. Li, “A survey of federated learning for
edge computing: Research problems and solutions,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">High-Confidence
Computing</em>, vol. 1, no. 1, p. 100008, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F. Zhang, G. Liu, X. Fu, and R. Yahyapour, “A survey on virtual machine
migration: Challenges, techniques, and open issues,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Surveys &amp; Tutorials</em>, vol. 20, no. 2, pp. 1206–1243, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
G. Singh and P. Singh, “A taxonomy and survey on container migration
techniques in cloud computing,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Sustainable Development Through
Engineering Innovations</em>.   Springer,
2021, pp. 419–429.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Nadgowda, S. Suneja, N. Bila, and C. Isci, “Voyager: Complete container
state migration,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 37th International Conference on
Distributed Computing Systems (ICDCS)</em>, 2017, pp. 2137–2142.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. of 20th Artificial Intelligence and Statistics</em>, 2017,
pp. 1273–1282.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Gao, M. Kim, S. Abuadbba, Y. Kim, C. Thapa, K. Kim, S. A. Camtepe, H. Kim,
and S. Nepal, “End-to-end evaluation of federated learning and split
learning for internet of things,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.13376</em>,
2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Lockhart, P. Harvey, P. Imai, P. Willis, and B. Varghese, “Scission:
Performance-driven and context-aware cloud-edge distribution of deep neural
networks,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/ACM 13th International Conference on Utility
and Cloud Computing (UCC)</em>, 2020, pp. 257–268.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar, “Split learning for health:
Distributed deep learning without sharing raw patient data,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv:1812.00564</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. Wu, R. Ullah, P. Harvey, P. Kilpatrick, I. Spence, and B. Varghese,
“Fedadapt: Adaptive offloading for iot devices in federated learning,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.04271</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Krizhevsky, G. Hinton <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning multiple layers of features
from tiny images,” Technical report, University of Toronto, 2009.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Rehmat Ullah</span> 
is a research fellow at the University of St Andrews, UK. His research focuses on the broader areas of network and distributed systems, particularly edge computing and information centric networking, with a recent focus on federated learning for edge computing systems. More information is available from <a href="www.rehmatkhan.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.rehmatkhan.com</a>.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Di Wu</span>  is currently pursuing a PhD degree in computer science at University of St Andrews, UK. His major interests are in the areas of federated learning, distributed machine learning, edge computing, model compression, and Internet-of-Things.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Paul Harvey</span>  is one of
the original founders of the Autonomous Networks
Research and Innovation Lab in Rakuten Mobile,
Japan, and is a co-chair in the ITU focus group
on autonomous networks. He is Research Lead at the Autonomous Networking Research and Innovation Department, Rakuten Mobile, Japan.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Peter Kilpatrick</span>  is a Reader in computer science at Queen’s University Belfast, UK. His interests include parallel programming models and cloud and edge computing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td">
<span id="tab5.1.1.1.1" class="ltx_inline-block">
<span id="tab5.1.1.1.1.1" class="ltx_p"><span id="tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Ivor Spence</span>  is a Reader in computer science at Queen’s University Belfast, UK, where he leads the artificial intelligence (AI) research theme with a focus on heterogeneous computing systems for AI.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab6" class="ltx_float biography">
<table id="tab6.1" class="ltx_tabular">
<tr id="tab6.1.1" class="ltx_tr">
<td id="tab6.1.1.1" class="ltx_td">
<span id="tab6.1.1.1.1" class="ltx_inline-block">
<span id="tab6.1.1.1.1.1" class="ltx_p"><span id="tab6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Blesson Varghese</span> 
is a Reader in computer science at the University of St Andrews, UK, and the Principal Investigator of the Edge Computing Hub. His recent interests are at the intersection of the cloud-edge-device continuum and machine learning. More information is available from <a href="www.blessonv.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.blessonv.com</a>.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.01514" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.01516" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.01516">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.01516" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.01517" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 15:57:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
