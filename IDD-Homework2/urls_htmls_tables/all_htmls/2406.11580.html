<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation</title>
<!--Generated on Mon Jun 17 14:12:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.11580v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S1" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S2" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S3" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Comparison DA+SQM and MQM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S4" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Error Span Annotation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S4.SS0.SSS0.Px1" title="In 4 Error Span Annotation ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Annotation process.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S4.SS0.SSS0.Px2" title="In 4 Error Span Annotation ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Segment-level scores.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S4.SS0.SSS0.Px3" title="In 4 Error Span Annotation ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Advantages.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S5" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS1" title="In 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Score distribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS2" title="In 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>System ranking capabilities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS3" title="In 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Agreement with other protocols</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS4" title="In 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Quality of annotations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS4.SSS0.Px1" title="In 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Intra annotator agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS4.SSS0.Px2" title="In 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Inter annotator agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS4.SSS0.Px3" title="In 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Error span agreement.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS4.SSS0.Px4" title="In 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Quality control.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS5" title="In 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Annotation time</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.SS5.SSS0.Px1" title="In 6.5 Annotation time ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Speedup during annotations.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S7" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>User Guidelines</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS1" title="In Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>ESA (Error Span Annotations)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS1.SSS0.Px1" title="In A.1 ESA (Error Span Annotations) ‣ Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Higlighting errors:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS1.SSS0.Px2" title="In A.1 ESA (Error Span Annotations) ‣ Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Score:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS2" title="In Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>MQM (Multidimensional Quality Metrics)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS2.SSS0.Px1" title="In A.2 MQM (Multidimensional Quality Metrics) ‣ Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Higlighting errors:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1.SS2.SSS0.Px2" title="In A.2 MQM (Multidimensional Quality Metrics) ‣ Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title">Error types:</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2" title="In Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.SS1" title="In Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>From error spans to final score</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\DeclareCaptionType</span>
<p class="ltx_p" id="p1.2">example[Example][List of examples]
<span class="ltx_ERROR undefined" id="p1.2.1">\DeclareCaptionType</span>prompt[Prompt][List of prompts]
<span class="ltx_ERROR undefined" id="p1.2.2">\DeclareCaptionType</span>equ[Equation][List of equations]























   











<span class="ltx_ERROR undefined" id="p1.2.3">\contourlength</span>0.2pt



</p>
</div>
<h1 class="ltx_title ltx_title_document">
<span class="ltx_inline-block ltx_transformed_outer" id="id17.id1" style="width:7.5pt;height:7.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.3pt,-0.3pt) scale(1.1,1.1) ;">
<span class="ltx_p" id="id17.id1.1">E</span>
</span></span>rror 
<span class="ltx_inline-block ltx_transformed_outer" id="id18.id2" style="width:6.1pt;height:7.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.3pt,-0.3pt) scale(1.1,1.1) ;">
<span class="ltx_p" id="id18.id2.1">S</span>
</span></span>pan 
<span class="ltx_inline-block ltx_transformed_outer" id="id19.id3" style="width:8.3pt;height:7.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.4pt,-0.3pt) scale(1.1,1.1) ;">
<span class="ltx_p" id="id19.id3.1">A</span>
</span></span>nnotation:
<br class="ltx_break"/>A Balanced Approach for Human Evaluation of Machine Translation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tom Kocmi<sup class="ltx_sup" id="id20.17.id1"><span class="ltx_text ltx_font_italic" id="id20.17.id1.1">★</span></sup><sup class="ltx_sup" id="id21.18.id2">1</sup>   Vilém Zouhar<sup class="ltx_sup" id="id22.19.id3"><span class="ltx_text ltx_font_italic" id="id22.19.id3.1">★</span></sup><sup class="ltx_sup" id="id23.20.id4">2</sup>   Eleftherios Avramidis<sup class="ltx_sup" id="id24.21.id5">3</sup>   Roman Grundkiewicz<sup class="ltx_sup" id="id25.22.id6">1</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id16.16.10">Marzena Karpinska<sup class="ltx_sup" id="id16.16.10.7"><span class="ltx_text ltx_font_medium" id="id16.16.10.7.1">4</span></sup>   Maja Popović<sup class="ltx_sup" id="id16.16.10.8"><span class="ltx_text ltx_font_medium" id="id16.16.10.8.1">5</span></sup>   Mrinmaya Sachan<sup class="ltx_sup" id="id16.16.10.9"><span class="ltx_text ltx_font_medium" id="id16.16.10.9.1">2</span></sup>   Mariya Shmatova<sup class="ltx_sup" id="id16.16.10.10"><span class="ltx_text ltx_font_medium" id="id16.16.10.10.1">6</span></sup>
<br class="ltx_break"/>
<span class="ltx_tabular ltx_align_middle" id="id16.16.10.6">
<span class="ltx_tbody">
<span class="ltx_tr" id="id13.13.7.3.3">
<span class="ltx_td ltx_align_center" id="id11.11.5.1.1.1"><sup class="ltx_sup" id="id11.11.5.1.1.1.1"><span class="ltx_text ltx_font_medium" id="id11.11.5.1.1.1.1.1">1</span></sup>Microsoft</span>
<span class="ltx_td ltx_align_center" id="id12.12.6.2.2.2"><sup class="ltx_sup" id="id12.12.6.2.2.2.1"><span class="ltx_text ltx_font_medium" id="id12.12.6.2.2.2.1.1">2</span></sup>ETH Zürich</span>
<span class="ltx_td ltx_align_center" id="id13.13.7.3.3.3"><sup class="ltx_sup" id="id13.13.7.3.3.3.1"><span class="ltx_text ltx_font_medium" id="id13.13.7.3.3.3.1.1">3</span></sup>DFKI</span></span>
<span class="ltx_tr" id="id16.16.10.6.6">
<span class="ltx_td ltx_align_center" id="id14.14.8.4.4.1"><sup class="ltx_sup" id="id14.14.8.4.4.1.1"><span class="ltx_text ltx_font_medium" id="id14.14.8.4.4.1.1.1">4</span></sup>University of Massachusetts Amherst</span>
<span class="ltx_td ltx_align_center" id="id15.15.9.5.5.2"><sup class="ltx_sup" id="id15.15.9.5.5.2.1"><span class="ltx_text ltx_font_medium" id="id15.15.9.5.5.2.1.1">5</span></sup>Dublin City University</span>
<span class="ltx_td ltx_align_center" id="id16.16.10.6.6.3"><sup class="ltx_sup" id="id16.16.10.6.6.3.1"><span class="ltx_text ltx_font_medium" id="id16.16.10.6.6.3.1.1">6</span></sup>Dubformer</span></span>
</span>
</span>
<br class="ltx_break"/>
<a class="ltx_ref ltx_href ltx_font_medium" href="mailto:tomkocmi@microsoft.com" style="color:#000000;" title=""> tomkocmi@microsoft.com</a></span><span class="ltx_text" id="id26.23.id7" style="color:#000000;">
 <a class="ltx_ref ltx_href" href="mailto:vzouhar@inf.ethz.ch" title=""> vzouhar@inf.ethz.ch</a>
</span><span class="ltx_text ltx_font_bold" id="id27.24.id8">
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id28.id1">High-quality Machine Translation (MT) evaluation relies heavily on human judgments.
Comprehensive error classification methods, such as Multidimensional Quality Metrics (MQM), are expensive as they are time-consuming and can only be done by experts, whose availability may be limited especially for low-resource languages.
On the other hand, just assigning overall scores, like Direct Assessment (DA), is simpler and faster and can be done by translators of any level, but are less reliable.
In this paper, we introduce Error Span Annotation (ESA), a human evaluation protocol which combines the continuous rating of DA with the high-level error severity span marking of MQM.
We validate ESA by comparing it to MQM and DA for 12 MT systems and one human reference translation (English to German) from WMT23.
The results show that ESA offers faster and cheaper annotations than MQM at the same quality level, without the requirement of expensive MQM experts.</p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex1.1"><span class="ltx_text ltx_font_italic" id="footnotex1.1.1">★</span></sup>Equal contributions. Others alphabetically.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
 <sup class="ltx_sup" id="footnotex2.1">0</sup>Code &amp; data will be released before WMT 2024 at:
<br class="ltx_break"/><span class="ltx_text" id="footnotex2.2"></span> <a class="ltx_ref ltx_href" href="https://github.com/wmt-conference/ErrorSpanAnnotations" title="">github.com/wmt-conference/ErrorSpanAnnotations</a>
</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">While automatic evaluation metrics are important and invaluable tools for rapid development of Machine Translation (MT) systems, human assessment remains the gold standard of translation quality <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite>.
The translation quality is conceptually measured through adequacy (preservation of the original meaning) and fluency (grammaticality of the translated text; <cite class="ltx_cite ltx_citemacro_citep">Koehn and Monz, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib17" title="">2006</a></cite>), and sometimes through comprehension (how readable or understandable the translation is; <cite class="ltx_cite ltx_citemacro_citep">White et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib27" title="">1994</a></cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Annotators are usually asked to assign a score on a particular quality aspect.
Likert and 0–100 scale are often used for discrete and continuous scales.
The most popular scoring method in machine translation field in recent years is Direct Assessment <cite class="ltx_cite ltx_citemacro_citep">(DA; Graham et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib11" title="">2013</a>)</cite>, which is used to portray a human assessment of MT quality in the <a class="ltx_ref ltx_href" href="https://www2.statmt.org/wmt23/translation-task.html" title="">WMT shared tasks</a> since 2016.
Since 2022, the DA+SQM metric is used, namely direct assessment enriched with more objective Scalar Quality Metrics (SQM) guidelines <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib15" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="828" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The annotation user interface with Error Span Annotation (ESA). The annotator first marks errors with <span class="ltx_text" id="S1.F1.3.1" style="background-color:#FFCCCC;">minor</span> and <span class="ltx_text" id="S1.F1.4.2" style="background-color:#FF8080;">major</span> severity and then assigns a final score. This is more robust than asking for score directly.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Our experiments are on English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="footnotex3.m1a.1"><semantics id="footnotex3.m1a.1c"><mo id="footnotex3.m1a.1.1" stretchy="false" xref="footnotex3.m1a.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnotex3.m1a.1d"><ci id="footnotex3.m1a.1.1.cmml" xref="footnotex3.m1a.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex3.m1a.1e">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="footnotex3.m1a.1f">→</annotation></semantics></math>German. In <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#footnote2" title="In Figure 1 ‣ 1 Introduction ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Footnote</span> <span class="ltx_text ltx_ref_tag">2</span></a>, Spanish<math alttext="\rightarrow" class="ltx_Math" display="inline" id="footnotex3.m2a.1"><semantics id="footnotex3.m2a.1c"><mo id="footnotex3.m2a.1.1" stretchy="false" xref="footnotex3.m2a.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnotex3.m2a.1d"><ci id="footnotex3.m2a.1.1.cmml" xref="footnotex3.m2a.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex3.m2a.1e">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="footnotex3.m2a.1f">→</annotation></semantics></math>English is only an illustration for English-speaking readers. The first example, based on our data, omits <span class="ltx_text ltx_font_italic" id="footnote2.1">Rainy days!</span> and incorrectly translates <span class="ltx_text ltx_font_italic" id="footnote2.2">about field items</span> as <span class="ltx_text ltx_font_italic" id="footnote2.3">im Feld</span> (<span class="ltx_text ltx_font_italic" id="footnote2.4">=in the field</span>). The second example, for illustrative purposes, does not capitalize <span class="ltx_text ltx_font_italic" id="footnote2.5">H</span> and mistakenly adds extra <span class="ltx_text ltx_font_italic" id="footnote2.6">week</span>.</span></span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Translation scores indicate the overall quality of a translation, but they can be subjective and do not provide details about the translation errors.
The usual way to overcome this drawback is error classification: asking the evaluators to mark each translation error and assign an error tag from a set of predefined categories, such as <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">terminology</span> or <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">style</span>.
In recent years, the dominant error classification protocol is the Multidimensional Quality Metrics <cite class="ltx_cite ltx_citemacro_citep">(MQM; Lommel et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib19" title="">2014</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>.
MQM error classification is the standard human metric in the WMT Metrics shared task since 2021 <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib10" title="">2021b</a>)</cite>.
While error classification provides interesting insights into the distribution of different types of errors, it requires much more time and effort, both for annotators and task organizers, who need to prepare the error taxonomy, annotation guidelines and training examples.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A simpler alternative in the form of highlighting errors without assigning their error types is a compromise between overall scoring and error classification <cite class="ltx_cite ltx_citemacro_cite">Kreutzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib18" title="">2020</a>); Popović (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib21" title="">2020</a>)</cite>.
While the previously reported findings on this method are promising, no systematic comparison for the purposes of evaluating machine translation systems has been carried out so far.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We present a new evaluation protocol based on highlighting errors and assigning scores, Error Span Annotation (ESA), and compare it to the MQM error classification and DA+SQM scores.
We compare MQM, DA+SQM and ESA annotations in parallel on a subset of English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" stretchy="false" xref="S1.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">→</annotation></semantics></math>German machine translation outputs from WMT23.
We find that the proposed ESA protocol is faster and cheaper than MQM whilst providing the same usefulness in ranking MT systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Assigning overall scores was the very first method of manual MT evaluation <cite class="ltx_cite ltx_citemacro_citep">(ALPAC, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib1" title="">1966</a>; White et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib27" title="">1994</a>)</cite>, where the evaluators assessed some or all of the translation quality criteria at once: adequacy, comprehensibility, and fluency.
<a class="ltx_ref ltx_href" href="http://statmt.org/wmt06/" title="">The first WMT</a> (Workshop/Conference on Machine Translation) shared task in 2006 and the subsequent task in 2007 adopted this technique and used adequacy and fluency scores as official metrics <cite class="ltx_cite ltx_citemacro_citep">(Koehn and Monz, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib17" title="">2006</a>; Callison-Burch et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib4" title="">2007</a>)</cite>.
Later, <cite class="ltx_cite ltx_citemacro_citet">Vilar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib24" title="">2007</a>)</cite> proposed binary ranking of two or more MT outputs, which became the official metric at WMT 2008 <cite class="ltx_cite ltx_citemacro_cite">Callison-Burch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib5" title="">2008</a>)</cite>.
It required less effort and showed better inter-annotator agreement than adequacy and fluency scores.
This remained the official WMT metric until 2017 when it was replaced by continuous Direct Assessment (DA).
DA <cite class="ltx_cite ltx_citemacro_cite">Graham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib11" title="">2013</a>)</cite> does not use discrete scales, but a continuous one between 0 and 100.
<cite class="ltx_cite ltx_citemacro_citet">Bojar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib3" title="">2016</a>)</cite> scrutinized the quality criteria and recommended to focus on adequacy and use fluency to break ties only.
DA replaced ranking methods in 2017 <cite class="ltx_cite ltx_citemacro_cite">Bojar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib2" title="">2017</a>)</cite> and since 2022 <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib15" title="">2022</a>)</cite> it is used with SQM guidelines <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite> in a slightly modified version with more descriptive scale labels, which increased the inter-annotator agreement.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">None of the described methods provides information about the erroneous or problematic parts of the translation.
An early work of <cite class="ltx_cite ltx_citemacro_citet">Vilar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib25" title="">2006</a>)</cite> analyzes errors in translation outputs assigning them to error classes from a predefined error typology. Most popular error typology recently is <a class="ltx_ref ltx_href" href="https://themqm.org/error-types-2/typology/" title="">Multidimensional Quality Metrics</a> <cite class="ltx_cite ltx_citemacro_citep">(MQM; Lommel et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib19" title="">2014</a>; Klubička et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib12" title="">2018</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>, which is used in WMT metrics task since 2021 <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib10" title="">2021b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Several error span marking methods have been proposed recently <cite class="ltx_cite ltx_citemacro_cite">Kreutzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib18" title="">2020</a>); Popović (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib21" title="">2020</a>)</cite> as a less demanding error annotation approach than error classification.
While it does not provide the fine-grained details about different error classes, it still gives the information about the position and amount of errors, and also enables further fine-grained analysis on the annotated data, if necessary (e.g. classification of already marked errors, identifying linguistic phenomena causing the errors, or focusing on particular error type).
Although promising, the potential and limits of error marking have not been systematically investigated so far.
Specifically, they were not directly compared to other methods and used in the context of WMT campaign evaluation and system ranking.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Comparison DA+SQM and MQM</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our proposed method lies between DA+SQM and MQM protocols, so we provide a detailed comparison between the two before describing ESA in details in the next section.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">While both DA+SQM and MQM generally exhibit low inter-annotator agreement <cite class="ltx_cite ltx_citemacro_citep">(Knowles and Lo, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib13" title="">2024</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>, DA+SQM scores have high variance, which needs to be compensated with higher number of annotations per system <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib26" title="">2022</a>)</cite>.
On the other hand, MQM requires human experts trained with the MQM protocol and error classification.
Trained experts can be twice as expensive as translators or bilingual speakers evaluating DA+SQM.
The required expertise is a hard constraint which makes evaluation on some languages prohibitively expensive or not possible at all, especially low-resource languages.
Furthermore, assigning a DA+SQM numerical score to a segment is anecdotally and intuitively much faster than MQM, where the evaluators need to mark each error span, classify it and assign severity.
This altogether can make each MQM annotated segment up to approximately 10<math alttext="\times" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mo id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><times id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">×</annotation></semantics></math> more expensive.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">DA+SQM is usually based on sentence-level scores, and the paragraph-level score is computed as the average of all sentence scores in the paragraph.
Paragraph-level DA+SQM evaluation is possible, but evaluating an entire paragraph takes more time, substantially decreases the total number of collected scores, and is more demanding cognitively, which negatively impacts the inter-annotator agreement <cite class="ltx_cite ltx_citemacro_citep">(Castilho, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib6" title="">2020</a>)</cite>.
On the other hand, MQM as an error classification annotation is agnostic to the choice of annotation unit.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">As an example, in App. <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.F8" title="In B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> we show the system ranking based on two approaches DA+SQM and MQM on Chinese-English (sentence-level evaluation) and English-German (paragraph-level evaluation) language pairs.
Although both technique reach same order of system clusters, DA+SQM produces much fewer clusters in paragraph-level setup, thus putting many systems within a single cluster.
On the other hand, MQM is better able to distinguish different systems.
To increase statistical power of DA+SQM, we would have to collect much more DA+SQM samples <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib26" title="">2022</a>)</cite>, which would further drive the cost up.
In addition, DA is much more skewed towards fluency as opposed to adequacy <cite class="ltx_cite ltx_citemacro_citep">(Martindale and Carpuat, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib20" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The cost difference was one of the main reason behind the high usage of DA+SQM at the WMT General MT shared task <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib15" title="">2022</a>)</cite>.
For all these reasons, our hypothesis is that a new annotation protocol, ESA, which is between DA+SQM and MQM can provide better annotations than DA+SQM at a lower cost than MQM.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:319.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(45.8pt,-33.8pt) scale(1.2679506150277,1.2679506150277) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Source/Translation+ESA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.2.1.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1.1.1">SRC</span>: … I’ve entered the burrata dimension.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.2.1.2.1">70%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.3.2.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.2.1.1">TGT</span>: … ich <span class="ltx_text" id="S3.T1.1.1.3.2.1.2" style="background-color:#FFCCCC;">habe</span> die Burrata-Dimension <span class="ltx_text" id="S3.T1.1.1.3.2.1.3" style="background-color:#FFCCCC;">eingegeben</span>.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.4.3.1" style="padding-bottom:10.00002pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.3.1.1">gloss</span>: <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.4.3.1.2">habe eingegeben(=I put in)</span> should be <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.4.3.1.3">bin eingetreten</span>
</td>
<td class="ltx_td" id="S3.T1.1.1.4.3.2" style="padding-bottom:10.00002pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.5.4.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.5.4.1.1">SRC</span>: Not like other tomb raider games</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.5.4.2.1">35%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.6.5.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.6.5.1.1">TGT</span>: Nicht wie andere <span class="ltx_text" id="S3.T1.1.1.6.5.1.2" style="background-color:#FF8080;">Gräberüberfäller</span> Spiele</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.7.6.1" style="padding-bottom:10.00002pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.6.1.1">gloss</span>: <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.7.6.1.2">Gräberüberfäller</span> overtranslates <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.7.6.1.3">Tomb Raider</span>
</td>
<td class="ltx_td" id="S3.T1.1.1.7.6.2" style="padding-bottom:10.00002pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.8.7.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.8.7.1.1">SRC</span>: (PERSON2) Yeah, so just know like-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.8.7.2.1">86%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.9.8.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.9.8.1.1">TGT</span>: Ja, also weißt du einfach… <span class="ltx_text" id="S3.T1.1.1.9.8.1.2" style="background-color:#FFCCCC;">[missing]</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.9">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.10.9.1" style="padding-bottom:10.00002pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.10.9.1.1">gloss</span>: <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.10.9.1.2">PERSON2</span> is missing</td>
<td class="ltx_td" id="S3.T1.1.1.10.9.2" style="padding-bottom:10.00002pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.10">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.11.10.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.11.10.1.1">SRC</span>: All collards, kale, chard is transplanted.</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.10.2" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.11.10.2.1">17%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.11">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.12.11.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.12.11.1.1">TGT</span>: Alle <span class="ltx_text" id="S3.T1.1.1.12.11.1.2" style="background-color:#FF8080;">Kohlköpfe</span>, Grünkohl, <span class="ltx_text" id="S3.T1.1.1.12.11.1.3" style="background-color:#FF8080;">Schmalz</span> sind verpflanzt.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13.12">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.13.12.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.13.12.1.1">gloss</span>: <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.13.12.1.2">Kohlköpfe(=cabbages)</span> and <span class="ltx_text ltx_font_italic" id="S3.T1.1.1.13.12.1.3">Schmalz(=lard)</span> are</td>
<td class="ltx_td" id="S3.T1.1.1.13.12.2"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.1.14.13.1">          incorrect translations</td>
<td class="ltx_td ltx_border_bb" id="S3.T1.1.1.14.13.2"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.T1.2">{example}</span></div>
</div>
<figcaption class="ltx_caption">ESA-annotated examples with associated manual score. The error severity distinction is between <span class="ltx_text" id="S3.T1.9.1" style="background-color:#FFCCCC;">minor</span> and <span class="ltx_text" id="S3.T1.10.2" style="background-color:#FF8080;">major</span>. The first example has a single error (confusion <span class="ltx_text ltx_font_italic" id="S3.T1.11.3">eingeben(=put in)</span> as <span class="ltx_text ltx_font_italic" id="S3.T1.12.4">enter</span> meaning <span class="ltx_text ltx_font_italic" id="S3.T1.13.5">go in</span>) which also affects the auxiliary verb <span class="ltx_text" id="S3.T1.14.6">habe/bin</span>. Thus, the same error is marked twice.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Error Span Annotation</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Annotation process.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">In Error Span Annotation (ESA), the evaluators first mark all erroneous or problematic parts (characters, words, phrases, sentences) in the translated text.
For each marked span, they are asked to provide one of the two severity levels: <span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px1.p1.1.1">major</span> (e.g. changed meaning) or <span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px1.p1.1.2">minor</span> (e.g. incorrect grammar, style, etc.; see <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S3.T1" title="In 3 Comparison DA+SQM and MQM ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
Because all error span annotations are made in the translation, not the source text, we include a special tag for marking omission errors.
This was an intentional design choice over annotating the source text to make the annotation protocol forward-compatible with other translation modalities, such as audio and video translations.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">After the annotators mark all the error spans, they provide an overall score for the entire segment, on the scale from 0 to 100, reminiscent of DA+SQM.
We implement this protocol annotation interface in Appraise <cite class="ltx_cite ltx_citemacro_citep">(Federmann, <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib7" title="">2018</a>)</cite> and show a screenshot in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#footnote2" title="In Figure 1 ‣ 1 Introduction ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Footnote</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
The full guidelines displayed to annotators are shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A1" title="Appendix A User Guidelines ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Segment-level scores.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">To rank systems, we need scalar values.
There are two evident ways to extract them from ESA: (1) using the annotator’s overall segment-level score directly, like DA+SQM, or (2) converting error span severity levels into a segment-level score, like MQM.
By instructing the annotators to identify and mark all errors first, we <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.1">prime</span> them to be more accurate when assessing the overall quality of the segment—when making the decision about the score, they see all marked errors in the segment and can take them into consideration.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p2.1">MQM is primarily error diagnostics protocol which has been repurposed for translation segment scoring.
The transition from error spans into a single segment score has been proposed to be done with the formula based on error severity counts <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>:<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The MQM score calculation weights “untranslated” with -25 and “fluency/punctuation” with -0.1. ESA does not follow this due to lacking error categories.</span></span></span></p>
<table class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table" id="A2.EGx1">
<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\displaystyle\text{MQM-like}=-5\cdot\textsc{\#major}-1\cdot\textsc{\#minor}" class="ltx_Math" display="block" id="S4.Ex1.m1.1"><semantics id="S4.Ex1.m1.1a"><mrow id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml"><mtext id="S4.Ex1.m1.1.1.2" xref="S4.Ex1.m1.1.1.2a.cmml">MQM-like</mtext><mo id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S4.Ex1.m1.1.1.3" xref="S4.Ex1.m1.1.1.3.cmml"><mrow id="S4.Ex1.m1.1.1.3.2" xref="S4.Ex1.m1.1.1.3.2.cmml"><mo id="S4.Ex1.m1.1.1.3.2a" xref="S4.Ex1.m1.1.1.3.2.cmml">−</mo><mrow id="S4.Ex1.m1.1.1.3.2.2" xref="S4.Ex1.m1.1.1.3.2.2.cmml"><mn id="S4.Ex1.m1.1.1.3.2.2.2" xref="S4.Ex1.m1.1.1.3.2.2.2.cmml">5</mn><mo id="S4.Ex1.m1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S4.Ex1.m1.1.1.3.2.2.1.cmml">⋅</mo><mtext class="ltx_font_smallcaps" id="S4.Ex1.m1.1.1.3.2.2.3" xref="S4.Ex1.m1.1.1.3.2.2.3a.cmml">#major</mtext></mrow></mrow><mo id="S4.Ex1.m1.1.1.3.1" xref="S4.Ex1.m1.1.1.3.1.cmml">−</mo><mrow id="S4.Ex1.m1.1.1.3.3" xref="S4.Ex1.m1.1.1.3.3.cmml"><mn id="S4.Ex1.m1.1.1.3.3.2" xref="S4.Ex1.m1.1.1.3.3.2.cmml">1</mn><mo id="S4.Ex1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.Ex1.m1.1.1.3.3.1.cmml">⋅</mo><mtext class="ltx_font_smallcaps" id="S4.Ex1.m1.1.1.3.3.3" xref="S4.Ex1.m1.1.1.3.3.3a.cmml">#minor</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.1b"><apply id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"><eq id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"></eq><ci id="S4.Ex1.m1.1.1.2a.cmml" xref="S4.Ex1.m1.1.1.2"><mtext id="S4.Ex1.m1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.2">MQM-like</mtext></ci><apply id="S4.Ex1.m1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.3"><minus id="S4.Ex1.m1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.3.1"></minus><apply id="S4.Ex1.m1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.3.2"><minus id="S4.Ex1.m1.1.1.3.2.1.cmml" xref="S4.Ex1.m1.1.1.3.2"></minus><apply id="S4.Ex1.m1.1.1.3.2.2.cmml" xref="S4.Ex1.m1.1.1.3.2.2"><ci id="S4.Ex1.m1.1.1.3.2.2.1.cmml" xref="S4.Ex1.m1.1.1.3.2.2.1">⋅</ci><cn id="S4.Ex1.m1.1.1.3.2.2.2.cmml" type="integer" xref="S4.Ex1.m1.1.1.3.2.2.2">5</cn><ci id="S4.Ex1.m1.1.1.3.2.2.3a.cmml" xref="S4.Ex1.m1.1.1.3.2.2.3"><mtext class="ltx_font_smallcaps" id="S4.Ex1.m1.1.1.3.2.2.3.cmml" xref="S4.Ex1.m1.1.1.3.2.2.3">#major</mtext></ci></apply></apply><apply id="S4.Ex1.m1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.3.3"><ci id="S4.Ex1.m1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.1.1.3.3.1">⋅</ci><cn id="S4.Ex1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.Ex1.m1.1.1.3.3.2">1</cn><ci id="S4.Ex1.m1.1.1.3.3.3a.cmml" xref="S4.Ex1.m1.1.1.3.3.3"><mtext class="ltx_font_smallcaps" id="S4.Ex1.m1.1.1.3.3.3.cmml" xref="S4.Ex1.m1.1.1.3.3.3">#minor</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.1c">\displaystyle\text{MQM-like}=-5\cdot\textsc{\#major}-1\cdot\textsc{\#minor}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.1d">MQM-like = - 5 ⋅ #major - 1 ⋅ #minor</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p3.2">Notice that this does not scale with different text sizes.
As an example, if translation of a segment has two major errors, it receives the score of <math alttext="-10" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p3.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p3.1.m1.1a"><mrow id="S4.SS0.SSS0.Px2.p3.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1.cmml"><mo id="S4.SS0.SSS0.Px2.p3.1.m1.1.1a" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1.cmml">−</mo><mn id="S4.SS0.SSS0.Px2.p3.1.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p3.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1"><minus id="S4.SS0.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1"></minus><cn id="S4.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p3.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p3.1.m1.1c">-10</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p3.1.m1.1d">- 10</annotation></semantics></math>.
However, if the source is repeated twice and the corresponding translation as well, the score would be further decreased to <math alttext="-20" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p3.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p3.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p3.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1.cmml"><mo id="S4.SS0.SSS0.Px2.p3.2.m2.1.1a" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1.cmml">−</mo><mn id="S4.SS0.SSS0.Px2.p3.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1.2.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p3.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1"><minus id="S4.SS0.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1"></minus><cn id="S4.SS0.SSS0.Px2.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p3.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p3.2.m2.1c">-20</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p3.2.m2.1d">- 20</annotation></semantics></math>.
This is especially problematic for paragraph-level evaluation which features segments of different length.
Additionally, the segment-level MQM score might not correspond with the segment-level translation quality, such as when marking one error affecting several places in the segment, as in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S3.T1" title="In 3 Comparison DA+SQM and MQM ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> (top).
To avoid such issues, we use the annotators’ direct scores as the main scoring approach for the ESA protocol unless specified differently.
The system-level scores for all human evaluation methods in this work are calculated as the average of all segment-level scores for particular system.
We further revisit the score computation in App. <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.SS1" title="B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.1</span></a>.
In some analysis, we use ESA error spans to calculate MQM-like score, we refer to such scores as ESA<sub class="ltx_sub" id="S4.SS0.SSS0.Px2.p3.2.1">spans</sub>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Advantages.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Assigning overall scores is guided by errors in the translation, and through error marking, the annotator can first focus on direct highlighting of these issues instead of determining the overall score directly.
The advantage of ESA over error classification is that it is less demanding, while still informative—the annotations can be further refined in subsequent analyses. Furthermore, the evaluators are not limited to any pre-defined annotation protocol and can highlight a larger range of errors.
The error marking approach can be seen as <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">descriptive</span> (encouraging annotator subjectivity and capturing their
individual beliefs) and the error classification as a <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.2">prescriptive</span> (discouraging annotator subjectivity and asking annotators to align with one specific belief, in this case the pre-defined error protocol), as per <cite class="ltx_cite ltx_citemacro_citet">Rottger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib23" title="">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We conduct experiments comparing the ESA protocol with MQM and DA+SQM protocols.
We design them in a way, which makes it comparable with the previously collected annotations for MQM<sup class="ltx_sup" id="S5.p1.1.1">WMT</sup> and DA+SQM<sup class="ltx_sup" id="S5.p1.1.2">WMT</sup>.
For this reason, we reproduce the human evaluation campaign for the WMT23 English to German systems <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite> with ESA and our reimplementation of MQM.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The original campaign featured 13 translations of 557 source segments.
To facilitate running multiple campaigns for proper comparison, we had to scale down and subsampled 207 segments per system (74 documents), which yields 2,691 segments in total.
To keep the ESA annotation comparable to other protocols, we subsample by selecting a subset of documents evaluated by <cite class="ltx_cite ltx_citemacro_citet">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite> keeping the entire documents.
This differs from <cite class="ltx_cite ltx_citemacro_citet">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite>, who removed some paragraphs from the ends of long documents.
In our analysis we only consider segments overlapping with both previous annotation collections, thus obtaining 2,027 annotations evaluated across all protocols.
We note that the subsampling makes all annotations protocols statistically less powerful, but keeps them fair in terms of statistical power per evaluated segment.
Therefore, clustering and final system ranking in our analysis differs from <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>); Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite>.
To keep the study comparable with <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>)</cite>, we use the Wilcoxon rank-sum test with <math alttext="p{&lt;}0.05" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mi id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><lt id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1"></lt><ci id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">𝑝</ci><cn id="S5.p2.1.m1.1.1.3.cmml" type="float" xref="S5.p2.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">p{&lt;}0.05</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math> when producing system clusters. However, as all systems are evaluated on the same set of segments, we advice to use Wilcoxon signed-rank test when evaluation ESA as proposed by <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib16" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:433.6pt;height:127.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.8pt,-18.8pt) scale(1.41672218266903,1.41672218266903) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.3.3">
<td class="ltx_td ltx_border_tt" id="S5.T2.3.3.3.4"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1">ESA<sub class="ltx_sub" id="S5.T2.1.1.1.1.1">1</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.2.2.2">ESA<sub class="ltx_sub" id="S5.T2.2.2.2.2.1">2</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.5">MQM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.3">MQM<sup class="ltx_sup" id="S5.T2.3.3.3.3.1">WMT</sup>
</th>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.1"># error spans</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.2">0.45</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.3">1.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.4">0.53</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.3.4.1.5">3.37</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.5.2">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.1">% minor</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.2">63%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.3">68%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.4">67%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.5.2.5">67%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.6.3">
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.1">% major</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.2">37%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.3">32%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.4">33%</td>
<td class="ltx_td ltx_align_left" id="S5.T2.3.3.6.3.5">33%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.7.4.1">Score (MQM-like)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.7.4.2">81.8 (-1.1)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.7.4.3">84.5 (-2.2)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.7.4.4">(-1.2)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.3.7.4.5">(-7.1)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Average number of error spans per segment, ratio between minor and major errors, and scores across different annotation protocols.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="623" id="S5.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of scores for one annotation campaign. For ESA, we either use the manual score or ESA<sub class="ltx_sub" id="S5.F2.4.1">spans</sub> computation based on error severities. For MQMs, the distribution is clipped <math alttext="\geq-15" class="ltx_Math" display="inline" id="S5.F2.2.m1.1"><semantics id="S5.F2.2.m1.1b"><mrow id="S5.F2.2.m1.1.1" xref="S5.F2.2.m1.1.1.cmml"><mi id="S5.F2.2.m1.1.1.2" xref="S5.F2.2.m1.1.1.2.cmml"></mi><mo id="S5.F2.2.m1.1.1.1" xref="S5.F2.2.m1.1.1.1.cmml">≥</mo><mrow id="S5.F2.2.m1.1.1.3" xref="S5.F2.2.m1.1.1.3.cmml"><mo id="S5.F2.2.m1.1.1.3b" xref="S5.F2.2.m1.1.1.3.cmml">−</mo><mn id="S5.F2.2.m1.1.1.3.2" xref="S5.F2.2.m1.1.1.3.2.cmml">15</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F2.2.m1.1c"><apply id="S5.F2.2.m1.1.1.cmml" xref="S5.F2.2.m1.1.1"><geq id="S5.F2.2.m1.1.1.1.cmml" xref="S5.F2.2.m1.1.1.1"></geq><csymbol cd="latexml" id="S5.F2.2.m1.1.1.2.cmml" xref="S5.F2.2.m1.1.1.2">absent</csymbol><apply id="S5.F2.2.m1.1.1.3.cmml" xref="S5.F2.2.m1.1.1.3"><minus id="S5.F2.2.m1.1.1.3.1.cmml" xref="S5.F2.2.m1.1.1.3"></minus><cn id="S5.F2.2.m1.1.1.3.2.cmml" type="integer" xref="S5.F2.2.m1.1.1.3.2">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.2.m1.1d">\geq-15</annotation><annotation encoding="application/x-llamapun" id="S5.F2.2.m1.1e">≥ - 15</annotation></semantics></math> for higher resolution.</figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.2">To analyze inter annotator agreement, we run ESA protocol twice with different sets of annotators.
We hired 28 annotators to evaluate our protocols and each protocol was evaluated by different sets of experts for less biased results. Specifically, we had 8 bilingual annotators for the initial run of ESA<sub class="ltx_sub" id="S5.p3.2.1">1</sub>, 10 translators for ESA<sub class="ltx_sub" id="S5.p3.2.2">2</sub> (different vendor), and 10 MQM professionals to evaluate MQM protocol.
For MQM, we hired professionals already experienced with MQM annotation protocol, while for ESA, we hired translators or bilingual speakers.
All of them were native speakers of the target language, i.e., German.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Score distribution</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.4">As per <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S5.T2" title="In 5 Experimental setup ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, on average the ESA<sub class="ltx_sub" id="S6.SS1.p1.4.1">1</sub> annotators mark 0.45 error spans per segments, which is close to MQM’s 0.53 error spans per segment.
The second run of ESA<sub class="ltx_sub" id="S6.SS1.p1.4.2">2</sub> has more than double of error spans per segment, which could be the result of different characteristics of annotators group, specifically experienced annotators in ESA<sub class="ltx_sub" id="S6.SS1.p1.4.3">1</sub> versus translators in ESA<sub class="ltx_sub" id="S6.SS1.p1.4.4">2</sub>.
On the other hand, MQM<sup class="ltx_sup" id="S6.SS1.p1.4.5">WMT</sup> contains 7x more errors per segment than our rerun of MQM.
The severity levels are distributed similarly across campaigns.
Important insights are in the score range distribution presented in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S5.F2" title="In 5 Experimental setup ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>: the MQM-like score computation creates more skewed distribution around 0, which is in addition unbounded and can go to -infinity the longer the evaluated segment is, complicating modeling and comparisons.
In contrast, the manual scores from annotators are more spread out and guaranteed to be in the interval [0, 100].</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="199" id="S6.F3.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Each point represents a system, with the original MQM<sup class="ltx_sup" id="S6.F3.10.1">WMT</sup> scores on the <span class="ltx_text ltx_font_italic" id="S6.F3.11.2">y</span>-axis plotted against our rerun of DA+SQM<sup class="ltx_sup" id="S6.F3.12.3">WMT</sup> (first plot), ESA (second plot), ESA<sub class="ltx_sub" id="S6.F3.13.4">spans</sub> (third plot), and MQM (forth plot). Stripped lines indicate cluster separations determined by each method with alpha threshold 0.05. We compute Spearman correlation <math alttext="\rho" class="ltx_Math" display="inline" id="S6.F3.3.m1.1"><semantics id="S6.F3.3.m1.1b"><mi id="S6.F3.3.m1.1.1" xref="S6.F3.3.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S6.F3.3.m1.1c"><ci id="S6.F3.3.m1.1.1.cmml" xref="S6.F3.3.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.3.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S6.F3.3.m1.1e">italic_ρ</annotation></semantics></math> and pairwise accuracy <span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S6.F3.14.5">Acc</span>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>System ranking capabilities</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We now investigate how well different techniques can rank MT systems.
For purpose of this experiment, we consider MQM<sup class="ltx_sup" id="S6.SS2.p1.1.1">WMT</sup> as the gold standard.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">When comparing two protocols, we ideally want them to rank all systems in the same order. This is not always possible as some systems are very similar and cannot be significantly distinguished with the evaluated sample size. Another problem is that different protocols may weight different phenomena (e.g. fluency or adequacy) differently.
To compare different protocols in the task of ranking systems, we use pairwise accuracy <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib16" title="">2021</a>)</cite>, which is also used in WMT Metrics shared task when comparing different automatic metrics <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib9" title="">2023</a>)</cite>.
Pairwise accuracy measures how many system pairs does a protocol rank the same way as MQM<sup class="ltx_sup" id="S6.SS2.p2.1.1">WMT</sup>.
As we have only 78 system pairs, any wrong system pair will change pairwise accuracy by 1.28%.
Therefore, we also calculate Spearman’s correlations as we mainly want protocols to have monotonic ranking.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">In <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.F3" title="In 6.1 Score distribution ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, each subplot compares system-level scores between one protocol on x-axis and MQM<sup class="ltx_sup" id="S6.SS2.p3.1.1">WMT</sup> on y-axis.
Our repeated MQM experiment and ESA protocol rank systems identically (94.9%), while ESA has slighly higher Spearman’s correlation with MQM<sup class="ltx_sup" id="S6.SS2.p3.1.2">WMT</sup>.
On the other hand, DA+SQM<sup class="ltx_sup" id="S6.SS2.p3.1.3">WMT</sup> significantly lacks behind both protocols.
This suggest that our ESA protocol has comparable system ranking capabilities to MQM and is superior to DA+SQM<sup class="ltx_sup" id="S6.SS2.p3.1.4">WMT</sup>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">The <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.F3" title="In 6.1 Score distribution ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> also shows, that relying on error spans only isn’t optimal, as ESA<sub class="ltx_sub" id="S6.SS2.p4.1.1">spans</sub> has lower accuracy and Spearman’s correlation than ESA. We can notice that this is even lower than our rerun of MQM. This can be attributed to the evaluation crowd, where we used professional MQM annotators for MQM protocol, while we used bilingual speakers and translators for the ESA protocol.</p>
</div>
<div class="ltx_para" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1">Further focusing on the clustering, MQM<sup class="ltx_sup" id="S6.SS2.p5.1.1">WMT</sup> significantly differentiates the top system from others (highlighted in orange), while DA+SQM<sup class="ltx_sup" id="S6.SS2.p5.1.2">WMT</sup> strongly puts this system into the second cluster.
This system is human reference, which we assume should be of highest quality.
The reduced number of clusters in contrast to <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.F8" title="In B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> is due to the lower sample size.
This may be result of DA+SQM<sup class="ltx_sup" id="S6.SS2.p5.1.3">WMT</sup> higher sensitivity to fluency and style errors, which contribute to 60% of all errors in human reference as marked by MQM<sup class="ltx_sup" id="S6.SS2.p5.1.4">WMT</sup>.
This conflict in clustering is one of the critiques of DA+SQM<sup class="ltx_sup" id="S6.SS2.p5.1.5">WMT</sup> if we assume that human reference should be the highest scoring translation.
For example, in <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>)</cite>, human reference was the best translation only in 2 out of 8 language pairs.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.8" style="width:303.5pt;height:282.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.1pt,-78.2pt) scale(2.24183670352926,2.24183670352926) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T3.1.1.1.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1">MQM<sup class="ltx_sup" id="S6.T3.1.1.1.1.1">WMT</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.2.2.2.1">ESA<sub class="ltx_sub" id="S6.T3.2.2.2.1.1">1</sub>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T3.2.2.2.2">0.227</td>
</tr>
<tr class="ltx_tr" id="S6.T3.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.4.4.4.2">ESA<sub class="ltx_sub" id="S6.T3.4.4.4.2.1">1</sub><math alttext="{}_{\mathrm{\,spans}}" class="ltx_Math" display="inline" id="S6.T3.4.4.4.2.m2.1"><semantics id="S6.T3.4.4.4.2.m2.1a"><msub id="S6.T3.4.4.4.2.m2.1.1" xref="S6.T3.4.4.4.2.m2.1.1.cmml"><mi id="S6.T3.4.4.4.2.m2.1.1a" xref="S6.T3.4.4.4.2.m2.1.1.cmml"></mi><mi id="S6.T3.4.4.4.2.m2.1.1.1" xref="S6.T3.4.4.4.2.m2.1.1.1.cmml">spans</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.4.2.m2.1b"><apply id="S6.T3.4.4.4.2.m2.1.1.cmml" xref="S6.T3.4.4.4.2.m2.1.1"><ci id="S6.T3.4.4.4.2.m2.1.1.1.cmml" xref="S6.T3.4.4.4.2.m2.1.1.1">spans</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.4.2.m2.1c">{}_{\mathrm{\,spans}}</annotation><annotation encoding="application/x-llamapun" id="S6.T3.4.4.4.2.m2.1d">start_FLOATSUBSCRIPT roman_spans end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S6.T3.4.4.4.3">0.170</td>
</tr>
<tr class="ltx_tr" id="S6.T3.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.5.5.5.1">ESA<sub class="ltx_sub" id="S6.T3.5.5.5.1.1">2</sub>
</th>
<td class="ltx_td ltx_align_left" id="S6.T3.5.5.5.2">0.250</td>
</tr>
<tr class="ltx_tr" id="S6.T3.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.7.7.7.2">ESA<sub class="ltx_sub" id="S6.T3.7.7.7.2.1">2</sub><math alttext="{}_{\mathrm{\,spans}}" class="ltx_Math" display="inline" id="S6.T3.7.7.7.2.m2.1"><semantics id="S6.T3.7.7.7.2.m2.1a"><msub id="S6.T3.7.7.7.2.m2.1.1" xref="S6.T3.7.7.7.2.m2.1.1.cmml"><mi id="S6.T3.7.7.7.2.m2.1.1a" xref="S6.T3.7.7.7.2.m2.1.1.cmml"></mi><mi id="S6.T3.7.7.7.2.m2.1.1.1" xref="S6.T3.7.7.7.2.m2.1.1.1.cmml">spans</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T3.7.7.7.2.m2.1b"><apply id="S6.T3.7.7.7.2.m2.1.1.cmml" xref="S6.T3.7.7.7.2.m2.1.1"><ci id="S6.T3.7.7.7.2.m2.1.1.1.cmml" xref="S6.T3.7.7.7.2.m2.1.1.1">spans</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.7.7.2.m2.1c">{}_{\mathrm{\,spans}}</annotation><annotation encoding="application/x-llamapun" id="S6.T3.7.7.7.2.m2.1d">start_FLOATSUBSCRIPT roman_spans end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S6.T3.7.7.7.3">0.236</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.8.8.9.1.1">MQM</th>
<td class="ltx_td ltx_align_left" id="S6.T3.8.8.9.1.2">0.189</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.8.8.8.1">DA+SQM<sup class="ltx_sup" id="S6.T3.8.8.8.1.1">WMT</sup>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T3.8.8.8.2">0.209</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Kendall <math alttext="\tau" class="ltx_Math" display="inline" id="S6.T3.10.m1.1"><semantics id="S6.T3.10.m1.1b"><mi id="S6.T3.10.m1.1.1" xref="S6.T3.10.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.T3.10.m1.1c"><ci id="S6.T3.10.m1.1.1.cmml" xref="S6.T3.10.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.10.m1.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.T3.10.m1.1e">italic_τ</annotation></semantics></math> segment-level correlations between evaluation protocols.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Agreement with other protocols</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We now compare how different protocols correspond on the segment-level.
We use MQM<sup class="ltx_sup" id="S6.SS3.p1.1.2">WMT</sup> as the gold standard to compare against because it was done outside of our setup.
We analyze two aspects: scores and spans.
For this evaluation, we use <a class="ltx_ref ltx_href" href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Tau-c" title="">Kendall <math alttext="\tau" class="ltx_Math" display="inline" id="S6.SS3.p1.1.1.m1.1"><semantics id="S6.SS3.p1.1.1.m1.1a"><mi id="S6.SS3.p1.1.1.m1.1.1" xref="S6.SS3.p1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.1.m1.1b"><ci id="S6.SS3.p1.1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.1.1.m1.1d">italic_τ</annotation></semantics></math> variant C</a>, which is more suitable for data with different underlying scales and many ties.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">In <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T3" title="In 6.2 System ranking capabilities ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we see that although all protocols correlate similarly with MQM<sup class="ltx_sup" id="S6.SS3.p2.1.1">WMT</sup>, our protocols obtain the highest <math alttext="\tau" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.1"><semantics id="S6.SS3.p2.1.m1.1a"><mi id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><ci id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.1d">italic_τ</annotation></semantics></math> for both runs. The segment-level correlation also confirms that relying only on the error spans isn’t optimal and ESA<sub class="ltx_sub" id="S6.SS3.p2.1.2">spans</sub> obtains lower Kendall score.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.2">In <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T4" title="In 6.3 Agreement with other protocols ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> we focus on the annotated spans.
We consider any spans that overlap as matching, irrespective of severity.
Because different protocols have different average number of error spans, presenting just the size of the intersection would be misleading.
Instead, we show normalized set similarity that is not symmetric.
It answers the questions: <span class="ltx_text ltx_font_italic" id="S6.SS3.p3.2.2">What proportion of samples in <math alttext="B" class="ltx_Math" display="inline" id="S6.SS3.p3.1.1.m1.1"><semantics id="S6.SS3.p3.1.1.m1.1a"><mi id="S6.SS3.p3.1.1.m1.1.1" xref="S6.SS3.p3.1.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p3.1.1.m1.1b"><ci id="S6.SS3.p3.1.1.m1.1.1.cmml" xref="S6.SS3.p3.1.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p3.1.1.m1.1c">B</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p3.1.1.m1.1d">italic_B</annotation></semantics></math> were covered by <math alttext="A" class="ltx_Math" display="inline" id="S6.SS3.p3.2.2.m2.1"><semantics id="S6.SS3.p3.2.2.m2.1a"><mi id="S6.SS3.p3.2.2.m2.1.1" xref="S6.SS3.p3.2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p3.2.2.m2.1b"><ci id="S6.SS3.p3.2.2.m2.1.1.cmml" xref="S6.SS3.p3.2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p3.2.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p3.2.2.m2.1d">italic_A</annotation></semantics></math>?</span>
Both ESA and MQM cover MQM<sup class="ltx_sup" id="S6.SS3.p3.2.3">WMT</sup> similarly, with 29% and 32% respectively.
At the same time, MQM<sup class="ltx_sup" id="S6.SS3.p3.2.4">WMT</sup> covers ESA and MQM with 93% and 85%.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S6.T4.2" style="width:368.6pt;height:128.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.2pt,-28.4pt) scale(1.78753137399699,1.78753137399699) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.2.2.2.2">
<math alttext="\bm{\downarrow}" class="ltx_Math" display="inline" id="S6.T4.1.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.1.m1.1a"><mo class="ltx_mathvariant_bold" id="S6.T4.1.1.1.1.m1.1.1" mathvariant="bold" stretchy="false" xref="S6.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">bold-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\bm{\downarrow}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.1.m1.1d">bold_↓</annotation></semantics></math>A  B<math alttext="\bm{\rightarrow}" class="ltx_Math" display="inline" id="S6.T4.2.2.2.2.m2.1"><semantics id="S6.T4.2.2.2.2.m2.1a"><mo class="ltx_mathvariant_bold" id="S6.T4.2.2.2.2.m2.1.1" mathvariant="bold" stretchy="false" xref="S6.T4.2.2.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m2.1b"><ci id="S6.T4.2.2.2.2.m2.1.1.cmml" xref="S6.T4.2.2.2.2.m2.1.1">bold-→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m2.1c">\bm{\rightarrow}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.2.m2.1d">bold_→</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2.3">ESA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2.4">MQM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2.5">MQM<sup class="ltx_sup" id="S6.T4.2.2.2.5.1">WMT</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.2.2.3.1.1">ESA</th>
<td class="ltx_td ltx_border_t" id="S6.T4.2.2.3.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.3">77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.2.3.1.4">29%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.2.2.4.2.1">MQM</th>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.2">85%</td>
<td class="ltx_td" id="S6.T4.2.2.4.2.3"></td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.2.4.2.4">32%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.2.2.5.3.1">MQM<sup class="ltx_sup" id="S6.T4.2.2.5.3.1.1">WMT</sup>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.2.5.3.2">93%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.2.5.3.3">89%</td>
<td class="ltx_td ltx_border_bb" id="S6.T4.2.2.5.3.4"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.T4.4.2"><math alttext="\bm{|A\cap B|/|B|}" class="ltx_Math" display="inline" id="S6.T4.3.1.m1.2"><semantics id="S6.T4.3.1.m1.2a"><mrow id="S6.T4.3.1.m1.2.2" xref="S6.T4.3.1.m1.2.2.cmml"><mrow id="S6.T4.3.1.m1.2.2.1.1" xref="S6.T4.3.1.m1.2.2.1.2.cmml"><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.1.1.2" mathvariant="bold" stretchy="false" xref="S6.T4.3.1.m1.2.2.1.2.1.cmml">|</mo><mrow id="S6.T4.3.1.m1.2.2.1.1.1" xref="S6.T4.3.1.m1.2.2.1.1.1.cmml"><mi id="S6.T4.3.1.m1.2.2.1.1.1.2" xref="S6.T4.3.1.m1.2.2.1.1.1.2.cmml">𝑨</mi><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.1.1.1.1" mathvariant="bold" xref="S6.T4.3.1.m1.2.2.1.1.1.1.cmml">∩</mo><mi id="S6.T4.3.1.m1.2.2.1.1.1.3" xref="S6.T4.3.1.m1.2.2.1.1.1.3.cmml">𝑩</mi></mrow><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.1.1.3" mathvariant="bold" stretchy="false" xref="S6.T4.3.1.m1.2.2.1.2.1.cmml">|</mo></mrow><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.2" mathvariant="bold" xref="S6.T4.3.1.m1.2.2.2.cmml">/</mo><mrow id="S6.T4.3.1.m1.2.2.3.2" xref="S6.T4.3.1.m1.2.2.3.1.cmml"><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.3.2.1" mathvariant="bold" stretchy="false" xref="S6.T4.3.1.m1.2.2.3.1.1.cmml">|</mo><mi id="S6.T4.3.1.m1.1.1" xref="S6.T4.3.1.m1.1.1.cmml">𝑩</mi><mo class="ltx_mathvariant_bold" id="S6.T4.3.1.m1.2.2.3.2.2" mathvariant="bold" stretchy="false" xref="S6.T4.3.1.m1.2.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.3.1.m1.2b"><apply id="S6.T4.3.1.m1.2.2.cmml" xref="S6.T4.3.1.m1.2.2"><divide id="S6.T4.3.1.m1.2.2.2.cmml" xref="S6.T4.3.1.m1.2.2.2"></divide><apply id="S6.T4.3.1.m1.2.2.1.2.cmml" xref="S6.T4.3.1.m1.2.2.1.1"><abs id="S6.T4.3.1.m1.2.2.1.2.1.cmml" xref="S6.T4.3.1.m1.2.2.1.1.2"></abs><apply id="S6.T4.3.1.m1.2.2.1.1.1.cmml" xref="S6.T4.3.1.m1.2.2.1.1.1"><intersect id="S6.T4.3.1.m1.2.2.1.1.1.1.cmml" xref="S6.T4.3.1.m1.2.2.1.1.1.1"></intersect><ci id="S6.T4.3.1.m1.2.2.1.1.1.2.cmml" xref="S6.T4.3.1.m1.2.2.1.1.1.2">𝑨</ci><ci id="S6.T4.3.1.m1.2.2.1.1.1.3.cmml" xref="S6.T4.3.1.m1.2.2.1.1.1.3">𝑩</ci></apply></apply><apply id="S6.T4.3.1.m1.2.2.3.1.cmml" xref="S6.T4.3.1.m1.2.2.3.2"><abs id="S6.T4.3.1.m1.2.2.3.1.1.cmml" xref="S6.T4.3.1.m1.2.2.3.2.1"></abs><ci id="S6.T4.3.1.m1.1.1.cmml" xref="S6.T4.3.1.m1.1.1">𝑩</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.1.m1.2c">\bm{|A\cap B|/|B|}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.3.1.m1.2d">bold_| bold_italic_A bold_∩ bold_italic_B bold_| bold_/ bold_| bold_italic_B bold_|</annotation></semantics></math>  (100% for <math alttext="B=\emptyset" class="ltx_Math" display="inline" id="S6.T4.4.2.m2.1"><semantics id="S6.T4.4.2.m2.1a"><mrow id="S6.T4.4.2.m2.1.1" xref="S6.T4.4.2.m2.1.1.cmml"><mi id="S6.T4.4.2.m2.1.1.2" xref="S6.T4.4.2.m2.1.1.2.cmml">B</mi><mo id="S6.T4.4.2.m2.1.1.1" xref="S6.T4.4.2.m2.1.1.1.cmml">=</mo><mi id="S6.T4.4.2.m2.1.1.3" mathvariant="normal" xref="S6.T4.4.2.m2.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.4.2.m2.1b"><apply id="S6.T4.4.2.m2.1.1.cmml" xref="S6.T4.4.2.m2.1.1"><eq id="S6.T4.4.2.m2.1.1.1.cmml" xref="S6.T4.4.2.m2.1.1.1"></eq><ci id="S6.T4.4.2.m2.1.1.2.cmml" xref="S6.T4.4.2.m2.1.1.2">𝐵</ci><emptyset id="S6.T4.4.2.m2.1.1.3.cmml" xref="S6.T4.4.2.m2.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.2.m2.1c">B=\emptyset</annotation><annotation encoding="application/x-llamapun" id="S6.T4.4.2.m2.1d">italic_B = ∅</annotation></semantics></math>)</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Similarity between spans of different annotation protocols (one campaign) computed as percentage how much of <math alttext="B" class="ltx_Math" display="inline" id="S6.T4.7.m1.1"><semantics id="S6.T4.7.m1.1b"><mi id="S6.T4.7.m1.1.1" xref="S6.T4.7.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S6.T4.7.m1.1c"><ci id="S6.T4.7.m1.1.1.cmml" xref="S6.T4.7.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.m1.1d">B</annotation><annotation encoding="application/x-llamapun" id="S6.T4.7.m1.1e">italic_B</annotation></semantics></math> does <math alttext="A" class="ltx_Math" display="inline" id="S6.T4.8.m2.1"><semantics id="S6.T4.8.m2.1b"><mi id="S6.T4.8.m2.1.1" xref="S6.T4.8.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S6.T4.8.m2.1c"><ci id="S6.T4.8.m2.1.1.cmml" xref="S6.T4.8.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.8.m2.1d">A</annotation><annotation encoding="application/x-llamapun" id="S6.T4.8.m2.1e">italic_A</annotation></semantics></math> contain.
For example, 93% of ESA spans were also in MQM.
Any span that overlaps with another one is considered a hit.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Quality of annotations</h3>
<section class="ltx_paragraph" id="S6.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Intra annotator agreement</h4>
<div class="ltx_para" id="S6.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS4.SSS0.Px1.p1.1">We want the human evaluation protocol to consistently assign similar scores for the same translations over time.
A good indicator of the annotation quality is how noisy and subjective it is, which is reflected by how much annotators agree on the same segments (inter annotator agreement) as well as how a single annotator agrees with themselves (intra annotator agreement).
To measure intra-AA, we ask the same annotators to again annotate the same documents two months later.
We prepare an identical campaign with the same distribution of systems in the same order as originally, asking the same annotators to redo it again for both ESA and MQM.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS4.SSS0.Px1.p2.1">It is not obvious how to measure the agreements for protocols that have different features.
One issue is the frequency of ties, where MQM has more ties than rating from ESA or DA+SQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px1.p2.1.1">WMT</sup>.For example, MQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px1.p2.1.2">WMT</sup> contains 30.8% no-errors, while DA+SQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px1.p2.1.3">WMT</sup> contains only 5.2% of score 100.
Secondly, each protocol uses different range and distribution of scores, which makes calculation of agreement complicated. See App. <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.F6" title="In Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> to understand different distribution of scores.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS0.Px1.p3">
<p class="ltx_p" id="S6.SS4.SSS0.Px1.p3.1">Previous works comparing different protocols discretize the scale into bins <cite class="ltx_cite ltx_citemacro_citep">(Graham et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib11" title="">2013</a>; Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>, however, this approach is sensitive to subjective selection of bin sizes and benefits already discrete protocols.
Instead, we propose to use Kendall’s Tau-c correlation to measure inter-annotator agreement.
Secondly, we also want to take into consideration that small changes in scores are less damaging than large shifts, therefore, we want inter annotator’s scores to correlate linearly, ideally having identical score each time.
To measure this, we use Pearson’s correlation.
Lastly, we measure recall of how often annotator mark <span class="ltx_text ltx_font_italic" id="S6.SS4.SSS0.Px1.p3.1.1">any</span> error in the same segment in contrast to leaving the segment without marked errors.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS0.Px1.p4">
<p class="ltx_p" id="S6.SS4.SSS0.Px1.p4.1"><a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T5" title="In Intra annotator agreement ‣ 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that ESA has all scores higher than MQM.
Higher Kendall and Pearson suggest that the task is easier for annotators to agree on the score.
We expect the recall to be comparable for both techniques as the task is similar, we hypothesize that the drop in MQM could be explained by annotators saving time and skipping minor errors as the annotation is more complicated for MQM than ESA, which can be confirmed when looking at minor error’s recall only.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS0.Px1.p5">
<p class="ltx_p" id="S6.SS4.SSS0.Px1.p5.1">Lastly, evaluating inter annotator agreement is heavily affected by the strategy of annotators, where different annotation strategy doesn’t mean different performance of the task as <cite class="ltx_cite ltx_citemacro_citet">Riley et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib22" title="">2024</a>)</cite> showed. Secondly, the MQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px1.p5.1.1">WMT</sup> was collected with a different tooling and the documents have been presented to annotators in different order, which could also impact the inter annotator agreement.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.1" style="width:411.9pt;height:213.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.4pt,-43.7pt) scale(1.69443906586997,1.69443906586997) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T5.1.1.1.1.2">Intra AA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T5.1.1.1.1.3">Inter AA</th>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T5.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T5.1.1.2.2.2">ESA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S6.T5.1.1.2.2.3">MQM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T5.1.1.2.2.4">ESA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T5.1.1.2.2.5">MQM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.1.1.3.1.1">Kendall’s Tau-c</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T5.1.1.3.1.2">0.149</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.1.1.3.1.3">0.109</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T5.1.1.3.1.4">0.254</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T5.1.1.3.1.5">0.116</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.1.1.4.2.1">Pearson</th>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.4.2.2">0.403</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T5.1.1.4.2.3">0.189</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.4.2.4">0.482</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.4.2.5">0.281</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.1.1.5.3.1">Error recall</th>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.5.3.2">69.6%</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T5.1.1.5.3.3">61.9%</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.5.3.4">66.6%</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.5.3.5">40.1%</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T5.1.1.6.4.1">Minor e. recall</th>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.6.4.2">70.7%</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T5.1.1.6.4.3">66.2%</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.6.4.4">67.7%</td>
<td class="ltx_td ltx_align_left" id="S6.T5.1.1.6.4.5">44.4%</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T5.1.1.7.5.1">Major e. recall</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T5.1.1.7.5.2">82.6%</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T5.1.1.7.5.3">82.1%</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T5.1.1.7.5.4">84.8%</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T5.1.1.7.5.5">62.9%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Intra- and inter-annotator agreement on segment-level.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Inter annotator agreement</h4>
<div class="ltx_para" id="S6.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS4.SSS0.Px2.p1.2">To measure how different annotators agree between themselves on the same protocol, we compare ESA<sub class="ltx_sub" id="S6.SS4.SSS0.Px2.p1.2.1">1</sub> to ESA<sub class="ltx_sub" id="S6.SS4.SSS0.Px2.p1.2.2">2</sub>, where each protocol was evaluated by different group of annotators (bilingual annotators vs. translators).
To calculate MQM’s inter-annotator agreement, we compare our MQM run with MQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px2.p1.2.3">WMT</sup>.
However, the comparison is not as 1:1 as for ESA protocol.
Our MQM was collected with different interface and system outputs have been shown to annotators in a different order.
Results in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T5" title="In Intra annotator agreement ‣ 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> suggest, that ESA has also higher inter-annotator agreement than MQM.
Unfortunately, we could not rerun DA+SQM protocol to calculate agreements, which needs to be reevaluated in future work.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Error span agreement.</h4>
<div class="ltx_para" id="S6.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS4.SSS0.Px3.p1.1">We now investigate how much MQM annotators agree on the error spans, error categories and severity levels. We evaluate from two angles: <span class="ltx_text ltx_font_italic" id="S6.SS4.SSS0.Px3.p1.1.1">intra</span>, where we check if the same annotator marks the same error spans or have overlapping parts, and <span class="ltx_text ltx_font_italic" id="S6.SS4.SSS0.Px3.p1.1.2">inter</span>, where we compare our MQM error spans to MQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px3.p1.1.3">WMT</sup>.
<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T6" title="In Error span agreement. ‣ 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows that only in 30% of cases, the same annotator marked at least part of the same segment as an error regardless error severity and category. If we look at cases preserving severity and category, this number drops to 8.4%.</p>
</div>
<div class="ltx_para" id="S6.SS4.SSS0.Px3.p2">
<p class="ltx_p" id="S6.SS4.SSS0.Px3.p2.1">When comparing the inter annotator agreement, only 50% of errors are overlapping. This number is higher as the total number of errors in MQM<sup class="ltx_sup" id="S6.SS4.SSS0.Px3.p2.1.1">WMT</sup> is 7<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS4.SSS0.Px3.p2.1.m1.1"><semantics id="S6.SS4.SSS0.Px3.p2.1.m1.1a"><mo id="S6.SS4.SSS0.Px3.p2.1.m1.1.1" xref="S6.SS4.SSS0.Px3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px3.p2.1.m1.1b"><times id="S6.SS4.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.SSS0.Px3.p2.1.m1.1d">×</annotation></semantics></math> higher than in MQM, therefore it is more likely an error will have overlap.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.1" style="width:411.9pt;height:198.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(94.0pt,-45.3pt) scale(1.83946129450176,1.83946129450176) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.1.1.1.1">
<td class="ltx_td ltx_border_tt" id="S6.T6.1.1.1.1.1"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.2">Intra AA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.3">Inter AA</th>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.1.1.2.2.1">Any errors</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.1.1.2.2.2">29.3%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.1.1.2.2.3">50.2%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.3.3">
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.3.3.1">Same severity</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.3.3.2">16.9%</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.3.3.3">23.7%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.4.4">
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.4.4.1">Same category</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.4.4.2">18.9%</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.4.4.3">24.1%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.5.5">
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.5.5.1">Same sev. + categ.</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.5.5.2">11.6%</td>
<td class="ltx_td ltx_align_left" id="S6.T6.1.1.5.5.3">10.0%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T6.1.1.6.6.1">Same sev. + subcateg.</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T6.1.1.6.6.2">8.4%</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T6.1.1.6.6.3">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>ESA intra- and inter-annotator agreement (frequency) on marking overlapping errors with same severities, categories or subcategories.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Quality control.</h4>
<div class="ltx_para" id="S6.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S6.SS4.SSS0.Px4.p1.1">To measure the quality of annotations, we added “attention checks” in the form of segments for which we can reliably check whether the annotator annotated them correctly or not.
In random documents, we perturbed the translation by replacing part of it with random sequence of words of the same length introducing a major translation error.
Within 100 segments annotated by the annotator, they see both original and perturbed versions of that document.
We can control the annotation quality by checking if the perturbed documents received more error spans.
We show a worked-out <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.F4" title="In Quality control. ‣ 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F4.1.1" style="width:403.3pt;"><span class="ltx_text ltx_font_bold" id="S6.F4.1.1.2">SRC</span><span class="ltx_text ltx_font_italic" id="S6.F4.1.1.3">:   Sie haben gestern das Treffen wieder verschoben. 
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="S6.F4.1.1.4">TGT</span><span class="ltx_text ltx_font_italic" id="S6.F4.1.1.5">:   He postponed the meeting again yesterday. 
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="S6.F4.1.1.1">TGT<math alttext="{}^{\textbf{P}}" class="ltx_Math" display="inline" id="S6.F4.1.1.1.m1.1"><semantics id="S6.F4.1.1.1.m1.1a"><msup id="S6.F4.1.1.1.m1.1.1" xref="S6.F4.1.1.1.m1.1.1.cmml"><mi id="S6.F4.1.1.1.m1.1.1a" xref="S6.F4.1.1.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_bold" id="S6.F4.1.1.1.m1.1.1.1" xref="S6.F4.1.1.1.m1.1.1.1a.cmml">P</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F4.1.1.1.m1.1b"><apply id="S6.F4.1.1.1.m1.1.1.cmml" xref="S6.F4.1.1.1.m1.1.1"><ci id="S6.F4.1.1.1.m1.1.1.1a.cmml" xref="S6.F4.1.1.1.m1.1.1.1"><mtext class="ltx_mathvariant_bold" id="S6.F4.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.F4.1.1.1.m1.1.1.1">P</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.1.1.1.m1.1c">{}^{\textbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S6.F4.1.1.1.m1.1d">start_FLOATSUPERSCRIPT P end_FLOATSUPERSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_italic" id="S6.F4.1.1.6">: He postponed the meeting <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.F4.1.1.6.1">squirrels tense</span>.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F4.6">{example}</span></div>
</div>
<figcaption class="ltx_caption ltx_centering">An example of a perturbed translation <span class="ltx_text ltx_font_bold" id="S6.F4.4.1">TGT<math alttext="{}^{\textbf{P}}" class="ltx_Math" display="inline" id="S6.F4.4.1.m1.1"><semantics id="S6.F4.4.1.m1.1b"><msup id="S6.F4.4.1.m1.1.1" xref="S6.F4.4.1.m1.1.1.cmml"><mi id="S6.F4.4.1.m1.1.1b" xref="S6.F4.4.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_bold" id="S6.F4.4.1.m1.1.1.1" xref="S6.F4.4.1.m1.1.1.1a.cmml">P</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F4.4.1.m1.1c"><apply id="S6.F4.4.1.m1.1.1.cmml" xref="S6.F4.4.1.m1.1.1"><ci id="S6.F4.4.1.m1.1.1.1a.cmml" xref="S6.F4.4.1.m1.1.1.1"><mtext class="ltx_mathvariant_bold" id="S6.F4.4.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.F4.4.1.m1.1.1.1">P</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.4.1.m1.1d">{}^{\textbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S6.F4.4.1.m1.1e">start_FLOATSUPERSCRIPT P end_FLOATSUPERSCRIPT</annotation></semantics></math></span> based on the original system translation <span class="ltx_text ltx_font_bold" id="S6.F4.13.3">TGT</span>. The <span class="ltx_text ltx_font_bold" id="S6.F4.14.4">TGT</span> has one error (<span class="ltx_text ltx_font_italic" id="S6.F4.15.5">He</span> should be <span class="ltx_text ltx_font_italic" id="S6.F4.16.6">You</span> or <span class="ltx_text ltx_font_italic" id="S6.F4.17.7">They</span>) and <span class="ltx_text ltx_font_bold" id="S6.F4.5.2">TGT<math alttext="{}^{\textbf{P}}" class="ltx_Math" display="inline" id="S6.F4.5.2.m1.1"><semantics id="S6.F4.5.2.m1.1b"><msup id="S6.F4.5.2.m1.1.1" xref="S6.F4.5.2.m1.1.1.cmml"><mi id="S6.F4.5.2.m1.1.1b" xref="S6.F4.5.2.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_bold" id="S6.F4.5.2.m1.1.1.1" xref="S6.F4.5.2.m1.1.1.1a.cmml">P</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.F4.5.2.m1.1c"><apply id="S6.F4.5.2.m1.1.1.cmml" xref="S6.F4.5.2.m1.1.1"><ci id="S6.F4.5.2.m1.1.1.1a.cmml" xref="S6.F4.5.2.m1.1.1.1"><mtext class="ltx_mathvariant_bold" id="S6.F4.5.2.m1.1.1.1.cmml" mathsize="70%" xref="S6.F4.5.2.m1.1.1.1">P</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.5.2.m1.1d">{}^{\textbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S6.F4.5.2.m1.1e">start_FLOATSUPERSCRIPT P end_FLOATSUPERSCRIPT</annotation></semantics></math></span> introduces one more errors (<span class="ltx_text ltx_font_italic" id="S6.F4.18.8">squirrels tense</span>).</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.SSS0.Px4.p2">
<p class="ltx_p" id="S6.SS4.SSS0.Px4.p2.1">The MQM and ESA setups used the same perturbations and we show their results in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.T7" title="In Quality control. ‣ 6.4 Quality of annotations ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>.
The scores comparing original and perturbed segments for ESA and MQM are vastly different,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The protocols use different scales, for example, one major error under MQM is <math alttext="-5" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><mrow id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml"><mo id="footnote4.m1.1.1b" xref="footnote4.m1.1.1.cmml">−</mo><mn id="footnote4.m1.1.1.2" xref="footnote4.m1.1.1.2.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><apply id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1"><minus id="footnote4.m1.1.1.1.cmml" xref="footnote4.m1.1.1"></minus><cn id="footnote4.m1.1.1.2.cmml" type="integer" xref="footnote4.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">-5</annotation><annotation encoding="application/x-llamapun" id="footnote4.m1.1e">- 5</annotation></semantics></math> points, while 25 points in ESA represents quarter of the full scale.</span></span></span> showing that annotators paid attention to the quality control items.
For ESA the original segment had a higher score than the perturbed one in 86% of cases, while for MQM in 78% of cases. When investigating whether an annotator marked the error span or not, MQM has higher recall than ESA, however, this is less crucial for ESA as annotator can adjust the ranking without marking the error.</p>
</div>
<figure class="ltx_table" id="S6.T7">
<div class="ltx_inline-block ltx_transformed_outer" id="S6.T7.1" style="width:433.6pt;height:193.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.6pt,-33.7pt) scale(1.53553926581393,1.53553926581393) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T7.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T7.1.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T7.1.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T7.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.3.1">Original</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T7.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.4.1">Perturbed</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T7.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.5.1">OK</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.2.1.1.1">ESA</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.1.2.1.2">Score</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.2.1.3">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.2.1.4">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.2.1.5">86%</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.1.3.2.1">Span count</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.3.2.2">0.85</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.3.2.3">1.86</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.3.2.4">54%</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.1.4.3.1" style="padding-bottom:6.99997pt;">Perturbation marked</th>
<td class="ltx_td" id="S6.T7.1.1.4.3.2" style="padding-bottom:6.99997pt;"></td>
<td class="ltx_td" id="S6.T7.1.1.4.3.3" style="padding-bottom:6.99997pt;"></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.4.3.4" style="padding-bottom:6.99997pt;">56%</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T7.1.1.5.4.1" rowspan="3"><span class="ltx_text" id="S6.T7.1.1.5.4.1.1">
<span class="ltx_inline-block ltx_align_left" id="S6.T7.1.1.5.4.1.1.1">
<span class="ltx_p" id="S6.T7.1.1.5.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.5.4.1.1.1.1.1">MQM</span></span>
</span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.1.5.4.2">Score</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.5.4.3">-1.87</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.5.4.4">-6.49</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.5.4.5">78%</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.1.6.5.1">Span count</th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.6.5.2">0.66</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.6.5.3">1.68</td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.1.6.5.4">70%</td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T7.1.1.7.6.1">Perturbation marked</th>
<td class="ltx_td ltx_border_bb" id="S6.T7.1.1.7.6.2"></td>
<td class="ltx_td ltx_border_bb" id="S6.T7.1.1.7.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.1.7.6.4">76%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Annotations assigned to perturbed attention check items (either scores or number of spans).
<span class="ltx_text ltx_font_bold" id="S6.T7.3.1">OK</span> is percentage in how many cases the non-perturbed item received a higher score or had fewer error spans, and how often the perturbed span was marked by the annotator.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Annotation time</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">One of the reasons behind development of the new protocol is reducing the time requirements of the evaluation.
In this section, we analyze times on our experiments only, therefore our rerun of MQM in contrast to ESA.
Assessing the speed of annotations is challenging as annotators took breaks during the annotation (from short breaks taking several minutes up to several hours).
This makes the evaluation of time problematic and we therefore investigate the time estimate in several ways.
Counting all annotators together, the median time for annotation per single paragraph for MQM is 38 seconds and for ESA is 29 seconds, a reduction of 23%.
However, as median time for each annotator fluctuates, we look at the average median time across annotators.
<span class="ltx_text ltx_font_bold" id="S6.SS5.p1.1.1">For MQM, the median is 49 seconds and for ESA it is 34 seconds, a reduction of 32%</span>.
This can be contributed mainly to the less demanding error span annotation approach used in ESA.</p>
</div>
<section class="ltx_paragraph" id="S6.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Speedup during annotations.</h4>
<div class="ltx_para" id="S6.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS5.SSS0.Px1.p1.1">Naturally, the reported total annotation time does not distinguish between the duration of the first and last annotations.
In practice, annotators <span class="ltx_text ltx_font_italic" id="S6.SS5.SSS0.Px1.p1.1.1">learn</span> to perform the annotation task more effectively.
In <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S6.F5" title="In Speedup during annotations. ‣ 6.5 Annotation time ‣ 6 Analysis ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> we show time per segment depending on how many segments the annotator already processed.
For both MQM and ESA, there is a small learning effect.
For MQM, with each segment, the annotator becomes 0.20s faster, while for ESA this is 0.17s.</p>
</div>
<figure class="ltx_figure" id="S6.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="311" id="S6.F5.g1" src="x4.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="311" id="S6.F5.g2" src="x5.png" width="830"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Time per segment with respect to progression in the annotation. The faint gray lines represent individual annotators, while the bold black line shows the average time. The lines are smoothed with a window of size 15 segments. We also compute the average speed at the beginning and at the end, which yields the <span class="ltx_text ltx_font_italic" id="S6.F5.2.1">learned speedup</span>. This is how much the annotator speeds up after working on one segment.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Existing annotation protocols for machine translation evaluation are either expensive because they require expert labor (MQM), or they are noisy and less reliable (DA+SQM).
To this end, we propose, describe, and analyze <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">Error Span Annotation (ESA)</span>, which builds on top of previous protocols to enable economic evaluation at scale.
It works by asking the annotators to mark error spans, but with only the error severities and not types.
In contrast to MQM, we also solicit final translation score.
This is more reliable than DA+SQM, as the annotators are primed and informed about the translation errors to assess quality of longer documents.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We showed that our protocol has the higher inter and intra annotator agreement than MQM while being 32% faster. In addition, the protocol does not require annotators trained in MQM categorizations.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Lastly, we showed that relying only on error spans and not using the ranking score as we did in ESA<sub class="ltx_sub" id="S7.p3.1.1">spans</sub> produces suboptimal scoring, therefore the combination of error spans and ranking seems to produce the best results.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">A possible limitation in contrast to MQM is that now the system evaluation does not provide breakdown of error types, which could help practitioners in improving their systems.
ESA does not provide this, though the goal is primarily <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.1">evaluation</span> and not <span class="ltx_text ltx_font_italic" id="Sx1.p1.1.2">diagnosis</span>.
Because of this and the costs of scaling expert labor, we are convinced that this is not a true shortcoming of ESA. Furthermore, the annotated errors can be further classified and analysed, if necessary.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Our experiments, for monetary reasons, were done only on one language pair, English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="Sx1.p2.1.m1.1"><semantics id="Sx1.p2.1.m1.1a"><mo id="Sx1.p2.1.m1.1.1" stretchy="false" xref="Sx1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Sx1.p2.1.m1.1b"><ci id="Sx1.p2.1.m1.1.1.cmml" xref="Sx1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="Sx1.p2.1.m1.1d">→</annotation></semantics></math>German.
Nevertheless, it is unlikely that the results would be vastly different for other languages.
The most difficult setup could be with Chinese and Japanese texts that do not use spaces.
However, we made a deliberate decision to allow highlighting of individual characters, as opposed to words, so that the user experience is unified across all languages.
This was done in spite of speed improvements (selecting on the word level is easier than selecting individual character boundaries) in order to make the tool scalable to a large range of languages.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The annotators were paid a standard commercial translator wage in the respective country. The experts in the MQM annotation has been paid double the hourly wage.
No personal data was collected and the showed data was screened for potentially disturbing content.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">We follow up with a questionnaire asking annotators on their feedback. Almost all annotators specified that the annotation experience was positive and instructions were clear. The main concern they mentioned was that some documents have been too long to evaluate.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ALPAC (1966)</span>
<span class="ltx_bibblock">
ALPAC. 1966.

</span>
<span class="ltx_bibblock">Language and machines. Computers in translation and linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2017)</span>
<span class="ltx_bibblock">
Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4717" title="">Findings of the 2017 conference on machine translation (WMT17)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Second Conference on Machine Translation</em>, 169–214. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2016)</span>
<span class="ltx_bibblock">
Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W16-2301" title="">Findings of the 2016 conference on machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers</em>, 131–198. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Callison-Burch et al. (2007)</span>
<span class="ltx_bibblock">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W07-0718" title="">(meta-) evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Second Workshop on Statistical Machine Translation</em>, 136–158. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Callison-Burch et al. (2008)</span>
<span class="ltx_bibblock">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W08-0309" title="">Further meta-evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Third Workshop on Statistical Machine Translation</em>, 70–106. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castilho (2020)</span>
<span class="ltx_bibblock">
Sheila Castilho. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.137" title="">On the same page? comparing inter-annotator agreement in sentence and document level human machine translation evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, 1150–1159. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Federmann (2018)</span>
<span class="ltx_bibblock">
Christian Federmann. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/C18-2019" title="">Appraise evaluation framework for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</em>, 86–88. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2021a)</span>
<span class="ltx_bibblock">
Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00437" title="">Experts, errors, and context: A large-scale study of human evaluation for machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>, 9:1460–1474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2023)</span>
<span class="ltx_bibblock">
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.51" title="">Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, 578–628. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2021b)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.73" title="">Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, 733–774. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graham et al. (2013)</span>
<span class="ltx_bibblock">
Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W13-2305" title="">Continuous measurement scales in human evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</em>, 33–41. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klubička et al. (2018)</span>
<span class="ltx_bibblock">
Filip Klubička, Antonio Toral, and Víctor M. Sánchez-Cartagena. 2018.

</span>
<span class="ltx_bibblock">Quantitative Fine-grained Human Evaluation of Machine Translation Systems: A Case Study on English to Croatian.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Machine Translation</em>, 32(3):195–215.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knowles and Lo (2024)</span>
<span class="ltx_bibblock">
Rebecca Knowles and Chi-kiu Lo. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/nlp.2024.5" title="">Calibration and context in human evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Natural Language Processing</em>, 1–25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.1" title="">Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, 1–42. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.1" title="">Findings of the 2022 conference on machine translation (WMT22)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, 1–45. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2021)</span>
<span class="ltx_bibblock">
Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.57" title="">To ship or not to ship: An extensive evaluation of automatic metrics for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, 478–494. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Monz (2006)</span>
<span class="ltx_bibblock">
Philipp Koehn and Christof Monz. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W06-3114" title="">Manual and automatic evaluation of machine translation between European languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings on the Workshop on Statistical Machine Translation</em>, 102–121. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutzer et al. (2020)</span>
<span class="ltx_bibblock">
Julia Kreutzer, Nathaniel Berger, and Stefan Riezler. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.15" title="">Correct me if you can: Learning from error corrections and markings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, 135–144. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lommel et al. (2014)</span>
<span class="ltx_bibblock">
Arle Lommel, Aljoscha Burchardt, Maja Popović, Kim Harris, Eleftherios Avramidis, and Hans Uszkoreit. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2014.eamt-1.38" title="">Using a new analytic measure for the annotation and analysis of MT errors on real data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 17th Annual Conference of the European Association for Machine Translation</em>, 165–172. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martindale and Carpuat (2018)</span>
<span class="ltx_bibblock">
Marianna Martindale and Marine Carpuat. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W18-1803" title="">Fluency over adequacy: A pilot study in measuring user trust in imperfect MT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</em>, 13–25. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2020)</span>
<span class="ltx_bibblock">
Maja Popović. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.444" title="">Informative manual evaluation of machine translation output</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, 5059–5069. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riley et al. (2024)</span>
<span class="ltx_bibblock">
Parker Riley, Daniel Deutsch, George Foster, Viresh Ratnakar, Ali Dabirmoghaddam, and Markus Freitag. 2024.

</span>
<span class="ltx_bibblock">Finding replicable human evaluations via stable ranking probability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2404.01474</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rottger et al. (2022)</span>
<span class="ltx_bibblock">
Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pierrehumbert. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.13" title="">Two contrasting data annotation paradigms for subjective NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 175–190. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al. (2007)</span>
<span class="ltx_bibblock">
David Vilar, Gregor Leusch, Hermann Ney, and Rafael E. Banchs. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W07-0713" title="">Human evaluation of machine translation through binary system comparisons</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Second Workshop on Statistical Machine Translation</em>, 96–103. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al. (2006)</span>
<span class="ltx_bibblock">
David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/413_pdf.pdf" title="">Error analysis of statistical machine translation output</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06)</em>. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Johnny Wei, Tom Kocmi, and Christian Federmann. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.7" title="">Searching for a higher power in the human evaluation of MT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, 129–139. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al. (1994)</span>
<span class="ltx_bibblock">
John S. White, Theresa A. O’Connell, and Francis E. O’Mara. 1994.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/1994.amta-1.25" title="">The ARPA MT evaluation methodologies: Evolution, lessons, and future approaches</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the First Conference of the Association for Machine Translation in the Americas</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>User Guidelines</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The following are are annotation guidelines for our local ESA and MQM campaigns.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>ESA (Error Span Annotations)</h3>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Higlighting errors:</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">Highlight the text fragment where you have identified a translation error (drag or click start &amp; end).
Click repeatedly on the highlighted fragment to increase its severity level or to remove the selection.</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">Minor Severity:</span> Style/grammar/lexical choice could be better/more natural.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">Major Severity:</span> Seriously changed meaning, difficult to read, decreases usability.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.2">If something is missing from the text, mark it as an error on the <span class="ltx_text ltx_font_bold" id="A1.SS1.SSS0.Px1.p1.2.1">[MISSING]</span> word.
The highlights do not have to have character-level precision. It’s sufficient if you highlight the word or rough area where the error appears.
Each error should have a separate highlight.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Score:</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">After highlighting all errors, please set the overall segment translation scores. The quality levels associated with numerical scores on the slider:</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">0%: No meaning preserved: Nearly all information is lost in the translation.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">33%: Some meaning preserved: Some of the meaning is preserved but significant parts are missing. The narrative is hard to follow due to errors. Grammar may be poor.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">66%: Most meaning preserved and few grammar mistakes: The translation retains most of the meaning. It may have some grammar mistakes or minor inconsistencies.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">100%: Perfect meaning and grammar: The meaning and grammar of the translation is completely consistent with the source.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>MQM (Multidimensional Quality Metrics) </h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Higlighting errors:</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">Highlight the text fragment where you have identified a translation error (drag or click start &amp; end).
Click repeatedly on the highlighted fragment to increase its severity level or to remove the selection.</p>
<ul class="ltx_itemize" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i1.p1.1.1">Minor Severity</span>: Style/grammar/lexical choice could be better/more natural.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i2.p1.1.1">Major Severity</span>: Seriously changed meaning, difficult to read, decreases usability.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.2">If something is missing from the text, mark it as an error on the <span class="ltx_text ltx_font_bold" id="A1.SS2.SSS0.Px1.p1.2.1">[MISSING]</span> word.
The highlights do not have to have character-level precision. It’s sufficient if you highlight the word or rough area where the error appears.
Each error should have a separate highlight.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Error types:</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">After highlighting an error fragment, you will be asked to select the specific error type (main category and subcategory).
If you are unsure about which errors fall under which categories, please consult the <a class="ltx_ref ltx_href" href="https://themqm.org/the-mqm-typology/" title="">typology definitions</a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional results</h2>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="397" id="A2.F6.g1" src="x6.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Intra annotator agreement - changes in scoring by the same annotator when evaluated again. Each point represents single annotated segment with x-axis being annotator’s score assigned in March and y-axis their score assigned in May.</figcaption>
</figure>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>From error spans to final score</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">To find out what influences the score, we show correlation between individual segment-level features in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.T8" title="In B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>.
On average, longer segments have lower translation quality.
Importantly, the error counts normalized by segment length correlate less than the non-normalized counterparts.
However, we note that the normalized scores are more continuous that the non-normalized MQM computation, as per <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#S5.F2" title="In 5 Experimental setup ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="A2.T8">
<div class="ltx_inline-block ltx_transformed_outer" id="A2.T8.1" style="width:433.6pt;height:243.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(72.7pt,-40.8pt) scale(1.50415393101308,1.50415393101308) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A2.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.2.1">Feature</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.1.1">Corr. with ESA score (<math alttext="\bm{\rho}" class="ltx_Math" display="inline" id="A2.T8.1.1.1.1.1.m1.1"><semantics id="A2.T8.1.1.1.1.1.m1.1a"><mi id="A2.T8.1.1.1.1.1.m1.1.1" xref="A2.T8.1.1.1.1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A2.T8.1.1.1.1.1.m1.1b"><ci id="A2.T8.1.1.1.1.1.m1.1.1.cmml" xref="A2.T8.1.1.1.1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.1.1.1.1.1.m1.1c">\bm{\rho}</annotation><annotation encoding="application/x-llamapun" id="A2.T8.1.1.1.1.1.m1.1d">bold_italic_ρ</annotation></semantics></math>)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T8.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T8.1.1.2.1.1">Source token count</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.2.1.2">-0.16</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.3.2.1">Target token count</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.3.2.2">-0.06</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.4.3.1">Minor error count</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.4.3.2">-0.20</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.5.4.1">Major error count</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.5.4.2">-0.52</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.6.5.1">Missing error count</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.6.5.2">-0.45</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.7.6.1">Minor error count (normalized)</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.7.6.2">-0.13</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.1.8.7.1">Major error count (normalized)</th>
<td class="ltx_td ltx_align_center" id="A2.T8.1.1.8.7.2">-0.37</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T8.1.1.9.8.1">Missing error count (normalized)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.9.8.2">-0.31</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Segment-level Pearson correlation of individual features with the ESA score.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="443" id="A2.F7.g1" src="x7.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Correlation between direct ESA score and scores computed from error spans with minor errors having weight <math alttext="-1" class="ltx_Math" display="inline" id="A2.F7.3.m1.1"><semantics id="A2.F7.3.m1.1b"><mrow id="A2.F7.3.m1.1.1" xref="A2.F7.3.m1.1.1.cmml"><mo id="A2.F7.3.m1.1.1b" xref="A2.F7.3.m1.1.1.cmml">−</mo><mn id="A2.F7.3.m1.1.1.2" xref="A2.F7.3.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F7.3.m1.1c"><apply id="A2.F7.3.m1.1.1.cmml" xref="A2.F7.3.m1.1.1"><minus id="A2.F7.3.m1.1.1.1.cmml" xref="A2.F7.3.m1.1.1"></minus><cn id="A2.F7.3.m1.1.1.2.cmml" type="integer" xref="A2.F7.3.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F7.3.m1.1d">-1</annotation><annotation encoding="application/x-llamapun" id="A2.F7.3.m1.1e">- 1</annotation></semantics></math> and major errors <math alttext="-x" class="ltx_Math" display="inline" id="A2.F7.4.m2.1"><semantics id="A2.F7.4.m2.1b"><mrow id="A2.F7.4.m2.1.1" xref="A2.F7.4.m2.1.1.cmml"><mo id="A2.F7.4.m2.1.1b" xref="A2.F7.4.m2.1.1.cmml">−</mo><mi id="A2.F7.4.m2.1.1.2" xref="A2.F7.4.m2.1.1.2.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.F7.4.m2.1c"><apply id="A2.F7.4.m2.1.1.cmml" xref="A2.F7.4.m2.1.1"><minus id="A2.F7.4.m2.1.1.1.cmml" xref="A2.F7.4.m2.1.1"></minus><ci id="A2.F7.4.m2.1.1.2.cmml" xref="A2.F7.4.m2.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F7.4.m2.1d">-x</annotation><annotation encoding="application/x-llamapun" id="A2.F7.4.m2.1e">- italic_x</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">The MQM formula was crafted with respect to preserving system ranking and not segment-level matching from the same annotator <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib8" title="">2021a</a>)</cite>.
However, the construction of ESA allows us to revisit this problem as each annotator gives both the error spans and the final score.
We scan for multiple minor/major ratios of error weights and show the results in <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.F7" title="In B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>.
We find that the optimal formula that optimizes the correlation between the direct score and the score from the spans has the following form:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx2">
<tbody id="A2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\textsc{Seg,Score}=-1\cdot\textsc{\#minor}-4.8\cdot\textsc{\#major}" class="ltx_Math" display="inline" id="A2.Ex2.m1.1"><semantics id="A2.Ex2.m1.1a"><mrow id="A2.Ex2.m1.1.1" xref="A2.Ex2.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.2" xref="A2.Ex2.m1.1.1.2a.cmml">Seg,Score</mtext><mo id="A2.Ex2.m1.1.1.1" xref="A2.Ex2.m1.1.1.1.cmml">=</mo><mrow id="A2.Ex2.m1.1.1.3" xref="A2.Ex2.m1.1.1.3.cmml"><mrow id="A2.Ex2.m1.1.1.3.2" xref="A2.Ex2.m1.1.1.3.2.cmml"><mo id="A2.Ex2.m1.1.1.3.2a" xref="A2.Ex2.m1.1.1.3.2.cmml">−</mo><mrow id="A2.Ex2.m1.1.1.3.2.2" xref="A2.Ex2.m1.1.1.3.2.2.cmml"><mn id="A2.Ex2.m1.1.1.3.2.2.2" xref="A2.Ex2.m1.1.1.3.2.2.2.cmml">1</mn><mo id="A2.Ex2.m1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="A2.Ex2.m1.1.1.3.2.2.1.cmml">⋅</mo><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.3.2.2.3" xref="A2.Ex2.m1.1.1.3.2.2.3a.cmml">#minor</mtext></mrow></mrow><mo id="A2.Ex2.m1.1.1.3.1" xref="A2.Ex2.m1.1.1.3.1.cmml">−</mo><mrow id="A2.Ex2.m1.1.1.3.3" xref="A2.Ex2.m1.1.1.3.3.cmml"><mn id="A2.Ex2.m1.1.1.3.3.2" xref="A2.Ex2.m1.1.1.3.3.2.cmml">4.8</mn><mo id="A2.Ex2.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A2.Ex2.m1.1.1.3.3.1.cmml">⋅</mo><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.3.3.3" xref="A2.Ex2.m1.1.1.3.3.3a.cmml">#major</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex2.m1.1b"><apply id="A2.Ex2.m1.1.1.cmml" xref="A2.Ex2.m1.1.1"><eq id="A2.Ex2.m1.1.1.1.cmml" xref="A2.Ex2.m1.1.1.1"></eq><ci id="A2.Ex2.m1.1.1.2a.cmml" xref="A2.Ex2.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.2.cmml" xref="A2.Ex2.m1.1.1.2">Seg,Score</mtext></ci><apply id="A2.Ex2.m1.1.1.3.cmml" xref="A2.Ex2.m1.1.1.3"><minus id="A2.Ex2.m1.1.1.3.1.cmml" xref="A2.Ex2.m1.1.1.3.1"></minus><apply id="A2.Ex2.m1.1.1.3.2.cmml" xref="A2.Ex2.m1.1.1.3.2"><minus id="A2.Ex2.m1.1.1.3.2.1.cmml" xref="A2.Ex2.m1.1.1.3.2"></minus><apply id="A2.Ex2.m1.1.1.3.2.2.cmml" xref="A2.Ex2.m1.1.1.3.2.2"><ci id="A2.Ex2.m1.1.1.3.2.2.1.cmml" xref="A2.Ex2.m1.1.1.3.2.2.1">⋅</ci><cn id="A2.Ex2.m1.1.1.3.2.2.2.cmml" type="integer" xref="A2.Ex2.m1.1.1.3.2.2.2">1</cn><ci id="A2.Ex2.m1.1.1.3.2.2.3a.cmml" xref="A2.Ex2.m1.1.1.3.2.2.3"><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.3.2.2.3.cmml" xref="A2.Ex2.m1.1.1.3.2.2.3">#minor</mtext></ci></apply></apply><apply id="A2.Ex2.m1.1.1.3.3.cmml" xref="A2.Ex2.m1.1.1.3.3"><ci id="A2.Ex2.m1.1.1.3.3.1.cmml" xref="A2.Ex2.m1.1.1.3.3.1">⋅</ci><cn id="A2.Ex2.m1.1.1.3.3.2.cmml" type="float" xref="A2.Ex2.m1.1.1.3.3.2">4.8</cn><ci id="A2.Ex2.m1.1.1.3.3.3a.cmml" xref="A2.Ex2.m1.1.1.3.3.3"><mtext class="ltx_font_smallcaps" id="A2.Ex2.m1.1.1.3.3.3.cmml" xref="A2.Ex2.m1.1.1.3.3.3">#major</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex2.m1.1c">\displaystyle\textsc{Seg,Score}=-1\cdot\textsc{\#minor}-4.8\cdot\textsc{\#major}</annotation><annotation encoding="application/x-llamapun" id="A2.Ex2.m1.1d">Seg,Score = - 1 ⋅ #minor - 4.8 ⋅ #major</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS1.p2.2">which is very close to the originally proposed 1:5 ratio.
From <a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#A2.F7" title="In B.1 From error spans to final score ‣ Appendix B Additional results ‣ Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>, the weight of the major error class seems to have a much bigger effect on the final translation score, suggesting that minor errors play a lesser role.</p>
</div>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="277" id="A2.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>System scores illustrating differences between DA+SQM and MQM. Each point is a single system and dashed lines mark clusters. DA+SQM produces fewer clusters and groups many systems into one single cluster, while MQM better distinguishes different systems. Scores and clusters are from <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.11580v1#bib.bib14" title="">2023</a>)</cite>.
</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="278" id="A2.F9.g1" src="extracted/5672911/img/appraise_tutorial_0.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="280" id="A2.F9.g2" src="extracted/5672911/img/appraise_tutorial_1.png" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Tutorial to ESA annotations shown at the beginning of the campaign. All segments needed to be annotated correctly before continuing.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 17 14:12:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
