<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–</title>
<!--Generated on Thu Jan 25 22:55:21 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.14559v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1" title="Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S1" title="1.1 Background and Motivation ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Background and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2" title="1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Research Questions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1" title="1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1 </span><span class="ltx_text ltx_font_smallcaps">RQ1. Adaptive and Interactive MT</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS1" title="1.2.1.1 RQ1.a Translation autosuggestions and autocompletion ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1.1 </span>RQ1.a Translation autosuggestions and autocompletion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS2" title="1.2.1.2 RQ1.b Adaptive MT with LLMs ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1.2 </span>RQ1.b Adaptive MT with LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS3" title="1.2.1.3 RQ1.c Terminology-constrained MT with LLMs ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1.3 </span>RQ1.c Terminology-constrained MT with LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2" title="1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.2 </span><span class="ltx_text ltx_font_smallcaps">RQ2. Domain-specific Text Generation for MT</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS1" title="1.2.2.1 RQ2.a Insufficient in-domain data ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.2.1 </span>RQ2.a Insufficient in-domain data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS2" title="1.2.2.2 RQ2.c Terminology-aware text generation ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.2.2 </span>RQ2.c Terminology-aware text generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS2.Px1" title="Term-based generation: ‣ 1.2.2.2 RQ2.c Terminology-aware text generation ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Term-based generation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS2.Px2" title="Term-constrained generation: ‣ 1.2.2.2 RQ2.c Terminology-aware text generation ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Term-constrained generation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS2.Px3" title="Bilingual few-shot in-context learning: ‣ 1.2.2.2 RQ2.c Terminology-aware text generation ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Bilingual few-shot in-context learning:</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S3" title="1.3 Publications ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Publications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4" title="1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Research Context</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS1" title="1.4.1 Adaptive and Interactive MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.1 </span>Adaptive and Interactive MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS2" title="1.4.2 Terminology-Constrained MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.2 </span>Terminology-Constrained MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS3" title="1.4.3 Retrieval-Augmented MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.3 </span>Retrieval-Augmented MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS4" title="1.4.4 Retrieval-Augmented LLMs ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.4 </span>Retrieval-Augmented LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS5" title="1.4.5 In-Context Learning for LLMs and MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.5 </span>In-Context Learning for LLMs and MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S5" title="1.5 Contribution ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Contribution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Domain-Specific Text Generation for Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S1" title="2.1 Context ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S2" title="2.2 Introduction ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S3" title="2.3 Related Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4.SS1" title="2.4.1 Use Case 1: Limited bilingual in-domain data available ‣ 2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Use Case 1: Limited bilingual in-domain data available</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4.SS2" title="2.4.2 Use Case 2: Zero bilingual in-domain data available ‣ 2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Use Case 2: Zero bilingual in-domain data available</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5" title="2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS1" title="2.5.1 Datasets ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS2" title="2.5.2 Vocabulary ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.2 </span>Vocabulary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS3" title="2.5.3 NMT Model Architecture ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.3 </span>NMT Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS4" title="2.5.4 Training ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS5" title="2.5.5 Domain-Specific Data Generation with LMs ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.5 </span>Domain-Specific Data Generation with LMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS6" title="2.5.6 Back-Translation ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.6 </span>Back-Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS7" title="2.5.7 Mixed Fine-tuning ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.7 </span>Mixed Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6" title="2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS1" title="2.6.1 Automatic Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.1 </span>Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS2" title="2.6.2 Human Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.2 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS3" title="2.6.3 Linguistic Analysis ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.3 </span>Linguistic Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S7" title="2.7 Conclusion and Future Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Translation Word-Level Auto-Completion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S1" title="3.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S2" title="3.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S3" title="3.3 User Survey ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>User Survey</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4" title="3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4.SS0.SSS0.Px1" title="Models ‣ 3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4.SS0.SSS0.Px2" title="Tokenisers ‣ 3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Tokenisers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4.SS0.SSS0.Px3" title="Inference Engine ‣ 3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Inference Engine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4.SS0.SSS0.Px4" title="Pinyin ‣ 3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Pinyin</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S5" title="3.5 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S6" title="3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Other Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S7" title="3.7 Conclusion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Adaptive Machine Translation with Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S1" title="4.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S2" title="4.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S3" title="4.3 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S4" title="4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Adaptive MT with Fuzzy Matches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S5" title="4.5 GPT-3 vs Encoder-Decoder MT Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>GPT-3 vs Encoder-Decoder MT Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6" title="4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Incorporating Encoder-Decoder MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6.SS1" title="4.6.1 Fuzzy matches + new segment MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Fuzzy matches + new segment MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6.SS2" title="4.6.2 Fuzzy matches + all segments MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>Fuzzy matches + all segments MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S7" title="4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Bilingual Terminology Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S8" title="4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Terminology-Constrained MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S9" title="4.9 ChatGPT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.9 </span>ChatGPT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S10" title="4.10 BLOOM and BLOOMZ ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.10 </span>BLOOM and BLOOMZ</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S11" title="4.11 Conclusion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.11 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12" title="4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12 </span>Prompts</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12.SS0.SSS1" title="4.12.0.1 Zero-shot Translation ‣ 4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12.0.1 </span>Zero-shot Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12.SS0.SSS2" title="4.12.0.2 Adaptive MT with Fuzzy Matches ‣ 4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12.0.2 </span>Adaptive MT with Fuzzy Matches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12.SS0.SSS3" title="4.12.0.3 MT Post-editing ‣ 4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12.0.3 </span>MT Post-editing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12.SS0.SSS4" title="4.12.0.4 Terminology Extraction ‣ 4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12.0.4 </span>Terminology Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12.SS0.SSS5" title="4.12.0.5 Terminology-constrained MT ‣ 4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12.0.5 </span>Terminology-constrained MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Fine-tuning Large Language Models for Adaptive Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S1" title="5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2" title="5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Information Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2.SS0.SSS0.Px1" title="Embedding: ‣ 5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Embedding:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2.SS0.SSS0.Px2" title="Indexing: ‣ 5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Indexing:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2.SS0.SSS0.Px3" title="Semantic Search: ‣ 5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Semantic Search:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S3" title="5.3 Fine-tuning ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S4" title="5.4 Inference ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Inference</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S4.SS0.SSS0.Px1" title="Mistral 7B: ‣ 5.4 Inference ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">Mistral 7B:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S4.SS0.SSS0.Px2" title="ChatGPT: ‣ 5.4 Inference ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">ChatGPT:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S4.SS0.SSS0.Px3" title="NLLB-200: ‣ 5.4 Inference ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title">NLLB-200:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S5" title="5.5 Conclusion and Future Work ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Domain Terminology <span class="ltx_text">Integration</span> into Machine Translation:
<span class="ltx_text">Leveraging</span> Large Language <span class="ltx_text">Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S1" title="6.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S2" title="6.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3" title="6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS1" title="6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Synthetic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS2" title="6.3.2 Fine-tuning ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS3" title="6.3.3 Terminology-constrained Automatic Post-Editing ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.3 </span>Terminology-constrained Automatic Post-Editing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4" title="6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4.SS1" title="6.4.1 Term-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.1 </span>Term-level Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4.SS2" title="6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.2 </span>Sentence-level Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S5" title="6.5 Conclusion and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7" title="Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S1" title="7.1 Conclusions ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2" title="7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS1" title="7.2.1 MT adaptation for multilingual and low-resource settings ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>MT adaptation for multilingual and low-resource settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS2" title="7.2.2 Domain-specific word-level autosuggestions and autocompletion ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Domain-specific word-level autosuggestions and autocompletion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS3" title="7.2.3 Analysis of critical errors and text generation for difficult-to-translate words ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Analysis of critical errors and text generation for difficult-to-translate words</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS4" title="7.2.4 Improving random sampling via simulated annealing ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.4 </span>Improving random sampling via simulated annealing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS5" title="7.2.5 Domain-Specific Efficient Fine-tuning of LLMs ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.5 </span>Domain-Specific Efficient Fine-tuning of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS6" title="7.2.6 Cross-Lingual Retrieval-Augmented Generation ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.6 </span>Cross-Lingual Retrieval-Augmented Generation</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: nowidow</li>
<li>failed: arabtex</li>
<li>failed: utf8</li>
<li>failed: layout</li>
<li>failed: hyphenat</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.14559v1 [cs.CL] 25 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\setcode</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">utf8














</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span class="ltx_ERROR undefined" id="id8.id1">\nohyphens</span>
<p class="ltx_p" id="id9.id2">Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected <span class="ltx_text" id="id9.id2.1">translations</span> in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. Such capabilities have opened new horizons for domain-specific data augmentation and real-time adaptive MT. This work attempts to address two main relevant questions: 1) in scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? and 2) in the absence of sufficient in-domain data, can we use pre-trained large-scale language models to improve the process of MT domain adaptation?</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span class="ltx_ERROR undefined" id="id10.id1">\nohyphens</span>
<p class="ltx_p" id="id11.id2">Preservation of domain knowledge from the source to target is crucial in any translation workflow. It is common in the translation industry to receive highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune Machine Translation (MT) models, producing translations that are consistent with the relevant context is challenging. In this work, we propose a novel approach to domain adaptation leveraging state-of-the-art pretrained language models (LMs) for <span class="ltx_text" id="id11.id2.1">domain-specific</span> data augmentation for MT, simulating the domain characteristics of either (a) a small bilingual dataset, or (b) the monolingual source text to be translated. Combining this idea with back-translation, we can generate huge amounts of synthetic bilingual <span class="ltx_text" id="id11.id2.2">in-domain</span> data for both use-cases. For our investigation, we use the <span class="ltx_text" id="id11.id2.3">state-of-the-art</span> Transformer architecture. We employ mixed fine-tuning to train models that significantly improve translation of in-domain texts. More specifically, in both scenarios, our proposed methods achieve improvements of approximately <span class="ltx_text" id="id11.id2.4">5-6 BLEU</span> and 2-3 BLEU, respectively, on the <span class="ltx_text" id="id11.id2.5">Arabic-to-English</span> and <span class="ltx_text" id="id11.id2.6">English-to-Arabic</span> language pairs. Furthermore, the outcome of human evaluation corroborates the automatic evaluation results.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Ch0.Sx1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">Declaration</h3>
<div class="ltx_para" id="Ch0.Sx1.p1">
<p class="ltx_p" id="Ch0.Sx1.p1.1">I hereby certify that this material which I now submit for assessment on the programme of study leading to the award of PhD is entirely my own work, and that I have exercised reasonable care to ensure that the work is original, and does not to the best of my knowledge breach any law of copyright, and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div class="ltx_para" id="Ch0.Sx1.p2">
<p class="ltx_p" id="Ch0.Sx1.p2.1">Signed: Yasmin Moslem (Candidate)</p>
</div>
<div class="ltx_para" id="Ch0.Sx1.p3">
<p class="ltx_p" id="Ch0.Sx1.p3.1">ID No.: 19215697</p>
</div>
<div class="ltx_para" id="Ch0.Sx1.p4">
<p class="ltx_p" id="Ch0.Sx1.p4.1">Date: 1 January 2024</p>
</div>
<figure class="ltx_figure" id="Ch0.Sx1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="38" id="Ch0.Sx1.1.g1" src="x1.png" width="149"/>
</figure>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch0.Sx2">
<h3 class="ltx_title ltx_centering ltx_font_bold ltx_title_section">Dedication</h3>
<span class="ltx_ERROR ltx_centering undefined" id="Ch0.Sx2.1">\&lt;</span>
<div class="ltx_para ltx_align_center" id="Ch0.Sx2.p1">
<p class="ltx_p" id="Ch0.Sx2.p1.1">إلى أحبتي: ¿</p>
</div>
<span class="ltx_ERROR ltx_centering undefined" id="Ch0.Sx2.2">\&lt;</span>
<div class="ltx_para ltx_align_center" id="Ch0.Sx2.p2">
<p class="ltx_p" id="Ch0.Sx2.p2.1">ماما * بابا * منار ¿</p>
</div>
<span class="ltx_ERROR ltx_centering undefined" id="Ch0.Sx2.3">\&lt;</span>
<div class="ltx_para ltx_align_center" id="Ch0.Sx2.p3">
<p class="ltx_p" id="Ch0.Sx2.p3.1">بوركتم ¿</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch0.Sx3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">Acknowledgements</h3>
<div class="ltx_para" id="Ch0.Sx3.p1">
<p class="ltx_p" id="Ch0.Sx3.p1.1">This work would not have been possible without the generous support of the <span class="ltx_text" id="Ch0.Sx3.p1.1.1">Science</span> Foundation Ireland (SFI) Centre for Research Training in Digitally-Enhanced Reality (d-real) under Grant No. 18/CRT/6224, the ADAPT Centre for Digital Content Technology under SFI’s Grant No. <span class="ltx_text" id="Ch0.Sx3.p1.1.2">13/RC/2106_P2</span>, and Microsoft Research.</p>
</div>
<div class="ltx_para" id="Ch0.Sx3.p2">
<p class="ltx_p" id="Ch0.Sx3.p2.1">I am truly grateful to my supervisors, Prof Andy Way, Dr Rejwanul Haque, and Prof John Kelleher, for their invaluable guidance throughout my PhD journey. Their insightful feedback and constant encouragement played a significant role in my academic achievement and were a source of support in challenging times. I sincerely appreciate their contributions and wish them good health and further success.</p>
</div>
<div class="ltx_para" id="Ch0.Sx3.p3">
<p class="ltx_p" id="Ch0.Sx3.p3.1">Moreover, I would like to extend my sincere thanks to Julie Locquet, Senior <span class="ltx_text" id="Ch0.Sx3.p3.1.1">Linguist</span>; Philippe <span class="ltx_text" id="Ch0.Sx3.p3.1.2">Locquet</span>, Senior Linguist and Academic Program Manager at Wordfast; and Dr Muhammed Yaman Muhaisen, Ophthalmologist and Linguist, for conducting the linguistic evaluation of the translation tasks.</p>
</div>
<div class="ltx_para" id="Ch0.Sx3.p4">
<p class="ltx_p" id="Ch0.Sx3.p4.1">Finally, special thanks go to colleagues and alumni of Irish universities as well as colleagues in the language technology industry for generously offering advice and inspiration.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch0.Sx4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">Publications</h3>
<div class="ltx_para" id="Ch0.Sx4.p1">
<ol class="ltx_enumerate" id="Ch0.Sx4.I1">
<li class="ltx_item" id="Ch0.Sx4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i1.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i1.p1.1.1">Yasmin Moslem</span>, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2023.eamt-1.22/" title="">Adaptive Machine Translation with Large Language Models</a>. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227–237, Tampere, Finland. European Association for Machine Translation.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i2.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i2.p1.1.1">Yasmin Moslem</span>, Gianfranco Romani, Mahdi Molaei, John D. Kelleher, Rejwanul Haque, and Andy Way. 2023. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2023.wmt-1.82/" title="">Domain Terminology Integration into Machine Translation: Leveraging Large Language Models</a>. In Proceedings of the Eighth Conference on Machine Translation, pages 902–911, Singapore. Association for Computational Linguistics.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i3.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i3.p1.1.1">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2023. <a class="ltx_ref ltx_href ltx_font_italic" href="https://arxiv.org/abs/2312.12740" title="">Fine-tuning Large Language Models for Adaptive Machine Translation</a>. arXiv preprint arXiv:2312.12740 [cs.CL].
</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i4.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i4.p1.1.1">Yasmin Moslem</span>, Rejwanul Haque, John D. Kelleher, and Andy Way. 2022. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2022.amta-research.2/" title="">Domain-Specific Text Generation for Machine Translation</a>. In Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 14–30, Orlando, USA. Association for Machine Translation in the Americas.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i5.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i5.p1.1.1">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2022. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2022.wmt-1.119/" title="">Translation Word-Level Auto-Completion: What Can We Achieve Out of the Box?</a> In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1176–1181, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i6.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i6.p1.1">Alp Öktem, Rodolfo Zevallos, <span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i6.p1.1.1">Yasmin Moslem</span>, Güneş Öztürk, and Karen Şarhon. 2022. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2022.eurali-1.18/" title="">Preparing an endangered language for the digital age: The Case of Judeo-Spanish</a>. In Proceedings of the Workshop on Resources and Technologies for Indigenous, Endangered and Lesser-resourced Languages in Eurasia within the 13th Language Resources and Evaluation Conference, pages 105–110, Marseille, France, June 2022. European Language Resources Association.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i7.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i7.p1.1.1">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2020. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2020.nlptea-1.2/" title="">Arabisc: Context-Sensitive Neural Spelling Checker</a>. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 11–19, Suzhou, China. Association for Computational Linguistics.</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i8.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i8.p1.1">Rejwanul Haque, <span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i8.p1.1.1">Yasmin Moslem</span>, and Andy Way. 2020. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2020.icon-adapmt.4/" title="">Terminology-Aware Sentence Mining for NMT Domain Adaptation: ADAPT’s Submission to the Adap-MT 2020 English-to-Hindi AI Translation Shared Task</a>. In Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task, pages 17–23, Patna, India. NLP Association of India (NLPAI).</p>
</div>
</li>
<li class="ltx_item" id="Ch0.Sx4.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="Ch0.Sx4.I1.i9.p1">
<p class="ltx_p" id="Ch0.Sx4.I1.i9.p1.1">Rejwanul Haque, <span class="ltx_text ltx_font_bold" id="Ch0.Sx4.I1.i9.p1.1.1">Yasmin Moslem</span>, and Andy Way. 2020. <a class="ltx_ref ltx_href ltx_font_italic" href="https://aclanthology.org/2020.ngt-1.17/" title="">The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task</a>. In Proceedings of the Fourth Workshop on Neural Generation and Translation, pages 144–152, Online. Association for Computational <span class="ltx_text" id="Ch0.Sx4.I1.i9.p1.1.2">Linguistics</span>.</p>
</div>
</li>
</ol>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1" title="Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S1" title="1.1 Background and Motivation ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Background and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2" title="1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Research Questions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1" title="1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1 </span><span class="ltx_text ltx_font_smallcaps">RQ1. Adaptive and Interactive MT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2" title="1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.2 </span><span class="ltx_text ltx_font_smallcaps">RQ2. Domain-specific Text Generation for MT</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S3" title="1.3 Publications ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Publications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4" title="1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Research Context</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS1" title="1.4.1 Adaptive and Interactive MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.1 </span>Adaptive and Interactive MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS2" title="1.4.2 Terminology-Constrained MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.2 </span>Terminology-Constrained MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS3" title="1.4.3 Retrieval-Augmented MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.3 </span>Retrieval-Augmented MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS4" title="1.4.4 Retrieval-Augmented LLMs ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.4 </span>Retrieval-Augmented LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS5" title="1.4.5 In-Context Learning for LLMs and MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.5 </span>In-Context Learning for LLMs and MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S5" title="1.5 Contribution ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Contribution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Domain-Specific Text Generation for Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S1" title="2.1 Context ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S2" title="2.2 Introduction ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S3" title="2.3 Related Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4.SS1" title="2.4.1 Use Case 1: Limited bilingual in-domain data available ‣ 2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Use Case 1: Limited bilingual in-domain data available</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4.SS2" title="2.4.2 Use Case 2: Zero bilingual in-domain data available ‣ 2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Use Case 2: Zero bilingual in-domain data available</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5" title="2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS1" title="2.5.1 Datasets ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS2" title="2.5.2 Vocabulary ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.2 </span>Vocabulary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS3" title="2.5.3 NMT Model Architecture ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.3 </span>NMT Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS4" title="2.5.4 Training ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS5" title="2.5.5 Domain-Specific Data Generation with LMs ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.5 </span>Domain-Specific Data Generation with LMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS6" title="2.5.6 Back-Translation ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.6 </span>Back-Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS7" title="2.5.7 Mixed Fine-tuning ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5.7 </span>Mixed Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6" title="2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS1" title="2.6.1 Automatic Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.1 </span>Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS2" title="2.6.2 Human Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.2 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS3" title="2.6.3 Linguistic Analysis ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6.3 </span>Linguistic Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S7" title="2.7 Conclusion and Future Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Translation Word-Level Auto-Completion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S1" title="3.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S2" title="3.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S3" title="3.3 User Survey ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>User Survey</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S4" title="3.4 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S5" title="3.5 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S6" title="3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Other Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S7" title="3.7 Conclusion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Adaptive Machine Translation with Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S1" title="4.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S2" title="4.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S3" title="4.3 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S4" title="4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Adaptive MT with Fuzzy Matches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S5" title="4.5 GPT-3 vs Encoder-Decoder MT Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>GPT-3 vs Encoder-Decoder MT Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6" title="4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Incorporating Encoder-Decoder MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6.SS1" title="4.6.1 Fuzzy matches + new segment MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Fuzzy matches + new segment MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6.SS2" title="4.6.2 Fuzzy matches + all segments MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>Fuzzy matches + all segments MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S7" title="4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Bilingual Terminology Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S8" title="4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Terminology-Constrained MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S9" title="4.9 ChatGPT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.9 </span>ChatGPT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S10" title="4.10 BLOOM and BLOOMZ ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.10 </span>BLOOM and BLOOMZ</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S11" title="4.11 Conclusion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.11 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12" title="4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.12 </span>Prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Fine-tuning Large Language Models for Adaptive Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S1" title="5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2" title="5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Information Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S3" title="5.3 Fine-tuning ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S4" title="5.4 Inference ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S5" title="5.5 Conclusion and Future Work ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Conclusion and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Domain Terminology <span class="ltx_text">Integration</span> into Machine Translation:
<span class="ltx_text">Leveraging</span> Large Language <span class="ltx_text">Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S1" title="6.1 Context ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S2" title="6.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3" title="6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS1" title="6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Synthetic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS2" title="6.3.2 Fine-tuning ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS3" title="6.3.3 Terminology-constrained Automatic Post-Editing ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.3 </span>Terminology-constrained Automatic Post-Editing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4" title="6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4.SS1" title="6.4.1 Term-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.1 </span>Term-level Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S4.SS2" title="6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.2 </span>Sentence-level Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S5" title="6.5 Conclusion and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7" title="Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S1" title="7.1 Conclusions ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2" title="7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS1" title="7.2.1 MT adaptation for multilingual and low-resource settings ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>MT adaptation for multilingual and low-resource settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS2" title="7.2.2 Domain-specific word-level autosuggestions and autocompletion ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Domain-specific word-level autosuggestions and autocompletion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS3" title="7.2.3 Analysis of critical errors and text generation for difficult-to-translate words ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Analysis of critical errors and text generation for difficult-to-translate words</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS4" title="7.2.4 Improving random sampling via simulated annealing ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.4 </span>Improving random sampling via simulated annealing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS5" title="7.2.5 Domain-Specific Efficient Fine-tuning of LLMs ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.5 </span>Domain-Specific Efficient Fine-tuning of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch7.S2.SS6" title="7.2.6 Cross-Lingual Retrieval-Augmented Generation ‣ 7.2 Future Work ‣ Chapter 7 Conclusions and Future Work ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.6 </span>Cross-Lingual Retrieval-Augmented Generation</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="Ch0.Sx4.p2">
<p class="ltx_p" id="Ch0.Sx4.p2.1">Language Modelling Approaches to Adaptive Machine Translation</p>
<p class="ltx_p" id="Ch0.Sx4.p2.2"><span class="ltx_text ltx_font_smallcaps" id="Ch0.Sx4.p2.2.1">Yasmin Moslem</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_chapter" id="Ch1">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 1      Introduction</h2>
<div class="ltx_para" id="Ch1.p1">
<p class="ltx_p" id="Ch1.p1.1">Neural Machine Translation (NMT) is capable of producing high-quality translations in terms of fluency and adequacy. The emergence of the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib7" title="">2015</a>; Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite> has revolutionised the field of Natural Language Processing (NLP) in general and NMT in particular, and paved the way for many subsequent breakthroughs in the field. Nevertheless, NMT still faces some challenges when it comes to translation of out-of-domain texts <cite class="ltx_cite ltx_citemacro_citep">(Koehn and Knowles,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib97" title="">2017</a>)</cite>. <span class="ltx_text" id="Ch1.p1.1.1">Domain</span> adaptation of MT systems using in-domain parallel texts has been an active area of research to handle this situation. Several research works on domain adaptation assume the availability of in-domain data. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations <cite class="ltx_cite ltx_citemacro_citep">(Axelrod et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib6" title="">2011</a>; Haddow and Koehn,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib59" title="">2012</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.p2">
<p class="ltx_p" id="Ch1.p2.1">Recent advances in language modelling techniques in general and large-scale <span class="ltx_text" id="Ch1.p2.1.1">language</span> models (LLMs) in particular have shown significant potential in improving a wide range of NLP tasks. Inspired by this idea, this research aims to answer two major <span class="ltx_text ltx_font_bold" id="Ch1.p2.1.2">Research Questions (RQ)</span>:</p>
</div>
<div class="ltx_para" id="Ch1.p3">
<ul class="ltx_itemize" id="Ch1.S0.I1">
<li class="ltx_item" id="Ch1.S0.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.ix1.1.1.1">RQ1</span></span>
<div class="ltx_para" id="Ch1.S0.I1.ix1.p1">
<p class="ltx_p" id="Ch1.S0.I1.ix1.p1.1">In scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? In the subsequent sections, I will be referring to this question as “<span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.ix1.p1.1.1">Adaptive and <span class="ltx_text" id="Ch1.S0.I1.ix1.p1.1.1.1">Interactive</span> MT</span>”.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S0.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.ix2.1.1.1">RQ2</span></span>
<div class="ltx_para" id="Ch1.S0.I1.ix2.p1">
<p class="ltx_p" id="Ch1.S0.I1.ix2.p1.1">In the absence of sufficient in-domain data, can we use pre-trained LLMs to improve the process of NMT domain adaptation? In the following sections, I will be referring to this question as “<span class="ltx_text ltx_font_bold" id="Ch1.S0.I1.ix2.p1.1.1">Domain-specific Text Generation for MT</span>”.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Ch1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="Ch1.F1.g1" src="extracted/5369614/img/research-chart.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1.1: </span>Translation environment, involving adaptive MT and human interaction</figcaption>
</figure>
<section class="ltx_section" id="Ch1.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">1.1     Background and Motivation</h3>
<div class="ltx_para" id="Ch1.S1.p1">
<p class="ltx_p" id="Ch1.S1.p1.1">Initially, as translators transitioned to using computers and text editors, they manually maintained previous translations and terminology in regular electronic documents. The introduction of computer-aided translation tools has significantly transformed traditional approaches, changing the landscape of translation workflows.</p>
</div>
<div class="ltx_para" id="Ch1.S1.p2">
<p class="ltx_p" id="Ch1.S1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.F1" title="Figure 1.1 ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.1</span></a> illustrates a typical workflow in translation environments nowadays, where the process starts with an MT suggestion that a translator revises to achieve the required quality. Approved translations are then saved into a translation memory (TM). After some time, bilingual text segments stored in the TM are used to fine-tune the MT model to adapt to the required domain and style. The fine-tuned MT model is then used to generate translation suggestions. Ideally, the model should be able to make use of similar translation pairs retrieved from the TM at inference time. It should also be able to interact with the translator by providing them with suggestions and adapting to their edits during translation. However, such real-time adaptivity and interactivity features are not currently implemented in the majority of MT systems that translators have been using, which highlights the challenges of transitioning research in this area into real-world production workflows.</p>
</div>
<div class="ltx_para" id="Ch1.S1.p3">
<p class="ltx_p" id="Ch1.S1.p3.1">Throughout my career in the translation technology industry, I have heard from several translators about situations where they edit a term or expression, yet subsequent MT-ed segments continue to repeat the same mistakes. Despite the wide research conducted on real-time adaptive MT and related fields such as context-aware MT and document-level MT, practical application of these techniques in real-world tools remains limited. Some of these approaches were proposed with architectures before the Transformer model<span class="ltx_note ltx_role_footnote" id="Ch1.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Given that LSTM models generally have been superseded by Transformer models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite>, performance gains achieved via methods originally proposed with LSTM might not be noticeable. This can also be due to the complexity of the Transformer architecture or using variations of tokenisation and data preparation methods <cite class="ltx_cite ltx_citemacro_citep">(Popović et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib150" title="">2023</a>)</cite>.</span></span></span> or verified only through medium-sized experiments, which sometimes means that such methods cannot achieve the same performance gains when implemented on a large scale. Others suffer from lack of efficiency, which makes deploying them in production challenging for several language service providers <cite class="ltx_cite ltx_citemacro_citep">(Meng et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib120" title="">2022</a>; Martins et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib119" title="">2023</a>; Treviso et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib190" title="">2023</a>)</cite>. In addition, even the details of the limited state-of-the-art commercial applications of adaptive MT are not necessarily shared in publicly accessible publications.</p>
</div>
<div class="ltx_para" id="Ch1.S1.p4">
<p class="ltx_p" id="Ch1.S1.p4.1">The emergence of the in-context learning capability of LLMs has opened new avenues for diverse NLP tasks, including MT. In-context learning refers to the ability of a model to refine its output based on the context provided within the input itself, without the need for fine-tuning on specific scenarios <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>. This capability is especially useful in retrieval-augmented generation, a technique that integrates external knowledge sources to enhance the responses of the model. For adaptive MT, this means that LLMs can dynamically adapt translations to various sources of context, such as fuzzy matches or terminology, leading to more accurate translations.</p>
</div>
</section>
<section class="ltx_section" id="Ch1.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">1.2     Research Questions</h3>
<div class="ltx_para" id="Ch1.S2.p1">
<p class="ltx_p" id="Ch1.S2.p1.1">This section provides an overview of the aforementioned research questions, while the following chapters elaborate on experimental setups and results. It is worth noting that the two questions overlap in several aspects, which make it possible to use any of the employed approaches to address both of them.</p>
</div>
<section class="ltx_subsection" id="Ch1.S2.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.2.1     <span class="ltx_text ltx_font_smallcaps" id="Ch1.S2.SS1.1.1">RQ1. Adaptive and Interactive MT</span>
</h4>
<div class="ltx_para" id="Ch1.S2.SS1.p1">
<p class="ltx_p" id="Ch1.S2.SS1.p1.1">Adaptive MT utilises user feedback to improve translation quality over time, particularly in domain-specific scenarios where baseline MT systems may lack relevant data. Incorporating user feedback into the translation process, especially at inference time, poses challenges.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.p2">
<p class="ltx_p" id="Ch1.S2.SS1.p2.1">In my research, I investigated two forms of utilising language modelling to improve MT real-time adaptivity and interactivity. This can be outlined into the following sub-questions:</p>
<ul class="ltx_itemize" id="Ch1.S2.I1">
<li class="ltx_item" id="Ch1.S2.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ1.a</span>
<div class="ltx_para" id="Ch1.S2.I1.ix1.p1">
<p class="ltx_p" id="Ch1.S2.I1.ix1.p1.1">Can we utilise the autoregressive property of NMT models, i.e. their ability to decode the target sentence word by word according to the translation history, to generate relevant autosuggestions? So instead of providing the most probable word, the system can interact with typed sequences at inference time to generate more accurate translations. In my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib127" title="">Moslem et al., 2022b, </a>)</cite>, I employed random sampling techniques to generate diverse alternatives, which led to improving the ability of an MT system in the scenario where the user types a few characters, and the system is expected to predict and auto-complete the correct word, given the current context (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS1" title="1.2.1.1 RQ1.a Translation autosuggestions and autocompletion ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2.1.1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S2.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ1.b</span>
<div class="ltx_para" id="Ch1.S2.I1.ix2.p1">
<p class="ltx_p" id="Ch1.S2.I1.ix2.p1.1">Can we improve adaptive MT by employing the in-context learning capability of LLMs? This involves learning from similar translations (fuzzy matches) found in approved TMs. In my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, I explored leveraging LLMs to boost adaptive MT at inference time. I concluded that LLMs can be leveraged for adapting new translations to match the terminology and style of pre-approved fuzzy matches, post-editing translations from encoder-decoder MT systems, and performing terminology-constrained MT (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS2" title="1.2.1.2 RQ1.b Adaptive MT with LLMs ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2.1.2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S2.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ1.c</span>
<div class="ltx_para" id="Ch1.S2.I1.ix3.p1">
<p class="ltx_p" id="Ch1.S2.I1.ix3.p1.1">Can we reinforce MT adherence to terminology through prompting LLMs to use pre-approved terms in translations? In my work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, I proposed adding relevant term pairs during translation with an LLM to enhance real-time terminology-constrained MT. Similarly, in my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, I used LLMs for terminology-constrained automatic post-editing, where an LLM is instructed to incorporate missing terms in translations originally generated by an MT model (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS1.SSS3" title="1.2.1.3 RQ1.c Terminology-constrained MT with LLMs ‣ 1.2.1 RQ1. Adaptive and Interactive MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2.1.3</span></a>).</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsubsection" id="Ch1.S2.SS1.SSS1">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.1.1 </span>RQ1.a Translation autosuggestions and autocompletion</h5>
<div class="ltx_para" id="Ch1.S2.SS1.SSS1.p1">
<p class="ltx_p" id="Ch1.S2.SS1.SSS1.p1.1">Looking at the NMT decoder as an autoregressive language model, that can predict the next word depending on previously generated words, several researchers studied the capabilities that this perspective can bring to the quality of NMT in general, and domain adaptation in particular. Research in this direction involves: a) iterative prediction–correction approaches, b) information retrieval from training datasets, and c) employing external language models at inference time.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS1.p2">
<p class="ltx_p" id="Ch1.S2.SS1.SSS1.p2.1">In a user survey I conducted <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib127" title="">Moslem et al., 2022b, </a>)</cite>, participants indicated that suggesting translation alternatives can be a source of inspiration. Moreover, it can be easier or faster than typing, and it can limit their need to refer to external resources. When a high-quality baseline MT model is employed, MT auto-completion can yield higher quality translation <cite class="ltx_cite ltx_citemacro_citep">(Green et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib54" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS1.p3">
<p class="ltx_p" id="Ch1.S2.SS1.SSS1.p3.1">The WMT’s Word-Level AutoCompletion (WLAC) shared task<span class="ltx_note ltx_role_footnote" id="Ch1.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://statmt.org/wmt22/word-autocompletion.html" title="">https://statmt.org/wmt22/word-autocompletion.html</a></span></span></span> addresses a more specific scenario, where the user types a few characters, and the NMT system predicts and auto-completes the correct word, given the current context. In 2022, I made submissions for Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German language directions. I employed random sampling to generate diverse alternatives, and achieved excellent results (1st and 2nd places in the shared task) based on both automatic and human evaluation. Random sampling is a decoding mode that randomly samples tokens from the model output distribution. To obtain diverse generations from the MT model, I rely on randomness in the decoding method, in particular through <span class="ltx_text" id="Ch1.S2.SS1.SSS1.p3.1.1">top-K</span> sampling that samples the next word from the top-K most probable choices <cite class="ltx_cite ltx_citemacro_citep">(Fan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib46" title="">2018</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib74" title="">2018</a>; Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>)</cite>, instead of aiming to decode text that maximises likelihood. More details on this topic can be found in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S2.SS1.SSS2">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.1.2 </span>RQ1.b Adaptive MT with LLMs</h5>
<div class="ltx_para" id="Ch1.S2.SS1.SSS2.p1">
<p class="ltx_p" id="Ch1.S2.SS1.SSS2.p1.1">Real-time translation that can adapt to changes in the context and terminology remains a challenging task. Autoregressive decoder-only LLMs such as BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, <span class="ltx_text" id="Ch1.S2.SS1.SSS2.p1.1.1">Falcon</span> <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>)</cite>, <span class="ltx_text" id="Ch1.S2.SS1.SSS2.p1.1.2">GPT-3</span> <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, and <span class="ltx_text" id="Ch1.S2.SS1.SSS2.p1.1.3">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib138" title="">2023</a>)</cite> are trained to predict the next word given the previous context. In-context learning allows these models to adapt their output to adhere to the terminology and style used in previously approved translation pairs without further training.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS2.p2">
<p class="ltx_p" id="Ch1.S2.SS1.SSS2.p2.1">In my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, I explored employing the in-context learning feature of LLMs to enhance real-time adaptive MT. The findings illustrated in the paper show that LLMs can effectively adapt to specific domains and terminologies at translation time, outperforming strong encoder-decoder MT systems, especially for high-resource languages. In this paper, I employed an embedding-based similarity approach for retrieving similar translation pairs (fuzzy matches) from a TM instead of providing random examples. The performance further improves as more fuzzy matches are added to the context.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS2.p3">
<p class="ltx_p" id="Ch1.S2.SS1.SSS2.p3.1">One significant advantage of in-context learning is its capacity for real-time customisation. This advantage can give MT systems the capability to learn from previous interactions and adapt their output to align with the user’s preferred terminology and style at inference time <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>. Although in-context learning can improve adaptive MT without any special fine-tuning, LLMs can be fine-tuned to further enhance their in-context learning ability at translation time <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>)</cite>. Overall, my research demonstrates the practical implications of using in-context learning of LLMs for real-time adaptive MT, offering opportunities to improve translation quality and efficiency in diverse language pairs and domains at inference time.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S2.SS1.SSS3">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.1.3 </span>RQ1.c Terminology-constrained MT with LLMs</h5>
<div class="ltx_para" id="Ch1.S2.SS1.SSS3.p1">
<p class="ltx_p" id="Ch1.S2.SS1.SSS3.p1.1">Terminology-constrained MT is the scenario where domain-specific terminology can be <span class="ltx_text" id="Ch1.S2.SS1.SSS3.p1.1.1">enforced</span> in a multi-domain NMT model at translation time. The approach should be capable of handling unseen terminology while retaining NMT’s ability to produce fluent output sequences <cite class="ltx_cite ltx_citemacro_citep">(Hokamp and Liu,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib72" title="">2017</a>; Dinu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib39" title="">2019</a>; Exel et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib44" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS3.p2">
<p class="ltx_p" id="Ch1.S2.SS1.SSS3.p2.1">In my work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, I investigated terminology-constrained MT with LLMs. Simply put, I added lists of relevant terms while prompting an LLM for translation. In addition to automatic evaluation, human evaluation for terminology-constrained MT with LLMs was conducted by professional linguists, who evaluated the adherence of the model to required terms and its impact on the overall translation quality. The evaluation compared diverse scenarios, such as zero-shot translation, zero-shot with glossary terms, two-shot translation with fuzzy matches, and two-shot translation with both fuzzy matches and glossary terms, to assess the usage of provided terms in the translations. The evaluators found that for Arabic, French, and Spanish, terminology-constrained MT was more successful at incorporating the provided glossary terms into the translations.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS1.SSS3.p3">
<p class="ltx_p" id="Ch1.S2.SS1.SSS3.p3.1">Similarly, in my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, LLMs were used for terminology-constrained automatic post-editing to insert missing terms into translations generated by an encoder-decoder MT system. In other words, if an MT model does not produce a translation that includes the terms provided by the organisers, the translation is fed into the LLM, prompting it to incorporate these terms while retaining the rest of the translation. While the approach can be used independently with a generic MT model, in this paper, it was used after fine-tuning an MT model on synthetic in-domain data to assess the extra gains that can be achieved from this step. The whole process almost doubled the use of terms across three language pairs, compared to the translations generated by a baseline generic model.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Ch1.S2.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.2.2     <span class="ltx_text ltx_font_smallcaps" id="Ch1.S2.SS2.1.1">RQ2. Domain-specific Text Generation for MT</span>
</h4>
<div class="ltx_para" id="Ch1.S2.SS2.p1">
<p class="ltx_p" id="Ch1.S2.SS2.p1.1">In-domain data scarcity is common in translation settings, which makes it challenging to fine-tune MT models, and produce translations that are consistent with the relevant context <cite class="ltx_cite ltx_citemacro_citep">(Axelrod et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib6" title="">2011</a>; Haddow and Koehn,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib59" title="">2012</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>. Purely synthetic data may be beneficial for domain adaptation when even monolingual in-domain natural data is unavailable, either due to lack of resources or an extremely narrow target domain. Synthetic sentences may be produced by a template or an external model, in conjunction with forward-translation or back-translation <cite class="ltx_cite ltx_citemacro_citep">(Saunders,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib164" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.p2">
<p class="ltx_p" id="Ch1.S2.SS2.p2.1">Recently, there has been a considerable advancement in training LLMs, not only for English, but also for diverse languages. My research investigates the feasibility of domain-specific text generation using LLMs and seeks to answer the following sub-questions:</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.p3">
<ul class="ltx_itemize" id="Ch1.S2.I2">
<li class="ltx_item" id="Ch1.S2.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ2.a</span>
<div class="ltx_para" id="Ch1.S2.I2.ix1.p1">
<p class="ltx_p" id="Ch1.S2.I2.ix1.p1.1">Can we improve the quality of MT domain adaptation by using <span class="ltx_text ltx_font_italic" id="Ch1.S2.I2.ix1.p1.1.1">a small in-domain dataset</span> to generate huge amounts of synthetic in-domain data? This includes two main scenarios: (a) where there is only a small bilingual dataset available, and (b) where only the monolingual source text to be translated is available. Given a pre-trained language model for the target language, I fed existing in-domain target sentences as language model prompts to generate additional examples, then back-translated the new synthetic target sentences to obtain the equivalent source sentences. Finally, I used the resulting synthetic bilingual data for domain adaptation of the baseline NMT system. This topic was covered in my paper, <span class="ltx_text ltx_font_italic" id="Ch1.S2.I2.ix1.p1.1.2">Domain-Specific Text Generation for Machine <span class="ltx_text" id="Ch1.S2.I2.ix1.p1.1.2.1">Translation</span></span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>, which won a Best Presentation Award at AMTA 2022. The work proposes a novel approach to domain adaptation, that makes use of huge amounts of synthetic in-domain data to fine-tune baseline NMT models, and demonstrates significant improvements in translation of in-domain texts in both scenarios (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS1" title="1.2.2.1 RQ2.a Insufficient in-domain data ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2.2.1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S2.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ2.b</span>
<div class="ltx_para" id="Ch1.S2.I2.ix2.p1">
<p class="ltx_p" id="Ch1.S2.I2.ix2.p1.1">Can we improve the quality of MT domain adaptation by using <span class="ltx_text ltx_font_italic" id="Ch1.S2.I2.ix2.p1.1.1">approved terminology</span> to generate huge amounts of synthetic bilingual in-domain data? In my WMT 2023 paper <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, I built on the approach described in the first sub-question (RQ2.a). Instead of generating the target only with an LLM and using back-translation to generate the source, I employed an LLM to generate both the source and target sides of the synthetic data, instructing it to incorporate a term pair in the generated translation pair (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S2.SS2.SSS2" title="1.2.2.2 RQ2.c Terminology-aware text generation ‣ 1.2.2 RQ2. Domain-specific Text Generation for MT ‣ 1.2 Research Questions ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2.2.2</span></a>).</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsubsection" id="Ch1.S2.SS2.SSS1">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.2.1 </span>RQ2.a Insufficient in-domain data</h5>
<div class="ltx_para" id="Ch1.S2.SS2.SSS1.p1">
<p class="ltx_p" id="Ch1.S2.SS2.SSS1.p1.1">In <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a </a></cite>, I investigated utilising large-scale autoregressive (GPT-like) language models <cite class="ltx_cite ltx_citemacro_citep">(Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>; Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, pre-trained to predict the next word in a sequence. When there is a small in-domain dataset, that is insufficient to fine-tune a baseline MT system, I can use each target sentence as a prompt to generate text. Interestingly, the generated text simulates the domain and linguistic characteristics of the authentic in-domain data. Combining the idea of in-domain text generation with back-translation, I was able to generate huge amounts of synthetic bilingual in-domain data. Finally, I fine-tuned the baseline MT model, on a mix of synthetic in-domain data and generic data. More details on the approach can be found in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.SSS1.p2">
<p class="ltx_p" id="Ch1.S2.SS2.SSS1.p2.1">If there is no in-domain dataset at all, we can first forward-translate the source text to be translated, or a portion of it, using the baseline MT model. Then, the same aforementioned steps are applied <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.SSS1.p3">
<p class="ltx_p" id="Ch1.S2.SS2.SSS1.p3.1">In the case that we have a list of pre-approved terms, we can use each target term or term pair to generate synthetic translation pairs that include the term <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>. Then, we can use the bilingual term-based synthetic data to fine-tune an MT model. The process is elaborated in the next section.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Ch1.S2.SS2.SSS2">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.2.2 </span>RQ2.c Terminology-aware text generation</h5>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.p1">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.p1.1">In <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a </a></cite>, I investigated using terminology for data mining of a target-side large monolingual dataset, to extract sentences similar to those in an in-domain test set. Then, back-translation is employed to generate the source. Finally, mixed fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Chu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite> is applied to the MT baseline to train an in-domain model. This terminology can be either pre-approved and provided by the client or mined with a terminology extraction tool such as KeyBERT<span class="ltx_note ltx_role_footnote" id="Ch1.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/MaartenGr/KeyBERT" title="">https://github.com/MaartenGr/KeyBERT</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Grootendorst,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib55" title="">2020</a>)</cite>, or TM2TB<span class="ltx_note ltx_role_footnote" id="Ch1.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/luismond/tm2tb" title="">https://github.com/luismond/tm2tb</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Mondragón,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib122" title="">2021</a>)</cite>. Even more similar terms can be generated using sense2vec<span class="ltx_note ltx_role_footnote" id="Ch1.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/explosion/sense2vec" title="">https://github.com/explosion/sense2vec</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Trask et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib189" title="">2015</a>)</cite>. Nowadays, LLMs can even be used for bilingual terminology extraction, as I demonstrated in previous work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, which is addressed in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> of this thesis. We can also use already identified terms from tasks such as the WMT biomedical shared task, ClinSpEn<span class="ltx_note ltx_role_footnote" id="Ch1.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.6497350" title="">https://doi.org/10.5281/zenodo.6497350</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Neves et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib134" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.p2">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.p2.1">There are multiple approaches to employing terminology and LLMs to generate more <span class="ltx_text" id="Ch1.S2.SS2.SSS2.p2.1.1">in-domain</span> sentences:</p>
</div>
<section class="ltx_paragraph" id="Ch1.S2.SS2.SSS2.Px1">
<h6 class="ltx_title ltx_title_paragraph">Term-based generation:</h6>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.Px1.p1.1">Each term can be used as a prompt to generate in-domain sentences. Alternatively, as in <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a </a></cite>, we can extract sentences that include the terms, and then generate text using these sentences as prompts <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch1.S2.SS2.SSS2.Px2">
<h6 class="ltx_title ltx_title_paragraph">Term-constrained generation:</h6>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Lin et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib113" title="">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhang et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib216" title="">2020</a>)</cite> investigated using lexical constraints while generating texts using language models. They introduced CommonGen models, which are fine-tuned versions of BLOOM, GPT-2, and T5.
Unlike prefix-constrained generation, CommonGen generates text that includes the term at any position, i.e. not necessarily at the beginning. This can help generate diverse sentence patterns.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch1.S2.SS2.SSS2.Px3">
<h6 class="ltx_title ltx_title_paragraph">Bilingual few-shot in-context learning:</h6>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.Px3.p1.1">During unsupervised pre-training, a language model develops a broad set of pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognise the desired task. In their experiments, <cite class="ltx_cite ltx_citemacro_citet">Brown et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, use the term “in-context learning” to describe the inner loop of this process.
In-context learning is a scenario where a pre-trained language model at inference time learns to replicate certain input-output text generation patterns, without further fine-tuning. They showed that autoregressive large language models such as GPT-3 can perform well in diverse tasks, through zero-shot, one-shot, and few-shot in-context learning without weight updates. Zero-shot generation can be used to create sentences from terms with strong LLMs out of the box. For some other LLMs, one-shot or few-shot in-context learning can achieve better results than zero-shot generation. The approach can be applied to a number of generation cases, including a) term-to-term generation, b) source-to-target-sentence generation, c) term-to-target-sentence generation, and d) term-to-bilingual-sentence generation.</p>
</div>
<div class="ltx_para" id="Ch1.S2.SS2.SSS2.Px3.p2">
<p class="ltx_p" id="Ch1.S2.SS2.SSS2.Px3.p2.1">In the paper describing our submission to the WMT 2023 Terminology Shared Task <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, ChatGPT was used to generate bilingual sentence pairs, based on the terms provided by the organisers. So, given a target term, the model was asked to generate multiple translation pairs, including both the source (e.g. German) and the target (e.g. English). This approach can be particularly useful in distilling bilingual terminology-based knowledge from an LLM, to a specialised encoder-decoder MT model, which can improve both the quality and efficiency, and reduce computing costs at inference time. After fine-tuning an encoder-decoder MT model with this terminology-based bilingual synthetic data, the model adherence to the pre-approved terminology was improved (cf. Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="Ch1.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">1.3     Publications</h3>
<div class="ltx_para" id="Ch1.S3.p1">
<p class="ltx_p" id="Ch1.S3.p1.1">The following publications are directly related to the research questions, and they have been incorporated as chapters within this thesis:</p>
</div>
<div class="ltx_para" id="Ch1.S3.p2">
<ul class="ltx_itemize" id="Ch1.S3.I1">
<li class="ltx_item" id="Ch1.S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i1.p1">
<p class="ltx_p" id="Ch1.S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I1.i1.p1.1.1">Adaptive <span class="ltx_text" id="Ch1.S3.I1.i1.p1.1.1.1">Machine</span> Translation with Large Language Models</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i1.p1.1.2">Yasmin Moslem</span>, Rejwanul Haque, John Kelleher, and Andy Way. 2023. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation (Research: Technical), pages 227–237, Tampere, Finland. Association for Machine Translation in the Americas.<span class="ltx_note ltx_role_footnote" id="Ch1.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>This paper is already well-cited in public literature.</span></span></span>
</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i2.p1">
<p class="ltx_p" id="Ch1.S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I1.i2.p1.1.1">Domain Terminology Integration into Machine Translation: Leveraging Large Language Models</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i2.p1.1.2">Yasmin Moslem</span>, Gianfranco Romani, Mahdi Molaei, Rejwanul Haque, John Kelleher, and Andy Way. 2023. In Proceedings of the Eighth Conference on Machine Translation, Sentosa, Singapore. Association for Computational Linguistics.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i3.p1">
<p class="ltx_p" id="Ch1.S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I1.i3.p1.1.1">Fine-tuning Large Language Models for Adaptive Machine Translation</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i3.p1.1.2">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2023. arXiv preprint arXiv:2312.12740 [cs.CL].</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i4.p1">
<p class="ltx_p" id="Ch1.S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I1.i4.p1.1.1">Domain-Specific Text Generation for Machine Translation</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i4.p1.1.2">Yasmin Moslem</span>, Rejwanul Haque, John Kelleher, and Andy Way. 2022. In Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 14–30, Orlando, USA. Association for Machine Translation in the Americas.<span class="ltx_note ltx_role_footnote" id="Ch1.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This paper won a “Best Presentation Award” at AMTA 2022.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I1.i5.p1">
<p class="ltx_p" id="Ch1.S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I1.i5.p1.1.1">Translation Word-level Auto-Completion: What can we achieve out of the box?</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib127" title="">Moslem et al., 2022b, </a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I1.i5.p1.1.2">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2022. In Proceedings of the Eighth Conference on Machine Translation. Abu Dhabi, UAE. Association for Computational Linguistics.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="Ch1.S3.p3">
<p class="ltx_p" id="Ch1.S3.p3.1">While the following publications still overlap in many aspects with my research topic, they are less relevant compared to the aforementioned publications; hence, they are only cited in this thesis as needed.</p>
</div>
<div class="ltx_para" id="Ch1.S3.p4">
<ul class="ltx_itemize" id="Ch1.S3.I2">
<li class="ltx_item" id="Ch1.S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I2.i1.p1">
<p class="ltx_p" id="Ch1.S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I2.i1.p1.1.1">Preparing an Endangered Language for the Digital Age: The Case of Judeo-Spanish</span> <cite class="ltx_cite ltx_citemacro_citep">(Öktem et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib137" title="">2022</a>)</cite>
<br class="ltx_break"/>Alp Öktem, Rodolfo Zevallos, <span class="ltx_text ltx_font_bold" id="Ch1.S3.I2.i1.p1.1.2">Yasmin Moslem</span>, Güneş Öztürk, Karen Şarhon. 2022. In Proceedings of the Workshop on Resources and Technologies for Indigenous, Endangered and Lesser-resourced Languages in Eurasia (EURALI), LREC 2022, pages 105–110, Marseille, France. European Language Resources Association (ELRA).</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I2.i2.p1">
<p class="ltx_p" id="Ch1.S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I2.i2.p1.1.1">Arabisc: Context-Sensitive Neural Spelling Checker</span> <cite class="ltx_cite ltx_citemacro_citep">(Moslem et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib126" title="">2020</a>)</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="Ch1.S3.I2.i2.p1.1.2">Yasmin Moslem</span>, Rejwanul Haque, and Andy Way. 2020. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 11–19, Suzhou, China. Association for Computational Linguistics.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I2.i3.p1">
<p class="ltx_p" id="Ch1.S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I2.i3.p1.1.1">Terminology-Aware Sentence Mining for NMT Domain Adaptation: ADAPT’s Submission to the Adap-MT 2020 English-to-Hindi AI Translation Shared Task</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a, </a>)</cite>
<br class="ltx_break"/>Rejwanul Haque, <span class="ltx_text ltx_font_bold" id="Ch1.S3.I2.i3.p1.1.2">Yasmin Moslem</span>, and Andy Way. 2020. In Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task, pages 17–23, Patna, India. NLP Association of India (NLPAI).</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S3.I2.i4.p1">
<p class="ltx_p" id="Ch1.S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Ch1.S3.I2.i4.p1.1.1">The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib62" title="">Haque et al., 2020b, </a>)</cite>
<br class="ltx_break"/>Rejwanul Haque, <span class="ltx_text ltx_font_bold" id="Ch1.S3.I2.i4.p1.1.2">Yasmin Moslem</span>, and Andy Way. 2020. In Proceedings of the Fourth Workshop on Neural Generation and Translation: Simultaneous Translation and Paraphrasing for Language Education (STAPLE), pages 144–152, Online. Association for Computational Linguistics.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch1.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">1.4     Research Context</h3>
<div class="ltx_para" id="Ch1.S4.p1">
<p class="ltx_p" id="Ch1.S4.p1.1">This section aims at reviewing the most related research work. It covers the following topics: adaptive and interactive MT, terminology-constrained MT, retrieval-augmented MT, retrieval-augmented LLMs, and in-context learning for LLMs and MT.</p>
</div>
<section class="ltx_subsection" id="Ch1.S4.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.4.1     Adaptive and Interactive MT</h4>
<div class="ltx_para" id="Ch1.S4.SS1.p1">
<p class="ltx_p" id="Ch1.S4.SS1.p1.1">Adaptive MT usually refers to modifying the MT output at inference time to simulate the characteristics of the text that is currently translated. Moreover, in such an adaptive environment, the system is supposed to learn in real time from the edits implemented by the users. In other words, the MT system should adapt to reduce the likelihood that the error will be repeated in subsequent translations <cite class="ltx_cite ltx_citemacro_citep">(Richardson and Rashid,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib161" title="">2007</a>; Farajian et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib47" title="">2017</a>; O’Brien,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib135" title="">2022</a>)</cite>, which improves consistency and boosts productivity. Interactive MT can be considered a type of adaptation, where an MT system can simultaneously adjust its output based on user input, such as typing part of the translation or selecting a word suggestion from a list of the most probable completions. In such an iterative prediction–correction process, every time the user corrects a word, the system reacts offering a new translation hypothesis, expected to be better than the previous one <cite class="ltx_cite ltx_citemacro_citep">(Langlais et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib104" title="">2000</a>; Peris et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib145" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS1.p2">
<p class="ltx_p" id="Ch1.S4.SS1.p2.1">State-of-the-art MT systems are usually Transformer-based encoder-decoder models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite>. Researchers investigated how to adjust the output of these models to adapt to certain domains at translation time. Such real-time adaptation might include fine-tuning a model on the fly on translation pairs similar to the current source segment, or manipulating the decoding step to adjust the output to match the characteristics of the source text and any other requirements such as terminology. There are several approaches that fall under adaptive and interactive MT, and I will try to cover popular methods that are relevant to my work.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS1.p3">
<p class="ltx_p" id="Ch1.S4.SS1.p3.1">To deal with a multi-domain NMT scenario, especially where the domain might not be known in advance, researchers proposed an unsupervised (on-the-fly) adaptation approach. Given a source input, the most similar translation pairs are extracted from the “context” dataset (TM) <cite class="ltx_cite ltx_citemacro_citep">(Li et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib111" title="">2018</a>; Farajian et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib47" title="">2017</a>)</cite>. Then, the baseline MT model is fine-tuned with the retrieved pairs, which is then applied to translate the source. After a linguist edits the MT translation, the approved translations are added to the dataset/TM. It is also recommended to dedicate a “context” dataset to each client or project. Finally, the adapted model is reset to the original parameters. The same process is applied for each source segment.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS1.p4">
<p class="ltx_p" id="Ch1.S4.SS1.p4.1">In contrast to the aforementioned instance-based adaptation approach that utilised retrieved fuzzy matches to fine-tune the NMT model for each source segment independently, some researchers investigated real-time augmentation. In other words, they experimented with augmenting the source input with one or more fuzzy matches at inference time, without the need to repeatedly fine-tune the model for each segment <cite class="ltx_cite ltx_citemacro_citep">(Bulte and Tezcan,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib18" title="">2019</a>; Xu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib205" title="">2020</a>)</cite>. Nevertheless, this approach usually works much better if the model is pre-trained to perform such a task. The main advantage of this approach over the instance-based adaptation approach is that it does not risk overfitting by fine-tuning the model on only a few segments. Moreover, as the model is already pre-trained, it does not necessarily need to be served on GPUs as it does not require further fine-tuning at inference time. Due to the relevance of such retrieval-augmented MT approaches to my work, I dedicate Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4.SS3" title="1.4.3 Retrieval-Augmented MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.4.3</span></a> to research on this area.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS1.p5">
<p class="ltx_p" id="Ch1.S4.SS1.p5.1">The concept of “online learning” in production environments has been the focus of many researchers, not only in NMT, but also in several fields of machine learning. However, according to <cite class="ltx_cite ltx_citemacro_citet">Etchegoyhen et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib43" title="">2021</a>)</cite>, this type of adaptation typically requires higher learning rates, which can affect the quality of the models over time. Alternatively, less aggressive online learning setups may preserve model stability, at the cost of reduced adaptation to user-generated corrections. Hence, in their work, they evaluated different online learning configurations over time, measuring their impact on user-generated samples, as well as separate in-domain and out-of-domain datasets. The results reported in their work indicate that mixed approaches combining online learning with periodical batch fine-tuning might be needed to balance the benefits of online learning with model stability.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS1.p6">
<p class="ltx_p" id="Ch1.S4.SS1.p6.2">When it comes to interactive MT, several approaches adopted the teacher forcing mode <cite class="ltx_cite ltx_citemacro_citep">(Williams and Zipser,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib201" title="">1989</a>)</cite> where ground truth previous tokens are fed into the decoder, instead of the predicted tokens y<sub class="ltx_sub" id="Ch1.S4.SS1.p6.2.1">i-1</sub> as suggested by <cite class="ltx_cite ltx_citemacro_citet">Bahdanau et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib7" title="">2015</a>)</cite>. Then, the engine expands the next <math alttext="N" class="ltx_Math" display="inline" id="Ch1.S4.SS1.p6.1.m1.1"><semantics id="Ch1.S4.SS1.p6.1.m1.1a"><mi id="Ch1.S4.SS1.p6.1.m1.1.1" xref="Ch1.S4.SS1.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS1.p6.1.m1.1b"><ci id="Ch1.S4.SS1.p6.1.m1.1.1.cmml" xref="Ch1.S4.SS1.p6.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS1.p6.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS1.p6.1.m1.1d">italic_N</annotation></semantics></math> most likely words, and continues (auto-completes) the decoding for these <math alttext="N" class="ltx_Math" display="inline" id="Ch1.S4.SS1.p6.2.m2.1"><semantics id="Ch1.S4.SS1.p6.2.m2.1a"><mi id="Ch1.S4.SS1.p6.2.m2.1.1" xref="Ch1.S4.SS1.p6.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS1.p6.2.m2.1b"><ci id="Ch1.S4.SS1.p6.2.m2.1.1.cmml" xref="Ch1.S4.SS1.p6.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS1.p6.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS1.p6.2.m2.1d">italic_N</annotation></semantics></math> hypotheses independently. <cite class="ltx_cite ltx_citemacro_citet">Peris and Casacuberta, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib144" title="">2019</a>)</cite> explored the incremental update of NMT systems during the post-editing or interactive translation processes. Such modifications aim to incorporate the new knowledge from the edited sentences into the translation system. Updates to the model are performed on-the-fly as sentences are corrected via online learning techniques. They implemented an interactive, adaptive system, able to react to single-character interactions, hoping to reduce the human effort required for obtaining high-quality translations. As interactive prediction can be computing intensive, <cite class="ltx_cite ltx_citemacro_citet">Wuebker et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib203" title="">2018</a>)</cite> demonstrated that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality, which significantly improves efficiency. They evaluated this technique for both batch and incremental adaptation across multiple data sets and language pairs.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch1.S4.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.4.2     Terminology-Constrained MT</h4>
<div class="ltx_para" id="Ch1.S4.SS2.p1">
<p class="ltx_p" id="Ch1.S4.SS2.p1.1">Despite the impressive quality improvements yielded by NMT systems, controlling their translation output to adhere to user-provided terminology constraints remains an open challenge <cite class="ltx_cite ltx_citemacro_citep">(Hasler et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib63" title="">2018</a>)</cite>. The end-to-end nature of NMT removes many ways of manually guiding the translation process that were available in older paradigms <cite class="ltx_cite ltx_citemacro_citep">(Post and Vilar,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib152" title="">2018</a>)</cite>, which makes it very sensitive to domain shift <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib78" title="">Hu et al., 2019a, </a>)</cite>. Hence, there have been several research efforts to boost NMT adherence to pre-approved terminology, either through adapting the decoding step only, or through training a terminology-aware NMT model.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS2.p2">
<p class="ltx_p" id="Ch1.S4.SS2.p2.1">Lexically constrained decoding is a modification to beam search that yields decoder outputs honouring user-supplied constraints. These constraints can be provided in the form of either positive constraints, which specify that certain token sequences <em class="ltx_emph ltx_font_italic" id="Ch1.S4.SS2.p2.1.1">must</em> be present in the output, or negative constraints, which specify token sequences that <em class="ltx_emph ltx_font_italic" id="Ch1.S4.SS2.p2.1.2">must not</em> be generated <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib79" title="">Hu et al., 2019b, </a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Hokamp and Liu, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib72" title="">2017</a>)</cite> proposed an approach to lexically constrained decoding using grid beam search, an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of words or phrases that must be present in the output sequence. Their experiments on interactive-predictive translation and domain adaptation of NMT showed that the approach can provide large improvements in translation quality in interactive scenarios. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Hasler et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib63" title="">2018</a>)</cite> introduced an approach to constrained neural decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS2.p3">
<p class="ltx_p" id="Ch1.S4.SS2.p3.1">However, the aforementioned approaches have computational complexities. Hence, <cite class="ltx_cite ltx_citemacro_citet">Post and Vilar, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib152" title="">2018</a>)</cite> presented an algorithm for lexically constrained decoding with less complexity, and higher efficiency. The algorithm was shipped as part of the Sockeye framework.<span class="ltx_note ltx_role_footnote" id="Ch1.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/awslabs/sockeye" title="">https://github.com/awslabs/sockeye</a></span></span></span> Likewise, the Fairseq framework<span class="ltx_note ltx_role_footnote" id="Ch1.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq" title="">https://github.com/facebookresearch/fairseq</a></span></span></span> has implemented this approach along with the algorithm for faster decoding described by <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib78" title="">Hu et al., 2019a </a></cite>, who extended research in lexically constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS2.p4">
<p class="ltx_p" id="Ch1.S4.SS2.p4.1">The aforementioned works have mainly proposed modifications to the decoding algorithm to constrain the output to include target terms at inference time. While effective, these constrained decoding methods add significant computational overhead to the inference step. In contrast, <cite class="ltx_cite ltx_citemacro_citet">Dinu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib39" title="">2019</a>)</cite> approached this challenge by training an NMT system to learn how to use custom terminology when provided with the input, leading to efficiency gains comparable to constraint-free decoding. The authors used inline annotation of the target terms in the source segment plus source factor embeddings during training and inference. Later, <cite class="ltx_cite ltx_citemacro_citet">Exel et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib44" title="">2020</a>)</cite> investigated variations of the approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Dinu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib39" title="">2019</a>)</cite> and compared them to constrained decoding. Similarly, <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib78" title="">Hu et al., 2019a </a></cite> proposed an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, they performed lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS2.p5">
<p class="ltx_p" id="Ch1.S4.SS2.p5.1">In the same context of teaching NMT models to use terminology, placeholders can be incorporated as part of a pre-processing step of the training data, and then training the system with this data that contains these special placeholders <cite class="ltx_cite ltx_citemacro_citep">(Crego et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib35" title="">2016</a>)</cite>. A similar workflow is applied at inference time. Firstly, pre-processing replaces source terms with placeholders. Secondly, post-processing is applied over the NMT output to replace placeholders with target terms. <cite class="ltx_cite ltx_citemacro_citet">Michon et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib121" title="">2020</a>)</cite> extended the approach to cover a wider variety of cases, and to control morphology. They used several placeholders indicating part-of-speech (POS) and morphological information, both in the source and target sides. For each source-target term pair, they encoded all possible inflections of the source and target word, labelled with inflection type. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Sun et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib180" title="">2022</a>)</cite> proposed a prompt-based method that pre-trains an NMT model to adapt to terms augmented to the input at translation time.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch1.S4.SS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.4.3     Retrieval-Augmented MT</h4>
<div class="ltx_para" id="Ch1.S4.SS3.p1">
<p class="ltx_p" id="Ch1.S4.SS3.p1.1">In the realm of NMT, researchers discovered that it is possible to improve the performance of encoder-decoder NMT models by retrieving external knowledge at inference time. This can be achieved through diverse means, ranging from memorising the whole training data to retrieving similar translation pairs from relevant TMs and incorporating such knowledge at inference time. Retrieval can take different forms, ranging from sentence-based to document-based retrieval. On the one hand, utilisation of the retrieved information can be relatively explicit, e.g. by interpolating the distribution of the retrieved target tokens with the output distribution from the pre-trained MT model. On the other hand, the model can be provided with the retrieved segment of text along with the input source text, and hopefully it will try to modify its output translation accordingly.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p2">
<p class="ltx_p" id="Ch1.S4.SS3.p2.1">Leveraging information retrieved from a TM is a simple yet powerful data augmentation approach for boosting MT performance. Researchers concatenated source segments with the targets of the retrieved fuzzy matches, and then trained an MT model on the augmented data <cite class="ltx_cite ltx_citemacro_citep">(Bulte and Tezcan,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib18" title="">2019</a>)</cite>. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Xu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib205" title="">2020</a>)</cite> explored data augmentation methods for training an NMT system to make use of fuzzy matches. In particular, they simply provided the neural model with information from both source and target sides of the fuzzy matches. This augmentation step can be incorporated into both the training and inference stages to boost the NMT model quality as well as adherence to the preferred domain features. <cite class="ltx_cite ltx_citemacro_citet">Pham et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib147" title="">2020</a>)</cite> compared diverse approaches to augmentation of fuzzy matches and retrieval algorithms. While both <cite class="ltx_cite ltx_citemacro_citet">Pham et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib147" title="">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Xu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib205" title="">2020</a>)</cite> tagged related and unrelated words in the retrieved target tokens to avoid copying them to the new translation, <cite class="ltx_cite ltx_citemacro_citet">Pham et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib147" title="">2020</a>)</cite> modified the decoding process to make it more efficient. <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib213" title="">Zhang et al., 2018b </a></cite> proposed a method to improve MT of low-frequency words or phrases by retrieving similar sentence pairs, extracting “translation pieces”, and finally rewarding the outputs that contain the retrieved translation pieces at decoding time.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p3">
<p class="ltx_p" id="Ch1.S4.SS3.p3.1">In contrast to the majority of work that uses a bilingual corpus as TM and employs source-side similarity search for memory retrieval, <cite class="ltx_cite ltx_citemacro_citet">Cai et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib20" title="">2021</a>)</cite> proposed a framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Hence, instead of retrieving bilingual translation pairs based on the similarity with current source and retrieved source, they use cross-lingual retrieval to measure the similarity of the current source with monolingual data in the target language. Moreover, the ability to leverage monolingual data makes this approach effective in low-resource and domain adaptation scenarios. Different from previous works that make use of mutually similar but redundant TMs, <cite class="ltx_cite ltx_citemacro_citet">Cheng et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib26" title="">2022</a>)</cite> proposed contrastively retrieving TMs that are holistically similar to the source sentence while individually contrastive to each other, providing maximal information gains.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p4">
<p class="ltx_p" id="Ch1.S4.SS3.p4.1"><cite class="ltx_cite ltx_citemacro_citet">Dabre et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib36" title="">2017</a>)</cite> explored a simple solution to “multi-source” NMT which relies solely on preprocessing a multilingual corpus without modifying the model architecture or training procedure. They simply concatenated the source sentences to form a single long multi-source input sentence while keeping the target-side sentence as is and trained an NMT system using this preprocessed corpus. Hence, in institutions that maintain their proceedings in multiple languages, they can use, for example, two languages to generate a translation in a third language. The authors evaluated the method in low-resource as well as rich-resource settings and showed its effectiveness, demonstrating how the NMT system leverages multilingual information to improve the translation to a target language. <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib212" title="">Zhang et al., 2018a </a></cite> introduced an additional context encoder on the source side that receives the previous two source sentences as context, encodes them and passes the context encodings to both the encoder and decoder, integrating them using additional multi-head attention mechanisms.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p5">
<p class="ltx_p" id="Ch1.S4.SS3.p5.1">Previous approaches to leveraging TMs to improve translation generated by encoder-decoder NMT models require either a significant update of the model architecture and/or additional training efforts to make the models well-behaved when TMs are taken as additional input. In their work, <cite class="ltx_cite ltx_citemacro_citet">Reheman et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib157" title="">2023</a>)</cite> presented a simple but effective method to introduce TMs into NMT systems in a prompting fashion. Specifically, they treat fuzzy matches as prompts to the NMT model at inference time, without changing the training process. Although this approach can work with strong encoder-decoder NMT models without further adaptation, <cite class="ltx_cite ltx_citemacro_citet">Reinauer et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib160" title="">2023</a>)</cite> found that it is most effective when the model is fine-tuned towards this task by concatenating similar translations to the training data.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p6">
<p class="ltx_p" id="Ch1.S4.SS3.p6.5">In their work, <cite class="ltx_cite ltx_citemacro_citet">Khandelwal et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib90" title="">2021</a>)</cite> introduced <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p6.1.m1.1"><semantics id="Ch1.S4.SS3.p6.1.m1.1a"><mi id="Ch1.S4.SS3.p6.1.m1.1.1" xref="Ch1.S4.SS3.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p6.1.m1.1b"><ci id="Ch1.S4.SS3.p6.1.m1.1.1.cmml" xref="Ch1.S4.SS3.p6.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p6.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p6.1.m1.1d">italic_k</annotation></semantics></math>-nearest-neighbour machine translation (<math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p6.2.m2.1"><semantics id="Ch1.S4.SS3.p6.2.m2.1a"><mi id="Ch1.S4.SS3.p6.2.m2.1.1" xref="Ch1.S4.SS3.p6.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p6.2.m2.1b"><ci id="Ch1.S4.SS3.p6.2.m2.1.1.cmml" xref="Ch1.S4.SS3.p6.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p6.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p6.2.m2.1d">italic_k</annotation></semantics></math>NN-MT), and demonstrated that it can lead to significant performance boosts over standard NMT systems. <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p6.3.m3.1"><semantics id="Ch1.S4.SS3.p6.3.m3.1a"><mi id="Ch1.S4.SS3.p6.3.m3.1.1" xref="Ch1.S4.SS3.p6.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p6.3.m3.1b"><ci id="Ch1.S4.SS3.p6.3.m3.1.1.cmml" xref="Ch1.S4.SS3.p6.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p6.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p6.3.m3.1d">italic_k</annotation></semantics></math>NN-MT predicts tokens with a nearest neighbour classifier over a large datastore of cached examples, using representations from an NMT model for similarity search. In other words, the translation is generated word-by-word, and at each time step, they found the most similar contexts in the datastore, and computed a distribution over the corresponding target tokens. This distribution was then interpolated with the output distribution from the pre-trained MT model. The authors demonstrated that memorising the training data improves MT generalisation, and allows a multilingual model to specialise. As a result, a single translation model can adapt to multiple domains by memorising domain-specific data, without any in-domain training. They interpolated a pre-trained autoregressive language model (an NMT decoder, in this case) with a <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p6.4.m4.1"><semantics id="Ch1.S4.SS3.p6.4.m4.1a"><mi id="Ch1.S4.SS3.p6.4.m4.1.1" xref="Ch1.S4.SS3.p6.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p6.4.m4.1b"><ci id="Ch1.S4.SS3.p6.4.m4.1.1.cmml" xref="Ch1.S4.SS3.p6.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p6.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p6.4.m4.1d">italic_k</annotation></semantics></math>NN model, with no additional training. However, <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p6.5.m5.1"><semantics id="Ch1.S4.SS3.p6.5.m5.1a"><mi id="Ch1.S4.SS3.p6.5.m5.1.1" xref="Ch1.S4.SS3.p6.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p6.5.m5.1b"><ci id="Ch1.S4.SS3.p6.5.m5.1.1.cmml" xref="Ch1.S4.SS3.p6.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p6.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p6.5.m5.1d">italic_k</annotation></semantics></math>NN-MT heavily relies on high-quality in-domain parallel corpora, limiting its capability on unsupervised domain adaptation, where in-domain parallel corpora are scarce or non-existent. Hence, <cite class="ltx_cite ltx_citemacro_citet">Zheng et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib218" title="">2021</a>)</cite> proposed a framework that directly uses in-domain monolingual sentences in the target language to construct an effective datastore for k-nearest-neighbour retrieval. To this end, they first introduced an autoencoder task based on the target language, and then inserted lightweight adapters into the original NMT model to map the token-level representation of this task to the ideal representation of the translation task. Their experiments on multi-domain datasets demonstrated that the proposed approach significantly improves translation accuracy with target-side monolingual data, while achieving comparable performance with back-translation <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib170" title="">2016</a>; Poncelas et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib148" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS3.p7">
<p class="ltx_p" id="Ch1.S4.SS3.p7.6">Another downside of <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.1.m1.1"><semantics id="Ch1.S4.SS3.p7.1.m1.1a"><mi id="Ch1.S4.SS3.p7.1.m1.1.1" xref="Ch1.S4.SS3.p7.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.1.m1.1b"><ci id="Ch1.S4.SS3.p7.1.m1.1.1.cmml" xref="Ch1.S4.SS3.p7.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.1.m1.1d">italic_k</annotation></semantics></math>NN-MT is its inefficient performance, since it uses the entire reference corpus as the datastore for the nearest neighbour search at inference time. This means each step for each beam in the beam search has to search over the entire reference corpus. Therefore, other researchers extended the previous work and proposed approaches to improve efficiency <cite class="ltx_cite ltx_citemacro_citep">(Meng et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib120" title="">2022</a>; Wang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib195" title="">2022</a>)</cite>. In their work, <cite class="ltx_cite ltx_citemacro_citet">Meng et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib120" title="">2022</a>)</cite> proposed Fast <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.2.m2.1"><semantics id="Ch1.S4.SS3.p7.2.m2.1a"><mi id="Ch1.S4.SS3.p7.2.m2.1.1" xref="Ch1.S4.SS3.p7.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.2.m2.1b"><ci id="Ch1.S4.SS3.p7.2.m2.1.1.cmml" xref="Ch1.S4.SS3.p7.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.2.m2.1d">italic_k</annotation></semantics></math>NN-MT to address this issue. Fast <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.3.m3.1"><semantics id="Ch1.S4.SS3.p7.3.m3.1a"><mi id="Ch1.S4.SS3.p7.3.m3.1.1" xref="Ch1.S4.SS3.p7.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.3.m3.1b"><ci id="Ch1.S4.SS3.p7.3.m3.1.1.cmml" xref="Ch1.S4.SS3.p7.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.3.m3.1d">italic_k</annotation></semantics></math>NN-MT constructs a significantly smaller datastore for the nearest neighbour search: for each word in a source sentence, Fast <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.4.m4.1"><semantics id="Ch1.S4.SS3.p7.4.m4.1a"><mi id="Ch1.S4.SS3.p7.4.m4.1.1" xref="Ch1.S4.SS3.p7.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.4.m4.1b"><ci id="Ch1.S4.SS3.p7.4.m4.1.1.cmml" xref="Ch1.S4.SS3.p7.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.4.m4.1d">italic_k</annotation></semantics></math>NN-MT first selects its nearest token-level neighbours, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids searching through the whole datastore for nearest neighbours and hence improves decoding efficiency without loss of performance. The authors suggested that their approach enables the practical use of <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.5.m5.1"><semantics id="Ch1.S4.SS3.p7.5.m5.1a"><mi id="Ch1.S4.SS3.p7.5.m5.1.1" xref="Ch1.S4.SS3.p7.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.5.m5.1b"><ci id="Ch1.S4.SS3.p7.5.m5.1.1.cmml" xref="Ch1.S4.SS3.p7.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.5.m5.1d">italic_k</annotation></semantics></math>NN-MT systems in real-world MT applications. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Wang et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib195" title="">2022</a>)</cite> explored a more efficient <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS3.p7.6.m6.1"><semantics id="Ch1.S4.SS3.p7.6.m6.1a"><mi id="Ch1.S4.SS3.p7.6.m6.1.1" xref="Ch1.S4.SS3.p7.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS3.p7.6.m6.1b"><ci id="Ch1.S4.SS3.p7.6.m6.1.1.cmml" xref="Ch1.S4.SS3.p7.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS3.p7.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS3.p7.6.m6.1d">italic_k</annotation></semantics></math>NN-MT implementation and proposed to use clustering for feature reduction to improve the retrieval efficiency, while retaining translation quality.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch1.S4.SS4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.4.4     Retrieval-Augmented LLMs</h4>
<div class="ltx_para" id="Ch1.S4.SS4.p1">
<p class="ltx_p" id="Ch1.S4.SS4.p1.1">In recent years, several pre-trained LLMs have been made available to the research community, covering a wide range of linguistic tasks. Among the state-of-the-art LLMs are GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite>, GPT-NeoX <cite class="ltx_cite ltx_citemacro_citep">(Black et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib13" title="">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, GLM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib210" title="">2022</a>)</cite>, OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib214" title="">2022</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>)</cite>, Llama <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib186" title="">Touvron et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib187" title="">Touvron et al., 2023b, </a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, and Phi-2 <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib112" title="">Li et al., 2023c, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS4.p2">
<p class="ltx_p" id="Ch1.S4.SS4.p2.1">According to <cite class="ltx_cite ltx_citemacro_citet">Lewis et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib106" title="">2020</a>)</cite>, large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned for downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Hence, the authors explored a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG), models which combine pre-trained parametric and non-parametric memory for language generation. The authors introduced RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. They found that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.
Similarly, augmenting language model pre-training with a knowledge-retriever allows it to capture knowledge in a more modular and interpretable way. In this context, <cite class="ltx_cite ltx_citemacro_citet">Guu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib57" title="">2020</a>)</cite> augmented language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. They demonstrated the effectiveness of retrieval-augmented language model pre-training by fine-tuning on the challenging task of open-domain question answering. Even when there are multiple sources of knowledge, it is possible to use tools such as Toolformer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib166" title="">2023</a>)</cite>, a model trained to decide which sources of knowledge (in this case, APIs) to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. The authors showed that their approach achieves substantially improved zero-shot performance across various downstream tasks, often competitive with much larger models, without sacrificing its core language modelling abilities.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS4.p3">
<p class="ltx_p" id="Ch1.S4.SS4.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Borgeaud et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib15" title="">2021</a>)</cite> demonstrated that autoregressive language models can be enhanced by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. Their Retrieval-Enhanced Transformer (RETRO) obtained comparable performance to the state-of-the-art LLMs despite using 25<math alttext="\times" class="ltx_Math" display="inline" id="Ch1.S4.SS4.p3.1.m1.1"><semantics id="Ch1.S4.SS4.p3.1.m1.1a"><mo id="Ch1.S4.SS4.p3.1.m1.1.1" xref="Ch1.S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS4.p3.1.m1.1b"><times id="Ch1.S4.SS4.p3.1.m1.1.1.cmml" xref="Ch1.S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS4.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS4.p3.1.m1.1d">×</annotation></semantics></math> fewer parameters. After fine-tuning, RETRO performance can translate to downstream knowledge-intensive tasks such as question answering.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS4.p4">
<p class="ltx_p" id="Ch1.S4.SS4.p4.3">When it comes to few-shot in-context learning with retrieval-augmented generation, LLMs have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. <cite class="ltx_cite ltx_citemacro_citet">Izacard et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib83" title="">2022</a>)</cite> showed that retrieval-augmented LLMs excel at knowledge-intensive tasks without the need for as many parameters even in few-shot settings. In their work, they presented Atlas, a carefully designed and pre-trained retrieval-augmented language model able to learn knowledge-intensive tasks with very few training examples. They reported that Atlas outperforms a 540B parameters model by 3% despite having 50x fewer parameters. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Shi et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib171" title="">2022</a>)</cite> further addressed whether retrieval-augmented LLMs achieve similar gains in few-shot and zero-shot end-task accuracy. They believed that the main challenge was to achieve coverage of the verbaliser tokens that define the different end-task class labels. To address this challenge, they introduced <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS4.p4.1.m1.1"><semantics id="Ch1.S4.SS4.p4.1.m1.1a"><mi id="Ch1.S4.SS4.p4.1.m1.1.1" xref="Ch1.S4.SS4.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS4.p4.1.m1.1b"><ci id="Ch1.S4.SS4.p4.1.m1.1.1.cmml" xref="Ch1.S4.SS4.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS4.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS4.p4.1.m1.1d">italic_k</annotation></semantics></math>NN-Prompt, a simple and effective <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS4.p4.2.m2.1"><semantics id="Ch1.S4.SS4.p4.2.m2.1a"><mi id="Ch1.S4.SS4.p4.2.m2.1.1" xref="Ch1.S4.SS4.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS4.p4.2.m2.1b"><ci id="Ch1.S4.SS4.p4.2.m2.1.1.cmml" xref="Ch1.S4.SS4.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS4.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS4.p4.2.m2.1d">italic_k</annotation></semantics></math>NN-LM with automatically expanded fuzzy verbalisers. In other words, they expanded a word like “great” to also include “excellent” and other task-specific synonyms for sentiment classification. In their experiments, <math alttext="k" class="ltx_Math" display="inline" id="Ch1.S4.SS4.p4.3.m3.1"><semantics id="Ch1.S4.SS4.p4.3.m3.1a"><mi id="Ch1.S4.SS4.p4.3.m3.1.1" xref="Ch1.S4.SS4.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS4.p4.3.m3.1b"><ci id="Ch1.S4.SS4.p4.3.m3.1.1.cmml" xref="Ch1.S4.SS4.p4.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS4.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS4.p4.3.m3.1d">italic_k</annotation></semantics></math>NN-Prompt was effective for domain adaptation with no further training.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS4.p5">
<p class="ltx_p" id="Ch1.S4.SS4.p5.1">In spite of the success of retrieval-augmented generation, LLMs can only afford fixed-sized inputs due to the input length limit, preventing them from utilising rich long-context information from past inputs. For instance, GPT-3 increases the input length from 1k tokens supported by GPT-2 to 2k tokens for capturing better long-range dependencies. However, this approach typically incurs computation-intensive training. To address this, <cite class="ltx_cite ltx_citemacro_citet"><a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib198" title="">Wang et al., 2023b </a></cite> proposed a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorise long history. They designed a novel decoupled network architecture, with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Enhanced with memory-augmented adaptation training, LongMem can thus memorise long past context and use long-term memory for language modelling. Typically, LongMem can enlarge the long-form memory to 65k tokens (instead of the original 1k supported by GPT-2) and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. It is worth noting that LongMem is inspired by the previous work “Memorising Transformer” (MemTRM) <cite class="ltx_cite ltx_citemacro_citep">(Wu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib202" title="">2022</a>)</cite>. The main difference is that MemTRM faces a memory staleness challenge during training due to its coupled memory design, which uses a single model for encoding memory and fusing memory for language modelling, while uses a decoupled memory module to address the issue of memory staleness. The results demonstrate that the proposed method is effective in helping language models to memorise and utilise long-form contents.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch1.S4.SS5">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">1.4.5     In-Context Learning for LLMs and MT</h4>
<div class="ltx_para" id="Ch1.S4.SS5.p1">
<p class="ltx_p" id="Ch1.S4.SS5.p1.1">Recent research has shown that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. The idea was highlighted by <cite class="ltx_cite ltx_citemacro_citet">Brown et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite> who trained GPT-3, an autoregressive language model with 175 billion parameters, and tested its performance in the few-shot setting. For all tasks, GPT-3 was applied without any gradient updates or fine-tuning, with few-shot demonstrations specified purely via text interaction with the model. In few-shot settings, the model is given <math alttext="K" class="ltx_Math" display="inline" id="Ch1.S4.SS5.p1.1.m1.1"><semantics id="Ch1.S4.SS5.p1.1.m1.1a"><mi id="Ch1.S4.SS5.p1.1.m1.1.1" xref="Ch1.S4.SS5.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="Ch1.S4.SS5.p1.1.m1.1b"><ci id="Ch1.S4.SS5.p1.1.m1.1.1.cmml" xref="Ch1.S4.SS5.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch1.S4.SS5.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="Ch1.S4.SS5.p1.1.m1.1d">italic_K</annotation></semantics></math> examples of the task at inference time. An example typically has a context and a desired completion (e.g. a source sentence and its translation), and then one final example of context, with the model expected to provide the completion. In their experiments, GPT-3 achieved strong performance on many NLP datasets, including question answering and translation. Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.F2" title="Figure 1.2 ‣ 1.4.5 In-Context Learning for LLMs and MT ‣ 1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.2</span></a> shows examples of popular LLMs and illustrates how research in this area has evolved from scaling LLMs <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite> to building smaller and efficient LLMs with comparable performance <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib112" title="">Li et al., 2023c, </a>; Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib187" title="">Touvron et al., 2023b, </a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="Ch1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="247" id="Ch1.F2.g1" src="extracted/5369614/img/LLMs.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1.2: </span>Large language models moving from scaling to efficiency</figcaption>
</figure>
<div class="ltx_para" id="Ch1.S4.SS5.p2">
<p class="ltx_p" id="Ch1.S4.SS5.p2.1">The introduction of such a new paradigm opened up new horizons for researchers to explore and leverage the in-context learning capabilities of LLMs in diverse NLP tasks. <cite class="ltx_cite ltx_citemacro_citet">Wang et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib197" title="">2021</a>)</cite> demonstrated that a single language model (LM4MT) can achieve comparable performance with strong encoder-decoder NMT models on standard MT benchmarks. Similarly, LLMs that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. <cite class="ltx_cite ltx_citemacro_citet">Lin et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib114" title="">2022</a>)</cite> trained XGLM, a multilingual generative language model (up to 7.5 billion parameters), and presented a comprehensive study of multilingual zero-shot and in-context few-shot learning. They trained the models using a large-scale corpus of 500B tokens that comprises 30 diverse languages, over-sampling the less-resourced languages to render a more balanced language representation. They evaluated the models on multiple multilingual tasks, including translation. They conducted an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Vilar et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib192" title="">2023</a>)</cite> conducted an in-depth study demonstrating the strong translation performance of PaLM among similarly trained LLMs. They investigated various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS5.p3">
<p class="ltx_p" id="Ch1.S4.SS5.p3.1">Researchers, including myself, explored the use of few-shot prompting strategies for translation, showing that the number, quality, and domain of the in-context examples significantly impact translation performance <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; Mu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib130" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib211" title="">Zhang et al., 2023a, </a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Garcia et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib50" title="">2023</a>)</cite> demonstrated that by incorporating examples of high-quality translation pairs at inference time, a Transformer decoder-only model trained solely with self-supervised learning is able to match specialised supervised state-of-the-art models. <cite class="ltx_cite ltx_citemacro_citet">Sarti et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib163" title="">2023</a>)</cite> introduced an in-context learning approach to leverage attribute annotations and similar same-language or cross-lingual examples for better prompting quality. They demonstrated its effectiveness with multilingual LLMs for both formality-controlled and gender-controlled translation. Interestingly, such translation capabilities of LLMs are not only limited to commercial systems, but they are also available in open-source LLMs such as BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Yvon,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib11" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite> and GLM <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib211" title="">Zhang et al., 2023a, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS5.p4">
<p class="ltx_p" id="Ch1.S4.SS5.p4.1">In addition to real-time domain adaptation with fuzzy matches, my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite> was among the first efforts to study in-domain lexical and <span class="ltx_text" id="Ch1.S4.SS5.p4.1.1">terminology-constrained</span> translation with LLMs, and showed how incorporating domain-related information while prompting LLMs enhances in-context learning and improves the translation quality of domain-specific texts. Later, <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib52" title="">2023</a>)</cite> proposed using bilingual dictionaries to provide control hints in the prompts, thereby enabling fine-grained phrase-level prompted control of the LLM. Moreover, they showed the effectiveness of their approach even for low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS5.p5">
<p class="ltx_p" id="Ch1.S4.SS5.p5.1"><cite class="ltx_cite ltx_citemacro_citet">Schioppa et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib167" title="">2023</a>)</cite> demonstrated that pre-training Large Language Models on a mixture of a self-supervised language modelling objective and the supervised MT objective, therefore including multilingual parallel data during pre-training, yields models with better in-context learning abilities. <cite class="ltx_cite ltx_citemacro_citet">Chen et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib25" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib117" title="">2023</a>)</cite> proposed methods to enhance the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences and adding a global instruction representation on the following input and response representations, respectively.</p>
</div>
<div class="ltx_para" id="Ch1.S4.SS5.p6">
<p class="ltx_p" id="Ch1.S4.SS5.p6.1">When it comes to low-resource languages, researchers evaluated the translation performance of LLMs for a wide range of languages and dialects, revealing that these models approach or exceed traditional MT model performance for some high-resource languages, but consistently lag for low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Kadaoui et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib89" title="">2023</a>; Ojo and Ogueji,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib136" title="">2023</a>; Robinson et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib162" title="">2023</a>)</cite>. This matches my research outcomes <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, while we observe that leveraging in-context learning via augmenting the translation prompt with similar translation pairs or even aligned phrases can significantly improve the quality of translation of low-resource languages. However, for very low-resource languages, fine-tuning the LLM might be required.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch1.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">1.5     Contribution</h3>
<div class="ltx_para" id="Ch1.S5.p1">
<p class="ltx_p" id="Ch1.S5.p1.1">With real-world production scenarios in mind, I designed and conducted the experiments in this research, building on the efforts of previous researchers to boost real-time adaptivity of MT systems. Exploring new horizons beyond traditional MT, I have taken a step forward towards leveraging LLMs for scenarios involving human interaction and continuous feedback, where the MT system is expected to concurrently adapt to such interactions.</p>
</div>
<div class="ltx_para" id="Ch1.S5.p2">
<p class="ltx_p" id="Ch1.S5.p2.1">In the light of the research questions and related work, the contribution of this research can be outlined as follows:</p>
</div>
<div class="ltx_para" id="Ch1.S5.p3">
<ul class="ltx_itemize" id="Ch1.S5.I1">
<li class="ltx_item" id="Ch1.S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S5.I1.i1.p1">
<p class="ltx_p" id="Ch1.S5.I1.i1.p1.1">leveraging pretrained LLMs for domain-specific data augmentation, through generating bilingual in-domain synthetic data and using it to fine-tune an MT model, resulting in significant improvements in translation quality and adherence to pre-approved terminology <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>. The detailed experiments are in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2</span></a> and Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S5.I1.i2.p1">
<p class="ltx_p" id="Ch1.S5.I1.i2.p1.1">exploring the gains that can be achieved through incorporating LLMs in real-time adaptive MT workflows, showing that LLMs can adapt to pre-approved in-domain translation pairs and terminology, while being solely used for translation, or while post-editing translations generated by specialised MT systems <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>. The detailed experiments are in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a>, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5</span></a>, and Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="Ch1.S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch1.S5.I1.i3.p1">
<p class="ltx_p" id="Ch1.S5.I1.i3.p1.1">investigating approaches to encouraging MT output to use pre-approved terms, which is usually referred to as terminology-constrained MT <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>, or to complete user-typed sequences (which can be terms or regular words) through either word-level auto-completion or predictive auto-suggestions <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib127" title="">Moslem et al., 2022b, </a>)</cite>. The detailed experiments are in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3</span></a>, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> and Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>.
</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch2">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 2      Domain-Specific Text Generation for Machine Translation</h2>
<div class="ltx_para" id="Ch2.p1">
<p class="ltx_p" id="Ch2.p1.1">Yasmin Moslem, Rejwanul Haque, John Kelleher, and Andy Way</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch2.p2">
<p class="ltx_p" id="Ch2.p2.1">In Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (AMTA 2022), Volume 1: Research Track, pages 14–30, Orlando, USA. Association for Machine Translation in the Americas.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Published at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.amta-research.2/" title="">https://aclanthology.org/2022.amta-research.2/</a></span></span></span>
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Ch2.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.1     Context</h3>
<div class="ltx_para" id="Ch2.S1.p1">
<p class="ltx_p" id="Ch2.S1.p1.1">The emergence of open-source large language models (LLMs) such as GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>)</cite> and BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib38" title="">2019</a>)</cite> opened the door for researchers to explore the capabilities of such models. At the time, the use of these models was limited to certain NLP tasks, and was conditioned by their architectures (e.g. encoder-only, decoder-only, or encoder-decoder models), which work for some tasks better than for others. In 2020, the release of GPT-3, an autoregressive LM with 175 billion parameters, was announced. The associated paper <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite> mentioned an attractive feature that the authors called “in-context learning”. This refers to pattern recognition abilities that an LLM develops during pre-training and then uses at inference time to rapidly adapt to or recognise the desired task. The authors showed how to employ “in-context learning” in a range of tasks, including translation, question answering, natural language inference, and reasoning. For some of these tasks, the results reported in the paper were surprising, since the use of an autoregressive decoder-only model such as GPT-2 or GPT-3 was not the common practice for certain tasks. The authors noted that there was a consistent trend of quality improvement as the model scales, as well as a tendency for translation into English to be stronger than translation from English. When translating French, German, and Romanian into English, the reported scores for few-shot translation with GPT-3 were either on par with or better than the state-of-the-art WMT results for these language pairs. Similarly, the authors demonstrated the advantages of using “in-context learning” in other tasks. However, as GPT-3 was never open-sourced, this ‘magical power’ was kept in a black box. As the scalability ‘space race’ started to emerge, companies and institutions built larger and larger LMs, some of which were propriety such as Gopher <cite class="ltx_cite ltx_citemacro_citep">(Rae et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib155" title="">2021</a>)</cite>, MT-NLG <cite class="ltx_cite ltx_citemacro_citep">(Smith et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib175" title="">2022</a>)</cite>, and PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite>, while others were open-sourced such as GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite>, GPT-NeoX <cite class="ltx_cite ltx_citemacro_citep">(Black et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib13" title="">2022</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib172" title="">2022</a>)</cite>, and OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib214" title="">2022</a>)</cite>. By the time of writing this paper, some of these models were publicly available, such as GPT-2, and GPT-J, while others were released later. I thought that we should try these models to improve MT domain adaptation, especially in scenarios of in-domain data scarcity, and the co-authors (my supervisors) encouraged the idea, which led to this research work. This paper won a Best Presentation Award at AMTA 2022.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.2     Introduction</h3>
<div class="ltx_para" id="Ch2.S2.p1">
<p class="ltx_p" id="Ch2.S2.p1.1">Preservation of domain knowledge from the source to target is crucial in any translation workflow. <span class="ltx_text" id="Ch2.S2.p1.1.1">Domain</span> adaptation of MT systems on in-domain parallel texts has been an active area of research to handle this situation. Among popular contributions to the domain adaptation research, <cite class="ltx_cite ltx_citemacro_citet">Luong and Manning, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib118" title="">2015</a>)</cite> proposed to adapt an already existing NMT system to a new domain, with further training on the in-domain data only. In an effort to avoid overfitting on the in-domain data, <cite class="ltx_cite ltx_citemacro_citet">Chu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite> employed the mixed fine-tuning approach, resuming training the baseline NMT model on a mix of in-domain and out-of-domain data. Other researchers suggested adding domain tags to either the source or target sentences of the in-domain data, to inform the NMT model about the domain during training and decoding <cite class="ltx_cite ltx_citemacro_citep">(Britz et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib16" title="">2017</a>; Kobus et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib95" title="">2017</a>; Stergiadis et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib179" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.p2">
<p class="ltx_p" id="Ch2.S2.p2.1">In this sense, several research works on domain adaptation assume the availability of in-domain data. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. To tackle this problem, researchers have proposed diverse approaches, such as utilising large monolingual datasets through selecting instances related to a given test set, then automatically translating this source-synthetic corpus, and finally fine-tuning the general NMT system on this data <cite class="ltx_cite ltx_citemacro_citep">(Chinea-Ríos et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib28" title="">2017</a>)</cite>. Similarly, some works have investigated retrieving similar translations (fuzzy matches) from bilingual datasets, and then applying on-the-fly domain adaptation through fine-tuning the baseline model at translation time <span class="ltx_text" id="Ch2.S2.p2.1.1"><cite class="ltx_cite ltx_citemacro_citep">(Farajian et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib47" title="">2017</a>)</cite></span>, or integrating them into NMT training <cite class="ltx_cite ltx_citemacro_citep">(Bulte and Tezcan,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib18" title="">2019</a>; Xu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib205" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S2.p3">
<p class="ltx_p" id="Ch2.S2.p3.1">While the aforementioned approaches prove to be helpful in certain scenarios of domain adaptation, we believe there is a need for further research in this area to address current challenges of in-domain data scarcity and synthetic data creation. Some approaches, such as on-the-fly domain adaptation, require using GPUs synchronously at translation time, which presents a challenge for some institutions due to the lack of resources. When it comes to mining monolingual or bilingual datasets for similar instances, in several domains a good similar sentence can be a mix of portions of multiple sentences. Besides, with the lack of in-house specialised translation memories, mining publicly available datasets can be an inefficient process.</p>
</div>
<div class="ltx_para" id="Ch2.S2.p4">
<p class="ltx_p" id="Ch2.S2.p4.1">In this work, we introduce a new approach to MT domain adaptation, leveraging state-of-the-art pre-trained language models (LMs) for domain-specific data augmentation. Our method can generate an unlimited number of in-domain sentences out of the box.
Recently, there has been a considerable advancement in training large LMs <cite class="ltx_cite ltx_citemacro_citep">(Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>; Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Black et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib13" title="">2022</a>; Zhang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib214" title="">2022</a>)</cite>, not only for English, but also for diverse languages <cite class="ltx_cite ltx_citemacro_citep">(Antoun et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib5" title="">2021</a>; Zhang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib217" title="">2021</a>; Müller and Laurent,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib132" title="">2022</a>)</cite>. More specifically, our current work exploits GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite> and mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib172" title="">2022</a>)</cite> to generate texts from in-domain sentences. We investigate the <span class="ltx_text" id="Ch2.S2.p4.1.1">feasibility</span> of this domain-specific text generation technique when either no or limited bilingual in-domain dataset is available. Incorporating this approach in a process of bilingual in-domain synthetic data creation and then fine-tuning our baseline generic MT model on the new dataset (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a>), we report significant improvements in the translation quality of the in-domain test set (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6" title="2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.6</span></a>).</p>
</div>
<div class="ltx_para" id="Ch2.S2.p5">
<p class="ltx_p" id="Ch2.S2.p5.1">The rest of the paper is organised as follows. In Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S3" title="2.3 Related Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we discuss the related work in detail. Then, we present our methods in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5" title="2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.5</span></a>, we describe the experimental setup and present the results of our experiments in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6" title="2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.6</span></a>. Finally, we conclude the paper and discuss future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S7" title="2.7 Conclusion and Future Work ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.7</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.3     Related Work</h3>
<div class="ltx_para" id="Ch2.S3.p1">
<p class="ltx_p" id="Ch2.S3.p1.1">In recent years, several pre-trained large LMs have been made available to the research community, covering a wide range of linguistic tasks. Among the state-of-the-art LMs are GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib38" title="">2019</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib116" title="">2019</a>)</cite>, XLNet <cite class="ltx_cite ltx_citemacro_citep">(Yang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib209" title="">2019</a>)</cite>, GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, ELECTRA <cite class="ltx_cite ltx_citemacro_citep">(Clark et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib32" title="">2020</a>)</cite>, DeBERTa <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib66" title="">He et al., 2021b, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib65" title="">He et al., 2021a, </a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib156" title="">2020</a>)</cite>, Gopher <cite class="ltx_cite ltx_citemacro_citep">(Rae et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib155" title="">2021</a>)</cite>, GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite>, GPT-NeoX <cite class="ltx_cite ltx_citemacro_citep">(Black et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib13" title="">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite>, Chinchilla <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib71" title="">2022</a>)</cite>, ELMFOREST <cite class="ltx_cite ltx_citemacro_citep">(Li et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib110" title="">2022</a>)</cite>, MT-NLG <cite class="ltx_cite ltx_citemacro_citep">(Smith et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib175" title="">2022</a>)</cite>, and OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib214" title="">2022</a>)</cite>. Some of these models are multilingual, such as BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, AlexaTM <cite class="ltx_cite ltx_citemacro_citep">(FitzGerald et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib48" title="">2022</a>)</cite> and mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib172" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p2">
<p class="ltx_p" id="Ch2.S3.p2.1">Using LMs for specialised domains has been explored by previous works for diverse tasks. Researchers explored the possibility to retrieve factual knowledge from LMs in various domains <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib146" title="">2019</a>; Sung et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib181" title="">2021</a>)</cite>. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Horawalavithana et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib75" title="">2022</a>)</cite> developed large-scale models of foundational scientific knowledge that can be effectively used to perform a wide range of in-domain and out-of-domain tasks.
</p>
</div>
<div class="ltx_para" id="Ch2.S3.p3">
<p class="ltx_p" id="Ch2.S3.p3.1">LMs have been used in Unsupervised NMT <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib103" title="">2019</a>; Chronopoulou et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib30" title="">2021</a>; Wang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib197" title="">2021</a>)</cite>. Large-scale pre-trained LMs have also been employed in a variety of MT tasks, to improve the robustness of MT models or their ability to work on domain texts <cite class="ltx_cite ltx_citemacro_citep">(Bawden et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib10" title="">2020</a>; Specia et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib178" title="">2020</a>; Wenzek et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib200" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p4">
<p class="ltx_p" id="Ch2.S3.p4.1">Recently, <cite class="ltx_cite ltx_citemacro_citet">Chang et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib23" title="">2021</a>)</cite> aimed at addressing the lack of training data for new application domains for data-to-text generation. They automatically augmented the data available for training by (a) generating new text samples by replacing specific values with alternative ones from the same category, (b) generating new text samples using <span class="ltx_text" id="Ch2.S3.p4.1.1">GPT-2</span>, and (c) proposing an automatic method for pairing the new text samples with data samples. Their approach boosted the performance of a standard seq2seq model by over 5 BLEU points. <cite class="ltx_cite ltx_citemacro_citet">Sawai et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib165" title="">2021</a>)</cite> investigated the use of <span class="ltx_text" id="Ch2.S3.p4.1.2">GPT-2</span> for source-side data augmentation to improve the robustness of a generic pre-trained NMT model. They first fine-tuned the pre-trained model, BERT-fused <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib219" title="">2020</a>)</cite>, on authentic bilingual data. Then, they augmented the English source with data generated by <span class="ltx_text" id="Ch2.S3.p4.1.3">GPT-2</span>. Thereafter, they forward-translated the source-side English monolingual data with the fine-tuned version of BERT-fused. Finally, they fine-tuned the model on a combination of the authentic and synthetic data. While the reported results showed reasonable improvement (approx. 2.0 BLEU points) for the English-to-Japanese language direction, insignificant improvement (avg. 0.3 BLEU) was achieved for both English-to-German and English-to-Chinese language directions. The authors concluded that the result could be due to the relatively small amount of the original English-to-Japanese data compared to the other two language directions. We conjecture that more factors might have led to this result, including using forward-translation (rather than back-translation) of a huge amount of data, due to the noise it introduces for the decoder <cite class="ltx_cite ltx_citemacro_citep">(Haddow et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib58" title="">2022</a>)</cite>. In our current work, we try to be more specific about the task description, focussing on domain adaptation in the absence of enough in-domain data; utilising back-translation as an effective data augmentation technique <cite class="ltx_cite ltx_citemacro_citep">(Edunov et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib41" title="">2018</a>; Caswell et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib22" title="">2019</a>)</cite>; and giving more attention to data distribution through applying approaches like mixed fine-tuning and oversampling <cite class="ltx_cite ltx_citemacro_citep">(Chu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p5">
<p class="ltx_p" id="Ch2.S3.p5.1">Back-translation <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib170" title="">2016</a>; Fadaee and Monz,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib45" title="">2018</a>; Poncelas et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib148" title="">2019</a>)</cite> <span class="ltx_text" id="Ch2.S3.p5.1.1">corresponds</span> to the scenario where target-side monolingual data is translated using an MT system to give corresponding synthetic source sentences, the idea being that it is particularly beneficial for the MT decoder to see well-formed sentences <cite class="ltx_cite ltx_citemacro_citep">(Haddow et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib58" title="">2022</a>)</cite>. Back-translation has become a popular strategy among MT researchers, especially in low-resource scenarios <cite class="ltx_cite ltx_citemacro_citep">(Haque et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib60" title="">2021</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Burlot and Yvon, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib19" title="">2018</a>)</cite> performed a systematic study, which showed that forward-translation might lead to some improvements in translation quality, but not nearly as much as back-translation. <cite class="ltx_cite ltx_citemacro_citet">Bogoychev and Sennrich, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib14" title="">2019</a>)</cite> concluded that forward-translation is more sensitive to the quality of the system used to produce synthetic data. Compared to back-translation, biases and errors in synthetic data are intuitively more problematic in forward-translation, since they directly affect the gold labels. The authors also reported that human evaluators favoured their back-translation systems over forward-translation systems, mostly in terms of fluency, while adequacy was largely the same across all of them, especially on the original translation direction. In their analysis, <cite class="ltx_cite ltx_citemacro_citet">Edunov et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib41" title="">2018</a>)</cite> showed that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. <cite class="ltx_cite ltx_citemacro_citet">Caswell et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib22" title="">2019</a>)</cite> proposed a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. <cite class="ltx_cite ltx_citemacro_citet">Hoang et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib70" title="">2018</a>)</cite> empirically showed that the quality of the back-translation system matters for synthetic corpus creation, and that NMT performance can be improved by iterative back-translation in both high-resource and low-resource scenarios.</p>
</div>
<div class="ltx_para" id="Ch2.S3.p6">
<p class="ltx_p" id="Ch2.S3.p6.1">When it comes to fine-tuning strategies for MT domain adaptation, researchers demonstrated that applying the right data distribution can significantly mitigate catastrophic forgetting of strong baselines in domain adaptation settings. <cite class="ltx_cite ltx_citemacro_citet">Chu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite> proposed the mixed fine-tuning method, whose training procedure is as follows: (a) train an NMT model on out-of-domain data until convergence, and (b) resume training the NMT model from the first step on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) until convergence. According to the authors, mixed fine-tuning can address the overfitting problem of regular fine-tuning. In addition, mixed fine-tuning does not worsen the quality of out-of-domain translations, while regular fine-tuning does. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Hasler et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib64" title="">2021</a>)</cite> studied the problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original MT system. They found that they could improve over the performance trade-off offered by Elastic Weight Consolidation <cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib91" title="">2017</a>)</cite> with a relatively simple data mixing strategy.</p>
</div>
</section>
<section class="ltx_section" id="Ch2.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.4     Methods</h3>
<div class="ltx_para" id="Ch2.S4.p1">
<p class="ltx_p" id="Ch2.S4.p1.1">In this work, we investigate two scenarios of in-domain data scarcity, and propose approaches to leverage pre-trained LMs for domain-specific data generation for MT <span class="ltx_text" id="Ch2.S4.p1.1.1">training</span>.</p>
</div>
<section class="ltx_subsection" id="Ch2.S4.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.4.1     Use Case 1: Limited bilingual in-domain data available</h4>
<div class="ltx_para" id="Ch2.S4.SS1.p1">
<p class="ltx_p" id="Ch2.S4.SS1.p1.1">This is a common scenario where a specialised translation project is received, and although there is a large bilingual generic dataset and a small bilingual in-domain dataset (e.g. translation memory), the in-domain data is insufficient for fine-tuning a baseline model. From now on, we will refer to this use case as “Setup 1”. To handle this situation, we propose the following steps:</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS1.p2">
<ol class="ltx_enumerate" id="Ch2.S4.I1">
<li class="ltx_item" id="Ch2.S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Ch2.S4.I1.i1.p1">
<p class="ltx_p" id="Ch2.S4.I1.i1.p1.1">We employ text generation with a large LM in the target language to augment the in-domain data. In this process, each target sentence in the in-domain dataset is used as a prompt to generate synthetic segments using the pre-trained language model. As expected, the generated text preserves the domain characteristics of the authentic in-domain data. This step enables us to have sufficient data in the target language.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S4.I1.i2" style="list-style-type:none;padding-top:-1.0pt;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ch2.S4.I1.i2.p1">
<p class="ltx_p" id="Ch2.S4.I1.i2.p1.1">To obtain parallel source sentences, we back-translate the target-side synthetic sentences that were generated in the previous step.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S4.I1.i3" style="list-style-type:none;padding-top:-1.0pt;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ch2.S4.I1.i3.p1">
<p class="ltx_p" id="Ch2.S4.I1.i3.p1.1">We apply mixed fine-tuning proposed by <cite class="ltx_cite ltx_citemacro_citet">Chu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite> to the baseline model. In other words, we continue training our baseline model on a mix of (a) the synthetic bilingual in-domain dataset we got from the two previous steps, and (b) a randomly sampled portion of the original generic dataset, with a data size ratio of 1:9, respectively. To apply oversampling, we employ the dataset weights feature in OpenNMT-tf<span class="ltx_note ltx_role_footnote" id="Ch2.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenNMT/OpenNMT-tf" title="">https://github.com/OpenNMT/OpenNMT-tf</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib92" title="">Klein et al., 2020a, </a>)</cite>, with weights 0.9 and 0.1, respectively. Hence, the dataset weights are inversely proportional to the sizes of the two datasets.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This configuration creates a weighted dataset where examples are randomly sampled from the data files according to the provided weights. In simple words, it sequentially samples 9 examples from the smaller in-domain dataset, and 1 example from the larger generic dataset, and so on.</span></span></span> As the in-domain corpus is smaller than the generic corpus, oversampling allows the model to pay equal attention to both corpora. As a result of the mixed fine-tuning process, we obtained a new model that translates in-domain data significantly better than the baseline (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6" title="2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.6</span></a>).<span class="ltx_note ltx_role_footnote" id="Ch2.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Inspired by <cite class="ltx_cite ltx_citemacro_citet">Hasler et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib64" title="">2021</a>)</cite> who applied 20x oversampling, we experimented with a higher oversampling ratio. Increasing both the data size and weight degraded performance on the in-domain test set, compared to our applied 9x ratio, while increasing the weight only did not result in a significant improvement. We might investigate the effect of changing the oversampling ratio further in the future.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="Ch2.S4.I1.i4" style="list-style-type:none;padding-top:-1.0pt;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ch2.S4.I1.i4.p1">
<p class="ltx_p" id="Ch2.S4.I1.i4.p1.1">Although the new fine-tuned model can still adequately translate generic data, we noticed it can degrade performance by 1-2 BLEU points. Therefore, we experimented with <span class="ltx_text" id="Ch2.S4.I1.i4.p1.1.1">checkpoint</span> averaging <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite> of the fine-tuned model with the baseline model to reduce <span class="ltx_text" id="Ch2.S4.I1.i4.p1.1.2">variability</span> between trainings and address rapid overfitting during fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Tran et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib188" title="">2021</a>)</cite>. This step helps regain the higher evaluation score of the baseline model on generic data, while retaining the improved score of the fine-tuned model on in-domain data.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S4.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.4.2     Use Case 2: Zero bilingual in-domain data available</h4>
<div class="ltx_para" id="Ch2.S4.SS2.p1">
<p class="ltx_p" id="Ch2.S4.SS2.p1.1">In this case, we assume that there is no bilingual in-domain data at all. There is only the source text that requires translation. From now on, we will refer to this use case as “Setup 2”.</p>
</div>
<div class="ltx_para" id="Ch2.S4.SS2.p2">
<p class="ltx_p" id="Ch2.S4.SS2.p2.1">The first step is to use the baseline MT model for forward-translation of the source text. The generated translation might not be perfect; however, it can still include useful information about the domain. This approach bootstraps some parallel data for a situation where there was none. Then, we follow the same four steps mentioned in the previous use case.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch2.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.5     Experimental Setup</h3>
<section class="ltx_subsection" id="Ch2.S5.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.1     Datasets</h4>
<div class="ltx_para" id="Ch2.S5.SS1.p1">
<p class="ltx_p" id="Ch2.S5.SS1.p1.1">For training Arabic-to-English and English-to-Arabic generic models, we collect high-quality datasets from OPUS <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib183" title="">2012</a>)</cite>.
The breakdown of segment numbers in our datasets before and after filtering is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T1" title="Table 2.1 ‣ 2.5.3 NMT Model Architecture ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
To ensure the quality of our datasets, we apply a multi-filtering process. First, we apply rule-based filtering to individual datasets, removing duplicates, source-copied segments, those with too long source/target (ratio 200% and <math alttext="&gt;" class="ltx_Math" display="inline" id="Ch2.S5.SS1.p1.1.m1.1"><semantics id="Ch2.S5.SS1.p1.1.m1.1a"><mo id="Ch2.S5.SS1.p1.1.m1.1.1" xref="Ch2.S5.SS1.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch2.S5.SS1.p1.1.m1.1b"><gt id="Ch2.S5.SS1.p1.1.m1.1.1.cmml" xref="Ch2.S5.SS1.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S5.SS1.p1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch2.S5.SS1.p1.1.m1.1d">&gt;</annotation></semantics></math> 200 words), and HTML tags. Then, we calculate the similarity between each source and target to semantically filter out segments with a similarity threshold lower than 0.45.
Finally, we concatenate the datasets and apply global filtering. For the development and test datasets, we randomly sampled 5000 segments each from the original dataset.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Our MT preparation scripts are publicly available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/MT-Preparation" title="">https://github.com/ymoslem/MT-Preparation</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch2.S5.SS1.p2">
<p class="ltx_p" id="Ch2.S5.SS1.p2.1">For in-domain NMT models, we use TICO-19 <cite class="ltx_cite ltx_citemacro_citep">(Anastasopoulos et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib4" title="">2020</a>)</cite>, a dataset in the Public Health domain. After filtering, the dataset includes 3062 segments. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T2" title="Table 2.2 ‣ 2.5.3 NMT Model Architecture ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.2</span></a> shows the dataset details.
We split the TICO-19 dataset into a development dataset, with 1000 segments, and a test dataset which includes the rest, i.e. 2062 segments. The whole TICO-19 dataset is used for generating a large synthetic in-domain training dataset, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S5.SS5" title="2.5.5 Domain-Specific Data Generation with LMs ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.5.5</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.2     Vocabulary</h4>
<div class="ltx_para" id="Ch2.S5.SS2.p1">
<p class="ltx_p" id="Ch2.S5.SS2.p1.1">To create our vocabulary, we first train SentencePiece unigram models <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib100" title="">2018</a>; Kudo,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib99" title="">2018</a>)</cite> for the source and target individually, to learn subword units from untokenised text.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>In SentencePiece, we utilise the training options <a class="ltx_ref ltx_url ltx_font_typewriter" href="--split_digits" title="">--split_digits</a> to split all digits into separate pieces, and <a class="ltx_ref ltx_url ltx_font_typewriter" href="--byte_fallback" title="">--byte_fallback</a> to decompose unknown pieces into UTF-8 byte pieces to help avoid out-of-vocabulary tokens.</span></span></span> Then, we utilise this SentencePiece model to subword our dataset. We use a vocabulary size of 50,000. Subsequently, we convert the learned subword units into our final vocabulary in the format supported by OpenNMT-tf. Segments are automatically augmented with start and end tokens via <span class="ltx_text ltx_font_italic" id="Ch2.S5.SS2.p1.1.1">source_sequence_controls</span> option.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.3     NMT Model Architecture</h4>
<div class="ltx_para" id="Ch2.S5.SS3.p1">
<p class="ltx_p" id="Ch2.S5.SS3.p1.1">Our baseline generic NMT models use the Transformer “Big” architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite> as implemented in OpenNMT-tf, and relative position representations (Shaw et al., 2018) with a clipping distance k=20. The models consist of 6 layers with a model dimension of 1,024, split into 16 heads, and a feedforward dimension of 4,096.</p>
</div>
<figure class="ltx_table" id="Ch2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T1.1.1.1">
<td class="ltx_td ltx_border_tt" id="Ch2.T1.1.1.1.1"></td>
<td class="ltx_td ltx_border_tt" id="Ch2.T1.1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Ch2.T1.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T1.1.1.1.3.1">Filtering</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.2.2">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.2.2.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.2.2.2.1">Raw</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T1.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T1.1.2.2.3.1">Rule-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T1.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="Ch2.T1.1.2.2.4.1">Semantic</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.3.3.1">Bible</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.3.3.2">62,195</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.3.3.3">47,699</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.3.3.4">43,951</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.4.4">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.4.4.1">ELRC_2922</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.4.4.2">15,129</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.4.4.3">14,937</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.4.4.4">14,850</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.5.5">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.5.5.1">GlobalVoices</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.5.5.2">63,071</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.5.5.3">55,201</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.5.5.4">51,220</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.6.6">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.6.6.1">GNOME</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.6.6.2">150</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.6.6.3">143</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.6.6.4">134</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.7.7">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.7.7.1">Infopankki</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.7.7.2">50,769</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.7.7.3">15,531</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.7.7.4">14,635</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.8.8">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.8.8.1">KDE4</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.8.8.2">116,239</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.8.8.3">85,003</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.8.8.4">68,180</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.9.9">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.9.9.1">MultiUN</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.9.9.2">9,759,125</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.9.9.3">7,807,811</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.9.9.4">7,508,443</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.10.10">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.10.10.1">News-Commentary</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.10.10.2">97,384</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.10.10.3">80,744</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.10.10.4">77,715</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.11.11">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.11.11.1">OpenSubtitles</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.11.11.2">29,823,188</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.11.11.3">23,666,245</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.11.11.4">20,176,228</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.12.12">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.12.12.1">Tatoeba</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.12.12.2">27,905</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.12.12.3">27,649</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.12.12.4">26,714</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.13.13">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.13.13.1">Ubuntu</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.13.13.2">5,978</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.13.13.3">5,617</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.13.13.4">5,340</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.14.14">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.14.14.1">UN</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.14.14.2">74,067</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.14.14.3">63,074</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.14.14.4">62,901</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.15.15">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.15.15.1">UNPC</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.15.15.2">20,044,478</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.15.15.3">15,696,210</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.15.15.4">15,441,996</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.16.16">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.16.16.1">Wikimedia</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.16.16.2">407,543</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.16.16.3">335,783</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.16.16.4">317,285</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.17.17">
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.17.17.1">Wikipedia</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.17.17.2">151,136</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.17.17.3">117,859</td>
<td class="ltx_td ltx_align_left" id="Ch2.T1.1.17.17.4">116,940</td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.18.18.1"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.18.18.1.1">Total</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.18.18.2"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.18.18.2.1">60,698,357</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.18.18.3"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.18.18.3.1">48,019,506</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T1.1.18.18.4"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.18.18.4.1">43,926,532</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T1.1.19.19">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch2.T1.1.19.19.1"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.19.19.1.1">Global Filtering</span></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="Ch2.T1.1.19.19.2"></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch2.T1.1.19.19.3"><span class="ltx_text ltx_font_bold" id="Ch2.T1.1.19.19.3.1">40,207,905</span></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="Ch2.T1.1.19.19.4"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.1: </span>Generic datasets</figcaption>
</figure>
<figure class="ltx_table" id="Ch2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="Ch2.T2.1.1.1.1" style="width:71.1pt;"></td>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="Ch2.T2.1.1.1.2" style="width:39.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="Ch2.T2.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T2.1.1.1.3.1">Filtering</span></th>
</tr>
<tr class="ltx_tr" id="Ch2.T2.1.2.2">
<td class="ltx_td ltx_align_justify" id="Ch2.T2.1.2.2.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T2.1.2.2.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_justify" id="Ch2.T2.1.2.2.2" style="width:39.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="Ch2.T2.1.2.2.2.1">Raw</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="Ch2.T2.1.2.2.3.1">Rule-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T2.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="Ch2.T2.1.2.2.4.1">Semantic</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T2.1.3.3">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="Ch2.T2.1.3.3.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T2.1.3.3.1.1">TICO-19</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="Ch2.T2.1.3.3.2" style="width:39.8pt;">
<p class="ltx_p ltx_align_top" id="Ch2.T2.1.3.3.2.1">3,071</p>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch2.T2.1.3.3.3">3,069</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch2.T2.1.3.3.4">3,062</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.2: </span>In-domain dataset (Public Health)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.4     Training</h4>
<div class="ltx_para" id="Ch2.S5.SS4.p1">
<p class="ltx_p" id="Ch2.S5.SS4.p1.1">The training takes place on 2x NVIDIA RTX A4000 GPUs, with a batch size of 2048 tokens per GPU, for an effective batch size of 25k tokens/step. The Arabic-to-English model is trained for 240k steps, while the English-to-Arabic model is trained for 105k steps. Early stopping is used after 3 evaluations with less than 0.01 BLEU improvement on the development dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS5">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.5     Domain-Specific Data Generation with LMs</h4>
<div class="ltx_para" id="Ch2.S5.SS5.p1">
<p class="ltx_p" id="Ch2.S5.SS5.p1.1">For English, we use GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite>, a Transformer-based language model with 6B trainable parameters.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/EleutherAI/gpt-j-6B" title="">https://huggingface.co/EleutherAI/gpt-j-6B</a></span></span></span>
For Arabic, we use mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib172" title="">2022</a>)</cite>, a multilingual language model.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sberbank-ai/mGPT" title="">https://huggingface.co/sberbank-ai/mGPT</a></span></span></span>
</p>
</div>
<div class="ltx_para" id="Ch2.S5.SS5.p2">
<p class="ltx_p" id="Ch2.S5.SS5.p2.1">To fit the models onto an NVIDIA RTX A4000 GPU (16 GB of GPU memory), the half-precision floating-point (float16) format is used.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>In Hugging Face Transformers, we also set the option <a class="ltx_ref ltx_url ltx_font_typewriter" href="low_cpu_mem_usage" title="">low_cpu_mem_usage</a> to <a class="ltx_ref ltx_url ltx_font_typewriter" href="True" title="">True</a>.</span></span></span> We also use a batch size of 1.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>It is worth mentioning though that for batch generation (i.e. <math alttext="&gt;" class="ltx_Math" display="inline" id="Ch2.footnote10.m1.1"><semantics id="Ch2.footnote10.m1.1b"><mo id="Ch2.footnote10.m1.1.1" xref="Ch2.footnote10.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch2.footnote10.m1.1c"><gt id="Ch2.footnote10.m1.1.1.cmml" xref="Ch2.footnote10.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch2.footnote10.m1.1d">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch2.footnote10.m1.1e">&gt;</annotation></semantics></math>1), padding and attention masking should be used; note that left padding is required for GPT-like models.</span></span></span> For inference, we employ 50 Top-K sampling and 0.95 Top-p (nucleus) sampling <cite class="ltx_cite ltx_citemacro_citep">(Fan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib46" title="">2018</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib74" title="">2018</a>; Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib73" title="">2020</a>)</cite>. The maximum length of the generated text is set to 300 tokens, and we return 5 sequences for each segment, to get multiple independently sampled outputs. Finally, we split the generated text into sentences.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Our scripts are available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/MT-LM" title="">https://github.com/ymoslem/MT-LM</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch2.S5.SS5.p3">
<p class="ltx_p" id="Ch2.S5.SS5.p3.1">As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a>, we have two use-cases: (a) a small bilingual in-domain dataset is available; and (b) the source only is available, so we utilise forward-translation to generate the target side. After that, each target sentence of the in-domain dataset TICO-19 (i.e. the authentic target in the first case, or the MT-ed target in the second case) is fed to the LM as a prompt to generate synthetic in-domain segments. We use random seeds to generate multiple datasets, namely 2 for English and 3 for Arabic.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>As two data generation runs for Arabic resulted in a less amount of data than for English, we increased the data size for Arabic by generating a third dataset (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T3" title="Table 2.3 ‣ 2.5.5 Domain-Specific Data Generation with LMs ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.3</span></a>).</span></span></span> We filter the concatenated datasets, by removing duplicates and cleaning lines with a wrong language, and those including only dashes or filenames. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T3" title="Table 2.3 ‣ 2.5.5 Domain-Specific Data Generation with LMs ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.3</span></a> illustrates the numbers of in-domain synthetic segments generated by the LMs.</p>
</div>
<figure class="ltx_table" id="Ch2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="Ch2.T3.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="Ch2.T3.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="5" id="Ch2.T3.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T3.1.1.1.3.1">Setup 1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="Ch2.T3.1.1.1.4">
<span class="ltx_text ltx_font_bold" id="Ch2.T3.1.1.1.4.1">Setup 2</span></th>
</tr>
<tr class="ltx_tr" id="Ch2.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="Ch2.T3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.1.1">Language</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="Ch2.T3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.2.1">LM</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.3"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.3.1">1st Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.4.1">2nd Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.5"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.5.1">3rd Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.6"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.6.1">Total</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.7.1">Filtered</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.8"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.8.1">1st Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.9"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.9.1">2nd Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.10"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.10.1">3rd Run</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.11"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.11.1">Total</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Ch2.T3.1.2.2.12"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.2.2.12.1">Filtered</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T3.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.3.1.1.1">English</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.2">GPT-J</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.3">131,730</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.4">131,554</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.5">N/A</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.6">263,284</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.7">242,469</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.8">137,705</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.9">138,702</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.10">N/A</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.11">276,407</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T3.1.3.1.12">253,287</td>
</tr>
<tr class="ltx_tr" id="Ch2.T3.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T3.1.4.2.1.1">Arabic</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.2">mGPT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.3">96,296</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.4">97,031</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.5">94,513</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.6">287,840</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.7">271,665</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.8">103,272</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.9">103,459</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.10">103,303</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.11">310,034</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T3.1.4.2.12">294,391</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.3: </span>Data generated by language models (LMs)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS6">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.6     Back-Translation</h4>
<div class="ltx_para" id="Ch2.S5.SS6.p1">
<p class="ltx_p" id="Ch2.S5.SS6.p1.1">For back-translation, we use OPUS models,<span class="ltx_note ltx_role_footnote" id="Ch2.footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models" title="">https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models</a></span></span></span> specifically the Transformer-Big versions. For efficiency purposes, we convert the models to the CTranslate2<span class="ltx_note ltx_role_footnote" id="Ch2.footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenNMT/CTranslate2" title="">https://github.com/OpenNMT/CTranslate2</a></span></span></span> format (INT8 quantisation). We use beam size 5. After back-translation, we run the same rule-based and semantic filtering on the generated dataset as we did for the original datasets. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T4" title="Table 2.4 ‣ 2.5.6 Back-Translation ‣ 2.5 Experimental Setup ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a> elaborates on the numbers.</p>
</div>
<figure class="ltx_table" id="Ch2.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Ch2.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T4.1.1.1">
<td class="ltx_td ltx_border_tt" id="Ch2.T4.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Ch2.T4.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.1.1.1.2.1">Setup 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="Ch2.T4.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.1.1.1.3.1">Setup 2</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.1.2.2">
<td class="ltx_td" id="Ch2.T4.1.2.2.1"></td>
<td class="ltx_td ltx_border_t" id="Ch2.T4.1.2.2.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Ch2.T4.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.1.2.2.3.1">Filtering</span></td>
<td class="ltx_td ltx_border_t" id="Ch2.T4.1.2.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="Ch2.T4.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="Ch2.T4.1.2.2.5.1">Filtering</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.1.3.3">
<td class="ltx_td ltx_align_left" id="Ch2.T4.1.3.3.1"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.1.1">Language</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.1.3.3.2"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.2.1">Translated</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.3.3.3"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.3.1">Rule-based</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.4.1">Semantic</span></td>
<td class="ltx_td ltx_align_left" id="Ch2.T4.1.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.5.1">Translated</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.3.3.6"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.6.1">Rule-based</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.3.3.7"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.3.3.7.1">Semantic</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.1"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.4.4.1.1">English</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.2">242,469</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.3">240,329</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.4">239,931</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.5">253,287</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.6">251,357</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T4.1.4.4.7">250,317</td>
</tr>
<tr class="ltx_tr" id="Ch2.T4.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.1"><span class="ltx_text ltx_font_bold" id="Ch2.T4.1.5.5.1.1">Arabic</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.2">271,665</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.3">271,645</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.4">270,743</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.5">294,391</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.6">294,234</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T4.1.5.5.7">293,252</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.4: </span>Back-translated datasets</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch2.S5.SS7">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.5.7     Mixed Fine-tuning</h4>
<div class="ltx_para" id="Ch2.S5.SS7.p1">
<p class="ltx_p" id="Ch2.S5.SS7.p1.1">Following <cite class="ltx_cite ltx_citemacro_citet">Chu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite>, we employ the mixed fine-tuning approach. We randomly sample a portion from the generic data we used to train the baseline model, and use it during the fine-tuning step along with the in-domain dataset. Oversampling the in-domain data is a crucial step, as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a>. We first train a baseline NMT model on out-of-domain data until convergence, and then continue training the NMT baseline model on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) until convergence.</p>
</div>
<div class="ltx_para" id="Ch2.S5.SS7.p2">
<p class="ltx_p" id="Ch2.S5.SS7.p2.1">In most experiments, we fine-tuned the baseline for 5000 steps. However, for Setup 2 of the English-to-Arabic language pair, we found that the best automatic evaluation scores were achieved with training for only 500 or 1000 steps. We believe that this might be due to the quality or distribution of the generated in-domain data compared to the original generic data. Although <cite class="ltx_cite ltx_citemacro_citet">Chu et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite> observed that both regular fine-tuning and mixed fine-tuning tend to converge after 1 epoch of training, it seems there is no golden rule as to how many steps or epochs the baseline model should be fine-tuned on the mixed data. Depending on the size of data, we recommend conducting less-frequent evaluations on the development dataset during the fine-tuning process for finding out the best model checkpoint.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch2.S6">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.6     Results</h3>
<div class="ltx_para" id="Ch2.S6.p1">
<p class="ltx_p" id="Ch2.S6.p1.1">In this section, we elaborate on our automatic and human evaluations and discuss the results. As Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T5" title="Table 2.5 ‣ 2.6.1 Automatic Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.5</span></a> shows, scores obtained from diverse automatic metics provide good correlation with the human evaluation. Moreover, the linguistic analysis (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S6.SS3" title="2.6.3 Linguistic Analysis ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.6.3</span></a>) supports these numerical results, and demonstrates how the models fine-tuned on synthetic in-domain data produce more accurate translations of the in-domain test set compared to the baseline model.</p>
</div>
<section class="ltx_subsection" id="Ch2.S6.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.6.1     Automatic Evaluation</h4>
<div class="ltx_para" id="Ch2.S6.SS1.p1">
<p class="ltx_p" id="Ch2.S6.SS1.p1.1">For automatic evaluation, we calculated spBLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib140" title="">2002</a>; Goyal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib53" title="">2022</a>)</cite> which uses a <span class="ltx_text" id="Ch2.S6.SS1.p1.1.1">SentencePiece</span> tokeniser with 256,000 tokens and then the BLEU score is <span class="ltx_text" id="Ch2.S6.SS1.p1.1.2">computed</span> on the sub-worded text. spBLEU has been recently added to sacreBLEU v2.1.0.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mjpost/sacrebleu" title="">https://github.com/mjpost/sacrebleu</a></span></span></span> <span class="ltx_text" id="Ch2.S6.SS1.p1.1.3"><cite class="ltx_cite ltx_citemacro_citet">Goyal et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib53" title="">2022</a>)</cite></span> showed that spBLEU exhibits a strong correlation with the tokenisation-independent chrF++, yet has the advantage of keeping the familiarity of BLEU. To verify our results, we employed other evaluation metrics, namely the character-based metric chrF++ <cite class="ltx_cite ltx_citemacro_citep">(Popović,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib149" title="">2017</a>)</cite>, and the word-based metric TER <cite class="ltx_cite ltx_citemacro_citep">(Snover et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib176" title="">2006</a>)</cite>, as implemented in sacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib151" title="">2018</a>)</cite>. Furthermore, we integrated COMET<span class="ltx_note ltx_role_footnote" id="Ch2.footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Unbabel/COMET" title="">https://github.com/Unbabel/COMET</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Rei et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib158" title="">2020</a>)</cite> as a semantic evaluation metric, with the “wmt20-comet-da” model.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS1.p2">
<p class="ltx_p" id="Ch2.S6.SS1.p2.1">We experimented with averaging parameters across multiple model checkpoints <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite>, to address bias towards recent training data <cite class="ltx_cite ltx_citemacro_citep">(Tran et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib188" title="">2021</a>)</cite>. Sometimes, averaging multiple checkpoints of a baseline model, or averaging a baseline model with a fine-tuned model could lead to extra improvements of the automatic and/or human evaluation of our <span class="ltx_text" id="Ch2.S6.SS1.p2.1.1">models</span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T5" title="Table 2.5 ‣ 2.6.1 Automatic Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.5</span></a> shows evaluation results on the in-domain test dataset, and Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.F1" title="Figure 2.1 ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.1</span></a> elaborates on all the automatic evaluation results, including the results for averaged models.</p>
</div>
<figure class="ltx_table" id="Ch2.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.1.1">Language</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.3.1">spBLEU ↑</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.4.1">chrF++ ↑</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.5.1">TER ↓</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.6.1">COMET ↑</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch2.T5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.1.1.7.1">Human ↑</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T5.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.2.1.1.1">AR-EN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.3">44.57</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.4">66.68</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.5">46.67</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.6">65.78</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.2.1.7">87.0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.1.3.2">
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.1">Setup 1 Mixed Fine-Tuning</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.2">49.79</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.3">70.54</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.4">43.32</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.5">71.89</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.3.2.6">93.5</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.1.4.3">
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.1">Setup 2 Mixed Fine-Tuning</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.2">47.22</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.3">69.38</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.4">45.38</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.5">70.08</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.4.3.6">94.5</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch2.T5.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch2.T5.1.5.4.1.1">EN-AR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.3">36.15</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.4">58.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.5">58.29</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.6">57.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch2.T5.1.5.4.7">87.0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.1.6.5">
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.1">Setup 1 Mixed Fine-Tuning</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.2">42.38</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.3">62.52</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.4">53.99</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.5">67.48</td>
<td class="ltx_td ltx_align_left" id="Ch2.T5.1.6.5.6">90.0</td>
</tr>
<tr class="ltx_tr" id="Ch2.T5.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.1">Setup 2 Mixed Fine-Tuning</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.2">37.91</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.3">59.42</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.4">55.95</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.5">59.47</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch2.T5.1.7.6.6">88.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.5: </span>Evaluation results on the in-domain test set, TICO-19</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch2.S6.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.6.2     Human Evaluation</h4>
<div class="ltx_para" id="Ch2.S6.SS2.p1">
<p class="ltx_p" id="Ch2.S6.SS2.p1.1">Since translation focusses mainly on word choice, syntax, and semantics, and how people perceive it, we decided to complement our evaluation process with human evaluation.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.p2">
<p class="ltx_p" id="Ch2.S6.SS2.p2.1">The evaluator was an Arabic native speaker and domain expert. We conducted a bilingual evaluation, providing the evaluator with both the original source sentences and translations generated by the MT models. The human test set contained 50 sentences, randomly extracted from the original test set, and verified as accepted translations. The evaluator was asked to assess the acceptability of each translation generated by our baselines and fine-tuned MT systems, using the scale proposed by <cite class="ltx_cite ltx_citemacro_citet">Coughlin, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib34" title="">2003</a>)</cite>, ranging from 1 to 4, and outlined as follows:</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.p3">
<ul class="ltx_itemize" id="Ch2.S6.I1">
<li class="ltx_item" id="Ch2.S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S6.I1.i1.p1">
<p class="ltx_p" id="Ch2.S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S6.I1.i1.p1.1.1">4 = Ideal:</span> Not necessarily a perfect translation, but grammatically correct, with all information accurately transferred.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S6.I1.i2" style="list-style-type:none;padding-top:-3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S6.I1.i2.p1">
<p class="ltx_p" id="Ch2.S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S6.I1.i2.p1.1.1">3 = Acceptable:</span> Not perfect (stylistically or grammatically odd), but definitely comprehensible, AND with accurate transfer of all important information.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S6.I1.i3" style="list-style-type:none;padding-top:-3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S6.I1.i3.p1">
<p class="ltx_p" id="Ch2.S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S6.I1.i3.p1.1.1">2 = Possibly Acceptable:</span> Possibly comprehensible (given enough context and/or time to work it out); some information transferred accurately.</p>
</div>
</li>
<li class="ltx_item" id="Ch2.S6.I1.i4" style="list-style-type:none;padding-top:-3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch2.S6.I1.i4.p1">
<p class="ltx_p" id="Ch2.S6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Ch2.S6.I1.i4.p1.1.1">1 = Unacceptable:</span> Absolutely not comprehensible and/or little or no information is accurately <span class="ltx_text" id="Ch2.S6.I1.i4.p1.1.2">transferred</span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch2.S6.SS2.p4">
<p class="ltx_p" id="Ch2.S6.SS2.p4.1">Human evaluation results on the in-domain dataset, TICO-19, are expressed in percentage points in the last column of Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T5" title="Table 2.5 ‣ 2.6.1 Automatic Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.5</span></a>. In addition, Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.T6" title="Table 2.6 ‣ 2.6.2 Human Evaluation ‣ 2.6 Results ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.6</span></a> elaborates on the results for all the systems, showing the mean value for each system on the 1-4 scale.<span class="ltx_note ltx_role_footnote" id="Ch2.footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>Sentence-level human evaluation is available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/MT-LM" title="">https://github.com/ymoslem/MT-LM</a></span></span></span> The models fine-tuned on the domain-specific synthetic dataset achieve improvements on the in-domain test set, while retaining the baseline’s quality on the generic holdout test set.</p>
</div>
<figure class="ltx_table" id="Ch2.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ch2.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch2.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch2.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.1.1">Language</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch2.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.2.1">Test Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.3.1">BS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.4.1">BS-Avg8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.5.1">MixFT-1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.6.1">MixFT-1+BS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.7.1">MixFT-1+BS-Avg8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.8"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.8.1">MixFT-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.9"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.9.1">MixFT-2+BS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch2.T6.1.1.1.10"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.1.1.10.1">MixFT-2+BS-Avg8</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch2.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T6.1.2.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.2.1.1.1">AR-EN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch2.T6.1.2.1.2"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.2.1.2.1">Generic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.3">3.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.4"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.2.1.4.1">3.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.5">3.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.6">3.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.7">3.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.8">3.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.9">3.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch2.T6.1.2.1.10">3.84</td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch2.T6.1.3.2.1"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.3.2.1.1">TICO-19</span></th>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.2">3.48</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.3">3.62</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.4">3.74</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.5"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.3.2.5.1">3.82</span></td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.6">3.80</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.7">3.78</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.8">3.72</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.3.2.9">3.74</td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch2.T6.1.4.3.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch2.T6.1.4.3.2"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.2.1">Generic</span></th>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.3"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.3.1">3.96</span></td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.4">3.90</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.5">3.82</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.6"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.6.1">3.96</span></td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.7">3.90</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.8">3.94</td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.9"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.9.1">3.96</span></td>
<td class="ltx_td ltx_align_center" id="Ch2.T6.1.4.3.10"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.4.3.10.1">3.96</span></td>
</tr>
<tr class="ltx_tr" id="Ch2.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch2.T6.1.5.4.1"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.5.4.1.1">TICO-19</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.2">3.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.3">3.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.4"><span class="ltx_text ltx_font_bold" id="Ch2.T6.1.5.4.4.1">3.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.5">3.54</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.6">3.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.7">3.54</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.8">3.56</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch2.T6.1.5.4.9">3.54</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2.6: </span><span class="ltx_ERROR undefined" id="Ch2.T6.3.1">\nohyphens</span>Human evaluation of MT models for Arabic-to-English (AR-EN) and English-to-Arabic (EN-AR) language pairs, the baseline (BS), baseline averaged 8 checkpoints (BS-Avg8), mixed fine-tuning model (MixFT), averaging MixFT with BS (MixFT+BS), and averaging MixFT with BS-Avg8 (MixFT+BS-Avg8). MixFT-1 refers to Setup 1 and MixFT-2 refers to Setup 2.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch2.S6.SS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">2.6.3     Linguistic Analysis</h4>
<div class="ltx_para" id="Ch2.S6.SS3.p1">
<p class="ltx_p" id="Ch2.S6.SS3.p1.1">We observe that in several cases, the fine-tuned (<span class="ltx_text" id="Ch2.S6.SS3.p1.1.1">in-domain</span>) models generate more idiomatic translations or better capture the meaning in the Public Health context. Samples from the test dataset translated by the baseline model and in-domain models reflect these improvements.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS3.p2">
<p class="ltx_p" id="Ch2.S6.SS3.p2.1">Among Arabic-to-English examples, the phrase “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p2.1.1">\&lt;</span>غير مسببة للأمراض في مضيفاتها المستودعة الطبيعية¿” was translated as “not pathogenic in their naturally <span class="ltx_text" id="Ch2.S6.SS3.p2.1.2">occurring</span> host” by the baseline, and “non-pathogenic in their natural reservoir hosts” by both in-domain models. The former <span class="ltx_text" id="Ch2.S6.SS3.p2.1.3">translation</span> somehow conveys the meaning; however, the latter translation is more idiomatic in the <span class="ltx_text" id="Ch2.S6.SS3.p2.1.4">medical</span> context. The baseline system translated “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p2.1.5">\&lt;</span>حمامات الولادة¿” as “maternity wards” which is an incorrect translation, while the in-domain models in Setup 1 and Setup 2 produced more relevant translations as “birthing pools” and “birth baths”, respectively. The baseline model translated “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p2.1.6">\&lt;</span>مسحة بلعومية أنفية¿” as “a nasal laryngeal swab” which is an inaccurate translation. In contrast, both in-domain models translated the term as “a nasal nasopharyngeal swab”, which uses the <span class="ltx_text" id="Ch2.S6.SS3.p2.1.7">accurate</span> “nasopharyngeal” medical term. It can still be edited by removing the redundant “nasal”; however, our evaluator gave it a higher score than the translation provided by the baseline. The term “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p2.1.8">\&lt;</span>اختبارات مَصليَّة¿” was translated as “serum tests” by the baseline, while it was translated as “serological tests” by both in-domain models, which is more idiomatic.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS3.p3">
<p class="ltx_p" id="Ch2.S6.SS3.p3.1">Examining some of the English-to-Arabic translations, the baseline model mistranslated “HCoVs” as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p3.1.1">\&lt;</span>فيروسات نقص المناعة البشرية<math alttext="/" class="ltx_Math" display="inline" id="Ch2.S6.SS3.p3.1.m1.1"><semantics id="Ch2.S6.SS3.p3.1.m1.1a"><mo id="Ch2.S6.SS3.p3.1.m1.1.1" xref="Ch2.S6.SS3.p3.1.m1.1.1.cmml">/</mo><annotation-xml encoding="MathML-Content" id="Ch2.S6.SS3.p3.1.m1.1b"><divide id="Ch2.S6.SS3.p3.1.m1.1.1.cmml" xref="Ch2.S6.SS3.p3.1.m1.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S6.SS3.p3.1.m1.1c">/</annotation><annotation encoding="application/x-llamapun" id="Ch2.S6.SS3.p3.1.m1.1d">/</annotation></semantics></math>متلازمة نقص المناعة المكتسب (الإيدز)¿” (HIV/AIDS), as opposed to the in-domain models, which correctly translated it as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p3.1.2">\&lt;</span>فيروسات كورونا البشرية¿” or just <span class="ltx_text" id="Ch2.S6.SS3.p3.1.3">“HCoV <span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p3.1.3.1">\&lt;</span>فيروسات¿”</span>. Interestingly, even for a simpler phrase like “five times more cases”, the <span class="ltx_text" id="Ch2.S6.SS3.p3.1.4">baseline</span> incorrectly translated it as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p3.1.5">\&lt;</span>خمس حالات¿” (five cases), whilst the in-domain models correctly conveyed the meaning as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p3.1.6">\&lt;</span>خمسة أضعاف الحالات¿”.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS3.p4">
<p class="ltx_p" id="Ch2.S6.SS3.p4.1">There are also examples where one of the in-domain systems generated the correct translation while the other could not. For instance, both the baseline and Setup 2 in-domain model translated “If you do wear a mask” as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.1">\&lt;</span>إذا كنت لا ارتداء قناع¿”, which is both syntactically and semantically incorrect. In contrast, the Setup 1 in-domain model perfectly translated it as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.2">\&lt;</span>إذا كنت ترتدي قناعًا¿”. The baseline model translated the phrase “passive antibody therapy” as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.3">\&lt;</span>العلاج السلبي للأجسام المضادة¿”, which uses the preposition “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.4">\&lt;</span>لـ¿” (of) instead of “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.5">\&lt;</span>بـ¿” (with), missing the fact that in this context “antibody” is equivalent to <span class="ltx_text" id="Ch2.S6.SS3.p4.1.6">“antibody-based”</span> rather than being the issue to be treated. Similarly, the Setup 2 in-domain model mistranslated it as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.7">\&lt;</span>العلاج المضاد السلبي¿” while the Setup 1 in-domain model accurately translated it as “<span class="ltx_ERROR undefined" id="Ch2.S6.SS3.p4.1.8">\&lt;</span>العلاج السلبي بالأجسام المضادة¿”.</p>
</div>
<div class="ltx_para" id="Ch2.S6.SS3.p5">
<p class="ltx_p" id="Ch2.S6.SS3.p5.1">Since some phrases can be expressed in multiple ways, we notice that sometimes the evaluator equally ranked different translations. This might explain why automatic metrics evaluate Arabic-to-English Setup 1 higher than Setup 2, whereas the human evaluation shows that the translation quality of both setups is comparable.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch2.S7">
<h3 class="ltx_title ltx_font_bold ltx_title_section">2.7     Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch2.S7.p1">
<p class="ltx_p" id="Ch2.S7.p1.1">In this work, we propose two simple methods to utilise pre-trained language models for domain-specific data augmentation for NMT systems. We report significant improvements, supported by both automatic and human evaluation. The proposed techniques enable the generation of large amounts of data, simulating the characteristics of the specialised text to be translated, and facilitating the domain adaptation process.</p>
</div>
<div class="ltx_para" id="Ch2.S7.p2">
<p class="ltx_p" id="Ch2.S7.p2.1">For the Arabic-to-English language direction, human evaluation demonstrates that Setup 2 is on par with Setup 1 even though in the former we did not have any authentic bilingual in-domain data (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2.S4" title="2.4 Methods ‣ Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2.4</span></a>). Nevertheless, the English-to-Arabic model in Setup 2 has lower performance compared to the Setup 1 model, although both setups outperform the baseline on the in-domain test set. We believe this might be due to the quality of synthetic data generated for Arabic, which is an interesting aspect to explore further.</p>
</div>
<div class="ltx_para" id="Ch2.S7.p3">
<p class="ltx_p" id="Ch2.S7.p3.1">In the future, we would like to investigate utilising terminology for domain-specific data generation, and experiment with employing the same proposed approaches for low-resource languages and multilingual settings.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<div class="ltx_titlepage" id="id2">
<p class="ltx_p" id="id2.1"><span class="ltx_text ltx_font_bold" id="id2.1.1">Figures: Automatic Evaluation</span>
</p>
</div>
<figure class="ltx_figure" id="fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf1.g1" src="extracted/5369614/img/DomainArabicEnglishExp1.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>AR-EN Setup 1 - In-Domain Test Set (TICO-19)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf2.g1" src="extracted/5369614/img/DomainArabicEnglishExp2.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>AR-EN Setup 2 - In-Domain Test Set (TICO-19)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf3.g1" src="extracted/5369614/img/GenericArabicEnglishExp1.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>AR-EN Setup 1 - Generic Test Set</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf4.g1" src="extracted/5369614/img/GenericArabicEnglishExp2.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>AR-EN Setup 2 - Generic Test Set</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</p>
</div>
<figure class="ltx_figure" id="Ch2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf5.g1" src="extracted/5369614/img/DomainEnglishArabicExp1.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>EN-AR Setup 1 - In-Domain Test Set (TICO-19)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf6.g1" src="extracted/5369614/img/DomainEnglishArabicExp2.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>EN-AR Setup 2 - In-Domain Test Set (TICO-19)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf7.g1" src="extracted/5369614/img/GenericEnglishArabicExp1.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>EN-AR Setup 1 - Generic Test Set</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="Ch2.F1.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="225" id="Ch2.F1.sf8.g1" src="extracted/5369614/img/GenericEnglishArabicExp2.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>EN-AR Setup 2 - Generic Test Set</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2.1: </span><span class="ltx_ERROR undefined" id="Ch2.F1.2.1">\nohyphens</span>Performance comparison of 5 models for Arabic-to-English (AR-EN) and English-to-Arabic (EN-AR) language pairs, the baseline (BS), baseline averaged 8 checkpoints (BS-Avg8), mixed fine-tuning model (MixFT), averaging MixFT with BS (MixFT+BS), and averaging MixFT with BS-Avg8 (MixFT+BS-Avg8). The MixFT models fine-tuned on the domain-specific synthetic dataset achieve improvements on the in-domain test set (a,b &amp; e,f), while retaining the baselines quality on the generic test set (c,d &amp; g,h).</figcaption>
</figure>
<section class="ltx_chapter" id="Ch3">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 3      Translation Word-Level Auto-Completion</h2>
<div class="ltx_para" id="Ch3.p1">
<p class="ltx_p" id="Ch3.p1.1">Yasmin Moslem, Rejwanul Haque, John Kelleher, and Andy Way</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch3.p2">
<p class="ltx_p" id="Ch3.p2.1">In Proceedings of the Seventh Conference on Machine Translation (WMT 2022), pages 1176–1181, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Published at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.wmt-1.119/" title="">https://aclanthology.org/2022.wmt-1.119/</a></span></span></span></p>
</div>
</section>
<div class="ltx_abstract">
<span class="ltx_ERROR undefined" id="id12.id1">\nohyphens</span>
<p class="ltx_p" id="id13.id2">Research on Machine Translation (MT) has achieved important breakthroughs in several areas. While there is much more to be done in order to build on this success, we believe that the language industry needs better ways to take full advantage of current achievements. Due to a combination of factors, including time, resources, and skills, businesses tend to apply pragmatism into their AI workflows. Hence, they concentrate more on outcomes, e.g. delivery, shipping, releases, and features, and adopt high-level working production solutions, where possible. Among the features thought to be helpful for translators are sentence-level and word-level translation auto-suggestion and auto-completion. Suggesting alternatives can inspire translators and limit their need to refer to external resources, which hopefully boosts their productivity. This work describes our submissions to WMT’s shared task on word-level auto-completion, for the Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German language directions. We investigate the possibility of using pre-trained models and out-of-the-box features from available libraries. We employ random sampling to generate diverse alternatives, which reveals good results. Furthermore, we introduce our open-source API, based on CTranslate2, to serve translations, auto-suggestions, and auto-completions.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Ch3.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.1     Context</h3>
<div class="ltx_para" id="Ch3.S1.p1">
<p class="ltx_p" id="Ch3.S1.p1.1">As large language models (LLMs) demonstrate a high level of generation diversity through sampling techniques, this reminds us that encoder-decoder MT models can also employ that autoaggressive feature at the decoder level. The research into interactive MT is not new. It was inspired by well-established techniques such as teacher forcing <cite class="ltx_cite ltx_citemacro_citep">(Williams and Zipser,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib201" title="">1989</a>)</cite>, where the ground truth previous tokens are fed into the decoder, instead of the predicted tokens y<sub class="ltx_sub" id="Ch3.S1.p1.1.1">i-1</sub> as suggested by <cite class="ltx_cite ltx_citemacro_citet">Bahdanau et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib7" title="">2015</a>)</cite>, and then the model is expected to predict the next words. In this context, <cite class="ltx_cite ltx_citemacro_citet">Langlais et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib104" title="">2000</a>)</cite> proposed a system that watches over the user while typing a translation and repeatedly suggests completions for the text already entered. Later, several researchers studied this feature to improve the interactivity of encoder-decoder MT models. For example, <cite class="ltx_cite ltx_citemacro_citet">Peris et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib145" title="">2017</a>)</cite> proposed segment-based interactive MT; besides correcting a wrong word, the user can validate segments (word sequences) to be kept in future iterations. The system then offers alternative hypotheses that take into account the corrected word together with the validated segments. Such auto-suggestions help to adapt the translation to the desired style and terminology at inference time. Recently, researchers started to look into even prompting encoder-decoder models in a manner similar to prompting decoder-only LLMs <cite class="ltx_cite ltx_citemacro_citep">(Patel et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib141" title="">2023</a>)</cite>. In 2022, WMT organised a shared task on MT word-level auto-completion. We found it a good opportunity to explore this direction that employs the autoregressive feature of the decoder of an MT model through random sampling techniques usually used in language modelling. The next sections elaborate on our submitted systems that employed random sampling to generate diverse alternatives at inference time, and achieved excellent results (1st and 2nd places in the shared task) based on both automatic and human evaluation.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.2     Introduction</h3>
<div class="ltx_para" id="Ch3.S2.p1">
<p class="ltx_p" id="Ch3.S2.p1.1">Translation auto-suggestion and auto-completion are among the important features that can help translators better utilise MT systems. In a Computer-Aided Translation (CAT) environment, a translator can make use of the MT word auto-suggestion feature as follows:</p>
<ul class="ltx_itemize" id="Ch3.S2.I1">
<li class="ltx_item" id="Ch3.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i1.p1">
<p class="ltx_p" id="Ch3.S2.I1.i1.p1.1">typing a few words, or clicking a word in a proposed MT translation, a list of suggestions is displayed, as illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.F1" title="Figure 3.1 ‣ 3.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.1</span></a>.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The image is from our demo at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.machinetranslation.io/" title="">https://www.machinetranslation.io/</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="Ch3.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S2.I1.i2.p1">
<p class="ltx_p" id="Ch3.S2.I1.i2.p1.1">selecting one of the word suggestions from the list, the rest of the translation is modified accordingly.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="Ch3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="172" id="Ch3.F1.g1" src="extracted/5369614/img/autosuggest.png" width="589"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3.1: </span>Auto-Suggest: Word Suggestions List</figcaption>
</figure>
<div class="ltx_para" id="Ch3.S2.p2">
<p class="ltx_p" id="Ch3.S2.p2.5">The WMT’s Word-Level AutoCompletion (WLAC) shared task addresses a more specific scenario, where the user types a few characters, and the system predicts and auto-completes the correct word, given the current context. The WLAC task even suggests that the context might be partial, and it can consist of preceding and/or following words. Given a source sequence <math alttext="x" class="ltx_Math" display="inline" id="Ch3.S2.p2.1.m1.1"><semantics id="Ch3.S2.p2.1.m1.1a"><mi id="Ch3.S2.p2.1.m1.1.1" xref="Ch3.S2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p2.1.m1.1b"><ci id="Ch3.S2.p2.1.m1.1.1.cmml" xref="Ch3.S2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p2.1.m1.1d">italic_x</annotation></semantics></math>, typed character sequence <math alttext="s" class="ltx_Math" display="inline" id="Ch3.S2.p2.2.m2.1"><semantics id="Ch3.S2.p2.2.m2.1a"><mi id="Ch3.S2.p2.2.m2.1.1" xref="Ch3.S2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p2.2.m2.1b"><ci id="Ch3.S2.p2.2.m2.1.1.cmml" xref="Ch3.S2.p2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p2.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p2.2.m2.1d">italic_s</annotation></semantics></math> and a context <math alttext="c" class="ltx_Math" display="inline" id="Ch3.S2.p2.3.m3.1"><semantics id="Ch3.S2.p2.3.m3.1a"><mi id="Ch3.S2.p2.3.m3.1.1" xref="Ch3.S2.p2.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p2.3.m3.1b"><ci id="Ch3.S2.p2.3.m3.1.1.cmml" xref="Ch3.S2.p2.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p2.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p2.3.m3.1d">italic_c</annotation></semantics></math>, WLAC aims to predict a target word <math alttext="w" class="ltx_Math" display="inline" id="Ch3.S2.p2.4.m4.1"><semantics id="Ch3.S2.p2.4.m4.1a"><mi id="Ch3.S2.p2.4.m4.1.1" xref="Ch3.S2.p2.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p2.4.m4.1b"><ci id="Ch3.S2.p2.4.m4.1.1.cmml" xref="Ch3.S2.p2.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p2.4.m4.1c">w</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p2.4.m4.1d">italic_w</annotation></semantics></math> which is to be placed in the middle between the left context c<sub class="ltx_sub" id="Ch3.S2.p2.5.1">l</sub> and right context c<sub class="ltx_sub" id="Ch3.S2.p2.5.2">r</sub> to constitute a partial translation. Note that the last word of c<sub class="ltx_sub" id="Ch3.S2.p2.5.3">l</sub>, the auto-completed word <math alttext="w" class="ltx_Math" display="inline" id="Ch3.S2.p2.5.m5.1"><semantics id="Ch3.S2.p2.5.m5.1a"><mi id="Ch3.S2.p2.5.m5.1.1" xref="Ch3.S2.p2.5.m5.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p2.5.m5.1b"><ci id="Ch3.S2.p2.5.m5.1.1.cmml" xref="Ch3.S2.p2.5.m5.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p2.5.m5.1c">w</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p2.5.m5.1d">italic_w</annotation></semantics></math>, and the first word of c<sub class="ltx_sub" id="Ch3.S2.p2.5.4">r</sub> are not necessary consecutive.</p>
</div>
<div class="ltx_para" id="Ch3.S2.p3">
<p class="ltx_p" id="Ch3.S2.p3.10">Previous work proposed diverse approaches, mostly to translation sentence-level auto-suggestion and auto-completion. In their work, <cite class="ltx_cite ltx_citemacro_citet">Li et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib108" title="">2021</a>)</cite> proposed an approach to tackle the word-level auto-completion task. Given a tuple (<math alttext="x" class="ltx_Math" display="inline" id="Ch3.S2.p3.1.m1.1"><semantics id="Ch3.S2.p3.1.m1.1a"><mi id="Ch3.S2.p3.1.m1.1.1" xref="Ch3.S2.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.1.m1.1b"><ci id="Ch3.S2.p3.1.m1.1.1.cmml" xref="Ch3.S2.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.1.m1.1d">italic_x</annotation></semantics></math>, <math alttext="c" class="ltx_Math" display="inline" id="Ch3.S2.p3.2.m2.1"><semantics id="Ch3.S2.p3.2.m2.1a"><mi id="Ch3.S2.p3.2.m2.1.1" xref="Ch3.S2.p3.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.2.m2.1b"><ci id="Ch3.S2.p3.2.m2.1.1.cmml" xref="Ch3.S2.p3.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.2.m2.1d">italic_c</annotation></semantics></math>, <math alttext="s" class="ltx_Math" display="inline" id="Ch3.S2.p3.3.m3.1"><semantics id="Ch3.S2.p3.3.m3.1a"><mi id="Ch3.S2.p3.3.m3.1.1" xref="Ch3.S2.p3.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.3.m3.1b"><ci id="Ch3.S2.p3.3.m3.1.1.cmml" xref="Ch3.S2.p3.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.3.m3.1d">italic_s</annotation></semantics></math>), the system decomposes the word autocompletion process into two parts: 1) model the distribution of the target word <math alttext="w" class="ltx_Math" display="inline" id="Ch3.S2.p3.4.m4.1"><semantics id="Ch3.S2.p3.4.m4.1a"><mi id="Ch3.S2.p3.4.m4.1.1" xref="Ch3.S2.p3.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.4.m4.1b"><ci id="Ch3.S2.p3.4.m4.1.1.cmml" xref="Ch3.S2.p3.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.4.m4.1c">w</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.4.m4.1d">italic_w</annotation></semantics></math> based on the source sequence <math alttext="x" class="ltx_Math" display="inline" id="Ch3.S2.p3.5.m5.1"><semantics id="Ch3.S2.p3.5.m5.1a"><mi id="Ch3.S2.p3.5.m5.1.1" xref="Ch3.S2.p3.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.5.m5.1b"><ci id="Ch3.S2.p3.5.m5.1.1.cmml" xref="Ch3.S2.p3.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.5.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.5.m5.1d">italic_x</annotation></semantics></math> and the translation context <math alttext="c" class="ltx_Math" display="inline" id="Ch3.S2.p3.6.m6.1"><semantics id="Ch3.S2.p3.6.m6.1a"><mi id="Ch3.S2.p3.6.m6.1.1" xref="Ch3.S2.p3.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.6.m6.1b"><ci id="Ch3.S2.p3.6.m6.1.1.cmml" xref="Ch3.S2.p3.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.6.m6.1c">c</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.6.m6.1d">italic_c</annotation></semantics></math>; and 2) find the most possible word <math alttext="w" class="ltx_Math" display="inline" id="Ch3.S2.p3.7.m7.1"><semantics id="Ch3.S2.p3.7.m7.1a"><mi id="Ch3.S2.p3.7.m7.1.1" xref="Ch3.S2.p3.7.m7.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.7.m7.1b"><ci id="Ch3.S2.p3.7.m7.1.1.cmml" xref="Ch3.S2.p3.7.m7.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.7.m7.1c">w</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.7.m7.1d">italic_w</annotation></semantics></math> based on the distribution and human typed sequence <math alttext="s" class="ltx_Math" display="inline" id="Ch3.S2.p3.8.m8.1"><semantics id="Ch3.S2.p3.8.m8.1a"><mi id="Ch3.S2.p3.8.m8.1.1" xref="Ch3.S2.p3.8.m8.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.8.m8.1b"><ci id="Ch3.S2.p3.8.m8.1.1.cmml" xref="Ch3.S2.p3.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.8.m8.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.8.m8.1d">italic_s</annotation></semantics></math>. Hence, they first use a single placeholder [MASK] to represent the unknown target word <math alttext="w" class="ltx_Math" display="inline" id="Ch3.S2.p3.9.m9.1"><semantics id="Ch3.S2.p3.9.m9.1a"><mi id="Ch3.S2.p3.9.m9.1.1" xref="Ch3.S2.p3.9.m9.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.9.m9.1b"><ci id="Ch3.S2.p3.9.m9.1.1.cmml" xref="Ch3.S2.p3.9.m9.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.9.m9.1c">w</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.9.m9.1d">italic_w</annotation></semantics></math>, and use the representation of [MASK] learned from the word prediction model, based on BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib38" title="">2019</a>)</cite>, to predict it. Then, the predicted distribution of the masked token is used over the vocabulary to filter out invalid words, namely those that do not start with the human typed <span class="ltx_text" id="Ch3.S2.p3.10.1">sequence <math alttext="s" class="ltx_Math" display="inline" id="Ch3.S2.p3.10.1.m1.1"><semantics id="Ch3.S2.p3.10.1.m1.1a"><mi id="Ch3.S2.p3.10.1.m1.1.1" xref="Ch3.S2.p3.10.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Ch3.S2.p3.10.1.m1.1b"><ci id="Ch3.S2.p3.10.1.m1.1.1.cmml" xref="Ch3.S2.p3.10.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S2.p3.10.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="Ch3.S2.p3.10.1.m1.1d">italic_s</annotation></semantics></math></span>. Finally, they return the token with the highest probability over the new distribution.</p>
</div>
<div class="ltx_para" id="Ch3.S2.p4">
<p class="ltx_p" id="Ch3.S2.p4.1">Researchers in other natural language processing areas such as language modelling offered approaches to improve predictions of decoder-only autoregressive models, trained to predict the next word given the previous context. Among these approaches are top-K sampling and top-p (nucleus) sampling <cite class="ltx_cite ltx_citemacro_citep">(Fan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib46" title="">2018</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib74" title="">2018</a>; Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib73" title="">2020</a>)</cite>. Since NMT inference depends on a decoder model, such approaches from language modelling can be employed. In particular, we investigate the use of top-K sampling during decoding to generate better word-level auto-completions.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.3     User Survey</h3>
<div class="ltx_para" id="Ch3.S3.p1">
<p class="ltx_p" id="Ch3.S3.p1.1">Previous work reported that a user can save over 60% of the keystrokes needed to produce a translation in a word completion scenario <cite class="ltx_cite ltx_citemacro_citep">(Langlais et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib104" title="">2000</a>)</cite>. Other researchers noted that post-editing is faster than MT auto-completion <cite class="ltx_cite ltx_citemacro_citep">(Koehn,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib96" title="">2009</a>)</cite>, while MT auto-completion can yield higher quality translation when the baseline MT quality is high <cite class="ltx_cite ltx_citemacro_citep">(Green et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib54" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch3.S3.p2">
<p class="ltx_p" id="Ch3.S3.p2.1">In a user survey we designed and distributed via social media networks, we asked participants whether they thought an MT word-level auto-suggestions feature could be helpful, and provided a simple definition and an illustrative image. If their answer was “yes”, the respondent was asked to specify a reason. By the time of writing this work, we received 41 responses to our survey. While we do not believe this survey is enough to justify introducing an auto-suggestions feature into every MT system, it can be an indicator as to why some users think such a feature could be helpful. To answer the question, “Which of the following best describes you?” 46.3% (19) of the respondents chose “Translator/Linguist”, 31.7% (13) selected “NLP Engineer/Researcher”, and the rest 22% (9) were other “MT Users”, not included in the two aforementioned categories.</p>
</div>
<figure class="ltx_figure" id="Ch3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="95" id="Ch3.F2.g1" src="extracted/5369614/img/autosuggest-categories.png" width="310"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3.2: </span>MT user categories</figcaption>
</figure>
<div class="ltx_para" id="Ch3.S3.p3">
<p class="ltx_p" id="Ch3.S3.p3.1">Among the respondents to the survey, 90.2% (37) answered “Yes” to the question “In general, do you believe that a word-level auto-suggestions feature is helpful?” Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.F3" title="Figure 3.3 ‣ 3.3 User Survey ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.3</span></a> shows the breakdown of answers to the question, “Why do you believe that a word-level auto-suggestions feature can be helpful?” taking into consideration those who answered “No” to the previous question.</p>
</div>
<div class="ltx_para" id="Ch3.S3.p4">
<p class="ltx_p" id="Ch3.S3.p4.1">Out of the 37 persons who believed a word-level auto-suggestions feature can be helpful, 40.5% (15) of the respondents specified that it can give them some inspiration. This answer is specifically interesting as it is not constrained by time-saving benefits; hence, it focusses more on effectiveness rather than efficiency. The respondent that answered with “Other” mentioned that it allows them to look for alternative senses or phrasings, especially when they suspect the initial translation is bad, and referred to this as “human in the loop”.</p>
</div>
<div class="ltx_para" id="Ch3.S3.p5">
<p class="ltx_p" id="Ch3.S3.p5.1">Respondents were allowed to give extra comments; among the notable comments were:</p>
<ul class="ltx_itemize" id="Ch3.S3.I1">
<li class="ltx_item" id="Ch3.S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I1.i1.p1">
<p class="ltx_p" id="Ch3.S3.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I1.i1.p1.1.1">I think word-level suggestions can be a <span class="ltx_text" id="Ch3.S3.I1.i1.p1.1.1.1">useful</span> feature, particularly when the target language can have several translations of a single source word.</em></p>
</div>
</li>
<li class="ltx_item" id="Ch3.S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I1.i2.p1">
<p class="ltx_p" id="Ch3.S3.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I1.i2.p1.1.1">Word-level suggestions can be helpful, but sometimes you end up spending a lot of time figuring out if the MT suggestion is a valid translation in that context. So, I’m not really sure yet how I feel about it.</em></p>
</div>
</li>
<li class="ltx_item" id="Ch3.S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I1.i3.p1">
<p class="ltx_p" id="Ch3.S3.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I1.i3.p1.1.1">It’s useful, as long as it’s seen as a suggestion, and not inserted in the target where the translator is typing.</em></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S3.p6">
<p class="ltx_p" id="Ch3.S3.p6.1">Among the respondents who chose “For me, it is easier or faster than typing”, comments included:</p>
<ul class="ltx_itemize" id="Ch3.S3.I2">
<li class="ltx_item" id="Ch3.S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I2.i1.p1">
<p class="ltx_p" id="Ch3.S3.I2.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I2.i1.p1.1.1">Though most of the time; the suggestions are lousy.</em></p>
</div>
</li>
<li class="ltx_item" id="Ch3.S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I2.i2.p1">
<p class="ltx_p" id="Ch3.S3.I2.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I2.i2.p1.1.1">I don’t think it gives me inspiration as I mostly need it for structures, not single words.</em></p>
</div>
</li>
<li class="ltx_item" id="Ch3.S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S3.I2.i3.p1">
<p class="ltx_p" id="Ch3.S3.I2.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch3.S3.I2.i3.p1.1.1">Auto-suggestion does not have to come from machine translation. History is much more useful.</em></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S3.p7">
<p class="ltx_p" id="Ch3.S3.p7.1">The last comment above might be referring to the fact that in some CAT tools, auto-suggestions can also include glossary terms, and translation memory sub-segments, which encourages further research efforts to investigate methods to enhance leveraging and interaction between various translation resources in human-in-the-loop environments.</p>
</div>
<figure class="ltx_figure" id="Ch3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="170" id="Ch3.F3.g1" src="extracted/5369614/img/autosuggest-why.png" width="310"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3.3: </span>How translators and other MT users perceive word-level auto-suggestions</figcaption>
</figure>
<div class="ltx_para" id="Ch3.S3.p8">
<p class="ltx_p" id="Ch3.S3.p8.1">We hope this survey will inspire future user <span class="ltx_text" id="Ch3.S3.p8.1.1">studies</span> to look deeper into how diverse users of MT and CAT tools prefer to utilise certain features, such as auto-suggestions, and the value they seek. More aspects should be taken into consideration, such as language pairs, translation workflows, and user interfaces. This can help improve these features to better support linguists and other MT users and boost their productivity as well as translation quality.</p>
</div>
</section>
<section class="ltx_section" id="Ch3.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.4     Experimental Setup</h3>
<section class="ltx_paragraph" id="Ch3.S4.SS0.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">Models</h6>
<div class="ltx_para" id="Ch3.S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Ch3.S4.SS0.SSS0.Px1.p1.1">We use OPUS pre-trained models<span class="ltx_note ltx_role_footnote" id="Ch3.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Helsinki-NLP/Tatoeba-Challenge" title="">https://github.com/Helsinki-NLP/Tatoeba-Challenge</a></span></span></span> based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite> for the Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German language directions.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch3.S4.SS0.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">Tokenisers</h6>
<div class="ltx_para" id="Ch3.S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Ch3.S4.SS0.SSS0.Px2.p1.2">OPUS models depend on SentencePiece<span class="ltx_note ltx_role_footnote" id="Ch3.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib100" title="">2018</a>)</cite> for tokenisation. Hence, we use their provided subword models in our pre-processing and post-processing processes. As OPUS’s English-to-Chinese model requires defining the target dialect using a pre-specified token, we prepend <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2">[“<math alttext="&gt;&gt;" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1"><semantics id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1a"><mo id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1.1" mathvariant="italic" xref="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml">&gt;&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1b"><csymbol cd="latexml" id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1.1">much-greater-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1c">&gt;&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px2.p1.1.1.m1.1d">&gt; &gt;</annotation></semantics></math>cmn_Hans<math alttext="&lt;&lt;" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1"><semantics id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1a"><mo id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1.1" mathvariant="italic" xref="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1.1.cmml">&lt;&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1b"><csymbol cd="latexml" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1c">&lt;&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px2.p1.2.2.m2.1d">&lt; &lt;</annotation></semantics></math>”]</span> to the list of tokens generated by SentencePiece. For word-level tokenisation, we use NLTK for English and German, and Jieba<span class="ltx_note ltx_role_footnote" id="Ch3.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/fxsjy/jieba" title="">https://github.com/fxsjy/jieba</a></span></span></span> for Chinese. This list of words can be used later to find the word that starts with the typed sequence.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch3.S4.SS0.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">Inference Engine</h6>
<div class="ltx_para" id="Ch3.S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Ch3.S4.SS0.SSS0.Px3.p1.8">We employ CTranslate2 <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib92" title="">Klein et al., 2020a, </a>)</cite> for sentence-level MT, as well as for translation auto-suggestions. To this end, we first convert OPUS models into the <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px3.p1.8.1">CTranslate2</span> format. After that, we utilise a number of CTranslate2 decoding features, including “alternatives at a position” and “auto-completion”.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenNMT/CTranslate2/blob/master/docs/decoding.md" title="">https://github.com/OpenNMT/CTranslate2/blob/master/docs/decoding.md</a></span></span></span> The translation options <math alttext="return\_alternatives" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.4.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.5" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.5.cmml">u</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1c" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.6" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.6.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1d" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.7" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.7.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1e" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.8" mathvariant="normal" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.8.cmml">_</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1f" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.9" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.9.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1g" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.10" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.10.cmml">l</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1h" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.11" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.11.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1i" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.12" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.12.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1j" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.13" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.13.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1k" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.14" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.14.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1l" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.15" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.15.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1m" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.16" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.16.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1n" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.17" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.17.cmml">i</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1o" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.18" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.18.cmml">v</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1p" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.19" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.19.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1q" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.20" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.20.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.2">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.3">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.4">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.5">𝑢</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.6.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.6">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.7.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.7">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.8.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.8">_</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.9.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.9">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.10.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.10">𝑙</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.11.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.11">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.12.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.12">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.13.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.13">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.14.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.14">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.15.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.15">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.16.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.16">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.17.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.17">𝑖</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.18.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.18">𝑣</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.19.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.19">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.20.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1.1.20">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1c">return\_alternatives</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.1.m1.1d">italic_r italic_e italic_t italic_u italic_r italic_n _ italic_a italic_l italic_t italic_e italic_r italic_n italic_a italic_t italic_i italic_v italic_e italic_s</annotation></semantics></math> and <math alttext="num\_hypotheses" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">u</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.4.cmml">m</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.5" mathvariant="normal" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.5.cmml">_</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1c" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.6" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.6.cmml">h</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1d" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.7" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.7.cmml">y</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1e" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.8" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.8.cmml">p</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1f" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.9" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.9.cmml">o</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1g" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.10" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.10.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1h" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.11" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.11.cmml">h</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1i" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.12" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.12.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1j" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.13" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.13.cmml">s</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1k" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.14" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.14.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1l" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.15" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.15.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.3">𝑢</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.4">𝑚</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.5">_</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.6.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.6">ℎ</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.7.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.7">𝑦</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.8.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.8">𝑝</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.9.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.9">𝑜</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.10.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.10">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.11.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.11">ℎ</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.12.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.12">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.13.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.13">𝑠</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.14.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.14">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.15.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1.1.15">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1c">num\_hypotheses</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.2.m2.1d">italic_n italic_u italic_m _ italic_h italic_y italic_p italic_o italic_t italic_h italic_e italic_s italic_e italic_s</annotation></semantics></math> are essential for all our experiments; the former should be set to <math alttext="True" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">T</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.4.cmml">u</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.5" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.5.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝑇</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.3">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.4">𝑢</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1.1.5">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1c">True</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.3.m3.1d">italic_T italic_r italic_u italic_e</annotation></semantics></math> while the latter determines the number of returned alternatives. These decoding options can be used with regular beam search, prefix-constrained <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px3.p1.8.2">decoding</span>, and/or random sampling. If the decoding <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px3.p1.8.3">option</span> <math alttext="return\_alternatives" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.4.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.5" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.5.cmml">u</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1c" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.6" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.6.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1d" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.7" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.7.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1e" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.8" mathvariant="normal" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.8.cmml">_</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1f" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.9" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.9.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1g" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.10" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.10.cmml">l</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1h" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.11" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.11.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1i" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.12" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.12.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1j" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.13" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.13.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1k" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.14" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.14.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1l" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.15" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.15.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1m" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.16" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.16.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1n" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.17" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.17.cmml">i</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1o" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.18" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.18.cmml">v</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1p" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.19" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.19.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1q" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.20" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.20.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.2">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.3">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.4">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.5">𝑢</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.6.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.6">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.7.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.7">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.8.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.8">_</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.9.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.9">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.10.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.10">𝑙</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.11.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.11">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.12.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.12">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.13.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.13">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.14.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.14">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.15.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.15">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.16.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.16">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.17.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.17">𝑖</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.18.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.18">𝑣</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.19.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.19">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.20.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1.1.20">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1c">return\_alternatives</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.4.m4.1d">italic_r italic_e italic_t italic_u italic_r italic_n _ italic_a italic_l italic_t italic_e italic_r italic_n italic_a italic_t italic_i italic_v italic_e italic_s</annotation></semantics></math> is used along with <math alttext="target\_prefix" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.4.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.5" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.5.cmml">g</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1c" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.6" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.6.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1d" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.7" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.7.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1e" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.8" mathvariant="normal" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.8.cmml">_</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1f" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.9" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.9.cmml">p</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1g" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.10" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.10.cmml">r</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1h" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.11" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.11.cmml">e</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1i" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.12" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.12.cmml">f</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1j" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.13" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.13.cmml">i</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1k" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.14" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.14.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.2">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.3">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.4">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.5">𝑔</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.6.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.6">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.7.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.7">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.8.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.8">_</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.9.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.9">𝑝</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.10.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.10">𝑟</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.11.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.11">𝑒</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.12.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.12">𝑓</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.13.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.13">𝑖</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.14.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1.1.14">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1c">target\_prefix</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.5.m5.1d">italic_t italic_a italic_r italic_g italic_e italic_t _ italic_p italic_r italic_e italic_f italic_i italic_x</annotation></semantics></math>, the provided target left <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px3.p1.8.4">context</span> is fed into the decoder in the teacher forcing mode,<span class="ltx_note ltx_role_footnote" id="Ch3.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>In <em class="ltx_emph ltx_font_italic" id="Ch3.footnote7.1">teacher forcing</em> <cite class="ltx_cite ltx_citemacro_citep">(Williams and Zipser,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib201" title="">1989</a>)</cite>, ground truth previous tokens are fed into the decoder, instead of the predicted tokens y<sub class="ltx_sub" id="Ch3.footnote7.2">i-1</sub> as suggested by <cite class="ltx_cite ltx_citemacro_citet">Bahdanau et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib7" title="">2015</a>)</cite></span></span></span> then the engine expands the next <math alttext="N" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1a"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1b"><ci id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.6.m6.1d">italic_N</annotation></semantics></math> most likely words, and continues (auto-completes) the decoding for these <math alttext="N" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1a"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1b"><ci id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.7.m7.1d">italic_N</annotation></semantics></math> hypotheses independently. The shared task investigates four context cases: <span class="ltx_text" id="Ch3.S4.SS0.SSS0.Px3.p1.8.5">(a) empty</span> context, (b) right context only, (c) left context only, and (d) both the right and left contexts are provided. Hence, for all cases we returned multiple alternative translations, while for (c) and (d) we also returned another set of alternative auto-completions using the left context as a target prefix. In this sense, it is worth noting that we make use only of the left context, when available, and we do not use the right context at all, which we might investigate further in the future. To enhance the diversity of translations, especially for (a) and (b), we applied random sampling with the CTranslate2’s decoding option <math alttext="sampling\_topk" class="ltx_Math" display="inline" id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1"><semantics id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1a"><mrow id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.cmml"><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.2" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml">s</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.3" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml">a</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1a" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.4" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.4.cmml">m</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1b" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.5" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.5.cmml">p</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1c" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.6" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.6.cmml">l</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1d" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.7" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.7.cmml">i</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1e" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.8" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.8.cmml">n</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1f" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.9" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.9.cmml">g</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1g" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.10" mathvariant="normal" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.10.cmml">_</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1h" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.11" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.11.cmml">t</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1i" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.12" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.12.cmml">o</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1j" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.13" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.13.cmml">p</mi><mo id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1k" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml">⁢</mo><mi id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.14" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.14.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1b"><apply id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1"><times id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.1"></times><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.2">𝑠</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.3">𝑎</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.4.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.4">𝑚</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.5.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.5">𝑝</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.6.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.6">𝑙</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.7.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.7">𝑖</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.8.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.8">𝑛</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.9.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.9">𝑔</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.10.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.10">_</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.11.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.11">𝑡</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.12.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.12">𝑜</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.13.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.13">𝑝</ci><ci id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.14.cmml" xref="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1.1.14">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1c">sampling\_topk</annotation><annotation encoding="application/x-llamapun" id="Ch3.S4.SS0.SSS0.Px3.p1.8.m8.1d">italic_s italic_a italic_m italic_p italic_l italic_i italic_n italic_g _ italic_t italic_o italic_p italic_k</annotation></semantics></math>, with various sampling temperatures. Our experiments are further elaborated in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S5" title="3.5 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.5</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S6" title="3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.6</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch3.S4.SS0.SSS0.Px4">
<h6 class="ltx_title ltx_title_paragraph">Pinyin</h6>
<div class="ltx_para" id="Ch3.S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="Ch3.S4.SS0.SSS0.Px4.p1.1">The official Romanisation system for Standard Mandarin Chinese is called Pinyin. Since the task organisers used the pypinyin library<span class="ltx_note ltx_role_footnote" id="Ch3.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mozillazg/python-pinyin" title="">https://github.com/mozillazg/python-pinyin</a></span></span></span> to prepare the test files, we did too. OPUS English-to-Chinese models accept Chinese input, so we had to use the library to convert between the two writing systems. Since the conversion from Chinese characters to Pinyin is a lossy process and cannot be perfectly converted back, we keep a list of Chinese words resulted from tokenisation with Jieba to be able to map Pinyin tokens to Chinese tokens later.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch3.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.5     Method</h3>
<div class="ltx_para" id="Ch3.S5.p1">
<p class="ltx_p" id="Ch3.S5.p1.1">We experimented with both beam search alternatives and random sampling, and found that the latter achieves better results. This could be due to the fact that alternatives generated from each beam are usually very similar, and lower beam values tend to generate translations of lower quality. This section elaborates on the actual methods we used for our submissions, while more details about the initial experiments that led us to these decisions are explained in <span class="ltx_text" id="Ch3.S5.p1.1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S6" title="3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.6</span></a></span>.</p>
</div>
<figure class="ltx_table" id="Ch3.T1">
<table class="ltx_tabular ltx_align_middle" id="Ch3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="Ch3.T1.1.1.1.1">Language</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Ch3.T1.1.1.1.2">Settings</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Ch3.T1.1.1.1.3">Accuracy</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Ch3.T1.1.1.1.4">Human</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.2.2.1">de-en</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.2.2.2">ST=1.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.2.2.3">0.61444</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.2.2.4">0.885</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.3.3">
<td class="ltx_td" id="Ch3.T1.1.3.3.1"></td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.3.3.2">ST=1.3</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.3.3.3">0.60924</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.3.3.4">0.8875</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.4.4.1" rowspan="2"><span class="ltx_text" id="Ch3.T1.1.4.4.1.1">en-de</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.4.4.2">ST=1.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.4.4.3">0.58942</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.4.4.4">0.6725</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.5.5">
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.5.5.1">ST=1.3</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.5.5.2">0.58494</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.5.5.3">0.655</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.6.6.1" rowspan="4"><span class="ltx_text" id="Ch3.T1.1.6.6.1.1">zh-en</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.6.6.2">ST=1.0 + detok</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.6.6.3">0.50411</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.6.6.4">0.8675</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.7.7">
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.7.7.1">ST=1.3 + detok</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.7.7.2">0.50260</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.7.7.3">0.8675</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.8.8">
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.8.8.1">ST=1.0</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.8.8.2">0.49348</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.8.8.3">0.86</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.9.9">
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.9.9.1">ST=1.3</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.9.9.2">0.49062</td>
<td class="ltx_td ltx_align_left" id="Ch3.T1.1.9.9.3">0.87</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch3.T1.1.10.10.1" rowspan="2"><span class="ltx_text" id="Ch3.T1.1.10.10.1.1">en-zh</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.10.10.2">ST=1.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.10.10.3">0.31942</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch3.T1.1.10.10.4">0.5775</td>
</tr>
<tr class="ltx_tr" id="Ch3.T1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch3.T1.1.11.11.1">ST=1.3</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch3.T1.1.11.11.2">0.31935</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch3.T1.1.11.11.3">0.5725</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3.1: </span>Evaluation results on the test datasets. Automatic evaluation uses the “Accuracy” metric. “Human” refers to human evaluation. Results obtained from sampling temperature (ST) 1.0 are slightly better than those with the value 1.3. When the source is Chinese, detokenisation (detok) resulted in slightly better scores.</figcaption>
</figure>
<div class="ltx_para" id="Ch3.S5.p2">
<p class="ltx_p" id="Ch3.S5.p2.1">Random sampling is a decoding mode that randomly samples tokens from the model output distribution. In our experiments, we restrict the sampling to the top-10 candidates at each time-step. To obtain diverse generations from the MT model, we rely on randomness in the decoding method, in particular through top-K sampling that samples the next word from the top-K most probable choices <cite class="ltx_cite ltx_citemacro_citep">(Fan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib46" title="">2018</a>; Holtzman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib74" title="">2018</a>; Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib154" title="">2019</a>)</cite>, instead of aiming to decode text that maximises likelihood.</p>
</div>
<div class="ltx_para" id="Ch3.S5.p3">
<p class="ltx_p" id="Ch3.S5.p3.1">For each translation, we use the CTranlsate2 option <em class="ltx_emph ltx_font_italic" id="Ch3.S5.p3.1.1">return_alternatives</em> to return 10 sequences, with 10 top-K sampling. If the entry has a left context starting with a capital letter, we use the prefix to constrain the decoding. In CTranslate2, combining <em class="ltx_emph ltx_font_italic" id="Ch3.S5.p3.1.2">target_prefix</em> with the <em class="ltx_emph ltx_font_italic" id="Ch3.S5.p3.1.3">return_alternatives</em> flag returns alternative sequences just after the prefix. We compose a list of alternatives with and without the prefix, and try to find the word starting with the typed sequence.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>In a prefix-free target sequence, if multiple words start with the typed sequence, we return the first word. In practice, users could be prompted to choose from potential options.</span></span></span> If the word is not found, we repeat the same process for up to five runs. In each new run, random sampling can generate a new set of alternatives. Our experiments show that returning 20 sequences with 20 top-K sampling could lead to more correctly predicted words (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.T2" title="Table 3.2 ‣ 3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.2</span></a>); however, we had to consider the trade-off between quality and efficiency.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Our scripts are available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/WLAC" title="">https://github.com/ymoslem/WLAC</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch3.S5.p4">
<p class="ltx_p" id="Ch3.S5.p4.1">Furthermore, we investigate increasing the randomness of the generation by using a value for sampling temperature between 1.0 and 1.3. For each run, a random value is generated in this range. The default sampling temperature in CTranslate2 is 1, which achieved relatively better results, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.T1" title="Table 3.1 ‣ 3.5 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.1</span></a>.<span class="ltx_note ltx_role_footnote" id="Ch3.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>To measure the performance of the submitted systems, the organisers chose “accuracy” as the automatic evaluation metric and defined it as follows: <math alttext="ACC" class="ltx_Math" display="inline" id="Ch3.footnote11.m1.1"><semantics id="Ch3.footnote11.m1.1b"><mrow id="Ch3.footnote11.m1.1.1" xref="Ch3.footnote11.m1.1.1.cmml"><mi id="Ch3.footnote11.m1.1.1.2" xref="Ch3.footnote11.m1.1.1.2.cmml">A</mi><mo id="Ch3.footnote11.m1.1.1.1" xref="Ch3.footnote11.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.footnote11.m1.1.1.3" xref="Ch3.footnote11.m1.1.1.3.cmml">C</mi><mo id="Ch3.footnote11.m1.1.1.1b" xref="Ch3.footnote11.m1.1.1.1.cmml">⁢</mo><mi id="Ch3.footnote11.m1.1.1.4" xref="Ch3.footnote11.m1.1.1.4.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m1.1c"><apply id="Ch3.footnote11.m1.1.1.cmml" xref="Ch3.footnote11.m1.1.1"><times id="Ch3.footnote11.m1.1.1.1.cmml" xref="Ch3.footnote11.m1.1.1.1"></times><ci id="Ch3.footnote11.m1.1.1.2.cmml" xref="Ch3.footnote11.m1.1.1.2">𝐴</ci><ci id="Ch3.footnote11.m1.1.1.3.cmml" xref="Ch3.footnote11.m1.1.1.3">𝐶</ci><ci id="Ch3.footnote11.m1.1.1.4.cmml" xref="Ch3.footnote11.m1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m1.1d">ACC</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m1.1e">italic_A italic_C italic_C</annotation></semantics></math> <math alttext="=" class="ltx_Math" display="inline" id="Ch3.footnote11.m2.1"><semantics id="Ch3.footnote11.m2.1b"><mo id="Ch3.footnote11.m2.1.1" xref="Ch3.footnote11.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m2.1c"><eq id="Ch3.footnote11.m2.1.1.cmml" xref="Ch3.footnote11.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m2.1d">=</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m2.1e">=</annotation></semantics></math> <math alttext="N_{match}" class="ltx_Math" display="inline" id="Ch3.footnote11.m3.1"><semantics id="Ch3.footnote11.m3.1b"><msub id="Ch3.footnote11.m3.1.1" xref="Ch3.footnote11.m3.1.1.cmml"><mi id="Ch3.footnote11.m3.1.1.2" xref="Ch3.footnote11.m3.1.1.2.cmml">N</mi><mrow id="Ch3.footnote11.m3.1.1.3" xref="Ch3.footnote11.m3.1.1.3.cmml"><mi id="Ch3.footnote11.m3.1.1.3.2" xref="Ch3.footnote11.m3.1.1.3.2.cmml">m</mi><mo id="Ch3.footnote11.m3.1.1.3.1" xref="Ch3.footnote11.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m3.1.1.3.3" xref="Ch3.footnote11.m3.1.1.3.3.cmml">a</mi><mo id="Ch3.footnote11.m3.1.1.3.1b" xref="Ch3.footnote11.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m3.1.1.3.4" xref="Ch3.footnote11.m3.1.1.3.4.cmml">t</mi><mo id="Ch3.footnote11.m3.1.1.3.1c" xref="Ch3.footnote11.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m3.1.1.3.5" xref="Ch3.footnote11.m3.1.1.3.5.cmml">c</mi><mo id="Ch3.footnote11.m3.1.1.3.1d" xref="Ch3.footnote11.m3.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m3.1.1.3.6" xref="Ch3.footnote11.m3.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m3.1c"><apply id="Ch3.footnote11.m3.1.1.cmml" xref="Ch3.footnote11.m3.1.1"><csymbol cd="ambiguous" id="Ch3.footnote11.m3.1.1.1.cmml" xref="Ch3.footnote11.m3.1.1">subscript</csymbol><ci id="Ch3.footnote11.m3.1.1.2.cmml" xref="Ch3.footnote11.m3.1.1.2">𝑁</ci><apply id="Ch3.footnote11.m3.1.1.3.cmml" xref="Ch3.footnote11.m3.1.1.3"><times id="Ch3.footnote11.m3.1.1.3.1.cmml" xref="Ch3.footnote11.m3.1.1.3.1"></times><ci id="Ch3.footnote11.m3.1.1.3.2.cmml" xref="Ch3.footnote11.m3.1.1.3.2">𝑚</ci><ci id="Ch3.footnote11.m3.1.1.3.3.cmml" xref="Ch3.footnote11.m3.1.1.3.3">𝑎</ci><ci id="Ch3.footnote11.m3.1.1.3.4.cmml" xref="Ch3.footnote11.m3.1.1.3.4">𝑡</ci><ci id="Ch3.footnote11.m3.1.1.3.5.cmml" xref="Ch3.footnote11.m3.1.1.3.5">𝑐</ci><ci id="Ch3.footnote11.m3.1.1.3.6.cmml" xref="Ch3.footnote11.m3.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m3.1d">N_{match}</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m3.1e">italic_N start_POSTSUBSCRIPT italic_m italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="/" class="ltx_Math" display="inline" id="Ch3.footnote11.m4.1"><semantics id="Ch3.footnote11.m4.1b"><mo id="Ch3.footnote11.m4.1.1" xref="Ch3.footnote11.m4.1.1.cmml">/</mo><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m4.1c"><divide id="Ch3.footnote11.m4.1.1.cmml" xref="Ch3.footnote11.m4.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m4.1d">/</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m4.1e">/</annotation></semantics></math> <math alttext="N_{all}" class="ltx_Math" display="inline" id="Ch3.footnote11.m5.1"><semantics id="Ch3.footnote11.m5.1b"><msub id="Ch3.footnote11.m5.1.1" xref="Ch3.footnote11.m5.1.1.cmml"><mi id="Ch3.footnote11.m5.1.1.2" xref="Ch3.footnote11.m5.1.1.2.cmml">N</mi><mrow id="Ch3.footnote11.m5.1.1.3" xref="Ch3.footnote11.m5.1.1.3.cmml"><mi id="Ch3.footnote11.m5.1.1.3.2" xref="Ch3.footnote11.m5.1.1.3.2.cmml">a</mi><mo id="Ch3.footnote11.m5.1.1.3.1" xref="Ch3.footnote11.m5.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m5.1.1.3.3" xref="Ch3.footnote11.m5.1.1.3.3.cmml">l</mi><mo id="Ch3.footnote11.m5.1.1.3.1b" xref="Ch3.footnote11.m5.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m5.1.1.3.4" xref="Ch3.footnote11.m5.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m5.1c"><apply id="Ch3.footnote11.m5.1.1.cmml" xref="Ch3.footnote11.m5.1.1"><csymbol cd="ambiguous" id="Ch3.footnote11.m5.1.1.1.cmml" xref="Ch3.footnote11.m5.1.1">subscript</csymbol><ci id="Ch3.footnote11.m5.1.1.2.cmml" xref="Ch3.footnote11.m5.1.1.2">𝑁</ci><apply id="Ch3.footnote11.m5.1.1.3.cmml" xref="Ch3.footnote11.m5.1.1.3"><times id="Ch3.footnote11.m5.1.1.3.1.cmml" xref="Ch3.footnote11.m5.1.1.3.1"></times><ci id="Ch3.footnote11.m5.1.1.3.2.cmml" xref="Ch3.footnote11.m5.1.1.3.2">𝑎</ci><ci id="Ch3.footnote11.m5.1.1.3.3.cmml" xref="Ch3.footnote11.m5.1.1.3.3">𝑙</ci><ci id="Ch3.footnote11.m5.1.1.3.4.cmml" xref="Ch3.footnote11.m5.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m5.1d">N_{all}</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m5.1e">italic_N start_POSTSUBSCRIPT italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext="N_{match}" class="ltx_Math" display="inline" id="Ch3.footnote11.m6.1"><semantics id="Ch3.footnote11.m6.1b"><msub id="Ch3.footnote11.m6.1.1" xref="Ch3.footnote11.m6.1.1.cmml"><mi id="Ch3.footnote11.m6.1.1.2" xref="Ch3.footnote11.m6.1.1.2.cmml">N</mi><mrow id="Ch3.footnote11.m6.1.1.3" xref="Ch3.footnote11.m6.1.1.3.cmml"><mi id="Ch3.footnote11.m6.1.1.3.2" xref="Ch3.footnote11.m6.1.1.3.2.cmml">m</mi><mo id="Ch3.footnote11.m6.1.1.3.1" xref="Ch3.footnote11.m6.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m6.1.1.3.3" xref="Ch3.footnote11.m6.1.1.3.3.cmml">a</mi><mo id="Ch3.footnote11.m6.1.1.3.1b" xref="Ch3.footnote11.m6.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m6.1.1.3.4" xref="Ch3.footnote11.m6.1.1.3.4.cmml">t</mi><mo id="Ch3.footnote11.m6.1.1.3.1c" xref="Ch3.footnote11.m6.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m6.1.1.3.5" xref="Ch3.footnote11.m6.1.1.3.5.cmml">c</mi><mo id="Ch3.footnote11.m6.1.1.3.1d" xref="Ch3.footnote11.m6.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m6.1.1.3.6" xref="Ch3.footnote11.m6.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m6.1c"><apply id="Ch3.footnote11.m6.1.1.cmml" xref="Ch3.footnote11.m6.1.1"><csymbol cd="ambiguous" id="Ch3.footnote11.m6.1.1.1.cmml" xref="Ch3.footnote11.m6.1.1">subscript</csymbol><ci id="Ch3.footnote11.m6.1.1.2.cmml" xref="Ch3.footnote11.m6.1.1.2">𝑁</ci><apply id="Ch3.footnote11.m6.1.1.3.cmml" xref="Ch3.footnote11.m6.1.1.3"><times id="Ch3.footnote11.m6.1.1.3.1.cmml" xref="Ch3.footnote11.m6.1.1.3.1"></times><ci id="Ch3.footnote11.m6.1.1.3.2.cmml" xref="Ch3.footnote11.m6.1.1.3.2">𝑚</ci><ci id="Ch3.footnote11.m6.1.1.3.3.cmml" xref="Ch3.footnote11.m6.1.1.3.3">𝑎</ci><ci id="Ch3.footnote11.m6.1.1.3.4.cmml" xref="Ch3.footnote11.m6.1.1.3.4">𝑡</ci><ci id="Ch3.footnote11.m6.1.1.3.5.cmml" xref="Ch3.footnote11.m6.1.1.3.5">𝑐</ci><ci id="Ch3.footnote11.m6.1.1.3.6.cmml" xref="Ch3.footnote11.m6.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m6.1d">N_{match}</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m6.1e">italic_N start_POSTSUBSCRIPT italic_m italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT</annotation></semantics></math> is the number of correct predicted words and <math alttext="N_{all}" class="ltx_Math" display="inline" id="Ch3.footnote11.m7.1"><semantics id="Ch3.footnote11.m7.1b"><msub id="Ch3.footnote11.m7.1.1" xref="Ch3.footnote11.m7.1.1.cmml"><mi id="Ch3.footnote11.m7.1.1.2" xref="Ch3.footnote11.m7.1.1.2.cmml">N</mi><mrow id="Ch3.footnote11.m7.1.1.3" xref="Ch3.footnote11.m7.1.1.3.cmml"><mi id="Ch3.footnote11.m7.1.1.3.2" xref="Ch3.footnote11.m7.1.1.3.2.cmml">a</mi><mo id="Ch3.footnote11.m7.1.1.3.1" xref="Ch3.footnote11.m7.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m7.1.1.3.3" xref="Ch3.footnote11.m7.1.1.3.3.cmml">l</mi><mo id="Ch3.footnote11.m7.1.1.3.1b" xref="Ch3.footnote11.m7.1.1.3.1.cmml">⁢</mo><mi id="Ch3.footnote11.m7.1.1.3.4" xref="Ch3.footnote11.m7.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Ch3.footnote11.m7.1c"><apply id="Ch3.footnote11.m7.1.1.cmml" xref="Ch3.footnote11.m7.1.1"><csymbol cd="ambiguous" id="Ch3.footnote11.m7.1.1.1.cmml" xref="Ch3.footnote11.m7.1.1">subscript</csymbol><ci id="Ch3.footnote11.m7.1.1.2.cmml" xref="Ch3.footnote11.m7.1.1.2">𝑁</ci><apply id="Ch3.footnote11.m7.1.1.3.cmml" xref="Ch3.footnote11.m7.1.1.3"><times id="Ch3.footnote11.m7.1.1.3.1.cmml" xref="Ch3.footnote11.m7.1.1.3.1"></times><ci id="Ch3.footnote11.m7.1.1.3.2.cmml" xref="Ch3.footnote11.m7.1.1.3.2">𝑎</ci><ci id="Ch3.footnote11.m7.1.1.3.3.cmml" xref="Ch3.footnote11.m7.1.1.3.3">𝑙</ci><ci id="Ch3.footnote11.m7.1.1.3.4.cmml" xref="Ch3.footnote11.m7.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch3.footnote11.m7.1d">N_{all}</annotation><annotation encoding="application/x-llamapun" id="Ch3.footnote11.m7.1e">italic_N start_POSTSUBSCRIPT italic_a italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is the number of <em class="ltx_emph ltx_font_italic" id="Ch3.footnote11.1">all</em> test examples <cite class="ltx_cite ltx_citemacro_citep">(Casacuberta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib21" title="">2022</a>)</cite>.</span></span></span></p>
</div>
</section>
<section class="ltx_section" id="Ch3.S6">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.6     Other Experiments</h3>
<div class="ltx_para" id="Ch3.S6.p1">
<p class="ltx_p" id="Ch3.S6.p1.1">This section elaborates on some initial experiments we conducted to decide what approach to use. The final approach we actually used in our submissions is explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.S5" title="3.5 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
<div class="ltx_para" id="Ch3.S6.p2">
<p class="ltx_p" id="Ch3.S6.p2.1">We used 10,000 entries of a Chinese-to-English golden sample provided by the organisers to evaluate various experiments. For sentence translation, when there is no left context, we experimented with the following values:</p>
<ul class="ltx_itemize" id="Ch3.S6.I1">
<li class="ltx_item" id="Ch3.S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S6.I1.i1.p1">
<p class="ltx_p" id="Ch3.S6.I1.i1.p1.1">beam size 1, 5, and 10, without sampling</p>
</div>
</li>
<li class="ltx_item" id="Ch3.S6.I1.i2" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch3.S6.I1.i2.p1">
<p class="ltx_p" id="Ch3.S6.I1.i2.p1.1">beam size 1, with random sampling top-K 10, 20, and 50</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ch3.S6.p3">
<p class="ltx_p" id="Ch3.S6.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3.T2" title="Table 3.2 ‣ 3.6 Other Experiments ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3.2</span></a> shows the results for these experiments, and demonstrates that random sampling achieves the best overall accuracy. Random sampling with beam size 1 reveals better results than mere beam size 1 and even beam sizes 5 and 10 without random sampling. Multiple runs of random sampling can result in more correctly predicted words.</p>
</div>
<figure class="ltx_table" id="Ch3.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch3.T2.1.1.1.1">Beam Size</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch3.T2.1.1.1.2">Sampling Top-K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch3.T2.1.1.1.3">Hypotheses</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch3.T2.1.1.1.4">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch3.T2.1.1.1.5">Runs</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.1.2.2.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.2.2.2">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.2.2.3">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.2.2.4">0.6519</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.2.2.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.3.3.1">5</th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.3.3.2">N/A</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.3.3.3">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.3.3.4">0.6588</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.3.3.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.4.4.1">10</th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.4.4.2">N/A</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.4.4.3">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.4.4.4">0.6573</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.4.4.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.1.5.5.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.5.5.2">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.5.5.3">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.5.5.4">0.6918</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.5.5.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.6.6.1">1</th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.6.6.2">20</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.6.6.3">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.6.6.4">0.6907</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.6.6.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.7.7.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.7.7.1.1">1</span></th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.7.7.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.7.7.2.1">20</span></td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.7.7.3"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.7.7.3.1">20</span></td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.7.7.4"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.7.7.4.1">0.7108</span></td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.7.7.5"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.7.7.5.1">1</span></td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.8.8.1">1</th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.8.8.2">50</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.8.8.3">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.8.8.4">0.6853</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.8.8.5">1</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch3.T2.1.9.9.1">5</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.9.9.2">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.9.9.3">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.9.9.4">0.6588</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch3.T2.1.9.9.5">5</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch3.T2.1.10.10.1">1</th>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.10.10.2">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.10.10.3">10</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.10.10.4">0.7165</td>
<td class="ltx_td ltx_align_center" id="Ch3.T2.1.10.10.5">5</td>
</tr>
<tr class="ltx_tr" id="Ch3.T2.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="Ch3.T2.1.11.11.1"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.11.11.1.1">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch3.T2.1.11.11.2"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.11.11.2.1">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch3.T2.1.11.11.3"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.11.11.3.1">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch3.T2.1.11.11.4"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.11.11.4.1">0.7310</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch3.T2.1.11.11.5"><span class="ltx_text ltx_font_bold" id="Ch3.T2.1.11.11.5.1">5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3.2: </span>Results for the Chinese-to-English golden sample dataset (10,000 entries). Random sampling outperforms even higher beam sizes.</figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch3.S7">
<h3 class="ltx_title ltx_font_bold ltx_title_section">3.7     Conclusion</h3>
<div class="ltx_para" id="Ch3.S7.p1">
<p class="ltx_p" id="Ch3.S7.p1.1">Random sampling is a decoding mode used for sequence generation. Instead of always selecting the most probable next word or token at each step, the model samples from the probability distribution over the vocabulary. In our experiments, we employed top-K sampling to obtain diverse generations from the MT model. In other words, the next word was sampled from the top-K most probable choices, given the typed context. We also investigated increasing the randomness of the generation using different values of sampling temperature. Our approach led to excellent results based on both automatic and human evaluation, across three language pairs.</p>
</div>
</section>
<section class="ltx_chapter" id="Ch4">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 4      Adaptive Machine Translation with Large Language Models</h2>
<div class="ltx_para" id="Ch4.p1">
<p class="ltx_p" id="Ch4.p1.1">Yasmin Moslem, Rejwanul Haque, John Kelleher, and Andy Way</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch4.p2">
<p class="ltx_p" id="Ch4.p2.1">In Proceedings of the 24th Annual Conference of the European Association for Machine Translation (EAMT 2023), Research: Technical, pages 227–237, Tampere, Finland. Association for Machine Translation in the Americas.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Published at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.eamt-1.22/" title="">https://aclanthology.org/2023.eamt-1.22/</a></span></span></span></p>
</div>
</section>
<div class="ltx_abstract">
<span class="ltx_ERROR undefined" id="id14.id1">\nohyphens</span>
<p class="ltx_p" id="id15.id2">Consistency is a key requirement of high-quality <span class="ltx_text" id="id15.id2.1">translation</span>. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, <span class="ltx_text" id="id15.id2.2">real-time</span> adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilise in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, LLMs can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).</p>
</div>
<section class="ltx_section" id="Ch4.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.1     Context</h3>
<div class="ltx_para" id="Ch4.S1.p1">
<p class="ltx_p" id="Ch4.S1.p1.1">As explained in the previous chapters, the release of LLMs such as BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite> (open-sourced), GPT-{3.5,4} <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Ouyang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib139" title="">2022</a>)</cite> (via a commercial API), and PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite> (limited access) has paved the way for new approaches that utilise their in-context learning capabilities <cite class="ltx_cite ltx_citemacro_citep">(Dong et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib40" title="">2022</a>)</cite>. By the end of 2022, many researchers started investigating the use of LLMs for all NLP areas. While this paper was a natural extension of my previous work, this research was among the earliest to conduct an extensive investigation into the employment of LLMs for MT in general and for adaptive MT in particular, and to compare the performance of a wide range of both open-source and commercial systems, across five language pairs. This paper first appeared as a preprint in January 2022, and then was peer-reviewed and published at EAMT 2022. By the time of writing this thesis, the paper is already well-cited in public literature. The following sections elaborate on the contribution of this work.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.2     Introduction</h3>
<div class="ltx_para" id="Ch4.S2.p1">
<p class="ltx_p" id="Ch4.S2.p1.1">Adaptive MT is a type of machine translation that utilises feedback from users to improve the quality of the translations over time. Feedback usually includes corrections to previous translations, terminology and style guides, as well as ratings of the quality of the translations. This can be particularly useful for domain-specific scenarios, where baseline MT systems may have insufficient relevant data to accurately translate certain terms or phrases. There are still several challenges to effectively incorporate user feedback into the translation process, especially at inference time. In this work, we use a relatively wide definition of adaptive MT to refer to learning from similar translations (fuzzy matches) found in approved translation memories (TMs) on the fly <cite class="ltx_cite ltx_citemacro_citep">(Farajian et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib47" title="">2017</a>; Wuebker et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib203" title="">2018</a>; Peris and Casacuberta,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib144" title="">2019</a>; Etchegoyhen et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib43" title="">2021</a>)</cite>, as well as real-time terminology-constrained MT <cite class="ltx_cite ltx_citemacro_citep">(Hokamp and Liu,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib72" title="">2017</a>; Post and Vilar,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib152" title="">2018</a>; Dinu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib39" title="">2019</a>; Michon et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib121" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch4.S2.p2">
<p class="ltx_p" id="Ch4.S2.p2.1">Autoregressive decoder-only LLMs, such as GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Ouyang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib139" title="">2022</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite>, and Llama <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib186" title="">Touvron et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib187" title="">Touvron et al., 2023b, </a>)</cite> are trained to predict the next word given the previous context. During unsupervised pre-training, a language model develops a broad set of pattern recognition abilities. It then uses these abilities at inference time to rapidly recognise and adapt to the desired task. In their experiments, <cite class="ltx_cite ltx_citemacro_citet">Brown et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite> use the term “in-context learning” to describe a scenario where a pre-trained language model at inference time learns to replicate certain input-output text generation patterns without further fine-tuning. They show that autoregressive LLMs such as GPT-3 can perform well on diverse tasks, through zero-shot, one-shot, and few-shot in-context learning without weight updates. Instead of asking the model to directly perform a given task, the input can be augmented with relevant examples, which help the model adapt its output. The key idea of in-context learning is to learn from analogy. The model is expected to learn the pattern hidden in the demonstration and accordingly make better predictions <cite class="ltx_cite ltx_citemacro_citep">(Dong et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib40" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch4.S2.p3">
<p class="ltx_p" id="Ch4.S2.p3.1">Previous researchers investigated using neural language models for MT through few-shot in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Vilar et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib192" title="">2023</a>)</cite> and even in zero-shot settings <cite class="ltx_cite ltx_citemacro_citep">(Wang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib197" title="">2021</a>)</cite>. Other researchers proposed using LLMs for generating synthetic domain-specific data for MT domain adaptation <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>. Recently, researchers <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib211" title="">Zhang et al., 2023a, </a>)</cite> confirmed the importance of in-context example selection for the quality of MT with LLMs.</p>
</div>
<figure class="ltx_figure" id="Ch4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="235" id="Ch4.F1.g1" src="extracted/5369614/img/context-avg.png" width="471"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4.1: </span>Evaluation results for GPT-3.5 zero-shot, and few-shot translation with random context or fuzzy matches. Average scores across EN-AR, EN-ES, EN-FR, and EN-ZH language pairs. While using a random context outperforms zero-shot translation, using fuzzy matches reveals the best results.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S2.p4">
<p class="ltx_p" id="Ch4.S2.p4.1">The main contribution of this paper is investigating the capabilities of LLMs such as <span class="ltx_text" id="Ch4.S2.p4.1.1">GPT-3.5</span>, GPT-4 (including ChatGPT), and BLOOM for real-time adaptive MT through in-context learning. As illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.F1" title="Figure 4.1 ‣ 4.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.1</span></a>, such LLMs can achieve better translation quality through adapting its output to adhere to the terminology and style used in previously approved translation pairs. In particular, we would like to understand the quality with which such models can perform the following tasks, without any further training:</p>
</div>
<div class="ltx_para" id="Ch4.S2.p5">
<ul class="ltx_itemize" id="Ch4.S2.I1">
<li class="ltx_item" id="Ch4.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i1.p1">
<p class="ltx_p" id="Ch4.S2.I1.i1.p1.1">Adapting new translations to match the terminology and style of previously approved TM fuzzy matches, at inference time;</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i2" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i2.p1">
<p class="ltx_p" id="Ch4.S2.I1.i2.p1.1">Matching or outperforming the quality of translations generated by encoder-decoder MT models across a number of languages;</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i3" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i3.p1">
<p class="ltx_p" id="Ch4.S2.I1.i3.p1.1">Fixing translations from stronger encoder-decoder MT systems using fuzzy matches, which is especially useful for low-resource languages; and</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S2.I1.i4" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S2.I1.i4.p1">
<p class="ltx_p" id="Ch4.S2.I1.i4.p1.1">Terminology-constrained MT, by first defining terminology in the relevant sentences or dataset, and then forcing new translations to use these terms.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch4.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.3     Experimental Setup</h3>
<div class="ltx_para" id="Ch4.S3.p1">
<p class="ltx_p" id="Ch4.S3.p1.1">In all our experiments, we use GPT-3.5 <em class="ltx_emph ltx_font_italic" id="Ch4.S3.p1.1.1">text-davinci-003</em> model via its official API.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/api/" title="">https://openai.com/api/</a></span></span></span> For parameters, we use <span class="ltx_text" id="Ch4.S3.p1.1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S3.p1.1.2.1">top-p</em> 1</span>, with <em class="ltx_emph ltx_font_italic" id="Ch4.S3.p1.1.3">temperature</em> 0.3 for the three translation tasks, and 0 for the terminology extraction task.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>To avoid over-generation, the option <em class="ltx_emph ltx_font_italic" id="Ch4.footnote3.1">stop</em> can be set to [‘\n’]. However, if a new line is generated by the model before the translation, this might result in not generating a translation. Alternatively, over-generation can be manually handled.</span></span></span> For the maximum length of tokens, we observe that French and Spanish tokens can be 3–4 times the number of English source words, while other languages can be longer. Hence, we roughly choose a length multiplier value, which we set to 8 for Arabic, 5 for Chinese and Kinyarwanda, and 4 for French and Spanish. We used batch requests with a batch size of 20 segments.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For higher values of few-shot translation into Arabic using <em class="ltx_emph ltx_font_italic" id="Ch4.footnote4.1">text-davinci-003</em>, we had to decrease the batch size to avoid exceeding the tokens-per-minute limit.</span></span></span> Our scripts are publicly <span class="ltx_text" id="Ch4.S3.p1.1.4">available.</span><span class="ltx_note ltx_role_footnote" id="Ch4.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/Adaptive-MT-LLM" title="">https://github.com/ymoslem/Adaptive-MT-LLM</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch4.S3.p2">
<p class="ltx_p" id="Ch4.S3.p2.1">As we aim to simulate a document-level scenario where translators are required to adhere to a project’s or client’s TM, we use the domain-specific dataset, TICO-19 <cite class="ltx_cite ltx_citemacro_citep">(Anastasopoulos et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib4" title="">2020</a>)</cite>, which includes 3070 unique segments. From now on, we will refer to it as the “context dataset”. We focus on a range of languages with diverse scripts and amounts of resources, namely English as the source language, and Arabic, Chinese, French, Kinyarwanda, and Spanish as the <span class="ltx_text" id="Ch4.S3.p2.1.1">target</span> languages.</p>
</div>
<figure class="ltx_table" id="Ch4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch4.T1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Ch4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.2.1">Context</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.3.1">spBLEU ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.4.1">chrF++ ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.5.1">TER ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.1.1.6.1">COMET ↑</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.2.2.1" rowspan="8"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.2.2.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.2.2.2">zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.2.2.3">27.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.2.2.4">48.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.2.2.5">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.2.2.6">41.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.3.3.1">random 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.3.3.2">28.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.3.3.3">49.35</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.3.3.4">70.55</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.3.3.5">43.32</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.4.4.1">fuzzy 1-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.4.4.2">36.38</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.4.4.3">55.08</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.4.4.4">63.99</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.4.4.5">55.1</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.5.5.1">fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.5.5.2">38.41</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.5.5.3">56.57</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.5.5.4">62.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.5.5.5">57.36</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.6.6.1">fuzzy 3-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.6.6.2">39.75</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.6.6.3">57.52</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.6.6.4">61.12</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.6.6.5">59.68</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.7.7.1">fuzzy 4-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.7.7.2">40.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.7.7.3">58.27</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.7.7.4">60.39</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.7.7.5">62.16</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.8.8.1">fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.8.8.2">41.33</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.8.8.3">58.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.8.8.4">59.95</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.8.8.5">62.65</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.9.9.1">fuzzy 7-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.9.9.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.9.9.2.1">41.81</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.9.9.3.1">59.1</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.9.9.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.9.9.4.1">59.38</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.9.9.5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.9.9.5.1">64.01</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.10.10.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.10.10.1.1">EN-ES</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.10.10.2">zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.10.10.3">53.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.10.10.4">72.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.10.10.5">36.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.10.10.6">84.0</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.11.11.1">random 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.11.11.2">54.78</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.11.11.3">73.12</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.11.11.4">36.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.11.11.5">85.25</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.12.12.1">fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.12.12.2">59.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.12.12.3">75.83</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.12.12.4">32.56</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.12.12.5">90.37</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.13.13.1">fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.13.13.2">61.24</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.13.13.3">76.73</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.13.13.4">31.32</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.13.13.5">91.51</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.14.14.1">fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.14.14.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.14.14.2.1">61.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.14.14.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.14.14.3.1">77.05</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.14.14.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.14.14.4.1">30.9</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.14.14.5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.14.14.5.1">92.0</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.15.15.1" rowspan="8"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.15.15.1.1">EN-FR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.15.15.2">zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.15.15.3">44.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.15.15.4">65.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.15.15.5">50.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.15.15.6">58.67</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.16.16.1">random 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.16.16.2">45.91</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.16.16.3">65.4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.16.16.4">49.92</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.16.16.5">57.6</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.17.17.1">fuzzy 1-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.17.17.2">48.39</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.17.17.3">66.58</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.17.17.4">48.18</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.17.17.5">59.49</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.18.18.1">fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.18.18.2">49.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.18.18.3">67.41</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.18.18.4">46.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.18.18.5">61.38</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.19.19.1">fuzzy 3-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.19.19.2">50.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.19.19.3">68.06</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.19.19.4">45.85</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.19.19.5">61.97</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.20.20.1">fuzzy 4-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.20.20.2">51.89</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.20.20.3">68.5</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.20.20.4">44.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.20.20.5">62.7</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.21.21.1">fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.21.21.2">51.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.21.21.3">68.43</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.21.21.4">45.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.21.21.5">62.81</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.22.22.1">fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.22.22.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.22.22.2.1">53.72</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.22.22.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.22.22.3.1">69.39</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.22.22.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.22.22.4.1">43.82</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.22.22.5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.22.22.5.1">63.57</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.23.23.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.23.23.1.1">EN-RW</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.23.23.2">zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.23.23.3">2.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.23.23.4">22.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.23.23.5">143.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.23.23.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.24.24.1">random 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.24.24.2">3.8</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.24.24.3">25.19</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.24.24.4">129.88</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.24.24.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.25.25.1">fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.25.25.2">12.23</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.25.25.3">36.66</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.25.25.4">105.54</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.25.25.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.26.26.1">fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.26.26.2">14.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.26.26.3">39.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.26.26.4">100.11</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.26.26.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.27.27.1">fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.27.27.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.27.27.2.1">17.87</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.27.27.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.27.27.3.1">41.44</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.27.27.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.27.27.4.1">92.84</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.27.27.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.28.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ch4.T1.1.28.28.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.28.28.1.1">EN-ZH</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T1.1.28.28.2">zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.28.28.3">32.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.28.28.4">40.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.28.28.5">99.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T1.1.28.28.6">59.87</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.29.29">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.29.29.1">random 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.29.29.2">38.72</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.29.29.3">44.06</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.29.29.4">87.56</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.29.29.5">68.39</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.30.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.30.30.1">fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.30.30.2">46.18</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.30.30.3">49.12</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.30.30.4">69.0</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.30.30.5">73.9</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.31.31">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T1.1.31.31.1">fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.31.31.2">47.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.31.31.3">50.28</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.31.31.4">64.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T1.1.31.31.5">74.86</td>
</tr>
<tr class="ltx_tr" id="Ch4.T1.1.32.32">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch4.T1.1.32.32.1">fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T1.1.32.32.2"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.32.32.2.1">49.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T1.1.32.32.3"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.32.32.3.1">51.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T1.1.32.32.4"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.32.32.4.1">63.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T1.1.32.32.5"><span class="ltx_text ltx_font_bold" id="Ch4.T1.1.32.32.5.1">75.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.1: </span>Adaptive MT with fuzzy matches for GPT-3.5 few-shot in-context learning outperforms using random sentence pairs as context examples. Increasing the number of fuzzy matches can improve the translation quality further. The table shows consistent results for EN-AR, EN-ES, EN-FR, EN-RW, and EN-ZH language pairs.</figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch4.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.4     Adaptive MT with Fuzzy Matches</h3>
<div class="ltx_para" id="Ch4.S4.p1">
<p class="ltx_p" id="Ch4.S4.p1.1">In translation environments, similar approved translated segments are usually referred to as “fuzzy matches”, and are stored in parallel datasets, known as translation memories (TMs).<span class="ltx_note ltx_role_footnote" id="Ch4.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Segments stored in a TM can be smaller than a full sentence (e.g. a title) or larger. However, as most segments in a TM are supposed to be sentence pairs, we use the two words interchangeably throughout the paper.</span></span></span> Researchers have investigated the possibilities of improving MT quality and consistency with fuzzy matches <cite class="ltx_cite ltx_citemacro_citep">(Knowles et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib94" title="">2018</a>; Bulte and Tezcan,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib18" title="">2019</a>; Xu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib205" title="">2020</a>)</cite>. Incorporating fuzzy matches into the MT process can help the system generate more accurate translations, and try to ensure adherence to pre-approved terminology and preferred style requirements.</p>
</div>
<div class="ltx_para" id="Ch4.S4.p2">
<p class="ltx_p" id="Ch4.S4.p2.1">In this set of experiments, we investigate the possibility of forcing the translation of a new sentence pair to adapt to fuzzy matches in the <span class="ltx_text" id="Ch4.S4.p2.1.1">context</span> dataset. To extract fuzzy matches, we use embedding similarity-based retrieval. Previous researchers have shown that approaches that depend on embeddings to retrieve fuzzy matches can outperform those that use Edit Distance <cite class="ltx_cite ltx_citemacro_citep">(Hosseini et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib76" title="">2020</a>; Pham et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib147" title="">2020</a>)</cite>. To this end, we employ the paraphrase mining module from the Sentence-Transformers library <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib159" title="">2019</a>)</cite>. We use the <span class="ltx_text" id="Ch4.S4.p2.1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.p2.1.2.1">all-MiniLM-L6-v2</em></span> model because of its high accuracy and efficiency.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sbert.net/" title="">https://www.sbert.net/</a></span></span></span> For each sentence, we retrieve up to <math alttext="top\_k" class="ltx_Math" display="inline" id="Ch4.S4.p2.1.m1.1"><semantics id="Ch4.S4.p2.1.m1.1a"><mrow id="Ch4.S4.p2.1.m1.1.1" xref="Ch4.S4.p2.1.m1.1.1.cmml"><mi id="Ch4.S4.p2.1.m1.1.1.2" xref="Ch4.S4.p2.1.m1.1.1.2.cmml">t</mi><mo id="Ch4.S4.p2.1.m1.1.1.1" xref="Ch4.S4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S4.p2.1.m1.1.1.3" xref="Ch4.S4.p2.1.m1.1.1.3.cmml">o</mi><mo id="Ch4.S4.p2.1.m1.1.1.1a" xref="Ch4.S4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S4.p2.1.m1.1.1.4" xref="Ch4.S4.p2.1.m1.1.1.4.cmml">p</mi><mo id="Ch4.S4.p2.1.m1.1.1.1b" xref="Ch4.S4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S4.p2.1.m1.1.1.5" mathvariant="normal" xref="Ch4.S4.p2.1.m1.1.1.5.cmml">_</mi><mo id="Ch4.S4.p2.1.m1.1.1.1c" xref="Ch4.S4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S4.p2.1.m1.1.1.6" xref="Ch4.S4.p2.1.m1.1.1.6.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch4.S4.p2.1.m1.1b"><apply id="Ch4.S4.p2.1.m1.1.1.cmml" xref="Ch4.S4.p2.1.m1.1.1"><times id="Ch4.S4.p2.1.m1.1.1.1.cmml" xref="Ch4.S4.p2.1.m1.1.1.1"></times><ci id="Ch4.S4.p2.1.m1.1.1.2.cmml" xref="Ch4.S4.p2.1.m1.1.1.2">𝑡</ci><ci id="Ch4.S4.p2.1.m1.1.1.3.cmml" xref="Ch4.S4.p2.1.m1.1.1.3">𝑜</ci><ci id="Ch4.S4.p2.1.m1.1.1.4.cmml" xref="Ch4.S4.p2.1.m1.1.1.4">𝑝</ci><ci id="Ch4.S4.p2.1.m1.1.1.5.cmml" xref="Ch4.S4.p2.1.m1.1.1.5">_</ci><ci id="Ch4.S4.p2.1.m1.1.1.6.cmml" xref="Ch4.S4.p2.1.m1.1.1.6">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.p2.1.m1.1c">top\_k</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.p2.1.m1.1d">italic_t italic_o italic_p _ italic_k</annotation></semantics></math> other sentences. We experiment with diverse values of <span class="ltx_text" id="Ch4.S4.p2.1.3">1 to 10</span> sentence(s) from the <span class="ltx_text" id="Ch4.S4.p2.1.4">context</span> dataset.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>For Arabic, we could only integrate up to 7 matches (not 10 matches) because the tokeniser used by GPT-3.5 generates many more tokens for some Unicode languages, which can easily hit the max length of 4097 tokens. We observe that the issue has been alleviated by newer models.</span></span></span> <span class="ltx_text" id="Ch4.S4.p2.1.5">Table</span> <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T2" title="Table 4.2 ‣ 4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.2</span></a> elaborates on the statistics of fuzzy matches based on their similarity to the new source sentence in 2-shot and 5-shot scenarios.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>While creating prompts, we arrange fuzzy matches in descending order, making higher matches closer to the segment to be translated. We experimented with reversing the order, and there was no significant difference in terms of translation quality.</span></span></span></p>
</div>
<figure class="ltx_table" id="Ch4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T2.1.1.1.1" rowspan="2">
<span class="ltx_text" id="Ch4.T2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="Ch4.T2.1.1.1.1.1.1">
<span class="ltx_tr" id="Ch4.T2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="Ch4.T2.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T2.1.1.1.1.1.1.1.1.1">Similarity</span></span></span>
<span class="ltx_tr" id="Ch4.T2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_center" id="Ch4.T2.1.1.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="Ch4.T2.1.1.1.1.1.1.2.1.1">Score</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="Ch4.T2.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T2.1.1.1.2.1">Segment Statistics</span></th>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="Ch4.T2.1.2.2.1">
<span class="ltx_text ltx_font_bold" id="Ch4.T2.1.2.2.1.1">fuzzy 2-shot</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="Ch4.T2.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T2.1.2.2.2.1">fuzzy 5-shot</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T2.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.1.3.1.1">&gt;90%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.1.3.1.2">167</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.1.3.1.3">2.7%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.1.3.1.4">168</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T2.1.3.1.5">1.1%</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.4.2">
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.4.2.1">89-80%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.4.2.2">751</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.4.2.3">12.2%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.4.2.4">1,103</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.4.2.5">7.2%</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.5.3">
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.5.3.1">79-70%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.5.3.2">1,593</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.5.3.3">25.9%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.5.3.4">3,143</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.5.3.5">20.5%</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.6.4">
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.6.4.1">69-60%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.6.4.2">1,825</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.6.4.3">29.7%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.6.4.4">4,661</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.6.4.5">30.4%</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.7.5">
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.7.5.1">&lt;60%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.7.5.2">1,804</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.7.5.3">29.4%</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.7.5.4">6,275</td>
<td class="ltx_td ltx_align_left" id="Ch4.T2.1.7.5.5">40.9%</td>
</tr>
<tr class="ltx_tr" id="Ch4.T2.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ch4.T2.1.8.6.1">Total</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" colspan="2" id="Ch4.T2.1.8.6.2">6,140 = 3,070*2</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" colspan="2" id="Ch4.T2.1.8.6.3">15,350 = 3,070*5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.2: </span>Numbers and percentages of segments based on their similarity to the new source segment, in the 2-shot and 5-shot experiments using fuzzy matches for in-context learning. The English source is used to calculate similarity across the 5 language pairs.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S4.p3">
<p class="ltx_p" id="Ch4.S4.p3.1">The following illustrations show the difference between zero-shot and few-shot translation prompts. In the zero-shot prompt, only the source sentence and language names are provided, encouraging the model to generate the translation. The few-shot prompt incorporates translation examples to influence the style of the output.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div class="ltx_para ltx_noindent" id="Ch4.S4.p4">
<svg class="ltx_picture" height="149.09" id="Ch4.S4.p4.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,149.09) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 133.05 C 0 135.99 2.38 138.37 5.32 138.37 L 243.75 138.37 C 246.68 138.37 249.07 135.99 249.07 133.05 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 133.05 C 1.38 135.22 3.15 136.99 5.32 136.99 L 243.75 136.99 C 245.92 136.99 247.68 135.22 247.68 133.05 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 126.56)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 326.02 22.54 C 327.76 22.54 329.17 21.13 329.17 19.39 L 329.17 3.15 C 329.17 1.41 327.76 0 326.02 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 326.02 21.35 C 327.11 21.35 327.99 20.47 327.99 19.39 L 327.99 3.15 C 327.99 2.06 327.11 1.18 326.02 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S4.p4.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-AR zero-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 116.66)"><foreignobject height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S4.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S4.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S4.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S4.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S4.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S4.I1">
<span class="ltx_item" id="Ch4.S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I1.ix1.p1">
<span class="ltx_p" id="Ch4.S4.I1.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I1.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I1.ix1.p1.1.m1.1"><semantics id="Ch4.S4.I1.ix1.p1.1.m1.1a"><mo id="Ch4.S4.I1.ix1.p1.1.m1.1.1" xref="Ch4.S4.I1.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I1.ix1.p1.1.m1.1b"><lt id="Ch4.S4.I1.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S4.I1.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I1.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I1.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I1.ix1.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I1.ix1.p1.2.m2.1"><semantics id="Ch4.S4.I1.ix1.p1.2.m2.1a"><mo id="Ch4.S4.I1.ix1.p1.2.m2.1.1" xref="Ch4.S4.I1.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I1.ix1.p1.2.m2.1b"><gt id="Ch4.S4.I1.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S4.I1.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I1.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I1.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I1.ix1.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I1.ix2.p1">
<span class="ltx_p" id="Ch4.S4.I1.ix2.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I1.ix2.p1.1.1">Arabic:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Ch4.S4.p5">
<svg class="ltx_picture" height="149.09" id="Ch4.S4.p5.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,149.09) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 133.05 C 0 135.99 2.38 138.37 5.32 138.37 L 243.75 138.37 C 246.68 138.37 249.07 135.99 249.07 133.05 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 133.05 C 1.38 135.22 3.15 136.99 5.32 136.99 L 243.75 136.99 C 245.92 136.99 247.68 135.22 247.68 133.05 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 126.56)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 322.91 22.54 C 324.65 22.54 326.06 21.13 326.06 19.39 L 326.06 3.15 C 326.06 1.41 324.65 0 322.91 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 322.91 21.35 C 324 21.35 324.88 20.47 324.88 19.39 L 324.88 3.15 C 324.88 2.06 324 1.18 322.91 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S4.p5.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-AR two-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 98.67)"><foreignobject height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S4.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S4.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S4.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S4.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S4.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S4.I2">
<span class="ltx_item" id="Ch4.S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix1.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix1.p1.1.m1.1"><semantics id="Ch4.S4.I2.ix1.p1.1.m1.1a"><mo id="Ch4.S4.I2.ix1.p1.1.m1.1.1" xref="Ch4.S4.I2.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix1.p1.1.m1.1b"><lt id="Ch4.S4.I2.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S4.I2.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix1.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S4.I2.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix1.p1.2.m2.1"><semantics id="Ch4.S4.I2.ix1.p1.2.m2.1a"><mo id="Ch4.S4.I2.ix1.p1.2.m2.1.1" xref="Ch4.S4.I2.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix1.p1.2.m2.1b"><gt id="Ch4.S4.I2.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S4.I2.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I2.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix2.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix2.p1.2.1">Arabic: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix2.p1.1.m1.1"><semantics id="Ch4.S4.I2.ix2.p1.1.m1.1a"><mo id="Ch4.S4.I2.ix2.p1.1.m1.1.1" xref="Ch4.S4.I2.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix2.p1.1.m1.1b"><lt id="Ch4.S4.I2.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S4.I2.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix2.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S4.I2.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix2.p1.2.m2.1"><semantics id="Ch4.S4.I2.ix2.p1.2.m2.1a"><mo id="Ch4.S4.I2.ix2.p1.2.m2.1.1" xref="Ch4.S4.I2.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix2.p1.2.m2.1b"><gt id="Ch4.S4.I2.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S4.I2.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I2.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix3.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix3.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix3.p1.1.m1.1"><semantics id="Ch4.S4.I2.ix3.p1.1.m1.1a"><mo id="Ch4.S4.I2.ix3.p1.1.m1.1.1" xref="Ch4.S4.I2.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix3.p1.1.m1.1b"><lt id="Ch4.S4.I2.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S4.I2.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix3.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S4.I2.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix3.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix3.p1.2.m2.1"><semantics id="Ch4.S4.I2.ix3.p1.2.m2.1a"><mo id="Ch4.S4.I2.ix3.p1.2.m2.1.1" xref="Ch4.S4.I2.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix3.p1.2.m2.1b"><gt id="Ch4.S4.I2.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S4.I2.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I2.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix4.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix4.p1.2.1">Arabic: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix4.p1.1.m1.1"><semantics id="Ch4.S4.I2.ix4.p1.1.m1.1a"><mo id="Ch4.S4.I2.ix4.p1.1.m1.1.1" xref="Ch4.S4.I2.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix4.p1.1.m1.1b"><lt id="Ch4.S4.I2.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S4.I2.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix4.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S4.I2.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix4.p1.2.m2.1"><semantics id="Ch4.S4.I2.ix4.p1.2.m2.1a"><mo id="Ch4.S4.I2.ix4.p1.2.m2.1.1" xref="Ch4.S4.I2.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix4.p1.2.m2.1b"><gt id="Ch4.S4.I2.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S4.I2.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I2.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix5.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix5.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix5.p1.1.m1.1"><semantics id="Ch4.S4.I2.ix5.p1.1.m1.1a"><mo id="Ch4.S4.I2.ix5.p1.1.m1.1.1" xref="Ch4.S4.I2.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix5.p1.1.m1.1b"><lt id="Ch4.S4.I2.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S4.I2.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix5.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S4.I2.ix5.p1.2.m2.1"><semantics id="Ch4.S4.I2.ix5.p1.2.m2.1a"><mo id="Ch4.S4.I2.ix5.p1.2.m2.1.1" xref="Ch4.S4.I2.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S4.I2.ix5.p1.2.m2.1b"><gt id="Ch4.S4.I2.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S4.I2.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S4.I2.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S4.I2.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S4.I2.ix5.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S4.I2.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S4.I2.ix6.p1">
<span class="ltx_p" id="Ch4.S4.I2.ix6.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S4.I2.ix6.p1.1.1">Arabic:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<div class="ltx_para" id="Ch4.S4.p6">
<p class="ltx_p" id="Ch4.S4.p6.1">Results illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.F1" title="Figure 4.1 ‣ 4.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.1</span></a> show that few-shot translation with <span class="ltx_text" id="Ch4.S4.p6.1.1">GPT-3.5</span> using fuzzy matches as context outperforms few-shot translation with random examples, although using random sentence pairs outperforms zero-shot translation. As demonstrated by Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T1" title="Table 4.1 ‣ 4.3 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.1</span></a>, across five language pairs, adding more fuzzy matches improves translation quality further. At some point, there might be diminishing returns of adding more similar sentences as their similarity score decreases. In other words, increasing the number of fuzzy matches from 2 sentences to 5 or 10 sentences incrementally improves translation quality, but with smaller quality gains.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.5     GPT-3 vs Encoder-Decoder MT Models</h3>
<div class="ltx_para" id="Ch4.S5.p1">
<p class="ltx_p" id="Ch4.S5.p1.1">In this section, we aim to compare evaluation results we obtained from various MT encoder-decoder Transformer-based systems <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite> with those from GPT-3.5. To this end, we translate our context dataset with a range of open-source and commercial MT models, including DeepL Translate API,<span class="ltx_note ltx_role_footnote" id="Ch4.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>DeepL supports French, Spanish and Chinese, but not Arabic and Kinyarwanda.</span></span></span> Google Cloud Translation API, OPUS <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib184" title="">2020</a>)</cite>,<span class="ltx_note ltx_role_footnote" id="Ch4.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>We use OPUS models from the Tatoeba-Challenge, specifically the models augmented with back-translation, and trained with Transformer-Big.</span></span></span> and NLLB-200 <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib33" title="">2022</a>)</cite>. We converted OPUS and NLLB models to the <span class="ltx_text" id="Ch4.S5.p1.1.1">CTranslate2</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib93" title="">Klein et al., 2020b, </a>)</cite> format with int8 quantisation for efficiency. Inference parameters include <em class="ltx_emph ltx_font_italic" id="Ch4.S5.p1.1.2">beam_size 4</em> and <em class="ltx_emph ltx_font_italic" id="Ch4.S5.p1.1.3">max_batch_size 2024</em>, on a GPU <em class="ltx_emph ltx_font_italic" id="Ch4.S5.p1.1.4">A100-SXM4-40GB</em> (Google Colab Pro). For tokenisation, we used SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib100" title="">2018</a>)</cite> with the source and target sub-word models provided for each OPUS model, and the multilingual model provided by NLLB for tokenisation.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><math alttext="flores200\_sacrebleu\_tokenizer\_spm.model" class="ltx_Math" display="inline" id="Ch4.footnote12.m1.2"><semantics id="Ch4.footnote12.m1.2b"><mrow id="Ch4.footnote12.m1.2.2.2" xref="Ch4.footnote12.m1.2.2.3.cmml"><mrow id="Ch4.footnote12.m1.1.1.1.1" xref="Ch4.footnote12.m1.1.1.1.1.cmml"><mi id="Ch4.footnote12.m1.1.1.1.1.2" xref="Ch4.footnote12.m1.1.1.1.1.2.cmml">f</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.3" xref="Ch4.footnote12.m1.1.1.1.1.3.cmml">l</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1b" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.4" xref="Ch4.footnote12.m1.1.1.1.1.4.cmml">o</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1c" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.5" xref="Ch4.footnote12.m1.1.1.1.1.5.cmml">r</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1d" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.6" xref="Ch4.footnote12.m1.1.1.1.1.6.cmml">e</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1e" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.7" xref="Ch4.footnote12.m1.1.1.1.1.7.cmml">s</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1f" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mn id="Ch4.footnote12.m1.1.1.1.1.8" xref="Ch4.footnote12.m1.1.1.1.1.8.cmml">200</mn><mo id="Ch4.footnote12.m1.1.1.1.1.1g" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.9" mathvariant="normal" xref="Ch4.footnote12.m1.1.1.1.1.9.cmml">_</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1h" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.10" xref="Ch4.footnote12.m1.1.1.1.1.10.cmml">s</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1i" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.11" xref="Ch4.footnote12.m1.1.1.1.1.11.cmml">a</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1j" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.12" xref="Ch4.footnote12.m1.1.1.1.1.12.cmml">c</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1k" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.13" xref="Ch4.footnote12.m1.1.1.1.1.13.cmml">r</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1l" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.14" xref="Ch4.footnote12.m1.1.1.1.1.14.cmml">e</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1m" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.15" xref="Ch4.footnote12.m1.1.1.1.1.15.cmml">b</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1n" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.16" xref="Ch4.footnote12.m1.1.1.1.1.16.cmml">l</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1o" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.17" xref="Ch4.footnote12.m1.1.1.1.1.17.cmml">e</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1p" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.18" xref="Ch4.footnote12.m1.1.1.1.1.18.cmml">u</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1q" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.19" mathvariant="normal" xref="Ch4.footnote12.m1.1.1.1.1.19.cmml">_</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1r" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.20" xref="Ch4.footnote12.m1.1.1.1.1.20.cmml">t</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1s" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.21" xref="Ch4.footnote12.m1.1.1.1.1.21.cmml">o</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1t" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.22" xref="Ch4.footnote12.m1.1.1.1.1.22.cmml">k</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1u" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.23" xref="Ch4.footnote12.m1.1.1.1.1.23.cmml">e</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1v" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.24" xref="Ch4.footnote12.m1.1.1.1.1.24.cmml">n</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1w" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.25" xref="Ch4.footnote12.m1.1.1.1.1.25.cmml">i</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1x" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.26" xref="Ch4.footnote12.m1.1.1.1.1.26.cmml">z</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1y" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.27" xref="Ch4.footnote12.m1.1.1.1.1.27.cmml">e</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1z" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.28" xref="Ch4.footnote12.m1.1.1.1.1.28.cmml">r</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1aa" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.29" mathvariant="normal" xref="Ch4.footnote12.m1.1.1.1.1.29.cmml">_</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1ab" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.30" xref="Ch4.footnote12.m1.1.1.1.1.30.cmml">s</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1ac" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.31" xref="Ch4.footnote12.m1.1.1.1.1.31.cmml">p</mi><mo id="Ch4.footnote12.m1.1.1.1.1.1ad" xref="Ch4.footnote12.m1.1.1.1.1.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.1.1.1.1.32" xref="Ch4.footnote12.m1.1.1.1.1.32.cmml">m</mi></mrow><mo id="Ch4.footnote12.m1.2.2.2.3" lspace="0em" rspace="0.167em" xref="Ch4.footnote12.m1.2.2.3a.cmml">.</mo><mrow id="Ch4.footnote12.m1.2.2.2.2" xref="Ch4.footnote12.m1.2.2.2.2.cmml"><mi id="Ch4.footnote12.m1.2.2.2.2.2" xref="Ch4.footnote12.m1.2.2.2.2.2.cmml">m</mi><mo id="Ch4.footnote12.m1.2.2.2.2.1" xref="Ch4.footnote12.m1.2.2.2.2.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.2.2.2.2.3" xref="Ch4.footnote12.m1.2.2.2.2.3.cmml">o</mi><mo id="Ch4.footnote12.m1.2.2.2.2.1b" xref="Ch4.footnote12.m1.2.2.2.2.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.2.2.2.2.4" xref="Ch4.footnote12.m1.2.2.2.2.4.cmml">d</mi><mo id="Ch4.footnote12.m1.2.2.2.2.1c" xref="Ch4.footnote12.m1.2.2.2.2.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.2.2.2.2.5" xref="Ch4.footnote12.m1.2.2.2.2.5.cmml">e</mi><mo id="Ch4.footnote12.m1.2.2.2.2.1d" xref="Ch4.footnote12.m1.2.2.2.2.1.cmml">⁢</mo><mi id="Ch4.footnote12.m1.2.2.2.2.6" xref="Ch4.footnote12.m1.2.2.2.2.6.cmml">l</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch4.footnote12.m1.2c"><apply id="Ch4.footnote12.m1.2.2.3.cmml" xref="Ch4.footnote12.m1.2.2.2"><csymbol cd="ambiguous" id="Ch4.footnote12.m1.2.2.3a.cmml" xref="Ch4.footnote12.m1.2.2.2.3">formulae-sequence</csymbol><apply id="Ch4.footnote12.m1.1.1.1.1.cmml" xref="Ch4.footnote12.m1.1.1.1.1"><times id="Ch4.footnote12.m1.1.1.1.1.1.cmml" xref="Ch4.footnote12.m1.1.1.1.1.1"></times><ci id="Ch4.footnote12.m1.1.1.1.1.2.cmml" xref="Ch4.footnote12.m1.1.1.1.1.2">𝑓</ci><ci id="Ch4.footnote12.m1.1.1.1.1.3.cmml" xref="Ch4.footnote12.m1.1.1.1.1.3">𝑙</ci><ci id="Ch4.footnote12.m1.1.1.1.1.4.cmml" xref="Ch4.footnote12.m1.1.1.1.1.4">𝑜</ci><ci id="Ch4.footnote12.m1.1.1.1.1.5.cmml" xref="Ch4.footnote12.m1.1.1.1.1.5">𝑟</ci><ci id="Ch4.footnote12.m1.1.1.1.1.6.cmml" xref="Ch4.footnote12.m1.1.1.1.1.6">𝑒</ci><ci id="Ch4.footnote12.m1.1.1.1.1.7.cmml" xref="Ch4.footnote12.m1.1.1.1.1.7">𝑠</ci><cn id="Ch4.footnote12.m1.1.1.1.1.8.cmml" type="integer" xref="Ch4.footnote12.m1.1.1.1.1.8">200</cn><ci id="Ch4.footnote12.m1.1.1.1.1.9.cmml" xref="Ch4.footnote12.m1.1.1.1.1.9">_</ci><ci id="Ch4.footnote12.m1.1.1.1.1.10.cmml" xref="Ch4.footnote12.m1.1.1.1.1.10">𝑠</ci><ci id="Ch4.footnote12.m1.1.1.1.1.11.cmml" xref="Ch4.footnote12.m1.1.1.1.1.11">𝑎</ci><ci id="Ch4.footnote12.m1.1.1.1.1.12.cmml" xref="Ch4.footnote12.m1.1.1.1.1.12">𝑐</ci><ci id="Ch4.footnote12.m1.1.1.1.1.13.cmml" xref="Ch4.footnote12.m1.1.1.1.1.13">𝑟</ci><ci id="Ch4.footnote12.m1.1.1.1.1.14.cmml" xref="Ch4.footnote12.m1.1.1.1.1.14">𝑒</ci><ci id="Ch4.footnote12.m1.1.1.1.1.15.cmml" xref="Ch4.footnote12.m1.1.1.1.1.15">𝑏</ci><ci id="Ch4.footnote12.m1.1.1.1.1.16.cmml" xref="Ch4.footnote12.m1.1.1.1.1.16">𝑙</ci><ci id="Ch4.footnote12.m1.1.1.1.1.17.cmml" xref="Ch4.footnote12.m1.1.1.1.1.17">𝑒</ci><ci id="Ch4.footnote12.m1.1.1.1.1.18.cmml" xref="Ch4.footnote12.m1.1.1.1.1.18">𝑢</ci><ci id="Ch4.footnote12.m1.1.1.1.1.19.cmml" xref="Ch4.footnote12.m1.1.1.1.1.19">_</ci><ci id="Ch4.footnote12.m1.1.1.1.1.20.cmml" xref="Ch4.footnote12.m1.1.1.1.1.20">𝑡</ci><ci id="Ch4.footnote12.m1.1.1.1.1.21.cmml" xref="Ch4.footnote12.m1.1.1.1.1.21">𝑜</ci><ci id="Ch4.footnote12.m1.1.1.1.1.22.cmml" xref="Ch4.footnote12.m1.1.1.1.1.22">𝑘</ci><ci id="Ch4.footnote12.m1.1.1.1.1.23.cmml" xref="Ch4.footnote12.m1.1.1.1.1.23">𝑒</ci><ci id="Ch4.footnote12.m1.1.1.1.1.24.cmml" xref="Ch4.footnote12.m1.1.1.1.1.24">𝑛</ci><ci id="Ch4.footnote12.m1.1.1.1.1.25.cmml" xref="Ch4.footnote12.m1.1.1.1.1.25">𝑖</ci><ci id="Ch4.footnote12.m1.1.1.1.1.26.cmml" xref="Ch4.footnote12.m1.1.1.1.1.26">𝑧</ci><ci id="Ch4.footnote12.m1.1.1.1.1.27.cmml" xref="Ch4.footnote12.m1.1.1.1.1.27">𝑒</ci><ci id="Ch4.footnote12.m1.1.1.1.1.28.cmml" xref="Ch4.footnote12.m1.1.1.1.1.28">𝑟</ci><ci id="Ch4.footnote12.m1.1.1.1.1.29.cmml" xref="Ch4.footnote12.m1.1.1.1.1.29">_</ci><ci id="Ch4.footnote12.m1.1.1.1.1.30.cmml" xref="Ch4.footnote12.m1.1.1.1.1.30">𝑠</ci><ci id="Ch4.footnote12.m1.1.1.1.1.31.cmml" xref="Ch4.footnote12.m1.1.1.1.1.31">𝑝</ci><ci id="Ch4.footnote12.m1.1.1.1.1.32.cmml" xref="Ch4.footnote12.m1.1.1.1.1.32">𝑚</ci></apply><apply id="Ch4.footnote12.m1.2.2.2.2.cmml" xref="Ch4.footnote12.m1.2.2.2.2"><times id="Ch4.footnote12.m1.2.2.2.2.1.cmml" xref="Ch4.footnote12.m1.2.2.2.2.1"></times><ci id="Ch4.footnote12.m1.2.2.2.2.2.cmml" xref="Ch4.footnote12.m1.2.2.2.2.2">𝑚</ci><ci id="Ch4.footnote12.m1.2.2.2.2.3.cmml" xref="Ch4.footnote12.m1.2.2.2.2.3">𝑜</ci><ci id="Ch4.footnote12.m1.2.2.2.2.4.cmml" xref="Ch4.footnote12.m1.2.2.2.2.4">𝑑</ci><ci id="Ch4.footnote12.m1.2.2.2.2.5.cmml" xref="Ch4.footnote12.m1.2.2.2.2.5">𝑒</ci><ci id="Ch4.footnote12.m1.2.2.2.2.6.cmml" xref="Ch4.footnote12.m1.2.2.2.2.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.footnote12.m1.2d">flores200\_sacrebleu\_tokenizer\_spm.model</annotation><annotation encoding="application/x-llamapun" id="Ch4.footnote12.m1.2e">italic_f italic_l italic_o italic_r italic_e italic_s 200 _ italic_s italic_a italic_c italic_r italic_e italic_b italic_l italic_e italic_u _ italic_t italic_o italic_k italic_e italic_n italic_i italic_z italic_e italic_r _ italic_s italic_p italic_m . italic_m italic_o italic_d italic_e italic_l</annotation></semantics></math> is used for both tokenisation for NLLB and also for spBLEU <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib53" title="">2022</a>)</cite> in sacreBLEU.</span></span></span></p>
</div>
<figure class="ltx_figure" id="Ch4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="290" id="Ch4.F2.g1" src="extracted/5369614/img/compare-mt.png" width="589"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4.2: </span>Evaluation results for GPT-3.5 few-shot translation with 5 or 10 fuzzy matches compared to encoder-decoder MT models (DeepL, Google, OPUS, and NLLB). Specifically, for EN-ES, EN-FR, and EN-ZH language pairs, few-shot translation with GPT-3.5 outperforms conventional systems.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S5.p2">
<p class="ltx_p" id="Ch4.S5.p2.1">We observe that for high-resource languages, adaptive MT with fuzzy matches using <span class="ltx_text" id="Ch4.S5.p2.1.1">GPT-3.5</span> few-shot in-context learning (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S4" title="4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.4</span></a>) can outperform strong encoder-decoder MT systems. For the English-to-French and English-to-Spanish language pairs, few-shot translation with <span class="ltx_text" id="Ch4.S5.p2.1.2">GPT-3.5</span> incorporating only 5 fuzzy matches outperforms strong encoder-decoder MT models, as demonstrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.F2" title="Figure 4.2 ‣ 4.5 GPT-3 vs Encoder-Decoder MT Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.2</span></a>. For English-to-Chinese translation, only when we used 10 fuzzy matches could we achieve better results. However, for English-to-Arabic and English-to-Kinyarwanda translations, results were not on par with the other three language pairs. The results are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T3" title="Table 4.3 ‣ 4.6.1 Fuzzy matches + new segment MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<div class="ltx_para" id="Ch4.S5.p3">
<p class="ltx_p" id="Ch4.S5.p3.1">Among the popular adaptive encoder-decoder MT systems is ModernMT.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.modernmt.com/" title="">https://www.modernmt.com/</a></span></span></span> Originally, the system adopted the instance-based adaptation approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Farajian et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib47" title="">2017</a>)</cite>. To control our experiments with ModernMT to match those with <span class="ltx_text" id="Ch4.S5.p3.1.1">GPT-3.5</span> few-shot translation, we created a new TM for each segment to include only the top-10 fuzzy matches for this segment. <span class="ltx_text" id="Ch4.S5.p3.1.2">Table</span> <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T3" title="Table 4.3 ‣ 4.6.1 Fuzzy matches + new segment MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.3</span></a> illustrates the evaluation results of <span class="ltx_text" id="Ch4.S5.p3.1.3">ModernMT</span> translation with and without a TM. In general, using a TM with ModernMT improves translation quality. Moreover, we observe that zero-shot translation performance (without a TM) of <span class="ltx_text" id="Ch4.S5.p3.1.4">ModernMT</span> outperforms <span class="ltx_text" id="Ch4.S5.p3.1.5">GPT-3.5</span> for the 4 supported language pairs. However, except for English-to-Arabic, few-shot translation with <span class="ltx_text" id="Ch4.S5.p3.1.6">GPT-3.5</span> using either 5 or 10 fuzzy matches outperforms the translation quality of ModernMT using a TM with 10 fuzzy matches per segment, for English-to-Chinese, English-to-French, and English-to-Spanish language pairs.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S6">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.6     Incorporating Encoder-Decoder MT</h3>
<div class="ltx_para" id="Ch4.S6.p1">
<p class="ltx_p" id="Ch4.S6.p1.1">As we demonstrated in the previous section, encoder-decoder MT models have achieved high translation quality for several language pairs. <span class="ltx_text" id="Ch4.S6.p1.1.1">Nevertheless</span>, adaptive MT with LLM few-shot in-context learning can surpass such quality, especially for high-resource languages. In this section, we investigate whether we can utilise encoder-decoder MT models to further improve adaptive translation with <span class="ltx_text" id="Ch4.S6.p1.1.2">GPT-3.5</span>. In the next subsections, we study two scenarios:</p>
<ul class="ltx_itemize" id="Ch4.S6.I1">
<li class="ltx_item" id="Ch4.S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S6.I1.i1.p1">
<p class="ltx_p" id="Ch4.S6.I1.i1.p1.1">appending fuzzy matches with MT from an encoder-decoder model to enhance in-context learning.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>This scenario can be considered an approach to “automatic post-editing” of MT generated by task-oriented models such as Google, OPUS and NLLB.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="Ch4.S6.I1.i2" style="list-style-type:none;padding-top:-3.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S6.I1.i2.p1">
<p class="ltx_p" id="Ch4.S6.I1.i2.p1.1">translating the source side of fuzzy matches, and using these MT translations for few-shot in-context learning along with the original translations.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="Ch4.S6.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">4.6.1     Fuzzy matches + new segment MT</h4>
<div class="ltx_para" id="Ch4.S6.SS1.p1">
<p class="ltx_p" id="Ch4.S6.SS1.p1.1">Incorporating a translation from an encoder-decoder MT model with fuzzy matches, we could achieve substantial improvements over the baseline MT performance. As illustrated by Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T5" title="Table 4.5 ‣ 4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.5</span></a>, although OPUS English-to-Arabic translation quality outperforms <span class="ltx_text" id="Ch4.S6.SS1.p1.1.1">GPT-3.5</span> few-shot translation with 5 fuzzy matches, appending these fuzzy matches with OPUS translation outperforms both OPUS translation only and <span class="ltx_text" id="Ch4.S6.SS1.p1.1.2">GPT-3.5</span> translation with fuzzy matches only. Similarly, adding Google English-to-Chinese translation to 5 fuzzy matches outperforms both baselines. Even for the very low-resource English-to-Kinyarwanda language pair, we relatively notice a similar behaviour, using MT outputs of OPUS or NLLB models.</p>
</div>
<div class="ltx_para" id="Ch4.S6.SS1.p2">
<p class="ltx_p" id="Ch4.S6.SS1.p2.1">However, we observe that if the translation with only fuzzy matches is significantly better than the encoder-decoder MT baseline, we may not achieve further gains. For example, the <span class="ltx_text" id="Ch4.S6.SS1.p2.1.1">GPT-3.5</span> translations with 5 fuzzy matches are already much better than the OPUS translation for English-to-French or Google translation for English-to-Spanish. That is why incorporating the MT output from OPUS or Google did not enhance the <span class="ltx_text" id="Ch4.S6.SS1.p2.1.2">GPT-3.5</span> translation quality for these language pairs.</p>
</div>
<figure class="ltx_table" id="Ch4.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Ch4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.2.1">System</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.3.1">spBLEU ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.4.1">chrF++ ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.5.1">TER ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T3.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.1.1.6.1">COMET ↑</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.2.2.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.2.2.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.2.2.2">OPUS (bt-big)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.2.2.3">43.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.2.2.4">60.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.2.2.5">57.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.2.2.6">63.64</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.3.3.1">NLLB 600M</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.3.3.2">35.66</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.3.3.3">54.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.3.3.4">62.07</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.3.3.5">54.53</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.4.4.1">NLLB 1.2B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.4.4.2">41.1</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.4.4.3">58.51</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.4.4.4">57.15</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.4.4.5">63.85</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.5.5.1">NLLB 3.3B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.5.5.2">43.42</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.5.5.3">60.11</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.5.5.4">55.58</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.5.5.5">66.8</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.6.6.1">Google API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.6.6.2">43.56</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.6.6.3">61.58</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.6.6.4">57.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.6.6.5">65.5</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.7.7.1">ModernMT (no TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.7.7.2">47.17</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.7.7.3">62.82</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.7.7.4">53.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.7.7.5">66.64</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.8.8.1">ModernMT (TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.8.8.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.8.8.2.1">50.33</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.8.8.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.8.8.3.1">65.19</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.8.8.4"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.8.8.4.1">50.19</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.8.8.5"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.8.8.5.1">71.0</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.9.9.1">GPT-3 zero-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.9.9.2">27.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.9.9.3">48.36</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.9.9.4">70.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.9.9.5">41.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.10.10.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.10.10.2">41.33</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.10.10.3">58.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.10.10.4">59.95</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.10.10.5">62.65</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.11.11.1">GPT-3 fuzzy 7-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.11.11.2">41.81</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.11.11.3">59.1</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.11.11.4">59.38</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.11.11.5">64.01</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.12.12.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.12.12.1.1">EN-ES</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.12.12.2">OPUS (bt-big)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.12.12.3">54.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.12.12.4">72.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.12.12.5">36.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.12.12.6">83.69</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.13.13.1">NLLB 600M</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.13.13.2">53.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.13.13.3">72.19</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.13.13.4">37.13</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.13.13.5">83.09</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.14.14.1">NLLB 1.2B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.14.14.2">56.1</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.14.14.3">73.85</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.14.14.4">34.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.14.14.5">85.91</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.15.15.1">NLLB 3.3B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.15.15.2">57.47</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.15.15.3">74.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.15.15.4">33.99</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.15.15.5">86.86</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.16.16.1">DeepL API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.16.16.2">55.39</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.16.16.3">72.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.16.16.4">36.21</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.16.16.5">85.68</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.17.17.1">Google API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.17.17.2">58.98</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.17.17.3">75.17</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.17.17.4">32.46</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.17.17.5">86.62</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.18.18.1">ModernMT (no TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.18.18.2">57.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.18.18.3">74.2</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.18.18.4">34.27</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.18.18.5">85.53</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.19.19.1">ModernMT (TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.19.19.2">59.22</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.19.19.3">75.4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.19.19.4">32.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.19.19.5">86.99</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.20.20.1">GPT-3 zero-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.20.20.2">53.91</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.20.20.3">72.61</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.20.20.4">36.86</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.20.20.5">84.0</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.21.21.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.21.21.2">61.24</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.21.21.3">76.73</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.21.21.4">31.32</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.21.21.5">91.51</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.22.22">
<th class="ltx_td ltx_th ltx_th_row" id="Ch4.T3.1.22.22.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.22.22.2">GPT-3 fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.22.22.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.22.22.3.1">61.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.22.22.4"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.22.22.4.1">77.05</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.22.22.5"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.22.22.5.1">30.9</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.22.22.6"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.22.22.6.1">92.0</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.23.23.1" rowspan="11"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.23.23.1.1">EN-FR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.23.23.2">OPUS (bt-big)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.23.23.3">46.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.23.23.4">65.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.23.23.5">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.23.23.6">56.29</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.24.24.1">NLLB 600M</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.24.24.2">43.25</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.24.24.3">64.17</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.24.24.4">51.28</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.24.24.5">56.16</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.25.25.1">NLLB 1.2B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.25.25.2">46.3</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.25.25.3">66.25</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.25.25.4">48.68</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.25.25.5">59.76</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.26.26.1">NLLB 3.3B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.26.26.2">47.27</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.26.26.3">66.89</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.26.26.4">48.19</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.26.26.5">60.91</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.27.27.1">DeepL API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.27.27.2">47.38</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.27.27.3">66.45</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.27.27.4">48.47</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.27.27.5">61.01</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.28.28">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.28.28.1">Google API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.28.28.2">46.81</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.28.28.3">66.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.28.28.4">47.01</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.28.28.5">59.01</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.29.29">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.29.29.1">ModernMT (no TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.29.29.2">47.17</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.29.29.3">66.28</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.29.29.4">47.91</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.29.29.5">58.46</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.30.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.30.30.1">ModernMT (TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.30.30.2">49.24</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.30.30.3">67.41</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.30.30.4">46.17</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.30.30.5">59.84</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.31.31">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.31.31.1">GPT-3 zero-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.31.31.2">44.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.31.31.3">65.29</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.31.31.4">50.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.31.31.5">58.67</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.32.32">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.32.32.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.32.32.2">51.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.32.32.3">68.43</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.32.32.4">45.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.32.32.5">62.81</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.33.33.1">GPT-3 fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.33.33.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.33.33.2.1">53.72</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.33.33.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.33.33.3.1">69.39</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.33.33.4"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.33.33.4.1">43.82</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.33.33.5"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.33.33.5.1">63.57</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.34.34">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.34.34.1" rowspan="8"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.34.34.1.1">EN-RW</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.34.34.2">OPUS (Tatoeba 2021)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.34.34.3">1.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.34.34.4">15.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.34.34.5">153.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.34.34.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.35.35">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.35.35.1">OPUS (2020)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.35.35.2">5.58</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.35.35.3">27.05</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.35.35.4">101.25</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.35.35.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.36.36">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.36.36.1">NLLB 600M</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.36.36.2">19.46</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.36.36.3">47.61</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.36.36.4">80.01</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.36.36.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.37.37">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.37.37.1">NLLB 1.2B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.37.37.2">23.6</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.37.37.3">50.73</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.37.37.4">74.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.37.37.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.38.38">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.38.38.1">NLLB 3.3B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.38.38.2"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.38.38.2.1">25.17</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.38.38.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.38.38.3.1">52.59</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.38.38.4"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.38.38.4.1">73.06</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.38.38.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.39.39">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.39.39.1">Google API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.39.39.2">20.63</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.39.39.3">48.37</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.39.39.4">73.54</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.39.39.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.40.40">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.40.40.1">GPT-3 zero-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.40.40.2">2.82</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.40.40.3">22.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.40.40.4">143.12</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.40.40.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.41.41">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.41.41.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.41.41.2">14.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.41.41.3">39.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.41.41.4">100.11</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.41.41.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.42.42">
<th class="ltx_td ltx_th ltx_th_row" id="Ch4.T3.1.42.42.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.42.42.2">GPT-3 fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.42.42.3">17.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.42.42.4">41.44</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.42.42.5">92.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.42.42.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.43.43">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.43.43.1" rowspan="9"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.43.43.1.1">EN-ZH</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T3.1.43.43.2">OPUS (bt-big)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.43.43.3">37.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.43.43.4">40.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.43.43.5">121.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T3.1.43.43.6">50.4</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.44.44">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.44.44.1">NLLB 600M</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.44.44.2">24.9</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.44.44.3">33.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.44.44.4">109.37</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.44.44.5">39.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.45.45">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.45.45.1">NLLB 1.2B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.45.45.2">29.02</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.45.45.3">37.45</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.45.45.4">110.22</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.45.45.5">50.05</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.46.46">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.46.46.1">NLLB 3.3B</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.46.46.2">31.35</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.46.46.3">39.08</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.46.46.4">109.52</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.46.46.5">53.89</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.47.47">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.47.47.1">DeepL API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.47.47.2">37.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.47.47.3">47.67</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.47.47.4">100.83</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.47.47.5">69.92</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.48.48">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.48.48.1">Google API</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.48.48.2">48.58</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.48.48.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.48.48.3.1">52.02</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.48.48.4">70.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.48.48.5">73.62</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.49.49">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.49.49.1">ModernMT (no TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.49.49.2">37.61</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.49.49.3">48.46</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.49.49.4">102.18</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.49.49.5">67.45</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.50.50">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.50.50.1">ModernMT (TM)</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.50.50.2">39.85</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.50.50.3">50.95</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.50.50.4">101.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.50.50.5">69.64</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.51.51">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.51.51.1">GPT-3 zero-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.51.51.2">32.41</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.51.51.3">40.82</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.51.51.4">99.45</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.51.51.5">59.87</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.52.52">
<th class="ltx_td ltx_th ltx_th_row" id="Ch4.T3.1.52.52.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T3.1.52.52.2">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.52.52.3">47.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.52.52.4">50.28</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.52.52.5">64.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T3.1.52.52.6">74.86</td>
</tr>
<tr class="ltx_tr" id="Ch4.T3.1.53.53">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="Ch4.T3.1.53.53.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch4.T3.1.53.53.2">GPT-3 fuzzy 10-shot</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T3.1.53.53.3"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.53.53.3.1">49.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T3.1.53.53.4">51.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T3.1.53.53.5"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.53.53.5.1">63.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T3.1.53.53.6"><span class="ltx_text ltx_font_bold" id="Ch4.T3.1.53.53.6.1">75.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.3: </span>Comparing GPT-3.5 few-shot translation using fuzzy matches with encoder-decoder MT systems, DeepL Translate API, Google Cloud Translation API, OPUS (Tatoeba-Challenge, with back-translation and Transformer-Big), and NLLB-200 (600M, 1.2B &amp; 3.3B parameters).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch4.S6.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">4.6.2     Fuzzy matches + all segments MT</h4>
<div class="ltx_para" id="Ch4.S6.SS2.p1">
<p class="ltx_p" id="Ch4.S6.SS2.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6.SS1" title="4.6.1 Fuzzy matches + new segment MT ‣ 4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.6.1</span></a>, we added MT of the new segment from an encoder-decoder model to fuzzy matches, which enhanced <span class="ltx_text" id="Ch4.S6.SS2.p1.1.1">GPT-3.5</span> in-context learning. In this experiment, we include MT for all fuzzy matches and also for the new source segment to be translated. For the English-to-Kinyarwanda and English-to-Spanish language pairs, it is not clear whether including MT for all in-context examples can significantly outperform including MT for only the new source segment to be translated. Again, this depends on the quality of the original MT and requires further investigation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch4.S7">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.7     Bilingual Terminology Extraction</h3>
<div class="ltx_para" id="Ch4.S7.p1">
<p class="ltx_p" id="Ch4.S7.p1.1">Terminology extraction is the task of automatically defining domain-specific terms in a dataset. Extracted terms are naturally used for building glossaries to help translators. Furthermore, it is possible to improve MT performance through finding sentences that include these terms and fine-tuning the system with them <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib78" title="">Hu et al., 2019a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch4.S7.p2">
<p class="ltx_p" id="Ch4.S7.p2.1">In this set of experiments, we ask <span class="ltx_text" id="Ch4.S7.p2.1.1">GPT-3.5</span> to extract 5 bilingual terms from each sentence pair in the context dataset. For parameters, we use temperature 0 and <math alttext="top\_p" class="ltx_Math" display="inline" id="Ch4.S7.p2.1.m1.1"><semantics id="Ch4.S7.p2.1.m1.1a"><mrow id="Ch4.S7.p2.1.m1.1.1" xref="Ch4.S7.p2.1.m1.1.1.cmml"><mi id="Ch4.S7.p2.1.m1.1.1.2" xref="Ch4.S7.p2.1.m1.1.1.2.cmml">t</mi><mo id="Ch4.S7.p2.1.m1.1.1.1" xref="Ch4.S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S7.p2.1.m1.1.1.3" xref="Ch4.S7.p2.1.m1.1.1.3.cmml">o</mi><mo id="Ch4.S7.p2.1.m1.1.1.1a" xref="Ch4.S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S7.p2.1.m1.1.1.4" xref="Ch4.S7.p2.1.m1.1.1.4.cmml">p</mi><mo id="Ch4.S7.p2.1.m1.1.1.1b" xref="Ch4.S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S7.p2.1.m1.1.1.5" mathvariant="normal" xref="Ch4.S7.p2.1.m1.1.1.5.cmml">_</mi><mo id="Ch4.S7.p2.1.m1.1.1.1c" xref="Ch4.S7.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S7.p2.1.m1.1.1.6" xref="Ch4.S7.p2.1.m1.1.1.6.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch4.S7.p2.1.m1.1b"><apply id="Ch4.S7.p2.1.m1.1.1.cmml" xref="Ch4.S7.p2.1.m1.1.1"><times id="Ch4.S7.p2.1.m1.1.1.1.cmml" xref="Ch4.S7.p2.1.m1.1.1.1"></times><ci id="Ch4.S7.p2.1.m1.1.1.2.cmml" xref="Ch4.S7.p2.1.m1.1.1.2">𝑡</ci><ci id="Ch4.S7.p2.1.m1.1.1.3.cmml" xref="Ch4.S7.p2.1.m1.1.1.3">𝑜</ci><ci id="Ch4.S7.p2.1.m1.1.1.4.cmml" xref="Ch4.S7.p2.1.m1.1.1.4">𝑝</ci><ci id="Ch4.S7.p2.1.m1.1.1.5.cmml" xref="Ch4.S7.p2.1.m1.1.1.5">_</ci><ci id="Ch4.S7.p2.1.m1.1.1.6.cmml" xref="Ch4.S7.p2.1.m1.1.1.6">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S7.p2.1.m1.1c">top\_p</annotation><annotation encoding="application/x-llamapun" id="Ch4.S7.p2.1.m1.1d">italic_t italic_o italic_p _ italic_p</annotation></semantics></math> 1.</p>
</div>
<figure class="ltx_table" id="Ch4.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T4.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch4.T4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch4.T4.3.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.1.1.2.1">Sentences</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T4.3.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.1.1.3.1">Terms</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T4.3.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.1.1.4.1">Correct</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T4.3.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.1.1.5.1">%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T4.3.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T4.3.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.2.1.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T4.3.2.1.2">500</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.3.2.1.3">2,500</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.3.2.1.4">2,427</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T4.3.2.1.5">97.08</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.3.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch4.T4.3.3.2.1"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.3.2.1.1">EN-ES</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch4.T4.3.3.2.2">500</th>
<td class="ltx_td ltx_align_center" id="Ch4.T4.3.3.2.3">2,500</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.3.3.2.4">2,397</td>
<td class="ltx_td ltx_align_center" id="Ch4.T4.3.3.2.5">95.88</td>
</tr>
<tr class="ltx_tr" id="Ch4.T4.3.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="Ch4.T4.3.4.3.1"><span class="ltx_text ltx_font_bold" id="Ch4.T4.3.4.3.1.1">EN-FR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="Ch4.T4.3.4.3.2">500</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T4.3.4.3.3">2,500</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T4.3.4.3.4">2,382</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T4.3.4.3.5">95.28</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.4: </span>Human evaluation results for the terminology extraction task for English-to-Arabic (EN-AR), English-to-Spanish (EN-ES), and English-to-French (EN-FR) language pairs. The majority of the terms that GPT-3 extracted (<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.T4.2.m1.1"><semantics id="Ch4.T4.2.m1.1b"><mo id="Ch4.T4.2.m1.1.1" xref="Ch4.T4.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.T4.2.m1.1c"><gt id="Ch4.T4.2.m1.1.1.cmml" xref="Ch4.T4.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.T4.2.m1.1d">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.T4.2.m1.1e">&gt;</annotation></semantics></math> 95%) were accurate.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S7.p3">
<p class="ltx_p" id="Ch4.S7.p3.1">Human evaluation was performed for Arabic, French,<span class="ltx_note ltx_role_footnote" id="Ch4.footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>We observe that the original English-to-French TICO-19 dataset includes several misaligned translation pairs. This can negatively affect the quality of tasks using such sentences. That is why it is important to filter parallel datasets to remove possible misalignments. The evaluation sample has been manually refined to include only well-aligned translation pairs. Automatic semantic filtering approaches can be applied to large datasets.</span></span></span> and Spanish. We provided the evaluators with a random sample of 500 sentences and their extracted terms. They were asked to use a 0-1 scale to determine whether each source and target term were equivalent, and whether the extracted terms were actually in the sentence pair (relevant inflexions are acceptable). In several cases where the evaluators marked the extracted term pair with 0, the model had made up either the source, target, or both; although it might be correct, it was not in the provided sentence pair. In other cases, the extracted term was partial, sometimes due to reaching the maximum length of tokens. Nevertheless, as Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T4" title="Table 4.4 ‣ 4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.4</span></a> illustrates, the majority of the terms in the provided sample were accurately extracted by the model.</p>
</div>
<figure class="ltx_table" id="Ch4.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch4.T5.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.2.1">System</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.3.1">spBLEU ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.4.1">chrF++ ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.5.1">TER ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.1.1.6.1">COMET ↑</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.2.2.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.2.2.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.2.2.2">MT (OPUS)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.2.2.3">43.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.2.2.4">60.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.2.2.5">57.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.2.2.6">63.64</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.3.3.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.3.3.2">41.33</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.3.3.3">58.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.3.3.4">59.95</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.3.3.5">62.65</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.4.4.1">GPT-3 fuzzy 5-shot + 1-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.4.4.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.4.4.2.1">45.9</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.4.4.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.4.4.3.1">62.9</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.4.4.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.4.4.4.1">55.14</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.4.4.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.4.4.5.1">67.74</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.5.5.1" rowspan="7"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.5.5.1.1">EN-ES</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.5.5.2">MT (Google)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.5.5.3">58.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.5.5.4">75.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.5.5.5">32.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.5.5.6">86.62</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.6.6.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.6.6.2">59.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.6.6.3">75.83</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.6.6.4">32.56</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.6.6.5">90.37</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.7.7.1">GPT-3 fuzzy 2-shot + 1-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.7.7.2">59.82</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.7.7.3">75.73</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.7.7.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.7.7.4.1">32.16</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.7.7.5">89.0</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.8.8.1">GPT-3 fuzzy 2-shot + all-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.8.8.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.8.8.2.1">60.2</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.8.8.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.8.8.3.1">76.06</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.8.8.4">32.32</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.8.8.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.8.8.5.1">92.0</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.9.9.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.9.9.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.9.9.2.1">61.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.9.9.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.9.9.3.1">76.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.9.9.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.9.9.4.1">31.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.9.9.5">91.51</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.10.10.1">GPT-3 fuzzy 5-shot + 1-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.10.10.2">60.49</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.10.10.3">76.16</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.10.10.4">31.49</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.10.10.5">89.55</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.11.11.1">GPT-3 fuzzy 5-shot + all-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.11.11.2">61.1</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.11.11.3">76.52</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.11.11.4">31.8</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.11.11.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.11.11.5.1">92.07</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.12.12.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.12.12.1.1">EN-FR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.12.12.2">MT (OPUS)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.12.12.3">46.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.12.12.4">65.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.12.12.5">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.12.12.6">56.29</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.13.13.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.13.13.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.13.13.2.1">51.94</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.13.13.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.13.13.3.1">68.43</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.13.13.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.13.13.4.1">45.09</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.13.13.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.13.13.5.1">62.81</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.14.14.1">GPT-3 fuzzy 5-shot + 1-MT</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.14.14.2">47.95</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.14.14.3">66.72</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.14.14.4">48.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.14.14.5">59.69</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.15.15.1" rowspan="7"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.15.15.1.1">EN-RW</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.15.15.2">MT #1 (Google)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.15.15.3">20.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.15.15.4">48.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.15.15.5">73.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.15.15.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.16.16.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.16.16.2">14.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.16.16.3">39.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.16.16.4">100.11</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.16.16.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.17.17.1">GPT-3 fuzzy 5-shot + 1-MT #1</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.17.17.2">22.51</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.17.17.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.17.17.3.1">49.69</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.17.17.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.17.17.4.1">72.97</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.17.17.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.18.18.1">GPT-3 fuzzy 5-shot + all-MT #1</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.18.18.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.18.18.2.1">25.01</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.18.18.3">49.43</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.18.18.4">74.75</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.18.18.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.19.19.1">MT #2 (NLLB 3.3B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.19.19.2">25.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.19.19.3">52.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.19.19.4">73.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.19.19.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.20.20.1">GPT-3 fuzzy 5-shot + 1-MT #2</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.20.20.2">25.59</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.20.20.3">53.12</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.20.20.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.20.20.4.1">72.73</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.20.20.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.21.21.1">GPT-3 fuzzy 5-shot + all-MT #2</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.21.21.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.21.21.2.1">27.52</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.21.21.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.21.21.3.1">53.23</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.21.21.4">73.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.21.21.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.22.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ch4.T5.1.22.22.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.22.22.1.1">EN-ZH</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T5.1.22.22.2">MT (Google)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.22.22.3">48.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.22.22.4">52.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.22.22.5">70.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T5.1.22.22.6">73.62</td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T5.1.23.23.1">GPT-3 fuzzy 5-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.23.23.2">47.94</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.23.23.3">50.28</td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.23.23.4"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.23.23.4.1">64.96</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T5.1.23.23.5"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.23.23.5.1">74.86</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T5.1.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch4.T5.1.24.24.1">GPT-3 fuzzy 5-shot + 1-MT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T5.1.24.24.2"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.24.24.2.1">49.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T5.1.24.24.3"><span class="ltx_text ltx_font_bold" id="Ch4.T5.1.24.24.3.1">52.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T5.1.24.24.4">67.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T5.1.24.24.5">74.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.5: </span>Combining fuzzy matches with high-quality MT from encoder-decoder systems can improve translation quality with GPT-3.5 few-shot in-context learning, especially for low-resource and medium-resource languages. 1-MT refers to appending fuzzy matches with the MT of the segment to be translated, while all-MT refers to additionally adding MT for each segment of the fuzzy matches along with its approved translation. For EN-AR and EN-RW improvements are clearer than for EN-ES, EN-FR and EN-ZH, potentially due to the limited support of EN-AR and EN-RW by GPT-3.5, which made them benefit more from incorporating MT from stronger encoder-decoder models.</figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch4.S8">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.8     Terminology-Constrained MT</h3>
<div class="ltx_para" id="Ch4.S8.p1">
<p class="ltx_p" id="Ch4.S8.p1.1">As observed in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S4" title="4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.4</span></a>, adding more fuzzy matches enhances in-context learning and hence improves translation quality. However, early in a real-world translation project, we might not have so many fuzzy matches. By incorporating domain-specific terminology, the system can produce translations that are more accurate and consistent with the terminology used in that field. In this section, we investigate integrating terms in the process when there are <math alttext="N" class="ltx_Math" display="inline" id="Ch4.S8.p1.1.m1.1"><semantics id="Ch4.S8.p1.1.m1.1a"><mi id="Ch4.S8.p1.1.m1.1.1" xref="Ch4.S8.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Ch4.S8.p1.1.m1.1b"><ci id="Ch4.S8.p1.1.m1.1.1.cmml" xref="Ch4.S8.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S8.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="Ch4.S8.p1.1.m1.1d">italic_N</annotation></semantics></math> fuzzy matches. For example, if we have only two fuzzy matches, we either extract terms from these similar sentences or from a <span class="ltx_text" id="Ch4.S8.p1.1.1">glossary</span>, and use those that match up to 5-gram phrases in the source sentence to be translated. In this work, we use the terminology extraction process elaborated in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S7" title="4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.7</span></a>. Obviously, if a pre-approved glossary is available, it can be used instead. We investigate three scenarios:</p>
</div>
<div class="ltx_para" id="Ch4.S8.p2">
<ul class="ltx_itemize" id="Ch4.S8.I1">
<li class="ltx_item" id="Ch4.S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S8.I1.i1.p1">
<p class="ltx_p" id="Ch4.S8.I1.i1.p1.1">Few-shot translation with 2 fuzzy matches and their terms. As we do not have terms for the segment to be translated, we use terms from the 2 fuzzy matches if they are found in a set of n-grams (1-5) of the source segment to be translated. Integrating terms into two-shot prediction, i.e. using both terms and two fuzzy matches for in-context learning, <span class="ltx_text" id="Ch4.S8.I1.i1.p1.1.1">outperforms</span> using fuzzy matches only.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S8.I1.i2" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S8.I1.i2.p1">
<p class="ltx_p" id="Ch4.S8.I1.i2.p1.1">We automatically compile a glossary including all terms from the dataset, with 2+ frequency, and up to 5-grams. If there are multiple targets for the same source, the term pair with the highest frequency is selected. Stop words and terms with empty source or target sides are excluded. The list is sorted by n-gram length, so terms with longer n-grams are prioritised. As illustrated by Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T6" title="Table 4.6 ‣ 4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.6</span></a>, integrating terms from a glossary outperforms adding terms from only two fuzzy matches, most likely due to the diversity that this option offers. In prompts (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S12" title="4.12 Prompts ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.12</span></a>), we use terms found in a set of n-grams (1-5) of the source segment to be translated. We experiment with adding maximum 5 terms and maximum 10 terms, which does not show a huge difference in performance; in some cases only a smaller number of terms is available in the glossary.</p>
</div>
</li>
<li class="ltx_item" id="Ch4.S8.I1.i3" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch4.S8.I1.i3.p1">
<p class="ltx_p" id="Ch4.S8.I1.i3.p1.1">Zero-shot translation, i.e. without any fuzzy matches. This is similar to the previous scenario, except that we only use terms from the glossary. In zero-shot prediction, adding terms from the glossary improves translation quality. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T6" title="Table 4.6 ‣ 4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.6</span></a>, improvements are significant across all 5 language pairs.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="Ch4.T6">
<table class="ltx_tabular ltx_align_middle" id="Ch4.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.1.1">Lang</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Ch4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.2.1">GPT-3.5 Context</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.3.1">spBLEU ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.4.1">chrF++ ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.5.1">TER ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch4.T6.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.1.1.6.1">COMET ↑</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.2.2.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.2.2.1.1">EN-AR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.2.2.2">zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.2.2.3">27.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.2.2.4">48.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.2.2.5">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.2.2.6">41.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.3.3">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.3.3.1">zero-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.3.3.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.3.3.2.1">35.38</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.3.3.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.3.3.3.1">54.53</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.3.3.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.3.3.4.1">65.36</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.3.3.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.3.3.5.1">54.91</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.4.4.1">fuzzy 2-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.4.4.2">38.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.4.4.3">56.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.4.4.4">62.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.4.4.5">57.36</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.5.5">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.5.5.1">fuzzy 2-shot + terms (fuzzy)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.5.5.2">39.38</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.5.5.3">57.22</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.5.5.4">62.01</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.5.5.5">59.36</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.6.6">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.6.6.1">fuzzy 2-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.6.6.2">41.27</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.6.6.3">58.84</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.6.6.4">60.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.6.6.5">62.17</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.7.7">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.7.7.1">fuzzy 2-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.7.7.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.7.7.2.1">41.95</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.7.7.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.7.7.3.1">59.34</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.7.7.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.7.7.4.1">59.45</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.7.7.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.7.7.5.1">62.48</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.8.8.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.8.8.1.1">EN-ES</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.8.8.2">zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.8.8.3">53.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.8.8.4">72.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.8.8.5">36.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.8.8.6">84.0</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.9.9">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.9.9.1">zero-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.9.9.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.9.9.2.1">55.99</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.9.9.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.9.9.3.1">74.18</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.9.9.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.9.9.4.1">35.3</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.9.9.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.9.9.5.1">87.21</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.10.10.1">fuzzy 2-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.10.10.2">59.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.10.10.3">75.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.10.10.4">32.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.10.10.5">90.37</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.11.11">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.11.11.1">fuzzy 2-shot + terms (fuzzy)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.11.11.2">59.66</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.11.11.3">75.91</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.11.11.4">32.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.11.11.5">90.04</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.12.12">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.12.12.1">fuzzy 2-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.12.12.2">60.5</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.12.12.3">76.55</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.12.12.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.12.12.4.1">31.93</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.12.12.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.12.12.5.1">91.05</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.13.13">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.13.13.1">fuzzy 2-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.13.13.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.13.13.2.1">60.54</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.13.13.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.13.13.3.1">76.58</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.13.13.4">32.02</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.13.13.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.13.13.5.1">91.05</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.14.14.1" rowspan="7"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.14.14.1.1">EN-FR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.14.14.2">zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.14.14.3">44.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.14.14.4">65.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.14.14.5">50.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.14.14.6">58.67</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.15.15">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.15.15.1">zero-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.15.15.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.15.15.2.1">45.94</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.15.15.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.15.15.3.1">66.01</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.15.15.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.15.15.4.1">49.22</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.15.15.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.15.15.5.1">59.78</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.16.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.16.16.1">fuzzy 2-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.16.16.2">49.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.16.16.3">67.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.16.16.4">46.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.16.16.5">61.38</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.17.17">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.17.17.1">fuzzy 2-shot + terms (fuzzy)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.17.17.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.17.17.2.1">50.58</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.17.17.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.17.17.3.1">67.93</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.17.17.4">45.81</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.17.17.5">62.04</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.18.18">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.18.18.1">fuzzy 2-shot + max 3 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.18.18.2">50.46</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.18.18.3">67.69</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.18.18.4">46.22</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.18.18.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.18.18.5.1">68.94</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.19.19">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.19.19.1">fuzzy 2-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.19.19.2">50.55</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.19.19.3">67.78</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.19.19.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.19.19.4.1">46.19</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.19.19.5">60.24</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.20.20">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.20.20.1">fuzzy 2-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.20.20.2">49.64</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.20.20.3">66.86</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.20.20.4">47.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.20.20.5">58.57</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.21.21">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.21.21.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.21.21.1.1">EN-RW</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.21.21.2">zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.21.21.3">2.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.21.21.4">22.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.21.21.5">143.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.21.21.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.22.22">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.22.22.1">zero-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.22.22.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.22.22.2.1">7.26</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.22.22.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.22.22.3.1">30.83</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.22.22.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.22.22.4.1">115.44</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.22.22.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.23.23">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.23.23.1">fuzzy 2-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.23.23.2">12.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.23.23.3">36.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.23.23.4">105.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.23.23.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.24.24">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.24.24.1">fuzzy 2-shot + terms (fuzzy)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.24.24.2">12.43</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.24.24.3">36.48</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.24.24.4">102.22</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.24.24.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.25.25">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.25.25.1">fuzzy 2-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.25.25.2">15.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.25.25.3">39.96</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.25.25.4">96.09</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.25.25.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.26.26">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.26.26.1">fuzzy 2-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.26.26.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.26.26.2.1">15.49</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.26.26.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.26.26.3.1">40.53</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.26.26.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.26.26.4.1">96.0</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.26.26.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.27.27">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch4.T6.1.27.27.1" rowspan="7"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.27.27.1.1">EN-ZH</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.27.27.2">zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.27.27.3">32.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.27.27.4">40.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.27.27.5">99.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.27.27.6">59.87</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.28.28">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.28.28.1">zero-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.28.28.2">36.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.28.28.3">44.72</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.28.28.4">96.45</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.28.28.5">68.6</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.29.29">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.29.29.1">zero-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.29.29.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.29.29.2.1">36.64</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.29.29.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.29.29.3.1">45.06</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.29.29.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.29.29.4.1">96.24</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.29.29.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.29.29.5.1">68.94</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.30.30">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T6.1.30.30.1">fuzzy 2-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.30.30.2">46.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.30.30.3">49.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.30.30.4">69.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T6.1.30.30.5"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.30.30.5.1">73.9</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.31.31">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.31.31.1">fuzzy 2-shot + terms (fuzzy)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.31.31.2">46.16</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.31.31.3">49.11</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.31.31.4"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.31.31.4.1">68.79</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.31.31.5">73.41</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.32.32">
<td class="ltx_td ltx_align_left" id="Ch4.T6.1.32.32.1">fuzzy 2-shot + max 5 terms (glossary)</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.32.32.2"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.32.32.2.1">46.6</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.32.32.3"><span class="ltx_text ltx_font_bold" id="Ch4.T6.1.32.32.3.1">49.51</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.32.32.4">69.46</td>
<td class="ltx_td ltx_align_center" id="Ch4.T6.1.32.32.5">73.88</td>
</tr>
<tr class="ltx_tr" id="Ch4.T6.1.33.33">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch4.T6.1.33.33.1">fuzzy 2-shot + max 10 terms (glossary)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T6.1.33.33.2">46.31</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T6.1.33.33.3">49.25</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T6.1.33.33.4">69.39</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T6.1.33.33.5">73.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.6: </span>Terminology-constrained MT with GPT 3.5 outperforms both zero-shot and 2-shot translation with fuzzy matches, although gains are much higher for zero-shot translation. For zero-shot translation, we experimented with adding terms from a glossary. For 2-shot translation with fuzzy matches, we compared adding terms from these 2 fuzzy matches to adding terms from a glossary. The latter revealed better results.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S8.p3">
<p class="ltx_p" id="Ch4.S8.p3.1">We conducted human evaluation for English-to-Arabic, English-to-French, and English-to-Spanish terminology-constrained MT, to see to what extent the model adheres to the required terms, and how this affects the overall translation quality. The evaluators are professional linguists in the respective languages. We provided the evaluators with 4 sets of 100 randomly selected sentence pairs (zero-shot, zero-shot with glossary terms, fuzzy two-shot, and fuzzy two-shot with glossary terms). They were asked to evaluate the sentence-level translation quality on a 1-4 scale <cite class="ltx_cite ltx_citemacro_citep">(Coughlin,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib34" title="">2003</a>)</cite> and the usage of each provided term in the translation on a 0-1 scale, as elaborated by <span class="ltx_text" id="Ch4.S8.p3.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T7" title="Table 4.7 ‣ 4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.7</span></a>.</span></p>
</div>
<figure class="ltx_table" id="Ch4.T7">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Ch4.T7.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch4.T7.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T7.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T7.1.1.1.2.1">GPT-3 Context</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.1.1.3.1">Human Eval. ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.1.1.4.1">Terms ↑</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T7.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.2.1.1" rowspan="4"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.2.1.1.1">EN-AR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.2.1.2">Zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.2.1.3">2.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.2.1.4">0.67</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.3.2">
<td class="ltx_td ltx_align_left" id="Ch4.T7.1.3.2.1">Zero-shot + glossary terms</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.3.2.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.3.2.2.1">3.19</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.3.2.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.3.2.3.1">0.94</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.4.3.1">Fuzzy two-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.4.3.2">2.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.4.3.3">0.80</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.5.4">
<td class="ltx_td ltx_align_left" id="Ch4.T7.1.5.4.1">Fuzzy two-shot + glossary terms</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.5.4.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.5.4.2.1">3.03</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.5.4.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.5.4.3.1">0.94</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.6.5.1" rowspan="4"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.6.5.1.1">EN-ES</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.6.5.2">Zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.6.5.3">3.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.6.5.4">0.87</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.7.6">
<td class="ltx_td ltx_align_left" id="Ch4.T7.1.7.6.1">Zero-shot + glossary terms</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.7.6.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.7.6.2.1">3.93</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.7.6.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.7.6.3.1">0.96</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.8.7.1">Fuzzy two-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.8.7.2">3.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.8.7.3">0.89</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.9.8">
<td class="ltx_td ltx_align_left" id="Ch4.T7.1.9.8.1">Fuzzy two-shot + glossary terms</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.9.8.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.9.8.2.1">3.84</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.9.8.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.9.8.3.1">0.97</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch4.T7.1.10.9.1" rowspan="4"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.10.9.1.1">EN-FR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.10.9.2">Zero-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.10.9.3">3.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.10.9.4">0.89</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.11.10">
<td class="ltx_td ltx_align_left" id="Ch4.T7.1.11.10.1">Zero-shot + glossary terms</td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.11.10.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.11.10.2.1">3.64</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T7.1.11.10.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.11.10.3.1">0.97</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch4.T7.1.12.11.1">Fuzzy two-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.12.11.2">3.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T7.1.12.11.3">0.91</td>
</tr>
<tr class="ltx_tr" id="Ch4.T7.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch4.T7.1.13.12.1">Fuzzy two-shot + glossary terms</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T7.1.13.12.2"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.13.12.2.1">3.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T7.1.13.12.3"><span class="ltx_text ltx_font_bold" id="Ch4.T7.1.13.12.3.1">0.92</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.7: </span>Human evaluation of terminology-constrained MT, for EN-AR, EN-ES, and EN-FR. The results cover zero-shot and two-shot translation without and with (maximum 5) glossary terms. The column “Human Eval.” refers to the average evaluation score on a 1-4 scale. The column “Terms” refers to the average number of terms that the model has successfully transferred into the translation on a 0-1 scale.</figcaption>
</figure>
<div class="ltx_para" id="Ch4.S8.p4">
<p class="ltx_p" id="Ch4.S8.p4.1">According to the evaluators, for Arabic, French and Spanish, terminology-constrained MT successfully transferred the provided glossary terms into the target more often than zero-shot and few-shot translation without terminology incorporation. In several cases, forcing glossary terms to be used could help improve the overall translation quality; however, sometimes it was detrimental to grammatical accuracy. Although we provided the model with longer terms before shorter ones, contradictory terms can hurt translation quality. Hence, it might be better to exclude shorter terms if they overlap with longer ones.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>For example, “New York Times” can be transferred without translation into the target, while “New York” might be translated. If the model is provided with both terms while it is actually supposed to use the former, this can cause confusion.</span></span></span> In production workflows, linguists can be provided with translation alternatives with and without fuzzy matches and/or terminology to be able to use the best translation. Alternatively, automatic quality estimation can be conducted to select the best translation.</p>
</div>
<div class="ltx_para" id="Ch4.S8.p5">
<p class="ltx_p" id="Ch4.S8.p5.1">Among interesting observations that human evaluation reveals is that in few-shot translation with fuzzy matches (even <em class="ltx_emph ltx_font_italic" id="Ch4.S8.p5.1.1">without</em> terms), the number of successfully used terms is more than those in zero-shot translation. This can help enhance <span class="ltx_text" id="Ch4.S8.p5.1.2">consistency</span> with approved translations. Moreover, incorporating glossary terms in a zero-shot prompt can result in quality gains comparable to those of few-shot translation with fuzzy matches.</p>
</div>
</section>
<section class="ltx_section" id="Ch4.S9">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.9     ChatGPT</h3>
<div class="ltx_para" id="Ch4.S9.p1">
<p class="ltx_p" id="Ch4.S9.p1.1">At the time of writing this paper, OpenAI has released new conversational models, publicly <span class="ltx_text" id="Ch4.S9.p1.1.1">referred</span> to as ChatGPT. This range of models includes: GPT-3.5 Turbo and GPT-4. In this section, we briefly investigate the translation capabilities of these models compared to GPT-3.5 Davinci. Generally, we observe that both of the new models solve some tokenisation issues, especially for non-Latin languages such as <span class="ltx_text" id="Ch4.S9.p1.1.2">Arabic</span>. While <em class="ltx_emph ltx_font_italic" id="Ch4.S9.p1.1.3">gpt-3.5-turbo</em> is more efficient than <em class="ltx_emph ltx_font_italic" id="Ch4.S9.p1.1.4">text-davinci-003</em>, it shows comparable quality for both zero-shot and few-shot translation (with fuzzy matches). The newest model <em class="ltx_emph ltx_font_italic" id="Ch4.S9.p1.1.5">gpt-4</em> provides better zero-shot translation quality, while the quality of few-shot translation is relatively similar to that of the two other models. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T8" title="Table 4.8 ‣ 4.9 ChatGPT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.8</span></a> demonstrates the results.</p>
</div>
<figure class="ltx_table" id="Ch4.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch4.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.3.1">Context</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.4.1">spBLEU ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.5.1">chrF++ ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.6.1">TER ↓</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T8.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.1.1.7.1">COMET ↑</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T8.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T8.1.2.1.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.2.1.1.1">EN-AR</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.2">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.3" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.2.1.3.1">0-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.4">27.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.5">48.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.6">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.2.1.7">41.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.3.2.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.3.2.2">38.06</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.3.2.3">56.35</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.3.2.4">61.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.3.2.5">62.68</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.4.3">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.4.3.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.4.3.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.4.3.2.1">40.29</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.4.3.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.4.3.3.1">57.86</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.4.3.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.4.3.4.1">59.55</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.4.3.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.4.3.5.1">64.25</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.1">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.2" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.5.4.2.1">2-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.3">38.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.4">56.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.5">62.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.5.4.6">57.36</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.6.5">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.6.5.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.6.5.2">46.04</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.6.5.3">62.18</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.6.5.4">55.03</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.6.5.5">73.35</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.7.6">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.7.6.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.7.6.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.7.6.2.1">47.52</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.7.6.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.7.6.3.1">63.28</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.7.6.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.7.6.4.1">53.04</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.7.6.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.7.6.5.1">73.7</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T8.1.8.7.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.8.7.1.1">EN-ES</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.2">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.3" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.8.7.3.1">0-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.4">53.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.5">72.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.6">36.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.8.7.7">84.0</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.9.8">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.9.8.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.9.8.2">52.91</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.9.8.3">70.87</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.9.8.4">38.86</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.9.8.5">82.28</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.10.9">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.10.9.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.10.9.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.10.9.2.1">56.93</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.10.9.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.10.9.3.1">74.41</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.10.9.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.10.9.4.1">34.35</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.10.9.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.10.9.5.1">87.89</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.1">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.2" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.11.10.2.1">2-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.3">59.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.4">75.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.5">32.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.11.10.6">90.37</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.12.11">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.12.11.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.12.11.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.12.11.2.1">60.35</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.12.11.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.12.11.3.1">76.51</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.12.11.4">32.05</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.12.11.5">91.57</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.13.12">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.13.12.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.13.12.2">60.16</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.13.12.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.13.12.3.1">76.51</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.13.12.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.13.12.4.1">31.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.13.12.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.13.12.5.1">91.86</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T8.1.14.13.1" rowspan="6"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.14.13.1.1">EN-FR</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.2">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.3" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.14.13.3.1">0-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.4">44.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.5">65.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.6">50.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.14.13.7">58.67</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.15.14">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.15.14.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.15.14.2">46.85</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.15.14.3">66.75</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.15.14.4">48.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.15.14.5">61.34</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.16.15">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.16.15.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.16.15.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.16.15.2.1">47.39</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.16.15.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.16.15.3.1">67.14</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.16.15.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.16.15.4.1">48.03</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.16.15.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.16.15.5.1">61.93</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.1">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.2" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.17.16.2.1">2-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.3">49.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.4">67.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.5">46.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.17.16.6">61.38</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.18.17">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.18.17.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.18.17.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.18.17.2.1">49.88</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.18.17.3">68.33</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.18.17.4">46.27</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.18.17.5">63.62</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.19.18">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.19.18.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.19.18.2">49.75</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.19.18.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.19.18.3.1">68.38</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.19.18.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.19.18.4.1">45.97</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.19.18.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.19.18.5.1">64.04</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T8.1.20.19.1" rowspan="6">
<span class="ltx_text ltx_font_bold" id="Ch4.T8.1.20.19.1.1">EN-RW</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.2">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.3" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.20.19.3.1">0-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.4">2.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.5">22.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.6">143.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.20.19.7">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.21.20">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.21.20.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.21.20.2">5.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.21.20.3">29.77</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.21.20.4">114.34</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.21.20.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.22.21">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.22.21.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.22.21.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.22.21.2.1">8.95</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.22.21.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.22.21.3.1">35.28</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.22.21.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.22.21.4.1">93.15</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.22.21.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.1">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.2" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.23.22.2.1">2-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.3">12.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.4">36.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.5">105.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.23.22.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.24.23">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.24.23.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.24.23.2">12.49</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.24.23.3">39.37</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.24.23.4">105.51</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.24.23.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.25.24">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.25.24.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.25.24.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.25.24.2.1">16.78</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.25.24.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.25.24.3.1">44.21</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.25.24.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.25.24.4.1">83.31</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.25.24.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.26.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ch4.T8.1.26.25.1" rowspan="6">
<span class="ltx_text ltx_font_bold" id="Ch4.T8.1.26.25.1.1">EN-ZH</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.2">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.3" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.26.25.3.1">0-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.4">32.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.5">40.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.6">99.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.26.25.7">59.87</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.27.26">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.27.26.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.27.26.2">36.83</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.27.26.3">45.77</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.27.26.4">99.83</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.27.26.5">69.13</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.28.27">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.28.27.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.28.27.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.28.27.2.1">37.65</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.28.27.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.28.27.3.1">47.02</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.28.27.4"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.28.27.4.1">99.37</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.28.27.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.28.27.5.1">70.75</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.29.28.1">GPT-3.5 Davinci</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch4.T8.1.29.28.2" rowspan="3"><span class="ltx_text" id="Ch4.T8.1.29.28.2.1">2-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.29.28.3">46.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.29.28.4">49.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.29.28.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.29.28.5.1">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T8.1.29.28.6">73.9</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.30.29">
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.30.29.1">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.30.29.2"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.30.29.2.1">45.95</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.30.29.3">49.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.30.29.4">74.53</td>
<td class="ltx_td ltx_align_center" id="Ch4.T8.1.30.29.5">74.63</td>
</tr>
<tr class="ltx_tr" id="Ch4.T8.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T8.1.31.30.1">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T8.1.31.30.2">45.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T8.1.31.30.3"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.31.30.3.1">50.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T8.1.31.30.4">79.29</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T8.1.31.30.5"><span class="ltx_text ltx_font_bold" id="Ch4.T8.1.31.30.5.1">74.9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.8: </span>Comparing GPT-3.5 <em class="ltx_emph ltx_font_italic" id="Ch4.T8.5.1">text-davinci-003</em> to ChatGPT models <em class="ltx_emph ltx_font_italic" id="Ch4.T8.6.2">gpt-3.5-turbo</em> and <em class="ltx_emph ltx_font_italic" id="Ch4.T8.7.3">gpt-4</em> for zero-shot and few-shot translation with 2 fuzzy matches</figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch4.S10">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.10     BLOOM and BLOOMZ</h3>
<div class="ltx_para" id="Ch4.S10.p1">
<p class="ltx_p" id="Ch4.S10.p1.1">In this section, we compare GPT-3.5 to open-source multilingual models, namely BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite> and BLOOMZ <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib131" title="">2022</a>)</cite>. While BLOOM is a general-purpose LLM, BLOOMZ belongs to a family of models capable of following human instructions in a zero-shot manner.</p>
</div>
<div class="ltx_para" id="Ch4.S10.p2">
<p class="ltx_p" id="Ch4.S10.p2.1">We use BLOOM and BLOOMZ via the Hugging Face’s Inference API.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/inference-api" title="">https://huggingface.co/inference-api</a></span></span></span> As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S3" title="4.3 Experimental Setup ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.3</span></a>, recommended (sampling) parameters for translation with <span class="ltx_text" id="Ch4.S10.p2.1.1">GPT-3.5</span> are top-p 1 and temperature up to 0.3. For BLOOM, the same parameters are not good for translation.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>Using lower sampling values of top-p and temperature such as 0.9 and 0.1, respectively, can generate good outputs. However, greedy search shows better translation performance.</span></span></span> We found that “greedy search” achieves better results for BLOOM, which are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.T9" title="Table 4.9 ‣ 4.10 BLOOM and BLOOMZ ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.9</span></a>. We use a batch size of 1, and set the <math alttext="max\_new\_tokens" class="ltx_Math" display="inline" id="Ch4.S10.p2.1.m1.1"><semantics id="Ch4.S10.p2.1.m1.1a"><mrow id="Ch4.S10.p2.1.m1.1.1" xref="Ch4.S10.p2.1.m1.1.1.cmml"><mi id="Ch4.S10.p2.1.m1.1.1.2" xref="Ch4.S10.p2.1.m1.1.1.2.cmml">m</mi><mo id="Ch4.S10.p2.1.m1.1.1.1" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.3" xref="Ch4.S10.p2.1.m1.1.1.3.cmml">a</mi><mo id="Ch4.S10.p2.1.m1.1.1.1a" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.4" xref="Ch4.S10.p2.1.m1.1.1.4.cmml">x</mi><mo id="Ch4.S10.p2.1.m1.1.1.1b" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.5" mathvariant="normal" xref="Ch4.S10.p2.1.m1.1.1.5.cmml">_</mi><mo id="Ch4.S10.p2.1.m1.1.1.1c" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.6" xref="Ch4.S10.p2.1.m1.1.1.6.cmml">n</mi><mo id="Ch4.S10.p2.1.m1.1.1.1d" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.7" xref="Ch4.S10.p2.1.m1.1.1.7.cmml">e</mi><mo id="Ch4.S10.p2.1.m1.1.1.1e" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.8" xref="Ch4.S10.p2.1.m1.1.1.8.cmml">w</mi><mo id="Ch4.S10.p2.1.m1.1.1.1f" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.9" mathvariant="normal" xref="Ch4.S10.p2.1.m1.1.1.9.cmml">_</mi><mo id="Ch4.S10.p2.1.m1.1.1.1g" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.10" xref="Ch4.S10.p2.1.m1.1.1.10.cmml">t</mi><mo id="Ch4.S10.p2.1.m1.1.1.1h" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.11" xref="Ch4.S10.p2.1.m1.1.1.11.cmml">o</mi><mo id="Ch4.S10.p2.1.m1.1.1.1i" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.12" xref="Ch4.S10.p2.1.m1.1.1.12.cmml">k</mi><mo id="Ch4.S10.p2.1.m1.1.1.1j" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.13" xref="Ch4.S10.p2.1.m1.1.1.13.cmml">e</mi><mo id="Ch4.S10.p2.1.m1.1.1.1k" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.14" xref="Ch4.S10.p2.1.m1.1.1.14.cmml">n</mi><mo id="Ch4.S10.p2.1.m1.1.1.1l" xref="Ch4.S10.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="Ch4.S10.p2.1.m1.1.1.15" xref="Ch4.S10.p2.1.m1.1.1.15.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch4.S10.p2.1.m1.1b"><apply id="Ch4.S10.p2.1.m1.1.1.cmml" xref="Ch4.S10.p2.1.m1.1.1"><times id="Ch4.S10.p2.1.m1.1.1.1.cmml" xref="Ch4.S10.p2.1.m1.1.1.1"></times><ci id="Ch4.S10.p2.1.m1.1.1.2.cmml" xref="Ch4.S10.p2.1.m1.1.1.2">𝑚</ci><ci id="Ch4.S10.p2.1.m1.1.1.3.cmml" xref="Ch4.S10.p2.1.m1.1.1.3">𝑎</ci><ci id="Ch4.S10.p2.1.m1.1.1.4.cmml" xref="Ch4.S10.p2.1.m1.1.1.4">𝑥</ci><ci id="Ch4.S10.p2.1.m1.1.1.5.cmml" xref="Ch4.S10.p2.1.m1.1.1.5">_</ci><ci id="Ch4.S10.p2.1.m1.1.1.6.cmml" xref="Ch4.S10.p2.1.m1.1.1.6">𝑛</ci><ci id="Ch4.S10.p2.1.m1.1.1.7.cmml" xref="Ch4.S10.p2.1.m1.1.1.7">𝑒</ci><ci id="Ch4.S10.p2.1.m1.1.1.8.cmml" xref="Ch4.S10.p2.1.m1.1.1.8">𝑤</ci><ci id="Ch4.S10.p2.1.m1.1.1.9.cmml" xref="Ch4.S10.p2.1.m1.1.1.9">_</ci><ci id="Ch4.S10.p2.1.m1.1.1.10.cmml" xref="Ch4.S10.p2.1.m1.1.1.10">𝑡</ci><ci id="Ch4.S10.p2.1.m1.1.1.11.cmml" xref="Ch4.S10.p2.1.m1.1.1.11">𝑜</ci><ci id="Ch4.S10.p2.1.m1.1.1.12.cmml" xref="Ch4.S10.p2.1.m1.1.1.12">𝑘</ci><ci id="Ch4.S10.p2.1.m1.1.1.13.cmml" xref="Ch4.S10.p2.1.m1.1.1.13">𝑒</ci><ci id="Ch4.S10.p2.1.m1.1.1.14.cmml" xref="Ch4.S10.p2.1.m1.1.1.14">𝑛</ci><ci id="Ch4.S10.p2.1.m1.1.1.15.cmml" xref="Ch4.S10.p2.1.m1.1.1.15">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S10.p2.1.m1.1c">max\_new\_tokens</annotation><annotation encoding="application/x-llamapun" id="Ch4.S10.p2.1.m1.1d">italic_m italic_a italic_x _ italic_n italic_e italic_w _ italic_t italic_o italic_k italic_e italic_n italic_s</annotation></semantics></math> parameter to be double the number of words of the source sentence if it is less than 250, the maximum number of new tokens allowed by BLOOM’s API; otherwise, we set it to 250 tokens. For comparison purposes, we use the same values for BLOOMZ.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>BLOOMZ is trained to generate the required output only; however, using BLOOM, we had to truncate over-generated text outputs, excluding anything generated in a new line.</span></span></span></p>
</div>
<div class="ltx_para" id="Ch4.S10.p3">
<p class="ltx_p" id="Ch4.S10.p3.1">When providing each system with two fuzzy matches, generally GPT-3.5 outperforms both BLOOM and BLOOMZ for most language pairs, except English-to-Arabic translation. The English-to-French translation quality of BLOOM and <span class="ltx_text" id="Ch4.S10.p3.1.1">GPT-3.5</span> is comparable.</p>
</div>
<figure class="ltx_table" id="Ch4.T9">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch4.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch4.T9.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch4.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch4.T9.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T9.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.3.1">spBLEU ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T9.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.4.1">chrF++ ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T9.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.5.1">TER ↓</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch4.T9.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.1.1.6.1">COMET ↑</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch4.T9.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.2.1.1.1">EN-AR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.2.1.2">BLOOM fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.2.1.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.2.1.3.1">43.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.2.1.4"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.2.1.4.1">59.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.2.1.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.2.1.5.1">57.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.2.1.6"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.2.1.6.1">67.36</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.3.2.1">BLOOMZ fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.3.2.2">36.29</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.3.2.3">53.33</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.3.2.4">66.86</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.3.2.5">58.4</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.4.3.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.4.3.2">38.41</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.4.3.3">56.57</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.4.3.4">62.31</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.4.3.5">57.36</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch4.T9.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.5.4.1.1">EN-ES</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.5.4.2">BLOOM fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.5.4.3">57.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.5.4.4">74.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.5.4.5">34.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.5.4.6">86.48</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.6.5.1">BLOOMZ fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.6.5.2">53.07</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.6.5.3">70.44</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.6.5.4">40.45</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.6.5.5">81.38</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.7.6.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.7.6.2"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.7.6.2.1">59.64</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.7.6.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.7.6.3.1">75.83</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.7.6.4"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.7.6.4.1">32.56</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.7.6.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.7.6.5.1">90.37</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch4.T9.1.8.7.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.8.7.1.1">EN-FR</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.8.7.2">BLOOM fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.8.7.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.8.7.3.1">50.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.8.7.4">66.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.8.7.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.8.7.5.1">46.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.8.7.6">55.74</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.9.8.1">BLOOMZ fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.9.8.2">45.1</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.9.8.3">62.73</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.9.8.4">51.69</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.9.8.5">47.49</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.10.9.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.10.9.2">49.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.10.9.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.10.9.3.1">67.41</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.10.9.4">46.79</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.10.9.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.10.9.5.1">61.38</span></td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Ch4.T9.1.11.10.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.11.10.1.1">EN-RW</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.11.10.2">BLOOM fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.11.10.3">10.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.11.10.4">31.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.11.10.5">91.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.11.10.6">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.12.11.1">BLOOMZ fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.12.11.2"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.12.11.2.1">12.26</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.12.11.3">35.44</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.12.11.4"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.12.11.4.1">88.36</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.12.11.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.13.12.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.13.12.2">12.23</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.13.12.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.13.12.3.1">36.66</span></td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.13.12.4">105.54</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.13.12.5">N/A</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="Ch4.T9.1.14.13.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.14.13.1.1">EN-ZH</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ch4.T9.1.14.13.2">BLOOM fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.14.13.3">40.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.14.13.4">40.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.14.13.5">75.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch4.T9.1.14.13.6">66.23</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ch4.T9.1.15.14.1">BLOOMZ fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.15.14.2">34.82</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.15.14.3">38.23</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.15.14.4">80.03</td>
<td class="ltx_td ltx_align_center" id="Ch4.T9.1.15.14.5">59.92</td>
</tr>
<tr class="ltx_tr" id="Ch4.T9.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ch4.T9.1.16.15.1">GPT-3 fuzzy 2-shot</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T9.1.16.15.2"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.16.15.2.1">46.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T9.1.16.15.3"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.16.15.3.1">49.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T9.1.16.15.4"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.16.15.4.1">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch4.T9.1.16.15.5"><span class="ltx_text ltx_font_bold" id="Ch4.T9.1.16.15.5.1">73.9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4.9: </span>Comparing GPT-3.5 to BLOOM and BLOOMZ for few-shot translation with 2 fuzzy matches</figcaption>
</figure>
</section>
<section class="ltx_section" id="Ch4.S11">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.11     Conclusion</h3>
<div class="ltx_para" id="Ch4.S11.p1">
<p class="ltx_p" id="Ch4.S11.p1.1">In this work, we conducted several experiments to assess the performance of GPT-3.5 across multiple translation tasks, namely adaptive MT using fuzzy matches (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S4" title="4.4 Adaptive MT with Fuzzy Matches ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.4</span></a>), MT post-editing (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6" title="4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.6</span></a>), terminology extraction <span class="ltx_text" id="Ch4.S11.p1.1.1">(cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S7" title="4.7 Bilingual Terminology Extraction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.7</span></a>)</span>, and terminology-constrained MT (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S8" title="4.8 Terminology-Constrained MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.8</span></a>). Moreover, we compared its translation quality with strong encoder-decoder MT systems. Generally speaking, results obtained from these experiments are very promising. While some high-resource languages such as English-to-French, English-to-Spanish and even English-to-Chinese show excellent results, other languages have lower support either because they are low-resource languages such as English-to-Kinyarwanda or because of <span class="ltx_text" id="Ch4.S11.p1.1.2">issues</span> in the <span class="ltx_text" id="Ch4.S11.p1.1.3">GPT-3.5</span> tokeniser such as English-to-Arabic. Nevertheless, when we used GPT-3.5 for MT post-editing of the English-to-Arabic translation obtained from OPUS, the quality significantly surpassed that obtained from both OPUS and Google Translation API. This means that different pipelines can be adopted in production for different language pairs, based on the level of support of these languages by an LLM.</p>
</div>
<div class="ltx_para" id="Ch4.S11.p2">
<p class="ltx_p" id="Ch4.S11.p2.1">Furthermore, we briefly compared <span class="ltx_text" id="Ch4.S11.p2.1.1">GPT-3.5</span> translation quality with open-source LLMs such as BLOOM and BLOOMZ. In the future, we would like to expand our experiments with open-source LLMs to cover more aspects.</p>
</div>
<div class="ltx_para" id="Ch4.S11.p3">
<p class="ltx_p" id="Ch4.S11.p3.1">For adaptive MT with fuzzy matches, it would be interesting to investigate <em class="ltx_emph ltx_font_italic" id="Ch4.S11.p3.1.1">dynamic</em> few-shot example selection. For instance, instead of selecting 5 fuzzy matches for all sentences, only high-quality fuzzy matches up to a certain similarity score are used. Similarly, when incorporating glossary terms or MT outputs from other systems, only those with certain quality characteristics are utilised. This can potentially enhance performance gains.</p>
</div>
<div class="ltx_para" id="Ch4.S11.p4">
<p class="ltx_p" id="Ch4.S11.p4.1">For terminology extraction, we would like to try “phrases” instead of “terms”. This would generate longer strings. We would like to see the effect of using such longer phrases, especially for low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch4.S11.p5">
<p class="ltx_p" id="Ch4.S11.p5.1">This work mainly aims at understanding the quality and level of support that LLMs can achieve (out of the box) for a range of translation tasks across diverse language pairs. In the future, we might consider starting with fine-tuning the model, and then conducting similar experiments.<span class="ltx_note ltx_role_footnote" id="Ch4.footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates preliminary experiments of fine-tuning Mistral 7B for the purpose of adaptive MT.</span></span></span> This can be especially beneficial for low-resource languages and rare domains, and can help enhance quality and efficiency.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Ch4.S12">
<h3 class="ltx_title ltx_font_bold ltx_title_section">4.12     Prompts</h3>
<div class="ltx_para" id="Ch4.S12.p1">
<p class="ltx_p" id="Ch4.S12.p1.1">These are some examples of the prompts we used for our experiments.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<section class="ltx_subsubsection" id="Ch4.S12.SS0.SSS1">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.12.0.1 </span>Zero-shot Translation</h5>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS1.p1">
<svg class="ltx_picture" height="45.63" id="Ch4.S12.SS0.SSS1.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,45.63) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 29.58 C 0 32.52 2.38 34.91 5.32 34.91 L 243.75 34.91 C 246.68 34.91 249.07 32.52 249.07 29.58 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 29.58 C 1.38 31.76 3.15 33.52 5.32 33.52 L 243.75 33.52 C 245.92 33.52 247.68 31.76 247.68 29.58 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 23.09)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 326.02 22.54 C 327.76 22.54 329.17 21.13 329.17 19.39 L 329.17 3.15 C 329.17 1.41 327.76 0 326.02 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 326.02 21.35 C 327.11 21.35 327.99 20.47 327.99 19.39 L 327.99 3.15 C 327.99 2.06 327.11 1.18 326.02 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS1.p1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-AR zero-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I1">
<span class="ltx_item" id="Ch4.S12.I1.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I1.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I1.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I1.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I1.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I1.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I1.ix1.p1.1.m1.1.1" xref="Ch4.S12.I1.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I1.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I1.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I1.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I1.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I1.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I1.ix1.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I1.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I1.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I1.ix1.p1.2.m2.1.1" xref="Ch4.S12.I1.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I1.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I1.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I1.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I1.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I1.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I1.ix1.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I1.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I1.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I1.ix2.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I1.ix2.p1.1.1">Arabic:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsubsection" id="Ch4.S12.SS0.SSS2">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.12.0.2 </span>Adaptive MT with Fuzzy Matches</h5>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS2.p1">
<svg class="ltx_picture" height="63.62" id="Ch4.S12.SS0.SSS2.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,63.62) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 47.57 C 0 50.51 2.38 52.89 5.32 52.89 L 243.75 52.89 C 246.68 52.89 249.07 50.51 249.07 47.57 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 47.57 C 1.38 49.75 3.15 51.51 5.32 51.51 L 243.75 51.51 C 245.92 51.51 247.68 49.75 247.68 47.57 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 41.08)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 322.91 22.54 C 324.65 22.54 326.06 21.13 326.06 19.39 L 326.06 3.15 C 326.06 1.41 324.65 0 322.91 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 322.91 21.35 C 324 21.35 324.88 20.47 324.88 19.39 L 324.88 3.15 C 324.88 2.06 324 1.18 322.91 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS2.p1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-AR two-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I2">
<span class="ltx_item" id="Ch4.S12.I2.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I2.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I2.ix1.p1.1.m1.1.1" xref="Ch4.S12.I2.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I2.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I2.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix1.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I2.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I2.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I2.ix1.p1.2.m2.1.1" xref="Ch4.S12.I2.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I2.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I2.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I2.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix2.p1.2.1">Arabic: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I2.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I2.ix2.p1.1.m1.1.1" xref="Ch4.S12.I2.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I2.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I2.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix2.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I2.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I2.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I2.ix2.p1.2.m2.1.1" xref="Ch4.S12.I2.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I2.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I2.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I2.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix3.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix3.p1.1.m1.1"><semantics id="Ch4.S12.I2.ix3.p1.1.m1.1a"><mo id="Ch4.S12.I2.ix3.p1.1.m1.1.1" xref="Ch4.S12.I2.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix3.p1.1.m1.1b"><lt id="Ch4.S12.I2.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S12.I2.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix3.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I2.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix3.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix3.p1.2.m2.1"><semantics id="Ch4.S12.I2.ix3.p1.2.m2.1a"><mo id="Ch4.S12.I2.ix3.p1.2.m2.1.1" xref="Ch4.S12.I2.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix3.p1.2.m2.1b"><gt id="Ch4.S12.I2.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S12.I2.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I2.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix4.p1.2.1">Arabic: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I2.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I2.ix4.p1.1.m1.1.1" xref="Ch4.S12.I2.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I2.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I2.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix4.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I2.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I2.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I2.ix4.p1.2.m2.1.1" xref="Ch4.S12.I2.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I2.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I2.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I2.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix5.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix5.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix5.p1.1.m1.1"><semantics id="Ch4.S12.I2.ix5.p1.1.m1.1a"><mo id="Ch4.S12.I2.ix5.p1.1.m1.1.1" xref="Ch4.S12.I2.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix5.p1.1.m1.1b"><lt id="Ch4.S12.I2.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S12.I2.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix5.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I2.ix5.p1.2.m2.1"><semantics id="Ch4.S12.I2.ix5.p1.2.m2.1a"><mo id="Ch4.S12.I2.ix5.p1.2.m2.1.1" xref="Ch4.S12.I2.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I2.ix5.p1.2.m2.1b"><gt id="Ch4.S12.I2.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S12.I2.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I2.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I2.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I2.ix5.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I2.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I2.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I2.ix6.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I2.ix6.p1.1.1">Arabic:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsubsection" id="Ch4.S12.SS0.SSS3">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.12.0.3 </span>MT Post-editing</h5>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS3.p1">
<svg class="ltx_picture" height="77.38" id="Ch4.S12.SS0.SSS3.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,77.38) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 61.33 C 0 64.27 2.38 66.65 5.32 66.65 L 243.75 66.65 C 246.68 66.65 249.07 64.27 249.07 61.33 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 61.33 C 1.38 63.51 3.15 65.27 5.32 65.27 L 243.75 65.27 C 245.92 65.27 247.68 63.51 247.68 61.33 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 54.84)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 305.31 22.54 C 307.05 22.54 308.46 21.13 308.46 19.39 L 308.46 3.15 C 308.46 1.41 307.05 0 305.31 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 305.31 21.35 C 306.39 21.35 307.28 20.47 307.28 19.39 L 307.28 3.15 C 307.28 2.06 306.39 1.18 305.31 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS3.p1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-ZH two-shot + 1-MT</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="44.2" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I3">
<span class="ltx_item" id="Ch4.S12.I3.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix1.p1.1.m1.1.1" xref="Ch4.S12.I3.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix1.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I3.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix1.p1.2.m2.1.1" xref="Ch4.S12.I3.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix2.p1.2.1">Chinese: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix2.p1.1.m1.1.1" xref="Ch4.S12.I3.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix2.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I3.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix2.p1.2.m2.1.1" xref="Ch4.S12.I3.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix3.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix3.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix3.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix3.p1.1.m1.1.1" xref="Ch4.S12.I3.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix3.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix3.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I3.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix3.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix3.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix3.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix3.p1.2.m2.1.1" xref="Ch4.S12.I3.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix3.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix4.p1.2.1">Chinese: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix4.p1.1.m1.1.1" xref="Ch4.S12.I3.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix4.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I3.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix4.p1.2.m2.1.1" xref="Ch4.S12.I3.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix5.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix5.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix5.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix5.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix5.p1.1.m1.1.1" xref="Ch4.S12.I3.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix5.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix5.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix5.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix5.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix5.p1.2.m2.1.1" xref="Ch4.S12.I3.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix5.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix5.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix6.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix6.p1.2.1">MT: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix6.p1.1.m1.1"><semantics id="Ch4.S12.I3.ix6.p1.1.m1.1a"><mo id="Ch4.S12.I3.ix6.p1.1.m1.1.1" xref="Ch4.S12.I3.ix6.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix6.p1.1.m1.1b"><lt id="Ch4.S12.I3.ix6.p1.1.m1.1.1.cmml" xref="Ch4.S12.I3.ix6.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix6.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix6.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix6.p1.2.2">mt_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I3.ix6.p1.2.m2.1"><semantics id="Ch4.S12.I3.ix6.p1.2.m2.1a"><mo id="Ch4.S12.I3.ix6.p1.2.m2.1.1" xref="Ch4.S12.I3.ix6.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I3.ix6.p1.2.m2.1b"><gt id="Ch4.S12.I3.ix6.p1.2.m2.1.1.cmml" xref="Ch4.S12.I3.ix6.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I3.ix6.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I3.ix6.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I3.ix6.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I3.ix7" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I3.ix7.p1">
<span class="ltx_p" id="Ch4.S12.I3.ix7.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I3.ix7.p1.1.1">Chinese:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS3.p2">
<svg class="ltx_picture" height="80.22" id="Ch4.S12.SS0.SSS3.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,80.22) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 64.18 C 0 67.12 2.38 69.5 5.32 69.5 L 243.75 69.5 C 246.68 69.5 249.07 67.12 249.07 64.18 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 64.18 C 1.38 66.35 3.15 68.11 5.32 68.11 L 243.75 68.11 C 245.92 68.11 247.68 66.35 247.68 64.18 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 57.69)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 312.99 22.54 C 314.73 22.54 316.14 21.13 316.14 19.39 L 316.14 3.15 C 316.14 1.41 314.73 0 312.99 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 312.99 21.35 C 314.08 21.35 314.96 20.47 314.96 19.39 L 314.96 3.15 C 314.96 2.06 314.08 1.18 312.99 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS3.p2.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-ZH two-shot + all-MT</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS3.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS3.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS3.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS3.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS3.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I4">
<span class="ltx_item" id="Ch4.S12.I4.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix1.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix1.p1.1.m1.1.1" xref="Ch4.S12.I4.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix1.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix1.p1.2.m2.1.1" xref="Ch4.S12.I4.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix2.p1.2.1">MT: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix2.p1.1.m1.1.1" xref="Ch4.S12.I4.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix2.p1.2.2">mt_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix2.p1.2.m2.1.1" xref="Ch4.S12.I4.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix3.p1.2.1">Chinese: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix3.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix3.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix3.p1.1.m1.1.1" xref="Ch4.S12.I4.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix3.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix3.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix3.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix3.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix3.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix3.p1.2.m2.1.1" xref="Ch4.S12.I4.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix3.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix4.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix4.p1.1.m1.1.1" xref="Ch4.S12.I4.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix4.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix4.p1.2.m2.1.1" xref="Ch4.S12.I4.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix5.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix5.p1.2.1">MT: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix5.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix5.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix5.p1.1.m1.1.1" xref="Ch4.S12.I4.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix5.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix5.p1.2.2">mt_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix5.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix5.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix5.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix5.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix5.p1.2.m2.1.1" xref="Ch4.S12.I4.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix5.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix5.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix6.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix6.p1.2.1">Chinese: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix6.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix6.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix6.p1.1.m1.1.1" xref="Ch4.S12.I4.ix6.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix6.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix6.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix6.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix6.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix6.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix6.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I4.ix6.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix6.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix6.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix6.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix6.p1.2.m2.1.1" xref="Ch4.S12.I4.ix6.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix6.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix6.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix6.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix6.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix6.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix6.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix7" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix7.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix7.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix7.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix7.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix7.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix7.p1.1.m1.1.1" xref="Ch4.S12.I4.ix7.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix7.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix7.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix7.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix7.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix7.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix7.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix7.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix7.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix7.p1.2.m2.1.1" xref="Ch4.S12.I4.ix7.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix7.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix7.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix7.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix7.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix7.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix7.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix8" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix8.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix8.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix8.p1.2.1">MT: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix8.p1.1.m1.1"><semantics id="Ch4.S12.I4.ix8.p1.1.m1.1a"><mo id="Ch4.S12.I4.ix8.p1.1.m1.1.1" xref="Ch4.S12.I4.ix8.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix8.p1.1.m1.1b"><lt id="Ch4.S12.I4.ix8.p1.1.m1.1.1.cmml" xref="Ch4.S12.I4.ix8.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix8.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix8.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix8.p1.2.2">mt_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I4.ix8.p1.2.m2.1"><semantics id="Ch4.S12.I4.ix8.p1.2.m2.1a"><mo id="Ch4.S12.I4.ix8.p1.2.m2.1.1" xref="Ch4.S12.I4.ix8.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I4.ix8.p1.2.m2.1b"><gt id="Ch4.S12.I4.ix8.p1.2.m2.1.1.cmml" xref="Ch4.S12.I4.ix8.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I4.ix8.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I4.ix8.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I4.ix8.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I4.ix9" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I4.ix9.p1">
<span class="ltx_p" id="Ch4.S12.I4.ix9.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I4.ix9.p1.1.1">Chinese:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsubsection" id="Ch4.S12.SS0.SSS4">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.12.0.4 </span>Terminology Extraction</h5>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS4.p1">
<svg class="ltx_picture" height="80.07" id="Ch4.S12.SS0.SSS4.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,80.07) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 64.02 C 0 66.96 2.38 69.34 5.32 69.34 L 243.75 69.34 C 246.68 69.34 249.07 66.96 249.07 64.02 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 64.02 C 1.38 66.2 3.15 67.96 5.32 67.96 L 243.75 67.96 C 245.92 67.96 247.68 66.2 247.68 64.02 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 57.53)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 289.36 22.54 C 291.1 22.54 292.51 21.13 292.51 19.39 L 292.51 3.15 C 292.51 1.41 291.1 0 289.36 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 289.36 21.35 C 290.44 21.35 291.33 20.47 291.33 19.39 L 291.33 3.15 C 291.33 2.06 290.44 1.18 289.36 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS4.p1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: terminology extraction</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 1.17 13.19)"><foreignobject height="46.89" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="226.83"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:163.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I5">
<span class="ltx_item" id="Ch4.S12.I5.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I5.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I5.ix1.p1.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I5.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I5.ix1.p1.1.m1.1.1" xref="Ch4.S12.I5.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I5.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I5.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I5.ix1.p1.4.1">source_lang</em><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I5.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I5.ix1.p1.2.m2.1.1" xref="Ch4.S12.I5.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I5.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I5.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix1.p1.4.2">: </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix1.p1.3.m3.1"><semantics id="Ch4.S12.I5.ix1.p1.3.m3.1a"><mo id="Ch4.S12.I5.ix1.p1.3.m3.1.1" xref="Ch4.S12.I5.ix1.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix1.p1.3.m3.1b"><lt id="Ch4.S12.I5.ix1.p1.3.m3.1.1.cmml" xref="Ch4.S12.I5.ix1.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix1.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix1.p1.3.m3.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix1.p1.4.3">source_sentence</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix1.p1.4.m4.1"><semantics id="Ch4.S12.I5.ix1.p1.4.m4.1a"><mo id="Ch4.S12.I5.ix1.p1.4.m4.1.1" xref="Ch4.S12.I5.ix1.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix1.p1.4.m4.1b"><gt id="Ch4.S12.I5.ix1.p1.4.m4.1.1.cmml" xref="Ch4.S12.I5.ix1.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix1.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix1.p1.4.m4.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix1.p1.4.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I5.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I5.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I5.ix2.p1.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I5.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I5.ix2.p1.1.m1.1.1" xref="Ch4.S12.I5.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I5.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I5.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I5.ix2.p1.4.1">target_lang</em><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I5.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I5.ix2.p1.2.m2.1.1" xref="Ch4.S12.I5.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I5.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I5.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix2.p1.4.2">: </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix2.p1.3.m3.1"><semantics id="Ch4.S12.I5.ix2.p1.3.m3.1a"><mo id="Ch4.S12.I5.ix2.p1.3.m3.1.1" xref="Ch4.S12.I5.ix2.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix2.p1.3.m3.1b"><lt id="Ch4.S12.I5.ix2.p1.3.m3.1.1.cmml" xref="Ch4.S12.I5.ix2.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix2.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix2.p1.3.m3.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix2.p1.4.3">target_sentence</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix2.p1.4.m4.1"><semantics id="Ch4.S12.I5.ix2.p1.4.m4.1a"><mo id="Ch4.S12.I5.ix2.p1.4.m4.1.1" xref="Ch4.S12.I5.ix2.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix2.p1.4.m4.1b"><gt id="Ch4.S12.I5.ix2.p1.4.m4.1.1.cmml" xref="Ch4.S12.I5.ix2.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix2.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix2.p1.4.m4.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix2.p1.4.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I5.ix3" style="list-style-type:none;"></span>
<span class="ltx_item" id="Ch4.S12.I5.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I5.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I5.ix4.p1.8"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.1">Extract </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I5.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I5.ix4.p1.1.m1.1.1" xref="Ch4.S12.I5.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I5.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.2">number</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I5.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I5.ix4.p1.2.m2.1.1" xref="Ch4.S12.I5.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I5.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.3"> terms from the above sentence pair. Type each </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.3.m3.1"><semantics id="Ch4.S12.I5.ix4.p1.3.m3.1a"><mo id="Ch4.S12.I5.ix4.p1.3.m3.1.1" xref="Ch4.S12.I5.ix4.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.3.m3.1b"><lt id="Ch4.S12.I5.ix4.p1.3.m3.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.3.m3.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.4">source_lang</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.4.m4.1"><semantics id="Ch4.S12.I5.ix4.p1.4.m4.1a"><mo id="Ch4.S12.I5.ix4.p1.4.m4.1.1" xref="Ch4.S12.I5.ix4.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.4.m4.1b"><gt id="Ch4.S12.I5.ix4.p1.4.m4.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.4.m4.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.5"> term and its </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.5.m5.1"><semantics id="Ch4.S12.I5.ix4.p1.5.m5.1a"><mo id="Ch4.S12.I5.ix4.p1.5.m5.1.1" xref="Ch4.S12.I5.ix4.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.5.m5.1b"><lt id="Ch4.S12.I5.ix4.p1.5.m5.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.5.m5.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.6">target_lang</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.6.m6.1"><semantics id="Ch4.S12.I5.ix4.p1.6.m6.1a"><mo id="Ch4.S12.I5.ix4.p1.6.m6.1.1" xref="Ch4.S12.I5.ix4.p1.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.6.m6.1b"><gt id="Ch4.S12.I5.ix4.p1.6.m6.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.6.m6.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.6.m6.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.7"> equivalent in one line, separated by ’</span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.7.m7.1"><semantics id="Ch4.S12.I5.ix4.p1.7.m7.1a"><mo id="Ch4.S12.I5.ix4.p1.7.m7.1.1" xref="Ch4.S12.I5.ix4.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.7.m7.1b"><lt id="Ch4.S12.I5.ix4.p1.7.m7.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.7.m7.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.8">separator</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I5.ix4.p1.8.m8.1"><semantics id="Ch4.S12.I5.ix4.p1.8.m8.1a"><mo id="Ch4.S12.I5.ix4.p1.8.m8.1.1" xref="Ch4.S12.I5.ix4.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I5.ix4.p1.8.m8.1b"><gt id="Ch4.S12.I5.ix4.p1.8.m8.1.1.cmml" xref="Ch4.S12.I5.ix4.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I5.ix4.p1.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I5.ix4.p1.8.m8.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I5.ix4.p1.8.9">’.</span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I5.ix5" style="list-style-type:none;"></span>
<span class="ltx_item" id="Ch4.S12.I5.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I5.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I5.ix6.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I5.ix6.p1.1.1">1.</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsubsection" id="Ch4.S12.SS0.SSS5">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.12.0.5 </span>Terminology-constrained MT</h5>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS5.p1">
<svg class="ltx_picture" height="63.62" id="Ch4.S12.SS0.SSS5.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,63.62) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 47.57 C 0 50.51 2.38 52.89 5.32 52.89 L 243.75 52.89 C 246.68 52.89 249.07 50.51 249.07 47.57 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 47.57 C 1.38 49.75 3.15 51.51 5.32 51.51 L 243.75 51.51 C 245.92 51.51 247.68 49.75 247.68 47.57 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 41.08)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 359.27 22.54 C 361.01 22.54 362.42 21.13 362.42 19.39 L 362.42 3.15 C 362.42 1.41 361.01 0 359.27 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 359.27 21.35 C 360.36 21.35 361.24 20.47 361.24 19.39 L 361.24 3.15 C 361.24 2.06 360.36 1.18 359.27 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-ES zero-shot + glossary terms</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 1.17 13.19)"><foreignobject height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="226.83"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:163.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I6">
<span class="ltx_item" id="Ch4.S12.I6.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I6.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I6.ix1.p1.15"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I6.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I6.ix1.p1.1.m1.1.1" xref="Ch4.S12.I6.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I6.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.2">src_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I6.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I6.ix1.p1.2.m2.1.1" xref="Ch4.S12.I6.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I6.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.4"> </span><math alttext="=" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.3.m3.1"><semantics id="Ch4.S12.I6.ix1.p1.3.m3.1a"><mo id="Ch4.S12.I6.ix1.p1.3.m3.1.1" xref="Ch4.S12.I6.ix1.p1.3.m3.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.3.m3.1b"><eq id="Ch4.S12.I6.ix1.p1.3.m3.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.3.m3.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.3.m3.1c">=</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.3.m3.1d">=</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.5"> </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.4.m4.1"><semantics id="Ch4.S12.I6.ix1.p1.4.m4.1a"><mo id="Ch4.S12.I6.ix1.p1.4.m4.1.1" xref="Ch4.S12.I6.ix1.p1.4.m4.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.4.m4.1b"><lt id="Ch4.S12.I6.ix1.p1.4.m4.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.4.m4.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.4.m4.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.4.m4.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.6">tgt_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.7"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.7.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.5.m5.1"><semantics id="Ch4.S12.I6.ix1.p1.5.m5.1a"><mo id="Ch4.S12.I6.ix1.p1.5.m5.1.1" xref="Ch4.S12.I6.ix1.p1.5.m5.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.5.m5.1b"><gt id="Ch4.S12.I6.ix1.p1.5.m5.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.5.m5.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.5.m5.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.5.m5.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.8"> - </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.6.m6.1"><semantics id="Ch4.S12.I6.ix1.p1.6.m6.1a"><mo id="Ch4.S12.I6.ix1.p1.6.m6.1.1" xref="Ch4.S12.I6.ix1.p1.6.m6.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.6.m6.1b"><lt id="Ch4.S12.I6.ix1.p1.6.m6.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.6.m6.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.6.m6.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.6.m6.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.9">src_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.10"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.10.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.7.m7.1"><semantics id="Ch4.S12.I6.ix1.p1.7.m7.1a"><mo id="Ch4.S12.I6.ix1.p1.7.m7.1.1" xref="Ch4.S12.I6.ix1.p1.7.m7.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.7.m7.1b"><gt id="Ch4.S12.I6.ix1.p1.7.m7.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.7.m7.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.7.m7.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.7.m7.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.11"> </span><math alttext="=" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.8.m8.1"><semantics id="Ch4.S12.I6.ix1.p1.8.m8.1a"><mo id="Ch4.S12.I6.ix1.p1.8.m8.1.1" xref="Ch4.S12.I6.ix1.p1.8.m8.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.8.m8.1b"><eq id="Ch4.S12.I6.ix1.p1.8.m8.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.8.m8.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.8.m8.1c">=</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.8.m8.1d">=</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.12"> </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.9.m9.1"><semantics id="Ch4.S12.I6.ix1.p1.9.m9.1a"><mo id="Ch4.S12.I6.ix1.p1.9.m9.1.1" xref="Ch4.S12.I6.ix1.p1.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.9.m9.1b"><lt id="Ch4.S12.I6.ix1.p1.9.m9.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.9.m9.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.9.m9.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.13">tgt_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.14"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.14.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.10.m10.1"><semantics id="Ch4.S12.I6.ix1.p1.10.m10.1a"><mo id="Ch4.S12.I6.ix1.p1.10.m10.1.1" xref="Ch4.S12.I6.ix1.p1.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.10.m10.1b"><gt id="Ch4.S12.I6.ix1.p1.10.m10.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.10.m10.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.10.m10.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.15"> … </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.11.m11.1"><semantics id="Ch4.S12.I6.ix1.p1.11.m11.1a"><mo id="Ch4.S12.I6.ix1.p1.11.m11.1.1" xref="Ch4.S12.I6.ix1.p1.11.m11.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.11.m11.1b"><lt id="Ch4.S12.I6.ix1.p1.11.m11.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.11.m11.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.11.m11.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.11.m11.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.16">src_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.17"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.17.1">5</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.12.m12.1"><semantics id="Ch4.S12.I6.ix1.p1.12.m12.1a"><mo id="Ch4.S12.I6.ix1.p1.12.m12.1.1" xref="Ch4.S12.I6.ix1.p1.12.m12.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.12.m12.1b"><gt id="Ch4.S12.I6.ix1.p1.12.m12.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.12.m12.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.12.m12.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.12.m12.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.18"> </span><math alttext="=" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.13.m13.1"><semantics id="Ch4.S12.I6.ix1.p1.13.m13.1a"><mo id="Ch4.S12.I6.ix1.p1.13.m13.1.1" xref="Ch4.S12.I6.ix1.p1.13.m13.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.13.m13.1b"><eq id="Ch4.S12.I6.ix1.p1.13.m13.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.13.m13.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.13.m13.1c">=</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.13.m13.1d">=</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.19"> </span><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.14.m14.1"><semantics id="Ch4.S12.I6.ix1.p1.14.m14.1a"><mo id="Ch4.S12.I6.ix1.p1.14.m14.1.1" xref="Ch4.S12.I6.ix1.p1.14.m14.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.14.m14.1b"><lt id="Ch4.S12.I6.ix1.p1.14.m14.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.14.m14.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.14.m14.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.14.m14.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.20">tgt_term</span><sub class="ltx_sub" id="Ch4.S12.I6.ix1.p1.15.21"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.21.1">5</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix1.p1.15.m15.1"><semantics id="Ch4.S12.I6.ix1.p1.15.m15.1a"><mo id="Ch4.S12.I6.ix1.p1.15.m15.1.1" xref="Ch4.S12.I6.ix1.p1.15.m15.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix1.p1.15.m15.1b"><gt id="Ch4.S12.I6.ix1.p1.15.m15.1.1.cmml" xref="Ch4.S12.I6.ix1.p1.15.m15.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix1.p1.15.m15.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix1.p1.15.m15.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix1.p1.15.22"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I6.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I6.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I6.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix2.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I6.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I6.ix2.p1.1.m1.1.1" xref="Ch4.S12.I6.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I6.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I6.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix2.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I6.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I6.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I6.ix2.p1.2.m2.1.1" xref="Ch4.S12.I6.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I6.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I6.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I6.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I6.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I6.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I6.ix2.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I6.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I6.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I6.ix3.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I6.ix3.p1.1.1">Spanish:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS5.p2">
<svg class="ltx_picture" height="84.37" id="Ch4.S12.SS0.SSS5.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,84.37) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 68.33 C 0 71.27 2.38 73.65 5.32 73.65 L 243.75 73.65 C 246.68 73.65 249.07 71.27 249.07 68.33 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 68.33 C 1.38 70.5 3.15 72.27 5.32 72.27 L 243.75 72.27 C 245.92 72.27 247.68 70.5 247.68 68.33 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 61.84)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 339.44 22.54 C 341.18 22.54 342.59 21.13 342.59 19.39 L 342.59 3.15 C 342.59 1.41 341.18 0 339.44 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 339.44 21.35 C 340.53 21.35 341.41 20.47 341.41 19.39 L 341.41 3.15 C 341.41 2.06 340.53 1.18 339.44 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p2.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-ES two-shot + fuzzy terms</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="51.2" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS5.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS5.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS5.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS5.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I7">
<span class="ltx_item" id="Ch4.S12.I7.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix1.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix1.p1.1.m1.1.1" xref="Ch4.S12.I7.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix1.p1.2.2">terms_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix1.p1.2.m2.1.1" xref="Ch4.S12.I7.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix2.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix2.p1.1.m1.1.1" xref="Ch4.S12.I7.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix2.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix2.p1.2.m2.1.1" xref="Ch4.S12.I7.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix3.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix3.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix3.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix3.p1.1.m1.1.1" xref="Ch4.S12.I7.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix3.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix3.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix3.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix3.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix3.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix3.p1.2.m2.1.1" xref="Ch4.S12.I7.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix3.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix4.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix4.p1.1.m1.1.1" xref="Ch4.S12.I7.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix4.p1.2.2">terms_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix4.p1.2.m2.1.1" xref="Ch4.S12.I7.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix5.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix5.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix5.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix5.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix5.p1.1.m1.1.1" xref="Ch4.S12.I7.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix5.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix5.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix5.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix5.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix5.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix5.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix5.p1.2.m2.1.1" xref="Ch4.S12.I7.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix5.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix5.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix6.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix6.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix6.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix6.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix6.p1.1.m1.1.1" xref="Ch4.S12.I7.ix6.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix6.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix6.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix6.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix6.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix6.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix6.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I7.ix6.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix6.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix6.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix6.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix6.p1.2.m2.1.1" xref="Ch4.S12.I7.ix6.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix6.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix6.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix6.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix6.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix6.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix6.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix7" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix7.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix7.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix7.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix7.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix7.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix7.p1.1.m1.1.1" xref="Ch4.S12.I7.ix7.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix7.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix7.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix7.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix7.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix7.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix7.p1.2.2">terms_from_fuzzy_matches</span><sub class="ltx_sub" id="Ch4.S12.I7.ix7.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix7.p1.2.3.1">1+2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix7.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix7.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix7.p1.2.m2.1.1" xref="Ch4.S12.I7.ix7.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix7.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix7.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix7.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix7.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix7.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix7.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix8" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix8.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix8.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix8.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix8.p1.1.m1.1"><semantics id="Ch4.S12.I7.ix8.p1.1.m1.1a"><mo id="Ch4.S12.I7.ix8.p1.1.m1.1.1" xref="Ch4.S12.I7.ix8.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix8.p1.1.m1.1b"><lt id="Ch4.S12.I7.ix8.p1.1.m1.1.1.cmml" xref="Ch4.S12.I7.ix8.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix8.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix8.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix8.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I7.ix8.p1.2.m2.1"><semantics id="Ch4.S12.I7.ix8.p1.2.m2.1a"><mo id="Ch4.S12.I7.ix8.p1.2.m2.1.1" xref="Ch4.S12.I7.ix8.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I7.ix8.p1.2.m2.1b"><gt id="Ch4.S12.I7.ix8.p1.2.m2.1.1.cmml" xref="Ch4.S12.I7.ix8.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I7.ix8.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I7.ix8.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I7.ix8.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I7.ix9" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I7.ix9.p1">
<span class="ltx_p" id="Ch4.S12.I7.ix9.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I7.ix9.p1.1.1">Spanish:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent" id="Ch4.S12.SS0.SSS5.p3">
<svg class="ltx_picture" height="80.22" id="Ch4.S12.SS0.SSS5.p3.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,80.22) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 64.18 C 0 67.12 2.38 69.5 5.32 69.5 L 243.75 69.5 C 246.68 69.5 249.07 67.12 249.07 64.18 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 64.18 C 1.38 66.35 3.15 68.11 5.32 68.11 L 243.75 68.11 C 245.92 68.11 247.68 66.35 247.68 64.18 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 57.69)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 356.16 22.54 C 357.9 22.54 359.31 21.13 359.31 19.39 L 359.31 3.15 C 359.31 1.41 357.9 0 356.16 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 356.16 21.35 C 357.25 21.35 358.13 20.47 358.13 19.39 L 358.13 3.15 C 358.13 2.06 357.25 1.18 356.16 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p3.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: EN-ES two-shot + glossary terms</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 13.19)"><foreignobject height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch4.S12.SS0.SSS5.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch4.S12.SS0.SSS5.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch4.S12.SS0.SSS5.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch4.S12.SS0.SSS5.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch4.S12.SS0.SSS5.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch4.S12.I8">
<span class="ltx_item" id="Ch4.S12.I8.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix1.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix1.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix1.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix1.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix1.p1.1.m1.1.1" xref="Ch4.S12.I8.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix1.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix1.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix1.p1.2.2">terms_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix1.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix1.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix1.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix1.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix1.p1.2.m2.1.1" xref="Ch4.S12.I8.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix1.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix1.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix1.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix2.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix2.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix2.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix2.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix2.p1.1.m1.1.1" xref="Ch4.S12.I8.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix2.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix2.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix2.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix2.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix2.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix2.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix2.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix2.p1.2.m2.1.1" xref="Ch4.S12.I8.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix2.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix2.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix2.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix3.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix3.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix3.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix3.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix3.p1.1.m1.1.1" xref="Ch4.S12.I8.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix3.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix3.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix3.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix3.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix3.p1.2.3.1">2</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix3.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix3.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix3.p1.2.m2.1.1" xref="Ch4.S12.I8.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix3.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix3.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix3.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix4.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix4.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix4.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix4.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix4.p1.1.m1.1.1" xref="Ch4.S12.I8.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix4.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix4.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix4.p1.2.2">terms_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix4.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix4.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix4.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix4.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix4.p1.2.m2.1.1" xref="Ch4.S12.I8.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix4.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix4.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix4.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix5.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix5.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix5.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix5.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix5.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix5.p1.1.m1.1.1" xref="Ch4.S12.I8.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix5.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix5.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix5.p1.2.2">source_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix5.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix5.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix5.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix5.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix5.p1.2.m2.1.1" xref="Ch4.S12.I8.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix5.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix5.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix5.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix6" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix6.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix6.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix6.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix6.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix6.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix6.p1.1.m1.1.1" xref="Ch4.S12.I8.ix6.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix6.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix6.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix6.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix6.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix6.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix6.p1.2.2">target_fuzzy_match</span><sub class="ltx_sub" id="Ch4.S12.I8.ix6.p1.2.3"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix6.p1.2.3.1">1</em></sub><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix6.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix6.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix6.p1.2.m2.1.1" xref="Ch4.S12.I8.ix6.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix6.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix6.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix6.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix6.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix6.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix6.p1.2.4"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix7" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix7.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix7.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix7.p1.2.1">Terms: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix7.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix7.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix7.p1.1.m1.1.1" xref="Ch4.S12.I8.ix7.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix7.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix7.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix7.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix7.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix7.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix7.p1.2.2">terms_from_glossary</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix7.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix7.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix7.p1.2.m2.1.1" xref="Ch4.S12.I8.ix7.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix7.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix7.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix7.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix7.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix7.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix7.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix8" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix8.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix8.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix8.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix8.p1.1.m1.1"><semantics id="Ch4.S12.I8.ix8.p1.1.m1.1a"><mo id="Ch4.S12.I8.ix8.p1.1.m1.1.1" xref="Ch4.S12.I8.ix8.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix8.p1.1.m1.1b"><lt id="Ch4.S12.I8.ix8.p1.1.m1.1.1.cmml" xref="Ch4.S12.I8.ix8.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix8.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix8.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix8.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch4.S12.I8.ix8.p1.2.m2.1"><semantics id="Ch4.S12.I8.ix8.p1.2.m2.1a"><mo id="Ch4.S12.I8.ix8.p1.2.m2.1.1" xref="Ch4.S12.I8.ix8.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch4.S12.I8.ix8.p1.2.m2.1b"><gt id="Ch4.S12.I8.ix8.p1.2.m2.1.1.cmml" xref="Ch4.S12.I8.ix8.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch4.S12.I8.ix8.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch4.S12.I8.ix8.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch4.S12.I8.ix8.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch4.S12.I8.ix9" style="list-style-type:none;">
<span class="ltx_para" id="Ch4.S12.I8.ix9.p1">
<span class="ltx_p" id="Ch4.S12.I8.ix9.p1.1"><em class="ltx_emph ltx_font_italic" id="Ch4.S12.I8.ix9.p1.1.1">Spanish:</em></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</section>
</section>
<section class="ltx_chapter" id="Ch5">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 5      Fine-tuning Large Language Models for Adaptive Machine Translation</h2>
<div class="ltx_para" id="Ch5.p1">
<p class="ltx_p" id="Ch5.p1.1">This chapter<span class="ltx_note ltx_role_footnote" id="Ch5.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>More details can be found in a preprint paper at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.12740" title="">https://arxiv.org/abs/2312.12740</a></span></span></span> demonstrates the setup and results of experiments with fine-tuning an LLM, namely Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, for “adaptive” MT. Hence, the model is not only fine-tuned for regular (zero-shot) translation, but also for adaptation to one fuzzy match (one-shot) at translation time <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>)</cite>. The experiments were conducted for Spanish-to-English medical adaptive MT.
The code used for these experiments is publicly available.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>GitHub repository: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning" title="">https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning</a></span></span></span></p>
</div>
<div class="ltx_para" id="Ch5.p2">
<p class="ltx_p" id="Ch5.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.T1" title="Table 5.1 ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a>, the last two rows show the results for fine-tuning Mistral 7B. The rest of the results are for baselines, i.e. without fine-tuning. As illustrated, fine-tuning has led to quality improvements in terms of both zero-shot and one-shot translation. The fine-tuned version of Mistral outperforms its own baseline (i.e. without fine-tuning) for both zero-shot and one-shot translation. Zero-shot translation quality of the fine-tuned Mistral outperforms ChatGPT “gpt-3.5-turbo”, while one-shot translation quality of the fine-tuned Mistral is on par with that of ChatGPT. Zero-shot translation of the fine-tuned Mistral is on par with NLLB 3.3B, while one-shot translation quality of the fine-tuned Mistral outperforms that of NLLB 3.3B. To conclude, fine-tuning an efficient LLM like Mistral 7B helps to produce a high-quality zero-shot translation comparable to that of MT task-oriented models such as NLLB 3.3B, while achieving adaptive gains of one-shot translation on par with commercial LLMs such as ChatGPT “gpt-3.5-turbo”.</p>
</div>
<figure class="ltx_table" id="Ch5.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.3.1">Context</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.4.1">BLEU ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.5.1">chrF++ ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.6.1">TER ↓</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch5.T1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.1.1.7.1">COMET ↑</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch5.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch5.T1.1.2.1.1" rowspan="8"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.2.1.1.1">ES-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.2" rowspan="2"><span class="ltx_text" id="Ch5.T1.1.2.1.2.1">NLLB 3.3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.3">Source only (zero-shot)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.4"><span class="ltx_text ltx_framed_underline" id="Ch5.T1.1.2.1.4.1">47.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.5">68.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.6">43.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.2.1.7">66.46</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.3.2.1">+ Fuzzy (one-shot)</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.3.2.2">47.42</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.3.2.3">68.77</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.3.2.4">45.26</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.3.2.5">64.57</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.1">ChatGPT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.2">Source only (zero-shot)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.3">44.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.4">68.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.5">44.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.4.3.6">74.48</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.1">“gpt-3.5-turbo”</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.2">+ Fuzzy (one-shot)</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.3">48.34</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.4">70.54</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.5">40.80</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.5.4.6">
<span class="ltx_text ltx_font_bold" id="Ch5.T1.1.5.4.6.1">80.25</span>*</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.1" rowspan="2"><span class="ltx_text" id="Ch5.T1.1.6.5.1.1">Mistral 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.2">Source only (zero-shot)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.3">42.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.4">66.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.5">46.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.6.5.6">69.56</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.7.6">
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.7.6.1">+ Fuzzy (one-shot)</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.7.6.2">47.35</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.7.6.3">69.25</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.7.6.4">42.53</td>
<td class="ltx_td ltx_align_center" id="Ch5.T1.1.7.6.5">76.37</td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.1">Mistral 7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.2">Source only (zero-shot)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.3">46.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.4"><span class="ltx_text ltx_framed_underline" id="Ch5.T1.1.8.7.4.1">69.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.5"><span class="ltx_text ltx_framed_underline" id="Ch5.T1.1.8.7.5.1">41.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch5.T1.1.8.7.6"><span class="ltx_text ltx_framed_underline" id="Ch5.T1.1.8.7.6.1">77.44</span></td>
</tr>
<tr class="ltx_tr" id="Ch5.T1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.1">“<span class="ltx_text ltx_font_bold" id="Ch5.T1.1.9.8.1.1">Fine-tuned</span>”</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.2">+ Fuzzy (one-shot)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.3"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.9.8.3.1">49.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.4"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.9.8.4.1">70.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.5"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.9.8.5.1">40.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch5.T1.1.9.8.6"><span class="ltx_text ltx_font_bold" id="Ch5.T1.1.9.8.6.1">79.62</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5.1: </span>Comparing adaptive MT with NLLB-200 3.3B, ChatGPT, and Mistral 7B (before and after fine-tuning). Our fine-tuned Mistral demonstrates quality gains for both zero-shot translation and one-shot adaptive MT.</figcaption>
</figure>
<section class="ltx_section" id="Ch5.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">5.1     Data</h3>
<div class="ltx_para" id="Ch5.S1.p1">
<p class="ltx_p" id="Ch5.S1.p1.1">In this experiment, fine-tuning uses a mix of 10,000 segments with zero-shot prompts and 10,000 segments with one-shot prompts. The whole dataset was split into 19,000 segments for training the model and 1,000 randomly selected segments for validation while training. Fuzzy matches are extracted from a “context dataset” including 50,000 translation pairs. The test dataset includes 10,000 sentences, and it has its own unique context dataset, which consists of 50,000 unique translation pairs. Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.F1" title="Figure 5.1 ‣ 5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a> shows examples of zero-shot and one-shot prompts. The retrieval process of fuzzy matches is detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S2" title="5.2 Information Retrieval ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<figure class="ltx_figure" id="Ch5.F1">
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<br class="ltx_break"/><svg class="ltx_picture" height="104.82" id="Ch5.F1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,104.82) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 88.77 C 0 91.71 2.38 94.09 5.32 94.09 L 243.75 94.09 C 246.68 94.09 249.07 91.71 249.07 88.77 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 88.77 C 1.38 90.95 3.15 92.71 5.32 92.71 L 243.75 92.71 C 245.92 92.71 247.68 90.95 247.68 88.77 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 82.28)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 322.57 22.54 C 324.3 22.54 325.71 21.13 325.71 19.39 L 325.71 3.15 C 325.71 1.41 324.3 0 322.57 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 322.57 21.35 C 323.65 21.35 324.53 20.47 324.53 19.39 L 324.53 3.15 C 324.53 2.06 323.65 1.18 322.57 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch5.F1.pic1.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: ES-EN zero-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 72.38)"><foreignobject height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch5.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch5.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch5.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch5.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch5.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch5.S1.I1">
<span class="ltx_item" id="Ch5.S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I1.ix1.p1">
<span class="ltx_p" id="Ch5.S1.I1.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I1.ix1.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I1.ix1.p1.1.m1.1"><semantics id="Ch5.S1.I1.ix1.p1.1.m1.1a"><mo id="Ch5.S1.I1.ix1.p1.1.m1.1.1" xref="Ch5.S1.I1.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I1.ix1.p1.1.m1.1b"><lt id="Ch5.S1.I1.ix1.p1.1.m1.1.1.cmml" xref="Ch5.S1.I1.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I1.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I1.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I1.ix1.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I1.ix1.p1.2.m2.1"><semantics id="Ch5.S1.I1.ix1.p1.2.m2.1a"><mo id="Ch5.S1.I1.ix1.p1.2.m2.1.1" xref="Ch5.S1.I1.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I1.ix1.p1.2.m2.1b"><gt id="Ch5.S1.I1.ix1.p1.2.m2.1.1.cmml" xref="Ch5.S1.I1.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I1.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I1.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I1.ix1.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch5.S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I1.ix2.p1">
<span class="ltx_p" id="Ch5.S1.I1.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I1.ix2.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I1.ix2.p1.1.m1.1"><semantics id="Ch5.S1.I1.ix2.p1.1.m1.1a"><mo id="Ch5.S1.I1.ix2.p1.1.m1.1.1" xref="Ch5.S1.I1.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I1.ix2.p1.1.m1.1b"><lt id="Ch5.S1.I1.ix2.p1.1.m1.1.1.cmml" xref="Ch5.S1.I1.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I1.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I1.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I1.ix2.p1.2.2">target_translation</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I1.ix2.p1.2.m2.1"><semantics id="Ch5.S1.I1.ix2.p1.2.m2.1a"><mo id="Ch5.S1.I1.ix2.p1.2.m2.1.1" xref="Ch5.S1.I1.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I1.ix2.p1.2.m2.1b"><gt id="Ch5.S1.I1.ix2.p1.2.m2.1.1.cmml" xref="Ch5.S1.I1.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I1.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I1.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I1.ix2.p1.2.3"></span></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/><svg class="ltx_picture" height="104.82" id="Ch5.F1.pic2" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,104.82) matrix(1 0 0 -1 0 0) translate(175.47,0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 5.32 L 0 88.77 C 0 91.71 2.38 94.09 5.32 94.09 L 243.75 94.09 C 246.68 94.09 249.07 91.71 249.07 88.77 L 249.07 5.32 C 249.07 2.38 246.68 0 243.75 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#FCFCFF" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 88.77 C 1.38 90.95 3.15 92.71 5.32 92.71 L 243.75 92.71 C 245.92 92.71 247.68 90.95 247.68 88.77 L 247.68 5.32 C 247.68 3.15 245.92 1.38 243.75 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 11.81 82.28)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 318.68 22.54 C 320.42 22.54 321.83 21.13 321.83 19.39 L 321.83 3.15 C 321.83 1.41 320.42 0 318.68 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#00E6E6" fill-opacity="1.000000"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 318.68 21.35 C 319.77 21.35 320.65 20.47 320.65 19.39 L 320.65 3.15 C 320.65 2.06 319.77 1.18 318.68 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.14"><span class="ltx_ERROR undefined" id="Ch5.F1.pic2.2.2.2.1.1.1.1">\tcb@lua@color</span>tcbcolupper</foreignobject><text transform="matrix(1 0 0 -1 0 0)">Prompt: ES-EN one-shot translation</text></g></g></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 9.47 54.39)"><foreignobject height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="218.53"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch5.F1.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:157.9pt;"><span class="ltx_ERROR undefined" id="Ch5.F1.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch5.F1.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch5.F1.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1">tcbcolupper
<em class="ltx_emph ltx_font_italic" id="Ch5.F1.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1"></em></span>
<span class="ltx_itemize" id="Ch5.S1.I2">
<span class="ltx_item" id="Ch5.S1.I2.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I2.ix1.p1">
<span class="ltx_p" id="Ch5.S1.I2.ix1.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I2.ix1.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix1.p1.1.m1.1"><semantics id="Ch5.S1.I2.ix1.p1.1.m1.1a"><mo id="Ch5.S1.I2.ix1.p1.1.m1.1.1" xref="Ch5.S1.I2.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix1.p1.1.m1.1b"><lt id="Ch5.S1.I2.ix1.p1.1.m1.1.1.cmml" xref="Ch5.S1.I2.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix1.p1.2.2">source_fuzzy_match</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix1.p1.2.m2.1"><semantics id="Ch5.S1.I2.ix1.p1.2.m2.1a"><mo id="Ch5.S1.I2.ix1.p1.2.m2.1.1" xref="Ch5.S1.I2.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix1.p1.2.m2.1b"><gt id="Ch5.S1.I2.ix1.p1.2.m2.1.1.cmml" xref="Ch5.S1.I2.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix1.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch5.S1.I2.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I2.ix2.p1">
<span class="ltx_p" id="Ch5.S1.I2.ix2.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I2.ix2.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix2.p1.1.m1.1"><semantics id="Ch5.S1.I2.ix2.p1.1.m1.1a"><mo id="Ch5.S1.I2.ix2.p1.1.m1.1.1" xref="Ch5.S1.I2.ix2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix2.p1.1.m1.1b"><lt id="Ch5.S1.I2.ix2.p1.1.m1.1.1.cmml" xref="Ch5.S1.I2.ix2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix2.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix2.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix2.p1.2.2">target_fuzzy_match</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix2.p1.2.m2.1"><semantics id="Ch5.S1.I2.ix2.p1.2.m2.1a"><mo id="Ch5.S1.I2.ix2.p1.2.m2.1.1" xref="Ch5.S1.I2.ix2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix2.p1.2.m2.1b"><gt id="Ch5.S1.I2.ix2.p1.2.m2.1.1.cmml" xref="Ch5.S1.I2.ix2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix2.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix2.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix2.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch5.S1.I2.ix3" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I2.ix3.p1">
<span class="ltx_p" id="Ch5.S1.I2.ix3.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I2.ix3.p1.2.1">Spanish: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix3.p1.1.m1.1"><semantics id="Ch5.S1.I2.ix3.p1.1.m1.1a"><mo id="Ch5.S1.I2.ix3.p1.1.m1.1.1" xref="Ch5.S1.I2.ix3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix3.p1.1.m1.1b"><lt id="Ch5.S1.I2.ix3.p1.1.m1.1.1.cmml" xref="Ch5.S1.I2.ix3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix3.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix3.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix3.p1.2.2">source_segment</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix3.p1.2.m2.1"><semantics id="Ch5.S1.I2.ix3.p1.2.m2.1a"><mo id="Ch5.S1.I2.ix3.p1.2.m2.1.1" xref="Ch5.S1.I2.ix3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix3.p1.2.m2.1b"><gt id="Ch5.S1.I2.ix3.p1.2.m2.1.1.cmml" xref="Ch5.S1.I2.ix3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix3.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix3.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix3.p1.2.3"></span></span>
</span></span>
<span class="ltx_item" id="Ch5.S1.I2.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch5.S1.I2.ix4.p1">
<span class="ltx_p" id="Ch5.S1.I2.ix4.p1.2"><em class="ltx_emph ltx_font_italic" id="Ch5.S1.I2.ix4.p1.2.1">English: </em><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix4.p1.1.m1.1"><semantics id="Ch5.S1.I2.ix4.p1.1.m1.1a"><mo id="Ch5.S1.I2.ix4.p1.1.m1.1.1" xref="Ch5.S1.I2.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix4.p1.1.m1.1b"><lt id="Ch5.S1.I2.ix4.p1.1.m1.1.1.cmml" xref="Ch5.S1.I2.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix4.p1.2.2">target_translation</span><math alttext="&gt;" class="ltx_Math" display="inline" id="Ch5.S1.I2.ix4.p1.2.m2.1"><semantics id="Ch5.S1.I2.ix4.p1.2.m2.1a"><mo id="Ch5.S1.I2.ix4.p1.2.m2.1.1" xref="Ch5.S1.I2.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch5.S1.I2.ix4.p1.2.m2.1b"><gt id="Ch5.S1.I2.ix4.p1.2.m2.1.1.cmml" xref="Ch5.S1.I2.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S1.I2.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch5.S1.I2.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="Ch5.S1.I2.ix4.p1.2.3"></span></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
<br class="ltx_break"/>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5.1: </span>Zero-shot and one-shot prompts used for fine-tuning Mistral</figcaption>
</figure>
<div class="ltx_para" id="Ch5.S1.p2">
<p class="ltx_p" id="Ch5.S1.p2.1">Originally, we mixed Spanish-to-English medical datasets from OPUS <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib183" title="">2012</a>)</cite>, namely ELRC <cite class="ltx_cite ltx_citemacro_citep">(Berzins et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib12" title="">2019</a>)</cite>, EMEA <cite class="ltx_cite ltx_citemacro_citep">(EMA,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib42" title="">2012</a>)</cite>, SciELO <cite class="ltx_cite ltx_citemacro_citep">(Soares et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib177" title="">2018</a>)</cite>, and TICO-19 <cite class="ltx_cite ltx_citemacro_citep">(Anastasopoulos et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib4" title="">2020</a>)</cite> datasets. Then we filtered<span class="ltx_note ltx_role_footnote" id="Ch5.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Filtering scripts are available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymoslem/MT-Preparation" title="">https://github.com/ymoslem/MT-Preparation</a></span></span></span> the resulted dataset to exclude duplicates and too long segments.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>As NLLB supports a maximum token length of 512 <span class="ltx_text ltx_font_italic" id="Ch5.footnote4.1">tokens</span>, we exclude any segment whose source or target text is longer than 70 <span class="ltx_text ltx_font_italic" id="Ch5.footnote4.2">words</span> to also take into account the one-shot case that will augment another segment to the original one. As the context window of Mistral is much larger (8K tokens), it is theoretically possible to translate longer segments.</span></span></span>
The whole dataset includes 1,868,505 segments before filtering, and 922,343 segments after filtering.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We observe that almost two-thirds of the EMEA dataset are duplicates.</span></span></span> However, we used only part of it for this preliminary experiment. In the future, we would like to increase the size of the training data and compare the performance. Nevertheless, achieving these results (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.T1" title="Table 5.1 ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a>) with such a small dataset (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S1" title="5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a>) shows how promising this approach is.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">5.2     Information Retrieval</h3>
<div class="ltx_para" id="Ch5.S2.p1">
<p class="ltx_p" id="Ch5.S2.p1.1">For indexing and retrieval of fuzzy matches, we use Sentence-Transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib159" title="">2019</a>)</cite> and <span class="ltx_text ltx_font_italic" id="Ch5.S2.p1.1.1">Faiss</span> <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib87" title="">2019</a>)</cite>, with a multilingual model to generate the embeddings for the datasets and later to extract fuzzy matches through semantic search.</p>
</div>
<section class="ltx_paragraph" id="Ch5.S2.SS0.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">Embedding:</h6>
<div class="ltx_para" id="Ch5.S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Ch5.S2.SS0.SSS0.Px1.p1.1">To encode all the translation segments into embeddings, we employ a <span class="ltx_text ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px1.p1.1.1">multilingual</span> model, namely Microsoft’s <span class="ltx_text ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px1.p1.1.2">“Multilingual-MiniLM-L12-H384”</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib199" title="">2020</a>)</cite>. These embeddings will be used later for both indexing and retrieval. The step of generating embeddings can be implemented with the Sentence-Transformers library<span class="ltx_note ltx_role_footnote" id="Ch5.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sbert.net/" title="">https://www.sbert.net/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib159" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch5.S2.SS0.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">Indexing:</h6>
<div class="ltx_para" id="Ch5.S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Ch5.S2.SS0.SSS0.Px2.p1.1">For indexing, we use Faiss<span class="ltx_note ltx_role_footnote" id="Ch5.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/faiss" title="">https://github.com/facebookresearch/faiss</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib87" title="">2019</a>)</cite>, a library for efficient similarity search and clustering of dense vectors. We train an <em class="ltx_emph ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px2.p1.1.1">IndexIVFFlat</em> index, which uses <em class="ltx_emph ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px2.p1.1.2">IndexFlatL2</em> as a quantiser.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/faiss/wiki/Faster-search" title="">https://github.com/facebookresearch/faiss/wiki/Faster-search</a></span></span></span>
The embedding size is 384, which is the same as the embedding size of model used.
For the number of clusters at indexing time, the <em class="ltx_emph ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px2.p1.1.3">nlist</em> parameter was set to 4096, while the number of clusters to explore at search time <span class="ltx_text ltx_font_italic" id="Ch5.S2.SS0.SSS0.Px2.p1.1.4">nprobe</span> was set to search for nearest neighbours in 32 clusters.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>According to Faiss’ guidelines for choosing an index, the number of clusters is recommended to be between <em class="ltx_emph ltx_font_italic" id="Ch5.footnote9.1">4*sqrt(N)</em> to <em class="ltx_emph ltx_font_italic" id="Ch5.footnote9.2">16*sqrt(N)</em>, where <em class="ltx_emph ltx_font_italic" id="Ch5.footnote9.3">N</em> is the size of the dataset. Obviously, this would depend on the available computational resources.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="Ch5.S2.SS0.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">Semantic Search:</h6>
<div class="ltx_para" id="Ch5.S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Ch5.S2.SS0.SSS0.Px3.p1.1">This step computes the cosine similarity between the query and all the documents in the corpus based on their embeddings, and retrieves the top <math alttext="k" class="ltx_Math" display="inline" id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="Ch5.S2.SS0.SSS0.Px3.p1.1.m1.1d">italic_k</annotation></semantics></math> matching entries. In this case, our query is each source segment, and the corpus is the unique “context dataset” (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.S1" title="5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a>) leveraged to extract fuzzy matches.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch5.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">5.3     Fine-tuning</h3>
<div class="ltx_para" id="Ch5.S3.p1">
<p class="ltx_p" id="Ch5.S3.p1.1">We used QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib77" title="">2021</a>; Dettmers et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib37" title="">2023</a>)</cite> for efficient fine-tuning with 4bit quantisation, with Hugging Face Transformers.<span class="ltx_note ltx_role_footnote" id="Ch5.footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers" title="">https://github.com/huggingface/transformers</a></span></span></span> Fine-tuning was for only one epoch, which revealed better results than fine-tuning for 4 epochs. The configuration of quantisation through <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.1">BitsAndBytes</span> includes: <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.2">load_in_4bit=True</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.3">bnb_4bit_quant_type=“nf4”</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.4">bnb_4bit_use_double_quant=True</span>, and <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.5">bnb_4bit_compute_dtype=torch.bfloat16</span>. LoRA configuration was set via the PEFT library<span class="ltx_note ltx_role_footnote" id="Ch5.footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a></span></span></span> as follows: the dimension of the low-rank matrices <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.6">r=64</span>, the scaling factor for the weight matrices <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.7">lora_alpha=16</span>, dropout probability of the LoRA layers <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.8">lora_dropout=0.1</span>, and without training the bias parameters for better performance <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.9">bias=“none”</span>.
Training arguments include: batch size for training and evaluation 32 examples, <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.10">warmup_steps=0.03</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.11">learning_rate=2e-3</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.12">lr_scheduler_type=“constant”</span>, and <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.13">bf16=True</span>.
Both training and inference utilise Google Colab Pro+ with one GPU <span class="ltx_text ltx_font_italic" id="Ch5.S3.p1.1.14">NVIDIA A100-SXM4-40GB</span>.</p>
</div>
</section>
<section class="ltx_section" id="Ch5.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">5.4     Inference</h3>
<div class="ltx_para" id="Ch5.S4.p1">
<p class="ltx_p" id="Ch5.S4.p1.1">For inference (translation), we experimented with a number of models including NLLB-200 <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib33" title="">2022</a>)</cite> whose architecture is encoder-decoder Transformer as well as ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Ouyang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib139" title="">2022</a>)</cite> and Mistral 7B, which are autoregressive decoder-only Transformer-based LLMs. Mistral 7B was used both without fine-tuning and after fine-tuning on a mix of zero-shot and one-shot translation prompts.</p>
</div>
<section class="ltx_paragraph" id="Ch5.S4.SS0.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">Mistral 7B:</h6>
<div class="ltx_para" id="Ch5.S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Ch5.S4.SS0.SSS0.Px1.p1.1">We converted both the baseline and our fine-tuned models of Mistral 7B to the CTranslate2 <span class="ltx_note ltx_role_footnote" id="Ch5.footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenNMT/CTranslate2" title="">https://github.com/OpenNMT/CTranslate2</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib93" title="">Klein et al., 2020b, </a>)</cite> format (with 8int quantisation) for more efficiency. We employed greedy search by setting <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px1.p1.1.1">sampling_topk=1</span> and added the new line <code class="ltx_verbatim ltx_font_typewriter" id="Ch5.S4.SS0.SSS0.Px1.p1.1.2">\n</code> character to <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px1.p1.1.3">end_token</span> to avoid overgeneration. Mistral through CTranslate2 translates the zero-shot test dataset that includes 10,000 sentences in 2–3 minutes (approx. 80 segments/second). The time almost doubles for the one-shot test dataset. Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.F1" title="Figure 5.1 ‣ 5.1 Data ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a> illustrates the prompts used for both inference and fine-tuning. Two versions of Mistral were tested, the baseline model without fine-tuning, and the model we fine-tuned on a mix of zero-shot and one-shot translation prompts. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.T1" title="Table 5.1 ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a> shows translation evaluation results of both models. Fine-tuning Mistral on a mix of zero-shot and one-shot prompts improved both its regular translation quality (i.e. when only the source text is available) and adaptive translation quality (in this case, when one fuzzy match is provided) at inference time.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch5.S4.SS0.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">ChatGPT:</h6>
<div class="ltx_para" id="Ch5.S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Ch5.S4.SS0.SSS0.Px2.p1.1">The model used is “gpt-3.5-turbo” with <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px2.p1.1.1">temperature=0.3</span> and <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px2.p1.1.2">top_p=1</span>. Requests were sent in batches of 20 segments, and the <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px2.p1.1.3">max_tokens</span> argument was set as the largest number of words per source segment in a batch multiplied by 4, which is a rough number that can be increased or decreased based on the language and model. When the source text is augmented with one fuzzy match, the translation quality is improved by several points across all the automatic evaluation metrics. However, it is worth noting that although batch processing was employed, there is no guarantee of the generation time with ChatGPT, which can range from several minutes to a couple of hours. In this sense, we observe that Mistral 7B is much more efficient.</p>
</div>
</section>
<section class="ltx_paragraph" id="Ch5.S4.SS0.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">NLLB-200:</h6>
<div class="ltx_para" id="Ch5.S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Ch5.S4.SS0.SSS0.Px3.p1.1">In this set of experiments, the NLLB-200 model was used as is, i.e. without fine-tuning. The first two rows of Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.T1" title="Table 5.1 ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a> show the evaluation scores of using NLLB 3.3B for translation without a fuzzy match (zero-shot) and with a fuzzy match (one-shot). The same test dataset and its unique context dataset were used; however, fuzzy matching augmentation was done differently to match the architecture of the NLLB model. Each source sentence was augmented with its best fuzzy match, and the two sentences were separated by the language code of the source language (in this case <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.1">“spa_Latn”</span>). As NLLB was mainly pre-trained on sentences, we had to add an extra token that usually comes at the beginning of sentences, such as a bullet point (“•”) after the language code between the two source sentences. Hence, the target fuzzy match was fed to the model as a prefix augmented by the target language code (in this case <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.2">“eng_Latn”</span>) and the extra token. In this sense, the model was encouraged to complete the translation through teacher-forcing <cite class="ltx_cite ltx_citemacro_citep">(Williams and Zipser,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib201" title="">1989</a>)</cite>, i.e. using the ground truth as input, instead of the model output. In other words, the model is not required to translate the target fuzzy match, but rather to use the provided translation as is to guide the translation of the new untranslated source sentence. Translation arguments include: <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.3">batch_type=“tokens”</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.4">max_batch_size=2024</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.5">beam_size=2</span>, <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.6">min_decoding_length=2</span>, and <span class="ltx_text ltx_font_italic" id="Ch5.S4.SS0.SSS0.Px3.p1.1.7">max_decoding_length=512</span>. Although there is a marginal improvement given the BLEU score, the performance degrades according to the other reported automatic evaluation metrics, chrF++, TER and COMET. The fact that NLLB was trained to translate individual sentences rather than a series of sentences or full documents could be the main reason for this. In the future, we would like to experiment with fine-tuning NLLB with fuzzy matching augmentation and compare the results.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch5.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">5.5     Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch5.S5.p1">
<p class="ltx_p" id="Ch5.S5.p1.1">In this chapter, we showed how fine-tuning a general purpose LLM such as Mistral 7B can improve its in-context learning ability, especially for real-time adaptive MT (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5.T1" title="Table 5.1 ‣ Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Moreover, such translation quality gains were achievable through fine-tuning using a relatively small dataset (20,000 segments). Incorporating a mix of zero-shot and one-shot prompts in the training data helps improve both regular zero-shot translation, and one-shot translation that incorporates a fuzzy match. It is worth noting that Mistral 7B is much more efficient than ChatGPT, which is an added benefit in production scenarios.</p>
</div>
<div class="ltx_para" id="Ch5.S5.p2">
<p class="ltx_p" id="Ch5.S5.p2.1">In the future, we would like to experiment with other domains and language pairs, including low-resource languages. As currently there are several multilingual LLMs such as BLOOM (46 languages) <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, Falcon (EN-DE-ES-FR) <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>)</cite>, larger versions of Mistral/Mixtral (EN-DE-ES-FR-IT) <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, Jais (AR-EN) <cite class="ltx_cite ltx_citemacro_citep">(Sengupta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib169" title="">2023</a>)</cite>, Baichuan (ZH) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib206" title="">Yang et al., 2023a, </a>)</cite>, and Qwen (ZH) <cite class="ltx_cite ltx_citemacro_citep">(Bai et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib8" title="">2023</a>)</cite>, it can be insightful to apply the same approach with these models. Furthermore, as we experimented with NLLB-200 without fine-tuning, we would like to experiment with fine-tuning for fair comparison. While we fine-tuned Mistral on a small dataset, it is recommended to experiment with fine-tuning on more data, especially from the same domain. It can also be helpful to incorporate different types of prompts, such as few-shot prompts, and terminology-based prompts.</p>
</div>
</section>
</section>
<section class="ltx_chapter" id="Ch6">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 6      Domain Terminology <span class="ltx_text ltx_align_left" id="Ch6.1.1">Integration</span> into Machine Translation:
<span class="ltx_text ltx_align_left" id="Ch6.2.2">Leveraging</span> Large Language <span class="ltx_text ltx_align_left" id="Ch6.3.3">Models</span>
</h2>
<div class="ltx_para" id="Ch6.p1">
<p class="ltx_p" id="Ch6.p1.1">Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, Rejwanul Haque, John Kelleher, and Andy Way</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch6.p2">
<p class="ltx_p" id="Ch6.p2.1">In Proceedings of the Eighth Conference on Machine Translation (WMT 2023), pages 902–911, Sentosa, Singapore. Association for Computational Linguistics.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Published at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.wmt-1.82/" title="">https://aclanthology.org/2023.wmt-1.82/</a></span></span></span></p>
</div>
</section>
<div class="ltx_abstract">
<span class="ltx_ERROR undefined" id="id16.id1">\nohyphens</span>
<p class="ltx_p" id="id17.id2">This paper discusses the methods that we used for our submissions to the WMT 2023 <span class="ltx_text" id="id17.id2.1">Terminology</span> Shared Task for German-to-English (DE-EN), English-to-Czech (EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to advance machine <span class="ltx_text" id="id17.id2.2">translation</span> (MT) by challenging participants to develop systems that accurately translate technical terms, ultimately enhancing communication and understanding in specialised domains. To this end, we conduct experiments that utilise large language models (LLMs) for two purposes: generating synthetic bilingual terminology-based data, and post-editing translations generated by an MT model through incorporating pre-approved terms. Our system employs a four-step process: (i) using an LLM to generate bilingual synthetic data based on the provided terminology, (ii) fine-tuning a generic encoder-decoder MT model, with a mix of the terminology-based synthetic data generated in the first step and a randomly sampled portion of the original generic training data, (iii) generating translations with the fine-tuned MT model, and (iv) finally, leveraging an LLM for terminology-constrained automatic post-editing of the translations that do not include the required terms. The results demonstrate the effectiveness of our proposed approach in improving the integration of pre-approved terms into translations. The number of terms incorporated into the translations of the blind dataset increases from an average of 36.67% with the generic model to an average of 72.88% by the end of the process. In other words, successful utilisation of terms nearly doubles across the three language pairs.</p>
</div>
<section class="ltx_section" id="Ch6.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">6.1     Context</h3>
<div class="ltx_para" id="Ch6.S1.p1">
<p class="ltx_p" id="Ch6.S1.p1.1">This work builds on our two previous papers for AMTA 2022 and EAMT 2023. It investigates techniques to enhance adherence to pre-approved terminology in translation, including terminology-based data generation and MT automatic post-editing with LLMs. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S2" title="6.2 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.2</span></a> elaborates on the research context of this paper.
</p>
</div>
</section>
<section class="ltx_section" id="Ch6.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">6.2     Introduction</h3>
<div class="ltx_para" id="Ch6.S2.p1">
<p class="ltx_p" id="Ch6.S2.p1.1">The primary goal of the WMT 2023 Terminology Shared Task is to evaluate the ability of MT systems to accurately translate technical terminology.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For the test dataset of the Chinese-to-English language pair, the organisers used the BWB corpus, which comprises texts extracted from novels. The test dataset of the English-to-Czech language pair consists of NLP paper abstracts, while the test dataset of the German-to-English language pair consists of medical paper abstracts <cite class="ltx_cite ltx_citemacro_citep">(Semenov et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib168" title="">2023</a>)</cite>.</span></span></span> The task aims to assess the extent to which MT models can utilise additional information regarding the translation of terminology. The shared task requires the participants to provide three translations, one without terms and the others with two individual sets of terms.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p2">
<p class="ltx_p" id="Ch6.S2.p2.1">There have been several advancements in the area of MT domain adaptation, where an MT model is expected to follow the style and terminology of a certain domain or client <cite class="ltx_cite ltx_citemacro_citep">(Chu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>; Kobus et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib95" title="">2017</a>)</cite>. Moreover, some researchers give special focus to terminology while training and fine-tuning MT systems <cite class="ltx_cite ltx_citemacro_citep">(Dinu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib39" title="">2019</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib78" title="">Hu et al., 2019a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a, </a>; Michon et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib121" title="">2020</a>; Nayak et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib133" title="">2023</a>)</cite>. However, forcing an MT model to adhere to certain terminology at inference time is among the most challenging aspects of MT. Hence, several researchers have investigated approaches to terminology-constrained decoding at translation time <cite class="ltx_cite ltx_citemacro_citep">(Hokamp and Liu,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib72" title="">2017</a>; Hasler et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib63" title="">2018</a>; Post and Vilar,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib152" title="">2018</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib79" title="">Hu et al., 2019b, </a>; Exel et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib44" title="">2020</a>)</cite>. The goal is to ensure that the MT system can accommodate unseen terminology while retaining translation accuracy and fluency.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="Ch6.S2.p3">
<p class="ltx_p" id="Ch6.S2.p3.1">Recently, since the emergence of advanced LLMs such as GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib29" title="">2022</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>)</cite>, Llama 2 <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib187" title="">Touvron et al., 2023b, </a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, and Jais <cite class="ltx_cite ltx_citemacro_citep">(Sengupta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib169" title="">2023</a>)</cite> to mention just a few, researchers have been exploring the capabilities of these models for a number of tasks including MT <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Yvon,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib11" title="">2023</a>; Hendy et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib68" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib86" title="">Jiao et al., 2023b, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; Vilar et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib192" title="">2023</a>)</cite>. Some work investigates whether it is possible to employ the in-context learning feature of LLMs for adaptive MT with fuzzy matches <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>, and terminology-constrained MT using a pre-defined glossary <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite> or even a dictionary <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib52" title="">2023</a>)</cite>. They found the approach is generally effective in increasing the number of terms used in the translation, even for low-resource languages.</p>
</div>
<div class="ltx_para" id="Ch6.S2.p4">
<p class="ltx_p" id="Ch6.S2.p4.1">We highlight our key contributions with the systems that we submitted for the WMT 2023 Terminology Shared Task as follows:</p>
</div>
<div class="ltx_para" id="Ch6.S2.p5">
<ul class="ltx_itemize" id="Ch6.S2.I1">
<li class="ltx_item" id="Ch6.S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch6.S2.I1.i1.p1">
<p class="ltx_p" id="Ch6.S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Ch6.S2.I1.i1.p1.1.1">LLMs for domain-specific data augmentation:</span> In our previous work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>, we employed LLMs, namely GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib194" title="">2021</a>)</cite> and mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib172" title="">2022</a>)</cite>, to generate domain-specific datasets based on the target sentences in a small authentic dataset, then generated the source sentences with back-translation <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib170" title="">2016</a>; Poncelas et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib148" title="">2019</a>)</cite>, and finally fine-tuned an encoder-decoder MT model on this data. In this work, we take a couple of steps forward by instructing an LLM, namely ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib17" title="">2020</a>; Ouyang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib139" title="">2022</a>)</cite>, to generate terminology-based bilingual synthetic data. In other words, the LLM will generate both the source and target sides of translation pairs, making sure the pre-approved target terms provided by the organisers are used in the translations.</p>
</div>
</li>
<li class="ltx_item" id="Ch6.S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Ch6.S2.I1.i2.p1">
<p class="ltx_p" id="Ch6.S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Ch6.S2.I1.i2.p1.1.1">LLMs for terminology-constrained MT and MT post-editing:</span> In our previous work, we utilised an LLM for translation and provided it with a list of terms to support in-context learning, which improved adherence to the required terminology at inference time <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>. We also investigated whether we could use an LLM for post-editing MT generated by other systems. In this work, we prompt ChatGPT to insert missing terms into translations generated by an encoder-decoder MT system. In other words, if some of the translations generated by a fine-tuned MT model still do not include the terms provided by the organisers, we feed these translations into an LLM, namely ChatGPT, instructing it to incorporate these terms while using the same translation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Ch6.S3">
<h3 class="ltx_title ltx_font_bold ltx_title_section">6.3     Method</h3>
<div class="ltx_para" id="Ch6.S3.p1">
<p class="ltx_p" id="Ch6.S3.p1.1">In our submissions to the WMT 2023 Terminology Shared Task, we followed these steps:
</p>
</div>
<div class="ltx_para" id="Ch6.S3.p2">
<ol class="ltx_enumerate" id="Ch6.S3.I1">
<li class="ltx_item" id="Ch6.S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="Ch6.S3.I1.i1.p1">
<p class="ltx_p" id="Ch6.S3.I1.i1.p1.1">Generate bilingual synthetic data based on the pre-approved terms, using an LLM, namely ChatGPT.</p>
</div>
</li>
<li class="ltx_item" id="Ch6.S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span>
<div class="ltx_para" id="Ch6.S3.I1.i2.p1">
<p class="ltx_p" id="Ch6.S3.I1.i2.p1.1">Fine-tune a generic model, OPUS <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann and Thottingal,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib185" title="">2020</a>)</cite>, on a mix of the terminology-based synthetic data generated in (i) and a randomly sampled portion of the original generic training data.</p>
</div>
</li>
<li class="ltx_item" id="Ch6.S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span>
<div class="ltx_para" id="Ch6.S3.I1.i3.p1">
<p class="ltx_p" id="Ch6.S3.I1.i3.p1.1">Generate translations of the dev, test, and blind datasets provided by the organisers with the fine-tuned model from (ii).</p>
</div>
</li>
<li class="ltx_item" id="Ch6.S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iv)</span>
<div class="ltx_para" id="Ch6.S3.I1.i4.p1">
<p class="ltx_p" id="Ch6.S3.I1.i4.p1.1">Apply terminology-constrained automatic post-editing using ChatGPT to incorporate missing terms into translations that do not yet include the required terminology.</p>
</div>
</li>
</ol>
</div>
<section class="ltx_subsection" id="Ch6.S3.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">6.3.1     Synthetic Data Generation</h4>
<div class="ltx_para" id="Ch6.S3.SS1.p1">
<p class="ltx_p" id="Ch6.S3.SS1.p1.1">We used ChatGPT “gpt-3.5-turbo”<span class="ltx_note ltx_role_footnote" id="Ch6.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The model “gpt-3.5-turbo” is a relatively efficient and cost-effective option, so we wanted to understand the quality we can achieve with it.</span></span></span> to generate bilingual sentence pairs, using the terms provided by the organisers. So, given a target term, the model was asked to generate multiple translation pairs, including both the source (e.g. German) and the target (e.g. English). For parameters of ChatGPT’s API, we used <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p1.1.1">top_p</em> 1 and <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p1.1.2">temperature</em> values 0 and 0.3 to generate diverse outputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch6.S3.SS1.p2">
<svg class="ltx_picture" height="73.53" id="Ch6.S3.SS1.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,73.53) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 4.21 L 0 69.32 C 0 71.64 1.89 73.53 4.21 73.53 L 595.79 73.53 C 598.11 73.53 600 71.64 600 69.32 L 600 4.21 C 600 1.89 598.11 0 595.79 0 L 4.21 0 C 1.89 0 0 1.89 0 4.21 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 0.28 4.21 L 0.28 52.8 L 599.72 52.8 L 599.72 4.21 C 599.72 2.04 597.96 0.28 595.79 0.28 L 4.21 0.28 C 2.04 0.28 0.28 2.04 0.28 4.21 Z" style="stroke:none"></path></g><g fill="#808080" fill-opacity="1.000000"><path d="M 0.28 53.08 L 0.28 69.32 C 0.28 71.49 2.04 73.25 4.21 73.25 L 595.79 73.25 C 597.96 73.25 599.72 71.49 599.72 69.32 L 599.72 53.08 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 8.36 12.09)"><foreignobject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="577.74"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch6.S3.SS1.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1" style="width:417.5pt;"><span class="ltx_ERROR undefined" id="Ch6.S3.SS1.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch6.S3.SS1.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch6.S3.SS1.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1">tcbcolupper
Please use the “Federal Ministry of Science” to generate just 20 numbered sentences in German-English in one Python dictionary format.</span>
</span></span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="Ch6.S3.SS1.p3">
<p class="ltx_p" id="Ch6.S3.SS1.p3.1">To filter the generated data, we first removed duplicate sentences from the whole dataset, based on both the source and target. Then, we applied language detection of both sides of the data using <span class="ltx_text ltx_font_italic" id="Ch6.S3.SS1.p3.1.1">fastText<span class="ltx_note ltx_role_footnote" id="Ch6.footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="Ch6.footnote4.1.1.1">4</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright" href="https://fasttext.cc/docs/en/language-identification.html" title="">https://fasttext.cc/docs/en/language-identification.html</a></span></span></span></span> and <span class="ltx_text ltx_font_italic" id="Ch6.S3.SS1.p3.1.2">pycld2<span class="ltx_note ltx_role_footnote" id="Ch6.footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="Ch6.footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright" href="https://github.com/aboSamoor/pycld2" title="">https://github.com/aboSamoor/pycld2</a></span></span></span></span> libraries to ensure that the generated sentences were in our desired languages. We excluded any sentences whose scores were below a certain threshold, namely 0.9 for <span class="ltx_text ltx_font_italic" id="Ch6.S3.SS1.p3.1.3">fastText</span> and 90 for <span class="ltx_text ltx_font_italic" id="Ch6.S3.SS1.p3.1.4">pycld2</span>.</p>
</div>
<div class="ltx_para" id="Ch6.S3.SS1.p4">
<p class="ltx_p" id="Ch6.S3.SS1.p4.1">The filtering step removed less than 1% of the generated data. However, due to computational resource and time limitations, we could not use all the generated data. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T1" title="Table 6.1 ‣ 6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.1</span></a> reports the number of generated, filtered, and used translation pairs.</p>
</div>
<div class="ltx_para" id="Ch6.S3.SS1.p5">
<p class="ltx_p" id="Ch6.S3.SS1.p5.1">Initially, we only had the development and test datasets, so we used them for the German-to-English language pair. Later, when the organisers released the blind dataset, we used the development, test and blind datasets for the Chinese-to-English and English-to-Czech language pairs.</p>
</div>
<figure class="ltx_table" id="Ch6.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch6.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch6.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="Ch6.T1.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T1.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="Ch6.T1.1.1.1.2.1">Raw</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T1.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="Ch6.T1.1.1.1.3.1">Filtered</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T1.1.1.1.4">
<span class="ltx_text ltx_font_bold" id="Ch6.T1.1.1.1.4.1">Used</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch6.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T1.1.2.1.1.1">DE-EN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.1.2.1.2">124,215</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.1.2.1.3">104,318</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Ch6.T1.1.2.1.4">68,265</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="Ch6.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="Ch6.T1.1.3.2.1.1">EN-CS</span></td>
<td class="ltx_td ltx_align_left" id="Ch6.T1.1.3.2.2">187,471</td>
<td class="ltx_td ltx_align_left" id="Ch6.T1.1.3.2.3">103,797</td>
<td class="ltx_td ltx_align_left" id="Ch6.T1.1.3.2.4">64,218</td>
</tr>
<tr class="ltx_tr" id="Ch6.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch6.T1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="Ch6.T1.1.4.3.1.1">ZH-EN</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch6.T1.1.4.3.2">90,538</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch6.T1.1.4.3.3">72,695</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Ch6.T1.1.4.3.4">49,001</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6.1: </span>Terminology-based bilingual data generated by <span class="ltx_text" id="Ch6.T1.3.1">ChatGPT</span> for fine-tuning the OPUS model</figcaption>
</figure>
<div class="ltx_para" id="Ch6.S3.SS1.p6">
<p class="ltx_p" id="Ch6.S3.SS1.p6.3">To assess the quality of the bilingual data generated by ChatGPT, we computed cross-entropy scores <cite class="ltx_cite ltx_citemacro_citep">(Moore and Lewis,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib123" title="">2010</a>)</cite> of the synthetic translation pairs based on the strong encoder-decoder MT model, NLLB-200 3.3B <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib33" title="">2022</a>)</cite>. For scoring, we used CTranslate2<span class="ltx_note ltx_role_footnote" id="Ch6.footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenNMT/CTranslate2" title="">https://github.com/OpenNMT/CTranslate2</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib93" title="">Klein et al., 2020b, </a>)</cite> <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p6.3.1">score_batch()</em> method with the parameters <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p6.3.2">batch_type</em> “tokens” and <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p6.3.3">max_batch_size</em> 2024. We scored each synthetic translation pair generated by ChatGPT, and then calculated the average score for the whole dataset. Computing dual cross-entropy scores according to two inverse translation models trained on clean data is an effective method to evaluate data quality <cite class="ltx_cite ltx_citemacro_citep">(Junczys-Dowmunt,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib88" title="">2018</a>)</cite>. Hence, we computed the scores of both directions of each language pair according to the multilingual MT model NLLB-200 3.3B because both directions are generated by ChatGPT. To produce a baseline for translation quality, we generated the translations of the same datasets using NLLB-200 3.3B for each language direction with <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS1.p6.3.4">beam_size</em> 4, and then scored these translations with the same model. As the scores are in the form of negative log probabilities, we converted them to their exponential equivalents for readability, which are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T2" title="Table 6.2 ‣ 6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.2</span></a>. It is normal that the model NLLB-200 generates higher scores for its own translations; however, we wanted to know to what extent such scores are comparable to those of ChatGPT’s synthetic translation pairs. According to the scores, the German<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch6.S3.SS1.p6.1.m1.1"><semantics id="Ch6.S3.SS1.p6.1.m1.1a"><mo id="Ch6.S3.SS1.p6.1.m1.1.1" stretchy="false" xref="Ch6.S3.SS1.p6.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.SS1.p6.1.m1.1b"><ci id="Ch6.S3.SS1.p6.1.m1.1.1.cmml" xref="Ch6.S3.SS1.p6.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.SS1.p6.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.SS1.p6.1.m1.1d">↔</annotation></semantics></math>English language pair had the most comparable quality, followed by Czech<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch6.S3.SS1.p6.2.m2.1"><semantics id="Ch6.S3.SS1.p6.2.m2.1a"><mo id="Ch6.S3.SS1.p6.2.m2.1.1" stretchy="false" xref="Ch6.S3.SS1.p6.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.SS1.p6.2.m2.1b"><ci id="Ch6.S3.SS1.p6.2.m2.1.1.cmml" xref="Ch6.S3.SS1.p6.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.SS1.p6.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.SS1.p6.2.m2.1d">↔</annotation></semantics></math>English, and Chinese <math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="Ch6.S3.SS1.p6.3.m3.1"><semantics id="Ch6.S3.SS1.p6.3.m3.1a"><mo id="Ch6.S3.SS1.p6.3.m3.1.1" stretchy="false" xref="Ch6.S3.SS1.p6.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.SS1.p6.3.m3.1b"><ci id="Ch6.S3.SS1.p6.3.m3.1.1.cmml" xref="Ch6.S3.SS1.p6.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.SS1.p6.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.SS1.p6.3.m3.1d">↔</annotation></semantics></math>English language pairs.</p>
</div>
<div class="ltx_para" id="Ch6.S3.SS1.p7">
<p class="ltx_p" id="Ch6.S3.SS1.p7.1">Among the approaches that can be employed for assessing the quality of synthetic bilingual data is semantic similarity between the two sides of each translation pair (e.g. with mUSE <cite class="ltx_cite ltx_citemacro_citep">(Yang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib208" title="">2020</a>)</cite>). However, the scoring approach that we previously described and used achieves a similar goal while comparing the quality of the synthetic bilingual data to the translation quality of a strong MT baseline model, namely NLLB-200 3.3B.</p>
</div>
<figure class="ltx_table" id="Ch6.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch6.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch6.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Ch6.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Ch6.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.1.1.2.1">ChatGPT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Ch6.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.1.1.3.1">NLLB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Ch6.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.1.1.4.1">Diff.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch6.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.2.1.1.1">DE-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T2.1.2.1.2">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T2.1.2.1.3">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T2.1.2.1.4">0.09</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.3.2.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.3.2.1.1">EN-DE</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.3.2.2">0.56</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.3.2.3">0.64</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.3.2.4">0.08</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.4.3">
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.4.3.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.4.3.1.1">Avg.</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.4.3.2">0.58</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.4.3.3">0.66</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.4.3.4">0.08</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.5.4.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.5.4.1.1">CS-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.5.4.2">0.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.5.4.3">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.5.4.4">0.12</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.6.5">
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.6.5.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.6.5.1.1">EN-CS</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.6.5.2">0.49</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.6.5.3">0.58</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.6.5.4">0.09</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.7.6">
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.7.6.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.7.6.1.1">Avg.</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.7.6.2">0.54</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.7.6.3">0.64</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.7.6.4">0.10</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.8.7.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.8.7.1.1">ZH-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.8.7.2">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.8.7.3">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T2.1.8.7.4">0.17</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.9.8">
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.9.8.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.9.8.1.1">EN-ZH</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.9.8.2">0.09</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.9.8.3">0.34</td>
<td class="ltx_td ltx_align_center" id="Ch6.T2.1.9.8.4">0.25</td>
</tr>
<tr class="ltx_tr" id="Ch6.T2.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T2.1.10.9.1"><span class="ltx_text ltx_font_bold" id="Ch6.T2.1.10.9.1.1">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T2.1.10.9.2">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T2.1.10.9.3">0.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T2.1.10.9.4">0.21</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6.2: </span>Scores of translation pairs generated by ChatGPT based on the NLLB-200 3.3B model</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Ch6.S3.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">6.3.2     Fine-tuning</h4>
<div class="ltx_para" id="Ch6.S3.SS2.p1">
<p class="ltx_p" id="Ch6.S3.SS2.p1.1">Using the term-based synthetic bilingual data generated in the previous step, we fine-tuned encoder-decoder Transformer-based MT models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib191" title="">2017</a>)</cite>. In particular, we fine-tuned OPUS MT models, with Hugging Face Transformers.<span class="ltx_note ltx_role_footnote" id="Ch6.footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers" title="">https://github.com/huggingface/transformers</a></span></span></span> We applied mixed fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Chu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib31" title="">2017</a>)</cite>; in other words, we fine-tuned the baseline model with a mix of the terminology-based synthetic data generated from the previous step (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS1" title="6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>) and a randomly sampled portion of the original generic data used to train the OPUS baseline model. The numbers of segments taken from the OPUS generic data are as follows: CS: 372,928, DE: 419,881, ZH: 462,780. We over-sampled the synthetic terminology-based data to make it the same size as the used portion of generic data. The fine-tuning parameters are as follows: <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.1">train</em> = 0.9, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.2">val</em> = 0.1, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.3">batch_size</em> = 32, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.4">learning_rate</em> = 2e-5, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.5">accumulate_gradient</em> = 4, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.6">weight_decay</em> = 0.01, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.7">num_train_epochs</em> = 1, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.8">max_input_length</em> = 256, <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS2.p1.1.9">max_target_length</em> = 256. Finally, we used the fine-tuned model to generate translations for the development, test, and blind sets.</p>
</div>
<div class="ltx_para" id="Ch6.S3.SS2.p2">
<p class="ltx_p" id="Ch6.S3.SS2.p2.1">At first glance, the fine-tuning step might look redundant if the LLM can achieve the same translation quality directly, either via zero-shot translation or few-shot in-context learning <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>)</cite>. However, domain-specific or terminology-based knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Treviso et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib190" title="">2023</a>)</cite> from a massive LLM to a compact task-oriented MT model can help boost efficiency at inference time while enhancing domain adaptation and terminology adherence. Obviously, when authentic in-domain data is available, it can be used for fine-tuning instead of synthetic data for domain adaptation of the MT model. In production workflows, only segments that do not meet specific quality criteria are passed to either human or automatic post-editing. Hence, deployment of a model fine-tuned on in-domain data can reduce the number of translations that need post-editing.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch6.S3.SS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">6.3.3     Terminology-constrained Automatic Post-Editing</h4>
<div class="ltx_para" id="Ch6.S3.SS3.p1">
<p class="ltx_p" id="Ch6.S3.SS3.p1.1">For the shared task, the organisers provided two term sets for each source sentence in the test and blind datasets, and expected the participants to generate two translations that use one term set each. In this step of terminology-constrained automatic post-editing, we aim to refine the translations generated by an MT system by inserting the required terminology. To this end, we checked the translations generated by the fine-tuned model from the previous step (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS2" title="6.3.2 Fine-tuning ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.3.2</span></a>). For each term set provided for the sentence, if the translation does not include all the terms, we ran this step of terminology insertion into the translation.</p>
</div>
<div class="ltx_para" id="Ch6.S3.SS3.p2">
<p class="ltx_p" id="Ch6.S3.SS3.p2.1">This step involves instructing ChatGPT to post-edit the translation by making sure it includes all the terms without changing the rest of the translation. For the API’s parameters, we used <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS3.p2.1.1">top_p</em> 1 and <em class="ltx_emph ltx_font_italic" id="Ch6.S3.SS3.p2.1.2">temperature</em> values 0 and 0.2, and then chose the generation that fixed more terms.</p>
</div>
<div class="ltx_para ltx_noindent" id="Ch6.S3.SS3.p3">
<svg class="ltx_picture" height="90.29" id="Ch6.S3.SS3.p3.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,90.29) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 4.21 L 0 86.07 C 0 88.4 1.89 90.29 4.21 90.29 L 595.79 90.29 C 598.11 90.29 600 88.4 600 86.07 L 600 4.21 C 600 1.89 598.11 0 595.79 0 L 4.21 0 C 1.89 0 0 1.89 0 4.21 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 0.28 4.21 L 0.28 69.56 L 599.72 69.56 L 599.72 4.21 C 599.72 2.04 597.96 0.28 595.79 0.28 L 4.21 0.28 C 2.04 0.28 0.28 2.04 0.28 4.21 Z" style="stroke:none"></path></g><g fill="#808080" fill-opacity="1.000000"><path d="M 0.28 69.84 L 0.28 86.07 C 0.28 88.25 2.04 90.01 4.21 90.01 L 595.79 90.01 C 597.96 90.01 599.72 88.25 599.72 86.07 L 599.72 69.84 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 8.36 12.09)"><foreignobject height="45.66" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="577.74"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="Ch6.S3.SS3.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1" style="width:417.5pt;"><span class="ltx_ERROR undefined" id="Ch6.S3.SS3.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1">\tcb@lua@color</span>
<span class="ltx_para" id="Ch6.S3.SS3.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="Ch6.S3.SS3.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1">tcbcolupper</span>
<span class="ltx_itemize" id="Ch6.S3.I2">
<span class="ltx_item" id="Ch6.S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_para" id="Ch6.S3.I2.ix1.p1">
<span class="ltx_p" id="Ch6.S3.I2.ix1.p1.8">In the following <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.1.m1.1"><semantics id="Ch6.S3.I2.ix1.p1.1.m1.1a"><mo id="Ch6.S3.I2.ix1.p1.1.m1.1.1" xref="Ch6.S3.I2.ix1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.1.m1.1b"><lt id="Ch6.S3.I2.ix1.p1.1.m1.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.1.m1.1d">&lt;</annotation></semantics></math>tgt_lang<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.2.m2.1"><semantics id="Ch6.S3.I2.ix1.p1.2.m2.1a"><mo id="Ch6.S3.I2.ix1.p1.2.m2.1.1" xref="Ch6.S3.I2.ix1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.2.m2.1b"><gt id="Ch6.S3.I2.ix1.p1.2.m2.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.2.m2.1d">&gt;</annotation></semantics></math> translation, use the <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.3.m3.1"><semantics id="Ch6.S3.I2.ix1.p1.3.m3.1a"><mo id="Ch6.S3.I2.ix1.p1.3.m3.1.1" xref="Ch6.S3.I2.ix1.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.3.m3.1b"><lt id="Ch6.S3.I2.ix1.p1.3.m3.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.3.m3.1d">&lt;</annotation></semantics></math>tgt_term<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.4.m4.1"><semantics id="Ch6.S3.I2.ix1.p1.4.m4.1a"><mo id="Ch6.S3.I2.ix1.p1.4.m4.1.1" xref="Ch6.S3.I2.ix1.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.4.m4.1b"><gt id="Ch6.S3.I2.ix1.p1.4.m4.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.4.m4.1d">&gt;</annotation></semantics></math> to translate the <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.5.m5.1"><semantics id="Ch6.S3.I2.ix1.p1.5.m5.1a"><mo id="Ch6.S3.I2.ix1.p1.5.m5.1.1" xref="Ch6.S3.I2.ix1.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.5.m5.1b"><lt id="Ch6.S3.I2.ix1.p1.5.m5.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.5.m5.1d">&lt;</annotation></semantics></math>src_lang<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.6.m6.1"><semantics id="Ch6.S3.I2.ix1.p1.6.m6.1a"><mo id="Ch6.S3.I2.ix1.p1.6.m6.1.1" xref="Ch6.S3.I2.ix1.p1.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.6.m6.1b"><gt id="Ch6.S3.I2.ix1.p1.6.m6.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.6.m6.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.6.m6.1d">&gt;</annotation></semantics></math> term <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.7.m7.1"><semantics id="Ch6.S3.I2.ix1.p1.7.m7.1a"><mo id="Ch6.S3.I2.ix1.p1.7.m7.1.1" xref="Ch6.S3.I2.ix1.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.7.m7.1b"><lt id="Ch6.S3.I2.ix1.p1.7.m7.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.7.m7.1d">&lt;</annotation></semantics></math>src_term<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix1.p1.8.m8.1"><semantics id="Ch6.S3.I2.ix1.p1.8.m8.1a"><mo id="Ch6.S3.I2.ix1.p1.8.m8.1.1" xref="Ch6.S3.I2.ix1.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix1.p1.8.m8.1b"><gt id="Ch6.S3.I2.ix1.p1.8.m8.1.1.cmml" xref="Ch6.S3.I2.ix1.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix1.p1.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix1.p1.8.m8.1d">&gt;</annotation></semantics></math>, and the…<span class="ltx_note ltx_role_footnote" id="Ch6.footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We can add more terms, if needed.</span></span></span></span>
</span></span>
<span class="ltx_item" id="Ch6.S3.I2.ix2" style="list-style-type:none;">
<span class="ltx_para" id="Ch6.S3.I2.ix2.p1">
<span class="ltx_p" id="Ch6.S3.I2.ix2.p1.1">Leave everything else the same.\n\n</span>
</span></span>
<span class="ltx_item" id="Ch6.S3.I2.ix3" style="list-style-type:none;"></span>
<span class="ltx_item" id="Ch6.S3.I2.ix4" style="list-style-type:none;">
<span class="ltx_para" id="Ch6.S3.I2.ix4.p1">
<span class="ltx_p" id="Ch6.S3.I2.ix4.p1.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix4.p1.1.m1.1"><semantics id="Ch6.S3.I2.ix4.p1.1.m1.1a"><mo id="Ch6.S3.I2.ix4.p1.1.m1.1.1" xref="Ch6.S3.I2.ix4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix4.p1.1.m1.1b"><lt id="Ch6.S3.I2.ix4.p1.1.m1.1.1.cmml" xref="Ch6.S3.I2.ix4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix4.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix4.p1.1.m1.1d">&lt;</annotation></semantics></math>src_lang<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix4.p1.2.m2.1"><semantics id="Ch6.S3.I2.ix4.p1.2.m2.1a"><mo id="Ch6.S3.I2.ix4.p1.2.m2.1.1" xref="Ch6.S3.I2.ix4.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix4.p1.2.m2.1b"><gt id="Ch6.S3.I2.ix4.p1.2.m2.1.1.cmml" xref="Ch6.S3.I2.ix4.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix4.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix4.p1.2.m2.1d">&gt;</annotation></semantics></math>: <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix4.p1.3.m3.1"><semantics id="Ch6.S3.I2.ix4.p1.3.m3.1a"><mo id="Ch6.S3.I2.ix4.p1.3.m3.1.1" xref="Ch6.S3.I2.ix4.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix4.p1.3.m3.1b"><lt id="Ch6.S3.I2.ix4.p1.3.m3.1.1.cmml" xref="Ch6.S3.I2.ix4.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix4.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix4.p1.3.m3.1d">&lt;</annotation></semantics></math>src_segment<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix4.p1.4.m4.1"><semantics id="Ch6.S3.I2.ix4.p1.4.m4.1a"><mo id="Ch6.S3.I2.ix4.p1.4.m4.1.1" xref="Ch6.S3.I2.ix4.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix4.p1.4.m4.1b"><gt id="Ch6.S3.I2.ix4.p1.4.m4.1.1.cmml" xref="Ch6.S3.I2.ix4.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix4.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix4.p1.4.m4.1d">&gt;</annotation></semantics></math>\n</span>
</span></span>
<span class="ltx_item" id="Ch6.S3.I2.ix5" style="list-style-type:none;">
<span class="ltx_para" id="Ch6.S3.I2.ix5.p1">
<span class="ltx_p" id="Ch6.S3.I2.ix5.p1.4"><math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix5.p1.1.m1.1"><semantics id="Ch6.S3.I2.ix5.p1.1.m1.1a"><mo id="Ch6.S3.I2.ix5.p1.1.m1.1.1" xref="Ch6.S3.I2.ix5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix5.p1.1.m1.1b"><lt id="Ch6.S3.I2.ix5.p1.1.m1.1.1.cmml" xref="Ch6.S3.I2.ix5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix5.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix5.p1.1.m1.1d">&lt;</annotation></semantics></math>tgt_lang<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix5.p1.2.m2.1"><semantics id="Ch6.S3.I2.ix5.p1.2.m2.1a"><mo id="Ch6.S3.I2.ix5.p1.2.m2.1.1" xref="Ch6.S3.I2.ix5.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix5.p1.2.m2.1b"><gt id="Ch6.S3.I2.ix5.p1.2.m2.1.1.cmml" xref="Ch6.S3.I2.ix5.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix5.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix5.p1.2.m2.1d">&gt;</annotation></semantics></math>: <math alttext="&lt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix5.p1.3.m3.1"><semantics id="Ch6.S3.I2.ix5.p1.3.m3.1a"><mo id="Ch6.S3.I2.ix5.p1.3.m3.1.1" xref="Ch6.S3.I2.ix5.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix5.p1.3.m3.1b"><lt id="Ch6.S3.I2.ix5.p1.3.m3.1.1.cmml" xref="Ch6.S3.I2.ix5.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix5.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix5.p1.3.m3.1d">&lt;</annotation></semantics></math>tgt_segment<math alttext="&gt;" class="ltx_Math" display="inline" id="Ch6.S3.I2.ix5.p1.4.m4.1"><semantics id="Ch6.S3.I2.ix5.p1.4.m4.1a"><mo id="Ch6.S3.I2.ix5.p1.4.m4.1.1" xref="Ch6.S3.I2.ix5.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Ch6.S3.I2.ix5.p1.4.m4.1b"><gt id="Ch6.S3.I2.ix5.p1.4.m4.1.1.cmml" xref="Ch6.S3.I2.ix5.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Ch6.S3.I2.ix5.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Ch6.S3.I2.ix5.p1.4.m4.1d">&gt;</annotation></semantics></math></span>
</span></span>
</span>
</span></span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_section" id="Ch6.S4">
<h3 class="ltx_title ltx_font_bold ltx_title_section">6.4     Evaluation</h3>
<div class="ltx_para" id="Ch6.S4.p1">
<p class="ltx_p" id="Ch6.S4.p1.1">To assess the effectiveness of our process, we conducted two types of evaluation: (i) term-level evaluation in order to measure the level of adherence to the required terminology, and (ii) sentence-level evaluation in order to see whether the process <span class="ltx_text" id="Ch6.S4.p1.1.1">affected</span> the quality of the overall translation.</p>
</div>
<section class="ltx_subsection" id="Ch6.S4.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">6.4.1     Term-level Evaluation</h4>
<div class="ltx_para" id="Ch6.S4.SS1.p1">
<p class="ltx_p" id="Ch6.S4.SS1.p1.1">In Tables <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T3" title="Table 6.3 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T4" title="Table 6.4 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.4</span></a>, we report the number of terms used in the translations of the test and blind datasets, respectively, in respect to the two term sets provided by the organisers. The results show the effectiveness of our proposed process, increasing the integration of the required terms in the final translations of the blind dataset from an average of 36.67% with the baseline generic model to an average of 72.88% after the LLM-based post-editing, across the three language pairs. Interestingly, prompting an LLM to integrate the required terms into the translations generated by a fine-tuned encoder-decoder MT model was more effective than solely using the fine-tuned model.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch6.S4.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">6.4.2     Sentence-level Evaluation</h4>
<div class="ltx_para" id="Ch6.S4.SS2.p1">
<p class="ltx_p" id="Ch6.S4.SS2.p1.1">After the end of the submission phase, the organisers released the references for the participants to conduct automatic evaluation. The main purpose of this sentence-based evaluation process is to determine whether terminology integration affected the overall quality of translation. In general, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T4" title="Table 6.4 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.4</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T5" title="Table 6.5 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.5</span></a>, this terminology-constrained automatic post-editing step significantly increased the inclusion of the necessary terms into the final translation while improving translation quality across the three language pairs.</p>
</div>
<figure class="ltx_table" id="Ch6.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch6.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.3.1">Total [1]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.4.1">Used [1]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.5.1">Total [2]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.6.1">Used [2]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T3.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.1.1.7.1">Avg %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch6.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.2.1.1.1">DE-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.3">432</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.4">291</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.5">317</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.6">168</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.2.1.7">60.18</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.2">432</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.3">302</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.4">317</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.5">165</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.3.2.6">60.98</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.4.3">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.2">432</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.4.3.3.1">397</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.4">317</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.4.3.5.1">239</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.4.3.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.4.3.6.1">83.65</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.5.4.1.1">EN-CS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.3">550</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.4">221</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.5">313</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.6">139</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.5.4.7">42.30</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.6.5">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.2">550</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.3">135</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.4">313</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.5">108</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.6.5.6">29.53</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.7.6">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.2">550</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.7.6.3.1">466</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.4">313</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.7.6.5.1">283</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.7.6.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.7.6.6.1">87.57</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.8.7.1.1">ZH-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.3">1779</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.4">498</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.5">1938</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.6">491</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.8.7.7">26.66</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.9.8">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.2">1779</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.3">854</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.4">1938</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.5">570</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.9.8.6">38.71</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.10.9">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.2">1779</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.10.9.3.1">1137</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.4">1938</td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.5"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.10.9.5.1">886</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.10.9.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.10.9.6.1">54.81</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch6.T3.1.11.10.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.11.10.1.1">Avg. %</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.11.10.2">Baseline</td>
<td class="ltx_td ltx_border_t" id="Ch6.T3.1.11.10.3"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T3.1.11.10.4"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T3.1.11.10.5"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T3.1.11.10.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T3.1.11.10.7">43.05</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.12.11">
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.12.11.1">Fine-tuned</td>
<td class="ltx_td" id="Ch6.T3.1.12.11.2"></td>
<td class="ltx_td" id="Ch6.T3.1.12.11.3"></td>
<td class="ltx_td" id="Ch6.T3.1.12.11.4"></td>
<td class="ltx_td" id="Ch6.T3.1.12.11.5"></td>
<td class="ltx_td ltx_align_center" id="Ch6.T3.1.12.11.6">43.07</td>
</tr>
<tr class="ltx_tr" id="Ch6.T3.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T3.1.13.12.1">Term APE</td>
<td class="ltx_td ltx_border_bb" id="Ch6.T3.1.13.12.2"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T3.1.13.12.3"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T3.1.13.12.4"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T3.1.13.12.5"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T3.1.13.12.6"><span class="ltx_text ltx_font_bold" id="Ch6.T3.1.13.12.6.1">75.34</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6.3: </span>For the test dataset, the number of terms used in the translations from the first term set [1] and the second term set [2]. According to the results, terminology-constrained automatic post-editing (“Term APE”) using ChatGPT achieved the best adoption of the required terminology.</figcaption>
</figure>
<figure class="ltx_table" id="Ch6.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch6.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch6.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.3.1">Total [1]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.4.1">Used [1]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.5.1">Total [2]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.6.1">Used [2]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T4.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.1.1.7.1">Avg %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch6.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.2.1.1.1">DE-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.3">11357</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.4">4120</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.5">11202</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.6">4623</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.2.1.7">38.77</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.2">11357</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.3">4130</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.4">11202</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.5">4621</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.3.2.6">38.81</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.4.3">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.2">11357</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.4.3.3.1">6257</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.4">11202</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.5"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.4.3.5.1">5893</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.4.3.6"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.4.3.6.1">53.85</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.5.4.1.1">EN-CS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.3">10626</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.4">3964</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.5">10563</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.6">5122</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.5.4.7">42.90</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.6.5">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.2">10626</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.3">3397</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.4">10563</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.5">4412</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.6.5.6">36.87</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.7.6">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.2">10626</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.7.6.3.1">8727</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.4">10563</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.5"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.7.6.5.1">8681</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.7.6.6"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.7.6.6.1">82.16</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.8.7.1.1">ZH-EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.3">2892</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.4">1375</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.5">2908</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.6">265</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.8.7.7">28.33</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.9.8">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.2">2892</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.3">1422</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.4">2908</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.5">970</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.9.8.6">41.26</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.10.9">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.1">Term APE</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.2">2892</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.10.9.3.1">2471</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.4">2908</td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.5"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.10.9.5.1">2322</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.10.9.6"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.10.9.6.1">82.65</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ch6.T4.1.11.10.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.11.10.1.1">Avg. %</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.11.10.2">Baseline</td>
<td class="ltx_td ltx_border_t" id="Ch6.T4.1.11.10.3"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T4.1.11.10.4"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T4.1.11.10.5"></td>
<td class="ltx_td ltx_border_t" id="Ch6.T4.1.11.10.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T4.1.11.10.7">36.67</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.12.11">
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.12.11.1">Fine-tuned</td>
<td class="ltx_td" id="Ch6.T4.1.12.11.2"></td>
<td class="ltx_td" id="Ch6.T4.1.12.11.3"></td>
<td class="ltx_td" id="Ch6.T4.1.12.11.4"></td>
<td class="ltx_td" id="Ch6.T4.1.12.11.5"></td>
<td class="ltx_td ltx_align_center" id="Ch6.T4.1.12.11.6">38.98</td>
</tr>
<tr class="ltx_tr" id="Ch6.T4.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T4.1.13.12.1">Term APE</td>
<td class="ltx_td ltx_border_bb" id="Ch6.T4.1.13.12.2"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T4.1.13.12.3"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T4.1.13.12.4"></td>
<td class="ltx_td ltx_border_bb" id="Ch6.T4.1.13.12.5"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T4.1.13.12.6"><span class="ltx_text ltx_font_bold" id="Ch6.T4.1.13.12.6.1">72.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6.4: </span>For the blind dataset, the number of terms used in the translations from the first term set [1] and the second term set [2]. According to the results, terminology-based automatic post-editing (“Term APE”) using ChatGPT achieved the best adoption of the required terminology.</figcaption>
</figure>
<div class="ltx_para" id="Ch6.S4.SS2.p2">
<p class="ltx_p" id="Ch6.S4.SS2.p2.1">For the automatic evaluation of each MT system, we used the BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib140" title="">2002</a>)</cite>, chrF++ <cite class="ltx_cite ltx_citemacro_citep">(Popović,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib149" title="">2017</a>)</cite>, and COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib158" title="">2020</a>)</cite> metrics. Since many of the Chinese-to-English segments in the blind dataset did not have two term sets, we evaluated only those that had two term sets (1629 segments out of 2640 segments). We observe that the evaluation scores of the Chinese-to-English translation task are much lower than those of the two other language pairs. This can be due to the literary nature of the blind dataset extracted from Chinese novels, which might be difficult for both the MT model and automatic evaluation metrics.</p>
</div>
<figure class="ltx_table" id="Ch6.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Ch6.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ch6.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch6.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ch6.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.2.1">Count</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.3.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.4.1">BLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.5.1">chrF++</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ch6.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.1.1.6.1">COMET</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ch6.T5.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch6.T5.1.2.1.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.2.1.1.1">DE-EN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Ch6.T5.1.2.1.2" rowspan="5"><span class="ltx_text" id="Ch6.T5.1.2.1.2.1">2963</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T5.1.2.1.3">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T5.1.2.1.4">19.81</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T5.1.2.1.5">48.04</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ch6.T5.1.2.1.6">21.81</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.3.2">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.3.2.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.3.2.2">19.27</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.3.2.3">47.75</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.3.2.4">21.51</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.4.3.1">Term APE [1]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.4.3.2">32.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.4.3.3">60.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.4.3.4">40.25</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.5.4">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.5.4.1">Term APE [2]</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.5.4.2">27.84</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.5.4.3">56.84</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.5.4.4">33.20</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.6.5">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.6.5.1">Term APE Avg.</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.6.5.2"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.6.5.2.1">30.10</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.6.5.3"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.6.5.3.1">58.84</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.6.5.4"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.6.5.4.1">36.73</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch6.T5.1.7.6.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.7.6.1.1">EN-CS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Ch6.T5.1.7.6.2" rowspan="5"><span class="ltx_text" id="Ch6.T5.1.7.6.2.1">3005</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.7.6.3">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.7.6.4">29.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.7.6.5">53.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.7.6.6">50.90</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.8.7">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.8.7.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.8.7.2">24.54</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.8.7.3">49.14</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.8.7.4">33.78</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.9.8.1">Term APE [1]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.9.8.2">45.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.9.8.3">67.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.9.8.4">79.84</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.10.9">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.10.9.1">Term APE [2]</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.10.9.2">37.88</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.10.9.3">61.19</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.10.9.4">63.64</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.11.10">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.11.10.1">Term APE Avg.</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.11.10.2"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.11.10.2.1">41.77</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.11.10.3"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.11.10.3.1">64.28</span></td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.11.10.4"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.11.10.4.1">71.74</span></td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ch6.T5.1.12.11.1" rowspan="5"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.12.11.1.1">ZH-EN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ch6.T5.1.12.11.2" rowspan="5"><span class="ltx_text" id="Ch6.T5.1.12.11.2.1">1629</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.12.11.3">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.12.11.4">6.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.12.11.5">27.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.12.11.6">-50.90</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.13.12">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.13.12.1">Fine-tuned</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.13.12.2">7.76</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.13.12.3">29.26</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.13.12.4">-38.83</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.14.13.1">Term APE [1]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.14.13.2">9.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.14.13.3">32.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ch6.T5.1.14.13.4">-18.96</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.15.14">
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.15.14.1">Term APE [2]</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.15.14.2">11.93</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.15.14.3">35.30</td>
<td class="ltx_td ltx_align_center" id="Ch6.T5.1.15.14.4">-13.51</td>
</tr>
<tr class="ltx_tr" id="Ch6.T5.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T5.1.16.15.1">Term APE Avg.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T5.1.16.15.2"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.16.15.2.1">10.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T5.1.16.15.3"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.16.15.3.1">34.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ch6.T5.1.16.15.4"><span class="ltx_text ltx_font_bold" id="Ch6.T5.1.16.15.4.1">-16.24</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6.5: </span>Automatic evaluation of the overall translation quality across the three language pairs based on the blind dataset. The “Baseline” refers to the OPUS model without fine-tuning, while “Fine-tuned” refers to the model after domain adaptation with the bilingual terminology-based synthetic data generated by an LLM. Finally, the three last rows for each language pair refer to using ChatGPT for terminology-constrained automatic post-editing (“Term APE”) of the MT output generated by the fine-tuned model. In other words, “Term APE [1]” indicates the results when the first term set was used to prompt ChatGPT to integrate terms of this set into the translation generated by the fine-tuned model, while “Term APE [2]” refers to using the second term set. Finally, “Term APE Avg.” is the average of “Term APE [1]” and “Term APE [2]” for each language pair. Terminology-constrained automatic post-editing with ChatGPT achieves the best results across the three language pairs in terms of the overall translation quality. As reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T4" title="Table 6.4 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.4</span></a>, the number of terms integrated after the automatic post-editing step also increased.</figcaption>
</figure>
<div class="ltx_para" id="Ch6.S4.SS2.p3">
<p class="ltx_p" id="Ch6.S4.SS2.p3.1">Moreover, it is worth noting that we used the English term while generating bilingual synthetic data (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS1" title="6.3.1 Synthetic Data Generation ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>) for the three language pairs. However, English is the target language for both Chinese-to-English and German-to-English language directions, while it is the source language for the English-to-Czech language direction. This can explain the performance degradation after the fine-tuning step in the English-to-Czech language direction (cf. Tables <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T4" title="Table 6.4 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T5" title="Table 6.5 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.5</span></a>). In other words, it is recommended in the step of bilingual synthetic data generation to either use the target term or both the source and target terms while prompting the LLM to generate translation pairs.</p>
</div>
<div class="ltx_para" id="Ch6.S4.SS2.p4">
<p class="ltx_p" id="Ch6.S4.SS2.p4.1">As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.S3.SS3" title="6.3.3 Terminology-constrained Automatic Post-Editing ‣ 6.3 Method ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.3.3</span></a>, our final step of terminology-constrained automatic post-editing involves instructing an LLM to insert terms that were missing from the output of the fine-tuned model. This significantly increased term usage across all the Chinese-to-English, English-to-Czech, and German-to-English language pairs (cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T4" title="Table 6.4 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.4</span></a>). Furthermore, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6.T5" title="Table 6.5 ‣ 6.4.2 Sentence-level Evaluation ‣ 6.4 Evaluation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6.5</span></a>, this step had no detrimental effects on translation quality. In fact, integrating the necessary terms into the translation using ChatGPT improved translation quality according to our automatic evaluation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Ch6.S5">
<h3 class="ltx_title ltx_font_bold ltx_title_section">6.5     Conclusion and Future Work</h3>
<div class="ltx_para" id="Ch6.S5.p1">
<p class="ltx_p" id="Ch6.S5.p1.1">In this work, we showed that applying a multistep process of mixed fine-tuning on terminology-based synthetic bilingual data and then terminology-constrained automatic post-editing with an LLM can increase the adherence to the pre-approved terms in the generated translations. By the end of the process, the use of the required terms has increased in the translations of the blind dataset across the three language pairs from an average of 36.67% with the baseline generic model to an average of 72.88% after instructing an LLM to integrate the required terms into the translations.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p2">
<p class="ltx_p" id="Ch6.S5.p2.1">Due to the task restrictions, we had to fine-tune OPUS models only. We would like to experiment with fine-tuning NLLB models, and probably the new SeamlessM4T <cite class="ltx_cite ltx_citemacro_citep">(Barrault et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib9" title="">2023</a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, and MADLAD-400 models <cite class="ltx_cite ltx_citemacro_citep">(Kudugunta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib101" title="">2023</a>)</cite>, on the same data and compare the output quality. In our experiments, we employed ChatGPT “gpt-3.5-turbo” for both terminology-based synthetic data generation and terminology-constrained automatic post-editing, as it is a relatively efficient and cost-effective option. In the future, we would like to repeat the same experiments with GPT-4 in order to assess the benefit of using a stronger language model on overall performance. We observe that BLOOM can be used as an alternative LLM for data generation; however, one-shot generation might work better than zero-shot generation. In this case, the prompt can consist of a term, a bilingual sentence pair, and then another term. Interestingly, the model will predict a new translation pair including the second term. While certain open-source models such as Llama 2 and Falcon might be employed for the terminology-constrained automatic post-editing step for certain languages, we suspect that they will need fine-tuning before being reliably usable for most languages.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p3">
<p class="ltx_p" id="Ch6.S5.p3.1">In future work, we will carry out a deeper analysis<span class="ltx_note ltx_role_footnote" id="Ch6.footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>It is recommended to ask professional linguists to analyse both the synthetic data and output translations, assess the quality, and report common linguistic characteristics.</span></span></span> of the generated synthetic data together with the outputs of the fine-tuned models in order to understand how the properties of the synthetic data affect the fine-tuning results. It is important also to test the same approach for other languages, especially low-resource language pairs.</p>
</div>
<div class="ltx_para" id="Ch6.S5.p4">
<p class="ltx_p" id="Ch6.S5.p4.1">Moreover, it would be interesting to exclude the fine-tuning step and assess the overall translation quality after LLM-based post-editing. It is possible that domain adaptation through fine-tuning the baseline MT model either on authentic or synthetic data would still be beneficial. It can lead to domain-specific improvements in the overall translation quality that may not be achievable by the baseline model or the terminology-constrained post-editing step. Again, deploying a model fine-tuned on in-domain data into production can enhance terminology adherence in initial translations. As there is no need to send the translations that already include the pre-approved terms to the LLM for terminology-constrained post-editing, this can reduce the number of translations that require post-editing. Such an efficient workflow can allow us to save resources, and minimise latency at inference time. Similarly, there are potential advantages of employing an LLM for post-editing rather than for direct translation. Instead of solely relying on the translation quality of the LLM, quality estimation can be performed to select the best MT model in general or for the current source text segment. <span class="ltx_text" id="Ch6.S5.p4.1.1">Ultimately,</span> only segments that do not meet quality criteria are then passed to the LLM for post-editing.</p>
</div>
</section>
<section class="ltx_chapter" id="Ch7">
<h2 class="ltx_title ltx_font_bold ltx_title_chapter">Chapter 7      Conclusions and Future Work</h2>
<div class="ltx_para" id="Ch7.p1">
<p class="ltx_p" id="Ch7.p1.1">The previous chapters explained how I addressed the two main research questions and elaborated on the findings of my research. This chapter concludes the accomplished progress and potential future work.</p>
</div>
<section class="ltx_section" id="Ch7.S1">
<h3 class="ltx_title ltx_font_bold ltx_title_section">7.1     Conclusions</h3>
<div class="ltx_para" id="Ch7.S1.p1">
<p class="ltx_p" id="Ch7.S1.p1.1">In the first research question, I examined how to leverage language modelling techniques in general and LLMs in particular to improve translation features that involve human interaction and continuous feedback, such as adaptive MT, terminology-constrained MT, domain-aware automatic post-editing, auto-suggestion and auto-completion. In the second research question, I addressed a common scenario in the translation industry, namely receiving highly specialised projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. One way to address this question is through domain-specific text generation with LLMs and then fine-tuning an MT model using the generated bilingual in-domain synthetic data. Furthermore, this question overlaps with the first question, since real-time adaptive MT with LLMs can serve as an alternative approach to address in-domain data scarcity.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p2">
<p class="ltx_p" id="Ch7.S1.p2.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2</span></a>, I proposed a novel approach to domain adaptation leveraging state-of-the-art pretrained LLMs for <span class="ltx_text" id="Ch7.S1.p2.1.1">domain-specific</span> data augmentation for MT, simulating the domain characteristics of either (a) a small bilingual dataset, or (b) the monolingual source text to be translated. Combining this idea with back-translation, I was able to generate huge amounts of synthetic bilingual <span class="ltx_text" id="Ch7.S1.p2.1.2">in-domain</span> data for both use-cases. For this investigation, I used the <span class="ltx_text" id="Ch7.S1.p2.1.3">state-of-the-art</span> Transformer architecture. I employed mixed fine-tuning to train models that significantly improve translation of in-domain texts. In this work, I suggested potential future work ideas, such as terminology-aware text generation, as well as low-resource and multilingual text generation for domain-specific MT, some of which I have already addressed in the next papers/chapters.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p3">
<p class="ltx_p" id="Ch7.S1.p3.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3</span></a>, I described our submissions to WMT 2022 Shared Task on Word-level Auto-completion, for the Chinese-to-English, English-to-Chinese,
German-to-English, and English-to-German language directions. I discussed how utilising random sampling to generate diverse alternatives can reveal good results. Moreover, the survey I conducted shows that suggesting alternatives can inspire translators and limit their need to refer to external resources, which hopefully boosts their productivity. In the future, I intend to expand this work by investigating whether domain adaptation and/or simulated annealing can improve translation suggestions. In a user survey I conducted (cf. Chapter 3), 90.2% of the participants stated that they believed that word-level auto-completion was helpful. This can be considered an indicator that users appreciate adaptivity and interactivity features in MT system.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p4">
<p class="ltx_p" id="Ch7.S1.p4.1">Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> emphasises the significance of consistency for high-quality translation, especially in domain-specific projects. It addresses the challenge of real-time adaptation through leveraging LLMs. In-context learning involves replicating text generation patterns without additional fine-tuning. Inspired by this idea, I explored the use of in-context learning to improve real-time adaptive MT. I investigated whether prompting LLMs with similar translations (fuzzy matches) and/or terminology enables them to simulate domain and style characteristics. Experiments strongly suggest that LLMs can adapt to domain-specific translation pairs and terminology, and that the quality of adaptive translations can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p5">
<p class="ltx_p" id="Ch7.S1.p5.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>, I described the systems submitted to the WMT 2023 Terminology Shared Task for German-to-English, English-to-Czech, and Chinese-to-English language pairs. The aim of the task was to improve MT by developing systems that can accurately translate technical terms. The methods used involve leveraging LLMs to generate synthetic bilingual terminology-based data and to post-edit translations though inserting missing terminology. The process consists of four steps: generating terminology-based synthetic data with an LLM, fine-tuning a generic encoder-decoder MT model, generating translations with the fine-tuned model, and finally using an LLM to fix the translations that do not include the required terms. The results indicate that the use of pre-approved terms in the translations doubles across the three language pairs, which is very encouraging.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p6">
<p class="ltx_p" id="Ch7.S1.p6.1">Revisiting my research questions, this research was able to answer both of the two main research questions and their sub-questions. For the first research question, this work explored a number of scenarios to employ language models to improve the quality of adaptive MT at inference time, and achieved performance gains based on both human and automatic evaluation. Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrated that by only utilising the autoregressive property of NMT models through teacher forcing, the performance of word-level auto-completion can surpass other approaches, without using any external models.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our systems achieved the first and second places at WMT 2023 Shared Task on Word-level Auto-completion.</span></span></span> Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> showed that LLM in-context learning can enhance real-time adaptive MT, leveraging user (translator) feedback and modifications. These models (e.g. ChatGPT, BLOOM, Mistral) can adapt to approved in-domain translation pairs and/or terminology while translating new texts. Moreover, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrated that LLMs can be prompted for terminology-constrained automatic post-editing of translations generated by MT models, which doubled the integration of the pre-approved terminology into the final translations. In other words, instead of inputting a raw source sentence to translate, providing more linguistic information improves translation quality. As for the second research question, this work demonstrated that pre-trained LLMs can be used for in-domain data augmentation to improve domain-specific MT models. In-domain texts or terminology can be fed to an LLM to generate more data, simulating the domain characteristics of the original text. As demonstrated by Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch2" title="Chapter 2 Domain-Specific Text Generation for Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">2</span></a> and Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>, if there is only a small in-domain dataset (e.g. 1000 sentences) or a glossary, LLMs can be used to generate large domain-specific datasets (e.g. 200,000 sentences). Then, the generic model can be fine-tuned on this synthetic data, which improves translation quality of in-domain texts.</p>
</div>
<div class="ltx_para" id="Ch7.S1.p7">
<p class="ltx_p" id="Ch7.S1.p7.1">This research is beneficial for translators and MT providers alike. For translators, it aims to provide them with better flexibility and consistency while translating. Boosting MT real-time adaptation can be considered one method of enhancing the user-friendliness of translation environments. Once a linguist edits a translation, the system should comply with the approved modifications in subsequent translations. For MT providers, adaptability is essential for serving a wider range of customers from diverse domains. While some MT providers offer the feature of fine-tuning on customised data, giving their clients the ability to adapt an MT system out of the box without any further fine-tuning (or on top of fine-tuning) is an added benefit. Language service providers seek to achieve the consistency required by their clients efficiently. However, new clients might not have sufficient TMs if any, and access to TMs owned by other clients is restricted by ethics and laws of intellectual property. Hence, I believe that the ability to employ both in-domain synthetic data for fine-tuning, and in-context learning for real-time adaptivity can help improve the productivity and satisfaction of both linguists and clients.</p>
</div>
</section>
<section class="ltx_section" id="Ch7.S2">
<h3 class="ltx_title ltx_font_bold ltx_title_section">7.2     Future Work</h3>
<div class="ltx_para" id="Ch7.S2.p1">
<p class="ltx_p" id="Ch7.S2.p1.1">The design and execution of these experiments were guided by a commitment to making the research findings highly applicable and beneficial to real-world production scenarios. For instance, in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a>, I tried to address a number of common use cases, such as real-time sentence-level translation adaptivity and terminology adherence. Moreover, throughout my research, I covered a wide range of languages with diverse scripts and availability of data resources, exploring the opportunities and limitations associated with these languages. While Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> addresses using LLMs for both translation and real-time adaptation to fuzzy matches and terminology, Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a> investigates using LLMs for terminology-constrained automatic post-editing of translations generated by other MT systems. As shown in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a>, when the original translation quality of an LLM is much lower than that produced by an (encoder-decoder) MT model for a language pair, starting from this strong baseline and trying to improve it can reveal better results (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4.S6" title="4.6 Incorporating Encoder-Decoder MT ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4.6</span></a>). Nevertheless, this depends on the language and its level of support by the LLM. As stated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch4" title="Chapter 4 Adaptive Machine Translation with Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">4</span></a> and Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch6" title="Chapter 6 Domain Terminology Integration into Machine Translation: Leveraging Large Language Models ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">6</span></a>, real-time adaptive MT is not a replacement for fine-tuning. Hence, starting with fine-tuning, when possible, should be more efficient as the original translation quality of a model will be better, which minimises the need for post-editing. As serving multiple models in production can be challenging, the optimal scenario would be to fine-tune an LLM on both zero-shot and few-shot translation data. As demonstrated in Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5</span></a>, this can help improve not only its ability to produce high-quality zero-shot translation, but also to improve its in-context learning capability to adapt to a range of translation-related instructions.</p>
</div>
<div class="ltx_para" id="Ch7.S2.p2">
<p class="ltx_p" id="Ch7.S2.p2.1">In the future, I would like to further focus on relevant topics, elaborated in the following sections.</p>
</div>
<section class="ltx_subsection" id="Ch7.S2.SS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.1     MT adaptation for multilingual and low-resource settings</h4>
<div class="ltx_para" id="Ch7.S2.SS1.p1">
<p class="ltx_p" id="Ch7.S2.SS1.p1.1">I aim to investigate the aforementioned approaches in multilingual and/or low-resource settings. During my PhD, I worked with more than 20 language pairs. In respect of my research questions, I already experimented with some low-resource languages, such as Czech and Kinyarwanda. Moreover, I co-authored a couple of works on MT for low-resource languages. For example, in our work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib61" title="">Haque et al., 2020a, </a>)</cite>, we applied domain adaptation to an English-to-Hindi MT system. To this end, we used terminology to select relevant target sentences for the AI domain, generated the sources with back-translation, and finally applied mixed fine-tuning of the generic model, which improved the translation quality for the AI domain while retaining the translation quality of generic texts. Similarly, in our work <cite class="ltx_cite ltx_citemacro_citep">(Öktem et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib137" title="">2022</a>)</cite>, we built NMT systems for a very low-resource language, Ladino, utilising diverse data augmentation approaches, employing rule-based MT and back-translation. We collected data in English-to-Spanish and Turkish-to-Spanish language pairs, since Spanish shares a wide range of linguistic characteristics with Ladino. Then, we used a rule-based system to translate Spanish to Ladino. This process resulted in English-to-Synthetic-Ladino and Turkish-to-Synthetic-Ladino datasets, which I used later for building NMT models. Furthermore, I have experience with building a multilingual MT model<span class="ltx_note ltx_role_footnote" id="Ch7.footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Notes on multilingual MT: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.machinetranslation.io/multilingual-nmt/" title="">https://blog.machinetranslation.io/multilingual-nmt/</a></span></span></span> for 10 low-resource Indic languages.<span class="ltx_note ltx_role_footnote" id="Ch7.footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>MT demo: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.machinetranslation.io/" title="">https://www.machinetranslation.io/</a></span></span></span> Multilingual models can exploit the similarity between languages and particularly benefit
low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Imankulova et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib81" title="">2019</a>; Liu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib115" title="">2020</a>; Gala et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib49" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS1.p2">
<p class="ltx_p" id="Ch7.S2.SS1.p2.1">Text generation for low-resource languages can be challenging, since some major LLMs lack decent support for such languages <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; Robinson et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib162" title="">2023</a>)</cite>. As the quality of synthetic data generated affects the quality of the MT model fine-tuned on such data, applying the same approach I used in my research <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite> might not work out of the box. Among the research directions to consider is to start by fine-tuning the LLM on monolingual data in the low-resource target language or a mix of monolingual data in both the source and target languages. Then the fine-tuned model can be used for text generation. Nevertheless, it is always important to evaluate the generated synthetic data through both human linguistic analysis and automatic quality estimation before using it for fine-tuning.
</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS1.p3">
<p class="ltx_p" id="Ch7.S2.SS1.p3.1">Similarly, real-time adaptive MT for low-resource languages might benefit from the availability of target-side monolingual data. Hence, instead of feeding the model with similar sentence pairs, monolingual sentences with higher similarity to the source can be used to adapt the translation. This would require multilingual embeddings to calculate similarity between monolingual data in the target language and the input text in the source language. Moreover, I recommend experimenting with extracting chunks of short bilingual phrases of a low-resource language, and then prompting an LLM to use these phrases either during translation or text generation. Augmenting NMT systems with phrases can be useful in diverse scenarios <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib56" title="">2021</a>; Ghazvininejad et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib52" title="">2023</a>; Puduppully et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib153" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S2.SS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.2     Domain-specific word-level autosuggestions and autocompletion</h4>
<div class="ltx_para" id="Ch7.S2.SS2.p1">
<p class="ltx_p" id="Ch7.S2.SS2.p1.1">In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch3" title="Chapter 3 Translation Word-Level Auto-Completion ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">3</span></a>, I concluded that random sampling can improve word-level auto-completion using generic NMT models. In the future, I hope to investigate the effect of incorporating domain adaptation into this process. Potentially, fine-tuning the baseline model on in-domain data can achieve better performance on the test dataset of the same domain, especially when combined with random sampling.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S2.SS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.3     Analysis of critical errors and text generation for difficult-to-translate words</h4>
<div class="ltx_para" id="Ch7.S2.SS3.p1">
<p class="ltx_p" id="Ch7.S2.SS3.p1.1">The core of this idea is first defining critical errors in translation of in-domain texts (e.g. in the medical domain) and then fixing these errors. In their work, <cite class="ltx_cite ltx_citemacro_citet">Fadaee and Monz, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib45" title="">2018</a>)</cite> introduced variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, they also target the contexts of difficult words and sample sentences that are similar in context. They proposed a two-stage approach, identifying difficult words and sampling with the objective of increasing occurrences of these words, and identifying contexts where these words are difficult to predict and sample sentences similar to the difficult contexts. With targeted sampling of sentences for back-translation, their approach achieved improvements of up to 1.7 BLEU points over back-translation using random sampling. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Huck et al., (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib80" title="">2019</a>)</cite> started by translating the new text and finding out-of-vocabulary words (OOVs). Then, they translated them using bilingual word embeddings. Afterwards, they used the 5-best target words as queries to mine target sentences, and back-translated these sentences, forcing the back-translation of each of the five proposed target OOVs to be the original source OOV. Finally, they used this synthetic data to fine-tune the system. As a result, the translation of OOVs was significantly improved. To boost the quality of target-to-source word alignments of attention-based models, researchers proposed guided alignment training <cite class="ltx_cite ltx_citemacro_citep">(Chen et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib24" title="">2016</a>; Garg et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib51" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS3.p2">
<p class="ltx_p" id="Ch7.S2.SS3.p2.1">In this sense, we can improve the robustness of the MT model by finding such difficult words, and then using them or their full sentences for text generation, or for prompting LLMs through in-context learning. The idea is similar to my previous work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib129" title="">Moslem et al., 2023c, </a>)</cite>, where I used pre-approved terminology to enhance the translation quality. However, it goes beyond terminology to consider other error categories. For example, these problematic words can be named entities (e.g. person, location, organisation, product) in the medical domain, or untranslatables (e.g. websites, email addresses, or phone numbers of hospitals in a certain country). Words or phrases that are difficult to translate can be automatically defined as in <cite class="ltx_cite ltx_citemacro_citet">Fadaee and Monz, (<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib45" title="">2018</a>)</cite>, or using human annotation of a sample dataset. They can also be extracted from localisation comments that are originally authored by software developers to provide rules or hints for localisers. While the idea can be employed to improve MT models through leveraging LLMs, the same concept can be applied to knowledge distillation from a larger teacher LLM to a smaller student LLM to enhance efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S2.SS4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.4     Improving random sampling via simulated annealing</h4>
<div class="ltx_para" id="Ch7.S2.SS4.p1">
<p class="ltx_p" id="Ch7.S2.SS4.p1.1">For word-level auto-completion and to complement our previous work <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib127" title="">Moslem et al., 2022b, </a>)</cite>, I will investigate using simulated annealing. In the beginning, we will allow the search process to explore less-probable options, and it will not always choose the token with the highest probability. Towards the end of the translation, the model will usually choose the option with the highest probability. Sampling temperature and potentially top-k/top-p parameters can control this. This idea can also be investigated to improve the quality of text generation using pre-trained language models <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib124" title="">Moslem et al., 2022a, </a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S2.SS5">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.5     Domain-Specific Efficient Fine-tuning of LLMs</h4>
<div class="ltx_para" id="Ch7.S2.SS5.p1">
<p class="ltx_p" id="Ch7.S2.SS5.p1.1">Several researchers have demonstrated the abilities of LLMs for diverse translation tasks <cite class="ltx_cite ltx_citemacro_citep">(Akter et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib2" title="">2023</a>; Bawden and Yvon,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib11" title="">2023</a>; Cheng et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib27" title="">2023</a>; He et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib67" title="">2023</a>; Hendy et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib68" title="">2023</a>; Hoang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib69" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib86" title="">Jiao et al., 2023b, </a>; Koneru et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib98" title="">2023</a>; Kumar et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib102" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib125" title="">Moslem et al., 2023a, </a>; Peng et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib143" title="">2023</a>; Sia and Duh,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib174" title="">2023</a>; Tan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib182" title="">2023</a>; Vilar et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib192" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib196" title="">Wang et al., 2023a, </a>; Zhu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib220" title="">2023</a>)</cite>. Nevertheless, fine-tuning LLMs specifically for MT tasks can help improve translation quality and efficiency <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>)</cite>. So far, training LLMs to work better with MT mostly involves either: (i) pre-training an LLM to work with zero-shot MT <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib107" title="">Li et al., 2023a, </a>; Schioppa et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib167" title="">2023</a>)</cite>; or (ii) fine-tuning LLMs with the purpose of improving zero-shot capabilities <cite class="ltx_cite ltx_citemacro_citep">(Sia and Duh,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib173" title="">2022</a>; Alves et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib3" title="">2023</a>; Iyer et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib82" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib85" title="">Jiao et al., 2023a, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib109" title="">Li et al., 2023b, </a>; Xu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib204" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib207" title="">Yang et al., 2023b, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib215" title="">Zhang et al., 2023b, </a>)</cite>. Moreover, there are several works that investigate pre-training or fine-tuning encoder-decoder MT models for adaptive MT (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch1.S4" title="1.4 Research Context ‣ Chapter 1 Introduction ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">1.4</span></a>: <span class="ltx_text ltx_font_italic" id="Ch7.S2.SS5.p1.1.1">Research Context</span>), and there is at least one work that compares this with using in-context learning of LLMs for adaptive MT <cite class="ltx_cite ltx_citemacro_citep">(Reinauer et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib160" title="">2023</a>)</cite>. However, there is still a need for research that instead investigates fine-tuning available open-source models to enhance their in-context learning ability for real-time adaptive MT and compares this to current approaches. To this end, these models can be fine-tuned to perform better at in-context learning scenarios, where special prompt templates incorporate in-domain sentences, phrases, or terminology. In Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5</span></a>, I demonstrated that with a relatively small dataset that incorporates a mix of zero-shot and one-shot prompts, fine-tuning significantly enhances Mistral’s in-context learning ability, especially for real-time adaptive MT <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib128" title="">Moslem et al., 2023b, </a>)</cite>. This direction can improve both translation quality and efficiency, especially as fewer examples might be required for in-context learning. Moreover, both inference and fine-tuning of the largest LLMs can be challenging. Hence, knowledge distillation approaches that transfer robust in-context learning features from stronger teachers to smaller students for the purpose of adaptive and interactive MT should be investigated.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS5.p2">
<p class="ltx_p" id="Ch7.S2.SS5.p2.1">Accordingly, I can start by fine-tuning an LLM for zero-shot, one-shot, two-shot translation, and then build on this. The model used can be at least one encoder-decoder model, e.g. NLLB-200 <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib33" title="">2022</a>)</cite>, and one decoder-only model, e.g. BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Le Scao et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib105" title="">2022</a>)</cite>. Furthermore, I can compare other models like Llama <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib187" title="">Touvron et al., 2023b, </a>)</cite>, Falcon (EN-DE-ES-FR) <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib142" title="">2023</a>)</cite>, Mistral/Mixtral (EN-DE-ES-FR-IT) <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib84" title="">2023</a>)</cite>, Jais (AR-EN) <cite class="ltx_cite ltx_citemacro_citep">(Sengupta et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib169" title="">2023</a>)</cite>, Baichuan (ZH) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib206" title="">Yang et al., 2023a, </a>)</cite>, and/or Qwen (ZH) <cite class="ltx_cite ltx_citemacro_citep">(Bai et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib8" title="">2023</a>)</cite>. I am interested in conducting an in-depth investigation into fine-tuning LLMs to enhance their domain-specific in-context learning capability, and improve their ability to work better in adaptive MT scenarios. To this end, I would like to conduct a range of experiments, including: instruction fine-tuning on the in-domain data only; instruction mixed fine-tuning on the in-domain data and a randomly sampled portion of the original generic data, over-sampling the in-domain data; and investigating diverse scenarios of boosting in-context learning for adaptive MT, ranging from zero-shot and few-shot translation to integration of terminology and in-domain monolingual data.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS5.p3">
<p class="ltx_p" id="Ch7.S2.SS5.p3.1">The aforementioned experiments should employ efficient fine-tuning approaches <cite class="ltx_cite ltx_citemacro_citep">(Wan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib193" title="">2023</a>)</cite> such as QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib77" title="">2021</a>; Dettmers et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#bib.bib37" title="">2023</a>)</cite>. I would like to apply the experiments to a range of diverse languages, including high-resource, medium-resource, and low-resource languages, as well as different domains. Preliminary experiments in this direction demonstrate promising results (cf. Chapter <a class="ltx_ref" href="https://arxiv.org/html/2401.14559v1#Ch5" title="Chapter 5 Fine-tuning Large Language Models for Adaptive Machine Translation ‣ Language Modelling Approaches to Adaptive Machine Translation ———————————————————————– PhD Thesis ———————————————————————–"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS5.p4">
<p class="ltx_p" id="Ch7.S2.SS5.p4.1">Among the benefits of fine-tuning open-source LLMs are efficient self-hosting. In other words, those who would like to serve their own LLMs for privacy reasons can utilise an open-source model, and efficiently achieve quality gains comparable to those of strong commercial LLMs. Moreover, so far, for very high-resource languages, LLMs can be used independently. However, for other languages, a hybrid approach using both conventional MT models and LLMs leads to better results, which means we have to deploy/use two models at translation time. If fine-tuning a small “standalone” LLM is possible for both regular (zero-shot) and adaptive (one-shot or few-shot) translation, this would be much more efficient. In addition, researchers can definitely build on this direction rather than having to rely on closed models. Open-source research can lead to more interpretability as we know better what is going on in the background.</p>
</div>
</section>
<section class="ltx_subsection" id="Ch7.S2.SS6">
<h4 class="ltx_title ltx_font_bold ltx_title_subsection">7.2.6     Cross-Lingual Retrieval-Augmented Generation</h4>
<div class="ltx_para" id="Ch7.S2.SS6.p1">
<p class="ltx_p" id="Ch7.S2.SS6.p1.1">Retrieval-augmented MT or cross-lingual generation is crucial in diverse scenarios. For example, legal companies receive huge amounts of documents in one language while they are operating in another language. In such case, they want a quick way to retrieve information in their working language. However, translating them manually is time-consuming and expensive. It can lead to inaccuracies and delays due to the complexity and specificity of legal terminology. Similarly, refugees and immigrants need to retrieve information, e.g. about health and education, in the hosting countries. However, they might require mass information from different countries, and they need to receive this information in their native languages.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS6.p2">
<p class="ltx_p" id="Ch7.S2.SS6.p2.1">Advances in technology have made it possible to use MT for this purpose to quickly translate large volumes of text, making it easier for legal professionals to access the information they need. Due to the specificity and importance of legal documents, these translations often still need to be reviewed or edited by human translators. However, if information retrieval is successfully used, only the documents that matter most to the case at hand can be professionally translated.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS6.p3">
<p class="ltx_p" id="Ch7.S2.SS6.p3.1">Employing information retrieval approaches such as semantic search entails that users would receive results that refer them to the most probable documents. Such an outcome can suffice if the user is seeking information in the style provided by search engines. Nowadays, users might prefer retrieving information in a more comprehensive way that probably summarises the whole document, relevant portions of the document, or even multiple documents, in a cross-lingual manner.</p>
</div>
<div class="ltx_para" id="Ch7.S2.SS6.p4">
<p class="ltx_p" id="Ch7.S2.SS6.p4.1">In this future work, I aim to understand the effect of using multilingual information retrieval approaches in combination with LLMs to retrieve information based on legal or medical queries. In other words, I would like to investigate the effect of initially employing MT before retrieving information with semantic search versus merely using multilingual embeddings, while the final output is generated by a multilingual LLM. Analysing the quality of the generated output by the LLM based on the retrieved information can help understand whether explicit MT is still required, or whether the use of cross-lingual semantic search based on multilingual embedding is sufficient while using a multilingual LLM to generate the final output.</p>
</div>
</section>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al.,  (2023)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad (2023).

</span>
<span class="ltx_bibblock">In-context Examples Selection for Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: ACL 2023</span>, pages 8857–8873, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akter et al.,  (2023)</span>
<span class="ltx_bibblock">
Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, and Graham Neubig (2023).

</span>
<span class="ltx_bibblock">An in-depth look at Gemini’s language abilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2312.11444 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al.,  (2023)</span>
<span class="ltx_bibblock">
Duarte Alves, Nuno Guerreiro, João Alves, José Pombal, Ricardo Rei, José de Souza, Pierre Colombo, and Andre Martins (2023).

</span>
<span class="ltx_bibblock">Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 11127–11148, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anastasopoulos et al.,  (2020)</span>
<span class="ltx_bibblock">
Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and Sylwia Tur (2020).

</span>
<span class="ltx_bibblock">TICO-19: the Translation Initiative for COvid-19.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020</span>, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antoun et al.,  (2021)</span>
<span class="ltx_bibblock">
Wissam Antoun, Fady Baly, and Hazem Hajj (2021).

</span>
<span class="ltx_bibblock">AraGPT2: Pre-Trained Transformer for Arabic Language Generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Sixth Arabic Natural Language Processing Workshop</span>, pages 196–207, Kyiv, Ukraine (Virtual). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Axelrod et al.,  (2011)</span>
<span class="ltx_bibblock">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao (2011).

</span>
<span class="ltx_bibblock">Domain Adaptation via Pseudo In-Domain Data Selection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</span>, pages 355–362, Edinburgh, Scotland, UK. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al.,  (2015)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio (2015).

</span>
<span class="ltx_bibblock">Neural Machine Translation by Jointly Learning to Align and Translate.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015</span>, San Diego, CA, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al.,  (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu (2023).

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2309.16609 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al.,  (2023)</span>
<span class="ltx_bibblock">
Loic Barrault, Andy Chung, David Dale, Ning Dong (ai), Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Peng-Jen Chen, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Abinesh Ramakrishnan, Alexandre Mourachko, Amanda Kallet, Ann Lee, Anna Sun, Bapi Akula, Benjamin Peloquin, Bernie Huang, Bokai Yu, Brian Ellis, Can Balioglu, Carleigh Wood, Changhan Wang, Christophe Ropers, Cynthia Gao, Daniel Li (fair), Elahe Kalbassi, Ethan Ye, Gabriel Mejia Gonzalez, Hirofumi Inaguma, Holger Schwenk, Igor Tufanov, Ilia Kulikov, Janice Lam, Jeff Wang (pm Ai), Juan Pino, Justin Haaheim, Justine Kao, Prangthip Hasanti, Kevin Tran, Maha Elbayad, Marta R Costa-jussa, Mohamed Ramadan, Naji El Hachem, Onur Çelebi, Paco Guzmán, Paden Tomasello, Pengwei Li, Pierre Andrews, Ruslan Mavlyutov, Russ Howes, Safiyyah Saleem, Skyler Wang, Somya Jain, Sravya Popuri, Tuan Tran, Vish Vogeti, Xutai Ma, and Yilin Yang (2023).

</span>
<span class="ltx_bibblock">SeamlessM4T—Massively Multilingual &amp; Multimodal Machine Translation.

</span>
<span class="ltx_bibblock">Meta AI.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Meta AI</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden et al.,  (2020)</span>
<span class="ltx_bibblock">
Rachel Bawden, Giorgio Maria Di Nunzio, Cristian Grozea, Inigo Jauregi Unanue, Antonio Jimeno Yepes, Nancy Mah, David Martinez, Aurélie Névéol, Mariana Neves, Maite Oronoz, Olatz Perez-de Viñaspre, Massimo Piccardi, Roland Roller, Amy Siu, Philippe Thomas, Federica Vezzani, Maika Vicente Navarro, Dina Wiemann, and Lana Yeganova (2020).

</span>
<span class="ltx_bibblock">Findings of the WMT 2020 biomedical translation shared task: Basque, Italian and Russian as new additional languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 660–687, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon,  (2023)</span>
<span class="ltx_bibblock">
Rachel Bawden and François Yvon (2023).

</span>
<span class="ltx_bibblock">Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 157–170, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berzins et al.,  (2019)</span>
<span class="ltx_bibblock">
Aivars Berzins, Khalid Choukri, Maria Giagkou, Andrea Lösch, Hélène Mazo, Stelios Piperidis, Mickaël Rigault, Eileen Schnur, Lilli Smal, Josef van Genabith, and Andrejs Vasiljevs (2019).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">ELRC White Paper: Sustainable Language Data Sharing to Support Language Equality in Multilingual Europe: why Language Data Matters</span>.

</span>
<span class="ltx_bibblock">European Language Resource Coordination &amp; OVD Verlag.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al.,  (2022)</span>
<span class="ltx_bibblock">
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach (2022).

</span>
<span class="ltx_bibblock">GPT-NeoX-20B: An Open-Source Autoregressive Language Model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of BigScience Episode #5 – Workshop on Challenges &amp; Perspectives in Creating Large Language Models</span>, pages 95–136, Stroudsburg, PA, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogoychev and Sennrich,  (2019)</span>
<span class="ltx_bibblock">
Nikolay Bogoychev and Rico Sennrich (2019).

</span>
<span class="ltx_bibblock">Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1911.03362 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al.,  (2021)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W Rae, Erich Elsen, and Laurent Sifre (2021).

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2112.04426 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Britz et al.,  (2017)</span>
<span class="ltx_bibblock">
Denny Britz, Quoc Le, and Reid Pryzant (2017).

</span>
<span class="ltx_bibblock">Effective Domain Mixing for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Second Conference on Machine Translation</span>, pages 118–126, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al.,  (2020)</span>
<span class="ltx_bibblock">
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei (2020).

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems (NeurIPS 2020)</span>, volume 33, pages 1877–1901, Virtual. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan,  (2019)</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan (2019).

</span>
<span class="ltx_bibblock">Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 1800–1809, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burlot and Yvon,  (2018)</span>
<span class="ltx_bibblock">
Franck Burlot and François Yvon (2018).

</span>
<span class="ltx_bibblock">Using Monolingual Data in Neural Machine Translation: a Systematic Study.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 144–155, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al.,  (2021)</span>
<span class="ltx_bibblock">
Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu (2021).

</span>
<span class="ltx_bibblock">Neural Machine Translation with Monolingual Translation Memory.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 7307–7318, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casacuberta et al.,  (2022)</span>
<span class="ltx_bibblock">
Francisco Casacuberta, George Foster, Guoping Huang, Philipp Koehn, Geza Kovacs, Lemao Liu, Shuming Shi, Taro Watanabe, and Chengqing Zong (2022).

</span>
<span class="ltx_bibblock">Findings of the Word-Level AutoCompletion Shared Task in WMT 2022.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages 812–820, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell et al.,  (2019)</span>
<span class="ltx_bibblock">
Isaac Caswell, Ciprian Chelba, and David Grangier (2019).

</span>
<span class="ltx_bibblock">Tagged Back-Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</span>, pages 53–63, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al.,  (2021)</span>
<span class="ltx_bibblock">
Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, and Hui Su (2021).

</span>
<span class="ltx_bibblock">Neural Data-to-Text Generation with LM-based Text Augmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</span>, pages 758–768, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.,  (2016)</span>
<span class="ltx_bibblock">
Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter (2016).

</span>
<span class="ltx_bibblock">Guided Alignment Training for Topic-Aware Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Conferences of the Association for Machine Translation in the Americas: MT Researchers’ Track</span>, pages 121–134, Austin, TX, USA. The Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.,  (2023)</span>
<span class="ltx_bibblock">
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou (2023).

</span>
<span class="ltx_bibblock">Improving translation faithfulness of large Language Models via augmenting instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2308.12674 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al.,  (2022)</span>
<span class="ltx_bibblock">
Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan (2022).

</span>
<span class="ltx_bibblock">Neural Machine Translation with Contrastive Translation Memories.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>, pages 3591–3601, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al.,  (2023)</span>
<span class="ltx_bibblock">
Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan (2023).

</span>
<span class="ltx_bibblock">SCALE: Synergized Collaboration of Asymmetric Language Translation Engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2309.17061 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chinea-Ríos et al.,  (2017)</span>
<span class="ltx_bibblock">
Mara Chinea-Ríos, Álvaro Peris, and Francisco Casacuberta (2017).

</span>
<span class="ltx_bibblock">Adapting Neural Machine Translation with Parallel Synthetic Data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Second Conference on Machine Translation</span>, pages 138–147, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al.,  (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel (2022).

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2204.02311 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chronopoulou et al.,  (2021)</span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser (2021).

</span>
<span class="ltx_bibblock">Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, pages 173–180, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al.,  (2017)</span>
<span class="ltx_bibblock">
Chenhui Chu, Raj Dabre, and Sadao Kurohashi (2017).

</span>
<span class="ltx_bibblock">An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 385–391, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al.,  (2020)</span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning (2020).

</span>
<span class="ltx_bibblock">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 8th International Conference on Learning Representations (ICLR)</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al.,  (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang (2022).

</span>
<span class="ltx_bibblock">No Language Left Behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2207.04672 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coughlin,  (2003)</span>
<span class="ltx_bibblock">
Deborah Coughlin (2003).

</span>
<span class="ltx_bibblock">Correlating automated and human assessments of machine translation quality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of Machine Translation Summit IX: Papers</span>, New Orleans, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crego et al.,  (2016)</span>
<span class="ltx_bibblock">
Josep Crego, Jungi Kim, Guillaume Klein, Anabel Rebollo, Kathy Yang, Jean Senellart, Egor Akhanov, Patrice Brunelle, Aurelien Coquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss, Joshua Johanson, Ardas Khalsa, Raoum Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux, Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, and Peter Zoldan (2016).

</span>
<span class="ltx_bibblock">SYSTRAN’s Pure Neural Machine Translation Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:1610.05540 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabre et al.,  (2017)</span>
<span class="ltx_bibblock">
Raj Dabre, Fabien Cromieres, and Sadao Kurohashi (2017).

</span>
<span class="ltx_bibblock">Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of Machine Translation Summit XVI: Research Track</span>, pages 96–107, Nagoya Japan.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al.,  (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer (2023).

</span>
<span class="ltx_bibblock">QLoRA: Efficient Finetuning of Quantized LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2305.14314 [cs.LG]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al.,  (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019).

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinu et al.,  (2019)</span>
<span class="ltx_bibblock">
Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan (2019).

</span>
<span class="ltx_bibblock">Training Neural Machine Translation to Apply Terminology Constraints.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 3063–3068, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al.,  (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui (2022).

</span>
<span class="ltx_bibblock">A Survey on In-context Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2301.00234 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al.,  (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier (2018).

</span>
<span class="ltx_bibblock">Understanding Back-Translation at Scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 489–500, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">EMA,  (2012)</span>
<span class="ltx_bibblock">
EMA (2012).

</span>
<span class="ltx_bibblock">European public assessment reports.

</span>
<span class="ltx_bibblock">The European Medicines Agency (EMA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Etchegoyhen et al.,  (2021)</span>
<span class="ltx_bibblock">
Thierry Etchegoyhen, David Ponce, Harritxu Gete, and Victor Ruiz (2021).

</span>
<span class="ltx_bibblock">Online Learning over Time in Adaptive Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</span>, pages 411–420, Held Online. INCOMA Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Exel et al.,  (2020)</span>
<span class="ltx_bibblock">
Miriam Exel, Bianka Buschbeck, Lauritz Brandt, and Simona Doneva (2020).

</span>
<span class="ltx_bibblock">Terminology-Constrained Neural Machine Translation at SAP.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</span>, pages 271–280, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fadaee and Monz,  (2018)</span>
<span class="ltx_bibblock">
Marzieh Fadaee and Christof Monz (2018).

</span>
<span class="ltx_bibblock">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 436–446, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al.,  (2018)</span>
<span class="ltx_bibblock">
Angela Fan, Mike Lewis, and Yann Dauphin (2018).

</span>
<span class="ltx_bibblock">Hierarchical Neural Story Generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 889–898, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farajian et al.,  (2017)</span>
<span class="ltx_bibblock">
M Amin Farajian, Marco Turchi, Matteo Negri, and Marcello Federico (2017).

</span>
<span class="ltx_bibblock">Multi-Domain Neural Machine Translation through Unsupervised Adaptation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the Second Conference on Machine Translation</span>, pages 127–137, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FitzGerald et al.,  (2022)</span>
<span class="ltx_bibblock">
Jack FitzGerald, Shankar Ananthakrishnan, Konstantine Arkoudas, Davide Bernardi, Abhishek Bhagia, Claudio Delli Bovi, Jin Cao, Rakesh Chada, Amit Chauhan, Luoxin Chen, Anurag Dwarakanath, Satyam Dwivedi, Turan Gojayev, Karthik Gopalakrishnan, Thomas Gueudre, Dilek Hakkani-Tur, Wael Hamza, Jonathan J Hüser, Kevin Martin Jose, Haidar Khan, Beiye Liu, Jianhua Lu, Alessandro Manzotti, Pradeep Natarajan, Karolina Owczarzak, Gokmen Oz, Enrico Palumbo, Charith Peris, Chandana Satya Prakash, Stephen Rawls, Andy Rosenbaum, Anjali Shenoy, Saleh Soltan, Mukund Harakere Sridhar, Lizhen Tan, Fabian Triefenbach, Pan Wei, Haiyang Yu, Shuai Zheng, Gokhan Tur, and Prem Natarajan (2022).

</span>
<span class="ltx_bibblock">Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</span>, KDD ’22, pages 2893–2902, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gala et al.,  (2023)</span>
<span class="ltx_bibblock">
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun Gumma, Sumanth Doddapaneni, Kumar M Aswanth, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M Khapra, Raj Dabre, and Anoop Kunchukuttan (2023).

</span>
<span class="ltx_bibblock">IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages.

</span>
<span class="ltx_bibblock">Transactions on Machine Learning Research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Transactions on Machine Learning Research</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al.,  (2023)</span>
<span class="ltx_bibblock">
Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat (2023).

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of few-shot learning for machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2302.01398 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garg et al.,  (2019)</span>
<span class="ltx_bibblock">
Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik (2019).

</span>
<span class="ltx_bibblock">Jointly Learning to Align and Translate with Transformer Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:1909.02074 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazvininejad et al.,  (2023)</span>
<span class="ltx_bibblock">
Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer (2023).

</span>
<span class="ltx_bibblock">Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2302.07856 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al.,  (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’aurelio Ranzato, Francisco Guzmán, and Angela Fan (2022).

</span>
<span class="ltx_bibblock">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation.

</span>
<span class="ltx_bibblock">Trans. Assoc. Comput. Linguist.

</span>
<span class="ltx_bibblock">10:522–538, <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Trans. Assoc. Comput. Linguist.</span>, 10:522–538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Green et al.,  (2014)</span>
<span class="ltx_bibblock">
Spence Green, Sida I Wang, Jason Chuang, Jeffrey Heer, Sebastian Schuster, and Christopher D Manning (2014).

</span>
<span class="ltx_bibblock">Human Effort and Machine Learnability in Computer Aided Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 1225–1236, Doha, Qatar. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grootendorst,  (2020)</span>
<span class="ltx_bibblock">
Maarten Grootendorst (2020).

</span>
<span class="ltx_bibblock">KeyBERT: Minimal keyword extraction with bert.

</span>
<span class="ltx_bibblock">GitHub.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">doi:10.5281/zenodo.4461265</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al.,  (2021)</span>
<span class="ltx_bibblock">
Kamal Kumar Gupta, Sukanta Sen, Rejwanul Haque, Asif Ekbal, Pushpak Bhattacharyya, and Andy Way (2021).

</span>
<span class="ltx_bibblock">Augmenting training data with syntactic phrasal-segments in low-resource neural machine translation.

</span>
<span class="ltx_bibblock">Machine Translation.

</span>
<span class="ltx_bibblock">35(4):661–685, <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Machine Translation</span>, 35(4):661–685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al.,  (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang (2020).

</span>
<span class="ltx_bibblock">REALM: Retrieval-Augmented Language Model Pre-Training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2002.08909 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddow et al.,  (2022)</span>
<span class="ltx_bibblock">
Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindřich Helcl, and Alexandra Birch (2022).

</span>
<span class="ltx_bibblock">Survey of Low-Resource Machine Translation.

</span>
<span class="ltx_bibblock">Computational Linguistics.

</span>
<span class="ltx_bibblock">06:1–67, <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Computational Linguistics</span>, 06:1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddow and Koehn,  (2012)</span>
<span class="ltx_bibblock">
Barry Haddow and Philipp Koehn (2012).

</span>
<span class="ltx_bibblock">Analysing the Effect of Out-of-Domain Data on SMT Systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proceedings of the Seventh Workshop on Statistical Machine Translation</span>, pages 422–432, Montréal, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque et al.,  (2021)</span>
<span class="ltx_bibblock">
Rejwanul Haque, Chao-Hong Liu, and Andy Way (2021).

</span>
<span class="ltx_bibblock">Recent Advances of Low-resource Neural Machine Translation.

</span>
<span class="ltx_bibblock">Machine Translation.

</span>
<span class="ltx_bibblock">35(4):451–474, <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Machine Translation</span>, 35(4):451–474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
Rejwanul Haque, Yasmin Moslem, and Andy Way (2020a).

</span>
<span class="ltx_bibblock">Terminology-Aware Sentence Mining for NMT Domain Adaptation: ADAPT’s Submission to the Adap-MT 2020 English-to-Hindi AI Translation Shared Task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task</span>, pages 17–23, Patna, India. NLP Association of India (NLPAI).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
Rejwanul Haque, Yasmin Moslem, and Andy Way (2020b).

</span>
<span class="ltx_bibblock">The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">Proceedings of the Fourth Workshop on Neural Generation and Translation</span>, pages 144–152, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasler et al.,  (2018)</span>
<span class="ltx_bibblock">
Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne (2018).

</span>
<span class="ltx_bibblock">Neural Machine Translation Decoding with Terminology Constraints.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</span>, pages 506–512, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasler et al.,  (2021)</span>
<span class="ltx_bibblock">
Eva Hasler, Tobias Domhan, Jonay Trenous, Ke Tran, Bill Byrne, and Felix Hieber (2021).

</span>
<span class="ltx_bibblock">Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 8470–8477, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen (2021a).

</span>
<span class="ltx_bibblock">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2111.09543 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen (2021b).

</span>
<span class="ltx_bibblock">DeBERTa: Decoding-enhanced BERT with Disentangled Attention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 9th International Conference on Learning Representations (ICLR)</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al.,  (2023)</span>
<span class="ltx_bibblock">
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang (2023).

</span>
<span class="ltx_bibblock">Exploring Human-Like Translation Strategy with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2305.04118 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et al.,  (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla (2023).

</span>
<span class="ltx_bibblock">How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2302.09210 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoang et al.,  (2023)</span>
<span class="ltx_bibblock">
Hieu Hoang, Huda Khayrallah, and Marcin Junczys-Dowmunt (2023).

</span>
<span class="ltx_bibblock">On-the-Fly Fusion of Large Language Models and Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2311.08306 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoang et al.,  (2018)</span>
<span class="ltx_bibblock">
Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn (2018).

</span>
<span class="ltx_bibblock">Iterative Back-Translation for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</span>, pages 18–24, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et al.,  (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W Rae, Oriol Vinyals, and Laurent Sifre (2022).

</span>
<span class="ltx_bibblock">Training Compute-Optimal Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2203.15556 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hokamp and Liu,  (2017)</span>
<span class="ltx_bibblock">
Chris Hokamp and Qun Liu (2017).

</span>
<span class="ltx_bibblock">Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 1535–1546, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al.,  (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi (2020).

</span>
<span class="ltx_bibblock">The Curious Case of Neural Text Degeneration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Proceedings of the 8th International Conference on Learning Representations (ICLR)</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al.,  (2018)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi (2018).

</span>
<span class="ltx_bibblock">Learning to Write with Cooperative Discriminators.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 1638–1649, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horawalavithana et al.,  (2022)</span>
<span class="ltx_bibblock">
Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova (2022).

</span>
<span class="ltx_bibblock">Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Proceedings of BigScience Episode #5 – Workshop on Challenges &amp; Perspectives in Creating Large Language Models</span>, pages 160–172, virtual+Dublin. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et al.,  (2020)</span>
<span class="ltx_bibblock">
Kasra Hosseini, Federico Nanni, and Mariona Coll Ardanuy (2020).

</span>
<span class="ltx_bibblock">DeezyMatch: A Flexible Deep Learning Approach to Fuzzy String Matching.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</span>, pages 62–69, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al.,  (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen (2021).

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2106.09685 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(78)</span>
<span class="ltx_bibblock">
Junjie Hu, Mengzhou Xia, Graham Neubig, and Jaime Carbonell (2019a).

</span>
<span class="ltx_bibblock">Domain Adaptation of Neural Machine Translation by Lexicon Induction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 2989–3001, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(79)</span>
<span class="ltx_bibblock">
J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme (2019b).

</span>
<span class="ltx_bibblock">Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 839–850, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huck et al.,  (2019)</span>
<span class="ltx_bibblock">
Matthias Huck, Viktor Hangya, and Alexander Fraser (2019).

</span>
<span class="ltx_bibblock">Better OOV Translation with Bilingual Terminology Mining.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 5809–5815, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imankulova et al.,  (2019)</span>
<span class="ltx_bibblock">
Aizhan Imankulova, Raj Dabre, Atsushi Fujita, and Kenji Imamura (2019).

</span>
<span class="ltx_bibblock">Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation.

</span>
<span class="ltx_bibblock">pages 128–139, Dublin, Ireland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et al.,  (2023)</span>
<span class="ltx_bibblock">
Vivek Iyer, Pinzhen Chen, and Alexandra Birch (2023).

</span>
<span class="ltx_bibblock">Towards Effective Disambiguation for Machine Translation with Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Proceedings of the Eighth Conference on Machine Translation</span>, pages 482–495, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al.,  (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave (2022).

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2208.03299 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al.,  (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed (2023).

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2310.06825 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(85)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Jen-Tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi, and Zhaopeng Tu (2023a).

</span>
<span class="ltx_bibblock">ParroT: Translating During Chat Using Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2304.02426 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(86)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, and Zhaopeng Tu (2023b).

</span>
<span class="ltx_bibblock">Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2301.08745 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al.,  (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou (2019).

</span>
<span class="ltx_bibblock">Billion-Scale Similarity Search with GPUs.

</span>
<span class="ltx_bibblock">IEEE Transactions on Big Data.

</span>
<span class="ltx_bibblock">7(3):535–547, <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">IEEE Transactions on Big Data</span>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt,  (2018)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt (2018).

</span>
<span class="ltx_bibblock">Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</span>, pages 888–895, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadaoui et al.,  (2023)</span>
<span class="ltx_bibblock">
Karima Kadaoui, Samar M Magdy, Abdul Waheed, Md Tawkat Islam Khondaker, Ahmed Oumar El-Shangiti, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed (2023).

</span>
<span class="ltx_bibblock">TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">The First Arabic Natural Language Processing Conference (ArabicNLP 2023)</span>, Sentosa, Singapore.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al.,  (2021)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis (2021).

</span>
<span class="ltx_bibblock">Nearest Neighbor Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">Proceedings of the 9th International Conference on Learning Representations (ICLR)</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al.,  (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell (2017).

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock">Proc. Natl. Acad. Sci. U. S. A.

</span>
<span class="ltx_bibblock">114(13):3521–3526, <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Proc. Natl. Acad. Sci. U. S. A.</span>, 114(13):3521–3526.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(92)</span>
<span class="ltx_bibblock">
Guillaume Klein, François Hernandez, Vincent Nguyen, and Jean Senellart (2020a).

</span>
<span class="ltx_bibblock">The OpenNMT Neural Machine Translation Toolkit: 2020 Edition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</span>, pages 102–109, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(93)</span>
<span class="ltx_bibblock">
Guillaume Klein, Dakun Zhang, Clément Chouteau, Josep Crego, and Jean Senellart (2020b).

</span>
<span class="ltx_bibblock">Efficient and high-quality neural machine translation with OpenNMT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">Proceedings of the Fourth Workshop on Neural Generation and Translation</span>, pages 211–217, Stroudsburg, PA, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knowles et al.,  (2018)</span>
<span class="ltx_bibblock">
Rebecca Knowles, John Ortega, and Philipp Koehn (2018).

</span>
<span class="ltx_bibblock">A Comparison of Machine Translation Paradigms for Use in Black-Box Fuzzy-Match Repair.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</span>, pages 249–255, Boston, MA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobus et al.,  (2017)</span>
<span class="ltx_bibblock">
Catherine Kobus, Josep Crego, and Jean Senellart (2017).

</span>
<span class="ltx_bibblock">Domain Control for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">Proceedings of Recent Advances in Natural Language Processing</span>, pages 372–378, Varna, Bulgaria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn,  (2009)</span>
<span class="ltx_bibblock">
Philipp Koehn (2009).

</span>
<span class="ltx_bibblock">A process study of computer-aided translation.

</span>
<span class="ltx_bibblock">Mach. Transl.

</span>
<span class="ltx_bibblock">23(4):241–263, <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">Mach. Transl.</span>, 23(4):241–263.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Knowles,  (2017)</span>
<span class="ltx_bibblock">
Philipp Koehn and Rebecca Knowles (2017).

</span>
<span class="ltx_bibblock">Six Challenges for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Proceedings of the First Workshop on Neural Machine Translation</span>, pages 28–39, Vancouver. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koneru et al.,  (2023)</span>
<span class="ltx_bibblock">
Sai Koneru, Miriam Exel, Matthias Huck, and Jan Niehues (2023).

</span>
<span class="ltx_bibblock">Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2310.14855 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo,  (2018)</span>
<span class="ltx_bibblock">
Taku Kudo (2018).

</span>
<span class="ltx_bibblock">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 66–75, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson,  (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson (2018).

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</span>, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudugunta et al.,  (2023)</span>
<span class="ltx_bibblock">
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat (2023).

</span>
<span class="ltx_bibblock">MADLAD-400: A Multilingual And Document-Level Large Audited Dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2309.04662 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al.,  (2023)</span>
<span class="ltx_bibblock">
Aswanth Kumar, Ratish Puduppully, Raj Dabre, and Anoop Kunchukuttan (2023).

</span>
<span class="ltx_bibblock">CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 7736–7752, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample and Conneau,  (2019)</span>
<span class="ltx_bibblock">
Guillaume Lample and Alexis Conneau (2019).

</span>
<span class="ltx_bibblock">Cross-lingual Language Model Pretraining.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Advances in Neural Information Processing Systems (NeurIPS 2019)</span>, volume 32, Vancouver, Canada. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Langlais et al.,  (2000)</span>
<span class="ltx_bibblock">
Philippe Langlais, George Foster, and Guy Lapalme (2000).

</span>
<span class="ltx_bibblock">TransType: a Computer-Aided Translation Typing System.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">ANLP-NAACL 2000 Workshop: Embedded Machine Translation Systems</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Scao et al.,  (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu
Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J Mielke, Wilson Y Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar
Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar
Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio
Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf (2022).

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2211.05100 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al.,  (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela (2020).

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2005.11401 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(107)</span>
<span class="ltx_bibblock">
Chunyou Li, Mingtong Liu, Hongxiao Zhang, Yufeng Chen, Jinan Xu, and Ming Zhou (2023a).

</span>
<span class="ltx_bibblock">MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 8616–8627, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.,  (2021)</span>
<span class="ltx_bibblock">
Huayang Li, Lemao Liu, Guoping Huang, and Shuming Shi (2021).

</span>
<span class="ltx_bibblock">GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 4792–4802, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(109)</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen (2023b).

</span>
<span class="ltx_bibblock">Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2305.15083 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.,  (2022)</span>
<span class="ltx_bibblock">
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer (2022).

</span>
<span class="ltx_bibblock">Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2208.03306 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.,  (2018)</span>
<span class="ltx_bibblock">
Xiaoqing Li, Jiajun Zhang, and Chengqing Zong (2018).

</span>
<span class="ltx_bibblock">One Sentence One Model for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</span>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(112)</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee (2023c).

</span>
<span class="ltx_bibblock">Textbooks Are All You Need II: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">arXiv preprint arXiv:2309.05463 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al.,  (2020)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren (2020).

</span>
<span class="ltx_bibblock">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</span>, pages 1823–1840, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al.,  (2022)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li (2022).

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Generative Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.,  (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer (2020).

</span>
<span class="ltx_bibblock">Multilingual Denoising Pre-training for Neural Machine Translation.

</span>
<span class="ltx_bibblock">Transactions of the Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">8:726–742, <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">Transactions of the Association for Computational Linguistics</span>, 8:726–742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.,  (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov (2019).

</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:1907.11692 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.,  (2023)</span>
<span class="ltx_bibblock">
Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou (2023).

</span>
<span class="ltx_bibblock">Instruction Position Matters in Sequence Generation with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">arXiv preprint arXiv:2308.12097 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong and Manning,  (2015)</span>
<span class="ltx_bibblock">
Minh-Thang Luong and Christopher Manning (2015).

</span>
<span class="ltx_bibblock">Stanford neural machine translation systems for spoken language domains.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign</span>, pages 76–79, Da Nang, Vietnam.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martins et al.,  (2023)</span>
<span class="ltx_bibblock">
Pedro Henrique Martins, João Alves, Tânia Vaz, Madalena Gonçalves, Beatriz Silva, Marianna Buchicchio, José G C de Souza, and André F T Martins (2023).

</span>
<span class="ltx_bibblock">Empirical Assessment of kNN-MT for Real-World Translation Scenarios.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 115–124, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al.,  (2022)</span>
<span class="ltx_bibblock">
Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, and Jiwei Li (2022).

</span>
<span class="ltx_bibblock">Fast Nearest Neighbor Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">Findings of the Association for Computational Linguistics: ACL 2022</span>, pages 555–565, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michon et al.,  (2020)</span>
<span class="ltx_bibblock">
Elise Michon, Josep Crego, and Jean Senellart (2020).

</span>
<span class="ltx_bibblock">Integrating Domain Terminology into Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">Proceedings of the 28th International Conference on Computational Linguistics</span>, pages 3925–3937, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mondragón,  (2021)</span>
<span class="ltx_bibblock">
Luis Mondragón (2021).

</span>
<span class="ltx_bibblock">TM2TB: Bilingual term extraction and matching with spaCy and sentence transformers.

</span>
<span class="ltx_bibblock">GitHub.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">doi:””</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moore and Lewis,  (2010)</span>
<span class="ltx_bibblock">
Robert C Moore and William Lewis (2010).

</span>
<span class="ltx_bibblock">Intelligent Selection of Language Model Training Data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">Proceedings of the ACL 2010 Conference Short Papers</span>, pages 220–224, Uppsala, Sweden. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(124)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John Kelleher, and Andy Way (2022a).

</span>
<span class="ltx_bibblock">Domain-Specific Text Generation for Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</span>, pages 14–30, Orlando, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(125)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John D Kelleher, and Andy Way (2023a).

</span>
<span class="ltx_bibblock">Adaptive Machine Translation with Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 227–237, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem et al.,  (2020)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, and Andy Way (2020).

</span>
<span class="ltx_bibblock">Arabisc: Context-Sensitive Neural Spelling Checker.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</span>, pages 11–19, Suzhou, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(127)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, and Andy Way (2022b).

</span>
<span class="ltx_bibblock">Translation Word-Level Auto-Completion: What Can We Achieve Out of the Box?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages 1176–1181, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(128)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, and Andy Way (2023b).

</span>
<span class="ltx_bibblock">Fine-tuning Large Language Models for Adaptive Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">arXiv preprint arXiv:2312.12740 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(129)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, John D Kelleher, Rejwanul Haque, and Andy Way (2023c).

</span>
<span class="ltx_bibblock">Domain Terminology Integration into Machine Translation: Leveraging Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">Proceedings of the Eighth Conference on Machine Translation</span>, pages 902–911, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al.,  (2023)</span>
<span class="ltx_bibblock">
Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, and Jingbo Zhu (2023).

</span>
<span class="ltx_bibblock">Augmenting Large Language Model Translators via Translation Memories.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">Findings of the Association for Computational Linguistics: ACL 2023</span>, pages 10287–10299, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al.,  (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel (2022).

</span>
<span class="ltx_bibblock">Crosslingual Generalization through Multitask Finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2211.01786 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller and Laurent,  (2022)</span>
<span class="ltx_bibblock">
Martin Müller and Florian Laurent (2022).

</span>
<span class="ltx_bibblock">Cedille: A large autoregressive French language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">arXiv preprint arXiv:2202.03371 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nayak et al.,  (2023)</span>
<span class="ltx_bibblock">
Prashanth Nayak, Rejwanul Haque, John D Kelleher, and Andy Way (2023).

</span>
<span class="ltx_bibblock">Instance-Based Domain Adaptation for Improving Terminology Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">Proceedings of Machine Translation Summit XIX: Research Track</span>, pages 222–231, Macau SAR, China. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neves et al.,  (2022)</span>
<span class="ltx_bibblock">
Mariana Neves, Antonio Jimeno Yepes, Amy Siu, Roland Roller, Philippe Thomas, Maika Vicente Navarro, Lana Yeganova, Dina Wiemann, Giorgio Maria Di Nunzio, Federica Vezzani, Christel Gerardin, Rachel Bawden, Darryl Johan Estrada, Salvador Lima-lopez, Eulalia Farre-maduel, Martin Krallinger, Cristian Grozea, and Aurelie Neveol (2022).

</span>
<span class="ltx_bibblock">Findings of the WMT 2022 Biomedical Translation Shared Task: Monolingual Clinical Case Reports.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages 694–723, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Brien,  (2022)</span>
<span class="ltx_bibblock">
Sharon O’Brien (2022).

</span>
<span class="ltx_bibblock">How to deal with errors in machine translation: Post-editing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">Machine translation for everyone</span>, pages 105–120. Language Science Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ojo and Ogueji,  (2023)</span>
<span class="ltx_bibblock">
Jessica Ojo and Kelechi Ogueji (2023).

</span>
<span class="ltx_bibblock">How Good are Commercial Large Language Models on African Languages?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">Proceedings of the 4th Workshop on African Natural Language Processing, AfricaNLP@ICLR 2023</span>, Kigali, Rwanda.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Öktem et al.,  (2022)</span>
<span class="ltx_bibblock">
Alp Öktem, Rodolfo Zevallos, Yasmin Moslem, Özgür Güneş Öztürk, and Karen Gerson Şarhon (2022).

</span>
<span class="ltx_bibblock">Preparing an endangered language for the digital age: The Case of Judeo-Spanish.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">Proceedings of the Workshop on Resources and Technologies for Indigenous, Endangered and Lesser-resourced Languages in Eurasia within the 13th Language Resources and Evaluation Conference</span>, pages 105–110, Marseille, France. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI,  (2023)</span>
<span class="ltx_bibblock">
OpenAI (2023).

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2303.08774 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al.,  (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe (2022).

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">Advances in Neural Information Processing Systems (NeurIPS 2022)</span>, pages 27730–27744, New Orleans, Louisiana, USA. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al.,  (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu (2002).

</span>
<span class="ltx_bibblock">Bleu: a Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al.,  (2023)</span>
<span class="ltx_bibblock">
Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch (2023).

</span>
<span class="ltx_bibblock">Bidirectional Language Models Are Also Few-shot Learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">Proceedings of the Eleventh International Conference on Learning Representations</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al.,  (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay (2023).

</span>
<span class="ltx_bibblock">The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2306.01116 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al.,  (2023)</span>
<span class="ltx_bibblock">
Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao (2023).

</span>
<span class="ltx_bibblock">Towards Making the Most of ChatGPT for Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 5622–5633, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peris and Casacuberta,  (2019)</span>
<span class="ltx_bibblock">
Álvaro Peris and Francisco Casacuberta (2019).

</span>
<span class="ltx_bibblock">Online learning for effort reduction in interactive neural machine translation.

</span>
<span class="ltx_bibblock">Computer Speech &amp; Language.

</span>
<span class="ltx_bibblock">58:98–126, <span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">Computer Speech &amp; Language</span>, 58:98–126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peris et al.,  (2017)</span>
<span class="ltx_bibblock">
Álvaro Peris, Miguel Domingo, and Francisco Casacuberta (2017).

</span>
<span class="ltx_bibblock">Interactive neural machine translation.

</span>
<span class="ltx_bibblock">Computer Speech &amp; Language.

</span>
<span class="ltx_bibblock">45:201–220, <span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">Computer Speech &amp; Language</span>, 45:201–220.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al.,  (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller (2019).

</span>
<span class="ltx_bibblock">Language Models as Knowledge Bases?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al.,  (2020)</span>
<span class="ltx_bibblock">
Minh Quang Pham, Jitao Xu, Josep Crego, François Yvon, and Jean Senellart (2020).

</span>
<span class="ltx_bibblock">Priming Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 516–527, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poncelas et al.,  (2019)</span>
<span class="ltx_bibblock">
Alberto Poncelas, Gideon Maillette de Buy Wenniger, and Andy Way (2019).

</span>
<span class="ltx_bibblock">Adaptation of Machine Translation Models with Back-Translated Data Using Transductive Data Selection Methods.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">Proceedings of the 20th International Conference on Computational Linguistics and Intelligent Text Processing CICLing 2019: Computational Linguistics and Intelligent Text Processing</span>, pages 567–579, La Rochelle, France. Springer Nature Switzerland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović,  (2017)</span>
<span class="ltx_bibblock">
Maja Popović (2017).

</span>
<span class="ltx_bibblock">chrF++: words helping character n-grams.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">Proceedings of the Second Conference on Machine Translation</span>, pages 612–618, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović et al.,  (2023)</span>
<span class="ltx_bibblock">
Maja Popović, Mohammad Arvan, Natalie Parde, and Anya Belz (2023).

</span>
<span class="ltx_bibblock">Exploring Variation of Results from Different Experimental Conditions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">Findings of the Association for Computational Linguistics: ACL 2023</span>, pages 2746–2757, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post,  (2018)</span>
<span class="ltx_bibblock">
Matt Post (2018).

</span>
<span class="ltx_bibblock">A Call for Clarity in Reporting BLEU Scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post and Vilar,  (2018)</span>
<span class="ltx_bibblock">
Matt Post and David Vilar (2018).

</span>
<span class="ltx_bibblock">Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 1314–1324, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puduppully et al.,  (2023)</span>
<span class="ltx_bibblock">
Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Ai Ti Aw, and Nancy Chen (2023).

</span>
<span class="ltx_bibblock">DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 4586–4602, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al.,  (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, D Luan, Dario Amodei, and Ilya Sutskever (2019).

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask Learners.

</span>
<span class="ltx_bibblock">Technical report, OpenAi.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et al.,  (2021)</span>
<span class="ltx_bibblock">
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving (2021).

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2112.11446 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al.,  (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu (2020).

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research.

</span>
<span class="ltx_bibblock">21(140):1–67, <span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">Journal of Machine Learning Research</span>, 21(140):1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reheman et al.,  (2023)</span>
<span class="ltx_bibblock">
Abudurexiti Reheman, Tao Zhou, Yingfeng Luo, Di Yang, Tong Xiao, and Jingbo Zhu (2023).

</span>
<span class="ltx_bibblock">Prompting Neural Machine Translation with Translation Memories.

</span>
<span class="ltx_bibblock">AAAI.

</span>
<span class="ltx_bibblock">37(11):13519–13527, <span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">AAAI</span>, 37(11):13519–13527.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al.,  (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie (2020).

</span>
<span class="ltx_bibblock">COMET: A Neural Framework for MT Evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych,  (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych (2019).

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages 3982–3992, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reinauer et al.,  (2023)</span>
<span class="ltx_bibblock">
Raphael Reinauer, Patrick Simianer, Kaden Uhlig, Johannes E M Mosig, and Joern Wuebker (2023).

</span>
<span class="ltx_bibblock">Neural Machine Translation Models Can Learn to be Few-shot Learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">arXiv preprint arXiv:2309.08590 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson and Rashid,  (2007)</span>
<span class="ltx_bibblock">
Stephen D Richardson and Richard F Rashid (2007).

</span>
<span class="ltx_bibblock">Adaptive Machine Translation.

</span>
<span class="ltx_bibblock">US Patent No. US7295963B2 (Microsoft). United States Patent and Trademark Office.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al.,  (2023)</span>
<span class="ltx_bibblock">
Nathaniel Robinson, Perez Ogayo, David R Mortensen, and Graham Neubig (2023).

</span>
<span class="ltx_bibblock">ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">Proceedings of the Eighth Conference on Machine Translation</span>, pages 392–418, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarti et al.,  (2023)</span>
<span class="ltx_bibblock">
Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde (2023).

</span>
<span class="ltx_bibblock">RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 1476–1490, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders,  (2022)</span>
<span class="ltx_bibblock">
Danielle Saunders (2022).

</span>
<span class="ltx_bibblock">Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey.

</span>
<span class="ltx_bibblock">JAIR.

</span>
<span class="ltx_bibblock">75:351–424, <span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">JAIR</span>, 75:351–424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sawai et al.,  (2021)</span>
<span class="ltx_bibblock">
Ranto Sawai, Incheon Paik, and Ayato Kuwana (2021).

</span>
<span class="ltx_bibblock">Sentence Augmentation for Language Translation Using GPT-2.

</span>
<span class="ltx_bibblock">Electronics.

</span>
<span class="ltx_bibblock">10(24):3082, <span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">Electronics</span>, 10(24):3082.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al.,  (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom (2023).

</span>
<span class="ltx_bibblock">Toolformer: Language Models Can Teach Themselves to Use Tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">arXiv preprint arXiv:2302.04761 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schioppa et al.,  (2023)</span>
<span class="ltx_bibblock">
Andrea Schioppa, Xavier Garcia, and Orhan Firat (2023).

</span>
<span class="ltx_bibblock">Cross-Lingual Supervision improves Large Language Models Pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">arXiv preprint arXiv:2305.11778 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Semenov et al.,  (2023)</span>
<span class="ltx_bibblock">
Kirill Semenov, Vilém Zouhar, Tom Kocmi, Dongdong Zhang, Wangchunshu Zhou, A Yuchen, Eleanor Jiang, Charles University, Eth Zürich, M Microsoft, A Aiwaves, and Chinese English (2023).

</span>
<span class="ltx_bibblock">Findings of the WMT 2023 shared task on machine translation with terminologies.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, Sentosa, Singapore.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sengupta et al.,  (2023)</span>
<span class="ltx_bibblock">
Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, and Eric Xing (2023).

</span>
<span class="ltx_bibblock">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">arXiv preprint arXiv:2308.16149 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al.,  (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch (2016).

</span>
<span class="ltx_bibblock">Improving Neural Machine Translation Models with Monolingual Data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 86–96, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al.,  (2022)</span>
<span class="ltx_bibblock">
Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer (2022).

</span>
<span class="ltx_bibblock">kNN-Prompt: Nearest Neighbor Zero-Shot Inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2205.13792 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et al.,  (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina (2022).

</span>
<span class="ltx_bibblock">mGPT: Few-Shot Learners Go Multilingual.

</span>
<span class="ltx_bibblock">Transactions of the Association for Computational Linguistics (TACL).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">Transactions of the Association for Computational Linguistics (TACL)</span>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.2.1">arXiv preprint arXiv:2204.07580 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sia and Duh,  (2022)</span>
<span class="ltx_bibblock">
Suzanna Sia and Kevin Duh (2022).

</span>
<span class="ltx_bibblock">Prefix Embeddings for In-context Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</span>, pages 45–57, Orlando, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sia and Duh,  (2023)</span>
<span class="ltx_bibblock">
Suzanna Sia and Kevin Duh (2023).

</span>
<span class="ltx_bibblock">In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track</span>, pages 173–185, Macau SAR, China. Asia-Pacific Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al.,  (2022)</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro (2022).

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2201.11990 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al.,  (2006)</span>
<span class="ltx_bibblock">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul (2006).

</span>
<span class="ltx_bibblock">A Study of Translation Edit Rate with Targeted Human Annotation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</span>, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soares et al.,  (2018)</span>
<span class="ltx_bibblock">
Felipe Soares, Viviane Moreira, and Karin Becker (2018).

</span>
<span class="ltx_bibblock">A Large Parallel Corpus of Full-Text Scientific Articles.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</span>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al.,  (2020)</span>
<span class="ltx_bibblock">
Lucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaudhary, Francisco Guzmán, Graham Neubig, Nadir Durrani, Yonatan Belinkov, Philipp Koehn, Hassan Sajjad, Paul Michel, and Xian Li (2020).

</span>
<span class="ltx_bibblock">Findings of the WMT 2020 Shared Task on Machine Translation Robustness.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 76–91, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stergiadis et al.,  (2021)</span>
<span class="ltx_bibblock">
Emmanouil Stergiadis, Satendra Kumar, Fedor Kovalev, and Pavel Levin (2021).

</span>
<span class="ltx_bibblock">Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">Proceedings of Machine Translation Summit XVIII: Users and Providers Track</span>, pages 396–420, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al.,  (2022)</span>
<span class="ltx_bibblock">
Zewei Sun, Qingnan Jiang, Shujian Huang, Jun Cao, Shanbo Cheng, and Mingxuan Wang (2022).

</span>
<span class="ltx_bibblock">Zero-shot Domain Adaptation for Neural Machine Translation with Retrieved Phrase-level Prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">arXiv preprint arXiv:2209.11409 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sung et al.,  (2021)</span>
<span class="ltx_bibblock">
Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, and Jaewoo Kang (2021).

</span>
<span class="ltx_bibblock">Can Language Models be Biomedical Knowledge Bases?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 4723–4734, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al.,  (2023)</span>
<span class="ltx_bibblock">
Weiting Tan, Haoran Xu, Lingfeng Shen, Shuyue Stella Li, Kenton Murray, Philipp Koehn, Benjamin Van Durme, and Yunmo Chen (2023).

</span>
<span class="ltx_bibblock">Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">arXiv preprint arXiv:2311.02310 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann,  (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann (2012).

</span>
<span class="ltx_bibblock">Parallel Data, Tools and Interfaces in OPUS.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</span>, pages 2214–2218, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann,  (2020)</span>
<span class="ltx_bibblock">
Jörg Tiedemann (2020).

</span>
<span class="ltx_bibblock">The Tatoeba Translation Challenge – Realistic Data Sets for Low Resource and Multilingual MT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 1174–1182, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal,  (2020)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Santhosh Thottingal (2020).

</span>
<span class="ltx_bibblock">OPUS-MT — Building open translation services for the World.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</span>, Lisbon, Portugal.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(186)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample (2023a).

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">arXiv preprint arXiv:2302.13971 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(187)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom (2023b).

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">arXiv preprint arXiv:2307.09288 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al.,  (2021)</span>
<span class="ltx_bibblock">
Chau Tran, Shruti Bhosale, James Cross, Philipp Koehn, Sergey Edunov, and Angela Fan (2021).

</span>
<span class="ltx_bibblock">Facebook AI’s WMT21 News Translation Task Submission.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">Proceedings of the Sixth Conference on Machine Translation</span>, pages 205–215, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trask et al.,  (2015)</span>
<span class="ltx_bibblock">
Andrew Trask, Phil Michalak, and John Liu (2015).

</span>
<span class="ltx_bibblock">sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">arXiv preprint arXiv:1511.06388 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Treviso et al.,  (2023)</span>
<span class="ltx_bibblock">
Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H Martins, André F T Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz (2023).

</span>
<span class="ltx_bibblock">Efficient methods for natural language processing: A survey.

</span>
<span class="ltx_bibblock">Transactions of the Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">11:826–860, <span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">Transactions of the Association for Computational Linguistics</span>, 11:826–860.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al.,  (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin (2017).

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">Advances in Neural Information Processing Systems (NIPS 2017)</span>, volume 30. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al.,  (2023)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster (2023).

</span>
<span class="ltx_bibblock">Prompting PaLM for Translation: Assessing Strategies and Performance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 15406–15427, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al.,  (2023)</span>
<span class="ltx_bibblock">
Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang (2023).

</span>
<span class="ltx_bibblock">Efficient Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">arXiv preprint arXiv:2312.03863 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Komatsuzaki,  (2021)</span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki (2021).

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock">Github (mesh-transformer-jax).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.,  (2022)</span>
<span class="ltx_bibblock">
Dexin Wang, Kai Fan, Boxing Chen, and Deyi Xiong (2022).

</span>
<span class="ltx_bibblock">Efficient Cluster-Based <math alttext="k" class="ltx_Math" display="inline" id="bib.bib195.1.m1.1"><semantics id="bib.bib195.1.m1.1a"><mi id="bib.bib195.1.m1.1.1" xref="bib.bib195.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="bib.bib195.1.m1.1b"><ci id="bib.bib195.1.m1.1.1.cmml" xref="bib.bib195.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib195.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="bib.bib195.1.m1.1d">italic_k</annotation></semantics></math>-Nearest-Neighbor Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib195.2.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 2175–2187, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(196)</span>
<span class="ltx_bibblock">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu (2023a).

</span>
<span class="ltx_bibblock">Document-Level Machine Translation with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">arXiv preprint arXiv:2304.02210 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.,  (2021)</span>
<span class="ltx_bibblock">
Shuo Wang, Zhaopeng Tu, Zhixing Tan, Wenxuan Wang, Maosong Sun, and Yang Liu (2021).

</span>
<span class="ltx_bibblock">Language Models are Good Translators.

</span>
<span class="ltx_bibblock">ArXiv.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib197.1.1">ArXiv</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(198)</span>
<span class="ltx_bibblock">
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei (2023b).

</span>
<span class="ltx_bibblock">Augmenting Language Models with Long-Term Memory.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib198.1.1">Advances in Neural Information Processing Systems (NeurIPS 2023)</span>, New Orleans, Louisiana, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.,  (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou (2020).

</span>
<span class="ltx_bibblock">MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib199.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</span>, number Article 485 in NIPS’20, pages 5776–5788, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et al.,  (2021)</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Vishrav Chaudhary, Angela Fan, Sahir Gomez, Naman Goyal, Somya Jain, Douwe Kiela, Tristan Thrush, and Francisco Guzmán (2021).

</span>
<span class="ltx_bibblock">Findings of the WMT 2021 Shared Task on Large-Scale Multilingual Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib200.1.1">Proceedings of the Sixth Conference on Machine Translation</span>, pages 89–99, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams and Zipser,  (1989)</span>
<span class="ltx_bibblock">
Ronald J Williams and David Zipser (1989).

</span>
<span class="ltx_bibblock">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks.

</span>
<span class="ltx_bibblock">Neural Comput.

</span>
<span class="ltx_bibblock">1(2):270–280, <span class="ltx_text ltx_font_italic" id="bib.bib201.1.1">Neural Comput.</span>, 1(2):270–280.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al.,  (2022)</span>
<span class="ltx_bibblock">
Yuhuai Wu, Markus Norman Rabe, Delesley Hutchins, and Christian Szegedy (2022).

</span>
<span class="ltx_bibblock">Memorizing Transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib202.1.1">Proceedings of the Tenth International Conference on Learning Representations</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wuebker et al.,  (2018)</span>
<span class="ltx_bibblock">
Joern Wuebker, Patrick Simianer, and John DeNero (2018).

</span>
<span class="ltx_bibblock">Compact Personalized Models for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib203.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 881–886, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al.,  (2023)</span>
<span class="ltx_bibblock">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla (2023).

</span>
<span class="ltx_bibblock">A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">arXiv preprint arXiv:2309.11674 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al.,  (2020)</span>
<span class="ltx_bibblock">
Jitao Xu, Josep Crego, and Jean Senellart (2020).

</span>
<span class="ltx_bibblock">Boosting Neural Machine Translation with Similar Translations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib205.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 1580–1590, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(206)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, Mingan Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu (2023a).

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">arXiv preprint arXiv:2309.10305 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(207)</span>
<span class="ltx_bibblock">
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong (2023b).

</span>
<span class="ltx_bibblock">BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">arXiv preprint arXiv:2305.18098 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al.,  (2020)</span>
<span class="ltx_bibblock">
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil (2020).

</span>
<span class="ltx_bibblock">Multilingual Universal Sentence Encoder for Semantic Retrieval.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib208.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</span>, pages 87–94, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al.,  (2019)</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le (2019).

</span>
<span class="ltx_bibblock">XLNet: Generalized Autoregressive Pretraining for Language Understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib209.1.1">Advances in Neural Information Processing Systems (NeurIPS 2019)</span>, volume 32, Vancouver, Canada. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al.,  (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang (2022).

</span>
<span class="ltx_bibblock">GLM-130B: An Open Bilingual Pre-trained Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">arXiv preprint arXiv:2210.02414 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(211)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch (2023a).

</span>
<span class="ltx_bibblock">Prompting large language model for machine translation: A case study.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib211.1.1">Proceedings of the 40 th International Conference on Machine Learning</span>, Honolulu, Hawaii, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(212)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu (2018a).

</span>
<span class="ltx_bibblock">Improving the Transformer Translation Model with Document-Level Context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib212.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 533–542, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(213)</span>
<span class="ltx_bibblock">
Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, and Satoshi Nakamura (2018b).

</span>
<span class="ltx_bibblock">Guiding Neural Machine Translation with Retrieved Translation Pieces.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib213.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 1325–1335, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer (2022).

</span>
<span class="ltx_bibblock">OPT: Open Pre-trained Transformer Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">arXiv preprint arXiv:2205.01068 [cs.CL]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(215)</span>
<span class="ltx_bibblock">
Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn (2023b).

</span>
<span class="ltx_bibblock">Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib215.1.1">Proceedings of the Eighth Conference on Machine Translation</span>, pages 468–481, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2020)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan (2020).

</span>
<span class="ltx_bibblock">POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib216.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 8649–8670, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2021)</span>
<span class="ltx_bibblock">
Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, and Maosong Sun (2021).

</span>
<span class="ltx_bibblock">CPM: A large-scale generative Chinese Pre-trained language model.

</span>
<span class="ltx_bibblock">AI Open.

</span>
<span class="ltx_bibblock">2:93–99, <span class="ltx_text ltx_font_italic" id="bib.bib217.1.1">AI Open</span>, 2:93–99.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al.,  (2021)</span>
<span class="ltx_bibblock">
Xin Zheng, Zhirui Zhang, Shujian Huang, Boxing Chen, Jun Xie, Weihua Luo, and Jiajun Chen (2021).

</span>
<span class="ltx_bibblock">Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib218.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</span>, pages 4234–4241, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al.,  (2020)</span>
<span class="ltx_bibblock">
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu (2020).

</span>
<span class="ltx_bibblock">Incorporating BERT into Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib219.1.1">Proceedings of the 8th International Conference on Learning Representations, ICLR 2020</span>, Virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al.,  (2023)</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li (2023).

</span>
<span class="ltx_bibblock">Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">arXiv preprint arXiv:2304.04675 [cs.CL]</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jan 25 22:55:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
