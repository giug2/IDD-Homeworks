<article class="ltx_document ltx_authors_1line ltx_leqno">
 <h1 class="ltx_title ltx_title_document">
  Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhenyu Wang
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">
      Tsinghua University
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id3.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id4.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:wangzy20@mails.tsinghua.edu.cn">
      wangzy20@mails.tsinghua.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Enze Xie
    <sup class="ltx_sup" id="id5.2.id1">
     ∗
    </sup>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id6.3.id1">
      Noah’s Ark Lab, Huawei
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id7.4.id2">
      Hong Kong
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id8.5.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:xieenze@connect.hku.hk">
      xieenze@connect.hku.hk
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Aoxue Li
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">
      Noah’s Ark Lab, Huawei
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id10.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id11.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:lax@pku.edu.cn">
      lax@pku.edu.cn
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhongdao Wang
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">
      Noah’s Ark Lab, Huawei
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id13.2.id2">
      Beijing
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id14.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:wangzhongdao@huawei.com">
      wangzhongdao@huawei.com
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   ,
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Xihui Liu
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">
      The University of Hong Kong
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id16.2.id2">
      Hong Kong
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id17.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:xihuiliu@eee.hku.hk">
      xihuiliu@eee.hku.hk
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
   and
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhenguo Li
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
     <span class="ltx_text ltx_affiliation_institution" id="id18.1.id1">
      Noah’s Ark Lab, Huawei
     </span>
     <span class="ltx_text ltx_affiliation_city" id="id19.2.id2">
      Hong Kong
     </span>
     <span class="ltx_text ltx_affiliation_country" id="id20.3.id3">
      China
     </span>
    </span>
    <span class="ltx_contact ltx_role_email">
     <a href="mailto:Li.Zhenguo@huawei.com">
      Li.Zhenguo@huawei.com
     </a>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract.
  </h6>
  <p class="ltx_p" id="id21.id1">
   Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose
   <span class="ltx_text ltx_font_italic" id="id21.id1.1">
    CompAgent
   </span>
   , a
   <span class="ltx_text ltx_font_italic" id="id21.id1.2">
    training-free
   </span>
   approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects.
Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.
  </p>
 </div>
 <div class="ltx_keywords">
  Image Generation, Compositional Text-to-Image Generation, Diffusion Models, LLM Agent
 </div>
 <div class="ltx_acknowledgements">
  Project page:
  <span class="ltx_text" id="id22.id1" style="color:#808080;">
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zhenyuw16.github.io/CompAgent/" target="_blank" title="">
    https://zhenyuw16.github.io/CompAgent/
   </a>
  </span>
 </div>
 <span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     copyright:
    </span>
    none
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Computing methodologies Computer vision
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Computing methodologies Image manipulation
   </span>
  </span>
 </span>
 <figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
  <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="342" id="S0.F1.g1" src="/html/2401.15688/assets/x1.png" width="420"/>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_figure">
    Figure 1.
   </span>
   For compositional text-to-image generation, existing state-of-the-art text-to-image models often fail to generate images that accurately correspond to the description of text inputs, especially about object attributes and relationships. In comparison, our method manages to generate images with correct objects and attributes according to the complex text prompts.
  </figcaption>
 </figure>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1.
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent advancements in text-to-image generation
    <cite class="ltx_cite ltx_citemacro_citep">
     (Rombach et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2022
     </a>
     ; Saharia et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022
     </a>
     ; Ramesh et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2022
     </a>
     ; Chang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     ; Chen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023b
     </a>
     )
    </cite>
    have demonstrated the remarkable capability in creating diverse and high-quality images based on language text inputs. However, even state-of-the-art text-to-image models often fail to generate images that accurately align with complex text prompts, where multiple objects with different attributes or relationships are composed into one scene
    <cite class="ltx_cite ltx_citemacro_citep">
     (Feng et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ; Huang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023
     </a>
     ; Chefer et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     )
    </cite>
    . As can be seen in Fig.
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , existing methods cannot create correct objects, attributes, or relationships within the generated images given these complex text prompts.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Addressing compositional text-to-image generation requires solving at least the following three issues: 1)
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">
     Object types and quantities.
    </span>
    Due to the presence of multiple objects, the generated images should accurately incorporate each object, avoiding issues such as incorrect object types, omissions of objects, and discrepancies in object quantities. 2)
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">
     Attribute binding.
    </span>
    Objects inherently possess distinctive attributes, like ”color”, ”shape” or ”texture”. It should be guaranteed that these object attributes are meticulously preserved within the generated images, avoiding issues like attribute misalignment or leakage. 3)
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">
     Object relationships.
    </span>
    There can be interaction relationships among the multiple objects, such as spatial relationships like ”left”, ”right” or non-spatial ones like ”holding”, ”playing”. The generation process should encapsulate and convey these relationships within the resultant images with precision and fidelity.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F2">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel" id="S1.F2.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="79" id="S1.F2.sf1.g1" src="/html/2401.15688/assets/x2.png" width="452"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        (a)
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel" id="S1.F2.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="116" id="S1.F2.sf2.g1" src="/html/2401.15688/assets/x3.png" width="452"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        (b)
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
   </div>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2.
    </span>
    <span class="ltx_text ltx_font_bold" id="S1.F2.2.1">
     The overview of existing text-to-image generation methods (a) and our CompAgent (b).
    </span>
    Existing methods generate images from text prompts in a single step. In comparison, guided by the LLM agent, CompAgent divides complex text prompts into individual objects, conquers them separately, and composes them into final images with the tool library.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    While existing text-to-image models do not possess the capability to address the aforementioned three issues, they do demonstrate proficiency in the generation of single objects, encompassing their distinctive types and attributes. Motivated by this, we propose
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">
     CompAgent
    </span>
    , a training-free approach founded upon the principle of divide-and-conquer for compositional text-to-image generation, coordinated by a large language model (LLM) agent. The fundamental idea revolves around breaking down intricate textual sentences into their constituent individual objects, initially ensuring the correctness of these isolated objects, then composing them together to produce the final images. The overview is illustrated in Fig.
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    The core of our method is an AI agent implemented by LLM, which serves as the ”brain” of the framework and is primarily responsible for the following tasks: 1)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">
     Decomposition.
    </span>
    The agent decomposes the complex compositional sentence, extracting and cataloging all objects and their associated attributes. Simultaneously, it designs the layout of the scene, defining the positions of the objects through the specification of bounding boxes. The text-to-image generation models are then engaged to generate images for each individual object. 2)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">
     Planning and Tool use.
    </span>
    The LLM agent conducts reasoning according to the complex text prompt and then formulates a strategic approach to image generation that is contingent upon the presence of object attributes and relationships. Then it employs external tools to perform image generation or editing. 3)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">
     Verification and Feedback.
    </span>
    By leveraging the vision ability of LLM or other visual models, the agent further scrutinizes the generated images, discerns and rectifies potential attribute errors. Additionally, human feedback can be seamlessly incorporated into scene layout refinement, thereby enhancing the quality of the ultimate outputs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Guided by the LLM agent, we introduce three tools to compose multiple individual objects into a single cohesive image according to the scene layout. The first is about
    <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">
     tuning-free multi-concept customization
    </span>
    . Specifically, we impose spatial layout constraints by cross-attention map editing and a pre-trained ControlNet
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib51" title="">
      2023
     </a>
     )
    </cite>
    on a tuning-free single-concept customization network
    <cite class="ltx_cite ltx_citemacro_citep">
     (Li et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023a
     </a>
     )
    </cite>
    for supporting multiple objects. It regards previously generated images of individual objects as user-specified subjects to create customized images. In this way, the object attributes can be guaranteed. The second is a
    <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">
     layout-to-image
    </span>
    model. Through latent updating
    <cite class="ltx_cite ltx_citemacro_citep">
     (Xie et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023
     </a>
     )
    </cite>
    , images are generated with the bounding box layout as the condition. The specification of object types and quantities is made feasible through the imposition of layout conditions, enabling the model to place emphasis on the accurate representation of object relationships. However, the layout-to-image model may not necessarily produce object attributes with absolute accuracy. Therefore, we further present the third tool,
    <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">
     local image editing
    </span>
    . The agent examines the objects with attribute errors in the generated images, which will be masked out through a segmentation model
    <cite class="ltx_cite ltx_citemacro_citep">
     (Kirillov et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023
     </a>
     )
    </cite>
    . Through cross-attention control, subject-driven editing is conducted through the previous multi-concept customization network to replace the erroneous objects with their correct attribute counterparts. In addition to the aforementioned three tools, our toolkit also involves existing text-to-image generation models and vision-language multi-modal models, to handle simple text prompts and assess attribute correctness.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    Our main contributions can be summarized as follows:
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We propose CompAgent to address compositional text-to-image generation through the divide-and-conquer approach. The LLM agent oversees the entire task, performing decomposition, reasoning and overall planning, and tool library use to solve complex cases in text-to-image generation.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       By employing both global and local layout control about the spatial arrangement, we propose a tuning-free multi-concept customization model to address the attribute binding problem. We also observe that the layout-to-image generation manner can ensure the faithfulness of object relationships.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       We introduce the verification and feedback mechanism into our LLM agent. By interacting with our designed local image editing tool, the potential attribute errors can be corrected and the generated images can be further refined.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    We evaluate CompAgent on the recent T2I-CompBench benchmark
    <cite class="ltx_cite ltx_citemacro_citep">
     (Huang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023
     </a>
     )
    </cite>
    for compositional text-to-image generation, involving extensive object attributes and relationships. Both qualitative and quantitative results demonstrate the superiority of our method. We achieve more than 10% improvement in the metrics about compositional text-to-image evaluation. Furthermore, CompAgent can be flexibly extended to various related applications, like multi-concept customized image generation, reference-based image editing, object placement, and so on.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2.
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">
     Text-to-image generation.
    </span>
    With the development of diffusion models
    <cite class="ltx_cite ltx_citemacro_citep">
     (Dhariwal and Nichol,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2021
     </a>
     ; Ho et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2020
     </a>
     )
    </cite>
    , text-to-image generation has achieved remarkable success
    <cite class="ltx_cite ltx_citemacro_citep">
     (Rombach et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2022
     </a>
     ; Saharia et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022
     </a>
     ; Chen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023b
     </a>
     )
    </cite>
    . These models can typically generate highly natural and realistic images, but cannot guarantee the controllability of texts over images. Many subsequent works are thus proposed for controllable text-to-image generation. ControlNet
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib51" title="">
      2023
     </a>
     )
    </cite>
    controls Stable Diffusion with various conditioning inputs like Canny edges, and
    <cite class="ltx_cite ltx_citemacro_citep">
     (Voynov et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023
     </a>
     )
    </cite>
    adopts sketch images for conditions. Layout-to-image methods
    <cite class="ltx_cite ltx_citemacro_citep">
     (Li et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023c
     </a>
     ; Xie et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023
     </a>
     ; Lian et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     ; Chen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      2024
     </a>
     )
    </cite>
    synthesize images according to the given bounding boxes of objects. And some image editing methods
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2023a
     </a>
     ; Chen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      2023a
     </a>
     ; Brooks et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     ; Parmar et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023
     </a>
     )
    </cite>
    edit images according to the user’s instructions. Despite the success of these methods, they are still limited in handling complex text prompts for image generation.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_italic" id="S2.p2.1.1">
     LLM agent.
    </span>
    Large language models (LLMs), like ChatGPT, GPT-4
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023
     </a>
     )
    </cite>
    , Llama
    <cite class="ltx_cite ltx_citemacro_citep">
     (Touvron et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib40" title="">
      2023a
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib41" title="">
      b
     </a>
     )
    </cite>
    , have demonstrated impressive capability in natural language processing. The involvement of vision ability in GPT-4V
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib48" title="">
      2023b
     </a>
     )
    </cite>
    further enables the model to process visual data. Recently, LLMs begin to be adopted as agents for executing complex tasks. These works
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib47" title="">
      2023d
     </a>
     ; Shen et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     ; Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib27" title="">
      2023a
     </a>
     )
    </cite>
    apply LLMs to learn to use tools for tasks like visual interaction, speech processing, and more recent works have extended to more impressive applications like software development
    <cite class="ltx_cite ltx_citemacro_citep">
     (Qian et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023
     </a>
     )
    </cite>
    , gaming
    <cite class="ltx_cite ltx_citemacro_citep">
     († et al
     <span class="ltx_text">
      .
     </span>
     (2022),
     <a class="ltx_ref" href="#bib.bib11" title="">
      FAIR
     </a>
     )
    </cite>
    , or APP use
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib50" title="">
      2023c
     </a>
     )
    </cite>
    . Different from them, we focus on the task of compositional text-to-image generation, and utilize a LLM agent for complex text prompt analysis and method planning.
   </p>
  </div>
  <figure class="ltx_figure" id="S2.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S2.F3.g1" src="/html/2401.15688/assets/x4.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3.
    </span>
    <span class="ltx_text ltx_font_bold" id="S2.F3.2.1">
     The framework of CompAgent.
    </span>
    Given the input containing the complex text prompt, the LLM agent conducts the decomposition and planning tasks to invoke external tools for image generation. It then performs verification or involves human feedback and interacts with the tools for image self-correction. The final image output will well satisfy the requirements from the input text prompt.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3.
   </span>
   Method
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    The overview of our CompAgent is illustrated in Fig.
    <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2. Related Work ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    . The LLM agent coordinates the entire framework. It decomposes the complex text prompt, analyzes the attributes within the text, and designs the plans for the tools to be used. Ultimately, it invokes tools to address the challenges inherent in compositional text-to-image generation.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1.
    </span>
    LLM Agent
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     The primary responsibilities of our LLM agent center around the execution of tasks including decomposition, planning, tool use, as well as the implementation of verification and feedback.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">
      Complex text prompt decomposition.
     </span>
     The input text prompts usually contain multiple objects, each characterized by its distinct attributes. These objects may also interact with each other with specific relationships. Existing image synthesis models cannot capture all these attributes and relationships simultaneously, making it difficult to accurately generate images that align with the input texts.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     To address the challenges, the LLM agent decomposes the complex texts. It extracts the discrete objects embedded within the text prompts, along with their associated attributes. The text-to-image generation model is then harnessed to generate several images for these objects. Since only one object is involved, existing models can typically generate images that match the attributes. The agent also formulates the scene layout represented as captioned bounding boxes, where the position of each foreground object is specified in the (x, y, width, height) format. The layout will guide the image generation process when composing separate objects together.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">
      Tool use.
     </span>
     Since the language model does not have the ability for image generation, external tools are required to be utilized to compose separate objects together for image generation:
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     1) Tuning-free multi-concept customization. It regards previously generated images of individual objects as the target images, and produces images that feature these specific objects according to the input text prompts. For these objects, the presence of multiple images, each characterizing them with accurate attributes, generally assures that the multi-concept customization model can effectively ensure the fidelity of object attributes. However, the model tends to focus excessively on preserving the features of the target images, thus could potentially overlook other information in the texts like object relationships. Therefore, this tool can effectively address the attribute binding problem, but may not guarantee the relationships.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p6">
    <p class="ltx_p" id="S3.SS1.p6.1">
     2) Layout-to-image generation. It generates images with the previously established scene layout as the condition, and does not utilize the target images of objects. The object types and quantities can be specified through the scene layout, thereby facilitating the model’s enhanced attention to information beyond the objects,
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.1">
      i.e.
     </span>
     the object relationships. As a result, such a layout-to-image generation tool can address the object relationship problem well, but the layout guidance only is not enough to guarantee the object attributes.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p7">
    <p class="ltx_p" id="S3.SS1.p7.1">
     3) Local image editing. Since the layout-to-image generation model may not consistently produce objects with correct attributes, we further design the local image editing tool to replace the object with the correct attribute one. Previously generated images of individual objects are leveraged here as the reference for object replacement. This tool will interact reciprocally with the verification mechanism of the agent, collaboratively determining which object requires to be modified.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p8">
    <p class="ltx_p" id="S3.SS1.p8.1">
     4) Text-to-image generation. Existing text-to-image generation models are utilized to generate individual object images during the decomposition step. They will also be used as the tool to generate images given simple text prompts for non-compositional generation.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p9">
    <p class="ltx_p" id="S3.SS1.p9.1">
     5) Vision-text multi-modal model. It assesses whether the object attributes in the images are correct for the verification function by leveraging the visual question answering ability of these models.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p10">
    <p class="ltx_p" id="S3.SS1.p10.1">
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p10.1.1">
      Planning.
     </span>
     In light of the diverse proficiency exhibited by the tools, the strategic selection of tool deployment is a core responsibility of our LLM agent. It mainly analyzes the input text prompt. If the text primarily centers around object attributes and the relationships are relatively straightforward (like spatial relationships ”left”, ”right”), the customization tool will be employed. If object relationships are contained while their attributes are simple (like the naive ”white” snow color attribute), the layout-to-image generation tool is suitable. If the text involves both attributes and relationships, the LLM agent first employs the layout-to-image model to represent object relationships. Then it will leverage the vision-text multi-modal models to scrutinize the correctness of object attributes for verification, and decide whether to adopt the local image editing tool. For simple text prompts with only straightforward attributes or relationships, the text-to-image generation tool will be directly utilized.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p11">
    <p class="ltx_p" id="S3.SS1.p11.1">
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p11.1.1">
      Verification and feedback.
     </span>
     Considering the potential limitation of the layout-to-image generation tool in accurately rendering object attributes, a verification process becomes imperative. We employ existing vision-language multi-modal models like GPT-4V
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib48" title="">
       2023b
      </a>
      )
     </cite>
     , MiniGPT-4
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib52" title="">
       2023
      </a>
      )
     </cite>
     , LLaVA
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023b
      </a>
      )
     </cite>
     , and query it about whether the attributes are correct. If the attributes of some objects are incorrect, the LLM agent will invoke the local image editing tool to substitute the objects with the correct ones.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p12">
    <p class="ltx_p" id="S3.SS1.p12.1">
     Besides, for too complicated text prompts with a higher number of objects, together with intricate attributes and relationships, relying solely on the agent to automatically decompose the text and design scene layout may not necessarily be entirely accurate. In this situation, human feedback can be involved. Humans can make adjustments to the scene layout, such as inappropriate object sizes, positions, missing or extra objects. They can also make modifications to planning and verification errors. Introducing human feedback in the form of layout can reduce the cost of human labor. The accommodation of our framework about human feedback makes our LLM agent more flexible for compositional generation.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2.
    </span>
    Tuning-Free Multi-Concept Customization
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     In this section, we mainly introduce our tuning-free multi-concept customization tool. Its overview is illustrated in Fig.
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2. Tuning-Free Multi-Concept Customization ‣ 3. Method ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . Training a tuning-free customization image generation model typically requires large-scale pre-training for subject representation learning. Currently, there are already tuning-free methods available that support single-concept customization. For our approach, we build upon the existing single-concept customization model, BLIP-Diffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023a
      </a>
      )
     </cite>
     , and extend its capability to accommodate multiple concepts, aided by the incorporation of the scene layout. Remarkably, we eliminate the need for large-scale upstream pre-training and directly construct a tuning-free multi-concept customization model to uphold the integrity of object attributes.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S3.F4.g1" src="/html/2401.15688/assets/x5.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.F4.2.1">
      Illustration of our tuning-free multi-concept customization tool.
     </span>
     ControlNet and cross-attention editing control the scene layout. The embeddings of multiple image concepts and the text prompt are concatenated together and forwarded into U-Net for image generation.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     Specifically, for each concept, we extract its subject prompt embedding with the BLIP-2 encoders and the multi-modal encoder
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023b
      </a>
      )
     </cite>
     . To gain a comprehensive understanding of the precise object attributes, we harness the information contained within multiple images corresponding to a single concept. We collect all the embeddings derived from these images and compute their average, generating the definitive subject prompt embedding for subsequent use. They are concatenated with the text prompt embedding and forwarded into the U-Net of the single-concept customization model
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023a
      </a>
      )
     </cite>
     for image generation.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     However, directly aggregating embeddings from multiple concepts can easily lead to interference between different objects during image generation, resulting in the issue of concept confusion. To avoid this, we leverage the scene layout to regulate the positioning of each object, thereby mitigating the risk of their interference. We employ two levels of layout control - globally and locally. As is seen in the top part of Fig.
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2. Tuning-Free Multi-Concept Customization ‣ 3. Method ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , we mask the background of the COCO dataset
     <cite class="ltx_cite ltx_citemacro_citep">
      (Lin et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2014
      </a>
      )
     </cite>
     , and train a ControlNet
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib51" title="">
       2023
      </a>
      )
     </cite>
     via a layout-to-image paradigm. The ControlNet is utilized to control the U-Net via residuals. It provides strong control at the global level, effectively distinguishing multiple objects, thus well avoiding their confusion. However, it can only control globally and cannot independently control the position of each individual object.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.2">
     For local layout control of individual objects separately, we further propose to edit the cross-attention map according to the scene layout, motivated by the fact that the cross-attention map directly affects the spatial layout of the generated image
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hertz et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023
      </a>
      )
     </cite>
     . Specifically, we collect the cross-attention map of each object words and their attribute words. We add a positive constant number
     <math alttext="\alpha^{+}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1">
      <semantics id="S3.SS2.p4.1.m1.1a">
       <msup id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">
        <mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">
         α
        </mi>
        <mo id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">
         +
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b">
        <apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">
          𝛼
         </ci>
         <plus id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">
         </plus>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">
        \alpha^{+}
       </annotation>
      </semantics>
     </math>
     to the regions corresponding to the presence of the object, while concurrently adding a negative constant number
     <math alttext="\alpha^{-}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1">
      <semantics id="S3.SS2.p4.2.m2.1a">
       <msup id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">
        <mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">
         α
        </mi>
        <mo id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">
         −
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b">
        <apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">
          𝛼
         </ci>
         <minus id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">
         </minus>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">
        \alpha^{-}
       </annotation>
      </semantics>
     </math>
     to the rest regions. Compared to ControlNet, cross-attention editing realizes significantly weaker layout control but can independently govern the position of each object. Therefore, when synergistically integrated with ControlNet, it effectively controls the overall layout of the entire image. Ultimately, guided by the layout, different objects can be distinguished from each other, avoiding the confusion problem and achieving multi-concept customization.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3.
    </span>
    Layout-to-Image Generation
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     To guarantee object relationships, we generate images directly from the scene layout. While our previously employed ControlNet and cross-attention editing approach can indeed tackle the layout-to-image problem, they are characterized by the imposition of too strong constraints upon the layout. Once the scene layout deviates from the precise depiction of object relationships, it becomes challenging to guarantee the accurate representation of these relationships. Therefore, we utilize the strategy of latent updating by backwarding the box-constrained loss
     <cite class="ltx_cite ltx_citemacro_citep">
      (Xie et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2023
      </a>
      )
     </cite>
     for image generation from the layout. It provides a relatively loose control over the layout, thus allowing a flexible assurance of object relationships.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4.
    </span>
    Local Image Editing
   </h3>
   <figure class="ltx_figure" id="S3.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S3.F5.g1" src="/html/2401.15688/assets/x6.png" width="368"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.F5.2.1">
      Illustration of our local image editing tool.
     </span>
     The concept images pass into the customization network for embedding extraction, and the masked segmentation map guides the image generation process.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     To rectify objects with incorrect attributes, we introduce our local image editing tool, as illustrated in Fig.
     <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.4. Local Image Editing ‣ 3. Method ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . By querying our LLM agent for verification, we can identify which object attributes are erroneous and require modification. We utilize the combination of Grounding DINO
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023c
      </a>
      )
     </cite>
     and SAM
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kirillov et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      )
     </cite>
     to segment the object out. The resulting segmented mask is utilized for cross-attention editing to provide position guidance for image editing. The image requiring editing is reconverted into the latent through DDIM inversion, serving as the initial latent for the subsequent image generation process. Images featuring objects characterized by correct attributes have already been generated earlier. These images, together with the text prompts, are processed in a manner analogous to the previous customization model, which serves as the conditional input for the U-Net. The process of image generation generally follows the previous multi-concept customization, with the image DDIM inversion as the initial latent. The segmentation masks are used as the guidance for cross-attention editing, while ControlNet is not employed. In this way, objects with incorrect attributes can be effectively substituted and rectified.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4.
   </span>
   Results
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    We mainly conduct experiments on the recent T2I-CompBench benchmark
    <cite class="ltx_cite ltx_citemacro_citep">
     (Huang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023
     </a>
     )
    </cite>
    , which mainly divides compositional text prompts into six sub-categories. For quantitative comparison, we following its setting, and utilize the BLIP-VQA metric for attribute binding evaluation, the UniDet-based metric for spatial relationship evaluation, CLIPScore for non-spatial relationship, and the 3-in-1 metric for complex prompts. We also include qualitative results in both the main paper and supplementary materials.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1.
    </span>
    Quantitative Comparison
   </h3>
   <figure class="ltx_table" id="S4.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.T1.9.1">
      Quantitative Comparison on T2I-CompBench with existing text-to-image generation models and compositional methods
     </span>
     . Our method demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. The best value is bolded, and the second-best value is underlined.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.7" style="width:433.6pt;height:309.7pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(2.6pt,-1.8pt) scale(1.01201236107396,1.01201236107396) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.7.7">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.T1.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.2" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">
           Model
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.1.1.3">
          <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">
           Attribute Binding
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.1.1.1.4">
          <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">
           Object Relationship
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.1" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">
           Complex
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.1.m1.1">
            <semantics id="S4.T1.1.1.1.1.1.1.m1.1a">
             <mo id="S4.T1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b">
              <ci id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.6.6.6">
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.1">
          <span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1.1">
           Color
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.1.m1.1">
            <semantics id="S4.T1.2.2.2.1.1.m1.1a">
             <mo id="S4.T1.2.2.2.1.1.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.1.m1.1b">
              <ci id="S4.T1.2.2.2.1.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.3.2">
          <span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.2.1">
           Shape
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.2.1.m1.1">
            <semantics id="S4.T1.3.3.3.2.1.m1.1a">
             <mo id="S4.T1.3.3.3.2.1.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.2.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.2.1.m1.1b">
              <ci id="S4.T1.3.3.3.2.1.m1.1.1.cmml" xref="S4.T1.3.3.3.2.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.3.3.3.2.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.4.3">
          <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.3.1">
           Texture
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.3.1.m1.1">
            <semantics id="S4.T1.4.4.4.3.1.m1.1a">
             <mo id="S4.T1.4.4.4.3.1.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.3.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.3.1.m1.1b">
              <ci id="S4.T1.4.4.4.3.1.m1.1.1.cmml" xref="S4.T1.4.4.4.3.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.4.4.4.3.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.5.4">
          <span class="ltx_text ltx_font_bold" id="S4.T1.5.5.5.4.1">
           Spatial
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.5.5.5.4.1.m1.1">
            <semantics id="S4.T1.5.5.5.4.1.m1.1a">
             <mo id="S4.T1.5.5.5.4.1.m1.1.1" stretchy="false" xref="S4.T1.5.5.5.4.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.4.1.m1.1b">
              <ci id="S4.T1.5.5.5.4.1.m1.1.1.cmml" xref="S4.T1.5.5.5.4.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.5.5.5.4.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.6.6.5">
          <span class="ltx_text ltx_font_bold" id="S4.T1.6.6.6.5.1">
           Non-Spatial
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.6.6.6.5.1.m1.1">
            <semantics id="S4.T1.6.6.6.5.1.m1.1a">
             <mo id="S4.T1.6.6.6.5.1.m1.1.1" stretchy="false" xref="S4.T1.6.6.6.5.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.5.1.m1.1b">
              <ci id="S4.T1.6.6.6.5.1.m1.1.1.cmml" xref="S4.T1.6.6.6.5.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.6.6.6.5.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.8.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.7.8.1.1">
          Stable v1.4
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.2">
          0.3765
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.3">
          0.3576
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.4">
          0.4156
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.5">
          0.1246
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.6">
          0.3079
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.8.1.7">
          0.3080
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.9.2">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.9.2.1">
          Stable v2
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.2">
          0.5065
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.3">
          0.4221
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.4">
          0.4922
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.5">
          0.1342
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.6">
          0.3096
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.9.2.7">
          0.3386
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.10.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.10.3.1">
          DALL-E 2
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.2">
          0.5750
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.3">
          0.5464
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.4">
          0.6374
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.5">
          0.1283
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.6">
          0.3043
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.10.3.7">
          0.3696
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.11.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.11.4.1">
          SDXL
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.2">
          0.6369
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.3">
          0.5408
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.4">
          0.5637
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.5">
          0.2032
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.6">
          0.3110
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.11.4.7">
          0.4091
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.7">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.7.1">
          PixArt-
          <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.T1.7.7.7.1.m1.1">
           <semantics id="S4.T1.7.7.7.1.m1.1a">
            <mi id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">
             α
            </mi>
            <annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b">
             <ci id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">
              𝛼
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">
             \alpha
            </annotation>
           </semantics>
          </math>
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.2">
          0.6886
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.3">
          0.5582
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.4">
          0.7044
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.5">
          0.2082
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.6">
          0.3179
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.7">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.7.7.1">
           0.4117
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.12.5">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.12.5.1">
          DALL-E 3
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.2">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.12.5.2.1">
           0.8110
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.3">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.12.5.3.1">
           0.6750
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.4">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.12.5.4.1">
           0.8070
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.5">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.6">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.12.5.7">
          -
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.13.6">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.7.13.6.1">
          Composable Diffusion
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.2">
          0.4063
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.3">
          0.3299
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.4">
          0.3645
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.5">
          0.0800
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.6">
          0.2980
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.7.13.6.7">
          0.2898
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.14.7">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.14.7.1">
          Attn-Mask-Control
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.2">
          0.4119
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.3">
          0.4649
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.4">
          0.4505
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.5">
          0.1249
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.6">
          0.3046
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.14.7.7">
          0.3779
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.15.8">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.15.8.1">
          StructureDiffusion
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.2">
          0.4990
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.3">
          0.4218
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.4">
          0.4900
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.5">
          0.1386
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.6">
          0.3111
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.15.8.7">
          0.3355
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.16.9">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.16.9.1">
          TokenCompose
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.2">
          0.5055
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.3">
          0.4852
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.4">
          0.5881
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.5">
          0.1815
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.6">
          0.3173
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.16.9.7">
          0.2937
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.17.10">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.17.10.1">
          Attn-Exct
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.2">
          0.6400
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.3">
          0.4517
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.4">
          0.5963
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.5">
          0.1455
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.6">
          0.3109
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.17.10.7">
          0.3401
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.18.11">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.18.11.1">
          GORS
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.2">
          0.6603
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.3">
          0.4785
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.4">
          0.6287
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.5">
          0.1815
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.6">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.18.11.6.1">
           0.3193
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.18.11.7">
          0.3328
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.19.12">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.19.12.1">
          ECLIPSE
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.2">
          0.6119
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.3">
          0.5429
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.4">
          0.6165
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.5">
          0.1903
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.6">
          0.3139
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.19.12.7">
          -
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.20.13">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.20.13.1">
          LMD
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.2">
          0.4838
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.3">
          0.5266
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.4">
          0.6215
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.5">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.20.13.5.1">
           0.4594
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.6">
          0.2735
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T1.7.7.20.13.7">
          0.3827
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.7.7.21.14">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.1">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.1.1">
           CompAgent (ours)
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.2">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.2.1">
           0.8488
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.3">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.3.1">
           0.7233
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.4">
          <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.7.21.14.4.1">
           0.7916
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.5">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.5.1">
           0.4837
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.6">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.6.1">
           0.3212
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.7.7.21.14.7">
          <span class="ltx_text ltx_font_bold" id="S4.T1.7.7.21.14.7.1">
           0.4863
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     We list the quantitative metric results of our CompAgent in Tab.
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.1. Quantitative Comparison ‣ 4. Results ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . We compare with existing state-of-the-art text-to-image synthesis methods and models that are designed for complex text prompts. For text-to-image generation, we compare with the recent Stable Diffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Rombach et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2022
      </a>
      )
     </cite>
     v1.4, v2 and XL
     <cite class="ltx_cite ltx_citemacro_citep">
      (Podell et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
      )
     </cite>
     model, DALL-E 2
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ramesh et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2022
      </a>
      )
     </cite>
     , PixArt-
     <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1">
      <semantics id="S4.SS1.p1.1.m1.1a">
       <mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">
        α
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b">
        <ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">
         𝛼
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">
        \alpha
       </annotation>
      </semantics>
     </math>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chen et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023b
      </a>
      )
     </cite>
     and DALL-E 3
     <cite class="ltx_cite ltx_citemacro_citep">
      (Betker et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2023
      </a>
      )
     </cite>
     . For compositional text-to-image generation methods, we compare with Composable Diffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2022
      </a>
      )
     </cite>
     , StructureDiffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Feng et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2023
      </a>
      )
     </cite>
     , Attn-Mask-Control
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023a
      </a>
      )
     </cite>
     , GORS
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023
      </a>
      )
     </cite>
     . TokenCompose
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023b
      </a>
      )
     </cite>
     , Attn-Exct
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chefer et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      )
     </cite>
     , ECLIPSE
     <cite class="ltx_cite ltx_citemacro_citep">
      (Patel et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2023
      </a>
      )
     </cite>
     and LMD
     <cite class="ltx_cite ltx_citemacro_citep">
      (Lian et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      )
     </cite>
     target at multiple objects within a sentence so we also compare with them.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     For attribute binding, our method achieves significantly higher BLIP-VQA metric compared to previous methods, 16.02%, 16.51% and 8.72% higher for the color, shape and texture attributes compared with PixArt-
     <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1">
      <semantics id="S4.SS1.p2.1.m1.1a">
       <mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">
        α
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b">
        <ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">
         𝛼
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">
        \alpha
       </annotation>
      </semantics>
     </math>
     respectively. Compared to DALL-E 3, the current state-of-the-art method for controllability in text-to-image generation, our approach performs on par with it in terms of the performance. For the color and shape attributes, our CompAgent is even 3.78% and 4.83% higher than it, based on the Stable Diffusion model. This demonstrates the capability of CompAgent to accurately generate object types as well as their attributes. Compared to single-step generation, the superiority of such the divide-and-conquer multi-step generation manner can thus be observed in attribute binding. For object relationships, CompAgent excels in both spatial and non-spatial relationships. In contrast, previous methods either lack such ability or are limited to handling only a single type of relationship. With the ability for both attribute binding and object relationship, our CompAgent can well address complex text prompts: we achieve the 48.63% 3-in-1 metric, which surpasses previous methods by more than 7%. The quantitative results well demonstrate that our method effectively addresses the challenges associated with compositional text-to-image generation.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2.
    </span>
    Ablation Study
   </h3>
   <figure class="ltx_table" id="S4.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.T2.6.1">
      Ablation study on T2I-CompBench about LLM agent planning and verification
     </span>
     . ”Customization” denotes our tuning-free multi-concept customization tool, and ”layout-to-image” denotes our layout-to-image generation tool.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.4" style="width:407.6pt;height:84.8pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-12.4pt,2.6pt) scale(0.942434555728378,0.942434555728378) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.4.4">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.T2.4.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T2.4.4.4.5">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.5.1">
           Model
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">
           Color
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1">
            <semantics id="S4.T2.1.1.1.1.1.m1.1a">
             <mo id="S4.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b">
              <ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.2.2">
          <span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.2.1">
           Shape
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.1.m1.1">
            <semantics id="S4.T2.2.2.2.2.1.m1.1a">
             <mo id="S4.T2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.2.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.m1.1b">
              <ci id="S4.T2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.3.3.3.3">
          <span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.1">
           Texture
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.1.m1.1">
            <semantics id="S4.T2.3.3.3.3.1.m1.1a">
             <mo id="S4.T2.3.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.3.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.1.m1.1b">
              <ci id="S4.T2.3.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.4.4">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.4.1">
           Complex
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.1.m1.1">
            <semantics id="S4.T2.4.4.4.4.1.m1.1a">
             <mo id="S4.T2.4.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.4.4.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.1.m1.1b">
              <ci id="S4.T2.4.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.4.4.5.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.5.1.1">
          customization
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.2">
          0.8160
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.3">
          0.6839
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.4">
          0.7692
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.5">
          0.4184
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.4.4.6.2">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.6.2.1">
          layout-to-image
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.2">
          0.6626
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.3">
          0.5505
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.4">
          0.6707
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.5">
          0.4387
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.4.4.7.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.7.3.1">
          + planning
         </th>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.2">
          0.8458
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.3">
          0.7211
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.4">
          0.7916
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.4.4.7.3.5">
          0.4642
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.4.4.8.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.4.4.8.4.1">
          + verification
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.8.4.2">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.2.1">
           0.8488
          </span>
          (
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.2.2" style="color:#39B54A;">
           +3.28%
          </span>
          )
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.8.4.3">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.3.1">
           0.7233
          </span>
          (
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.3.2" style="color:#39B54A;">
           +3.94%
          </span>
          )
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.8.4.4">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.4.1">
           0.7916
          </span>
          (
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.4.2" style="color:#39B54A;">
           +2.24%
          </span>
          )
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.8.4.5">
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.5.1">
           0.4863
          </span>
          (
          <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.4.5.2" style="color:#39B54A;">
           +4.76%
          </span>
          )
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <figure class="ltx_figure" id="S4.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S4.F6.g1" src="/html/2401.15688/assets/x7.png" width="456"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.F6.2.1">
      Visualized examples of CompAgent to show the generated images from different tools and how LLM agent plans to use the tools.
     </span>
     The LLM agent analyzes the text prompt. It employs the customization tool to address attribute binding, and the layout-to-image tool to address object relationships. For complex composition, the layout-to-image tool is utilized for object relationship, then the local image editing tool is used for attribute correction.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S4.F7">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="159" id="S4.F7.g1" src="/html/2401.15688/assets/x8.png" width="443"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.F7.2.1">
      Visualized examples to show the effect of human feedback.
     </span>
     The green boxes are well-generated by the LLM agent. The red boxes are the ones generated by the LLM agent with some issues, which are modified through human feedback into the blue boxes in the second row.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     We further conduct ablation study in this section to analyze the effect of the LLM agent under our framework.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">
      LLM agent planning and verification.
     </span>
     We first analyze the effect of the planning and verification mechanism of our LLM agent in Tab.
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.2. Ablation Study ‣ 4. Results ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . It can be observed that by leveraging the individual object images, the multi-concept customization tool performs well for attribute binding. However, because of the utilized ControlNet, the customization model can be inflexible for expressing the object relationships, which leads to its limited metric scores in complex compositions. In comparison, the layout-to-image generation tool can well generate images with object relationships, but cannot guarantee the accuracy of object attributes. Our LLM agent can analyze the complex text prompts, and plan the most suitable tool to use. As a result, LLM agent planning well helps address most situations for compositional text-to-image generation. The verification mechanism of our LLM agent further helps correct some attribute errors, especially in complex compositions where layout-to-image generation cannot handle the object attributes, which thus contributes to the 2.21% improvement. As we can see, by planning and verification, our LLM agent well utilizes the tools for compositional text-to-image generation.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     We provide some visualized examples in Fig.
     <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.2. Ablation Study ‣ 4. Results ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     . As we can see, the LLM agent well plans and employs the most suitable tool. The customization tool is utilized to strictly constrain object attributes, and the layout-to-image tool can make appropriate adjustments to the layout for object relationships. Besides, the local image editing tool can assist in rectifying objects with incorrect attributes.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">
      Human feedback.
     </span>
     We further provide some visualized examples in Fig.
     <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.2. Ablation Study ‣ 4. Results ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     to show the effect of human feedback. For the first example, although object types and attributes are correct, the size of the glass cup is too large. By involving human feedback to modify the scene layout, such a problem can be addressed. For the second example, the texts are quite complex and there are some mistakes in the scene layout - one less hot dog, small table and too small car. Humans can inspect and correct them, then CompAgent can generate accurate images. This also applies to the third example. Our CompAgent can incorporate human feedback, enabling it to generate more realistic images and handle more complex text prompts.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3.
    </span>
    Qualitative Comparison
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     We provide more visualized results in Fig.
     <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‣ 5. Conclusion ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     , Fig.
     <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‣ 5. Conclusion ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     , Fig.
     <a class="ltx_ref" href="#S5.F10" title="Figure 10 ‣ 5. Conclusion ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     . CompAgent generates correctly for object types, attributes and relationships. The excellent ability of our method for compositional text-to-image generation can thus be further demonstrated.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     We then provide the qualitative comparison with existing text-to-image generation methods and compositional text-to-image generation methods in Fig.
     <a class="ltx_ref" href="#S5.F11" title="Figure 11 ‣ 5. Conclusion ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     . For the text ”a black guitar and a brown amplifier”, existing methods are easy to confuse the color of the guitar and the amplifier. In the second example, where four objects exist in the text, the correct object number also cannot be guaranteed for existing methods. For some uncommon attributes, like the triangular shelf in the third example and the blue sink in the last example, existing models are also easy to make mistakes about the attributes. Besides, in the fifth example, most of existing methods cannot express the ”left” relationship accurately. Our CompAgent generates accurately for all these text prompts. This further demonstrates the superiority of our method over existing models when it comes to compositional text-to-image generation.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5.
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we propose a training-free approach, CompAgent, for compositional text-to-image generation. By decomposing, planning, verifying, and involving human feedback, the LLM agent coordinates the whole system and employs external tools to generate high-fidelity and accurate images according to the given complex text prompts. Extensive results demonstrate that CompAgent well addresses the object type and quantity, attribute binding, and object relationship problems in compositional text-to-image generation. We consider CompAgent as an important step towards the future of autonomous agents empowered by language models and the controllability in text-to-image generation.
   </p>
  </div>
  <figure class="ltx_figure" id="S5.F8">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S5.F8.g1" src="/html/2401.15688/assets/x9.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 8.
    </span>
    <span class="ltx_text ltx_font_bold" id="S5.F8.2.1">
     Visualized results of our method for attribute binding.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S5.F9">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="163" id="S5.F9.g1" src="/html/2401.15688/assets/x10.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 9.
    </span>
    <span class="ltx_text ltx_font_bold" id="S5.F9.2.1">
     Visualized results of our method for object relationship.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S5.F10">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S5.F10.g1" src="/html/2401.15688/assets/x11.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 10.
    </span>
    <span class="ltx_text ltx_font_bold" id="S5.F10.2.1">
     Visualized results of our method for complex composition.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S5.F11">
   <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="513" id="S5.F11.g1" src="/html/2401.15688/assets/x12.png" width="456"/>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 11.
    </span>
    <span class="ltx_text ltx_font_bold" id="S5.F11.2.1">
     Qualitative comparison between our approach and previous methods.
    </span>
    The first three lines are existing text-to-image generation models, and lines four to six are existing compositional text-to-image generation methods. Our approach performs significantly better in both attribute binding and object relationship aspects compared to previous methods.
   </figcaption>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Implementation Details
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.2">
    The main part of our experiments applies GPT-4
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023
     </a>
     )
    </cite>
    as our LLM agent. For our toolkits, the multi-modal models utilized for attribute verification are implemented by GPT-4V. The image generation tools are implemented based on Stable Diffusion
    <cite class="ltx_cite ltx_citemacro_citep">
     (Rombach et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2022
     </a>
     )
    </cite>
    . The text-to-image generation tool and the layout-to-image generation tool are based on Stable Diffusion v2, and the tuning-free multi-concept customization tool and the local image editing tool are based on Stable Diffusion v1-4. For training ControlNet
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib51" title="">
      2023
     </a>
     )
    </cite>
    in the layout-to-image manner, we utilize the COCO dataset
    <cite class="ltx_cite ltx_citemacro_citep">
     (Lin et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      2014
     </a>
     )
    </cite>
    and train for 10 epochs.
    <math alttext="\alpha^{+}" class="ltx_Math" display="inline" id="A1.p1.1.m1.1">
     <semantics id="A1.p1.1.m1.1a">
      <msup id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">
       <mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">
        α
       </mi>
       <mo id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">
        +
       </mo>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b">
       <apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">
        <csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">
         superscript
        </csymbol>
        <ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">
         𝛼
        </ci>
        <plus id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">
        </plus>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">
       \alpha^{+}
      </annotation>
     </semantics>
    </math>
    is set to 2.5 and
    <math alttext="\alpha^{-}" class="ltx_Math" display="inline" id="A1.p1.2.m2.1">
     <semantics id="A1.p1.2.m2.1a">
      <msup id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">
       <mi id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">
        α
       </mi>
       <mo id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">
        −
       </mo>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b">
       <apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">
        <csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1">
         superscript
        </csymbol>
        <ci id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">
         𝛼
        </ci>
        <minus id="A1.p1.2.m2.1.1.3.cmml" xref="A1.p1.2.m2.1.1.3">
        </minus>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">
       \alpha^{-}
      </annotation>
     </semantics>
    </math>
    is set to -10,000 for cross attention editing.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Agent Prompt
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    Our prompt input for the LLM agent mainly contains the following parts:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p2">
   <p class="ltx_p" id="A2.p2.1">
    1. Task specification:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p3">
   <svg class="ltx_picture" height="47.85" id="A2.p3.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,47.85) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 42.53 C 0 45.47 2.38 47.85 5.32 47.85 L 594.68 47.85 C 597.62 47.85 600 45.47 600 42.53 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 42.53 C 1.38 44.7 3.15 46.46 5.32 46.46 L 594.68 46.46 C 596.85 46.46 598.62 44.7 598.62 42.53 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.47 9.47)">
      <foreignobject color="#000000" height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="581.06">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.p3.pic1.1.1.1.1.1" style="width:419.9pt;">
        <span class="ltx_p" id="A2.p3.pic1.1.1.1.1.1.1">
         You are an intelligent research assistant. You have access to the following tools to address the compositional text-to-image generation problem:
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p4">
   <p class="ltx_p" id="A2.p4.1">
    2. Tool library introduction:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p5">
   <svg class="ltx_picture" height="147.47" id="A2.p5.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,147.47) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 142.15 C 0 145.09 2.38 147.47 5.32 147.47 L 594.68 147.47 C 597.62 147.47 600 145.09 600 142.15 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 142.15 C 1.38 144.33 3.15 146.09 5.32 146.09 L 594.68 146.09 C 596.85 146.09 598.62 144.33 598.62 142.15 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.47 9.47)">
      <foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="581.06">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.p5.pic1.1.1.1.1.1" style="width:419.9pt;">
        <span class="ltx_p" id="A2.p5.pic1.1.1.1.1.1.1">
         1)
         <span class="ltx_text ltx_font_italic" id="A2.p5.pic1.1.1.1.1.1.1.1">
          The multi-concept customization tool
         </span>
         . It is good at texts where objects are coupled complex attributes, like color, shape, texture. It can also handle certain spatial relationships, like “on the left of, on the right of” and some simple and straightforward object relationships, like the mirror hanging above, the snow covering.
        </span>
        <span class="ltx_p" id="A2.p5.pic1.1.1.1.1.1.2">
         2)
         <span class="ltx_text ltx_font_italic" id="A2.p5.pic1.1.1.1.1.1.2.1">
          The layout-to-image generation tool
         </span>
         . It is good at texts where objects are interacted with complicated relationships or actions, like playing, holding. It can also process attributes that are easy or straightforward, like a red apple, the white snow.
        </span>
        <span class="ltx_p" id="A2.p5.pic1.1.1.1.1.1.3">
         3)
         <span class="ltx_text ltx_font_italic" id="A2.p5.pic1.1.1.1.1.1.3.1">
          The text-to-image generation tool
         </span>
         . It can handle those simple texts without complex attributes and relationships.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p6">
   <p class="ltx_p" id="A2.p6.1">
    3. Decomposition details:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p7">
   <svg class="ltx_picture" height="147.47" id="A2.p7.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,147.47) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 142.15 C 0 145.09 2.38 147.47 5.32 147.47 L 594.68 147.47 C 597.62 147.47 600 145.09 600 142.15 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 142.15 C 1.38 144.33 3.15 146.09 5.32 146.09 L 594.68 146.09 C 596.85 146.09 598.62 144.33 598.62 142.15 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.47 9.47)">
      <foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="581.06">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.p7.pic1.1.1.1.1.1" style="width:419.9pt;">
        <span class="ltx_p" id="A2.p7.pic1.1.1.1.1.1.1">
         I will provide you with a caption for an image. Your should also extract individual objects and generate the bounding boxes for the objects mentioned in the caption. The images are of size 512x512. The top-left corner has coordinate [0, 0]. The bottom-right corner has coordinate [512, 512]. The bounding boxes should not go beyond the boundaries and it is better to avoid overlapping bounding boxes. Each bounding box should be in the format of (object name, [top-left x coordinate, top-left y coordinate, box width, box height]) and include exactly one object (i.e., start the object name with a or an if possible). If needed, you can make reasonable guesses. Please refer to the example below for the desired format.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p8">
   <p class="ltx_p" id="A2.p8.1">
    4. In-context examples:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p9">
   <svg class="ltx_picture" height="165.62" id="A2.p9.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,165.62) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 160.3 C 0 163.23 2.38 165.62 5.32 165.62 L 594.68 165.62 C 597.62 165.62 600 163.23 600 160.3 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 160.3 C 1.38 162.47 3.15 164.23 5.32 164.23 L 594.68 164.23 C 596.85 164.23 598.62 162.47 598.62 160.3 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.47 9.47)">
      <foreignobject color="#000000" height="146.67" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="581.06">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10" style="width:419.9pt;">
        <span class="ltx_p" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
         <math alttext="\mathcal{Q}_{1}" class="ltx_Math" display="inline" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1">
          <semantics id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a">
           <msub id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">
             𝒬
            </mi>
            <mn id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">
             1
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b">
            <apply id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">
              𝒬
             </ci>
             <cn id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">
              1
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">
            \mathcal{Q}_{1}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.11">
         Caption: a blue horse and a brown vase.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
         <math alttext="\mathcal{A}_{1}" class="ltx_Math" display="inline" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1">
          <semantics id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1a">
           <msub id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.2" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml">
             𝒜
            </mi>
            <mn id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.3" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml">
             1
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1b">
            <apply id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.2">
              𝒜
             </ci>
             <cn id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.3">
              1
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1c">
            \mathcal{A}_{1}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.12">
         Analysis: attribute-only.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.13">
         Objects: [(’a blue horse’, [50, 70, 220, 300]), (’a brown vase’, [300, 113, 150, 250])]
        </span>
        <span class="ltx_p" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
         <math alttext="\mathcal{Q}_{2}" class="ltx_Math" display="inline" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1">
          <semantics id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1a">
           <msub id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.2" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml">
             𝒬
            </mi>
            <mn id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.3" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml">
             2
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1b">
            <apply id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.1.cmml" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.2">
              𝒬
             </ci>
             <cn id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.3">
              2
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1c">
            \mathcal{Q}_{2}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.14">
         Caption: a fabric rug and a leather belt.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4">
         <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1">
          <semantics id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1a">
           <msub id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.2" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml">
             𝒜
            </mi>
            <mn id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.3" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml">
             2
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1b">
            <apply id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.1.cmml" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.2">
              𝒜
             </ci>
             <cn id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.3">
              2
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1c">
            \mathcal{A}_{2}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.15">
         Analysis: attribute-only.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.16">
         Objects: [(’a fabric rug’, [20, 200, 470, 150]), (’a leather belt’, [100, 250, 300, 20])]
        </span>
        <span class="ltx_p" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5">
         <math alttext="\mathcal{Q}_{3}" class="ltx_Math" display="inline" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1">
          <semantics id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1a">
           <msub id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.2" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml">
             𝒬
            </mi>
            <mn id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.3" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml">
             3
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1b">
            <apply id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.1.cmml" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.2">
              𝒬
             </ci>
             <cn id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.3">
              3
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1c">
            \mathcal{Q}_{3}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.17">
         Caption: A cat is wearing a collar with a bell on it.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6">
         <math alttext="\mathcal{A}_{3}" class="ltx_Math" display="inline" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1">
          <semantics id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1a">
           <msub id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.2" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.2.cmml">
             𝒜
            </mi>
            <mn id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.3" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.3.cmml">
             3
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1b">
            <apply id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.cmml" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.1.cmml" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.2.cmml" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.2">
              𝒜
             </ci>
             <cn id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.3">
              3
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1c">
            \mathcal{A}_{3}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.18">
         Analysis: relationship-only.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.19">
         Objects: [(’a cat’, [120, 150, 300, 300]), (’a collar’, [120, 300, 300, 50]), (’a bell, [250, 320, 110, 100])]
        </span>
        <span class="ltx_p" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7">
         <math alttext="\mathcal{Q}_{4}" class="ltx_Math" display="inline" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1">
          <semantics id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1a">
           <msub id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.2" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.2.cmml">
             𝒬
            </mi>
            <mn id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.3" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.3.cmml">
             4
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1b">
            <apply id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.1.cmml" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.2.cmml" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.2">
              𝒬
             </ci>
             <cn id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.3">
              4
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1c">
            \mathcal{Q}_{4}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.20">
         Caption: The blue bowl was on top of the white placemat.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8">
         <math alttext="\mathcal{A}_{4}" class="ltx_Math" display="inline" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1">
          <semantics id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1a">
           <msub id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.2" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.2.cmml">
             𝒜
            </mi>
            <mn id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.3" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.3.cmml">
             4
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1b">
            <apply id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.cmml" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.1.cmml" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.2.cmml" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.2">
              𝒜
             </ci>
             <cn id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.3">
              4
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1c">
            \mathcal{A}_{4}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.21">
         Analysis: both.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.22">
         Objects: [(’a rectangular mirror’, [170, 80, 172, 100]), (’a white sink’, [150, 200, 212, 150])]
        </span>
        <span class="ltx_p" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9">
         <math alttext="\mathcal{Q}_{5}" class="ltx_Math" display="inline" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1">
          <semantics id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1a">
           <msub id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2.cmml">
             𝒬
            </mi>
            <mn id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3.cmml">
             5
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1b">
            <apply id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.1.cmml" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2.cmml" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2">
              𝒬
             </ci>
             <cn id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3">
              5
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1c">
            \mathcal{Q}_{5}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.23">
         Caption: The red apple was on top of the plate.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10">
         <math alttext="\mathcal{A}_{5}" class="ltx_Math" display="inline" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1">
          <semantics id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1a">
           <msub id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.cmml">
            <mi class="ltx_font_mathcaligraphic" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.2" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.2.cmml">
             𝒜
            </mi>
            <mn id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.3" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.3.cmml">
             5
            </mn>
           </msub>
           <annotation-xml encoding="MathML-Content" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1b">
            <apply id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.cmml" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1">
             <csymbol cd="ambiguous" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.1.cmml" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1">
              subscript
             </csymbol>
             <ci id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.2.cmml" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.2">
              𝒜
             </ci>
             <cn id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.3.cmml" type="integer" xref="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.3">
              5
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1c">
            \mathcal{A}_{5}
           </annotation>
          </semantics>
         </math>
         :
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.24">
         Analysis: relationship-only.
        </span>
        <span class="ltx_p" id="A2.p9.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.25">
         Objects: [(’a red apple’, [235, 230, 60, 60]), (’a plate’, [175, 210, 180, 180])]
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para" id="A2.p10">
   <p class="ltx_p" id="A2.p10.1">
    After providing the above prompt to the agent, we then prompt the LLM agent for completion:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A2.p11">
   <svg class="ltx_picture" height="32.78" id="A2.p11.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,32.78) matrix(1 0 0 -1 0 0)">
     <g fill="#000000" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 27.46 C 0 30.4 2.38 32.78 5.32 32.78 L 594.68 32.78 C 597.62 32.78 600 30.4 600 27.46 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 27.46 C 1.38 29.63 3.15 31.4 5.32 31.4 L 594.68 31.4 C 596.85 31.4 598.62 29.63 598.62 27.46 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.47 9.47)">
      <foreignobject color="#000000" height="13.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="581.06">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.p11.pic1.1.1.1.1.1" style="width:419.9pt;">
        <span class="ltx_p" id="A2.p11.pic1.1.1.1.1.1.1">
         Caption: [input prompt from the user]
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para" id="A2.p12">
   <p class="ltx_p" id="A2.p12.1">
    The resulting text analysis and decomposition results from the LLM agent is then parsed and used for the subsequent image generation process.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   FURTHER ABLATIONS AND EXPERIMENTS
  </h2>
  <section class="ltx_subsection" id="A3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.1.
    </span>
    Agent Type
   </h3>
   <figure class="ltx_table" id="A3.T3">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.T3.8.1">
      The impact of LLM agent type on T2I-CompBench for compositional text-to-image generation
     </span>
     .
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T3.6" style="width:433.6pt;height:122pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(24.9pt,-7.0pt) scale(1.12987064209474,1.12987064209474) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T3.6.6">
       <thead class="ltx_thead">
        <tr class="ltx_tr" id="A3.T3.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T3.1.1.1.2" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.2.1">
           LLM Agent
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A3.T3.1.1.1.3">
          <span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.3.1">
           Attribute Binding
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A3.T3.1.1.1.4">
          <span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.4.1">
           Object Relationship
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T3.1.1.1.1" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.1.1">
           Complex
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.1.1.1.1.1.1.m1.1">
            <semantics id="A3.T3.1.1.1.1.1.1.m1.1a">
             <mo id="A3.T3.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T3.1.1.1.1.1.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.1.1.1.1.1.1.m1.1b">
              <ci id="A3.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="A3.T3.1.1.1.1.1.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.1.1.1.1.1.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
        </tr>
        <tr class="ltx_tr" id="A3.T3.6.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T3.2.2.2.1">
          <span class="ltx_text ltx_font_bold" id="A3.T3.2.2.2.1.1">
           Color
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.2.2.2.1.1.m1.1">
            <semantics id="A3.T3.2.2.2.1.1.m1.1a">
             <mo id="A3.T3.2.2.2.1.1.m1.1.1" stretchy="false" xref="A3.T3.2.2.2.1.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.2.2.2.1.1.m1.1b">
              <ci id="A3.T3.2.2.2.1.1.m1.1.1.cmml" xref="A3.T3.2.2.2.1.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.2.2.2.1.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T3.3.3.3.2">
          <span class="ltx_text ltx_font_bold" id="A3.T3.3.3.3.2.1">
           Shape
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.3.3.3.2.1.m1.1">
            <semantics id="A3.T3.3.3.3.2.1.m1.1a">
             <mo id="A3.T3.3.3.3.2.1.m1.1.1" stretchy="false" xref="A3.T3.3.3.3.2.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.3.3.3.2.1.m1.1b">
              <ci id="A3.T3.3.3.3.2.1.m1.1.1.cmml" xref="A3.T3.3.3.3.2.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.3.3.3.2.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T3.4.4.4.3">
          <span class="ltx_text ltx_font_bold" id="A3.T3.4.4.4.3.1">
           Texture
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.4.4.4.3.1.m1.1">
            <semantics id="A3.T3.4.4.4.3.1.m1.1a">
             <mo id="A3.T3.4.4.4.3.1.m1.1.1" stretchy="false" xref="A3.T3.4.4.4.3.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.4.4.4.3.1.m1.1b">
              <ci id="A3.T3.4.4.4.3.1.m1.1.1.cmml" xref="A3.T3.4.4.4.3.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.4.4.4.3.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T3.5.5.5.4">
          <span class="ltx_text ltx_font_bold" id="A3.T3.5.5.5.4.1">
           Spatial
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.5.5.5.4.1.m1.1">
            <semantics id="A3.T3.5.5.5.4.1.m1.1a">
             <mo id="A3.T3.5.5.5.4.1.m1.1.1" stretchy="false" xref="A3.T3.5.5.5.4.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.5.5.5.4.1.m1.1b">
              <ci id="A3.T3.5.5.5.4.1.m1.1.1.cmml" xref="A3.T3.5.5.5.4.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.5.5.5.4.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T3.6.6.6.5">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.6.5.1">
           Non-Spatial
           <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.6.6.6.5.1.m1.1">
            <semantics id="A3.T3.6.6.6.5.1.m1.1a">
             <mo id="A3.T3.6.6.6.5.1.m1.1.1" stretchy="false" xref="A3.T3.6.6.6.5.1.m1.1.1.cmml">
              ↑
             </mo>
             <annotation-xml encoding="MathML-Content" id="A3.T3.6.6.6.5.1.m1.1b">
              <ci id="A3.T3.6.6.6.5.1.m1.1.1.cmml" xref="A3.T3.6.6.6.5.1.m1.1.1">
               ↑
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A3.T3.6.6.6.5.1.m1.1c">
              \uparrow
             </annotation>
            </semantics>
           </math>
          </span>
         </th>
        </tr>
       </thead>
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="A3.T3.6.6.7.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T3.6.6.7.1.1">
          Llama-7B
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.2">
          0.6994
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.3">
          0.5740
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.4">
          0.6927
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.5">
          0.3138
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.6">
          0.3102
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T3.6.6.7.1.7">
          0.3515
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T3.6.6.8.2">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T3.6.6.8.2.1">
          Llama-70B
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.2">
          0.7400
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.3">
          0.6305
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.4">
          0.7102
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.5">
          0.3698
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.6">
          0.3104
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.8.2.7">
          0.4475
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T3.6.6.9.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T3.6.6.9.3.1">
          GPT-3.5
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.2">
          0.7925
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.3">
          0.6647
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.4">
          0.7743
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.5">
          0.4439
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.6">
          0.3195
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T3.6.6.9.3.7">
          0.4752
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T3.6.6.10.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T3.6.6.10.4.1">
          GPT-4
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.2">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.2.1">
           0.8488
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.3">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.3.1">
           0.7233
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.4">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.4.1">
           0.7916
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.5">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.5.1">
           0.4837
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.6">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.6.1">
           0.3212
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T3.6.6.10.4.7">
          <span class="ltx_text ltx_font_bold" id="A3.T3.6.6.10.4.7.1">
           0.4863
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="A3.SS1.p1">
    <p class="ltx_p" id="A3.SS1.p1.1">
     We first conduct compositional text-to-image generation experiments on the T2I-CompBench
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023
      </a>
      )
     </cite>
     guided by the agent implemented by various LLMs. The experimental results are listed in Tab.
     <a class="ltx_ref" href="#A3.T3" title="Table 3 ‣ C.1. Agent Type ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . We adopt GPT-3.5, GPT-4 and the open-sourced Llama 2
     <cite class="ltx_cite ltx_citemacro_citep">
      (Touvron et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib41" title="">
       2023b
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p2">
    <p class="ltx_p" id="A3.SS1.p2.1">
     It can be seen that as the language understanding ability of large language models improves, the performance of our CompAgent for compositional text-to-image generation also enhances. When adopting GPT-4 as the agent, the metric reaches the highest. This is because GPT-4 can not only decompose individual objects successfully, but also generate the scene layout where the shapes of bounding boxes are more suitable for the object types. Meanwhile, even utilizing the Llama-7B model as our agent, the compositional text-to-image generation metric is still higher than most of existing methods. This demonstrates the flexibility of our CompAgent across different large language models.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.2.
    </span>
    Image Generation with Layout Guidance
   </h3>
   <figure class="ltx_table" id="A3.T4">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 4.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.T4.7.1">
      Evaluation for layout-to-image generation
     </span>
     . We adopt the YOLO score to evaluate the correspondence of images and their layouts.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T4.5" style="width:368.6pt;height:247.5pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(36.8pt,-24.7pt) scale(1.24975010414914,1.24975010414914) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T4.5.5">
       <thead class="ltx_thead">
        <tr class="ltx_tr" id="A3.T4.5.5.5">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T4.5.5.5.6">
          Methods
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.1.1.1.1">
          AP
          <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T4.1.1.1.1.m1.1">
           <semantics id="A3.T4.1.1.1.1.m1.1a">
            <mo id="A3.T4.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T4.1.1.1.1.m1.1.1.cmml">
             ↑
            </mo>
            <annotation-xml encoding="MathML-Content" id="A3.T4.1.1.1.1.m1.1b">
             <ci id="A3.T4.1.1.1.1.m1.1.1.cmml" xref="A3.T4.1.1.1.1.m1.1.1">
              ↑
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="A3.T4.1.1.1.1.m1.1c">
             \uparrow
            </annotation>
           </semantics>
          </math>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.3.3.3.3">
          AP
          <sub class="ltx_sub" id="A3.T4.3.3.3.3.1">
           <span class="ltx_text ltx_font_italic" id="A3.T4.3.3.3.3.1.1">
            50
           </span>
          </sub>
          <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T4.3.3.3.3.m2.1">
           <semantics id="A3.T4.3.3.3.3.m2.1a">
            <mo id="A3.T4.3.3.3.3.m2.1.1" stretchy="false" xref="A3.T4.3.3.3.3.m2.1.1.cmml">
             ↑
            </mo>
            <annotation-xml encoding="MathML-Content" id="A3.T4.3.3.3.3.m2.1b">
             <ci id="A3.T4.3.3.3.3.m2.1.1.cmml" xref="A3.T4.3.3.3.3.m2.1.1">
              ↑
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="A3.T4.3.3.3.3.m2.1c">
             \uparrow
            </annotation>
           </semantics>
          </math>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.5.5.5.5">
          AP
          <sub class="ltx_sub" id="A3.T4.5.5.5.5.1">
           <span class="ltx_text ltx_font_italic" id="A3.T4.5.5.5.5.1.1">
            75
           </span>
          </sub>
          <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T4.5.5.5.5.m2.1">
           <semantics id="A3.T4.5.5.5.5.m2.1a">
            <mo id="A3.T4.5.5.5.5.m2.1.1" stretchy="false" xref="A3.T4.5.5.5.5.m2.1.1.cmml">
             ↑
            </mo>
            <annotation-xml encoding="MathML-Content" id="A3.T4.5.5.5.5.m2.1b">
             <ci id="A3.T4.5.5.5.5.m2.1.1.cmml" xref="A3.T4.5.5.5.5.m2.1.1">
              ↑
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="A3.T4.5.5.5.5.m2.1c">
             \uparrow
            </annotation>
           </semantics>
          </math>
         </th>
        </tr>
       </thead>
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="A3.T4.5.5.6.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T4.5.5.6.1.1">
          LostGAN
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.2">
          0.053
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.3">
          0.089
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.4">
          0.056
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.7.2">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.7.2.1">
          LAMA
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.2">
          0.102
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.3">
          0.153
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.4">
          0.117
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.8.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.8.3.1">
          TwFA
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.8.3.2">
          0.106
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.8.3.3">
          0.147
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.8.3.4">
          0.126
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.9.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.9.4.1">
          Stable Diffusion
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.9.4.2">
          0.028
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.9.4.3">
          0.092
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.9.4.4">
          0.011
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.10.5">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.10.5.1">
          GLIGEN
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.10.5.2">
          0.297
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.10.5.3">
          0.458
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.10.5.4">
          0.339
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.11.6">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.11.6.1">
          GLIGEN + BoxDiff
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.11.6.2">
          0.402
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.11.6.3">
          0.620
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.11.6.4">
          0.462
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.12.7">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T4.5.5.12.7.1">
          latent updating
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.12.7.2">
          0.224
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.12.7.3">
          0.468
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.12.7.4">
          0.178
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.13.8">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.13.8.1">
          cross-attention editing
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.13.8.2">
          0.060
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.13.8.3">
          0.190
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.13.8.4">
          0.021
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.14.9">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.5.5.14.9.1">
          ControlNet
         </th>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.14.9.2">
          0.338
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.14.9.3">
          0.521
         </td>
         <td class="ltx_td ltx_align_center" id="A3.T4.5.5.14.9.4">
          0.339
         </td>
        </tr>
        <tr class="ltx_tr" id="A3.T4.5.5.15.10">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T4.5.5.15.10.1">
          ControlNet + cross-attention editing
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.15.10.2">
          <span class="ltx_text ltx_font_bold" id="A3.T4.5.5.15.10.2.1">
           0.508
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.15.10.3">
          <span class="ltx_text ltx_font_bold" id="A3.T4.5.5.15.10.3.1">
           0.778
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.15.10.4">
          <span class="ltx_text ltx_font_bold" id="A3.T4.5.5.15.10.4.1">
           0.534
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="A3.SS2.p1">
    <p class="ltx_p" id="A3.SS2.p1.2">
     For composing multiple individual objects into the image, the generated scene layouts guide the generation process under our CompAgent framework. We compare the performance of different layout-to-image generation methods and list the results in Tab.
     <a class="ltx_ref" href="#A3.T4" title="Table 4 ‣ C.2. Image Generation with Layout Guidance ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . We compare with previous methods, including LostGAN
     <cite class="ltx_cite ltx_citemacro_citep">
      (Sun and Wu,
      <a class="ltx_ref" href="#bib.bib39" title="">
       2021
      </a>
      )
     </cite>
     , LAMA
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2021
      </a>
      )
     </cite>
     , TwFA
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2022
      </a>
      )
     </cite>
     , Stable Diffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Rombach et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2022
      </a>
      )
     </cite>
     , GLIGEN
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2023c
      </a>
      )
     </cite>
     and GLIGEN + BoxDiff
     <cite class="ltx_cite ltx_citemacro_citep">
      (Xie et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2023
      </a>
      )
     </cite>
     . Under our CompAgent framework, we mainly utilize latent updating, cross-attention editing and ControlNet three strategies. We conduct experiments on the benchmark proposed in
     <cite class="ltx_cite ltx_citemacro_citep">
      (Xie et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2023
      </a>
      )
     </cite>
     . We apply the YOLOv4
     <cite class="ltx_cite ltx_citemacro_citep">
      (Bochkovskiy et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2020
      </a>
      )
     </cite>
     to detect objects and obtain the YOLO score, including AP, AP
     <sub class="ltx_sub" id="A3.SS2.p1.2.1">
      <span class="ltx_text ltx_font_italic" id="A3.SS2.p1.2.1.1">
       50
      </span>
     </sub>
     and AP
     <sub class="ltx_sub" id="A3.SS2.p1.2.2">
      <span class="ltx_text ltx_font_italic" id="A3.SS2.p1.2.2.1">
       75
      </span>
     </sub>
     to evaluate the precision of the layout-to-image performance.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS2.p2">
    <p class="ltx_p" id="A3.SS2.p2.1">
     It can be seen that by utilizing cross-attention editing only to control image generation through bounding boxes, the obtained YOLO score is quite low, only 0.06, slightly higher than Stable Diffusion. This demonstrates that although editing cross-attention maps can provide layout guidance, it is still insufficient to control image generation accurately. Utilizing our trained ControlNet is much more effective, achieving the 0.338 AP, 4.1% higher than GLIGEN. However, ControlNet can only provide global control, not object-level. After combining ControlNet and cross-attention editing, the YOLO score reaches to 0.508, 10.6% higher than GLIGEN + BoxDiff. This demonstrates that our design can control the positions of objects more accurately. This well avoids the confusion of multiple objects, thus is suitable for the attribute binding problem. However, the reliance on the scene layout is too strong in this way, making it inappropriate for generating correct object relationships. Instead, latent updating can well follow the scene layout and can also be flexible meanwhile. Therefore, we utilize the latent updating strategy for the object relationship issue.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.3.
    </span>
    Qualitative Comparison
   </h3>
   <figure class="ltx_figure" id="A3.F12">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="442" id="A3.F12.g1" src="/html/2401.15688/assets/x13.png" width="456"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 12.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.F12.2.1">
      Qualitative comparison between our approach and existing state-of-the-art text-to-image generation methods.
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A3.F13">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="513" id="A3.F13.g1" src="/html/2401.15688/assets/x14.png" width="456"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 13.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.F13.2.1">
      Qualitative comparison between our approach and existing compositional text-to-image generation methods.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A3.SS3.p1">
    <p class="ltx_p" id="A3.SS3.p1.1">
     We then provide more visualized comparisons with existing state-of-the-art text-to-image generation methods in Fig.
     <a class="ltx_ref" href="#A3.F12" title="Figure 12 ‣ C.3. Qualitative Comparison ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       12
      </span>
     </a>
     . Existing methods are highly prone to the following errors. 1) Attribute confusion. For example, for the text ”a blue backpack and a red book”, existing models are easy to confuse the color of the backpack and the red book, or generate a backpack with red parts. 2) Constrained by common attributes or scenarios. For example, for the second example, existing models tend to generate a white sink since it is more common in reality, rather than the brown one. For the third example, existing methods also tend to generate a common living room scenario, while ignoring the required objects and attributes. 3) Incorrect relationship. For example, the ”left” relationship cannot be correctly expressed in the fifth example. In comparison, our CompAgent well avoids these problems, thus generates images that more accurately align with the description of input texts.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p2">
    <p class="ltx_p" id="A3.SS3.p2.1">
     We also provide visualized comparison with existing compositional text-to-image generation methods in Fig.
     <a class="ltx_ref" href="#A3.F13" title="Figure 13 ‣ C.3. Qualitative Comparison ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     . We can observe that existing compositional text-to-image methods also cannot address the above mentioned problems. As a result, the correct object types and quantities, object attributes and relationships cannot be guaranteed. CompAgent behaves equally well for these examples.
    </p>
   </div>
   <figure class="ltx_figure" id="A3.F14">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="A3.F14.g1" src="/html/2401.15688/assets/x15.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 14.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.F14.2.1">
      Visualized results of our method for attribute binding.
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A3.F15">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="A3.F15.g1" src="/html/2401.15688/assets/x16.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 15.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.F15.2.1">
      Visualized results of our method for object relationship.
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A3.F16">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="171" id="A3.F16.g1" src="/html/2401.15688/assets/x17.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 16.
     </span>
     <span class="ltx_text ltx_font_bold" id="A3.F16.2.1">
      Visualized results of our method for complex composition.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A3.SS3.p3">
    <p class="ltx_p" id="A3.SS3.p3.1">
     More visualized examples are provided in Fig.
     <a class="ltx_ref" href="#A3.F14" title="Figure 14 ‣ C.3. Qualitative Comparison ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     , Fig.
     <a class="ltx_ref" href="#A3.F15" title="Figure 15 ‣ C.3. Qualitative Comparison ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       15
      </span>
     </a>
     and Fig.
     <a class="ltx_ref" href="#A3.F16" title="Figure 16 ‣ C.3. Qualitative Comparison ‣ Appendix C FURTHER ABLATIONS AND EXPERIMENTS ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       16
      </span>
     </a>
     . These examples further demonstrate the excellent ability of our CompAgent for addressing the compositional text-to-image generation problem.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Extension to other tasks
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    Besides the compositional text-to-image generation task, our CompAgent can also be flexibly extended to other related image generation tasks with the help of the LLM agent and our toolkits. We mainly introduce about the multi-concept customization, the image editing and the object placement tasks.
   </p>
  </div>
  <figure class="ltx_figure" id="A4.F17">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="401" id="A4.F17.g1" src="/html/2401.15688/assets/x18.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 17.
    </span>
    <span class="ltx_text ltx_font_bold" id="A4.F17.2.1">
     Visualized results of our method for multi-concept customization.
    </span>
    Note that DreamBooth, Custom Diffusion and Cones 2 are all tuning-based methods, while our CompAgent is tuning-free.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="A4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.1.
    </span>
    Multi-Concept Customization
   </h3>
   <div class="ltx_para" id="A4.SS1.p1">
    <p class="ltx_p" id="A4.SS1.p1.1">
     We first conduct the multi-concept customization task to generate images according to the input text prompts containing the given subjects. We compare with existing state-of-the-art customization methods, including DreamBooth
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ruiz et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023
      </a>
      )
     </cite>
     , Custom Diffusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kumari et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023
      </a>
      )
     </cite>
     and Cones 2
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2023d
      </a>
      )
     </cite>
     . The comparison results are provided in Fig.
     <a class="ltx_ref" href="#A4.F17" title="Figure 17 ‣ Appendix D Extension to other tasks ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       17
      </span>
     </a>
     . It can be seen that DreamBooth and Custom Diffusion cannot generate the corresponding objects or their correct attributes. For example, for the first example, DreamBooth does not generate the vase in the image, and the object attributes from the Custom Diffusion image are incorrect. For the second example, Custom Diffusion does not generate the cup, while DreamBooth generates the cup with the incorrect color. Cones 2 performs better than them, generating accurate images with a red book and a yellow vase. However, it is also limited by common attributes. For example, it does not generate the correct color for the cow in the second example, which also applies to the car in the third example. Besides, in the fourth example, Cones 2 confuses the features of the cat and the dog, thus generates the image with two dogs. In comparison, our CompAgent accurately captures the subject features and avoids the object confusion problem, thus handles the multi-concept customization task better. Note that these previous methods are all tuning-based, while our CompAgent is tuning-free. Therefore, CompAgent can accurately address the customization task more efficiently.
    </p>
   </div>
   <figure class="ltx_figure" id="A4.F18">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="A4.F18.g1" src="/html/2401.15688/assets/x19.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 18.
     </span>
     <span class="ltx_text ltx_font_bold" id="A4.F18.2.1">
      Visualized results of our method for local image editing.
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A4.F19">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="A4.F19.g1" src="/html/2401.15688/assets/x20.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 19.
     </span>
     <span class="ltx_text ltx_font_bold" id="A4.F19.2.1">
      Visualized results of our method for object placement.
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.2.
    </span>
    Local Image Editing
   </h3>
   <div class="ltx_para" id="A4.SS2.p1">
    <p class="ltx_p" id="A4.SS2.p1.1">
     We then conduct the local image editing experiments and compare with the Paint-by-Example method
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib46" title="">
       2023a
      </a>
      )
     </cite>
     . It can be seen that although Paint-by-Example can perform the local image editing task, it cannot precisely catch the object attributes. For example, for the car example, the front window color from the Paint-by-Example generated image turns to blue. For the computer-desk example, Paint-by-Example does not edit the color of the table, and for the mirror-sink example, Paint-by-Example also does not modify the mirror shape from oval to rectangle. In comparison, our CompAgent can well guarantee the object attributes, thus better addressing the local image editing task. Besides, in the cup-coaster example, the shape of the cup from Paint-by-Example looks weird. In comparison, the generated image from our CompAgent looks more realistic.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.3.
    </span>
    Object Placement
   </h3>
   <div class="ltx_para" id="A4.SS3.p1">
    <p class="ltx_p" id="A4.SS3.p1.1">
     Finally we conduct the object placement task: to put a given object into the specified position in an image. The comparison results with Paint-by-Example are illustrated in Fig.
     <a class="ltx_ref" href="#A4.F19" title="Figure 19 ‣ D.1. Multi-Concept Customization ‣ Appendix D Extension to other tasks ‣ Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation">
      <span class="ltx_text ltx_ref_tag">
       19
      </span>
     </a>
     . Our method performs equally better. For example, for the vase example, Paint-by-Example only generates the flowers and does not understand the object type. For the bear-cup example, Paint-by-Example does not capture the cup color. Besides, Paint-by-Example also does not generate natural images for the dog example. Our CompAgent performs better for all these examples, which further demonstrates its ability.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     (1)
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Betker et al
     <span class="ltx_text" id="bib.bib2.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al
     <span class="ltx_text" id="bib.bib2.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Improving image generation with better captions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">
      Computer Science. https://cdn. openai.com/papers/dall-e-3.pdf
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bochkovskiy et al
     <span class="ltx_text" id="bib.bib3.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. 2020.
    </span>
    <span class="ltx_bibblock">
     Yolov4: Optimal speed and accuracy of object detection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">
      arXiv preprint arXiv:2004.10934
     </em>
     (2020).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brooks et al
     <span class="ltx_text" id="bib.bib4.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023.
    </span>
    <span class="ltx_bibblock">
     Instructpix2pix: Learning to follow image editing instructions. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chang et al
     <span class="ltx_text" id="bib.bib5.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al
     <span class="ltx_text" id="bib.bib5.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Muse: Text-to-image generation via masked generative transformers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">
      arXiv preprint arXiv:2301.00704
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chefer et al
     <span class="ltx_text" id="bib.bib6.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023.
    </span>
    <span class="ltx_bibblock">
     Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">
      ACM Transactions on Graphics
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al
     <span class="ltx_text" id="bib.bib7.3.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al
     <span class="ltx_text" id="bib.bib7.4.1">
      .
     </span>
     2023b.
    </span>
    <span class="ltx_bibblock">
     PixArt-
     <math alttext="\alpha" class="ltx_Math" display="inline" id="bib.bib7.1.m1.1">
      <semantics id="bib.bib7.1.m1.1a">
       <mi id="bib.bib7.1.m1.1.1" xref="bib.bib7.1.m1.1.1.cmml">
        α
       </mi>
       <annotation-xml encoding="MathML-Content" id="bib.bib7.1.m1.1b">
        <ci id="bib.bib7.1.m1.1.1.cmml" xref="bib.bib7.1.m1.1.1">
         𝛼
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="bib.bib7.1.m1.1c">
        \alpha
       </annotation>
      </semantics>
     </math>
     : Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.5.1">
      arXiv preprint arXiv:2310.00426
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al
     <span class="ltx_text" id="bib.bib8.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     Minghao Chen, Iro Laina, and Andrea Vedaldi. 2024.
    </span>
    <span class="ltx_bibblock">
     Training-free layout control with cross-attention guidance. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">
      Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al
     <span class="ltx_text" id="bib.bib9.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. 2023a.
    </span>
    <span class="ltx_bibblock">
     Anydoor: Zero-shot object-level image customization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">
      arXiv preprint arXiv:2307.09481
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dhariwal and Nichol (2021)
    </span>
    <span class="ltx_bibblock">
     Prafulla Dhariwal and Alexander Nichol. 2021.
    </span>
    <span class="ltx_bibblock">
     Diffusion models beat gans on image synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Advances in neural information processing systems
     </em>
     (2021).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     (11)
    </span>
    <span class="ltx_bibblock">
     Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al
     <span class="ltx_text" id="bib.bib11.2.1">
      .
     </span>
     2022.
    </span>
    <span class="ltx_bibblock">
     Human-level play in the game of Diplomacy by combining language models with strategic reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">
      Science
     </em>
     (2022).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al
     <span class="ltx_text" id="bib.bib12.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">
      International Conference on Learning Representations
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hertz et al
     <span class="ltx_text" id="bib.bib13.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. 2023.
    </span>
    <span class="ltx_bibblock">
     Prompt-to-Prompt Image Editing with Cross-Attention Control. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">
      International Conference on Learning Representations
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ho et al
     <span class="ltx_text" id="bib.bib14.2.2.1">
      .
     </span>
     (2020)
    </span>
    <span class="ltx_bibblock">
     Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.
    </span>
    <span class="ltx_bibblock">
     Denoising diffusion probabilistic models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">
      Advances in neural information processing systems
     </em>
     (2020).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al
     <span class="ltx_text" id="bib.bib15.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">
      Advances in Neural Information Processing Systems
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kirillov et al
     <span class="ltx_text" id="bib.bib16.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al
     <span class="ltx_text" id="bib.bib16.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Segment anything. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kumari et al
     <span class="ltx_text" id="bib.bib17.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023.
    </span>
    <span class="ltx_bibblock">
     Multi-concept customization of text-to-image diffusion. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib18.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Dongxu Li, Junnan Li, and Steven CH Hoi. 2023a.
    </span>
    <span class="ltx_bibblock">
     Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">
      arXiv preprint arXiv:2305.14720
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib19.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">
      arXiv preprint arXiv:2301.12597
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib20.2.2.1">
      .
     </span>
     (2023c)
    </span>
    <span class="ltx_bibblock">
     Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023c.
    </span>
    <span class="ltx_bibblock">
     Gligen: Open-set grounded text-to-image generation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib21.2.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. 2021.
    </span>
    <span class="ltx_bibblock">
     Image synthesis from layout with locality-aware mask adaption. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lian et al
     <span class="ltx_text" id="bib.bib22.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2023.
    </span>
    <span class="ltx_bibblock">
     LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">
      arXiv preprint arXiv:2305.13655
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lin et al
     <span class="ltx_text" id="bib.bib23.2.2.1">
      .
     </span>
     (2014)
    </span>
    <span class="ltx_bibblock">
     Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.
    </span>
    <span class="ltx_bibblock">
     Microsoft coco: Common objects in context. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">
      European conference on computer vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib24.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">
      Advances in Neural Information Processing Systems
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib25.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022.
    </span>
    <span class="ltx_bibblock">
     Compositional visual generation with composable diffusion models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">
      European Conference on Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib26.2.2.1">
      .
     </span>
     (2023c)
    </span>
    <span class="ltx_bibblock">
     Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al
     <span class="ltx_text" id="bib.bib26.3.1">
      .
     </span>
     2023c.
    </span>
    <span class="ltx_bibblock">
     Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.4.1">
      arXiv preprint arXiv:2303.05499
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib27.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al
     <span class="ltx_text" id="bib.bib27.3.1">
      .
     </span>
     2023a.
    </span>
    <span class="ltx_bibblock">
     Internchat: Solving vision-centric tasks by interacting with chatbots beyond language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.4.1">
      arXiv preprint arXiv:2305.05662
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib28.2.2.1">
      .
     </span>
     (2023d)
    </span>
    <span class="ltx_bibblock">
     Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. 2023d.
    </span>
    <span class="ltx_bibblock">
     Cones 2: Customizable Image Synthesis with Multiple Subjects.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">
      arXiv preprint arXiv:2305.19327
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     GPT-4 Technical Report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2303.08774
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Parmar et al
     <span class="ltx_text" id="bib.bib30.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. 2023.
    </span>
    <span class="ltx_bibblock">
     Zero-shot image-to-image translation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">
      ACM SIGGRAPH 2023 Conference Proceedings
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Patel et al
     <span class="ltx_text" id="bib.bib31.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. 2023.
    </span>
    <span class="ltx_bibblock">
     ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">
      arXiv preprint arXiv:2312.04655
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Podell et al
     <span class="ltx_text" id="bib.bib32.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023.
    </span>
    <span class="ltx_bibblock">
     Sdxl: Improving latent diffusion models for high-resolution image synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">
      arXiv preprint arXiv:2307.01952
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al
     <span class="ltx_text" id="bib.bib33.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023.
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">
      arXiv preprint arXiv:2307.07924
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ramesh et al
     <span class="ltx_text" id="bib.bib34.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.
    </span>
    <span class="ltx_bibblock">
     Hierarchical text-conditional image generation with clip latents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">
      arXiv preprint arXiv:2204.06125
     </em>
     (2022).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rombach et al
     <span class="ltx_text" id="bib.bib35.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.
    </span>
    <span class="ltx_bibblock">
     High-resolution image synthesis with latent diffusion models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ruiz et al
     <span class="ltx_text" id="bib.bib36.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023.
    </span>
    <span class="ltx_bibblock">
     Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Saharia et al
     <span class="ltx_text" id="bib.bib37.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al
     <span class="ltx_text" id="bib.bib37.3.1">
      .
     </span>
     2022.
    </span>
    <span class="ltx_bibblock">
     Photorealistic text-to-image diffusion models with deep language understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.4.1">
      Advances in Neural Information Processing Systems
     </em>
     (2022).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al
     <span class="ltx_text" id="bib.bib38.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">
      Advances in Neural Information Processing Systems
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun and Wu (2021)
    </span>
    <span class="ltx_bibblock">
     Wei Sun and Tianfu Wu. 2021.
    </span>
    <span class="ltx_bibblock">
     Learning layout and style reconfigurable gans for controllable image synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      IEEE transactions on pattern analysis and machine intelligence
     </em>
     (2021).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al
     <span class="ltx_text" id="bib.bib40.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al
     <span class="ltx_text" id="bib.bib40.3.1">
      .
     </span>
     2023a.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.4.1">
      arXiv preprint arXiv:2302.13971
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al
     <span class="ltx_text" id="bib.bib41.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al
     <span class="ltx_text" id="bib.bib41.3.1">
      .
     </span>
     2023b.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.4.1">
      arXiv preprint arXiv:2307.09288
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Voynov et al
     <span class="ltx_text" id="bib.bib42.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2023.
    </span>
    <span class="ltx_bibblock">
     Sketch-guided text-to-image diffusion models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">
      ACM SIGGRAPH 2023 Conference Proceedings
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al
     <span class="ltx_text" id="bib.bib43.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. 2023a.
    </span>
    <span class="ltx_bibblock">
     Compositional text-to-image synthesis with attention map control of diffusion models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">
      arXiv preprint arXiv:2305.13921
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al
     <span class="ltx_text" id="bib.bib44.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. 2023b.
    </span>
    <span class="ltx_bibblock">
     TokenCompose: Grounding Diffusion with Token-level Supervision.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">
      arXiv preprint arXiv:2312.03626
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al
     <span class="ltx_text" id="bib.bib45.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. 2023.
    </span>
    <span class="ltx_bibblock">
     Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib46.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. 2023a.
    </span>
    <span class="ltx_bibblock">
     Paint by example: Exemplar-based image editing with diffusion models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib47.2.2.1">
      .
     </span>
     (2023d)
    </span>
    <span class="ltx_bibblock">
     Hui Yang, Sifu Yue, and Yunzhong He. 2023d.
    </span>
    <span class="ltx_bibblock">
     Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">
      arXiv preprint arXiv:2306.02224
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib48.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023b.
    </span>
    <span class="ltx_bibblock">
     The dawn of lmms: Preliminary explorations with gpt-4v (ision).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">
      arXiv preprint arXiv:2309.17421
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib49.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Zuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. 2022.
    </span>
    <span class="ltx_bibblock">
     Modeling image composition for complex scene generation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib50.2.2.1">
      .
     </span>
     (2023c)
    </span>
    <span class="ltx_bibblock">
     Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023c.
    </span>
    <span class="ltx_bibblock">
     AppAgent: Multimodal Agents as Smartphone Users.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">
      arXiv preprint arXiv:2312.13771
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al
     <span class="ltx_text" id="bib.bib51.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023.
    </span>
    <span class="ltx_bibblock">
     Adding conditional control to text-to-image diffusion models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al
     <span class="ltx_text" id="bib.bib52.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">
      arXiv preprint arXiv:2304.10592
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
</article>
