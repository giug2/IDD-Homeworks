<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.00909] ViRED: Prediction of Visual Relations in Engineering Drawings</title><meta property="og:description" content="To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings.
Existing document understanding methods predominantly focu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ViRED: Prediction of Visual Relations in Engineering Drawings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ViRED: Prediction of Visual Relations in Engineering Drawings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.00909">

<!--Generated on Sun Oct  6 01:49:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Document understanding,  Visual relation prediction,  Engineering drawing
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ViRED: Prediction of Visual Relations in Engineering Drawings
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Chao Gu 1,
Ke Lin 2,
Yiyang Luo 3,
Jiahui Hou 1,
and Xiang-Yang Li 1






</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0004-6321-0475" title="ORCID identifier" class="ltx_ref">0009-0004-6321-0475</a></span>

<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0002-5376-7881" title="ORCID identifier" class="ltx_ref">0009-0002-5376-7881</a></span>

<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0008-8094-180X" title="ORCID identifier" class="ltx_ref">0009-0008-8094-180X</a></span>

<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-3340-8585" title="ORCID identifier" class="ltx_ref">0000-0002-3340-8585</a></span>

<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-6070-6625" title="ORCID identifier" class="ltx_ref">0000-0002-6070-6625</a></span>

<span class="ltx_contact ltx_role_affiliation">1School of Computer Science and Technology and LINKE Lab, University of Science and Technology of China, Hefei, China
</span>
<span class="ltx_contact ltx_role_affiliation">2School of Software, Tsinghua University, Beijing, China
</span>
<span class="ltx_contact ltx_role_affiliation">3School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore
</span>
<span class="ltx_contact ltx_role_affiliation">guch8017@mail.ustc.edu.cn,{leonard.keilin,lawrence.luoyy}@gmail.com,{jhhou,xiangyangli}@ustc.edu.cn
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings.
Existing document understanding methods predominantly focus on text as the main modality, which is not suitable for documents containing substantial image information.
In the field of visual relation detection, the structure of the task inherently limits its capacity to assess relationships among all entity pairs in the drawings.</p>
<p id="id2.id2" class="ltx_p">To address this issue, we propose a vision-based relation detection model, named ViRED, to identify the associations between tables and circuits in electrical engineering drawings.
Our model mainly consists of three parts: a vision encoder, an object encoder, and a relation decoder.</p>
<p id="id3.id3" class="ltx_p">We implement ViRED using PyTorch to evaluate its performance. To validate the efficacy of ViRED, we conduct a series of experiments.
The experimental results indicate that, within the engineering drawing dataset, our approach attained an accuracy of 96% in the task of relation prediction, marking a substantial improvement over existing methodologies. The results also show that ViRED can inference at a fast speed even when there are numerous objects in a single engineering drawing.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Document understanding, Visual relation prediction, Engineering drawing

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Digitization of engineering design drawings constitutes a crucial component of contemporary industrial processes.
Nevertheless, the automated recognition of these engineering drawings in image format still encounters considerable challenges.
Electrical engineering drawings, a subset of engineering drawings, are primarily used to depict equipment related to electrical systems.
Typically, electrical design engineers are required to review and recreate numerous electrical engineering drawings to transform technical illustrations into production-level drawings.
To alleviate this workload, it is crucial to develop automated recognition methods for electrical engineering drawings, which are stored in image format.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In general, a single electrical engineering drawing includes multiple circuits and tables as depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Each circuit describes different electrical devices, while each table presents the parameter settings for the corresponding circuits.
To enhance the identification of circuits and tables, it is necessary to extract these components from diagrams and analyze them using specialized models. While there is a considerable amount of existing work on detecting and classifying specific objects within images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the correspondence between these objects is often lost in this process.
Most current methods for extracting relations within documents depend on the relations between text pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, rendering them insufficient for addressing relations between diagrams and tables.
In visual relation detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the task and model design limit their capability to predict relationships across all circuit-table pairs, thereby causing missed detections.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.00909/assets/img/demo2_rm.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="327" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of electrical engineering drawing. There are 6 circuits and 6 tables in this electrical engineering drawing.
The engineering drawing has been modified to prevent copyright infringements.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The circuit-to-table relations in electrical engineering drawings are complex, potentially exhibiting one-to-one, one-to-many, and other variations.
Furthermore, the quantity of circuits and tables in each image differs, presenting difficulties in managing variable-length inputs using non-sequential modeling methods.
To address these issues, we propose ViRED, a <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Vi</span>sual <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">R</span>elation prediction model for <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">E</span>ngineering <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">D</span>rawings based on Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
ViRED consists of a visual encoder, an object encoder, and a relationship decoder.
The model is trained and fine-tuned utilizing the PubLayNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> dataset along with the proprietary electrical engineering drawing dataset, resulting in exceptional performance in the task of relation prediction.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The main contributions of this work are outlined as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present a novel vision-based relation detection approach, named ViRED, to address the issue of predicting relations for non-textual components in complex documents. This approach has been specifically implemented for the purpose of circuit-to-table relation matching in electrical design drawings.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop a dataset of electrical engineering drawings derived from industrial design data, and we annotate the instances and their relationships within the dataset.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We evaluate our method using various metrics on the electrical engineering drawing dataset. Furthermore, we perform comparative analyses with existing approaches and provide a performance comparison between the existing methods and our proposed technique.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We perform extensive ablation studies to compare the impact of different model architectures, hyperparameters, and training methods on the overall performance. Moreover, we refined our model architecture based on these comprehensive and comparative analysis.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The following sections of this paper are structured as follows. Section <a href="#S2" title="II Related Works ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reviews the related work pertinent to this study. Section <a href="#S3" title="III Methodology ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> elaborates on the proposed methods and models in detail. Section <a href="#S4" title="IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> discusses the experimental evaluation results of the proposed approach. Lastly, the article ends with a summary of our work in Section <a href="#S5" title="V Conclusion ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Visual Document Understanding</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The task of visual document understanding (VDU) focuses on understanding digital documents in image formats. There are several downstream tasks associated with VDU, including key information extraction, relation detection, document layout analysis, and others.
Most contemporary VDU techniques rely on deep neural networks that utilize visual, textual, or a mixture of visual and textual modalities.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Approaches for document understanding that utilize computer vision were initially proposed following advancements in convolutional neural networks (CNN). Hao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed the use of CNN for detecting tables in document images.
With the introduction of the R-CNN series model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the methodologies for detecting tables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and analyzing layouts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in document images have benefited from these models, resulting in enhanced performance.
Modern Optical Character Recognition (OCR) engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> have demonstrated significant efficacy in extracting text from document images. Furthermore, the advancement of language models has contributed to the tasks of document understanding. However, recent approaches in VDU incorporate other modalities such as textual information as auxiliary information.
LayoutLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> utilizes textual data in conjunction with the bounding box provided by the OCR engine for inference. DocFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, LayoutLMv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and LayoutLMv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> utilize early fusion techniques to better integrate and leverage visual and textual modality information.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.00909/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the general pipeline of ViRED.
(a) Engineering drawings are processed through the Vision Encoder, Object Encoder, Relation Decoder, and Relation Prediction Model.
(b) The Object Encoder converts the instance masks and types into mask and type embeddings, which are then aggregated to form the object tokens.
(c) The Relation Decoder utilizes the object tokens as inputs and integrates them with the image features from the Vision Encoder through a cross-attention mechanism.
Residual connections between layers are ignored for simplicity.
(d) While pretraining, the model encodes the document images and position masks, and after decoding through the relation decoder, it predicts the image classification of the position where the mask is located.
</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Visual Relation Detection</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Visual relation detection (VRD) involves the task of predicting the relations or interactions between pairs of objects within a single image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Typically, VDR is a mid-level vision task that extracts information from low-level vision tasks such as object detection and recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Despite the advancement of VDR techniques, the task remains difficult because of conflicts arising from the various potential relations and the lack of labeled data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> introduced the first VDR approach utilizing deep learning techniques. This method employed R-CNN for object and predicate detection and incorporated language priors to enhance the accuracy of predicted relations.
Subsequently, ViP-CNN was introduced by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to identify subject, predicate, and object simultaneously. This method incorporates a Phrase-Guided Message Passing Structure to investigate the interconnections of relation components.
Zhuang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed a context-aware interaction classification framework based on an attention mechanism, which is accurate, scalable, and enjoys good generalization ability to recognize unseen context-interaction combinations.
Though these methods succeeded in extracting relations in general images, few studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> have investigated the use of VRD techniques in domain-specific images, such as structured documents, mechanical blueprints, and engineering drawings.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Problem Definition</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Given an image of an electrical engineering drawing, let there be <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑁</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N_{c}</annotation></semantics></math> circuits and <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N_{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑁</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N_{t}</annotation></semantics></math> tables present within the image. There may be a relation between a circuit and a table.
That is, there may exist at most <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="N_{c}\times N_{t}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">N</mi><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><msub id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">N</mi><mi id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝑁</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">𝑐</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">𝑁</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">N_{c}\times N_{t}</annotation></semantics></math> relations in the engineering drawing.
Assuming the presence of bounding boxes and their associated instance type labels, we aim to determine if there exists a relation between specific circuits and tables.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Model Architecture</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">ViRED has three main components: a pretrained vision encoder, a lightweight object encoder, and a fast relation decoder.
Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Visual Document Understanding ‣ II Related Works ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the overview of our model pipeline.
We will describe these components in the following sections.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Vision encoder</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.7" class="ltx_p">The vision encoder in the model processes an image <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="I_{doc}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><msub id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">I</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">𝐼</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">𝑑</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">I_{doc}</annotation></semantics></math> of size <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="3\times H_{\textrm{Image}}\times W_{\textrm{Image}}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">H</mi><mtext id="S3.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3a.cmml">Image</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.2.m2.1.1.1a" xref="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.2.m2.1.1.4" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.4.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.2.cmml">W</mi><mtext id="S3.SS2.SSS1.p1.2.m2.1.1.4.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.3a.cmml">Image</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><times id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">3</cn><apply id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2">𝐻</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.3a.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3">Image</mtext></ci></apply><apply id="S3.SS2.SSS1.p1.2.m2.1.1.4.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.4.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.4.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.4.3a.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.3"><mtext mathsize="70%" id="S3.SS2.SSS1.p1.2.m2.1.1.4.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.4.3">Image</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">3\times H_{\textrm{Image}}\times W_{\textrm{Image}}</annotation></semantics></math> and produces an image embedding in either feature vector format <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="F_{\textrm{vision}}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msub id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">F</mi><mtext id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3a.cmml">vision</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">𝐹</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">vision</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">F_{\textrm{vision}}</annotation></semantics></math> or feature map format with dimensions <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="C_{v}\times H_{v}\times W_{v}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mrow id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml">C</mi><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.4.m4.1.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">H</mi><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.4.m4.1.1.1a" xref="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.4.m4.1.1.4" xref="S3.SS2.SSS1.p1.4.m4.1.1.4.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.4.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.4.2.cmml">W</mi><mi id="S3.SS2.SSS1.p1.4.m4.1.1.4.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.4.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><times id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.1"></times><apply id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2">𝐶</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3">𝑣</ci></apply><apply id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2">𝐻</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3">𝑣</ci></apply><apply id="S3.SS2.SSS1.p1.4.m4.1.1.4.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.4.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.4">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.4.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.4.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.4.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.4.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">C_{v}\times H_{v}\times W_{v}</annotation></semantics></math>, where <math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mi id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">C</annotation></semantics></math>, <math id="S3.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mi id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><ci id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">H</annotation></semantics></math>, and <math id="S3.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m7.1a"><mi id="S3.SS2.SSS1.p1.7.m7.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m7.1b"><ci id="S3.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m7.1c">W</annotation></semantics></math> represent the channels, width, and height of the image and feature map. In cases where feature maps are produced, they are flattened and converted into feature vectors. To prevent bias, a pre-trained vision encoder called the Masked Autoencoder (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is utilized in this study. The Document Image Transformer (DiT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is employed as our vision encoder. The vision encoder is only used once for each image, regardless of the number of objects or relationships in the image.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="F_{\textrm{Vision}}=\textrm{Vision-Encoder}(I_{doc})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">F</mi><mtext id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3a.cmml">Vision</mtext></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mtext id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3a.cmml">Vision-Encoder</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">I</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.1.1.3.4.cmml">c</mi></mrow></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝐹</ci><ci id="S3.E1.m1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">Vision</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.3"><mtext id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">Vision-Encoder</mtext></ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝐼</ci><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑑</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.4">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">F_{\textrm{Vision}}=\textrm{Vision-Encoder}(I_{doc})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="F_{\textrm{Vision}}=W\cdot\textrm{Flatten}(\textrm{Vision-Encoder}(I_{doc}))" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">F</mi><mtext id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3a.cmml">Vision</mtext></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">⋅</mo><mtext id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3a.cmml">Flatten</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3a.cmml">Vision-Encoder</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4.cmml">c</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐹</ci><ci id="S3.E2.m1.1.1.3.3a.cmml" xref="S3.E2.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">Vision</mtext></ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1">⋅</ci><ci id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">𝑊</ci><ci id="S3.E2.m1.1.1.1.3.3a.cmml" xref="S3.E2.m1.1.1.1.3.3"><mtext id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3">Flatten</mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><mtext id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">Vision-Encoder</mtext></ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">𝐼</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2">𝑑</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">F_{\textrm{Vision}}=W\cdot\textrm{Flatten}(\textrm{Vision-Encoder}(I_{doc}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Object encoder</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The object encoder efficiently maps a bounding box and its typing information into a vectorial embedding.
We use a convolutional neural network (CNN) to encode the bounding boxes. The object’s bounding box is shown as a one-channel image <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="I_{obj}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><msub id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">I</mi><mrow id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.4.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝐼</ci><apply id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2">𝑜</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3">𝑏</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.4">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">I_{obj}</annotation></semantics></math> with the same dimensions as the engineering drawing image. Pixels inside the bounding box are shown as 1, while those outside are shown as 0. The bounding box image is then encoded using a three-layer CNN.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="F_{\textrm{mask}}=\textrm{CNN}(I_{obj})" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">F</mi><mtext id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3a.cmml">mask</mtext></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3a.cmml">CNN</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">I</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.4" xref="S3.E3.m1.1.1.1.1.1.1.3.4.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐹</ci><ci id="S3.E3.m1.1.1.3.3a.cmml" xref="S3.E3.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">mask</mtext></ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">CNN</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">𝐼</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑜</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">𝑏</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.4">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">F_{\textrm{mask}}=\textrm{CNN}(I_{obj})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">To inform the relation decoder about whether the embedding token represents a circuit or a table, we aggregate the vector embeddings of bounding boxes <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="F_{\textrm{mask}}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">F</mi><mtext id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">F_{\textrm{mask}}</annotation></semantics></math> with two learned embeddings <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="F_{\textrm{type}}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">F</mi><mtext id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3a.cmml">type</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">type</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">F_{\textrm{type}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="F_{\textrm{object}}=F_{\textrm{mask}}+F_{\textrm{type}}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">F</mi><mtext id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3a.cmml">object</mtext></msub><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml">F</mi><mtext id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3a.cmml">mask</mtext></msub><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">F</mi><mtext id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3a.cmml">type</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">𝐹</ci><ci id="S3.E4.m1.1.1.2.3a.cmml" xref="S3.E4.m1.1.1.2.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">object</mtext></ci></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2">𝐹</ci><ci id="S3.E4.m1.1.1.3.2.3a.cmml" xref="S3.E4.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3">mask</mtext></ci></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝐹</ci><ci id="S3.E4.m1.1.1.3.3.3a.cmml" xref="S3.E4.m1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">type</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">F_{\textrm{object}}=F_{\textrm{mask}}+F_{\textrm{type}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Relation decoder</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">The relation decoder takes the encoded masks and vectorized images as input and predicts if there is a relation between each circuit-table pair. In detail, the relation decoder consists of two components: the fusion model and the relation prediction model.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">The fusion model is employed for feature fusion between object masks and image features.
Inspired by DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a transformer-decoder-based model is adopted.
We modify the standard transformer decoder by eliminating the relative position embedding and causal mask because of the lack of order between objects. This transforms the decoder into a bidirectional transformer decoder.
As shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Visual Document Understanding ‣ II Related Works ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, each decoder layer consists of four parts.

<span id="S3.I1" class="ltx_inline-enumerate">
<span id="S3.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">1.</span> <span id="S3.I1.i1.1" class="ltx_text">The tokens are initially processed by a self-attention model as shown in Eq. <a href="#S3.E5" title="In III-B3 Relation decoder ‣ III-B Model Architecture ‣ III Methodology ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, which enables the mask tokens to interact with each other. This step enables the objects to determine their relative positions.
</span></span>
<span id="S3.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">2.</span> <span id="S3.I1.i2.1" class="ltx_text">Next, a cross-attention model is introduced, where the image embeddings are used as the key and value vector, and mask tokens are used as the query vector like Eq. <a href="#S3.E6" title="In III-B3 Relation decoder ‣ III-B Model Architecture ‣ III Methodology ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. By utilizing the image-to-object cross-attention model, the mask tokens are modified through a combination of bounding box features and vision features.
</span></span>
<span id="S3.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">3.</span> <span id="S3.I1.i3.1" class="ltx_text">Then, a feedforward layer updates all the mask tokens, and a dropout layer is used to improve the model’s generalizability.
</span></span>
<span id="S3.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">4.</span> <span id="S3.I1.i4.1" class="ltx_text">Finally, a residual connection is added to every attention layer and feed-forward layer, in accordance with the typical transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
</span></span>
</span></p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="F_{\textrm{object}}=\textrm{Attention}(F_{\textrm{object}},F_{\textrm{object}},F_{\textrm{object}})" display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml"><msub id="S3.E5.m1.3.3.5" xref="S3.E5.m1.3.3.5.cmml"><mi id="S3.E5.m1.3.3.5.2" xref="S3.E5.m1.3.3.5.2.cmml">F</mi><mtext id="S3.E5.m1.3.3.5.3" xref="S3.E5.m1.3.3.5.3a.cmml">object</mtext></msub><mo id="S3.E5.m1.3.3.4" xref="S3.E5.m1.3.3.4.cmml">=</mo><mrow id="S3.E5.m1.3.3.3" xref="S3.E5.m1.3.3.3.cmml"><mtext id="S3.E5.m1.3.3.3.5" xref="S3.E5.m1.3.3.3.5a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.3.4" xref="S3.E5.m1.3.3.3.4.cmml">​</mo><mrow id="S3.E5.m1.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.3.3.3.4" xref="S3.E5.m1.3.3.3.3.4.cmml">(</mo><msub id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">F</mi><mtext id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3a.cmml">object</mtext></msub><mo id="S3.E5.m1.3.3.3.3.3.5" xref="S3.E5.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E5.m1.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.2.cmml"><mi id="S3.E5.m1.2.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.2.2.cmml">F</mi><mtext id="S3.E5.m1.2.2.2.2.2.2.3" xref="S3.E5.m1.2.2.2.2.2.2.3a.cmml">object</mtext></msub><mo id="S3.E5.m1.3.3.3.3.3.6" xref="S3.E5.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E5.m1.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.cmml"><mi id="S3.E5.m1.3.3.3.3.3.3.2" xref="S3.E5.m1.3.3.3.3.3.3.2.cmml">F</mi><mtext id="S3.E5.m1.3.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.3a.cmml">object</mtext></msub><mo stretchy="false" id="S3.E5.m1.3.3.3.3.3.7" xref="S3.E5.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3"><eq id="S3.E5.m1.3.3.4.cmml" xref="S3.E5.m1.3.3.4"></eq><apply id="S3.E5.m1.3.3.5.cmml" xref="S3.E5.m1.3.3.5"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.5.1.cmml" xref="S3.E5.m1.3.3.5">subscript</csymbol><ci id="S3.E5.m1.3.3.5.2.cmml" xref="S3.E5.m1.3.3.5.2">𝐹</ci><ci id="S3.E5.m1.3.3.5.3a.cmml" xref="S3.E5.m1.3.3.5.3"><mtext mathsize="70%" id="S3.E5.m1.3.3.5.3.cmml" xref="S3.E5.m1.3.3.5.3">object</mtext></ci></apply><apply id="S3.E5.m1.3.3.3.cmml" xref="S3.E5.m1.3.3.3"><times id="S3.E5.m1.3.3.3.4.cmml" xref="S3.E5.m1.3.3.3.4"></times><ci id="S3.E5.m1.3.3.3.5a.cmml" xref="S3.E5.m1.3.3.3.5"><mtext id="S3.E5.m1.3.3.3.5.cmml" xref="S3.E5.m1.3.3.3.5">Attention</mtext></ci><vector id="S3.E5.m1.3.3.3.3.4.cmml" xref="S3.E5.m1.3.3.3.3.3"><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">𝐹</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">object</mtext></ci></apply><apply id="S3.E5.m1.2.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.2.2.2.2.1.cmml" xref="S3.E5.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E5.m1.2.2.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2.2">𝐹</ci><ci id="S3.E5.m1.2.2.2.2.2.2.3a.cmml" xref="S3.E5.m1.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S3.E5.m1.2.2.2.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.2.2.2.3">object</mtext></ci></apply><apply id="S3.E5.m1.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3.3.3.3.1.cmml" xref="S3.E5.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E5.m1.3.3.3.3.3.3.2.cmml" xref="S3.E5.m1.3.3.3.3.3.3.2">𝐹</ci><ci id="S3.E5.m1.3.3.3.3.3.3.3a.cmml" xref="S3.E5.m1.3.3.3.3.3.3.3"><mtext mathsize="70%" id="S3.E5.m1.3.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3.3">object</mtext></ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">F_{\textrm{object}}=\textrm{Attention}(F_{\textrm{object}},F_{\textrm{object}},F_{\textrm{object}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.3" class="ltx_Math" alttext="F_{\textrm{object}}=\textrm{Attention}(F_{\textrm{object}},F_{\textrm{vision}},F_{\textrm{vision}})" display="block"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml"><msub id="S3.E6.m1.3.3.5" xref="S3.E6.m1.3.3.5.cmml"><mi id="S3.E6.m1.3.3.5.2" xref="S3.E6.m1.3.3.5.2.cmml">F</mi><mtext id="S3.E6.m1.3.3.5.3" xref="S3.E6.m1.3.3.5.3a.cmml">object</mtext></msub><mo id="S3.E6.m1.3.3.4" xref="S3.E6.m1.3.3.4.cmml">=</mo><mrow id="S3.E6.m1.3.3.3" xref="S3.E6.m1.3.3.3.cmml"><mtext id="S3.E6.m1.3.3.3.5" xref="S3.E6.m1.3.3.3.5a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.3.4" xref="S3.E6.m1.3.3.3.4.cmml">​</mo><mrow id="S3.E6.m1.3.3.3.3.3" xref="S3.E6.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E6.m1.3.3.3.3.3.4" xref="S3.E6.m1.3.3.3.3.4.cmml">(</mo><msub id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml">F</mi><mtext id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3a.cmml">object</mtext></msub><mo id="S3.E6.m1.3.3.3.3.3.5" xref="S3.E6.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E6.m1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.2.cmml">F</mi><mtext id="S3.E6.m1.2.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.2.2.3a.cmml">vision</mtext></msub><mo id="S3.E6.m1.3.3.3.3.3.6" xref="S3.E6.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E6.m1.3.3.3.3.3.3" xref="S3.E6.m1.3.3.3.3.3.3.cmml"><mi id="S3.E6.m1.3.3.3.3.3.3.2" xref="S3.E6.m1.3.3.3.3.3.3.2.cmml">F</mi><mtext id="S3.E6.m1.3.3.3.3.3.3.3" xref="S3.E6.m1.3.3.3.3.3.3.3a.cmml">vision</mtext></msub><mo stretchy="false" id="S3.E6.m1.3.3.3.3.3.7" xref="S3.E6.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3"><eq id="S3.E6.m1.3.3.4.cmml" xref="S3.E6.m1.3.3.4"></eq><apply id="S3.E6.m1.3.3.5.cmml" xref="S3.E6.m1.3.3.5"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.5.1.cmml" xref="S3.E6.m1.3.3.5">subscript</csymbol><ci id="S3.E6.m1.3.3.5.2.cmml" xref="S3.E6.m1.3.3.5.2">𝐹</ci><ci id="S3.E6.m1.3.3.5.3a.cmml" xref="S3.E6.m1.3.3.5.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.5.3.cmml" xref="S3.E6.m1.3.3.5.3">object</mtext></ci></apply><apply id="S3.E6.m1.3.3.3.cmml" xref="S3.E6.m1.3.3.3"><times id="S3.E6.m1.3.3.3.4.cmml" xref="S3.E6.m1.3.3.3.4"></times><ci id="S3.E6.m1.3.3.3.5a.cmml" xref="S3.E6.m1.3.3.3.5"><mtext id="S3.E6.m1.3.3.3.5.cmml" xref="S3.E6.m1.3.3.3.5">Attention</mtext></ci><vector id="S3.E6.m1.3.3.3.3.4.cmml" xref="S3.E6.m1.3.3.3.3.3"><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2">𝐹</ci><ci id="S3.E6.m1.1.1.1.1.1.1.3a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">object</mtext></ci></apply><apply id="S3.E6.m1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2">𝐹</ci><ci id="S3.E6.m1.2.2.2.2.2.2.3a.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S3.E6.m1.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3">vision</mtext></ci></apply><apply id="S3.E6.m1.3.3.3.3.3.3.cmml" xref="S3.E6.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.3.3.3.3.1.cmml" xref="S3.E6.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E6.m1.3.3.3.3.3.3.2.cmml" xref="S3.E6.m1.3.3.3.3.3.3.2">𝐹</ci><ci id="S3.E6.m1.3.3.3.3.3.3.3a.cmml" xref="S3.E6.m1.3.3.3.3.3.3.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.3.3.3.3.3.cmml" xref="S3.E6.m1.3.3.3.3.3.3.3">vision</mtext></ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">F_{\textrm{object}}=\textrm{Attention}(F_{\textrm{object}},F_{\textrm{vision}},F_{\textrm{vision}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<p id="S3.SS2.SSS3.p4.1" class="ltx_p">After receiving the object tokens from the fusion model of the relation decoder, it is essential to identify the relationships between them using the relation prediction model. The combined object tokens are concatenated to generate <math id="S3.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="(N_{c}+N_{t})^{2}" display="inline"><semantics id="S3.SS2.SSS3.p4.1.m1.1a"><msup id="S3.SS2.SSS3.p4.1.m1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS3.p4.1.m1.1.1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.2" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.2.cmml">N</mi><mi id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.2.cmml">N</mi><mi id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.3" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.SS2.SSS3.p4.1.m1.1.1.3" xref="S3.SS2.SSS3.p4.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.1.m1.1b"><apply id="S3.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1"><plus id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.1"></plus><apply id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.2">𝑁</ci><ci id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.2">𝑁</ci><ci id="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><cn type="integer" id="S3.SS2.SSS3.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.1.m1.1c">(N_{c}+N_{t})^{2}</annotation></semantics></math> combination tokens.
This process is depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Pretraining ‣ III Methodology ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
To streamline the training and inference procedures, redundant and unfeasible combinations are eliminated. The relation prediction model consists of a three-layer perceptron with ReLU activation and a linear projection layer. This model is utilized to convert hidden features into a two-dimensional logit output. Afterwards, the main objective of the relation prediction model is to determine whether a relationship exists between two objects.</p>
</div>
<div id="S3.SS2.SSS3.p5" class="ltx_para">
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.2" class="ltx_Math" alttext="F_{\textrm{relation}_{i,j}}=F_{\textrm{object}_{i}}\oplus F_{\textrm{object}_{j}}" display="block"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.3" xref="S3.E7.m1.2.3.cmml"><msub id="S3.E7.m1.2.3.2" xref="S3.E7.m1.2.3.2.cmml"><mi id="S3.E7.m1.2.3.2.2" xref="S3.E7.m1.2.3.2.2.cmml">F</mi><msub id="S3.E7.m1.2.2.2" xref="S3.E7.m1.2.2.2.cmml"><mtext id="S3.E7.m1.2.2.2.4" xref="S3.E7.m1.2.2.2.4a.cmml">relation</mtext><mrow id="S3.E7.m1.2.2.2.2.2.4" xref="S3.E7.m1.2.2.2.2.2.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E7.m1.2.2.2.2.2.4.1" xref="S3.E7.m1.2.2.2.2.2.3.cmml">,</mo><mi id="S3.E7.m1.2.2.2.2.2.2" xref="S3.E7.m1.2.2.2.2.2.2.cmml">j</mi></mrow></msub></msub><mo id="S3.E7.m1.2.3.1" xref="S3.E7.m1.2.3.1.cmml">=</mo><mrow id="S3.E7.m1.2.3.3" xref="S3.E7.m1.2.3.3.cmml"><msub id="S3.E7.m1.2.3.3.2" xref="S3.E7.m1.2.3.3.2.cmml"><mi id="S3.E7.m1.2.3.3.2.2" xref="S3.E7.m1.2.3.3.2.2.cmml">F</mi><msub id="S3.E7.m1.2.3.3.2.3" xref="S3.E7.m1.2.3.3.2.3.cmml"><mtext id="S3.E7.m1.2.3.3.2.3.2" xref="S3.E7.m1.2.3.3.2.3.2a.cmml">object</mtext><mi id="S3.E7.m1.2.3.3.2.3.3" xref="S3.E7.m1.2.3.3.2.3.3.cmml">i</mi></msub></msub><mo id="S3.E7.m1.2.3.3.1" xref="S3.E7.m1.2.3.3.1.cmml">⊕</mo><msub id="S3.E7.m1.2.3.3.3" xref="S3.E7.m1.2.3.3.3.cmml"><mi id="S3.E7.m1.2.3.3.3.2" xref="S3.E7.m1.2.3.3.3.2.cmml">F</mi><msub id="S3.E7.m1.2.3.3.3.3" xref="S3.E7.m1.2.3.3.3.3.cmml"><mtext id="S3.E7.m1.2.3.3.3.3.2" xref="S3.E7.m1.2.3.3.3.3.2a.cmml">object</mtext><mi id="S3.E7.m1.2.3.3.3.3.3" xref="S3.E7.m1.2.3.3.3.3.3.cmml">j</mi></msub></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.3.cmml" xref="S3.E7.m1.2.3"><eq id="S3.E7.m1.2.3.1.cmml" xref="S3.E7.m1.2.3.1"></eq><apply id="S3.E7.m1.2.3.2.cmml" xref="S3.E7.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.3.2.1.cmml" xref="S3.E7.m1.2.3.2">subscript</csymbol><ci id="S3.E7.m1.2.3.2.2.cmml" xref="S3.E7.m1.2.3.2.2">𝐹</ci><apply id="S3.E7.m1.2.2.2.cmml" xref="S3.E7.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.2.3.cmml" xref="S3.E7.m1.2.2.2">subscript</csymbol><ci id="S3.E7.m1.2.2.2.4a.cmml" xref="S3.E7.m1.2.2.2.4"><mtext mathsize="70%" id="S3.E7.m1.2.2.2.4.cmml" xref="S3.E7.m1.2.2.2.4">relation</mtext></ci><list id="S3.E7.m1.2.2.2.2.2.3.cmml" xref="S3.E7.m1.2.2.2.2.2.4"><ci id="S3.E7.m1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1">𝑖</ci><ci id="S3.E7.m1.2.2.2.2.2.2.cmml" xref="S3.E7.m1.2.2.2.2.2.2">𝑗</ci></list></apply></apply><apply id="S3.E7.m1.2.3.3.cmml" xref="S3.E7.m1.2.3.3"><csymbol cd="latexml" id="S3.E7.m1.2.3.3.1.cmml" xref="S3.E7.m1.2.3.3.1">direct-sum</csymbol><apply id="S3.E7.m1.2.3.3.2.cmml" xref="S3.E7.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.3.3.2.1.cmml" xref="S3.E7.m1.2.3.3.2">subscript</csymbol><ci id="S3.E7.m1.2.3.3.2.2.cmml" xref="S3.E7.m1.2.3.3.2.2">𝐹</ci><apply id="S3.E7.m1.2.3.3.2.3.cmml" xref="S3.E7.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.2.3.3.2.3.1.cmml" xref="S3.E7.m1.2.3.3.2.3">subscript</csymbol><ci id="S3.E7.m1.2.3.3.2.3.2a.cmml" xref="S3.E7.m1.2.3.3.2.3.2"><mtext mathsize="70%" id="S3.E7.m1.2.3.3.2.3.2.cmml" xref="S3.E7.m1.2.3.3.2.3.2">object</mtext></ci><ci id="S3.E7.m1.2.3.3.2.3.3.cmml" xref="S3.E7.m1.2.3.3.2.3.3">𝑖</ci></apply></apply><apply id="S3.E7.m1.2.3.3.3.cmml" xref="S3.E7.m1.2.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.2.3.3.3.1.cmml" xref="S3.E7.m1.2.3.3.3">subscript</csymbol><ci id="S3.E7.m1.2.3.3.3.2.cmml" xref="S3.E7.m1.2.3.3.3.2">𝐹</ci><apply id="S3.E7.m1.2.3.3.3.3.cmml" xref="S3.E7.m1.2.3.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.2.3.3.3.3.1.cmml" xref="S3.E7.m1.2.3.3.3.3">subscript</csymbol><ci id="S3.E7.m1.2.3.3.3.3.2a.cmml" xref="S3.E7.m1.2.3.3.3.3.2"><mtext mathsize="70%" id="S3.E7.m1.2.3.3.3.3.2.cmml" xref="S3.E7.m1.2.3.3.3.3.2">object</mtext></ci><ci id="S3.E7.m1.2.3.3.3.3.3.cmml" xref="S3.E7.m1.2.3.3.3.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">F_{\textrm{relation}_{i,j}}=F_{\textrm{object}_{i}}\oplus F_{\textrm{object}_{j}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.1" class="ltx_Math" alttext="\textrm{Relation-Prediction}=\textrm{MLP}(F_{\textrm{relation}})" display="block"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><mtext id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3a.cmml">Relation-Prediction</mtext><mo id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml"><mtext id="S3.E8.m1.1.1.1.3" xref="S3.E8.m1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.2" xref="S3.E8.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">F</mi><mtext id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3a.cmml">relation</mtext></msub><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"></eq><ci id="S3.E8.m1.1.1.3a.cmml" xref="S3.E8.m1.1.1.3"><mtext id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3">Relation-Prediction</mtext></ci><apply id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><times id="S3.E8.m1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.2"></times><ci id="S3.E8.m1.1.1.1.3a.cmml" xref="S3.E8.m1.1.1.1.3"><mtext id="S3.E8.m1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.3">MLP</mtext></ci><apply id="S3.E8.m1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">𝐹</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3a.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E8.m1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3">relation</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\textrm{Relation-Prediction}=\textrm{MLP}(F_{\textrm{relation}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Pretraining</span>
</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.00909/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="282" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The implementation of relation prediction model. The semi-transparent tokens represent the filtered parts, which do not participate in the relationship prediction computation.
</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/0_source.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="269" height="125" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>A.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/0_predict.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="269" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>A.2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/1_source.png" id="S3.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="269" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>B.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/1_predict.png" id="S3.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="269" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>B.2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/2_source.png" id="S3.F4.sf5.g1" class="ltx_graphics ltx_img_landscape" width="269" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>C.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/2_predict.png" id="S3.F4.sf6.g1" class="ltx_graphics ltx_img_landscape" width="269" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>C.2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/3_source.png" id="S3.F4.sf7.g1" class="ltx_graphics ltx_img_landscape" width="269" height="76" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>D.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.00909/assets/img/vis/3_predict.png" id="S3.F4.sf8.g1" class="ltx_graphics ltx_img_landscape" width="269" height="76" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>D.2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative result of our model. Caption X.1 and X.2 denotes the original electrical engineering drawings and the model prediction results, respectively.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Due to the scarcity of annotated training data for relation detection in engineering drawings, we utilize the PubLayNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> dataset during the pretraining phase.
PubLayNet offers a dataset consisting of 340,000 document images, including 3 million bounding boxes and type annotations for the instances within these images.
To enhance the model’s comprehension of positional data, we devise a pretraining task that employs our model to categorize the instance type of the masked region as shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Visual Document Understanding ‣ II Related Works ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-(d).
The architecture of our pretraining model exhibits minor variations compared to the relation prediction model.
Since it is necessary to predict the category of the provided bounding boxes (i.e., table or circuit), the type embeddings integrated into the vector embeddings of bounding boxes within the object encoder are excluded.
The relation prediction model is substituted with a multi-layer perceptron to perform classification tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
Comparison with visual-base relation prediction methods on our validation dataset.
</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:381.1pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(31.8pt,-9.0pt) scale(1.2,1.2) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Accuracy<math id="S4.T1.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.m1.1b"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1">(</mo><mo id="S4.T1.1.1.1.1.m1.1.2">%</mo><mo id="S4.T1.1.1.1.1.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T1.1.1.1.1.m1.1.4">↑</mo><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.1.1.2" class="ltx_tr">
<td id="S4.T1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">RVL-BERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Transformer</td>
<td id="S4.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">46.7</td>
</tr>
<tr id="S4.T1.1.1.3" class="ltx_tr">
<td id="S4.T1.1.1.3.1" class="ltx_td ltx_align_center">VTransE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S4.T1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">TransE + CNN</td>
<td id="S4.T1.1.1.3.3" class="ltx_td ltx_align_center">87.8</td>
</tr>
<tr id="S4.T1.1.1.4" class="ltx_tr">
<td id="S4.T1.1.1.4.1" class="ltx_td ltx_align_center">MBBR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">Transformer</td>
<td id="S4.T1.1.1.4.3" class="ltx_td ltx_align_center">86.8</td>
</tr>
<tr id="S4.T1.1.1.5" class="ltx_tr">
<td id="S4.T1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb">ViRED (<span id="S4.T1.1.1.5.1.1" class="ltx_text ltx_font_bold">Ours</span>)</td>
<td id="S4.T1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Transformer</td>
<td id="S4.T1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.5.3.1" class="ltx_text ltx_font_bold">96.3</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experiment Setup</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Dataset</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In the pretraining phase, our method is trained and tested on the PubLayNet datasets with the same train and test splits as the original dataset.
For the finetuning phase, we used a self-annotated dataset of engineering design diagrams containing 283 images, 4,566 entities, and 2,112 relations between entities.
The datasets are randomly divided, with 90% assigned for training and 10% for evaluation.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Implementation Details</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.3" class="ltx_p">The dimension <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><ci id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">D</annotation></semantics></math> of latent representation throughout the model pipeline is set to 768.
For the vision encoder backbone, we adopt the DINOv2-B model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
The model consists of 12 layers of Transformer encoders, each containing a 12-head multi-head attention block. The input image is cropped into patches of size 14, which are then processed and encoded to a 768-dimensional feature vector.
We finetuned the backbone network using the same MAE training method as DiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> based on its state-of-the-art performance on visual-based document downstream tasks.
The bounding box encoder is a three-layer convolution network with a ReLU activation function and a linear projection layer that maps the feature map to a 768-dimensional vectorized feature.
The relation decoder is a 2-layer Transformer Decoder for multi-modal feature fusion.
The input image size for the vision encoder is <math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="518\times 518" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">518</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.2.m2.1.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml">518</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><times id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">518</cn><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">518</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">518\times 518</annotation></semantics></math>, while the object encoder takes <math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mrow id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mn id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.3.m3.1.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><times id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">224</cn><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">224\times 224</annotation></semantics></math> masks as input. Despite the difference in input sizes, we maintain the relative positions between the images and the masks to help the model better understand the positional relations between them.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">We implement our model in PyTorch.
For the pretraining phase, our model is trained with a batch size of 64 for a total of 100 epochs on the PubLayNet dataset using <math id="S4.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mo id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">\sim</annotation></semantics></math>90 NVIDIA A800 GPU hours.
During the finetuning phase, we initialized the type embeddings to zero and the multi-layer perceptron of the relation extractor with uniformly random numbers. Meanwhile, other parameters remained unchanged from the pretraining checkpoint.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">We applied data augmentation to the data while training, including horizontal and vertical flipping with a probability of 0.2, <math id="S4.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="D_{4}" display="inline"><semantics id="S4.SS1.SSS2.p3.1.m1.1a"><msub id="S4.SS1.SSS2.p3.1.m1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p3.1.m1.1.1.2" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS2.p3.1.m1.1.1.3" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.1.m1.1b"><apply id="S4.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.1.m1.1c">D_{4}</annotation></semantics></math> dihedral group transformations, and fixed-size random cropping. Unlike common random cropping, we ensured that instances in the images were not lost due to cropping.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.5" class="ltx_p">For optimization, we use AdamW with hyper-parameters <math id="S4.SS1.SSS2.p4.1.m1.4" class="ltx_Math" alttext="\beta=(0.9,0.999),\epsilon=10^{-8}" display="inline"><semantics id="S4.SS1.SSS2.p4.1.m1.4a"><mrow id="S4.SS1.SSS2.p4.1.m1.4.4.2" xref="S4.SS1.SSS2.p4.1.m1.4.4.3.cmml"><mrow id="S4.SS1.SSS2.p4.1.m1.3.3.1.1" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.cmml"><mi id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.2" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.2.cmml">β</mi><mo id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.1" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.2" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.2.1" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.1.cmml">(</mo><mn id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml">0.9</mn><mo id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.2.2" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.1.cmml">,</mo><mn id="S4.SS1.SSS2.p4.1.m1.2.2" xref="S4.SS1.SSS2.p4.1.m1.2.2.cmml">0.999</mn><mo stretchy="false" id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.2.3" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.SSS2.p4.1.m1.4.4.2.3" xref="S4.SS1.SSS2.p4.1.m1.4.4.3a.cmml">,</mo><mrow id="S4.SS1.SSS2.p4.1.m1.4.4.2.2" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.cmml"><mi id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.2" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.2.cmml">ϵ</mi><mo id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.1" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.1.cmml">=</mo><msup id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.cmml"><mn id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.2" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.cmml"><mo id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3a" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.cmml">−</mo><mn id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.2" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.2.cmml">8</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.4b"><apply id="S4.SS1.SSS2.p4.1.m1.4.4.3.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.1.m1.4.4.3a.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1"><eq id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.1"></eq><ci id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.2">𝛽</ci><interval closure="open" id="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.3.3.1.1.3.2"><cn type="float" id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1">0.9</cn><cn type="float" id="S4.SS1.SSS2.p4.1.m1.2.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.2.2">0.999</cn></interval></apply><apply id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2"><eq id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.1"></eq><ci id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.2">italic-ϵ</ci><apply id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.2">10</cn><apply id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3"><minus id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3"></minus><cn type="integer" id="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.4.4.2.2.3.3.2">8</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.4c">\beta=(0.9,0.999),\epsilon=10^{-8}</annotation></semantics></math> and weight decay of <math id="S4.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S4.SS1.SSS2.p4.2.m2.1a"><msup id="S4.SS1.SSS2.p4.2.m2.1.1" xref="S4.SS1.SSS2.p4.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p4.2.m2.1.1.2" xref="S4.SS1.SSS2.p4.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p4.2.m2.1.1.3" xref="S4.SS1.SSS2.p4.2.m2.1.1.3.cmml"><mo id="S4.SS1.SSS2.p4.2.m2.1.1.3a" xref="S4.SS1.SSS2.p4.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS2.p4.2.m2.1.1.3.2" xref="S4.SS1.SSS2.p4.2.m2.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.2.m2.1b"><apply id="S4.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p4.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.2">10</cn><apply id="S4.SS1.SSS2.p4.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.3"><minus id="S4.SS1.SSS2.p4.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS1.SSS2.p4.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.2.m2.1c">10^{-3}</annotation></semantics></math>. The base learning rate for the optimizer is <math id="S4.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS1.SSS2.p4.3.m3.1a"><msup id="S4.SS1.SSS2.p4.3.m3.1.1" xref="S4.SS1.SSS2.p4.3.m3.1.1.cmml"><mn id="S4.SS1.SSS2.p4.3.m3.1.1.2" xref="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p4.3.m3.1.1.3" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml"><mo id="S4.SS1.SSS2.p4.3.m3.1.1.3a" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS2.p4.3.m3.1.1.3.2" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.3.m3.1b"><apply id="S4.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.2">10</cn><apply id="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.3"><minus id="S4.SS1.SSS2.p4.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS1.SSS2.p4.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.3.m3.1c">10^{-4}</annotation></semantics></math>, we employ a linear learning rate schedule from <math id="S4.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS1.SSS2.p4.4.m4.1a"><msup id="S4.SS1.SSS2.p4.4.m4.1.1" xref="S4.SS1.SSS2.p4.4.m4.1.1.cmml"><mn id="S4.SS1.SSS2.p4.4.m4.1.1.2" xref="S4.SS1.SSS2.p4.4.m4.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p4.4.m4.1.1.3" xref="S4.SS1.SSS2.p4.4.m4.1.1.3.cmml"><mo id="S4.SS1.SSS2.p4.4.m4.1.1.3a" xref="S4.SS1.SSS2.p4.4.m4.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS2.p4.4.m4.1.1.3.2" xref="S4.SS1.SSS2.p4.4.m4.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.4.m4.1b"><apply id="S4.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p4.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.2">10</cn><apply id="S4.SS1.SSS2.p4.4.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.3"><minus id="S4.SS1.SSS2.p4.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.3"></minus><cn type="integer" id="S4.SS1.SSS2.p4.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS2.p4.4.m4.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.4.m4.1c">10^{-4}</annotation></semantics></math> to <math id="S4.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS1.SSS2.p4.5.m5.1a"><msup id="S4.SS1.SSS2.p4.5.m5.1.1" xref="S4.SS1.SSS2.p4.5.m5.1.1.cmml"><mn id="S4.SS1.SSS2.p4.5.m5.1.1.2" xref="S4.SS1.SSS2.p4.5.m5.1.1.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p4.5.m5.1.1.3" xref="S4.SS1.SSS2.p4.5.m5.1.1.3.cmml"><mo id="S4.SS1.SSS2.p4.5.m5.1.1.3a" xref="S4.SS1.SSS2.p4.5.m5.1.1.3.cmml">−</mo><mn id="S4.SS1.SSS2.p4.5.m5.1.1.3.2" xref="S4.SS1.SSS2.p4.5.m5.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.5.m5.1b"><apply id="S4.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.5.m5.1.1.1.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p4.5.m5.1.1.2.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.2">10</cn><apply id="S4.SS1.SSS2.p4.5.m5.1.1.3.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.3"><minus id="S4.SS1.SSS2.p4.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.3"></minus><cn type="integer" id="S4.SS1.SSS2.p4.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS2.p4.5.m5.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.5.m5.1c">10^{-5}</annotation></semantics></math> with 20% epoches for warming up.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Metrics</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Unlike the commonly used R@x metric in relation detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we use mAP, precision, and recall to assess model performance in relation detection tasks. Standard relation detection tasks may not annotate all possible relations in an image. In our task, relations between tables and circuits are clear, so using mAP is a more accurate indicator of model performance. Accuracy is used as the metric for comparison experiments.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Experiment Results</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Qualitative Result</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Qualitative results are shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C Pretraining ‣ III Methodology ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The blue and green areas represent the bounding box regions of circuits and tables, respectively. In the presence of a relationship between a table and a circuit, a line segment will link the two entities.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Comparison</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In this section, we evaluate our model against current methods for relation detection. We focus on visual relation detection instead of text-based methods for document relation extraction. To align with the visual task, we modify the dataset by categorizing textual descriptions as either ”table” or ”circuit.” Relations are established between all circuit-table pairs, with existing relations labeled as ”describe” and non-existing relations labeled as ”not describe.”</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Table <a href="#S4.T1" title="TABLE I ‣ IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> provides a comparative analysis of our relation prediction method in contrast to several established approaches, with the accuracy metric used to measure the precision of the outcomes. Our findings demonstrate that our method surpasses the results of our engineering drawing relation prediction dataset. Importantly, we employ the evaluation methods used by the compared approaches to maintain consistency with our metrics. For MBBR, the accuracy is calculated based on the top-100 relation predictions.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>
Ablation study of model architecture. The precision and recall metrics report data only for the ”has relation” category, while mAP provides the overall average value.
</figcaption>
<div id="S4.T2.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:618.2pt;height:211.8pt;vertical-align:-1.2pt;"><span class="ltx_transformed_inner" style="transform:translate(44.9pt,-15.3pt) scale(1.17,1.17) ;">
<table id="S4.T2.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.3.3.4.1" class="ltx_text ltx_font_bold">Vision Encoder</span></td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.3.3.5.1" class="ltx_text ltx_font_bold">Object Encoder</span></td>
<td id="S4.T2.3.3.3.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.3.3.6.1" class="ltx_text ltx_font_bold">Image Size</span></td>
<td id="S4.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.3.3.7.1" class="ltx_text ltx_font_bold">#Rel. Decoder Layer</span></td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">mAP<math id="S4.T2.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1b"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1">(</mo><mo id="S4.T2.1.1.1.1.m1.1.2">%</mo><mo id="S4.T2.1.1.1.1.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.1.1.1.1.m1.1.4">↑</mo><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">Precision<math id="S4.T2.2.2.2.2.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.2.m1.1b"><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.1">(</mo><mo id="S4.T2.2.2.2.2.m1.1.2">%</mo><mo id="S4.T2.2.2.2.2.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.2.2.2.2.m1.1.4">↑</mo><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">Recall<math id="S4.T2.3.3.3.3.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.3.m1.1b"><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.1">(</mo><mo id="S4.T2.3.3.3.3.m1.1.2">%</mo><mo id="S4.T2.3.3.3.3.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.3.3.3.3.m1.1.4">↑</mo><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">ViT-B/14</td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3-Layer CNN</td>
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.4.4.4.1.m1.1a"><msup id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml"><mn id="S4.T2.4.4.4.1.m1.1.1.2" xref="S4.T2.4.4.4.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.4.4.4.1.m1.1.1.3" xref="S4.T2.4.4.4.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><apply id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.4.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.4.4.4.1.m1.1.1.2.cmml" xref="S4.T2.4.4.4.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.4.4.4.1.m1.1.1.3.cmml" xref="S4.T2.4.4.4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S4.T2.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">89.00</td>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.4.4.6.1" class="ltx_text ltx_font_bold">88.41</span></td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t">76.39</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<td id="S4.T2.5.5.5.2" class="ltx_td ltx_align_center">ViT-S/14</td>
<td id="S4.T2.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r">3-Layer CNN</td>
<td id="S4.T2.5.5.5.1" class="ltx_td ltx_align_center"><math id="S4.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.5.5.5.1.m1.1a"><msup id="S4.T2.5.5.5.1.m1.1.1" xref="S4.T2.5.5.5.1.m1.1.1.cmml"><mn id="S4.T2.5.5.5.1.m1.1.1.2" xref="S4.T2.5.5.5.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.5.5.5.1.m1.1.1.3" xref="S4.T2.5.5.5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><apply id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.5.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.5.5.5.1.m1.1.1.2.cmml" xref="S4.T2.5.5.5.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.5.5.5.1.m1.1.1.3.cmml" xref="S4.T2.5.5.5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center">86.37</td>
<td id="S4.T2.5.5.5.6" class="ltx_td ltx_align_center">85.83</td>
<td id="S4.T2.5.5.5.7" class="ltx_td ltx_align_center">59.27</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<td id="S4.T2.6.6.6.2" class="ltx_td ltx_align_center">ViT-L/14</td>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r">3-Layer CNN</td>
<td id="S4.T2.6.6.6.1" class="ltx_td ltx_align_center"><math id="S4.T2.6.6.6.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.6.6.6.1.m1.1a"><msup id="S4.T2.6.6.6.1.m1.1.1" xref="S4.T2.6.6.6.1.m1.1.1.cmml"><mn id="S4.T2.6.6.6.1.m1.1.1.2" xref="S4.T2.6.6.6.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.6.6.6.1.m1.1.1.3" xref="S4.T2.6.6.6.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.1.m1.1b"><apply id="S4.T2.6.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.6.6.6.1.m1.1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.6.6.6.1.m1.1.1.2.cmml" xref="S4.T2.6.6.6.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.6.6.6.1.m1.1.1.3.cmml" xref="S4.T2.6.6.6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S4.T2.6.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.6.6.5.1" class="ltx_text ltx_font_bold">89.80</span></td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center">82.79</td>
<td id="S4.T2.6.6.6.7" class="ltx_td ltx_align_center">72.66</td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<td id="S4.T2.7.7.7.2" class="ltx_td ltx_align_center">ResNet-50</td>
<td id="S4.T2.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r">3-Layer CNN</td>
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center"><math id="S4.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.7.7.7.1.m1.1a"><msup id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml"><mn id="S4.T2.7.7.7.1.m1.1.1.2" xref="S4.T2.7.7.7.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.7.7.7.1.m1.1.1.3" xref="S4.T2.7.7.7.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><apply id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.7.7.7.1.m1.1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.7.7.7.1.m1.1.1.2.cmml" xref="S4.T2.7.7.7.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.7.7.7.1.m1.1.1.3.cmml" xref="S4.T2.7.7.7.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S4.T2.7.7.7.5" class="ltx_td ltx_align_center">84.33</td>
<td id="S4.T2.7.7.7.6" class="ltx_td ltx_align_center">75.94</td>
<td id="S4.T2.7.7.7.7" class="ltx_td ltx_align_center">68.47</td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<td id="S4.T2.8.8.8.2" class="ltx_td ltx_align_center ltx_border_t">ViT-B/14</td>
<td id="S4.T2.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3-Layer CNN</td>
<td id="S4.T2.8.8.8.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.8.8.8.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.8.8.8.1.m1.1a"><msup id="S4.T2.8.8.8.1.m1.1.1" xref="S4.T2.8.8.8.1.m1.1.1.cmml"><mn id="S4.T2.8.8.8.1.m1.1.1.2" xref="S4.T2.8.8.8.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.8.8.8.1.m1.1.1.3" xref="S4.T2.8.8.8.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.m1.1b"><apply id="S4.T2.8.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.8.8.8.1.m1.1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.8.8.8.1.m1.1.1.2.cmml" xref="S4.T2.8.8.8.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.8.8.8.1.m1.1.1.3.cmml" xref="S4.T2.8.8.8.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S4.T2.8.8.8.5" class="ltx_td ltx_align_center ltx_border_t">84.77</td>
<td id="S4.T2.8.8.8.6" class="ltx_td ltx_align_center ltx_border_t">81.20</td>
<td id="S4.T2.8.8.8.7" class="ltx_td ltx_align_center ltx_border_t">75.05</td>
</tr>
<tr id="S4.T2.9.9.9" class="ltx_tr">
<td id="S4.T2.9.9.9.2" class="ltx_td ltx_align_center">ViT-B/14</td>
<td id="S4.T2.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">3-Layer CNN</td>
<td id="S4.T2.9.9.9.1" class="ltx_td ltx_align_center"><math id="S4.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.9.9.9.1.m1.1a"><msup id="S4.T2.9.9.9.1.m1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.cmml"><mn id="S4.T2.9.9.9.1.m1.1.1.2" xref="S4.T2.9.9.9.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.9.9.9.1.m1.1.1.3" xref="S4.T2.9.9.9.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.1b"><apply id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.9.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.9.9.9.1.m1.1.1.2.cmml" xref="S4.T2.9.9.9.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.9.9.9.1.m1.1.1.3.cmml" xref="S4.T2.9.9.9.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r">3</td>
<td id="S4.T2.9.9.9.5" class="ltx_td ltx_align_center">88.91</td>
<td id="S4.T2.9.9.9.6" class="ltx_td ltx_align_center">85.37</td>
<td id="S4.T2.9.9.9.7" class="ltx_td ltx_align_center">79.56</td>
</tr>
<tr id="S4.T2.10.10.10" class="ltx_tr">
<td id="S4.T2.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t">ViT-B/14</td>
<td id="S4.T2.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6-Layer ViT</td>
<td id="S4.T2.10.10.10.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.10.10.10.1.m1.1" class="ltx_Math" alttext="518^{2}" display="inline"><semantics id="S4.T2.10.10.10.1.m1.1a"><msup id="S4.T2.10.10.10.1.m1.1.1" xref="S4.T2.10.10.10.1.m1.1.1.cmml"><mn id="S4.T2.10.10.10.1.m1.1.1.2" xref="S4.T2.10.10.10.1.m1.1.1.2.cmml">518</mn><mn id="S4.T2.10.10.10.1.m1.1.1.3" xref="S4.T2.10.10.10.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.1.m1.1b"><apply id="S4.T2.10.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.10.10.10.1.m1.1.1.1.cmml" xref="S4.T2.10.10.10.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.10.10.10.1.m1.1.1.2.cmml" xref="S4.T2.10.10.10.1.m1.1.1.2">518</cn><cn type="integer" id="S4.T2.10.10.10.1.m1.1.1.3.cmml" xref="S4.T2.10.10.10.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.1.m1.1c">518^{2}</annotation></semantics></math></td>
<td id="S4.T2.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S4.T2.10.10.10.5" class="ltx_td ltx_align_center ltx_border_t">59.57</td>
<td id="S4.T2.10.10.10.6" class="ltx_td ltx_align_center ltx_border_t">50.00</td>
<td id="S4.T2.10.10.10.7" class="ltx_td ltx_align_center ltx_border_t">14.07</td>
</tr>
<tr id="S4.T2.11.11.11" class="ltx_tr">
<td id="S4.T2.11.11.11.2" class="ltx_td ltx_align_center ltx_border_t">ViT-B/14</td>
<td id="S4.T2.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3-Layer CNN</td>
<td id="S4.T2.11.11.11.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.11.11.11.1.m1.1" class="ltx_Math" alttext="294^{2}" display="inline"><semantics id="S4.T2.11.11.11.1.m1.1a"><msup id="S4.T2.11.11.11.1.m1.1.1" xref="S4.T2.11.11.11.1.m1.1.1.cmml"><mn id="S4.T2.11.11.11.1.m1.1.1.2" xref="S4.T2.11.11.11.1.m1.1.1.2.cmml">294</mn><mn id="S4.T2.11.11.11.1.m1.1.1.3" xref="S4.T2.11.11.11.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.m1.1b"><apply id="S4.T2.11.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.11.11.11.1.m1.1.1.1.cmml" xref="S4.T2.11.11.11.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.11.11.11.1.m1.1.1.2.cmml" xref="S4.T2.11.11.11.1.m1.1.1.2">294</cn><cn type="integer" id="S4.T2.11.11.11.1.m1.1.1.3.cmml" xref="S4.T2.11.11.11.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.m1.1c">294^{2}</annotation></semantics></math></td>
<td id="S4.T2.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S4.T2.11.11.11.5" class="ltx_td ltx_align_center ltx_border_t">88.16</td>
<td id="S4.T2.11.11.11.6" class="ltx_td ltx_align_center ltx_border_t">85.03</td>
<td id="S4.T2.11.11.11.7" class="ltx_td ltx_align_center ltx_border_t">79.18</td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr">
<td id="S4.T2.12.12.12.2" class="ltx_td ltx_align_center ltx_border_bb">ViT-B/14</td>
<td id="S4.T2.12.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">3-Layer CNN</td>
<td id="S4.T2.12.12.12.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T2.12.12.12.1.m1.1" class="ltx_Math" alttext="756^{2}" display="inline"><semantics id="S4.T2.12.12.12.1.m1.1a"><msup id="S4.T2.12.12.12.1.m1.1.1" xref="S4.T2.12.12.12.1.m1.1.1.cmml"><mn id="S4.T2.12.12.12.1.m1.1.1.2" xref="S4.T2.12.12.12.1.m1.1.1.2.cmml">756</mn><mn id="S4.T2.12.12.12.1.m1.1.1.3" xref="S4.T2.12.12.12.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.1.m1.1b"><apply id="S4.T2.12.12.12.1.m1.1.1.cmml" xref="S4.T2.12.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.12.12.12.1.m1.1.1.1.cmml" xref="S4.T2.12.12.12.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.12.12.12.1.m1.1.1.2.cmml" xref="S4.T2.12.12.12.1.m1.1.1.2">756</cn><cn type="integer" id="S4.T2.12.12.12.1.m1.1.1.3.cmml" xref="S4.T2.12.12.12.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.1.m1.1c">756^{2}</annotation></semantics></math></td>
<td id="S4.T2.12.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">2</td>
<td id="S4.T2.12.12.12.5" class="ltx_td ltx_align_center ltx_border_bb">87.52</td>
<td id="S4.T2.12.12.12.6" class="ltx_td ltx_align_center ltx_border_bb">86.55</td>
<td id="S4.T2.12.12.12.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.12.7.1" class="ltx_text ltx_font_bold">80.91</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.4.1.1" class="ltx_text">IV-C</span>3 </span>Ablation study</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In this section, we evaluate our method in different settings and strategies.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">We conduct an additive ablation study on the mAP, precision, and recall for relation prediction on the validation set, as illustrated in Tab. <a href="#S4.T3" title="TABLE III ‣ IV-C3 Ablation study ‣ IV-C Experiment Results ‣ IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
In the baseline model, we initialize the model with random parameters without pretraining and omit the type embedding from the object encoder. Introducing type embedding allows the model to better differentiate the categories of object tokens. When computing self-attention over object tokens, the relation decoder can simultaneously consider the categories of the objects, thereby reducing computations for improbable relations and enhancing the understanding of relations. Due to the insufficient data in our task’s dataset, we introduced a pretraining mechanism for both the vision encoder and the position encoder-decoder.
By undergoing unsupervised training on a large-scale document image dataset, the vision encoder more effectively extracts visual features from electrical engineering drawings. Supervised pretraining was utilized to predict the classification of selected regions based on given positional information, thereby enhancing the model’s understanding of positional inputs. As shown in Table <a href="#S4.T3" title="TABLE III ‣ IV-C3 Ablation study ‣ IV-C Experiment Results ‣ IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the inclusion of type embedding and model pretraining significantly improved the performance across all evaluation metrics.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>
Additive ablation study of sequentially applying different training techniques for relation prediction on valid split of electrical engineering dataset
</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S4.T3.3.3.4.1" class="ltx_text ltx_font_bold">Ablate</span></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">mAP<math id="S4.T3.1.1.1.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1b"><mo stretchy="false" id="S4.T3.1.1.1.m1.1.1">(</mo><mo id="S4.T3.1.1.1.m1.1.2">%</mo><mo id="S4.T3.1.1.1.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.1.1.1.m1.1.4">↑</mo><mo stretchy="false" id="S4.T3.1.1.1.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">Precision<math id="S4.T3.2.2.2.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.m1.1b"><mo stretchy="false" id="S4.T3.2.2.2.m1.1.1">(</mo><mo id="S4.T3.2.2.2.m1.1.2">%</mo><mo id="S4.T3.2.2.2.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.2.2.2.m1.1.4">↑</mo><mo stretchy="false" id="S4.T3.2.2.2.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">Recall<math id="S4.T3.3.3.3.m1.1" class="ltx_math_unparsed" alttext="(\%,\uparrow)" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mrow id="S4.T3.3.3.3.m1.1b"><mo stretchy="false" id="S4.T3.3.3.3.m1.1.1">(</mo><mo id="S4.T3.3.3.3.m1.1.2">%</mo><mo id="S4.T3.3.3.3.m1.1.3">,</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.3.3.3.m1.1.4">↑</mo><mo stretchy="false" id="S4.T3.3.3.3.m1.1.5">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">(\%,\uparrow)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.3.4" class="ltx_tr">
<td id="S4.T3.3.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Base model</td>
<td id="S4.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">88.26</td>
<td id="S4.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">81.52</td>
<td id="S4.T3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">75.44</td>
</tr>
<tr id="S4.T3.3.5" class="ltx_tr">
<td id="S4.T3.3.5.1" class="ltx_td ltx_align_left ltx_border_r">+ Type Embedding</td>
<td id="S4.T3.3.5.2" class="ltx_td ltx_align_center">90.48</td>
<td id="S4.T3.3.5.3" class="ltx_td ltx_align_center">81.15</td>
<td id="S4.T3.3.5.4" class="ltx_td ltx_align_center">76.27</td>
</tr>
<tr id="S4.T3.3.6" class="ltx_tr">
<td id="S4.T3.3.6.1" class="ltx_td ltx_align_left ltx_border_r">+ DiT-style Pretrain</td>
<td id="S4.T3.3.6.2" class="ltx_td ltx_align_center">88.93</td>
<td id="S4.T3.3.6.3" class="ltx_td ltx_align_center">85.69</td>
<td id="S4.T3.3.6.4" class="ltx_td ltx_align_center">79.16</td>
</tr>
<tr id="S4.T3.3.7" class="ltx_tr">
<td id="S4.T3.3.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">+ Position Pretrain</td>
<td id="S4.T3.3.7.2" class="ltx_td ltx_align_center ltx_border_bb">91.65</td>
<td id="S4.T3.3.7.3" class="ltx_td ltx_align_center ltx_border_bb">90.11</td>
<td id="S4.T3.3.7.4" class="ltx_td ltx_align_center ltx_border_bb">83.23</td>
</tr>
</table>
</figure>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">We also conducted ablation experiments on the model architecture. We compared the effects of various vision encoders, diverse object encoders, different input image resolutions, and different numbers of transformer decoder layers in the relation decoder on the performance.
The experimental results are shown in Tab. <a href="#S4.T2" title="TABLE II ‣ IV-C2 Comparison ‣ IV-C Experiment Results ‣ IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The hidden dimension of the model is determined by the output dimension of the vision encoder. For ViT-S, ViT-B, ViT-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, the dimensions for latent representation are set to 384, 768, 1024, and 768, respectively.
Upon analyzing the experimental results, it becomes evident that the object encoder architecture significantly influences model performance. The relation decoder struggles to interpret object tokens encoded by ViT, resulting in a notable decline in performance across various metrics.
Additionally, the vision encoder impacts the latent representation dimension of the model, thereby also playing a crucial role in determining model performance. The findings suggest that ViT-S performs poorly due to its constrained embedding dimension. For ViT-B and ViT-L, the limited task difficulty results in negligible performance differences.
Higher image resolutions improve the vision encoder’s capacity to capture more detailed image features. However, the vision encoder’s ability to encode all these details is restricted by the dimension of the image feature vector, leading to a minimal effect on performance.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS4.4.1.1" class="ltx_text">IV-C</span>4 </span>Inference Efficiency</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.3" class="ltx_p">To evaluate the inference efficiency of the model, we compute the FLOPs (Floating Point Operations) for the inference process. For uniformity, a batch size of 1 is employed for all models. The input image resolution is fixed at <math id="S4.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="518\times 518" display="inline"><semantics id="S4.SS3.SSS4.p1.1.m1.1a"><mrow id="S4.SS3.SSS4.p1.1.m1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS4.p1.1.m1.1.1.2" xref="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml">518</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS4.p1.1.m1.1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.SSS4.p1.1.m1.1.1.3" xref="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml">518</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.1.m1.1b"><apply id="S4.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1"><times id="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2">518</cn><cn type="integer" id="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.3">518</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.1.m1.1c">518\times 518</annotation></semantics></math>, with the number of objects to be predicted (<math id="S4.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.SSS4.p1.2.m2.1a"><mi id="S4.SS3.SSS4.p1.2.m2.1.1" xref="S4.SS3.SSS4.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.2.m2.1b"><ci id="S4.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.2.m2.1c">N</annotation></semantics></math>) ranging from 1 to 20. The relations to be predicted include all possible interactions between objects, amounting to <math id="S4.SS3.SSS4.p1.3.m3.1" class="ltx_Math" alttext="(N-1)^{2}" display="inline"><semantics id="S4.SS3.SSS4.p1.3.m3.1a"><msup id="S4.SS3.SSS4.p1.3.m3.1.1" xref="S4.SS3.SSS4.p1.3.m3.1.1.cmml"><mrow id="S4.SS3.SSS4.p1.3.m3.1.1.1.1" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.2" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.2" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.2.cmml">N</mi><mo id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.1" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.1.cmml">−</mo><mn id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.3" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.3" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.SS3.SSS4.p1.3.m3.1.1.3" xref="S4.SS3.SSS4.p1.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.3.m3.1b"><apply id="S4.SS3.SSS4.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1">superscript</csymbol><apply id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1"><minus id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.1"></minus><ci id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.2">𝑁</ci><cn type="integer" id="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1.1.1.1.3">1</cn></apply><cn type="integer" id="S4.SS3.SSS4.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS4.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.3.m3.1c">(N-1)^{2}</annotation></semantics></math>. The results are presented in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C4 Inference Efficiency ‣ IV-C Experiment Results ‣ IV Experiments ‣ ViRED: Prediction of Visual Relations in Engineering Drawings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
Owing to the utilization of a lightweight object encoder and relation decoder, the inference efficiency of ViRED is minimally impacted by the number of objects in the electrical engineering drawing. It sustains a rapid inference speed even when a single drawing contains numerous objects.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.00909/assets/x3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>FLOPs of different models with respect to different numbers of objects.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose a novel relation prediction method and present a dataset for relation detection in electrical engineering drawings. We apply this relation prediction method to the task of relation detection within the dataset, achieving high performance on the validation set through the processes of pretraining and finetuning the model.
We conduct a series of experiments, including comparative analyses with existing methods and ablation studies on training strategies and model architectures. The experimental results indicate that our method achieves superior performance on this task.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 2117–2125.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. Jocher, “YOLOv5 by Ultralytics,” May 2020. [Online]. Available: https://github.com/ultralytics/yolov5

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2020, pp. 213–229.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou, “Layoutlm: Pre-training of text and layout for document image understanding,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, ser. KDD ’20.   New York, NY, USA: Association for Computing Machinery, 2020, p. 1192–1200. [Online]. Available: https://doi.org/10.1145/3394486.3403172

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Yu, N. Lu, X. Qi, P. Gong, and R. Xiao, “Pick: processing key information extraction from documents using improved graph learning-convolutional networks,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2020 25th International Conference on Pattern Recognition (ICPR)</em>.   IEEE, 2021, pp. 4363–4370.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Q. Peng, Y. Pan, W. Wang, B. Luo, Z. Zhang, Z. Huang, T. Hu, W. Yin, Y. Chen, Y. Zhang <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding,” <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.06155</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Liang, Y. Guo, H. Chang, and X. Chen, “Visual relationship detection with deep structural ranking,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 32, no. 1, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Abdelkarim, A. Agarwal, P. Achlioptas, J. Chen, J. Huang, B. Li, K. Church, and M. Elhoseiny, “Exploring long tail visual relationship recognition with large vocabulary,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 15 921–15 930.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L. Zhao, L. Yuan, B. Gong, Y. Cui, F. Schroff, M.-H. Yang, H. Adam, and T. Liu, “Unified visual relationship detection with vision and language models,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 6962–6973.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Zhong, J. Tang, and A. Jimeno Yepes, “Publaynet: Largest dataset ever for document layout analysis,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and Recognition (ICDAR)</em>, 2019, pp. 1015–1022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L. Hao, L. Gao, X. Yi, and Z. Tang, “A table detection method for pdf documents based on convolutional neural networks,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2016 12th IAPR Workshop on Document Analysis Systems (DAS)</em>, 2016, pp. 287–292.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2014.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, December 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds., vol. 28.   Curran Associates, Inc., 2015. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask r-cnn,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, Oct 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, “Deepdesrt: Deep learning for detection and structure recognition of tables in document images,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</em>, vol. 01, 2017, pp. 1162–1167.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Prasad, A. Gadpal, K. Kapadni, M. Visave, and K. Sultanpure, “Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, June 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Fernandes, M. Simsek, B. Kantarci, and S. Khan, “Tabledet: An end-to-end deep learning approach for table detection and table image classification in data sheet images,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 468, pp. 317–334, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0925231221015010

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Soto and S. Yoo, “Visual detection with context for document layout analysis,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds.   Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 3464–3470. [Online]. Available: https://aclanthology.org/D19-1348

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Smith, “An overview of the tesseract ocr engine,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Ninth international conference on document analysis and recognition (ICDAR 2007)</em>, vol. 2.   IEEE, 2007, pp. 629–633.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Appalaraju, B. Jasani, B. U. Kota, Y. Xie, and R. Manmatha, “Docformer: End-to-end transformer for document understanding,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, October 2021, pp. 993–1003.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Xu, Y. Xu, T. Lv, L. Cui, F. Wei, G. Wang, Y. Lu, D. Florencio, C. Zhang, W. Che, M. Zhang, and L. Zhou, “LayoutLMv2: Multi-modal pre-training for visually-rich document understanding,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, C. Zong, F. Xia, W. Li, and R. Navigli, Eds.   Online: Association for Computational Linguistics, Aug. 2021, pp. 2579–2591. [Online]. Available: https://aclanthology.org/2021.acl-long.201

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei, “Layoutlmv3: Pre-training for document ai with unified text and image masking,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 4083–4091.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Cheng, L. Wang, J. Wu, X. Hu, G. Jeon, D. Tao, and M. Zhou, “Visual relationship detection: A survey,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cybernetics</em>, vol. 52, no. 8, pp. 8453–8466, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, “Visual relationship detection with language priors,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</em>.   Springer, 2016, pp. 852–869.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Li, W. Ouyang, X. Wang, and X. Tang, “Vip-cnn: Visual phrase guided convolutional neural network,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 1347–1356.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
B. Zhuang, L. Liu, C. Shen, and I. Reid, “Towards context-aware interaction recognition for visual relationship detection,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 589–598.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Li, C. Yuhui, and L. Xiaoting, “Engineering drawing recognition model with convolutional neural network,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence</em>, 2019, pp. 112–116.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 000–16 009.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Li, Y. Xu, T. Lv, L. Cui, C. Zhang, and F. Wei, “Dit: Self-supervised pre-training for document image transformer,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 3530–3539.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M.-J. Chiou, R. Zimmermann, and J. Feng, “Visual relationship detection with visual-linguistic knowledge from multimodal representations,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 9, pp. 50 441–50 451, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua, “Visual translation embedding network for visual relation detection,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 5532–5540.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Z. Anastasakis, D. Mallis, M. Diomataris, G. Alexandridis, S. Kollias, and V. Pitsikalis, “Self-supervised learning for visual relationship detection through masked bounding box reconstruction,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2024, pp. 1206–1215.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Dinov2: Learning robust visual features without supervision,” <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.00908" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.00909" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.00909">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.00909" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.00910" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:49:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
