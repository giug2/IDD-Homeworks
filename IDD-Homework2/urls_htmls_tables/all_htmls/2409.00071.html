<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation</title>
<!--Generated on Fri Aug 23 23:42:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Data augmentation,  generative adversarial networks,  low-resource languages,  natural language processing,  neural machine translation
" lang="en" name="keywords"/>
<base href="/html/2409.00071v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S1" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S2" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S2.SS1" title="In II Related Work ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Preliminaries on NMT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S2.SS2" title="In II Related Work ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Data Augmentation for Low-Resource NMT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S2.SS3" title="In II Related Work ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Preliminaries on GANs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S2.SS4" title="In II Related Work ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">GANs in NLP</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Model Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.SS1" title="In III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Overall Workflow</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.SS2" title="In III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Underlying Architectures</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S4" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Data</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S4.SS1" title="In IV Data ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Simulated Low-Resource Setting</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S4.SS2" title="In IV Data ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Training Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S4.SS3" title="In IV Data ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Test Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S4.SS4" title="In IV Data ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Preprocessing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS1" title="In V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Encoder-Decoder Performance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS2" title="In V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">GAN Performance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS3" title="In V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Error Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS3.SSS1" title="In V-C Error Analysis ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>Repeated Words</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS3.SSS2" title="In V-C Error Analysis ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>2 </span>Nonsensical Grammar</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS3.SSS3" title="In V-C Error Analysis ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>3 </span>Unrelated Words</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S6" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S7" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Limitations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A1" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Data Characteristics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A2" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A3" title="In Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Training</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generative-Adversarial Networks for Low-Resource Language Data
Augmentation in Machine Translation
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linda Zeng

 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">The Harker School
<br class="ltx_break"/></span>San Jose, United States of America 
<br class="ltx_break"/>26lindaz@students.harker.org
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Neural Machine Translation (NMT) systems struggle when translating to and from low-resource languages, which lack large-scale data corpora for models to use for training. As manual data curation is expensive and time-consuming, we propose utilizing a generative-adversarial network (GAN) to augment low-resource language data. When training on a very small amount of language data (under 20,000 sentences) in a simulated low-resource setting, our model shows potential at data augmentation, generating monolingual language data with sentences such as “ask me that healthy lunch im cooking up,” and “my grandfather work harder than your grandfather before.” Our novel data augmentation approach takes the first step in investigating the capability of GANs in low-resource NMT, and our results suggest that there is promise for future extension of GANs to low-resource NMT.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Data augmentation, generative adversarial networks, low-resource languages, natural language processing, neural machine translation

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Although technology has become a staple of daily life, society’s best-performing computing systems still fail to reflect the world’s diversity in languages. Current translation models frequently err when translating to and from “low-resource languages” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib1" title="">1</a>]</cite>, which are languages that do not have much digital data that the machine learning algorithms underlying the software can use as reference.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While vast and detailed language data exist for high-resource languages such as English and Spanish, low-resource languages, including many American indigenous languages like Aymara and Quechua <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib2" title="">2</a>]</cite>, lack large-scale corpora to be used for training. Because models learn the syntactic and lexical patterns underlying translations through processing training data, an insufficient amount of data hinders them from producing accurate translations, and consequently, models often generate incorrect translations for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Prior research has addressed this problem, but few truly solve the issue. Previous approaches focus on transferring learning between high-resource and low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib2" title="">2</a>]</cite>, which have limited efficacy depending on the similarity between the high-resource and low-resource languages being used. The direction of data augmentation with completely original sentences has not been fully explored <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib4" title="">4</a>]</cite> and holds promise for breakthrough, as data augmentation directly addresses the challenge of lacking training data. Both monolingual and parallel data augmentation is important for Neural Machine Translation (NMT), as training on monolingual corpora in addition to parallel data has been used to improve NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib7" title="">7</a>]</cite>, especially in low-resource NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib8" title="">8</a>]</cite>, and our system performs monolingual data augmentation.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In contrast to the human labor of creating new sentences in low-resource languages by hand, a generative-adversarial network (GAN) is capable of autonomously generating unlimited amounts of new data. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib12" title="">12</a>]</cite> have implemented GANs for general NMT, but at the time of our research, no previous models have used them for text augmentation of low-resource languages.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Using a simulated low-resource setting of only 20,000 training data points, we explore the capability of a GAN for monolingual low-resource language data augmentation to improve machine translation quality. We build on the structure of the GAN from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib9" title="">9</a>]</cite>, which directly translates between high-resource languages, to generate a completely new low-resource language corpus from noise. Overall, our research is the first to combine GANs, data augmentation, and low-resource NMT.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Preliminaries on NMT</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">NMT uses neural networks to translate between two different languages and commonly uses an encoder-decoder architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib15" title="">15</a>]</cite>. A sequence-to-sequence encoder-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib14" title="">14</a>]</cite> traditionally uses two recurrent neural networks (RNNs), called the encoder and the decoder. The encoder converts a sentence in a given language into latent space (encoding) while the decoder takes the latent space and converts it back into a sentence in the other language (decoding). As the latent space representations, also known as embeddings, represent the core meaning of the sentence, the output of the decoder is a direct translation of the sentence that was input to the encoder. Long-short term memory (LSTM) networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib16" title="">16</a>]</cite> are a type of RNN that can operate on a sequence of words and are commonly used in NMT because they can capture long-term dependencies between sequential data points. However, because they evaluate from left to right, the encoder-decoder does not examine the context that appears after a word. As a result, these networks require multiple instances of words appearing in diverse contexts in order to create vectors that accurately represent the context needed for these words in the latent space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib4" title="">4</a>]</cite>, causing NMT to frequently err with low-resource pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib3" title="">3</a>]</cite>. NMT models based on Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib17" title="">17</a>]</cite> have also risen in popularity, as the Transformer framework uses attention to improve parallelizability of training. After implementing both the RNNSearch and the Transformer on their GAN model, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib9" title="">9</a>]</cite> found that both architectures could be applied with similar performances.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Data Augmentation for Low-Resource NMT</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Other models and software have tried to tackle this issue by using multilingual transfer-learning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib3" title="">3</a>]</cite> and a word substitution approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib4" title="">4</a>]</cite>. The transfer-learning approach teaches a model to use its knowledge from high-resource language pairs and apply it to low-resource languages, but it involves finding high resource languages that are very close to the language at hand, which is difficult depending on the language family. While the Germanic language family has more related high-resource languages such as German and Danish <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib19" title="">19</a>]</cite>, African languages’ closest high-resource languages are English and French <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib20" title="">20</a>]</cite>. Conversely, the data augmentation method in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib4" title="">4</a>]</cite> involves altering translation data by replacing specific words in given sentences, thereby diversifying the context in which these words show up. However, they do not generate completely new sentences to be used for data augmentation. As a result, while the model learns contexts for individual words, the model still suffers from lack of diverse grammatical structures and sentence topics from which it can learn how to structure full sentences.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Preliminaries on GANs</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Commonly used in image generation and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib9" title="">9</a>]</cite>, GANs combine two machine learning models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib21" title="">21</a>]</cite>. The first one is called the generator, a model that takes in input and generates samples, which are then fed into the discriminator, the second machine learning model. The discriminator is given samples either from the real data or from the generator, and it must determine if this sample is real or generated. If it correctly predicts, this indicates that it is improving. If it incorrectly predicts, this indicates that the generator is improving. Then, depending on how well the discriminator predicted, both models tweak their weights to continue this cycle. The generator “wins” when the discriminator cannot tell the two samples apart, and the discriminator “wins” if it can tell them apart. Both models want to win against the other, so each one keeps improving by learning from the other until they finally reach equilibrium, where both models are performing optimally.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">GANs in NLP</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In the past, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib22" title="">22</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib23" title="">23</a>]</cite> were successful in generating synthetic text through adversarial training, and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib24" title="">24</a>]</cite> implemented a GAN for low-resource speech augmentation for text-to-speech. To our knowledge, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib9" title="">9</a>]</cite> was the first to use GANs for machine translation, and we base our model on theirs. Their generator learned to translate a source-language sentence into its target-language translation while the discriminator tried to distinguish between real and generated translations. Continuing with this research, they implemented two GANs to use as tools to ensure the efficacy of their encoder models in weight sharing for latent space embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib11" title="">11</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib10" title="">10</a>]</cite> introduced bidirectional GANs to improve translation quality by creating another generator model to act as the discriminator, and the <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib12" title="">12</a>]</cite> study used a latent space based GAN to translate bidirectionally in both a supervised and unsupervised setting. To our knowledge, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib25" title="">25</a>]</cite> is the only other work that has applied GANs to low-resource language NMT, and their work was conducted in parallel to ours. They use GANs for direct translation rather than data augmentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Model Architecture</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Overall Workflow</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our model workflow (shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.F1" title="Figure 1 ‣ III-A Overall Workflow ‣ III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>) includes three stages: 1) pre-training of the encoder-decoder, 2) training of the GAN, and 3) generation of the augmented data set.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="905" id="S3.F1.g1" src="extracted/5810534/workflow.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overall Workflow</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the first stage, the encoder-decoder is trained on the human-created parallel corpus and learns to translate from language X to language Y. The encoder encodes X data into latent space embeddings while the decodes the embeddings into the corresponding translations in Y. The Y data are compared against the reference translations in the corpus, and the errors are backward propagated. The model repeatedly adjusts its weights until it is performing optimally.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">In the second stage, the encoder-decoder’s weights are frozen to train the GAN. The generator takes in a batch of random noise (randomly generated numbers between -1 to 1 which by themselves had no meaning), and attempts to assign meaning to them by rearranging them into latent space embeddings. A batch of sentences in language X are fed into the encoder, and it encodes them into “real” latent space embeddings. Both the generated and real embeddings are given as input into the discriminator, which classifies a given embedding as generated or real.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The discriminator’s prediction is compared with the actual label of the embedding, and its errors are backpropagated up to the generator.
When the discriminator classifies correctly, it is rewarded, and when it classifies incorrectly, meaning it cannot differentiate between the two embeddings, the generator is rewarded. Consequently, despite not having direct access to the encoder’s embeddings, the generator learns to generate embeddings closer to the encoder’s embeddings because it is randomly guessing until it awarded each time it does so. Through trial and error, the generator learns to create encodings more similar to the encoder’s in order to fool the discriminator.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">In the third stage, no training is involved. Once the GAN is performing optimally, the generator is given noise and generates a large corpus of embeddings. The decoder decodes these embeddings into sentences in language Y that correspond to the embeddings’ meanings in latent space. These sentences form the newly-augmented monolingual data corpus in language Y. The generator can be run an infinite amount of times to generate as much data as necessary.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">It is important to note that while most GAN architectures use sentences straight from the reference corpus as reference or ”real” data, we use the encoder’s embedding outputs as the reference data. As a result, our generator learns to generate latent space embeddings rather than immediately intelligible sentences in specific languages. To generalize this data augmentation method to other languages, only the encoder-decoder needs to be retrained, so that it learns to embed different languages.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Underlying Architectures</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.F2" title="Figure 2 ‣ III-B Underlying Architectures ‣ III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, the encoder-decoder uses bidirectional LSTMs. First, an embedding layer learns to map words with analogous meanings to similar numerical vectors and inputs them into the encoder LSTM, which includes weight decay and dropout to minimize overfitting. The latent-space output of the encoder LSTM is copied as input for the decoder LSTM, using a repeat vector. The decoder LSTM includes weight decay and dropout and decodes the latent space representations into numerical outputs. The logits layer maps the numerical outputs of the decoder LSTM into vectors that correspond with each word. The model uses a softmax activation function and is optimized with a categorical cross entropy loss function and an Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.F2" title="Figure 2 ‣ III-B Underlying Architectures ‣ III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, the generator consists of a single fully-connected layer and a reLu activation function. It is optimized with a categorical cross entropy loss function and an Adam optimizer. The loss is calculated based on the final loss value of the GAN and is inversely proportional to the loss of the discriminator.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S3.F2" title="Figure 2 ‣ III-B Underlying Architectures ‣ III Model Architecture ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, the discriminator consists of three fully-connected layers, followed by a one-unit dense layer and a sigmoid activation function, which produces a prediction of whether the input is from the encoder or the generator. It is optimized with a binary cross entropy loss function and an Adam optimizer. The loss is calculated based on the final loss value of the GAN.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="79" id="S3.F2.g1" src="extracted/5810534/Figures.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model Architectures</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The GAN consists of the initialized generator, followed by the discriminator. The input of the GAN is noise, which is fed into the generator. The output of the generator is fed into the discriminator, and the output of the discriminator is the prediction, which is used for backpropagation through the entire system to calculate loss values for the generator and discriminator. It is trained using a binary cross entropy loss function and an Adam optimizer.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Hyperparameter values and loss values during training are included in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A2" title="Appendix B Implementation Details ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A3" title="Appendix C Training ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">C</span></a>, respectively.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Data</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Simulated Low-Resource Setting</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In order to investigate the GAN model’s ability to augment low-resource language data, we mimic the characteristics of low-resource languages in English and Spanish. In the English to Spanish data set, we reduced the amount of data and used only 20,000 of the data set’s 253,726 sentence pairs to train and test the model. As a result, we were able to replicate the effect on low-resource languages and evaluate the model’s effectiveness after it was trained on 20,000 sentences.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Training Data</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The English to Spanish data set we used to train the model is from Tatoeba<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://tatoeba.org/en/</span></span></span> and has been pre-processed and cleaned by a third party<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://www.manythings.org/</span></span></span> who downloaded the original file from Tatoeba. Founded by Trang Ho in 2006, Tatoeba is a database containing collections of parallel translations contributed by thousands of members. It offers data for 419 languages that are easy to download. 19,000 sentence pairs of the 20,000 in the dataset were used to train the encoder-decoder.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Within the 19,000 sentences of training data, 1,000 sentence pairs were separated to be used as a test during each epoch of training to display validation accuracy and indicate overfitting of the encoder-decoder.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Test Data</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The last 1,000 sentence pairs in the 20,000 sentence English to Spanish data set from Tatoeba were not seen by the encoder-decoder and were used as test data, given to the encoder-decoder after it finished training to evaluate the model’s efficacy.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Numerical characteristics of both the training and test data are included in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A1" title="Appendix A Data Characteristics ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Preprocessing</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We began by removing punctuation from the language data and converting it to lowercase in order to standardize it and retain only the words themselves. We then tokenized the sentence data using the Keras Tokenizer, which split each sentence into a list of probabilities representing its constituent words. Because not all of the sentences had the same number of words, we found the longest sentence and padded the rest with zeros so that every sentence would be the same length.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Encoder-Decoder Performance</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">On the test data, the encoder-decoder had a final accuracy of 69.3%. We accept this accuracy under two considerations. First, the model was trained on less than 20,000 sentences, less than one-tenth of the size of other low-resource language data sets, whose sizes are between 0.2 to 1 million <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib3" title="">3</a>]</cite>. Second, the encoder-decoder is not the final model we propose, as it serves only as a guide for our GAN to learn how to generate latent space representations. As this paper focuses on the data augmentation aspect of our model, accuracy is used to give a high-level indication of the encoder-decoder’s performance so that we may continue on to training the GAN.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In the future, a pre-trained encoder-decoder may be used to stream-line the process, and as the research shifts from the data generation task to the end-to-end machine translation task, further evaluation, including BLEU scores <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib28" title="">28</a>]</cite>, will be performed.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">GAN Performance</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The generator was able to generate successful, coherent sentences, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.T1" title="TABLE I ‣ V-B GAN Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a>. Upon manual inspection, we found that the model was able to reproduce both syntactic and semantic meaning in some cases, although the model displayed significant errors in many other cases. Analysis of the errors made and limitations is discussed further below. Syntactically, the model placed words in the correct places based on their part of speech. A majority of the sentences also centered around a cohesive theme, indicating the model’s successful understanding of word meanings, and communicated a specific message. From random noise, the generator was able to create its own completely new and logical sentences, a significant feat considering the lack of training data.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
Sample Sentences Generated by the GAN
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Sample Generated Sentences</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Qualitative Evaluation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.2.1.1">my grandfather work harder than your grandfather before</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.2.1.2">good</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T1.1.3.2.1">to consider quit job is this dream man</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.3.2.2">good</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T1.1.4.3.1">ask me that healthy lunch im cooking up</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.4.3.2">good</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T1.1.5.4.1">maryam discovered hes hes am am are are</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.5.4.2">repetition</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T1.1.6.5.1">home actually was everything everything listen actually everything</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.6.5.2">repetition</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T1.1.7.6.1">cheerful weird yourself punished music alone everybody everybody</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.7.6.2">nonsensical</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S5.T1.1.8.7.1">those in so friends so complicated english comes</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.8.7.2">nonsensical</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.1.9.8.1">stressed gloves eating eating worried online online online</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.1.9.8.2">unrelated</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">It remains a challenge to quantitatively evaluate synthetically-generated data due to the lack of comparable reference sentences, and due to limited resources, we did not qualitatively evaluate every generated sentence.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">Error Analysis</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">While the GAN was successfully able to generate some coherent sentences from scratch, the GAN made a significant number of errors. We qualitatively observed that the severity of the errors decreased as the GAN trained for more epochs. Thus, future models may train for a larger number of epochs to examine whether the frequency and severity of errors are able to reduce significantly further.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.4.1.1">V-C</span>1 </span>Repeated Words</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Frequently, the GAN generated sentences with repeating words, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.T1" title="TABLE I ‣ V-B GAN Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a>. We hypothesize that this issue, which occurs in most NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib29" title="">29</a>]</cite>, is due to the fact that the model is trying to generate words that are close in context to each other, which is necessary in order for a sentence to make sense. However, the model may not know or weigh in if it already used a word or not and ends up repeating that same word. It likely tries to find a word that is close in context to the prior word, and because related words have closer probabilities, it generates a probability very close to the previous word’s probability. Then, after the decoder translates the latent space representations into probabilities, the close probabilities may be reduced to the same word because the words with the highest close probabilities are chosen to represent the probabilities the GAN returned. There may not be enough words with intricate probabilities close to a given one, so that one is chosen for all words with probabilities in a certain range around it. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib29" title="">29</a>]</cite> theorize that the issue is in the nature of languages themselves, as some words tend to predict themselves as the next word in context. Some incoherencies in the generated sentences could also be due to the fact the model may not fully understand the grammatical structure of sentences and resorts to repeating words it does know how to represent in order to fill up space. Potential solutions would be to train the model to remember the previous probabilities it generated and to vary its generated probabilities more to ensure that it does not repeat very similar ones.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS2.4.1.1">V-C</span>2 </span>Nonsensical Grammar</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Other sentences contained minimal repetition but were still grammatically incorrect or nonsensical, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.T1" title="TABLE I ‣ V-B GAN Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a>. We believe that these sentences contain relatively randomly-placed words because the model has not learned about these words with enough context to know where to place them grammatically. Because it has seen these well-known yet complex words, it can generate them but is unsure of how many to generate, where to place them, or which words to surround them with. The model also may have mistakenly generated words with similar probabilities in search of words with close context. For example, “cheerful” and “weird” are both adjectives that describe “yourself,” so their probabilities may be similar enough for the model to generate them together. However, the model does not understand that these words have parallel meanings, and that only one should be used. A possible avenue for future work is to explore training the model on the difference between probabilities of words parallel in meaning and probabilities of related words that are required to be together in order to form a sentence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS3.4.1.1">V-C</span>3 </span>Unrelated Words</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">Although the model generally uses related words like “studies” and “novels” together, it occasionally groups unrelated words together, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.T1" title="TABLE I ‣ V-B GAN Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a>. We hypothesize that this is because it has not seen a word (like “gloves”) enough to understand its usual context (being put on people’s hands). However, it does understand that gloves is a noun and has previously seen nouns (such as people) being stressed, eating, worrying, and going online. A potential future path to explore would be incorporating a dictionary of words into the model’s training so that it better understands the words’ meanings.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Because of its ability to generate an unlimited amount of original sentences despite being trained on minimal data, this GAN architecture can be used as a tool to augment low-resource language data, allowing translation models to train on more sentences in order to generate more accurate translations. This research is the first to apply a GAN to data augmentation in the low-resource NMT task, and we find promising results in cohesiveness and coherency of generated sentences. This work serves as a reference to encourage future work combining GANs and low-resource NMT. Improvements can be made on this research to increase the comprehensiveness of model evaluation and to minimize the repetition and incoherence in many of the generated sentences. One promising future direction is to train the model to understand the previous words it has generated and to remember the grammatical relationship between a word and others of similar probabilities.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Limitations</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">As the model was trained in a simulated low-resource setting, its performance on real low-resource languages would depend on these languages’ similarities to English and Spanish. Specifically, isolating languages, which have limited morphology, would work better with this model.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">For the encoder-decoder, as the small amount of data lend itself to severe overfitting, further research could be done to minimize overfitting through reducing model capacity, implementing L1 regularization in addition to the current L2 regularization being used, experimenting with stronger dropout, and applying cross-validation.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Thoroughly discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS3" title="V-C Error Analysis ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>, various directions also exist to improve the accuracy and reliability of the GAN’s generated sentences, from remembering previous probabilities to training the model to distinguish words parallel in meaning.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">As mentioned in Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS1" title="V-A Encoder-Decoder Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#S5.SS2" title="V-B GAN Performance ‣ V Results ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>, we plan to include further evaluation. As a next step, we will include BLEU scores for the encoder-decoder. For the GAN, we will use statistical analysis and thorough human evaluation, and a future direction is to use the synthetic data to train an NMT model to improve upon current baselines.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">While this model contributes to machine translation by augmenting monolingual data and the use of monolingual corpora is becoming increasingly prevalent in NMT models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib30" title="">30</a>]</cite>, extending this research to generate parallel translations would allow for a larger impact, as NMT models often train on parallel data in addition to monolingual corpora. Also, future software can be developed to clean the generated sentences from the GAN and extract only the coherent ones in order to add them to data sets. Reinserting punctuation and capitalization serves as another future area for exploration.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Data Characteristics</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A1.T2" title="TABLE II ‣ Appendix A Data Characteristics ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">II</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A1.T3" title="TABLE III ‣ Appendix A Data Characteristics ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">III</span></a> capture key statistical characteristics of the training and test data, respectively.</p>
</div>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Characteristics of Training Data</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.1.1">Characteristic</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.2.1">English</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.3.1">Spanish</span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T2.1.2.2.1">Average Sentence Length</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.2.2.2">4.72</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.2.2.3">4.52</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.1.3.3.1">Max Sentence length</th>
<td class="ltx_td ltx_align_left" id="A1.T2.1.3.3.2">8</td>
<td class="ltx_td ltx_align_left" id="A1.T2.1.3.3.3">11</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.1.4.4.1">Mean</th>
<td class="ltx_td ltx_align_left" id="A1.T2.1.4.4.2">204.38</td>
<td class="ltx_td ltx_align_left" id="A1.T2.1.4.4.3">300.87</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A1.T2.1.5.5.1">Standard Deviation</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T2.1.5.5.2">595.44</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T2.1.5.5.3">1055.53</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A1.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Characteristics of Test Data</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.1.1">Characteristic</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.2.1">English</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.3.1">Spanish</span></td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.2.2.1">Average Sentence Length</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T3.1.2.2.2">4.71</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T3.1.2.2.3">4.53</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T3.1.3.3.1">Max Sentence length</th>
<td class="ltx_td ltx_align_left" id="A1.T3.1.3.3.2">7</td>
<td class="ltx_td ltx_align_left" id="A1.T3.1.3.3.3">9</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T3.1.4.4.1">Mean</th>
<td class="ltx_td ltx_align_left" id="A1.T3.1.4.4.2">213.62</td>
<td class="ltx_td ltx_align_left" id="A1.T3.1.4.4.3">337.18</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A1.T3.1.5.5.1">Standard Deviation</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T3.1.5.5.2">662.48</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A1.T3.1.5.5.3">1285.73</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We use layers imported from the Keras Python library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#bib.bib31" title="">31</a>]</cite>. The hyperparameters we used are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A2.T4" title="TABLE IV ‣ Appendix B Implementation Details ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Hyperparameter Values</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.1">Hyperparameter</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T4.1.2.1.1">Epochs (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T4.1.2.1.2">400</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.3.2">
<td class="ltx_td ltx_align_left" id="A2.T4.1.3.2.1">Batch Size (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.3.2.2">30</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.4.3">
<td class="ltx_td ltx_align_left" id="A2.T4.1.4.3.1">LSTM Units (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.4.3.2">256</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.5.4">
<td class="ltx_td ltx_align_left" id="A2.T4.1.5.4.1">LSTM Dropout (Encoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.5.4.2">0.5</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.6.5">
<td class="ltx_td ltx_align_left" id="A2.T4.1.6.5.1">LSTM Dropout (Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.6.5.2">0.5</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.7.6">
<td class="ltx_td ltx_align_left" id="A2.T4.1.7.6.1">Logits Dropout (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.7.6.2">0.5</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.8.7">
<td class="ltx_td ltx_align_left" id="A2.T4.1.8.7.1">L2 Regularizer (Encoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.8.7.2">5e-5</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.9.8">
<td class="ltx_td ltx_align_left" id="A2.T4.1.9.8.1">L2 Regularizer (Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.9.8.2">1e-5</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.10.9">
<td class="ltx_td ltx_align_left" id="A2.T4.1.10.9.1">Learning Rate (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.10.9.2">2e-3</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.11.10">
<td class="ltx_td ltx_align_left" id="A2.T4.1.11.10.1">Beta1 Decay (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.11.10.2">0.7</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.12.11">
<td class="ltx_td ltx_align_left" id="A2.T4.1.12.11.1">Beta2 Decay (Encoder-Decoder)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.12.11.2">0.97</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.13.12">
<td class="ltx_td ltx_align_left" id="A2.T4.1.13.12.1">Epochs (GAN)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.13.12.2">8000</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.14.13">
<td class="ltx_td ltx_align_left" id="A2.T4.1.14.13.1">Batch Size (GAN)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.14.13.2">1900</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.15.14">
<td class="ltx_td ltx_align_left" id="A2.T4.1.15.14.1">Learning Rate (GAN)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.15.14.2">1e-4</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.16.15">
<td class="ltx_td ltx_align_left" id="A2.T4.1.16.15.1">Dense Units (Generator)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.16.15.2">256</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.17.16">
<td class="ltx_td ltx_align_left" id="A2.T4.1.17.16.1">Learning Rate (Generator)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.17.16.2">4e-4</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.18.17">
<td class="ltx_td ltx_align_left" id="A2.T4.1.18.17.1">Dense Units (Discriminator)</td>
<td class="ltx_td ltx_align_left" id="A2.T4.1.18.17.2">1024</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_b" id="A2.T4.1.19.18.1">Learning Rate (Discriminator)</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A2.T4.1.19.18.2">1e-4</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">In order to find the most optimal hyperparameters, we first varied the values by a factor of either 2 or 10 and tested every combination of the values with each other. To optimize time, we used 5,000 sentences and 80 epochs. For the learning rates of the encoder-decoder, generator, discriminator, and GAN as well as the L2 regularizers, we tried a range of values from 1e-1 to 1e-8 decreasing in magnitude by a factor of 10 each time. When varying the number of units and batch sizes for the encoder-decoder’s LSTM layers, the generator’s dense layer, and the discriminator’s dense layers, we chose powers of 2 between 16 and 2048. For the encoder-decoder’s dropouts, we tried a range from 0.5 to 0.8.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">After finding the approximate values to optimize performance, we tested more specific values within the ideal range we found, isolating each of the models and incrementing values by around 1 to 10. We continued this process until we found the most optimal parameters and then increased the amount of training data and epochs.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Training</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Using a batch size of 30, the encoder-decoder trained across 400 epochs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A3.F3" title="Figure 3 ‣ Appendix C Training ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the progression of training and validation accuracy and loss through epochs. The encoder-decoder’s loss plateaued for the training data, reaching between 0.5 and 0 for the training data. It overfit to a certain extent, as the validation data’s loss began to increase. The encoder-decoder’s final training accuracy was 92.8%. On validation data, it had a peak accuracy of 71.4%.</p>
</div>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="958" id="A3.F3.g1" src="extracted/5810534/lossandaccuracy.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy and Loss of the Encoder-Decoder during Training</figcaption>
</figure>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">Using a batch size of 1900, the GAN trained across 8000 epochs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.00071v1#A3.F4" title="Figure 4 ‣ Appendix C Training ‣ Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> shows the loss of the generator and the discriminator for the first 1000 epochs. The GAN’s loss values steeply dropped and reached a plateau for both the generator and the discriminator, indicating that the models reached convergence and were both performing optimally against each other. The next 3000 epochs were run to further refine the models’ performance. The final loss values hovered around 0.581 for the generator and 0.438 for the discriminator.</p>
</div>
<figure class="ltx_figure" id="A3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="A3.F4.g1" src="extracted/5810534/ganloss.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Loss of the GAN</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Acknowledgment</h2>
<div class="ltx_para" id="Ax1.p1">
<p class="ltx_p" id="Ax1.p1.1">Many thanks to Anu Datar and Ricky Grannis-Vu for their ongoing encouragement and support.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Gu, H. Hassan, J. Devlin, and V. O. Li, “Universal neural machine translation for extremely low resource languages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>.   New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 344–354. [Online]. Available: https://aclanthology.org/N18-1032

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F. Zheng, M. Reid, E. Marrese-Taylor, and Y. Matsuo, “Low-resource machine translation using cross-lingual language model pretraining,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</em>.   Online: Association for Computational Linguistics, Jun. 2021, pp. 234–240. [Online]. Available: https://aclanthology.org/2021.americasnlp-1.26

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Zoph, D. Yuret, J. May, and K. Knight, “Transfer learning for low-resource neural machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>.   Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 1568–1575. [Online]. Available: https://aclanthology.org/D16-1163

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Fadaee, A. Bisazza, and C. Monz, “Data augmentation for low-resource neural machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>.   Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 567–573. [Online]. Available: https://aclanthology.org/P17-2090

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Zhang and C. Zong, “Exploiting source-side monolingual data in neural machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>.   Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 1535–1545. [Online]. Available: https://aclanthology.org/D16-1160

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine translation models with monolingual data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.   Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 86–96. [Online]. Available: https://aclanthology.org/P16-1009

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Cai, Y. Wang, H. Li, W. Lam, and L. Liu, “Neural machine translation with monolingual translation memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>.   Online: Association for Computational Linguistics, Aug. 2021, pp. 7307–7318. [Online]. Available: https://aclanthology.org/2021.acl-long.567

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Currey, A. V. Miceli Barone, and K. Heafield, “Copied monolingual data improves low-resource neural machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Second Conference on Machine Translation</em>.   Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 148–156. [Online]. Available: https://aclanthology.org/W17-4715

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Yang, W. Chen, F. Wang, and B. Xu, “Improving neural machine translation with conditional sequence generative adversarial nets,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>.   New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 1346–1355. [Online]. Available: https://aclanthology.org/N18-1122

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Zhang, S. Liu, M. Li, M. Zhou, and E. Chen, “Bidirectional generative adversarial networks for neural machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 22nd Conference on Computational Natural Language Learning</em>.   Brussels, Belgium: Association for Computational Linguistics, Oct. 2018, pp. 190–199. [Online]. Available: https://aclanthology.org/K18-1019

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Yang, W. Chen, F. Wang, and B. Xu, “Unsupervised neural machine translation with weight sharing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.   Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 46–55. [Online]. Available: https://aclanthology.org/P18-1005

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Rashid, A. Do-Omri, M. A. Haidar, Q. Liu, and M. Rezagholizadeh, “Bilingual-GAN: A step towards parallel text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</em>.   Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 55–64. [Online]. Available: https://aclanthology.org/W19-2307

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>.   Seattle, Washington, USA: Association for Computational Linguistics, Oct. 2013, pp. 1700–1709. [Online]. Available: https://aclanthology.org/D13-1176

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN encoder–decoder for statistical machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.   Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1724–1734. [Online]. Available: https://aclanthology.org/D14-1179

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Neural Comput.</em>, vol. 9, no. 8, p. 1735–1780, nov 1997. [Online]. Available: https://doi.org/10.1162/neco.1997.9.8.1735

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Tchistiakova, J. Alabi, K. D. Chowdhury, S. Dutta, and D. Ruiter, “Edinsaar@wmt21: North-germanic low-resource multilingual nmt,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
W.-R. Chen and M. Abdul-Mageed, “Machine translation of low-resource indo-european languages,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. M. B. Dione, “Multilingual dependency parsing for low-resource African languages: Case studies on Bambara, Wolof, and Yoruba,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</em>.   Online: Association for Computational Linguistics, Aug. 2021, pp. 84–92. [Online]. Available: https://aclanthology.org/2021.iwpt-1.9

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F. Betti, G. Ramponi, and M. Piccardi, “Controlled text generation with adversarial learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 13th International Conference on Natural Language Generation</em>.   Dublin, Ireland: Association for Computational Linguistics, Dec. 2020, pp. 29–34. [Online]. Available: https://aclanthology.org/2020.inlg-1.5

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Ahamad, “Generating text through adversarial training using skip-thought vectors,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</em>.   Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 53–60. [Online]. Available: https://aclanthology.org/N19-3008

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Elneima and M. Bińkowski, “Adversarial text-to-speech for low-resource languages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP)</em>.   Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, Dec. 2022, pp. 76–84. [Online]. Available: https://aclanthology.org/2022.wanlp-1.8

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Kumar, A. Pratap, and A. K. Singh, “Exploiting multilingualism in low-resource neural machine translation via adversarial learning,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Ranathunga, E.-S. A. Lee, M. P. Skenduli, R. Shekhar, M. Alam, and R. Kaur, “Neural machine translation for low-resource languages: A survey,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>.   Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311–318. [Online]. Available: https://aclanthology.org/P02-1040

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Fu, W. Lam, A. M.-C. So, and B. Shi, “A theoretical analysis of the repetition problem in text generation,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Cai, Y. Wang, H. Li, W. Lam, and L. Liu, “Neural machine translation with monolingual translation memory,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
F. Chollet <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et al.</em> (2015) Keras. [Online]. Available: https://github.com/fchollet/keras

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 23 23:42:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
