<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Questioning AI: Promoting Decision-Making Autonomy Through Reflection</title>
<!--Generated on Mon Sep 16 12:57:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="decision-making autonomy,  epistemological responsibility,  explainable AI,  human oversight" lang="en" name="keywords"/>
<base href="/html/2409.10250v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S1" title="In Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S2" title="In Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Shortcomings of Explanations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S2.SS1" title="In 2. Shortcomings of Explanations ‣ Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Related Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S3" title="In Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Engagement through Questions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S3.SS1" title="In 3. Engagement through Questions ‣ Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#S4" title="In Questioning AI: Promoting Decision-Making Autonomy Through Reflection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Concluding Remarks</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Questioning AI: Promoting Decision-Making Autonomy Through Reflection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon W.S. Fischer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:simon.fischer@donders.ru.nl">simon.fischer@donders.ru.nl</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-2992-6563" title="ORCID identifier">0000-0003-2992-6563</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Donders Institute for Brain, Cognition, and Behaviour</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Nijmegen</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Netherlands</span>
</span></span></span>
</div>
<div class="ltx_dates">(16 September 2024)</div>
<div class="ltx_keywords">decision-making autonomy, epistemological responsibility, explainable AI, human oversight
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>7th FAccTRec Workshop on Responsible Recommendation at RecSys 2024, Bari, Italy; October 14, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span></span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing HCI theory, concepts and models</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Decision support systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Decision-making is increasingly supported by machine recommendations. In healthcare, for example, a clinical decision support system is used by the physician to find a treatment option for a patient. In doing so, people can rely too much on these systems, which impairs their own reasoning process.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The European AI Act addresses the risk of over-reliance and postulates in Article 14 on human oversight that people should be able “to remain aware of the possible tendency of automatically relying or over-relying on the output” <cite class="ltx_cite ltx_citemacro_citep">(Union, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib16" title="">2024</a>)</cite>. Similarly, the EU High-Level Expert Group identifies human agency and oversight as the first of seven key requirements for trustworthy AI <cite class="ltx_cite ltx_citemacro_citep">(on AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib14" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The following position paper proposes a conceptual approach to generate machine questions about the decision at hand, in order to promote decision-making autonomy. This engagement in turn allows for oversight of recommender systems.
The systematic and interdisciplinary investigation (e.g., machine learning, user experience design, psychology, philosophy of technology) of human-machine interaction in relation to decision-making provides insights to questions like: <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">how to increase human oversight and calibrate over- and under-reliance on machine recommendations; how to increase decision-making autonomy and remain aware of other possibilities beyond automated suggestions that repeat the status-quo</span>?</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Shortcomings of Explanations</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To ensure oversight, Article 13 of the European AI Act requires the ability to “interpret a system’s output and use it appropriately” <cite class="ltx_cite ltx_citemacro_citep">(Union, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib16" title="">2024</a>)</cite>. The assumption is that the increased transparency of the system’s behaviour allows operators (i.e., decision-makers) to better understand and monitor the system, which ideally contributes to a more informed decision-making. One solution to increase transparency of recommender systems is explainable AI (XAI). Explanations are important to sense-making, but it is not trivial to provide them, as different people require different explanations depending on the context <cite class="ltx_cite ltx_citemacro_citep">(Zednik, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib19" title="">2019</a>)</cite>. As such, physicians require different explanations than patients, more so than model developers. The agent-relative nature of explanations leads to at least two interrelated problems.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">First, there is a gap between XAI methods and context-specific needs <cite class="ltx_cite ltx_citemacro_citep">(Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib10" title="">2020</a>)</cite>. Current XAI methods, with their algorithm-centred view, are primarily aimed at model developers and researchers, but not at operators or decision-makers, who have less technical knowledge and need situational information <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib11" title="">2019</a>; Ehsan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib7" title="">2021</a>)</cite>.
Second, explanations do not help to sufficiently calibrate reliance on machines. Explanations are either ignored (i.e., under-reliance) <cite class="ltx_cite ltx_citemacro_citep">(Van Der Waa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib18" title="">2021</a>; Burton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib4" title="">2020</a>)</cite>, or they can reinforce a decision regardless of its quality (i.e., over-reliance) <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib2" title="">2021</a>; Jacobs et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib9" title="">2021</a>)</cite>. The mere provision of more information in favour of transparency, which, however, does not meet the epistemic needs of the decision-maker, therefore does not automatically lead to the appropriate use and oversight of recommender systems.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Related Work</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">There is a small but growing body of literature that discusses various ways to engage the decision-maker in the decision-making process that involves machine recommendations and explanations. The overarching aim is to calibrate the reliance on these systems (i.e., avoid over- and under-reliance) <cite class="ltx_cite ltx_citemacro_citep">(Naiseh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib13" title="">2021</a>)</cite>. In one study, for example, different ways of presenting explanations, i.e., cognitive forcing strategies, were investigated: on demand; delayed after 30 seconds; and only once after the human had made their decision. The results show that these interventions reduce over-reliance on machine explanations, but the most effective were also the least favoured by participants <cite class="ltx_cite ltx_citemacro_citep">(Buçinca et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib3" title="">2021</a>)</cite>. Another paper proposes a hypothesis-driven recommender system that shows evidence for and against skin cancer, so that the physician can make an informed decision rather than simply accepting or rejecting a recommendation
<cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib12" title="">2023</a>)</cite>. Although a different study showed that presenting evidence for and against the diagnosis did not significantly improve diagnostic accuracy, the reflective aspect of this approach was appreciated by physicians <cite class="ltx_cite ltx_citemacro_citep">(Cabitza et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib5" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Engagement through Questions</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To further develop this line of research to increase engagement and thus human oversight over machine recommendations, this paper proposes the concept of a question-asking machine <cite class="ltx_cite ltx_citemacro_citep">(Haselager et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib8" title="">2023</a>)</cite>. This additional system (or component of an existing recommender system) asks the decision-maker questions to encourage critical thought and reflection on the decision at hand. Although this system has not yet been fully implemented and evaluated, another study found that framing causal explanations as questions can improve human judgement <cite class="ltx_cite ltx_citemacro_citep">(Danry et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib6" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To formalise evidence and hypotheses and generate meaningful machine questions, we need to combine case data, machine recommendations, XAI methods, and human domain knowledge in a probabilistic approach. The aim is to bridge the above-mentioned gap between explanations and context-specific needs to create more meaningful human-computer interaction with appropriate reliance.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Case Study</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">An illustrative example is the medical field, where a doctor uses a clinical decision support system to find the best treatment option for already diagnosed patients.
A physician has a professional responsibility to promote the health and well-being of the patient. This includes an epistemological responsibility of the physician for how they make a diagnosis and what treatment they suggest <cite class="ltx_cite ltx_citemacro_citep">(van Baalen and Boon, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib17" title="">2015</a>)</cite>. In other words, the physician needs to be able to provide reasons for their decisions, instead of merely referring to the machine recommendation. Remaining critical towards the output is therefore crucial.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Questions can be about the relevance and adequacy of the importance or significance of certain features that the recommender system considers. Next to using XAI methods like LIME or SHAP to provide feature importance, an additional step is added to help the physician reflect on and contextualise these probabilities.
Example questions could be “Is symptom <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">x</span> that contributed 62% to the outcome actually that relevant?” or “How does diagnosis <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">Y</span> follow from symptom <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">x</span>?”. This type of question examines the apparent causal dependencies or associations of symptoms embedded in the model. As different risks or diseases can have common symptoms, and patient information comes from heterogeneous sources, it is relevant to consider different alternatives. Especially as an explanation for only the most likely option can be misleading <cite class="ltx_cite ltx_citemacro_citep">(Rudin, <a class="ltx_ref" href="https://arxiv.org/html/2409.10250v1#bib.bib15" title="">2019</a>)</cite>. As such, other questions can be about the strongest evidence for the alternative decision, and whether these symptoms can be easily ignored.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Next, questions can be asked about scenarios in which the data looks different, similar to counterfactual explanations. To do so we need to perturb the input variables and simulate different scenarios. It can then be inquired whether a particular symptom could be reduced or increased by another smaller intervention to make the actual treatment more effective (e.g., “Is it possible to change symptom <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">x</span> from <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">p</span> to <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.3">q</span>? This would increase the chances of a positive treatment by 25%”). The patient’s expectation of recovery, for example, has a major influence on the treatment outcome. Besides, considering different input also allows the physician to reflect on and challenge built-int thresholds (e.g., “If the patient was 3 years older, which reduces the chances of a positive result, would you still recommend the same treatment?”).</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Other questions might relate to information that is not taken into account by the recommender system. This could be particularly important for diseases that are less well documented, or where the data distribution is skewed due to sampling bias, or simply because the information is not yet available because of an unperformed test (e.g., medical imaging).</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Overall, the aim is to promote the physician’s own judgement by critically questioning the machine recommendation and considering other hypotheses. In other words, instead of providing a recommendation (i.e., an answer), we want to make people aware of the range of possible answers in order to create solutions that fit the current case. The physician can benefit from data-based findings and at the same time re-contextualise case-related information.
In this way, the physician can form a better picture of the patient, ensuring that the patient is treated in the most appropriate way, which has an impact on the patient’s safety.
In addition, increasing the chances of a positive outcome is linked to the impact on fairness in relation to the patient’s treatment. Many systems use erroneous or distorted data, resulting in underrepresented diseases being misdiagnosed and incorrectly treated. Questions can sensitise the physician to embedded assumptions and values in order to counteract potential tunnel vision. Moreover, questions can also be used to elicit the needs and values of the patient, e.g., specific preferences for certain treatment options or other personal circumstances. This could serve more as a fallback option, as the physician will ideally inquire about this information.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Questions enable a more interactive interaction between the physician and the recommender system. Ideally, the recommender system could process the answer or feedback of the physician in order to adjust the output accordingly. For example, during the consultation, the physician realises that the patient has given a pain score in a screening questionnaire, which serves as input for the recommendation system, that is far above the appropriate value. Being able to adjust the input value and see how the output changes enhances the physician’s understanding of the model’s behaviour. More importantly, this interactivity increases the decision-making autonomy of the physician, thereby avoiding over-reliance on machine recommendations.
The interactivity between recommender system and physician in turn improves the interaction between physician and patient, as the physician can communicate and discuss how they reached a decision.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Concluding Remarks</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The proposed approach of posing questions about machine recommendations aims to put human expertise at the centre of decision-making while benefiting from data-driven insights from recommender systems. Particularly in domains such as medicine or law, where the decision-maker and decision-subject are different persons, epistemological responsibility, i.e., how a decision is made, is of crucial importance.
The assumption of our approach is that asking questions increases engagement and avoids over-reliance on machine recommendations, which allows the decision-maker to give a more complete account of the decision-making process.
It will be crucial to investigate what makes questions meaningful, how often and when to ask questions and when to simply provide information. These requirements highly depend on context. We thus also want to refrain from the implementation of large language models, as the relevance of the outputs bear the risk of being too unreliable. At a time when recommender systems are being increasingly deployed in decision-making processes, it is important to remain aware of other alternatives for which critical reflection on machine recommendations and explanations is necessary.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research is supported by the Donders Institute for Brain, Cognition, and Behaviour.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021.

</span>
<span class="ltx_bibblock">Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(CHI ’21)</em>. Association for Computing Machinery, New York, NY, USA, Article 81, 16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445717" title="">https://doi.org/10.1145/3411764.3445717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buçinca et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021.

</span>
<span class="ltx_bibblock">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proc. ACM Hum.-Comput. Interact.</em> 5, CSCW1, Article 188 (apr 2021), 21 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3449287" title="">https://doi.org/10.1145/3449287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burton et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jason W. Burton, Mari-Klara Stein, and Tina Blegind Jensen. 2020.

</span>
<span class="ltx_bibblock">A Systematic Review of Algorithm Aversion in Augmented Decision Making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Journal of Behavioral Decision Making</em> 33, 2 (April 2020), 220–239.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1002/bdm.2155" title="">https://doi.org/10.1002/bdm.2155</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cabitza et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Federico Cabitza, Andrea Campagner, Lorenzo Famiglini, Chiara Natali, Valerio Caccavella, and Enrico Gallazzi. 2023.

</span>
<span class="ltx_bibblock">Let Me Think! Investigating the Effect of Explanations Feeding Doubts About the AI Advice.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Machine Learning and Knowledge Extraction</em>, Andreas Holzinger, Peter Kieseberg, Federico Cabitza, Andrea Campagner, A Min Tjoa, and Edgar Weippl (Eds.). Vol. 14065. Springer Nature Switzerland, Cham, 155–169.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-40837-3_10" title="">https://doi.org/10.1007/978-3-031-40837-3_10</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danry et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. 2023.

</span>
<span class="ltx_bibblock">Don’t Just Tell Me, Ask Me: AI Systems that Intelligently Frame Explanations as Questions Improve Human Logical Discernment Accuracy over Causal AI explanations. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Hamburg, Germany) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 352, 13 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580672" title="">https://doi.org/10.1145/3544548.3580672</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsan et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021.

</span>
<span class="ltx_bibblock">Expanding Explainability: Towards Social Transparency in AI systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(CHI ’21)</em>. Association for Computing Machinery, New York, NY, USA, Article 82, 19 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445188" title="">https://doi.org/10.1145/3411764.3445188</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haselager et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Pim Haselager, Hanna Schraffenberger, Serge Thill, Simon Fischer, Pablo Lanillos, Sebastiaan Van De Groes, and Miranda Van Hooff. 2023.

</span>
<span class="ltx_bibblock">Reflection Machines: Supporting Effective Human Oversight Over Medical Decision Support Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Cambridge Quarterly of Healthcare Ethics</em> (Jan. 2023), 1–10.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1017/S0963180122000718" title="">https://doi.org/10.1017/S0963180122000718</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Maia Jacobs, Melanie F. Pradier, Thomas H. McCoy, Roy H. Perlis, Finale Doshi-Velez, and Krzysztof Z. Gajos. 2021.

</span>
<span class="ltx_bibblock">How Machine-Learning Recommendations Influence Clinician Treatment Selections: The Example of Antidepressant Selection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Translational Psychiatry</em> 11, 1 (Feb. 2021), 108.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41398-021-01224-x" title="">https://doi.org/10.1038/s41398-021-01224-x</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020.

</span>
<span class="ltx_bibblock">Questioning the AI: Informing Design Practices for Explainable AI User Experiences. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib10.4.2">(CHI ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376590" title="">https://doi.org/10.1145/3313831.3376590</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (2019)</span>
<span class="ltx_bibblock">
Tim Miller. 2019.

</span>
<span class="ltx_bibblock">Explanation in Artificial Intelligence: Insights from the Social Sciences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Artificial Intelligence</em> 267 (Feb. 2019), 1–38.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.artint.2018.07.007" title="">https://doi.org/10.1016/j.artint.2018.07.007</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (2023)</span>
<span class="ltx_bibblock">
Tim Miller. 2023.

</span>
<span class="ltx_bibblock">Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven Decision Support using Evaluative AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em> (Chicago, IL, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2">(FAccT ’23)</em>. Association for Computing Machinery, New York, NY, USA, 333–342.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3593013.3594001" title="">https://doi.org/10.1145/3593013.3594001</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naiseh et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mohammad Naiseh, Reem S. Al-Mansoori, Dena Al-Thani, Nan Jiang, and Raian Ali. 2021.

</span>
<span class="ltx_bibblock">Nudging through Friction: An Approach for Calibrating Trust in Explainable AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">2021 8th International Conference on Behavioral and Social Computing (BESC)</em>. IEEE, Doha, Qatar, 1–5.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/BESC53957.2021.9635271" title="">https://doi.org/10.1109/BESC53957.2021.9635271</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">on AI (2019)</span>
<span class="ltx_bibblock">
High-Level Expert Group on AI. 2019.

</span>
<span class="ltx_bibblock">Ethics Guidelines for Trustworthy AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" title="">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rudin (2019)</span>
<span class="ltx_bibblock">
Cynthia Rudin. 2019.

</span>
<span class="ltx_bibblock">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Nature Machine Intelligence</em> 1, 5 (May 2019), 206–215.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s42256-019-0048-x" title="">https://doi.org/10.1038/s42256-019-0048-x</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Union (2024)</span>
<span class="ltx_bibblock">
European Union. 2024.

</span>
<span class="ltx_bibblock">Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://data.europa.eu/eli/reg/2024/1689/oj" title="">http://data.europa.eu/eli/reg/2024/1689/oj</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Baalen and Boon (2015)</span>
<span class="ltx_bibblock">
Sophie van Baalen and Mieke Boon. 2015.

</span>
<span class="ltx_bibblock">An Epistemological Shift: From Evidence-Based Medicine to Epistemological Responsibility: From EBM to Epistemological Responsibility.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Journal of Evaluation in Clinical Practice</em> 21, 3 (June 2015), 433–439.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111/jep.12282" title="">https://doi.org/10.1111/jep.12282</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Der Waa et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jasper Van Der Waa, Sabine Verdult, Karel Van Den Bosch, Jurriaan Van Diggelen, Tjalling Haije, Birgit Van Der Stigchel, and Ioana Cocu. 2021.

</span>
<span class="ltx_bibblock">Moral Decision Making in Human-Agent Teams: Human Control and the Role of Explanations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Frontiers in Robotics and AI</em> 8 (May 2021), 640647.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3389/frobt.2021.640647" title="">https://doi.org/10.3389/frobt.2021.640647</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zednik (2019)</span>
<span class="ltx_bibblock">
Carlos Zednik. 2019.

</span>
<span class="ltx_bibblock">Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Philosophy &amp; Technology</em> 34 (Dec. 2019), 265–288.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s13347-019-00382-7" title="">https://doi.org/10.1007/s13347-019-00382-7</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 12:57:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
