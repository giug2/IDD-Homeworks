<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning</title>
<!--Generated on Tue Oct 15 18:08:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.11963v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S1" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S2" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>CtrlSynth</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.SS1" title="In 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Key Components</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.SS2" title="In 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Image Text Synthesis in CtrlSynth</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS1" title="In 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS2" title="In 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS3" title="In 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Performance on Long-tail Tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS4" title="In 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS4.SSS0.Px1" title="In 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Data-Efficiency of CtrlSynth in Training CLIP.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS4.SSS0.Px2" title="In 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Statistics and visualization of CtrlSynth Samples.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS4.SSS0.Px3" title="In 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Effects of Self-Filtering.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS4.SSS0.Px4" title="In 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Mixing Ratios of Synthetic Samples.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS5" title="In 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S5" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1" title="In CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS1" title="In Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Control Policies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS1.SSS0.Px1" title="In A.1 Control Policies ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Text Prompt Templates.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS1.SSS0.Px2" title="In A.1 Control Policies ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Image Prompt Templates.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS2" title="In Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Datasets Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS2.SSS0.Px1" title="In A.2 Datasets Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Evaluation Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS2.SSS0.Px2" title="In A.2 Datasets Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Long-tail Datasets.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS3" title="In Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS3.SSS0.Px1" title="In A.3 Training Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Pretraining Hyper-parameters.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4" title="In Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>CtrlSynth Inference Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4.SSS0.Px1" title="In A.4 CtrlSynth Inference Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">VTM.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4.SSS0.Px2" title="In A.4 CtrlSynth Inference Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">LLM.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4.SSS0.Px3" title="In A.4 CtrlSynth Inference Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">Text-to-image Model.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS5" title="In Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>More Analysis Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS5.SSS0.Px1" title="In A.5 More Analysis Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">CtrlSynth Samples.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS5.SSS0.Px2" title="In A.5 More Analysis Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_title">CtrlSynth Synthetic Texts.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingqing Cao &amp; Mahyar Najibi 
<br class="ltx_break"/>Apple &amp;Sachin Mehta 
<br class="ltx_break"/>Meta
</span><span class="ltx_author_notes">Work done while at Apple.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Pretraining robust vision or multimodal foundation models (<em class="ltx_emph ltx_font_italic" id="id1.id1.1">e.g</em>.<span class="ltx_text" id="id1.id1.2"></span>, CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (<em class="ltx_emph ltx_font_italic" id="id1.id1.3">e.g</em>.<span class="ltx_text" id="id1.id1.4"></span>, either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process.
In this paper, we design a <em class="ltx_emph ltx_font_italic" id="id1.id1.5">controllable</em> image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (<em class="ltx_emph ltx_font_italic" id="id1.id1.6">e.g</em>.<span class="ltx_text" id="id1.id1.7"></span>, remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">High-quality large-scale datasets have driven the success of large foundational AI models <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib44" title="">2021</a>; Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib46" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib52" title="">2023</a>)</cite>. Collecting and annotating datasets at large-scale is challenging and costly. One solution is to crawl data from the web; however, web data is noisy <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>); Kang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib19" title="">2023</a>)</cite>, has long-tail distributions <cite class="ltx_cite ltx_citemacro_citep">(Udandarao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib54" title="">2024</a>)</cite>, and often causes privacy or copyright issues <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib48" title="">2022</a>)</cite>. Synthetic data presents a viable and complementary alternative to overcome these challenges, as it allows for precise control over data generation and customization to meet specific requirements. A large body of work has focused on improving the quality of synthetic data for image and text data, from the generation of high-quality images <cite class="ltx_cite ltx_citemacro_cite">Dunlap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib9" title="">2023</a>); Islam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib18" title="">2024</a>)</cite> to the improvement of synthetic captions <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>); Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib11" title="">2023</a>)</cite>. While these works have shown that synthetic data successfully improves model performance for various vision or vision-language tasks, their synthetic pipeline is often ad hoc and tailored to specific purposes such as training better CLIP models or improving domain-specific vision models <cite class="ltx_cite ltx_citemacro_citep">(<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.2.2"></span>, DiffuseMix uses diffusion models to augment images and improves accuracy on image classification tasks Islam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib18" title="">2024</a>)</cite>. These data synthesis works also lack explicit fine-grained control over the generated texts or images, which are important for tasks with long-tail distribution (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">e.g</em>.<span class="ltx_text" id="S1.p1.1.4"></span>, augmenting tail class samples) or enforcing safety requirements <cite class="ltx_cite ltx_citemacro_citep">(<em class="ltx_emph ltx_font_italic" id="S1.p1.1.5.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.6.2"></span>, mitigating biased or sensitive content generation Schramowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib47" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.2"></span>, improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.). Our intuition is that large foundation models are already pretrained on a wide range of data and contain general knowledge about concepts, objects, and their relationships. For example, text-to-image models <cite class="ltx_cite ltx_citemacro_citep">(<em class="ltx_emph ltx_font_italic" id="S1.p2.1.3.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.4.2"></span>, Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib46" title="">2022</a>; Podell et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib43" title="">2024</a>)</cite> can generate detailed high-quality images based on text instructions. Similarly, large language models (LLMs)  <cite class="ltx_cite ltx_citemacro_citep">(<em class="ltx_emph ltx_font_italic" id="S1.p2.1.5.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.6.2"></span>, OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib40" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib52" title="">2023</a>)</cite> have strong instruction-following capabilities, which can be used to control the text data generation. CtrlSynth leverages these large pretrained models to build a modular and controllable synthetic data generation pipeline. CtrlSynth allows users to apply explicit control instructions to guide data generation for images and texts. Unlike previous data synthesis works that use image-captioning models to directly generate captions given an image <cite class="ltx_cite ltx_citemacro_citep">(<em class="ltx_emph ltx_font_italic" id="S1.p2.1.7.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.8.2"></span>, Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib26" title="">2024</a>; Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>)</cite>, CtrlSynth decomposes image-to-text generation process into two separate steps, providing more fine-grained control to users for synthesizing data. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S1.F1" title="In 1 Introduction ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows an overall architecture of the CtrlSynth pipeline.
For an input image, CtrlSynth first uses a pretrained vision model to extract key objects, attributes, and their relations as visual tags. It then uses a text controller to create text synthesis instructions and guide the LLM to use visual tags to generate high-quality text outputs. Similarly, we devise an image controller that steers how the text prompts (or caption) can be used to guide the diffusion model to generate a desired image. Users can also feed the generated synthetic images into the tagging model again, forming a closed-loop data pipeline. Then users can start with synthetic or original images and texts and further generate more image-text pairs. The text and image controllers are modular, allowing users to control any part of the text or image generation process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Compared to previous works, CtrlSynth provides three main benefits: (1) <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Controllable synthesis</span>: CtrlSynth allows users to define policies on the visual tags or texts; enabling granular control over text and image generation; (2) <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">Closed-loop system</span>: CtrlSynth requires no additional training and can synthesize text from images and vice-versa using existing pretrained models. This closed-loop design additionally provides automatic filtering and verification capabilities to discard undesirable synthetic samples without manual or heuristics-based rules. (3) <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">Flexible and scalable</span>: CtrlSynth is modular and allows users to change its components (<em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">e.g</em>.<span class="ltx_text" id="S1.p3.1.5"></span>, pretrained models) easily. (4) <span class="ltx_text ltx_font_bold" id="S1.p3.1.6">Extensive empirical results</span> We evaluate the effectiveness of CtrlSynth on different tasks (<em class="ltx_emph ltx_font_italic" id="S1.p3.1.7">e.g</em>.<span class="ltx_text" id="S1.p3.1.8"></span>, image classification, image-text retrieval, compositional reasoning, and long-tail tasks), covering <span class="ltx_text ltx_font_bold" id="S1.p3.1.9">31 datasets</span> for vision and vision-language domains. We observe that CtrlSynth generated data improves the accuracy by (a) 23.4% on retrieval tasks, (b) 5% on the SugarCrepe compositional reasoning benchmark, and (c) 16% <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">∼</annotation></semantics></math> 21% for long-tail vision tasks.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S1.F1.1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>CtrlSynth: A modular, closed-loop, controllable data synthesis system. The <span class="ltx_text ltx_font_italic" id="S1.F1.4.1">oval nodes</span> indicate that the pretrained models and <span class="ltx_text ltx_font_italic" id="S1.F1.5.2">rounded boxes</span> represent text or image data. The text and image controllers are used to guide the data synthesis.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Data-Efficient Vision-Language Representation Learning.</span> Contrastive Language-Image Pretraining (CLIP) <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib44" title="">2021</a>)</cite> has popularized visual representation learning from image-text pairs due to its strong zero-shot transfer capabilities. Many recent works have focused on improving the data efficiency of training CLIP models. SLIP <cite class="ltx_cite ltx_citemacro_citep">(Mu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib37" title="">2022</a>)</cite> brings self-supervised learning into a multitask learning framework to improve CLIP performance. FLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib27" title="">2023c</a>)</cite> masks out image patches during CLIP training, improving training efficiency and zero-shot accuracy over baselines. CLIPA <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib25" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib24" title="">a</a>)</cite> further improves over FLIP ideas and reduces the number of image text tokens by block and syntax masking for CLIP training and it significantly reduces the training costs of CLIP models. LiT <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib65" title="">2022</a>)</cite> freezes the image encoder in CLIP models and achieves strong zero-shot transfer for CLIP models using much fewer data samples. All these techniques focus on improving the training methods for CLIP models to enable better vision-language representations. CtrlSynth improves data augmentation for CLIP training by synthesizing diverse image text samples. Our method is orthogonal and could potentially benefit from these methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Image-text Data Augmentation.</span> Much recent work aims to improve the caption quality of image-text pairs. For example, VeCLIP <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>)</cite>, LaCLIP <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib11" title="">2023</a>)</cite>, and ReCap <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib26" title="">2024</a>)</cite> leverage LLMs to synthesize new captions that are more informative and contain rich descriptions about the image. The key difference of CtrlSynth is that we provide more diverse and high-quality captions that outperform prior works (we will show in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T5" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T6" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>). This is because CtrlSynth breaks down the image semantics to allow more fine-grained control and recombination using LLM.
Another line of work uses text-to-image models like diffusion models to generate synthetic images and augment downstream vision tasks. ALIA <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib9" title="">2023</a>)</cite> uses language to guide the image editing process and provides domain-specific diversity to augment the image samples. DiffuseMix <cite class="ltx_cite ltx_citemacro_citep">(Islam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib18" title="">2024</a>)</cite> augments image samples using diffusion models to blend original and generated images. EDA <cite class="ltx_cite ltx_citemacro_citep">(Trabucco et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib53" title="">2023</a>)</cite> generates variations of real images using diffusion models to maintain the semantics while augmenting image samples. These semantic image augmentation methods provide strong performance improvements on various vision datasets. Our CtrlSynth instead unifies the image and text synthesis via a closed-loop pipeline, it provides more flexibility and diverse synthetic samples while allowing more fine-grained control over the sample generation process.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CtrlSynth</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.11">CtrlSynth leverages semantic knowledge and reasoning skills of pretrained foundation models (<em class="ltx_emph ltx_font_italic" id="S3.p1.11.1">e.g</em>.<span class="ltx_text" id="S3.p1.11.2"></span>, large language and diffusion models) to generate diverse synthetic data samples in a controlled manner. Specifically, CtrlSynth consists of three foundation models: (1) a vision tagging model, (2) a large language model, and (3) a text-to-image model; plus the two text and image controllers. For a given real (
 <svg class="ltx_picture" height="17.82" id="S3.p1.1.pic1" overflow="visible" version="1.1" width="17.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.82) matrix(1 0 0 -1 0 0) translate(8.91,0) translate(0,8.91)"><text transform="matrix(1 0 0 -1 0 0)">1a</text><text transform="matrix(1 0 0 -1 0 0)">1a</text><g stroke="#CCCCCC"><path d="M 8.63 0 C 8.63 4.77 4.77 8.63 0 8.63 C -4.77 8.63 -8.63 4.77 -8.63 0 C -8.63 -4.77 -4.77 -8.63 0 -8.63 C 4.77 -8.63 8.63 -4.77 8.63 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.92 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text" id="S3.p1.1.pic1.1.1.1.1.1">1a</span></foreignobject></g></g></svg>
 in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S1.F1" title="In 1 Introduction ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>) or synthetic (
 <svg class="ltx_picture" height="17.21" id="S3.p1.2.pic2" overflow="visible" version="1.1" width="17.21"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.21) matrix(1 0 0 -1 0 0) translate(8.61,0) translate(0,8.61)"><text transform="matrix(1 0 0 -1 0 0)">1c</text><text transform="matrix(1 0 0 -1 0 0)">1c</text><g stroke="#CCCCCC"><path d="M 8.33 0 C 8.33 4.6 4.6 8.33 0 8.33 C -4.6 8.33 -8.33 4.6 -8.33 0 C -8.33 -4.6 -4.6 -8.33 0 -8.33 C 4.6 -8.33 8.33 -4.6 8.33 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.53 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.07"><span class="ltx_text" id="S3.p1.2.pic2.1.1.1.1.1">1c</span></foreignobject></g></g></svg>
) input image, a <em class="ltx_emph ltx_font_italic" id="S3.p1.11.3">vision tagging model</em> (
 <svg class="ltx_picture" height="17.82" id="S3.p1.3.pic3" overflow="visible" version="1.1" width="17.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.82) matrix(1 0 0 -1 0 0) translate(8.91,0) translate(0,8.91)"><text transform="matrix(1 0 0 -1 0 0)">2a</text><text transform="matrix(1 0 0 -1 0 0)">2a</text><g stroke="#CCCCCC"><path d="M 8.63 0 C 8.63 4.77 4.77 8.63 0 8.63 C -4.77 8.63 -8.63 4.77 -8.63 0 C -8.63 -4.77 -4.77 -8.63 0 -8.63 C 4.77 -8.63 8.63 -4.77 8.63 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.92 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text" id="S3.p1.3.pic3.1.1.1.1.1">2a</span></foreignobject></g></g></svg>
) extracts visual tags (<em class="ltx_emph ltx_font_italic" id="S3.p1.11.4">e.g</em>.<span class="ltx_text" id="S3.p1.11.5"></span>, objects, attributes, and their relationships) (
 <svg class="ltx_picture" height="17.21" id="S3.p1.4.pic4" overflow="visible" version="1.1" width="17.21"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.21) matrix(1 0 0 -1 0 0) translate(8.61,0) translate(0,8.61)"><text transform="matrix(1 0 0 -1 0 0)">1e</text><text transform="matrix(1 0 0 -1 0 0)">1e</text><g stroke="#CCCCCC"><path d="M 8.33 0 C 8.33 4.6 4.6 8.33 0 8.33 C -4.6 8.33 -8.33 4.6 -8.33 0 C -8.33 -4.6 -4.6 -8.33 0 -8.33 C 4.6 -8.33 8.33 -4.6 8.33 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.53 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.07"><span class="ltx_text" id="S3.p1.4.pic4.1.1.1.1.1">1e</span></foreignobject></g></g></svg>
). These tags describe the image’s visual concepts and semantic contexts. The <em class="ltx_emph ltx_font_italic" id="S3.p1.11.6">text controller</em> (
 <svg class="ltx_picture" height="17.82" id="S3.p1.5.pic5" overflow="visible" version="1.1" width="17.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.82) matrix(1 0 0 -1 0 0) translate(8.91,0) translate(0,8.91)"><text transform="matrix(1 0 0 -1 0 0)">3a</text><text transform="matrix(1 0 0 -1 0 0)">3a</text><g stroke="#CCCCCC"><path d="M 8.63 0 C 8.63 4.77 4.77 8.63 0 8.63 C -4.77 8.63 -8.63 4.77 -8.63 0 C -8.63 -4.77 -4.77 -8.63 0 -8.63 C 4.77 -8.63 8.63 -4.77 8.63 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.92 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><span class="ltx_text" id="S3.p1.5.pic5.1.1.1.1.1">3a</span></foreignobject></g></g></svg>
) takes the image tags and user-defined control policies as inputs and generates instructions for synthesizing new text. An example control policy is to edit the tags or optionally add the text (
 <svg class="ltx_picture" height="18.82" id="S3.p1.6.pic6" overflow="visible" version="1.1" width="18.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.82) matrix(1 0 0 -1 0 0) translate(9.41,0) translate(0,9.41)"><text transform="matrix(1 0 0 -1 0 0)">1b</text><text transform="matrix(1 0 0 -1 0 0)">1b</text><g stroke="#CCCCCC"><path d="M 9.13 0 C 9.13 5.05 5.05 9.13 0 9.13 C -5.05 9.13 -9.13 5.05 -9.13 0 C -9.13 -5.05 -5.05 -9.13 0 -9.13 C 5.05 -9.13 9.13 -5.05 9.13 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.3 -4.8)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.61"><span class="ltx_text" id="S3.p1.6.pic6.1.1.1.1.1">1b</span></foreignobject></g></g></svg>
) associated with the image.
A <em class="ltx_emph ltx_font_italic" id="S3.p1.11.7">large language model</em> (
 <svg class="ltx_picture" height="18.82" id="S3.p1.7.pic7" overflow="visible" version="1.1" width="18.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.82) matrix(1 0 0 -1 0 0) translate(9.41,0) translate(0,9.41)"><text transform="matrix(1 0 0 -1 0 0)">2b</text><text transform="matrix(1 0 0 -1 0 0)">2b</text><g stroke="#CCCCCC"><path d="M 9.13 0 C 9.13 5.05 5.05 9.13 0 9.13 C -5.05 9.13 -9.13 5.05 -9.13 0 C -9.13 -5.05 -5.05 -9.13 0 -9.13 C 5.05 -9.13 9.13 -5.05 9.13 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.3 -4.8)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.61"><span class="ltx_text" id="S3.p1.7.pic7.1.1.1.1.1">2b</span></foreignobject></g></g></svg>
) then follows the instructions and generates the synthetic text (
 <svg class="ltx_picture" height="18.82" id="S3.p1.8.pic8" overflow="visible" version="1.1" width="18.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.82) matrix(1 0 0 -1 0 0) translate(9.41,0) translate(0,9.41)"><text transform="matrix(1 0 0 -1 0 0)">1d</text><text transform="matrix(1 0 0 -1 0 0)">1d</text><g stroke="#CCCCCC"><path d="M 9.13 0 C 9.13 5.05 5.05 9.13 0 9.13 C -5.05 9.13 -9.13 5.05 -9.13 0 C -9.13 -5.05 -5.05 -9.13 0 -9.13 C 5.05 -9.13 9.13 -5.05 9.13 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.3 -4.8)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.61"><span class="ltx_text" id="S3.p1.8.pic8.1.1.1.1.1">1d</span></foreignobject></g></g></svg>
). The <em class="ltx_emph ltx_font_italic" id="S3.p1.11.8">image controller</em> (
 <svg class="ltx_picture" height="18.82" id="S3.p1.9.pic9" overflow="visible" version="1.1" width="18.82"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,18.82) matrix(1 0 0 -1 0 0) translate(9.41,0) translate(0,9.41)"><text transform="matrix(1 0 0 -1 0 0)">3b</text><text transform="matrix(1 0 0 -1 0 0)">3b</text><g stroke="#CCCCCC"><path d="M 9.13 0 C 9.13 5.05 5.05 9.13 0 9.13 C -5.05 9.13 -9.13 5.05 -9.13 0 C -9.13 -5.05 -5.05 -9.13 0 -9.13 C 5.05 -9.13 9.13 -5.05 9.13 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -7.3 -4.8)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="14.61"><span class="ltx_text" id="S3.p1.9.pic9.1.1.1.1.1">3b</span></foreignobject></g></g></svg>
) operates on the given input text
and applies user-defined image control policies to output instructions for image synthesis. An example policy is to specify the style for generating artistic, cinematic, or realistic images.
A <em class="ltx_emph ltx_font_italic" id="S3.p1.11.9">text-to-image model</em> (
 <svg class="ltx_picture" height="17.21" id="S3.p1.10.pic10" overflow="visible" version="1.1" width="17.21"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.21) matrix(1 0 0 -1 0 0) translate(8.61,0) translate(0,8.61)"><text transform="matrix(1 0 0 -1 0 0)">2c</text><text transform="matrix(1 0 0 -1 0 0)">2c</text><g stroke="#CCCCCC"><path d="M 8.33 0 C 8.33 4.6 4.6 8.33 0 8.33 C -4.6 8.33 -8.33 4.6 -8.33 0 C -8.33 -4.6 -4.6 -8.33 0 -8.33 C 4.6 -8.33 8.33 -4.6 8.33 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.53 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.07"><span class="ltx_text" id="S3.p1.10.pic10.1.1.1.1.1">2c</span></foreignobject></g></g></svg>
) takes an image synthesis instruction provided by the image controller as an input and produces a synthetic image as an output (
 <svg class="ltx_picture" height="17.21" id="S3.p1.11.pic11" overflow="visible" version="1.1" width="17.21"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.21) matrix(1 0 0 -1 0 0) translate(8.61,0) translate(0,8.61)"><text transform="matrix(1 0 0 -1 0 0)">1c</text><text transform="matrix(1 0 0 -1 0 0)">1c</text><g stroke="#CCCCCC"><path d="M 8.33 0 C 8.33 4.6 4.6 8.33 0 8.33 C -4.6 8.33 -8.33 4.6 -8.33 0 C -8.33 -4.6 -4.6 -8.33 0 -8.33 C 4.6 -8.33 8.33 -4.6 8.33 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.53 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.07"><span class="ltx_text" id="S3.p1.11.pic11.1.1.1.1.1">1c</span></foreignobject></g></g></svg>
).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Key Components</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<span class="ltx_ERROR undefined" id="S3.SS1.p1.1">\mdfdefinestyle</span>
<p class="ltx_p" id="S3.SS1.p1.2">mdfexample1innertopmargin=0.2em,innerbottommargin=0.2em,innerleftmargin=0.2em,innerrightmargin=0.2em,roundcorner=2pt,outerlinewidth=0.1,linecolor=blue!50,hidealllines=true</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Vision Tagging Model.</span>
The goal of a vision tagging model (VTM) is to extract the basic visual elements (or tags) of an image, including all objects or entities, attributes (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.2">e.g</em>.<span class="ltx_text" id="S3.SS1.p2.1.3"></span>, color, shape, and size), and visual relations (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.4">e.g</em>.<span class="ltx_text" id="S3.SS1.p2.1.5"></span>, interaction between objects).</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S3.F2">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.F2.1" style="width:424.9pt;height:305.9pt;vertical-align:-299.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.0pt,0.1pt) scale(0.972519756742635,0.972519756742635) ;">
<figure class="ltx_figure ltx_minipage ltx_align_top" id="S3.F2.1.1" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.F2.1.1.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.F2.1.1.1">[style=mdfexample1]
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="429" id="S3.F2.1.1.1.g1" src="extracted/5929450/fig/sven-brandsma-GZ5cKOgeIB0-unsplash.jpg" width="598"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_text ltx_font_bold" id="S3.F2.1.1.3.1">Objects and attributes</span>: light candle, patterned rug, white coffee table, sectional sofa
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.1.1.4.2">Relations</span>: in front of, on top, covered with 
<br class="ltx_break"/></figcaption>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Visual tags of an example image<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Image credit: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unsplash.com/photos/light-candle-on-round-white-coffee-table-and-sectional-sofa-GZ5cKOgeIB0" title="">https://unsplash.com/photos/light-candle-on-round-white-coffee-table-and-sectional-sofa-GZ5cKOgeIB0</a></span></span></span>. Tags are non-exhaustive.</figcaption>
</figure>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">An example of extracting visual tags from VTM is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#footnote2" title="In Figure 2 ‣ 3.1 Key Components ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Footnote</span> <span class="ltx_text ltx_ref_tag">2</span></a>. The tagging model can be either a multi-label image classifier <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib36" title="">2024b</a>)</cite> that predicts diverse tags in the image, or a black box system (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p3.1.2"></span> an API service) that takes the input image and outputs the tags.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">VTM, as a key component in CtrlSynth, can be a combination of an advanced captioning model <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib60" title="">2024</a>)</cite> that generates comprehensive image descriptions and an LLM that extracts the visual tags from the captions to decompose the visual semantics of an image into a set of fine-grained visual concepts. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4" title="A.4 CtrlSynth Inference Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.4</span></a> includes more details about this hybrid VTM. These fine-grained visual concepts can be easily modified and recomposed to create new visual contexts. This decompose-recompose feature of vision tags provides a large control space for synthesizing diverse texts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Existing caption rewriting works (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p5.1.2"></span>, VeCLIP <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>)</cite>) rely on a multimodal captioning model to generate captions that are short sentences containing visual concepts. Image captions can be very descriptive but often only cover the most salient object of the scene, they are coarse-grained in structure (whole sentence or paragraph), and are hard to modify.
Our key distinction is that VTM produces a comprehensive list of metadata information that describes the visual concepts in an image as completely as possible.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Language Model.</span> Large language models (LLMs) have exhibited strong instruction-following capabilities. The goal of an LLM in CtrlSynth is to take an input textual instruction on how to generate a synthetic text that meets the requirements specified in the instruction. CtrlSynth employs the reasoning and composition capability of LLMs to recombine the visual image tags in the task instruction and compose new synthetic texts. The instruction for an LLM consists of three parts (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.F3" title="In 3.1 Key Components ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>): <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">(i) <span class="ltx_text" id="S3.SS1.p6.1.2.1" style="color:#808000;">task template</span></span> that specifies the details of the text synthesis task, <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.3">(ii) <span class="ltx_text" id="S3.SS1.p6.1.3.1" style="color:#008080;">task content</span></span> that contains the actual visual tags (phrases) and an optional caption paired with the image, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.4">(iii) <span class="ltx_text" id="S3.SS1.p6.1.4.1" style="color:#BF8040;">task constraint</span></span> that describes the style and formatting of the output text. Users can also apply custom policies over the instructions to guide the text synthesis process.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><svg class="ltx_picture ltx_centering" height="118.63" id="S3.F3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,118.63) matrix(1 0 0 -1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 5.91 L 0 112.72 C 0 115.99 2.64 118.63 5.91 118.63 L 594.09 118.63 C 597.36 118.63 600 115.99 600 112.72 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="0.4"><path d="M 1.97 5.91 L 1.97 112.72 C 1.97 114.9 3.73 116.66 5.91 116.66 L 594.09 116.66 C 596.27 116.66 598.03 114.9 598.03 112.72 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 10.27 11.65)"><foreignobject color="#000000" height="95.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="579.46">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F3.pic1.1.1.1.1.1" style="width:418.8pt;">
<span class="ltx_p" id="S3.F3.pic1.1.1.1.1.1.1"><em class="ltx_emph ltx_font_italic" id="S3.F3.pic1.1.1.1.1.1.1.1">Write a faithful caption by integrating the given phrases with the original sentence. Ensure any objects from the original caption are preserved while elaborating on the visual relationships and attributes provided in the phrases to create a more detailed depiction. Given sentence: <span class="ltx_text ltx_font_upright" id="S3.F3.pic1.1.1.1.1.1.1.1.1">{</span>caption<span class="ltx_text ltx_font_upright" id="S3.F3.pic1.1.1.1.1.1.1.1.2">}</span>. Given phrases: <span class="ltx_text ltx_font_upright" id="S3.F3.pic1.1.1.1.1.1.1.1.3">{</span>phrases<span class="ltx_text ltx_font_upright" id="S3.F3.pic1.1.1.1.1.1.1.1.4">}</span>. The caption should not contain any NSFW words. It should be grammatically correct. It should be concise, but not too short. Directly output the caption and do not add any formatting.</em></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example instruction for LLMs to synthesize texts.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Text-to-Image Model.</span> Text-to-image models generate novel and diverse image samples based on different input text prompts. CtrlSynth applies an image controller to account for the user-specified control policies and accordingly, updates the input text instructions from the previous step (i.e., language model). These updated instructions are then fed to text-to-image models for generating the image as an output. In our experiments, we use StableDiffusion models for text-to-image generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">Text and Image Controllers.</span> The controller in CtrlSynth is a function that takes an input text and transforms it into a specific text instruction for the LLM or text-to-image model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">The text controller accepts the visual tags of an image and a user-defined policy along with an optional original text as input and produces instructions to control the generation of synthetic text. In CtrlSynth, we study three predefined policies: (a) editing (remove, add, or replace) visual tags, (b) constraining the semantic meaning of a given sentence, and (c) styling the output text. Editing visual tags allows fine-grained control of synthetic visual content, for example, one can remove unwanted objects or attributes so they do not appear in the generated text. Constraining the meaning of synthetic text is useful in generating high-quality captions because many web-crawled captions are noisy. Enforcing the styling of output texts such as outputting into structured formats (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p9.1.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p9.1.2"></span>, JSON) makes the texts easier to use in downstream tasks. In our experiments, we use 10 example text control policies for synthesizing image captions (see <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS1" title="A.1 Control Policies ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a> for details).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.1">The image controller is similar to the text controller in functionality. It mainly steers image generation via specific prompting. We study two simple control policies to show the controllability and utility of CtrlSynth. The first one involves weighting particular tags in the input prompt (lower or increase individual weights for a given tag) so that the output image has a different focus on the objects or attributes. The second policy applies different styles (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p10.1.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p10.1.2"></span>, cinematic, realistic, or art) to the output images for generating diverse content. Note that the control policies are flexible and can be easily modified for diverse use cases. For example, one can integrate more complex policies such as layout-guided <cite class="ltx_cite ltx_citemacro_citep">(Lian et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib28" title="">2023</a>)</cite> or planning-based <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib63" title="">2024b</a>)</cite> image generation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image Text Synthesis in CtrlSynth</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">CtrlSynth is a modular and closed-loop system by design and supports diverse image and text synthesis configurations. In this section, we first introduce different synthesis paths in CtrlSynth and then describe how the closed-loop feature allows CtrlSynth to filter out low-quality samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Flexible and diverse synthesis paths.</span> A data synthesis path (<math alttext="SP" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">S</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑆</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">SP</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_S italic_P</annotation></semantics></math>) starts and ends with a data node (rounded box in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S1.F1" title="In 1 Introduction ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>). We define the following synthesis paths:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3"><math alttext="SP(1)" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.2" xref="S3.SS2.p3.1.m1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.2.2" xref="S3.SS2.p3.1.m1.1.2.2.cmml">S</mi><mo id="S3.SS2.p3.1.m1.1.2.1" xref="S3.SS2.p3.1.m1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.2.3" xref="S3.SS2.p3.1.m1.1.2.3.cmml">P</mi><mo id="S3.SS2.p3.1.m1.1.2.1a" xref="S3.SS2.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p3.1.m1.1.2.4.2" xref="S3.SS2.p3.1.m1.1.2.cmml"><mo id="S3.SS2.p3.1.m1.1.2.4.2.1" stretchy="false" xref="S3.SS2.p3.1.m1.1.2.cmml">(</mo><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">1</mn><mo id="S3.SS2.p3.1.m1.1.2.4.2.2" stretchy="false" xref="S3.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.2"><times id="S3.SS2.p3.1.m1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.2.1"></times><ci id="S3.SS2.p3.1.m1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.2.2">𝑆</ci><ci id="S3.SS2.p3.1.m1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.2.3">𝑃</ci><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">SP(1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_S italic_P ( 1 )</annotation></semantics></math>: <math alttext="1a\rightarrow 2a\rightarrow 1e\rightarrow 3a\rightarrow 2b\rightarrow 1d" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mrow id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml"><mn id="S3.SS2.p3.2.m2.1.1.2.2" xref="S3.SS2.p3.2.m2.1.1.2.2.cmml">1</mn><mo id="S3.SS2.p3.2.m2.1.1.2.1" xref="S3.SS2.p3.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.2.3" xref="S3.SS2.p3.2.m2.1.1.2.3.cmml">a</mi></mrow><mo id="S3.SS2.p3.2.m2.1.1.3" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.3.cmml">→</mo><mrow id="S3.SS2.p3.2.m2.1.1.4" xref="S3.SS2.p3.2.m2.1.1.4.cmml"><mn id="S3.SS2.p3.2.m2.1.1.4.2" xref="S3.SS2.p3.2.m2.1.1.4.2.cmml">2</mn><mo id="S3.SS2.p3.2.m2.1.1.4.1" xref="S3.SS2.p3.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.4.3" xref="S3.SS2.p3.2.m2.1.1.4.3.cmml">a</mi></mrow><mo id="S3.SS2.p3.2.m2.1.1.5" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.5.cmml">→</mo><mrow id="S3.SS2.p3.2.m2.1.1.6" xref="S3.SS2.p3.2.m2.1.1.6.cmml"><mn id="S3.SS2.p3.2.m2.1.1.6.2" xref="S3.SS2.p3.2.m2.1.1.6.2.cmml">1</mn><mo id="S3.SS2.p3.2.m2.1.1.6.1" xref="S3.SS2.p3.2.m2.1.1.6.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.6.3" xref="S3.SS2.p3.2.m2.1.1.6.3.cmml">e</mi></mrow><mo id="S3.SS2.p3.2.m2.1.1.7" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.7.cmml">→</mo><mrow id="S3.SS2.p3.2.m2.1.1.8" xref="S3.SS2.p3.2.m2.1.1.8.cmml"><mn id="S3.SS2.p3.2.m2.1.1.8.2" xref="S3.SS2.p3.2.m2.1.1.8.2.cmml">3</mn><mo id="S3.SS2.p3.2.m2.1.1.8.1" xref="S3.SS2.p3.2.m2.1.1.8.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.8.3" xref="S3.SS2.p3.2.m2.1.1.8.3.cmml">a</mi></mrow><mo id="S3.SS2.p3.2.m2.1.1.9" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.9.cmml">→</mo><mrow id="S3.SS2.p3.2.m2.1.1.10" xref="S3.SS2.p3.2.m2.1.1.10.cmml"><mn id="S3.SS2.p3.2.m2.1.1.10.2" xref="S3.SS2.p3.2.m2.1.1.10.2.cmml">2</mn><mo id="S3.SS2.p3.2.m2.1.1.10.1" xref="S3.SS2.p3.2.m2.1.1.10.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.10.3" xref="S3.SS2.p3.2.m2.1.1.10.3.cmml">b</mi></mrow><mo id="S3.SS2.p3.2.m2.1.1.11" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.11.cmml">→</mo><mrow id="S3.SS2.p3.2.m2.1.1.12" xref="S3.SS2.p3.2.m2.1.1.12.cmml"><mn id="S3.SS2.p3.2.m2.1.1.12.2" xref="S3.SS2.p3.2.m2.1.1.12.2.cmml">1</mn><mo id="S3.SS2.p3.2.m2.1.1.12.1" xref="S3.SS2.p3.2.m2.1.1.12.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.12.3" xref="S3.SS2.p3.2.m2.1.1.12.3.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><and id="S3.SS2.p3.2.m2.1.1a.cmml" xref="S3.SS2.p3.2.m2.1.1"></and><apply id="S3.SS2.p3.2.m2.1.1b.cmml" xref="S3.SS2.p3.2.m2.1.1"><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">→</ci><apply id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2"><times id="S3.SS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1.2.1"></times><cn id="S3.SS2.p3.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.2.2">1</cn><ci id="S3.SS2.p3.2.m2.1.1.2.3.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3">𝑎</ci></apply><apply id="S3.SS2.p3.2.m2.1.1.4.cmml" xref="S3.SS2.p3.2.m2.1.1.4"><times id="S3.SS2.p3.2.m2.1.1.4.1.cmml" xref="S3.SS2.p3.2.m2.1.1.4.1"></times><cn id="S3.SS2.p3.2.m2.1.1.4.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.4.2">2</cn><ci id="S3.SS2.p3.2.m2.1.1.4.3.cmml" xref="S3.SS2.p3.2.m2.1.1.4.3">𝑎</ci></apply></apply><apply id="S3.SS2.p3.2.m2.1.1c.cmml" xref="S3.SS2.p3.2.m2.1.1"><ci id="S3.SS2.p3.2.m2.1.1.5.cmml" xref="S3.SS2.p3.2.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p3.2.m2.1.1.4.cmml" id="S3.SS2.p3.2.m2.1.1d.cmml" xref="S3.SS2.p3.2.m2.1.1"></share><apply id="S3.SS2.p3.2.m2.1.1.6.cmml" xref="S3.SS2.p3.2.m2.1.1.6"><times id="S3.SS2.p3.2.m2.1.1.6.1.cmml" xref="S3.SS2.p3.2.m2.1.1.6.1"></times><cn id="S3.SS2.p3.2.m2.1.1.6.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.6.2">1</cn><ci id="S3.SS2.p3.2.m2.1.1.6.3.cmml" xref="S3.SS2.p3.2.m2.1.1.6.3">𝑒</ci></apply></apply><apply id="S3.SS2.p3.2.m2.1.1e.cmml" xref="S3.SS2.p3.2.m2.1.1"><ci id="S3.SS2.p3.2.m2.1.1.7.cmml" xref="S3.SS2.p3.2.m2.1.1.7">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p3.2.m2.1.1.6.cmml" id="S3.SS2.p3.2.m2.1.1f.cmml" xref="S3.SS2.p3.2.m2.1.1"></share><apply id="S3.SS2.p3.2.m2.1.1.8.cmml" xref="S3.SS2.p3.2.m2.1.1.8"><times id="S3.SS2.p3.2.m2.1.1.8.1.cmml" xref="S3.SS2.p3.2.m2.1.1.8.1"></times><cn id="S3.SS2.p3.2.m2.1.1.8.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.8.2">3</cn><ci id="S3.SS2.p3.2.m2.1.1.8.3.cmml" xref="S3.SS2.p3.2.m2.1.1.8.3">𝑎</ci></apply></apply><apply id="S3.SS2.p3.2.m2.1.1g.cmml" xref="S3.SS2.p3.2.m2.1.1"><ci id="S3.SS2.p3.2.m2.1.1.9.cmml" xref="S3.SS2.p3.2.m2.1.1.9">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p3.2.m2.1.1.8.cmml" id="S3.SS2.p3.2.m2.1.1h.cmml" xref="S3.SS2.p3.2.m2.1.1"></share><apply id="S3.SS2.p3.2.m2.1.1.10.cmml" xref="S3.SS2.p3.2.m2.1.1.10"><times id="S3.SS2.p3.2.m2.1.1.10.1.cmml" xref="S3.SS2.p3.2.m2.1.1.10.1"></times><cn id="S3.SS2.p3.2.m2.1.1.10.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.10.2">2</cn><ci id="S3.SS2.p3.2.m2.1.1.10.3.cmml" xref="S3.SS2.p3.2.m2.1.1.10.3">𝑏</ci></apply></apply><apply id="S3.SS2.p3.2.m2.1.1i.cmml" xref="S3.SS2.p3.2.m2.1.1"><ci id="S3.SS2.p3.2.m2.1.1.11.cmml" xref="S3.SS2.p3.2.m2.1.1.11">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p3.2.m2.1.1.10.cmml" id="S3.SS2.p3.2.m2.1.1j.cmml" xref="S3.SS2.p3.2.m2.1.1"></share><apply id="S3.SS2.p3.2.m2.1.1.12.cmml" xref="S3.SS2.p3.2.m2.1.1.12"><times id="S3.SS2.p3.2.m2.1.1.12.1.cmml" xref="S3.SS2.p3.2.m2.1.1.12.1"></times><cn id="S3.SS2.p3.2.m2.1.1.12.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.12.2">1</cn><ci id="S3.SS2.p3.2.m2.1.1.12.3.cmml" xref="S3.SS2.p3.2.m2.1.1.12.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">1a\rightarrow 2a\rightarrow 1e\rightarrow 3a\rightarrow 2b\rightarrow 1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">1 italic_a → 2 italic_a → 1 italic_e → 3 italic_a → 2 italic_b → 1 italic_d</annotation></semantics></math>. This path (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.F4.sf1" title="In Figure 4 ‣ 3.2 Image Text Synthesis in CtrlSynth ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>) means CtrlSynth generates a new text that describes the original image. The synthetic text <math alttext="1d" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mn id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">1</mn><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></times><cn id="S3.SS2.p3.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1.2">1</cn><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">1 italic_d</annotation></semantics></math> may not align with the semantics in the original image since the LLM can create new combinations of the visual tags and add information that does not exist in the image. Such new information provides useful semantic augmentation over the original image while containing similar visual concepts.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_block" id="S3.F4.1">
<figure class="ltx_figure ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F4.sf1.g1" src="x2.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Synthesis path <math alttext="SP(1)" class="ltx_Math" display="inline" id="S3.F4.sf1.2.m1.1"><semantics id="S3.F4.sf1.2.m1.1b"><mrow id="S3.F4.sf1.2.m1.1.2" xref="S3.F4.sf1.2.m1.1.2.cmml"><mi id="S3.F4.sf1.2.m1.1.2.2" xref="S3.F4.sf1.2.m1.1.2.2.cmml">S</mi><mo id="S3.F4.sf1.2.m1.1.2.1" xref="S3.F4.sf1.2.m1.1.2.1.cmml">⁢</mo><mi id="S3.F4.sf1.2.m1.1.2.3" xref="S3.F4.sf1.2.m1.1.2.3.cmml">P</mi><mo id="S3.F4.sf1.2.m1.1.2.1b" xref="S3.F4.sf1.2.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F4.sf1.2.m1.1.2.4.2" xref="S3.F4.sf1.2.m1.1.2.cmml"><mo id="S3.F4.sf1.2.m1.1.2.4.2.1" stretchy="false" xref="S3.F4.sf1.2.m1.1.2.cmml">(</mo><mn id="S3.F4.sf1.2.m1.1.1" xref="S3.F4.sf1.2.m1.1.1.cmml">1</mn><mo id="S3.F4.sf1.2.m1.1.2.4.2.2" stretchy="false" xref="S3.F4.sf1.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.sf1.2.m1.1c"><apply id="S3.F4.sf1.2.m1.1.2.cmml" xref="S3.F4.sf1.2.m1.1.2"><times id="S3.F4.sf1.2.m1.1.2.1.cmml" xref="S3.F4.sf1.2.m1.1.2.1"></times><ci id="S3.F4.sf1.2.m1.1.2.2.cmml" xref="S3.F4.sf1.2.m1.1.2.2">𝑆</ci><ci id="S3.F4.sf1.2.m1.1.2.3.cmml" xref="S3.F4.sf1.2.m1.1.2.3">𝑃</ci><cn id="S3.F4.sf1.2.m1.1.1.cmml" type="integer" xref="S3.F4.sf1.2.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf1.2.m1.1d">SP(1)</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf1.2.m1.1e">italic_S italic_P ( 1 )</annotation></semantics></math></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F4.sf2.g1" src="x3.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Synthesis path <math alttext="SP(2)" class="ltx_Math" display="inline" id="S3.F4.sf2.2.m1.1"><semantics id="S3.F4.sf2.2.m1.1b"><mrow id="S3.F4.sf2.2.m1.1.2" xref="S3.F4.sf2.2.m1.1.2.cmml"><mi id="S3.F4.sf2.2.m1.1.2.2" xref="S3.F4.sf2.2.m1.1.2.2.cmml">S</mi><mo id="S3.F4.sf2.2.m1.1.2.1" xref="S3.F4.sf2.2.m1.1.2.1.cmml">⁢</mo><mi id="S3.F4.sf2.2.m1.1.2.3" xref="S3.F4.sf2.2.m1.1.2.3.cmml">P</mi><mo id="S3.F4.sf2.2.m1.1.2.1b" xref="S3.F4.sf2.2.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F4.sf2.2.m1.1.2.4.2" xref="S3.F4.sf2.2.m1.1.2.cmml"><mo id="S3.F4.sf2.2.m1.1.2.4.2.1" stretchy="false" xref="S3.F4.sf2.2.m1.1.2.cmml">(</mo><mn id="S3.F4.sf2.2.m1.1.1" xref="S3.F4.sf2.2.m1.1.1.cmml">2</mn><mo id="S3.F4.sf2.2.m1.1.2.4.2.2" stretchy="false" xref="S3.F4.sf2.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.sf2.2.m1.1c"><apply id="S3.F4.sf2.2.m1.1.2.cmml" xref="S3.F4.sf2.2.m1.1.2"><times id="S3.F4.sf2.2.m1.1.2.1.cmml" xref="S3.F4.sf2.2.m1.1.2.1"></times><ci id="S3.F4.sf2.2.m1.1.2.2.cmml" xref="S3.F4.sf2.2.m1.1.2.2">𝑆</ci><ci id="S3.F4.sf2.2.m1.1.2.3.cmml" xref="S3.F4.sf2.2.m1.1.2.3">𝑃</ci><cn id="S3.F4.sf2.2.m1.1.1.cmml" type="integer" xref="S3.F4.sf2.2.m1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf2.2.m1.1d">SP(2)</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf2.2.m1.1e">italic_S italic_P ( 2 )</annotation></semantics></math></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S3.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F4.sf3.g1" src="x4.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Synthesis path <math alttext="SP(3)" class="ltx_Math" display="inline" id="S3.F4.sf3.2.m1.1"><semantics id="S3.F4.sf3.2.m1.1b"><mrow id="S3.F4.sf3.2.m1.1.2" xref="S3.F4.sf3.2.m1.1.2.cmml"><mi id="S3.F4.sf3.2.m1.1.2.2" xref="S3.F4.sf3.2.m1.1.2.2.cmml">S</mi><mo id="S3.F4.sf3.2.m1.1.2.1" xref="S3.F4.sf3.2.m1.1.2.1.cmml">⁢</mo><mi id="S3.F4.sf3.2.m1.1.2.3" xref="S3.F4.sf3.2.m1.1.2.3.cmml">P</mi><mo id="S3.F4.sf3.2.m1.1.2.1b" xref="S3.F4.sf3.2.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F4.sf3.2.m1.1.2.4.2" xref="S3.F4.sf3.2.m1.1.2.cmml"><mo id="S3.F4.sf3.2.m1.1.2.4.2.1" stretchy="false" xref="S3.F4.sf3.2.m1.1.2.cmml">(</mo><mn id="S3.F4.sf3.2.m1.1.1" xref="S3.F4.sf3.2.m1.1.1.cmml">3</mn><mo id="S3.F4.sf3.2.m1.1.2.4.2.2" stretchy="false" xref="S3.F4.sf3.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.sf3.2.m1.1c"><apply id="S3.F4.sf3.2.m1.1.2.cmml" xref="S3.F4.sf3.2.m1.1.2"><times id="S3.F4.sf3.2.m1.1.2.1.cmml" xref="S3.F4.sf3.2.m1.1.2.1"></times><ci id="S3.F4.sf3.2.m1.1.2.2.cmml" xref="S3.F4.sf3.2.m1.1.2.2">𝑆</ci><ci id="S3.F4.sf3.2.m1.1.2.3.cmml" xref="S3.F4.sf3.2.m1.1.2.3">𝑃</ci><cn id="S3.F4.sf3.2.m1.1.1.cmml" type="integer" xref="S3.F4.sf3.2.m1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf3.2.m1.1d">SP(3)</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf3.2.m1.1e">italic_S italic_P ( 3 )</annotation></semantics></math></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S3.F4.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F4.sf4.g1" src="x5.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Synthesis path <math alttext="SP(4)" class="ltx_Math" display="inline" id="S3.F4.sf4.2.m1.1"><semantics id="S3.F4.sf4.2.m1.1b"><mrow id="S3.F4.sf4.2.m1.1.2" xref="S3.F4.sf4.2.m1.1.2.cmml"><mi id="S3.F4.sf4.2.m1.1.2.2" xref="S3.F4.sf4.2.m1.1.2.2.cmml">S</mi><mo id="S3.F4.sf4.2.m1.1.2.1" xref="S3.F4.sf4.2.m1.1.2.1.cmml">⁢</mo><mi id="S3.F4.sf4.2.m1.1.2.3" xref="S3.F4.sf4.2.m1.1.2.3.cmml">P</mi><mo id="S3.F4.sf4.2.m1.1.2.1b" xref="S3.F4.sf4.2.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F4.sf4.2.m1.1.2.4.2" xref="S3.F4.sf4.2.m1.1.2.cmml"><mo id="S3.F4.sf4.2.m1.1.2.4.2.1" stretchy="false" xref="S3.F4.sf4.2.m1.1.2.cmml">(</mo><mn id="S3.F4.sf4.2.m1.1.1" xref="S3.F4.sf4.2.m1.1.1.cmml">4</mn><mo id="S3.F4.sf4.2.m1.1.2.4.2.2" stretchy="false" xref="S3.F4.sf4.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.sf4.2.m1.1c"><apply id="S3.F4.sf4.2.m1.1.2.cmml" xref="S3.F4.sf4.2.m1.1.2"><times id="S3.F4.sf4.2.m1.1.2.1.cmml" xref="S3.F4.sf4.2.m1.1.2.1"></times><ci id="S3.F4.sf4.2.m1.1.2.2.cmml" xref="S3.F4.sf4.2.m1.1.2.2">𝑆</ci><ci id="S3.F4.sf4.2.m1.1.2.3.cmml" xref="S3.F4.sf4.2.m1.1.2.3">𝑃</ci><cn id="S3.F4.sf4.2.m1.1.1.cmml" type="integer" xref="S3.F4.sf4.2.m1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf4.2.m1.1d">SP(4)</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf4.2.m1.1e">italic_S italic_P ( 4 )</annotation></semantics></math></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Different synthesis paths in CtrlSynth.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.2"><math alttext="SP(2)" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.2" xref="S3.SS2.p4.1.m1.1.2.cmml"><mi id="S3.SS2.p4.1.m1.1.2.2" xref="S3.SS2.p4.1.m1.1.2.2.cmml">S</mi><mo id="S3.SS2.p4.1.m1.1.2.1" xref="S3.SS2.p4.1.m1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.m1.1.2.3" xref="S3.SS2.p4.1.m1.1.2.3.cmml">P</mi><mo id="S3.SS2.p4.1.m1.1.2.1a" xref="S3.SS2.p4.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p4.1.m1.1.2.4.2" xref="S3.SS2.p4.1.m1.1.2.cmml"><mo id="S3.SS2.p4.1.m1.1.2.4.2.1" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.cmml">(</mo><mn id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">2</mn><mo id="S3.SS2.p4.1.m1.1.2.4.2.2" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.2"><times id="S3.SS2.p4.1.m1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.1"></times><ci id="S3.SS2.p4.1.m1.1.2.2.cmml" xref="S3.SS2.p4.1.m1.1.2.2">𝑆</ci><ci id="S3.SS2.p4.1.m1.1.2.3.cmml" xref="S3.SS2.p4.1.m1.1.2.3">𝑃</ci><cn id="S3.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">SP(2)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_S italic_P ( 2 )</annotation></semantics></math>: <math alttext="1a\rightarrow 2a\rightarrow 1e\xrightarrow{1b}3a\rightarrow 2b\rightarrow 1d" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mrow id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml"><mn id="S3.SS2.p4.2.m2.1.1.2.2" xref="S3.SS2.p4.2.m2.1.1.2.2.cmml">1</mn><mo id="S3.SS2.p4.2.m2.1.1.2.1" xref="S3.SS2.p4.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.2.3" xref="S3.SS2.p4.2.m2.1.1.2.3.cmml">a</mi></mrow><mo id="S3.SS2.p4.2.m2.1.1.3" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.3.cmml">→</mo><mrow id="S3.SS2.p4.2.m2.1.1.4" xref="S3.SS2.p4.2.m2.1.1.4.cmml"><mn id="S3.SS2.p4.2.m2.1.1.4.2" xref="S3.SS2.p4.2.m2.1.1.4.2.cmml">2</mn><mo id="S3.SS2.p4.2.m2.1.1.4.1" xref="S3.SS2.p4.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.4.3" xref="S3.SS2.p4.2.m2.1.1.4.3.cmml">a</mi></mrow><mo id="S3.SS2.p4.2.m2.1.1.5" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.5.cmml">→</mo><mrow id="S3.SS2.p4.2.m2.1.1.6" xref="S3.SS2.p4.2.m2.1.1.6.cmml"><mn id="S3.SS2.p4.2.m2.1.1.6.2" xref="S3.SS2.p4.2.m2.1.1.6.2.cmml">1</mn><mo id="S3.SS2.p4.2.m2.1.1.6.1" xref="S3.SS2.p4.2.m2.1.1.6.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.6.3" xref="S3.SS2.p4.2.m2.1.1.6.3.cmml">e</mi></mrow><mover accent="true" id="S3.SS2.p4.2.m2.1.1.7" xref="S3.SS2.p4.2.m2.1.1.7.cmml"><mo id="S3.SS2.p4.2.m2.1.1.7.2" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.7.2.cmml">→</mo><mrow id="S3.SS2.p4.2.m2.1.1.7.1" xref="S3.SS2.p4.2.m2.1.1.7.1.cmml"><mn id="S3.SS2.p4.2.m2.1.1.7.1.2" xref="S3.SS2.p4.2.m2.1.1.7.1.2.cmml">1</mn><mo id="S3.SS2.p4.2.m2.1.1.7.1.1" xref="S3.SS2.p4.2.m2.1.1.7.1.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.7.1.3" xref="S3.SS2.p4.2.m2.1.1.7.1.3.cmml">b</mi></mrow></mover><mrow id="S3.SS2.p4.2.m2.1.1.8" xref="S3.SS2.p4.2.m2.1.1.8.cmml"><mn id="S3.SS2.p4.2.m2.1.1.8.2" xref="S3.SS2.p4.2.m2.1.1.8.2.cmml">3</mn><mo id="S3.SS2.p4.2.m2.1.1.8.1" xref="S3.SS2.p4.2.m2.1.1.8.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.8.3" xref="S3.SS2.p4.2.m2.1.1.8.3.cmml">a</mi></mrow><mo id="S3.SS2.p4.2.m2.1.1.9" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.9.cmml">→</mo><mrow id="S3.SS2.p4.2.m2.1.1.10" xref="S3.SS2.p4.2.m2.1.1.10.cmml"><mn id="S3.SS2.p4.2.m2.1.1.10.2" xref="S3.SS2.p4.2.m2.1.1.10.2.cmml">2</mn><mo id="S3.SS2.p4.2.m2.1.1.10.1" xref="S3.SS2.p4.2.m2.1.1.10.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.10.3" xref="S3.SS2.p4.2.m2.1.1.10.3.cmml">b</mi></mrow><mo id="S3.SS2.p4.2.m2.1.1.11" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.11.cmml">→</mo><mrow id="S3.SS2.p4.2.m2.1.1.12" xref="S3.SS2.p4.2.m2.1.1.12.cmml"><mn id="S3.SS2.p4.2.m2.1.1.12.2" xref="S3.SS2.p4.2.m2.1.1.12.2.cmml">1</mn><mo id="S3.SS2.p4.2.m2.1.1.12.1" xref="S3.SS2.p4.2.m2.1.1.12.1.cmml">⁢</mo><mi id="S3.SS2.p4.2.m2.1.1.12.3" xref="S3.SS2.p4.2.m2.1.1.12.3.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><and id="S3.SS2.p4.2.m2.1.1a.cmml" xref="S3.SS2.p4.2.m2.1.1"></and><apply id="S3.SS2.p4.2.m2.1.1b.cmml" xref="S3.SS2.p4.2.m2.1.1"><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">→</ci><apply id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2"><times id="S3.SS2.p4.2.m2.1.1.2.1.cmml" xref="S3.SS2.p4.2.m2.1.1.2.1"></times><cn id="S3.SS2.p4.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.2.2">1</cn><ci id="S3.SS2.p4.2.m2.1.1.2.3.cmml" xref="S3.SS2.p4.2.m2.1.1.2.3">𝑎</ci></apply><apply id="S3.SS2.p4.2.m2.1.1.4.cmml" xref="S3.SS2.p4.2.m2.1.1.4"><times id="S3.SS2.p4.2.m2.1.1.4.1.cmml" xref="S3.SS2.p4.2.m2.1.1.4.1"></times><cn id="S3.SS2.p4.2.m2.1.1.4.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.4.2">2</cn><ci id="S3.SS2.p4.2.m2.1.1.4.3.cmml" xref="S3.SS2.p4.2.m2.1.1.4.3">𝑎</ci></apply></apply><apply id="S3.SS2.p4.2.m2.1.1c.cmml" xref="S3.SS2.p4.2.m2.1.1"><ci id="S3.SS2.p4.2.m2.1.1.5.cmml" xref="S3.SS2.p4.2.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p4.2.m2.1.1.4.cmml" id="S3.SS2.p4.2.m2.1.1d.cmml" xref="S3.SS2.p4.2.m2.1.1"></share><apply id="S3.SS2.p4.2.m2.1.1.6.cmml" xref="S3.SS2.p4.2.m2.1.1.6"><times id="S3.SS2.p4.2.m2.1.1.6.1.cmml" xref="S3.SS2.p4.2.m2.1.1.6.1"></times><cn id="S3.SS2.p4.2.m2.1.1.6.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.6.2">1</cn><ci id="S3.SS2.p4.2.m2.1.1.6.3.cmml" xref="S3.SS2.p4.2.m2.1.1.6.3">𝑒</ci></apply></apply><apply id="S3.SS2.p4.2.m2.1.1e.cmml" xref="S3.SS2.p4.2.m2.1.1"><apply id="S3.SS2.p4.2.m2.1.1.7.cmml" xref="S3.SS2.p4.2.m2.1.1.7"><apply id="S3.SS2.p4.2.m2.1.1.7.1.cmml" xref="S3.SS2.p4.2.m2.1.1.7.1"><times id="S3.SS2.p4.2.m2.1.1.7.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.7.1.1"></times><cn id="S3.SS2.p4.2.m2.1.1.7.1.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.7.1.2">1</cn><ci id="S3.SS2.p4.2.m2.1.1.7.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.7.1.3">𝑏</ci></apply><ci id="S3.SS2.p4.2.m2.1.1.7.2.cmml" xref="S3.SS2.p4.2.m2.1.1.7.2">→</ci></apply><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p4.2.m2.1.1.6.cmml" id="S3.SS2.p4.2.m2.1.1f.cmml" xref="S3.SS2.p4.2.m2.1.1"></share><apply id="S3.SS2.p4.2.m2.1.1.8.cmml" xref="S3.SS2.p4.2.m2.1.1.8"><times id="S3.SS2.p4.2.m2.1.1.8.1.cmml" xref="S3.SS2.p4.2.m2.1.1.8.1"></times><cn id="S3.SS2.p4.2.m2.1.1.8.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.8.2">3</cn><ci id="S3.SS2.p4.2.m2.1.1.8.3.cmml" xref="S3.SS2.p4.2.m2.1.1.8.3">𝑎</ci></apply></apply><apply id="S3.SS2.p4.2.m2.1.1g.cmml" xref="S3.SS2.p4.2.m2.1.1"><ci id="S3.SS2.p4.2.m2.1.1.9.cmml" xref="S3.SS2.p4.2.m2.1.1.9">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p4.2.m2.1.1.8.cmml" id="S3.SS2.p4.2.m2.1.1h.cmml" xref="S3.SS2.p4.2.m2.1.1"></share><apply id="S3.SS2.p4.2.m2.1.1.10.cmml" xref="S3.SS2.p4.2.m2.1.1.10"><times id="S3.SS2.p4.2.m2.1.1.10.1.cmml" xref="S3.SS2.p4.2.m2.1.1.10.1"></times><cn id="S3.SS2.p4.2.m2.1.1.10.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.10.2">2</cn><ci id="S3.SS2.p4.2.m2.1.1.10.3.cmml" xref="S3.SS2.p4.2.m2.1.1.10.3">𝑏</ci></apply></apply><apply id="S3.SS2.p4.2.m2.1.1i.cmml" xref="S3.SS2.p4.2.m2.1.1"><ci id="S3.SS2.p4.2.m2.1.1.11.cmml" xref="S3.SS2.p4.2.m2.1.1.11">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p4.2.m2.1.1.10.cmml" id="S3.SS2.p4.2.m2.1.1j.cmml" xref="S3.SS2.p4.2.m2.1.1"></share><apply id="S3.SS2.p4.2.m2.1.1.12.cmml" xref="S3.SS2.p4.2.m2.1.1.12"><times id="S3.SS2.p4.2.m2.1.1.12.1.cmml" xref="S3.SS2.p4.2.m2.1.1.12.1"></times><cn id="S3.SS2.p4.2.m2.1.1.12.2.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.12.2">1</cn><ci id="S3.SS2.p4.2.m2.1.1.12.3.cmml" xref="S3.SS2.p4.2.m2.1.1.12.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">1a\rightarrow 2a\rightarrow 1e\xrightarrow{1b}3a\rightarrow 2b\rightarrow 1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">1 italic_a → 2 italic_a → 1 italic_e start_ARROW start_OVERACCENT 1 italic_b end_OVERACCENT → end_ARROW 3 italic_a → 2 italic_b → 1 italic_d</annotation></semantics></math>. This path (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.F4.sf2" title="In Figure 4 ‣ 3.2 Image Text Synthesis in CtrlSynth ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>) is similar to the previous path but a key difference is that it constrains the synthetic text to be faithful<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Or the opposite depending on the user-specified policy</span></span></span> to an original text. We can consider it as using the VTM and LLM to synthesize an improved text over the original one. We will show later in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a> that text samples generated from this path outperform previous works <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib11" title="">2023</a>)</cite> that rewrite noisy captions. We include the example prompts to reflect the control policies in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS1" title="A.1 Control Policies ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.9"><math alttext="SP(3)" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.2" xref="S3.SS2.p5.1.m1.1.2.cmml"><mi id="S3.SS2.p5.1.m1.1.2.2" xref="S3.SS2.p5.1.m1.1.2.2.cmml">S</mi><mo id="S3.SS2.p5.1.m1.1.2.1" xref="S3.SS2.p5.1.m1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p5.1.m1.1.2.3" xref="S3.SS2.p5.1.m1.1.2.3.cmml">P</mi><mo id="S3.SS2.p5.1.m1.1.2.1a" xref="S3.SS2.p5.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p5.1.m1.1.2.4.2" xref="S3.SS2.p5.1.m1.1.2.cmml"><mo id="S3.SS2.p5.1.m1.1.2.4.2.1" stretchy="false" xref="S3.SS2.p5.1.m1.1.2.cmml">(</mo><mn id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">3</mn><mo id="S3.SS2.p5.1.m1.1.2.4.2.2" stretchy="false" xref="S3.SS2.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.2"><times id="S3.SS2.p5.1.m1.1.2.1.cmml" xref="S3.SS2.p5.1.m1.1.2.1"></times><ci id="S3.SS2.p5.1.m1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.1.2.2">𝑆</ci><ci id="S3.SS2.p5.1.m1.1.2.3.cmml" xref="S3.SS2.p5.1.m1.1.2.3">𝑃</ci><cn id="S3.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">SP(3)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_S italic_P ( 3 )</annotation></semantics></math>: <math alttext="1a\rightarrow 2a\rightarrow 1e\rightarrow 3a\rightarrow 2b\rightarrow 1d%
\rightarrow 3b\rightarrow 2c\rightarrow 1c" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mrow id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml"><mn id="S3.SS2.p5.2.m2.1.1.2.2" xref="S3.SS2.p5.2.m2.1.1.2.2.cmml">1</mn><mo id="S3.SS2.p5.2.m2.1.1.2.1" xref="S3.SS2.p5.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.2.3" xref="S3.SS2.p5.2.m2.1.1.2.3.cmml">a</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.3" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.3.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.4" xref="S3.SS2.p5.2.m2.1.1.4.cmml"><mn id="S3.SS2.p5.2.m2.1.1.4.2" xref="S3.SS2.p5.2.m2.1.1.4.2.cmml">2</mn><mo id="S3.SS2.p5.2.m2.1.1.4.1" xref="S3.SS2.p5.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.4.3" xref="S3.SS2.p5.2.m2.1.1.4.3.cmml">a</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.5" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.5.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.6" xref="S3.SS2.p5.2.m2.1.1.6.cmml"><mn id="S3.SS2.p5.2.m2.1.1.6.2" xref="S3.SS2.p5.2.m2.1.1.6.2.cmml">1</mn><mo id="S3.SS2.p5.2.m2.1.1.6.1" xref="S3.SS2.p5.2.m2.1.1.6.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.6.3" xref="S3.SS2.p5.2.m2.1.1.6.3.cmml">e</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.7" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.7.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.8" xref="S3.SS2.p5.2.m2.1.1.8.cmml"><mn id="S3.SS2.p5.2.m2.1.1.8.2" xref="S3.SS2.p5.2.m2.1.1.8.2.cmml">3</mn><mo id="S3.SS2.p5.2.m2.1.1.8.1" xref="S3.SS2.p5.2.m2.1.1.8.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.8.3" xref="S3.SS2.p5.2.m2.1.1.8.3.cmml">a</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.9" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.9.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.10" xref="S3.SS2.p5.2.m2.1.1.10.cmml"><mn id="S3.SS2.p5.2.m2.1.1.10.2" xref="S3.SS2.p5.2.m2.1.1.10.2.cmml">2</mn><mo id="S3.SS2.p5.2.m2.1.1.10.1" xref="S3.SS2.p5.2.m2.1.1.10.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.10.3" xref="S3.SS2.p5.2.m2.1.1.10.3.cmml">b</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.11" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.11.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.12" xref="S3.SS2.p5.2.m2.1.1.12.cmml"><mn id="S3.SS2.p5.2.m2.1.1.12.2" xref="S3.SS2.p5.2.m2.1.1.12.2.cmml">1</mn><mo id="S3.SS2.p5.2.m2.1.1.12.1" xref="S3.SS2.p5.2.m2.1.1.12.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.12.3" xref="S3.SS2.p5.2.m2.1.1.12.3.cmml">d</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.13" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.13.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.14" xref="S3.SS2.p5.2.m2.1.1.14.cmml"><mn id="S3.SS2.p5.2.m2.1.1.14.2" xref="S3.SS2.p5.2.m2.1.1.14.2.cmml">3</mn><mo id="S3.SS2.p5.2.m2.1.1.14.1" xref="S3.SS2.p5.2.m2.1.1.14.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.14.3" xref="S3.SS2.p5.2.m2.1.1.14.3.cmml">b</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.15" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.15.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.16" xref="S3.SS2.p5.2.m2.1.1.16.cmml"><mn id="S3.SS2.p5.2.m2.1.1.16.2" xref="S3.SS2.p5.2.m2.1.1.16.2.cmml">2</mn><mo id="S3.SS2.p5.2.m2.1.1.16.1" xref="S3.SS2.p5.2.m2.1.1.16.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.16.3" xref="S3.SS2.p5.2.m2.1.1.16.3.cmml">c</mi></mrow><mo id="S3.SS2.p5.2.m2.1.1.17" stretchy="false" xref="S3.SS2.p5.2.m2.1.1.17.cmml">→</mo><mrow id="S3.SS2.p5.2.m2.1.1.18" xref="S3.SS2.p5.2.m2.1.1.18.cmml"><mn id="S3.SS2.p5.2.m2.1.1.18.2" xref="S3.SS2.p5.2.m2.1.1.18.2.cmml">1</mn><mo id="S3.SS2.p5.2.m2.1.1.18.1" xref="S3.SS2.p5.2.m2.1.1.18.1.cmml">⁢</mo><mi id="S3.SS2.p5.2.m2.1.1.18.3" xref="S3.SS2.p5.2.m2.1.1.18.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><and id="S3.SS2.p5.2.m2.1.1a.cmml" xref="S3.SS2.p5.2.m2.1.1"></and><apply id="S3.SS2.p5.2.m2.1.1b.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">→</ci><apply id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2"><times id="S3.SS2.p5.2.m2.1.1.2.1.cmml" xref="S3.SS2.p5.2.m2.1.1.2.1"></times><cn id="S3.SS2.p5.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.2.2">1</cn><ci id="S3.SS2.p5.2.m2.1.1.2.3.cmml" xref="S3.SS2.p5.2.m2.1.1.2.3">𝑎</ci></apply><apply id="S3.SS2.p5.2.m2.1.1.4.cmml" xref="S3.SS2.p5.2.m2.1.1.4"><times id="S3.SS2.p5.2.m2.1.1.4.1.cmml" xref="S3.SS2.p5.2.m2.1.1.4.1"></times><cn id="S3.SS2.p5.2.m2.1.1.4.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.4.2">2</cn><ci id="S3.SS2.p5.2.m2.1.1.4.3.cmml" xref="S3.SS2.p5.2.m2.1.1.4.3">𝑎</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1c.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.5.cmml" xref="S3.SS2.p5.2.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.4.cmml" id="S3.SS2.p5.2.m2.1.1d.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.6.cmml" xref="S3.SS2.p5.2.m2.1.1.6"><times id="S3.SS2.p5.2.m2.1.1.6.1.cmml" xref="S3.SS2.p5.2.m2.1.1.6.1"></times><cn id="S3.SS2.p5.2.m2.1.1.6.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.6.2">1</cn><ci id="S3.SS2.p5.2.m2.1.1.6.3.cmml" xref="S3.SS2.p5.2.m2.1.1.6.3">𝑒</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1e.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.7.cmml" xref="S3.SS2.p5.2.m2.1.1.7">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.6.cmml" id="S3.SS2.p5.2.m2.1.1f.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.8.cmml" xref="S3.SS2.p5.2.m2.1.1.8"><times id="S3.SS2.p5.2.m2.1.1.8.1.cmml" xref="S3.SS2.p5.2.m2.1.1.8.1"></times><cn id="S3.SS2.p5.2.m2.1.1.8.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.8.2">3</cn><ci id="S3.SS2.p5.2.m2.1.1.8.3.cmml" xref="S3.SS2.p5.2.m2.1.1.8.3">𝑎</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1g.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.9.cmml" xref="S3.SS2.p5.2.m2.1.1.9">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.8.cmml" id="S3.SS2.p5.2.m2.1.1h.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.10.cmml" xref="S3.SS2.p5.2.m2.1.1.10"><times id="S3.SS2.p5.2.m2.1.1.10.1.cmml" xref="S3.SS2.p5.2.m2.1.1.10.1"></times><cn id="S3.SS2.p5.2.m2.1.1.10.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.10.2">2</cn><ci id="S3.SS2.p5.2.m2.1.1.10.3.cmml" xref="S3.SS2.p5.2.m2.1.1.10.3">𝑏</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1i.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.11.cmml" xref="S3.SS2.p5.2.m2.1.1.11">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.10.cmml" id="S3.SS2.p5.2.m2.1.1j.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.12.cmml" xref="S3.SS2.p5.2.m2.1.1.12"><times id="S3.SS2.p5.2.m2.1.1.12.1.cmml" xref="S3.SS2.p5.2.m2.1.1.12.1"></times><cn id="S3.SS2.p5.2.m2.1.1.12.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.12.2">1</cn><ci id="S3.SS2.p5.2.m2.1.1.12.3.cmml" xref="S3.SS2.p5.2.m2.1.1.12.3">𝑑</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1k.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.13.cmml" xref="S3.SS2.p5.2.m2.1.1.13">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.12.cmml" id="S3.SS2.p5.2.m2.1.1l.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.14.cmml" xref="S3.SS2.p5.2.m2.1.1.14"><times id="S3.SS2.p5.2.m2.1.1.14.1.cmml" xref="S3.SS2.p5.2.m2.1.1.14.1"></times><cn id="S3.SS2.p5.2.m2.1.1.14.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.14.2">3</cn><ci id="S3.SS2.p5.2.m2.1.1.14.3.cmml" xref="S3.SS2.p5.2.m2.1.1.14.3">𝑏</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1m.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.15.cmml" xref="S3.SS2.p5.2.m2.1.1.15">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.14.cmml" id="S3.SS2.p5.2.m2.1.1n.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.16.cmml" xref="S3.SS2.p5.2.m2.1.1.16"><times id="S3.SS2.p5.2.m2.1.1.16.1.cmml" xref="S3.SS2.p5.2.m2.1.1.16.1"></times><cn id="S3.SS2.p5.2.m2.1.1.16.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.16.2">2</cn><ci id="S3.SS2.p5.2.m2.1.1.16.3.cmml" xref="S3.SS2.p5.2.m2.1.1.16.3">𝑐</ci></apply></apply><apply id="S3.SS2.p5.2.m2.1.1o.cmml" xref="S3.SS2.p5.2.m2.1.1"><ci id="S3.SS2.p5.2.m2.1.1.17.cmml" xref="S3.SS2.p5.2.m2.1.1.17">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p5.2.m2.1.1.16.cmml" id="S3.SS2.p5.2.m2.1.1p.cmml" xref="S3.SS2.p5.2.m2.1.1"></share><apply id="S3.SS2.p5.2.m2.1.1.18.cmml" xref="S3.SS2.p5.2.m2.1.1.18"><times id="S3.SS2.p5.2.m2.1.1.18.1.cmml" xref="S3.SS2.p5.2.m2.1.1.18.1"></times><cn id="S3.SS2.p5.2.m2.1.1.18.2.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1.18.2">1</cn><ci id="S3.SS2.p5.2.m2.1.1.18.3.cmml" xref="S3.SS2.p5.2.m2.1.1.18.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">1a\rightarrow 2a\rightarrow 1e\rightarrow 3a\rightarrow 2b\rightarrow 1d%
\rightarrow 3b\rightarrow 2c\rightarrow 1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">1 italic_a → 2 italic_a → 1 italic_e → 3 italic_a → 2 italic_b → 1 italic_d → 3 italic_b → 2 italic_c → 1 italic_c</annotation></semantics></math>. This path (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.F4.sf3" title="In Figure 4 ‣ 3.2 Image Text Synthesis in CtrlSynth ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(c)</span></a>) provides both synthetic text (<math alttext="1d" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mn id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.3.m3.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><times id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1"></times><cn id="S3.SS2.p5.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.p5.3.m3.1.1.2">1</cn><ci id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">1 italic_d</annotation></semantics></math>) and image (<math alttext="1c" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mn id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.4.m4.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><times id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1"></times><cn id="S3.SS2.p5.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p5.4.m4.1.1.2">1</cn><ci id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">1 italic_c</annotation></semantics></math>) samples. <math alttext="1c" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m5.1"><semantics id="S3.SS2.p5.5.m5.1a"><mrow id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mn id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.5.m5.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><times id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1"></times><cn id="S3.SS2.p5.5.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p5.5.m5.1.1.2">1</cn><ci id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m5.1d">1 italic_c</annotation></semantics></math> can be an effective image sample that augments the original image (<math alttext="1a" class="ltx_Math" display="inline" id="S3.SS2.p5.6.m6.1"><semantics id="S3.SS2.p5.6.m6.1a"><mrow id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml"><mn id="S3.SS2.p5.6.m6.1.1.2" xref="S3.SS2.p5.6.m6.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.6.m6.1.1.1" xref="S3.SS2.p5.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.6.m6.1.1.3" xref="S3.SS2.p5.6.m6.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><apply id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1"><times id="S3.SS2.p5.6.m6.1.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1.1"></times><cn id="S3.SS2.p5.6.m6.1.1.2.cmml" type="integer" xref="S3.SS2.p5.6.m6.1.1.2">1</cn><ci id="S3.SS2.p5.6.m6.1.1.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">1a</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.6.m6.1d">1 italic_a</annotation></semantics></math>) or can be paired with (<math alttext="1d" class="ltx_Math" display="inline" id="S3.SS2.p5.7.m7.1"><semantics id="S3.SS2.p5.7.m7.1a"><mrow id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml"><mn id="S3.SS2.p5.7.m7.1.1.2" xref="S3.SS2.p5.7.m7.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.7.m7.1.1.1" xref="S3.SS2.p5.7.m7.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.7.m7.1.1.3" xref="S3.SS2.p5.7.m7.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><apply id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1"><times id="S3.SS2.p5.7.m7.1.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1.1"></times><cn id="S3.SS2.p5.7.m7.1.1.2.cmml" type="integer" xref="S3.SS2.p5.7.m7.1.1.2">1</cn><ci id="S3.SS2.p5.7.m7.1.1.3.cmml" xref="S3.SS2.p5.7.m7.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.7.m7.1d">1 italic_d</annotation></semantics></math>) to augment the original image-text pair (<math alttext="1a" class="ltx_Math" display="inline" id="S3.SS2.p5.8.m8.1"><semantics id="S3.SS2.p5.8.m8.1a"><mrow id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml"><mn id="S3.SS2.p5.8.m8.1.1.2" xref="S3.SS2.p5.8.m8.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.8.m8.1.1.1" xref="S3.SS2.p5.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.8.m8.1.1.3" xref="S3.SS2.p5.8.m8.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><apply id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1"><times id="S3.SS2.p5.8.m8.1.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1.1"></times><cn id="S3.SS2.p5.8.m8.1.1.2.cmml" type="integer" xref="S3.SS2.p5.8.m8.1.1.2">1</cn><ci id="S3.SS2.p5.8.m8.1.1.3.cmml" xref="S3.SS2.p5.8.m8.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">1a</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.8.m8.1d">1 italic_a</annotation></semantics></math> and <math alttext="1b" class="ltx_Math" display="inline" id="S3.SS2.p5.9.m9.1"><semantics id="S3.SS2.p5.9.m9.1a"><mrow id="S3.SS2.p5.9.m9.1.1" xref="S3.SS2.p5.9.m9.1.1.cmml"><mn id="S3.SS2.p5.9.m9.1.1.2" xref="S3.SS2.p5.9.m9.1.1.2.cmml">1</mn><mo id="S3.SS2.p5.9.m9.1.1.1" xref="S3.SS2.p5.9.m9.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p5.9.m9.1.1.3" xref="S3.SS2.p5.9.m9.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m9.1b"><apply id="S3.SS2.p5.9.m9.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1"><times id="S3.SS2.p5.9.m9.1.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1.1"></times><cn id="S3.SS2.p5.9.m9.1.1.2.cmml" type="integer" xref="S3.SS2.p5.9.m9.1.1.2">1</cn><ci id="S3.SS2.p5.9.m9.1.1.3.cmml" xref="S3.SS2.p5.9.m9.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m9.1c">1b</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.9.m9.1d">1 italic_b</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.3"><math alttext="SP(4)" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.2" xref="S3.SS2.p6.1.m1.1.2.cmml"><mi id="S3.SS2.p6.1.m1.1.2.2" xref="S3.SS2.p6.1.m1.1.2.2.cmml">S</mi><mo id="S3.SS2.p6.1.m1.1.2.1" xref="S3.SS2.p6.1.m1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p6.1.m1.1.2.3" xref="S3.SS2.p6.1.m1.1.2.3.cmml">P</mi><mo id="S3.SS2.p6.1.m1.1.2.1a" xref="S3.SS2.p6.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p6.1.m1.1.2.4.2" xref="S3.SS2.p6.1.m1.1.2.cmml"><mo id="S3.SS2.p6.1.m1.1.2.4.2.1" stretchy="false" xref="S3.SS2.p6.1.m1.1.2.cmml">(</mo><mn id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">4</mn><mo id="S3.SS2.p6.1.m1.1.2.4.2.2" stretchy="false" xref="S3.SS2.p6.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.2"><times id="S3.SS2.p6.1.m1.1.2.1.cmml" xref="S3.SS2.p6.1.m1.1.2.1"></times><ci id="S3.SS2.p6.1.m1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.2.2">𝑆</ci><ci id="S3.SS2.p6.1.m1.1.2.3.cmml" xref="S3.SS2.p6.1.m1.1.2.3">𝑃</ci><cn id="S3.SS2.p6.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p6.1.m1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">SP(4)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_S italic_P ( 4 )</annotation></semantics></math>: <math alttext="1b\rightarrow 3b\rightarrow 2c\rightarrow 1c" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><mrow id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mrow id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml"><mn id="S3.SS2.p6.2.m2.1.1.2.2" xref="S3.SS2.p6.2.m2.1.1.2.2.cmml">1</mn><mo id="S3.SS2.p6.2.m2.1.1.2.1" xref="S3.SS2.p6.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.2.3" xref="S3.SS2.p6.2.m2.1.1.2.3.cmml">b</mi></mrow><mo id="S3.SS2.p6.2.m2.1.1.3" stretchy="false" xref="S3.SS2.p6.2.m2.1.1.3.cmml">→</mo><mrow id="S3.SS2.p6.2.m2.1.1.4" xref="S3.SS2.p6.2.m2.1.1.4.cmml"><mn id="S3.SS2.p6.2.m2.1.1.4.2" xref="S3.SS2.p6.2.m2.1.1.4.2.cmml">3</mn><mo id="S3.SS2.p6.2.m2.1.1.4.1" xref="S3.SS2.p6.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.4.3" xref="S3.SS2.p6.2.m2.1.1.4.3.cmml">b</mi></mrow><mo id="S3.SS2.p6.2.m2.1.1.5" stretchy="false" xref="S3.SS2.p6.2.m2.1.1.5.cmml">→</mo><mrow id="S3.SS2.p6.2.m2.1.1.6" xref="S3.SS2.p6.2.m2.1.1.6.cmml"><mn id="S3.SS2.p6.2.m2.1.1.6.2" xref="S3.SS2.p6.2.m2.1.1.6.2.cmml">2</mn><mo id="S3.SS2.p6.2.m2.1.1.6.1" xref="S3.SS2.p6.2.m2.1.1.6.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.6.3" xref="S3.SS2.p6.2.m2.1.1.6.3.cmml">c</mi></mrow><mo id="S3.SS2.p6.2.m2.1.1.7" stretchy="false" xref="S3.SS2.p6.2.m2.1.1.7.cmml">→</mo><mrow id="S3.SS2.p6.2.m2.1.1.8" xref="S3.SS2.p6.2.m2.1.1.8.cmml"><mn id="S3.SS2.p6.2.m2.1.1.8.2" xref="S3.SS2.p6.2.m2.1.1.8.2.cmml">1</mn><mo id="S3.SS2.p6.2.m2.1.1.8.1" xref="S3.SS2.p6.2.m2.1.1.8.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.8.3" xref="S3.SS2.p6.2.m2.1.1.8.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><and id="S3.SS2.p6.2.m2.1.1a.cmml" xref="S3.SS2.p6.2.m2.1.1"></and><apply id="S3.SS2.p6.2.m2.1.1b.cmml" xref="S3.SS2.p6.2.m2.1.1"><ci id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3">→</ci><apply id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2"><times id="S3.SS2.p6.2.m2.1.1.2.1.cmml" xref="S3.SS2.p6.2.m2.1.1.2.1"></times><cn id="S3.SS2.p6.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS2.p6.2.m2.1.1.2.2">1</cn><ci id="S3.SS2.p6.2.m2.1.1.2.3.cmml" xref="S3.SS2.p6.2.m2.1.1.2.3">𝑏</ci></apply><apply id="S3.SS2.p6.2.m2.1.1.4.cmml" xref="S3.SS2.p6.2.m2.1.1.4"><times id="S3.SS2.p6.2.m2.1.1.4.1.cmml" xref="S3.SS2.p6.2.m2.1.1.4.1"></times><cn id="S3.SS2.p6.2.m2.1.1.4.2.cmml" type="integer" xref="S3.SS2.p6.2.m2.1.1.4.2">3</cn><ci id="S3.SS2.p6.2.m2.1.1.4.3.cmml" xref="S3.SS2.p6.2.m2.1.1.4.3">𝑏</ci></apply></apply><apply id="S3.SS2.p6.2.m2.1.1c.cmml" xref="S3.SS2.p6.2.m2.1.1"><ci id="S3.SS2.p6.2.m2.1.1.5.cmml" xref="S3.SS2.p6.2.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p6.2.m2.1.1.4.cmml" id="S3.SS2.p6.2.m2.1.1d.cmml" xref="S3.SS2.p6.2.m2.1.1"></share><apply id="S3.SS2.p6.2.m2.1.1.6.cmml" xref="S3.SS2.p6.2.m2.1.1.6"><times id="S3.SS2.p6.2.m2.1.1.6.1.cmml" xref="S3.SS2.p6.2.m2.1.1.6.1"></times><cn id="S3.SS2.p6.2.m2.1.1.6.2.cmml" type="integer" xref="S3.SS2.p6.2.m2.1.1.6.2">2</cn><ci id="S3.SS2.p6.2.m2.1.1.6.3.cmml" xref="S3.SS2.p6.2.m2.1.1.6.3">𝑐</ci></apply></apply><apply id="S3.SS2.p6.2.m2.1.1e.cmml" xref="S3.SS2.p6.2.m2.1.1"><ci id="S3.SS2.p6.2.m2.1.1.7.cmml" xref="S3.SS2.p6.2.m2.1.1.7">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p6.2.m2.1.1.6.cmml" id="S3.SS2.p6.2.m2.1.1f.cmml" xref="S3.SS2.p6.2.m2.1.1"></share><apply id="S3.SS2.p6.2.m2.1.1.8.cmml" xref="S3.SS2.p6.2.m2.1.1.8"><times id="S3.SS2.p6.2.m2.1.1.8.1.cmml" xref="S3.SS2.p6.2.m2.1.1.8.1"></times><cn id="S3.SS2.p6.2.m2.1.1.8.2.cmml" type="integer" xref="S3.SS2.p6.2.m2.1.1.8.2">1</cn><ci id="S3.SS2.p6.2.m2.1.1.8.3.cmml" xref="S3.SS2.p6.2.m2.1.1.8.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">1b\rightarrow 3b\rightarrow 2c\rightarrow 1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">1 italic_b → 3 italic_b → 2 italic_c → 1 italic_c</annotation></semantics></math>. This path (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.F4.sf4" title="In Figure 4 ‣ 3.2 Image Text Synthesis in CtrlSynth ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(d)</span></a>) bypasses the language model and the original text is directly fed to the image controller and then generates a synthetic image (<math alttext="1c" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><mrow id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mn id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">1</mn><mo id="S3.SS2.p6.3.m3.1.1.1" xref="S3.SS2.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><times id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1.1"></times><cn id="S3.SS2.p6.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.p6.3.m3.1.1.2">1</cn><ci id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">1 italic_c</annotation></semantics></math>). The image sample could be a strong augmentation sample to the original image if the original text has a comprehensive and high-quality description.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">Note that CtrlSynth supports more synthesis paths that are not listed above. For example, one can start with original text and use LLM to add creative elements and generate synthetic text and further use it to generate an image, i.e. <math alttext="1b\rightarrow 3a\rightarrow 2b\rightarrow 1d\rightarrow 3b\rightarrow 2c%
\rightarrow 1c" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.1"><semantics id="S3.SS2.p7.1.m1.1a"><mrow id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml"><mrow id="S3.SS2.p7.1.m1.1.1.2" xref="S3.SS2.p7.1.m1.1.1.2.cmml"><mn id="S3.SS2.p7.1.m1.1.1.2.2" xref="S3.SS2.p7.1.m1.1.1.2.2.cmml">1</mn><mo id="S3.SS2.p7.1.m1.1.1.2.1" xref="S3.SS2.p7.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.2.3" xref="S3.SS2.p7.1.m1.1.1.2.3.cmml">b</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.3" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.3.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.4" xref="S3.SS2.p7.1.m1.1.1.4.cmml"><mn id="S3.SS2.p7.1.m1.1.1.4.2" xref="S3.SS2.p7.1.m1.1.1.4.2.cmml">3</mn><mo id="S3.SS2.p7.1.m1.1.1.4.1" xref="S3.SS2.p7.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.4.3" xref="S3.SS2.p7.1.m1.1.1.4.3.cmml">a</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.5" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.5.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.6" xref="S3.SS2.p7.1.m1.1.1.6.cmml"><mn id="S3.SS2.p7.1.m1.1.1.6.2" xref="S3.SS2.p7.1.m1.1.1.6.2.cmml">2</mn><mo id="S3.SS2.p7.1.m1.1.1.6.1" xref="S3.SS2.p7.1.m1.1.1.6.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.6.3" xref="S3.SS2.p7.1.m1.1.1.6.3.cmml">b</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.7" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.7.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.8" xref="S3.SS2.p7.1.m1.1.1.8.cmml"><mn id="S3.SS2.p7.1.m1.1.1.8.2" xref="S3.SS2.p7.1.m1.1.1.8.2.cmml">1</mn><mo id="S3.SS2.p7.1.m1.1.1.8.1" xref="S3.SS2.p7.1.m1.1.1.8.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.8.3" xref="S3.SS2.p7.1.m1.1.1.8.3.cmml">d</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.9" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.9.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.10" xref="S3.SS2.p7.1.m1.1.1.10.cmml"><mn id="S3.SS2.p7.1.m1.1.1.10.2" xref="S3.SS2.p7.1.m1.1.1.10.2.cmml">3</mn><mo id="S3.SS2.p7.1.m1.1.1.10.1" xref="S3.SS2.p7.1.m1.1.1.10.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.10.3" xref="S3.SS2.p7.1.m1.1.1.10.3.cmml">b</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.11" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.11.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.12" xref="S3.SS2.p7.1.m1.1.1.12.cmml"><mn id="S3.SS2.p7.1.m1.1.1.12.2" xref="S3.SS2.p7.1.m1.1.1.12.2.cmml">2</mn><mo id="S3.SS2.p7.1.m1.1.1.12.1" xref="S3.SS2.p7.1.m1.1.1.12.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.12.3" xref="S3.SS2.p7.1.m1.1.1.12.3.cmml">c</mi></mrow><mo id="S3.SS2.p7.1.m1.1.1.13" stretchy="false" xref="S3.SS2.p7.1.m1.1.1.13.cmml">→</mo><mrow id="S3.SS2.p7.1.m1.1.1.14" xref="S3.SS2.p7.1.m1.1.1.14.cmml"><mn id="S3.SS2.p7.1.m1.1.1.14.2" xref="S3.SS2.p7.1.m1.1.1.14.2.cmml">1</mn><mo id="S3.SS2.p7.1.m1.1.1.14.1" xref="S3.SS2.p7.1.m1.1.1.14.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.14.3" xref="S3.SS2.p7.1.m1.1.1.14.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1"><and id="S3.SS2.p7.1.m1.1.1a.cmml" xref="S3.SS2.p7.1.m1.1.1"></and><apply id="S3.SS2.p7.1.m1.1.1b.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3">→</ci><apply id="S3.SS2.p7.1.m1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.1.2"><times id="S3.SS2.p7.1.m1.1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.1.1.2.1"></times><cn id="S3.SS2.p7.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.2.2">1</cn><ci id="S3.SS2.p7.1.m1.1.1.2.3.cmml" xref="S3.SS2.p7.1.m1.1.1.2.3">𝑏</ci></apply><apply id="S3.SS2.p7.1.m1.1.1.4.cmml" xref="S3.SS2.p7.1.m1.1.1.4"><times id="S3.SS2.p7.1.m1.1.1.4.1.cmml" xref="S3.SS2.p7.1.m1.1.1.4.1"></times><cn id="S3.SS2.p7.1.m1.1.1.4.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.4.2">3</cn><ci id="S3.SS2.p7.1.m1.1.1.4.3.cmml" xref="S3.SS2.p7.1.m1.1.1.4.3">𝑎</ci></apply></apply><apply id="S3.SS2.p7.1.m1.1.1c.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.5.cmml" xref="S3.SS2.p7.1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p7.1.m1.1.1.4.cmml" id="S3.SS2.p7.1.m1.1.1d.cmml" xref="S3.SS2.p7.1.m1.1.1"></share><apply id="S3.SS2.p7.1.m1.1.1.6.cmml" xref="S3.SS2.p7.1.m1.1.1.6"><times id="S3.SS2.p7.1.m1.1.1.6.1.cmml" xref="S3.SS2.p7.1.m1.1.1.6.1"></times><cn id="S3.SS2.p7.1.m1.1.1.6.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.6.2">2</cn><ci id="S3.SS2.p7.1.m1.1.1.6.3.cmml" xref="S3.SS2.p7.1.m1.1.1.6.3">𝑏</ci></apply></apply><apply id="S3.SS2.p7.1.m1.1.1e.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.7.cmml" xref="S3.SS2.p7.1.m1.1.1.7">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p7.1.m1.1.1.6.cmml" id="S3.SS2.p7.1.m1.1.1f.cmml" xref="S3.SS2.p7.1.m1.1.1"></share><apply id="S3.SS2.p7.1.m1.1.1.8.cmml" xref="S3.SS2.p7.1.m1.1.1.8"><times id="S3.SS2.p7.1.m1.1.1.8.1.cmml" xref="S3.SS2.p7.1.m1.1.1.8.1"></times><cn id="S3.SS2.p7.1.m1.1.1.8.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.8.2">1</cn><ci id="S3.SS2.p7.1.m1.1.1.8.3.cmml" xref="S3.SS2.p7.1.m1.1.1.8.3">𝑑</ci></apply></apply><apply id="S3.SS2.p7.1.m1.1.1g.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.9.cmml" xref="S3.SS2.p7.1.m1.1.1.9">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p7.1.m1.1.1.8.cmml" id="S3.SS2.p7.1.m1.1.1h.cmml" xref="S3.SS2.p7.1.m1.1.1"></share><apply id="S3.SS2.p7.1.m1.1.1.10.cmml" xref="S3.SS2.p7.1.m1.1.1.10"><times id="S3.SS2.p7.1.m1.1.1.10.1.cmml" xref="S3.SS2.p7.1.m1.1.1.10.1"></times><cn id="S3.SS2.p7.1.m1.1.1.10.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.10.2">3</cn><ci id="S3.SS2.p7.1.m1.1.1.10.3.cmml" xref="S3.SS2.p7.1.m1.1.1.10.3">𝑏</ci></apply></apply><apply id="S3.SS2.p7.1.m1.1.1i.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.11.cmml" xref="S3.SS2.p7.1.m1.1.1.11">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p7.1.m1.1.1.10.cmml" id="S3.SS2.p7.1.m1.1.1j.cmml" xref="S3.SS2.p7.1.m1.1.1"></share><apply id="S3.SS2.p7.1.m1.1.1.12.cmml" xref="S3.SS2.p7.1.m1.1.1.12"><times id="S3.SS2.p7.1.m1.1.1.12.1.cmml" xref="S3.SS2.p7.1.m1.1.1.12.1"></times><cn id="S3.SS2.p7.1.m1.1.1.12.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.12.2">2</cn><ci id="S3.SS2.p7.1.m1.1.1.12.3.cmml" xref="S3.SS2.p7.1.m1.1.1.12.3">𝑐</ci></apply></apply><apply id="S3.SS2.p7.1.m1.1.1k.cmml" xref="S3.SS2.p7.1.m1.1.1"><ci id="S3.SS2.p7.1.m1.1.1.13.cmml" xref="S3.SS2.p7.1.m1.1.1.13">→</ci><share href="https://arxiv.org/html/2410.11963v1#S3.SS2.p7.1.m1.1.1.12.cmml" id="S3.SS2.p7.1.m1.1.1l.cmml" xref="S3.SS2.p7.1.m1.1.1"></share><apply id="S3.SS2.p7.1.m1.1.1.14.cmml" xref="S3.SS2.p7.1.m1.1.1.14"><times id="S3.SS2.p7.1.m1.1.1.14.1.cmml" xref="S3.SS2.p7.1.m1.1.1.14.1"></times><cn id="S3.SS2.p7.1.m1.1.1.14.2.cmml" type="integer" xref="S3.SS2.p7.1.m1.1.1.14.2">1</cn><ci id="S3.SS2.p7.1.m1.1.1.14.3.cmml" xref="S3.SS2.p7.1.m1.1.1.14.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">1b\rightarrow 3a\rightarrow 2b\rightarrow 1d\rightarrow 3b\rightarrow 2c%
\rightarrow 1c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.1d">1 italic_b → 3 italic_a → 2 italic_b → 1 italic_d → 3 italic_b → 2 italic_c → 1 italic_c</annotation></semantics></math>. Another category of examples includes starting with synthetic texts or images and creating more synthetic samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.5"><span class="ltx_text ltx_font_bold" id="S3.SS2.p8.5.1">Self-filtering for better synthetic data.</span> Synthetic samples often suffer from degraded quality especially when running at large scale. Synthetic systems often rely on heuristics or rule-based filtering techniques to filter out bad-quality samples. Because CtrlSynth pipeline is closed-loop, it implicitly provides self-filtering functionality. To check the quality of the synthetic text, we can detect if the synthetic text (<math alttext="1d" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.1"><semantics id="S3.SS2.p8.1.m1.1a"><mrow id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mn id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.p8.1.m1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><times id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1"></times><cn id="S3.SS2.p8.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p8.1.m1.1.1.2">1</cn><ci id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.1d">1 italic_d</annotation></semantics></math>) contains the visual tags (<math alttext="1e" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m2.1"><semantics id="S3.SS2.p8.2.m2.1a"><mrow id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mn id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">1</mn><mo id="S3.SS2.p8.2.m2.1.1.1" xref="S3.SS2.p8.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><times id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1.1"></times><cn id="S3.SS2.p8.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p8.2.m2.1.1.2">1</cn><ci id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">1e</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m2.1d">1 italic_e</annotation></semantics></math>), to filter out potentially misaligned or lower quality synthetic text samples, we define that at least some ratio <math alttext="p_{f}" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m3.1"><semantics id="S3.SS2.p8.3.m3.1a"><msub id="S3.SS2.p8.3.m3.1.1" xref="S3.SS2.p8.3.m3.1.1.cmml"><mi id="S3.SS2.p8.3.m3.1.1.2" xref="S3.SS2.p8.3.m3.1.1.2.cmml">p</mi><mi id="S3.SS2.p8.3.m3.1.1.3" xref="S3.SS2.p8.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.1b"><apply id="S3.SS2.p8.3.m3.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.3.m3.1.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p8.3.m3.1.1.2.cmml" xref="S3.SS2.p8.3.m3.1.1.2">𝑝</ci><ci id="S3.SS2.p8.3.m3.1.1.3.cmml" xref="S3.SS2.p8.3.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">p_{f}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> of the visual tags exist. For the synthetic image, we run it through the VTM again and output the visual tags, then we do the same check against the starting node text (<math alttext="1b" class="ltx_Math" display="inline" id="S3.SS2.p8.4.m4.1"><semantics id="S3.SS2.p8.4.m4.1a"><mrow id="S3.SS2.p8.4.m4.1.1" xref="S3.SS2.p8.4.m4.1.1.cmml"><mn id="S3.SS2.p8.4.m4.1.1.2" xref="S3.SS2.p8.4.m4.1.1.2.cmml">1</mn><mo id="S3.SS2.p8.4.m4.1.1.1" xref="S3.SS2.p8.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p8.4.m4.1.1.3" xref="S3.SS2.p8.4.m4.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.4.m4.1b"><apply id="S3.SS2.p8.4.m4.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1"><times id="S3.SS2.p8.4.m4.1.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1.1"></times><cn id="S3.SS2.p8.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p8.4.m4.1.1.2">1</cn><ci id="S3.SS2.p8.4.m4.1.1.3.cmml" xref="S3.SS2.p8.4.m4.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m4.1c">1b</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.4.m4.1d">1 italic_b</annotation></semantics></math> or <math alttext="1d" class="ltx_Math" display="inline" id="S3.SS2.p8.5.m5.1"><semantics id="S3.SS2.p8.5.m5.1a"><mrow id="S3.SS2.p8.5.m5.1.1" xref="S3.SS2.p8.5.m5.1.1.cmml"><mn id="S3.SS2.p8.5.m5.1.1.2" xref="S3.SS2.p8.5.m5.1.1.2.cmml">1</mn><mo id="S3.SS2.p8.5.m5.1.1.1" xref="S3.SS2.p8.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p8.5.m5.1.1.3" xref="S3.SS2.p8.5.m5.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.5.m5.1b"><apply id="S3.SS2.p8.5.m5.1.1.cmml" xref="S3.SS2.p8.5.m5.1.1"><times id="S3.SS2.p8.5.m5.1.1.1.cmml" xref="S3.SS2.p8.5.m5.1.1.1"></times><cn id="S3.SS2.p8.5.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p8.5.m5.1.1.2">1</cn><ci id="S3.SS2.p8.5.m5.1.1.3.cmml" xref="S3.SS2.p8.5.m5.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.5.m5.1c">1d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.5.m5.1d">1 italic_d</annotation></semantics></math>). Later in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.F7.sf1" title="In Figure 7 ‣ Effects of Self-Filtering. ‣ 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a>, we will show that self-filtering improves the synthetic samples.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Tasks and Datasets.</span> We adopt the CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib44" title="">2021</a>)</cite> model architecture for multimodal representation learning.
For pretraining CLIP models, we use two public image-text datasets: CC3M <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib49" title="">2018</a>)</cite> and CC12M <cite class="ltx_cite ltx_citemacro_citep">(Changpinyo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib3" title="">2021</a>)</cite>. To evaluate the representation quality of pretrained CLIP models, we measure the zero-shot performance on classification, retrieval, and compositional reasoning tasks. For image classification, we use 25 common vision datasets, including five ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib7" title="">2009</a>; Recht et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib45" title="">2019</a>)</cite> variants and the tasks from the VTAB benchmark <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib64" title="">2020</a>)</cite>. We list the detailed dataset information in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS2" title="A.2 Datasets Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>. We use COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib29" title="">2014</a>)</cite> and Flickr30k <cite class="ltx_cite ltx_citemacro_citep">(Plummer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib42" title="">2015</a>)</cite> for image-to-text and text-to-image retrieval tasks and report the metrics in recall@1. SugarCrepe <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib17" title="">2023</a>)</cite> is a recent benchmark that measures the compositional understanding of vision-language models, we report the zero-shot accuracy numbers. Additionally, to study the effects of CtrlSynth on long-tail tasks, we evaluate the task accuracy of Places-LT and ImageNet-LT datasets <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib30" title="">2019</a>)</cite> by augmenting the tail classes with CtrlSynth synthetic data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.6"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.6.1">Training and Baselines.</span> Note that CtrlSynth itself does not require any training. We conduct pretraining experiments on CLIP models to evaluate the quality of synthetic data. We use ViT-B/16 <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib8" title="">2020</a>)</cite> architecture for the CLIP vision backbone. For a fair comparison, we train all models for the same number of iterations on the original dataset (baseline) and the dataset mixed with CtrlSynth augmented samples.
We use CtrlSynth-cap to denote the original image and synthetic text pair <math alttext="(1a,1d)" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.2"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.2.2" xref="S4.SS1.p2.1.m1.2.2.3.cmml"><mo id="S4.SS1.p2.1.m1.2.2.2.3" stretchy="false" xref="S4.SS1.p2.1.m1.2.2.3.cmml">(</mo><mrow id="S4.SS1.p2.1.m1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.1.m1.1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p2.1.m1.1.1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.1.1.3.cmml">a</mi></mrow><mo id="S4.SS1.p2.1.m1.2.2.2.4" xref="S4.SS1.p2.1.m1.2.2.3.cmml">,</mo><mrow id="S4.SS1.p2.1.m1.2.2.2.2" xref="S4.SS1.p2.1.m1.2.2.2.2.cmml"><mn id="S4.SS1.p2.1.m1.2.2.2.2.2" xref="S4.SS1.p2.1.m1.2.2.2.2.2.cmml">1</mn><mo id="S4.SS1.p2.1.m1.2.2.2.2.1" xref="S4.SS1.p2.1.m1.2.2.2.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.1.m1.2.2.2.2.3" xref="S4.SS1.p2.1.m1.2.2.2.2.3.cmml">d</mi></mrow><mo id="S4.SS1.p2.1.m1.2.2.2.5" stretchy="false" xref="S4.SS1.p2.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><interval closure="open" id="S4.SS1.p2.1.m1.2.2.3.cmml" xref="S4.SS1.p2.1.m1.2.2.2"><apply id="S4.SS1.p2.1.m1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1"></times><cn id="S4.SS1.p2.1.m1.1.1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.1.1.2">1</cn><ci id="S4.SS1.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.3">𝑎</ci></apply><apply id="S4.SS1.p2.1.m1.2.2.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2"><times id="S4.SS1.p2.1.m1.2.2.2.2.1.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.1"></times><cn id="S4.SS1.p2.1.m1.2.2.2.2.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.2.2.2.2.2">1</cn><ci id="S4.SS1.p2.1.m1.2.2.2.2.3.cmml" xref="S4.SS1.p2.1.m1.2.2.2.2.3">𝑑</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">(1a,1d)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.2d">( 1 italic_a , 1 italic_d )</annotation></semantics></math> from synthesis path <math alttext="SP(1)" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.2" xref="S4.SS1.p2.2.m2.1.2.cmml"><mi id="S4.SS1.p2.2.m2.1.2.2" xref="S4.SS1.p2.2.m2.1.2.2.cmml">S</mi><mo id="S4.SS1.p2.2.m2.1.2.1" xref="S4.SS1.p2.2.m2.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.1.2.3" xref="S4.SS1.p2.2.m2.1.2.3.cmml">P</mi><mo id="S4.SS1.p2.2.m2.1.2.1a" xref="S4.SS1.p2.2.m2.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p2.2.m2.1.2.4.2" xref="S4.SS1.p2.2.m2.1.2.cmml"><mo id="S4.SS1.p2.2.m2.1.2.4.2.1" stretchy="false" xref="S4.SS1.p2.2.m2.1.2.cmml">(</mo><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S4.SS1.p2.2.m2.1.2.4.2.2" stretchy="false" xref="S4.SS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.2.cmml" xref="S4.SS1.p2.2.m2.1.2"><times id="S4.SS1.p2.2.m2.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.2.1"></times><ci id="S4.SS1.p2.2.m2.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.2.2">𝑆</ci><ci id="S4.SS1.p2.2.m2.1.2.3.cmml" xref="S4.SS1.p2.2.m2.1.2.3">𝑃</ci><cn id="S4.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">SP(1)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_S italic_P ( 1 )</annotation></semantics></math>. CtrlSynth-img stands for the synthetic image and original text pair <math alttext="(1b,1c)" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.2"><semantics id="S4.SS1.p2.3.m3.2a"><mrow id="S4.SS1.p2.3.m3.2.2.2" xref="S4.SS1.p2.3.m3.2.2.3.cmml"><mo id="S4.SS1.p2.3.m3.2.2.2.3" stretchy="false" xref="S4.SS1.p2.3.m3.2.2.3.cmml">(</mo><mrow id="S4.SS1.p2.3.m3.1.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.1.1.2" xref="S4.SS1.p2.3.m3.1.1.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.3.m3.1.1.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.1.1.3" xref="S4.SS1.p2.3.m3.1.1.1.1.3.cmml">b</mi></mrow><mo id="S4.SS1.p2.3.m3.2.2.2.4" xref="S4.SS1.p2.3.m3.2.2.3.cmml">,</mo><mrow id="S4.SS1.p2.3.m3.2.2.2.2" xref="S4.SS1.p2.3.m3.2.2.2.2.cmml"><mn id="S4.SS1.p2.3.m3.2.2.2.2.2" xref="S4.SS1.p2.3.m3.2.2.2.2.2.cmml">1</mn><mo id="S4.SS1.p2.3.m3.2.2.2.2.1" xref="S4.SS1.p2.3.m3.2.2.2.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.2.2.2.2.3" xref="S4.SS1.p2.3.m3.2.2.2.2.3.cmml">c</mi></mrow><mo id="S4.SS1.p2.3.m3.2.2.2.5" stretchy="false" xref="S4.SS1.p2.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.2b"><interval closure="open" id="S4.SS1.p2.3.m3.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.2"><apply id="S4.SS1.p2.3.m3.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1"><times id="S4.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1"></times><cn id="S4.SS1.p2.3.m3.1.1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.3.m3.1.1.1.1.2">1</cn><ci id="S4.SS1.p2.3.m3.1.1.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.3">𝑏</ci></apply><apply id="S4.SS1.p2.3.m3.2.2.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2"><times id="S4.SS1.p2.3.m3.2.2.2.2.1.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.1"></times><cn id="S4.SS1.p2.3.m3.2.2.2.2.2.cmml" type="integer" xref="S4.SS1.p2.3.m3.2.2.2.2.2">1</cn><ci id="S4.SS1.p2.3.m3.2.2.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.3">𝑐</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.2c">(1b,1c)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.2d">( 1 italic_b , 1 italic_c )</annotation></semantics></math> from synthesis path <math alttext="SP(4)" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.2" xref="S4.SS1.p2.4.m4.1.2.cmml"><mi id="S4.SS1.p2.4.m4.1.2.2" xref="S4.SS1.p2.4.m4.1.2.2.cmml">S</mi><mo id="S4.SS1.p2.4.m4.1.2.1" xref="S4.SS1.p2.4.m4.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.4.m4.1.2.3" xref="S4.SS1.p2.4.m4.1.2.3.cmml">P</mi><mo id="S4.SS1.p2.4.m4.1.2.1a" xref="S4.SS1.p2.4.m4.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p2.4.m4.1.2.4.2" xref="S4.SS1.p2.4.m4.1.2.cmml"><mo id="S4.SS1.p2.4.m4.1.2.4.2.1" stretchy="false" xref="S4.SS1.p2.4.m4.1.2.cmml">(</mo><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">4</mn><mo id="S4.SS1.p2.4.m4.1.2.4.2.2" stretchy="false" xref="S4.SS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.2.cmml" xref="S4.SS1.p2.4.m4.1.2"><times id="S4.SS1.p2.4.m4.1.2.1.cmml" xref="S4.SS1.p2.4.m4.1.2.1"></times><ci id="S4.SS1.p2.4.m4.1.2.2.cmml" xref="S4.SS1.p2.4.m4.1.2.2">𝑆</ci><ci id="S4.SS1.p2.4.m4.1.2.3.cmml" xref="S4.SS1.p2.4.m4.1.2.3">𝑃</ci><cn id="S4.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">SP(4)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_S italic_P ( 4 )</annotation></semantics></math>. CtrlSynth-capimg means the synthetic image and text pair <math alttext="(1d,1c)" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.2"><semantics id="S4.SS1.p2.5.m5.2a"><mrow id="S4.SS1.p2.5.m5.2.2.2" xref="S4.SS1.p2.5.m5.2.2.3.cmml"><mo id="S4.SS1.p2.5.m5.2.2.2.3" stretchy="false" xref="S4.SS1.p2.5.m5.2.2.3.cmml">(</mo><mrow id="S4.SS1.p2.5.m5.1.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.1.cmml"><mn id="S4.SS1.p2.5.m5.1.1.1.1.2" xref="S4.SS1.p2.5.m5.1.1.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.5.m5.1.1.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p2.5.m5.1.1.1.1.3" xref="S4.SS1.p2.5.m5.1.1.1.1.3.cmml">d</mi></mrow><mo id="S4.SS1.p2.5.m5.2.2.2.4" xref="S4.SS1.p2.5.m5.2.2.3.cmml">,</mo><mrow id="S4.SS1.p2.5.m5.2.2.2.2" xref="S4.SS1.p2.5.m5.2.2.2.2.cmml"><mn id="S4.SS1.p2.5.m5.2.2.2.2.2" xref="S4.SS1.p2.5.m5.2.2.2.2.2.cmml">1</mn><mo id="S4.SS1.p2.5.m5.2.2.2.2.1" xref="S4.SS1.p2.5.m5.2.2.2.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.5.m5.2.2.2.2.3" xref="S4.SS1.p2.5.m5.2.2.2.2.3.cmml">c</mi></mrow><mo id="S4.SS1.p2.5.m5.2.2.2.5" stretchy="false" xref="S4.SS1.p2.5.m5.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.2b"><interval closure="open" id="S4.SS1.p2.5.m5.2.2.3.cmml" xref="S4.SS1.p2.5.m5.2.2.2"><apply id="S4.SS1.p2.5.m5.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1"><times id="S4.SS1.p2.5.m5.1.1.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1.1"></times><cn id="S4.SS1.p2.5.m5.1.1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.5.m5.1.1.1.1.2">1</cn><ci id="S4.SS1.p2.5.m5.1.1.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.1.1.3">𝑑</ci></apply><apply id="S4.SS1.p2.5.m5.2.2.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2.2.2"><times id="S4.SS1.p2.5.m5.2.2.2.2.1.cmml" xref="S4.SS1.p2.5.m5.2.2.2.2.1"></times><cn id="S4.SS1.p2.5.m5.2.2.2.2.2.cmml" type="integer" xref="S4.SS1.p2.5.m5.2.2.2.2.2">1</cn><ci id="S4.SS1.p2.5.m5.2.2.2.2.3.cmml" xref="S4.SS1.p2.5.m5.2.2.2.2.3">𝑐</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.2c">(1d,1c)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.2d">( 1 italic_d , 1 italic_c )</annotation></semantics></math> from synthesis path <math alttext="SP(3)" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.1"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.2" xref="S4.SS1.p2.6.m6.1.2.cmml"><mi id="S4.SS1.p2.6.m6.1.2.2" xref="S4.SS1.p2.6.m6.1.2.2.cmml">S</mi><mo id="S4.SS1.p2.6.m6.1.2.1" xref="S4.SS1.p2.6.m6.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.6.m6.1.2.3" xref="S4.SS1.p2.6.m6.1.2.3.cmml">P</mi><mo id="S4.SS1.p2.6.m6.1.2.1a" xref="S4.SS1.p2.6.m6.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p2.6.m6.1.2.4.2" xref="S4.SS1.p2.6.m6.1.2.cmml"><mo id="S4.SS1.p2.6.m6.1.2.4.2.1" stretchy="false" xref="S4.SS1.p2.6.m6.1.2.cmml">(</mo><mn id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">3</mn><mo id="S4.SS1.p2.6.m6.1.2.4.2.2" stretchy="false" xref="S4.SS1.p2.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.2.cmml" xref="S4.SS1.p2.6.m6.1.2"><times id="S4.SS1.p2.6.m6.1.2.1.cmml" xref="S4.SS1.p2.6.m6.1.2.1"></times><ci id="S4.SS1.p2.6.m6.1.2.2.cmml" xref="S4.SS1.p2.6.m6.1.2.2">𝑆</ci><ci id="S4.SS1.p2.6.m6.1.2.3.cmml" xref="S4.SS1.p2.6.m6.1.2.3">𝑃</ci><cn id="S4.SS1.p2.6.m6.1.1.cmml" type="integer" xref="S4.SS1.p2.6.m6.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">SP(3)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">italic_S italic_P ( 3 )</annotation></semantics></math>. We define CtrlSynth-mix as taking one image-text pair from CtrlSynth-cap and another from CtrlSynth-capimg. We do not take CtrlSynth-img image-text pairs since we found the original texts are noisy and thus a substantial portion of synthetic images are bad quality. We refer CtrlSynth-mix as the default setting unless otherwise stated. We list detailed information in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS3" title="A.3 Training Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">CtrlSynth Models.</span> For the VTM, we adopt a hybrid approach by default, we combine the tags from a captioning plus tag extraction pipeline and an advanced multi-label image classifier. We use a recent vision foundation model called Florence-large <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib60" title="">2024</a>)</cite> to generate detailed image descriptions and then extract the objects, attributes, and relations using the Qwen2-7B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib62" title="">2024a</a>)</cite> LLM.
Then we use an accurate image classifier, the huge variant of CatLIP <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib36" title="">2024b</a>)</cite>, to output multiple high-confidence objects and attributes. We show later in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a> that this hybrid VTM provides the best visual tags compared with using individual approach alone. For the LLM, we use Mistral-NeMo-instruct model <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib1" title="">2024</a>)</cite> by default due to its strong instruction-following capability. We choose the stable-diffusion-xl-base-1.0 <cite class="ltx_cite ltx_citemacro_citep">(Podell et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib43" title="">2024</a>)</cite> for the text-to-image model by default. We describe the detailed setup in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS4" title="A.4 CtrlSynth Inference Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.4</span></a>. In <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>, we study different pretrained models for each of the three modules in CtrlSynth.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.T3.1" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 1: </span>Comparison of the zero-shot classification accuracy between the baseline and CtrlSynth. We report top-1 accuracy for 20 commonly used downstream vision datasets, including 12 tasks in the VTAB benchmark <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib64" title="">2020</a>)</cite> and 8 other ones. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Data <math alttext="\backslash" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.1.m1.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.1.m1.1d">\</annotation></semantics></math> Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.1.1.1.2">CC3M</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.1.3">CC12M</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2">
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.2.1">CLIP</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.2.2">CtrlSynth</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.2.3">CLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.2.4">CtrlSynth</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.3.1">CIFAR-10</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.1.1.3.2">41.5</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.3.1">70.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.1.1.3.4">75.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T3.1.1.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.5.1">82.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.4.1">CIFAR-100</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.4.2">14.1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.3.1">34.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.4.4">47.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.5.1">53.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.5.1">CLEVR Counts</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.5.2">7.1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.3.1">11.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.5.4">15.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.5.1">22.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.6.1">CLEVR Distance</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.6.2">16.1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.1">19.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.4.1">18.6</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.6.5">18.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.7.1">Caltech-101</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.7.2">43.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.3.1">68.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.7.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.4.1">76.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.7.5">76.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.8.1">Country211</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.8.2">0.4</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.3.1">0.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.8.4">1.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.5.1">1.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.9.1">DTD</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.9.2">11.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.9.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.3.1">17.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.9.4">23.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.9.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.5.1">29.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.10.1">EuroSAT</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.10.2">12.5</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.3.1">15.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.10.4">25.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.5.1">27.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.11.1">FGVC Aircraft</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.11.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.11.2.1">1.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.11.3">0.8</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.11.4">0.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.11.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.11.5.1">1.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.12.1">Food-101</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.12.2">9.5</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.12.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.12.3.1">23.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.12.4">53.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.12.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.12.5.1">61.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.13.1">GTSRB</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.13.2">4.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.13.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.13.3.1">9.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.13.4">14.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.13.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.13.5.1">19.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.14.1">KITTI</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.14.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.14.2.1">30.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.14.3">19.5</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.14.4">33.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.14.5">33.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.15.1">Oxford Flowers</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.15.2">10.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.15.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.15.3.1">24.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.15.4">34.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.15.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.15.5.1">38.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.16.1">Oxford-IIIT Pet</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.16.2">3.1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.16.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.16.3.1">7.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.16.4">8.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.16.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.16.5.1">9.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.17.1">PatchCamelyon</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.17.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.17.2.1">50.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.17.3">48.6</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.17.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.17.4.1">52.7</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.17.5">50.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.18.1">RESISC45</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.18.2">17.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.18.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.18.3.1">27.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.18.4">36.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.18.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.18.5.1">39.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.19.1">STL-10</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.19.2">70.4</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.19.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.19.3.1">90.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.19.4">92.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.19.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.19.5.1">94.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.20.1">SUN397</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.20.2">30.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.20.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.20.3.1">44.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.20.4">54.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.20.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.20.5.1">58.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.21.1">SVHN</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.21.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.21.2.1">12.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.21.3">6.8</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.21.4">10.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.21.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.21.5.1">14.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.22.1">Stanford Cars</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.22.2">0.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.22.3">0.6</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.1.22.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.22.4.1">2.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.1.1.22.5">2.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.23">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.1.23.1">Average</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.1.1.23.2">19.4</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.1.23.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.23.3.1">27.1 <span class="ltx_text" id="S4.T3.1.1.23.3.1.1" style="color:#008000;">(+7.7)</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.1.1.23.4">33.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.1.1.23.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.23.5.1">36.6 <span class="ltx_text" id="S4.T3.1.1.23.5.1.1" style="color:#008000;">(+2.5)</span></span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.T3.3" style="width:208.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Zero-shot top-1 accuracy between the baseline and CtrlSynth on 6 ImageNet datasets. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S4.T3.2.1">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.1">Data <math alttext="\backslash" class="ltx_Math" display="inline" id="S4.T3.2.1.1.1.1.1.m1.1"><semantics id="S4.T3.2.1.1.1.1.1.m1.1a"><mo id="S4.T3.2.1.1.1.1.1.m1.1.1" xref="S4.T3.2.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.1.1.1.1.1.m1.1b"><ci id="S4.T3.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.2.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.1.1.1.1.1.m1.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.1.1.1.1.1.m1.1d">\</annotation></semantics></math> Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.2.1.1.2">CC3M</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.2.1.1.3">CC12M</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.2">
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.2.1">CLIP</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.2.2">CtrlSynth</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.2.3">CLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.2.4">CtrlSynth</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.1.3.1">ImageNet-1K</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.1.3.2">20.2</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.3.3.1">25.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.1.3.4">39.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T3.2.1.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.3.5.1">41.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.1.4.1">ImageNet-V2</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.4.2">11.0</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.4.3.1">20.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.4.4">34.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.4.5.1">35.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.1.5.1">ImageNet-S</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.5.2">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.5.3.1">12.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.5.4">28.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.5.5.1">33.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.1.6.1">ImageNet-A</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.6.2">3.0</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.6.3.1">6.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.6.4">12.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.6.5.1">14.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.1.7.1">ImageNet-O</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.7.2">18.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.7.3.1">30.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.7.4">44.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.7.5.1">45.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.1.8.1">ImageNet-R</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.8.2">11.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.2.1.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.8.3.1">28.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.1.8.4">47.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.2.1.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.8.5.1">55.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.2.1.9.1">Average</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.2.1.9.2">11.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.2.1.9.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.9.3.1">20.7 <span class="ltx_text" id="S4.T3.2.1.9.3.1.1" style="color:#008000;">(+9.4)</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.2.1.9.4">34.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.2.1.9.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.9.5.1">37.7 <span class="ltx_text" id="S4.T3.2.1.9.5.1.1" style="color:#008000;">(+3.4)</span></span></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Zero-shot retrieval evaluation on the Flickr and COCO datasets. We report the recall@1 numbers. I2T means image-to-text retrieval, and T2I denotes text-to-image retrieval. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S4.T3.3.2">
<tr class="ltx_tr" id="S4.T3.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.3.2.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.1.1.1">Data <math alttext="\backslash" class="ltx_Math" display="inline" id="S4.T3.3.2.1.1.1.1.m1.1"><semantics id="S4.T3.3.2.1.1.1.1.m1.1a"><mo id="S4.T3.3.2.1.1.1.1.m1.1.1" xref="S4.T3.3.2.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.2.1.1.1.1.m1.1b"><ci id="S4.T3.3.2.1.1.1.1.m1.1.1.cmml" xref="S4.T3.3.2.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.2.1.1.1.1.m1.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.2.1.1.1.1.m1.1d">\</annotation></semantics></math> Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.3.2.1.2">CC3M</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.3.2.1.3">CC12M</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.2">
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.2.1">CLIP</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.3.2.2.2">CtrlSynth</td>
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.2.3">CLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.3.2.2.4">CtrlSynth</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.2.3.1">COCO I2T</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.3.2.3.2">10.9</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.3.2.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.3.3.1">32.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.3.2.3.4">40.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T3.3.2.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.3.5.1">49.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.2.4.1">COCO T2I</td>
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.4.2">7.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.3.2.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.4.3.1">19.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.4.4">26.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.3.2.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.4.5.1">32.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.2.5.1">Flickr I2T</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.3.2.5.2">21.3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.3.2.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.5.3.1">57.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.3.2.5.4">65.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T3.3.2.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.5.5.1">77.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.2.6.1">Flickr T2I</td>
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.6.2">14.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.3.2.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.6.3.1">39.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.3.2.6.4">48.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T3.3.2.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.6.5.1">58.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.3.2.7.1">Average</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.3.2.7.2">13.7</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.3.2.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.7.3.1">37.1 <span class="ltx_text" id="S4.T3.3.2.7.3.1.1" style="color:#008000;">(+23.4)</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.3.2.7.4">45.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.3.2.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.7.5.1">54.4 <span class="ltx_text" id="S4.T3.3.2.7.5.1.1" style="color:#008000;">(+9.0)</span></span></td>
</tr>
</table>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Image Classification Evaluation.</span> We conduct the zero-shot evaluation for image classification tasks. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T3" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the results across 20 commonly used vision datasets and <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T3" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the results of 6 ImageNet-related datasets. Notably, CtrlSynth outperforms the baseline consistently by 2.5% to 9.4% for the CLIP models trained on the CC3M and CC12M datasets. We observe that CtrlSynth significantly improves the zero-shot performance (by over 7.7%) by augmenting smaller datasets like CC3M, while the performance gains become smaller on larger datasets like CC12M.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Image-Text Retrieval Evaluation.</span> We evaluate the zero-shot image-text retrieval performance for our CtrlSynth and baseline CLIP models and present the recall@1 results in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T3" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>. CtrlSynth substantially improves the text-to-image and image-to-text retrieval recall by up to 24% and 36% for the Flickr dataset, and overall improves recall by 23.4% on average for CC3M models. CtrlSynth also brings over 9% retrieval gains for CC12M models on average. The improvements show that data samples from CtrlSynth have better coverage of visual concepts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Compositional Reasoning Results.</span> A key strength in CtrlSynth is the inclusion of visual tags that contain objects, attributes and relations from an image. To understand how the fine-grained visual attributes and relations affect visual reasoning performance, we evaluate CtrlSynth and baseline on the SugarCrepe <cite class="ltx_cite ltx_citemacro_cite">Hsieh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib17" title="">2023</a>)</cite> benchmark which measures the compositional reasoning capability of vision language models. We present the results in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T4" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>. CtrlSynth improves the baseline CLIP compositional reasoning by a large margin (4.5% for CC3M and 3% for CC12M on average). Note that most of the improvements come from the attribute and relation forms in the <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.2">Replace</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.3">Swap</span> categories, for example, CtrlSynth on CC3M improves the <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.4">Replace</span> relation accuracy by 4.3% and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p3.1.5">Swap</span> attribute by 14.8%, indicating CtrlSynth models are robust to the attribute and relation changes.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>We evaluate the compositional reasoning accuracy on the SugarCrepe <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib17" title="">2023</a>)</cite> benchmark.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.1">
<tr class="ltx_tr" id="S4.T4.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.1.1.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.1.3.1">Add</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T4.1.1.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.1.4.1">Replace</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.1.1.5"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.1.5.1">Swap</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.6" rowspan="2"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.1.1.6.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1">Attribute</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2">Object</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.3">Attribute</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.4">Object</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.5">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.6">Attribute</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.7">Object</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.3.1.1">CC3M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.2">CLIP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.3.1">69.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.4">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.5">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.6">80.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.7">55.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.8">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.9">50.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.3.10">64.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4">
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.1">CtrlSynth</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2">66.2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3">71.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.4.1">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.5.1">82.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.6.1">59.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.7.1">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.8.1">59.6</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.9.1">68.5 (<span class="ltx_text" id="S4.T4.1.4.9.1.1" style="color:#008000;">+4.5</span>)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.5.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.5.1.1">CC12M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.2">CLIP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.3">70.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.4">77.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.5">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.6">88.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.7">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.8">61.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.5.9">62.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.5.10">72.3</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.1">CtrlSynth</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.2.1">71.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.3.1">78.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.1">82.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.5">88.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.6.1">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.7.1">72.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.8.1">63.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.6.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.9.1">75.3 (<span class="ltx_text" id="S4.T4.1.6.9.1.1" style="color:#008000;">+3.0</span>)</span></td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Comparison with Prior Work.</span> CtrlSynth pipeline is flexible and supports synthesizing data from different paths. Previous work like VeCLIP <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib23" title="">2024</a>)</cite> and LaCLIP <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib11" title="">2023</a>)</cite> synthesizing new texts for the images by improving the captions. Though it is impossible to have a completely fair comparison with them<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Factors that prohibit apple-to-apple comparison include training software, variations of CC3M samples due to missing images, exact hardware set up, etc.</span></span></span>, the synthetic texts from the synthesis path (2) in CtrlSynth provide similar effects. We present the results on CLIP ViT/B16 models trained on CC3M for the tasks reported in each work. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T5" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that CtrlSynth outperforms VeCLIP on most VTAB datasets and improves zero-shot accuracy by 4.8% on average. CtrlSynth also surpasses VeCLIP by 7.9% on the ImageNet 1K dataset. We observe a similar trend when comparing CtrlSynth with LaCLIP in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T6" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>. Specifically, CtrlSynth achieves an average of 3.4% better accuracy than LaCLIP on 15 common datasets and 2.3% better accuracy on ImageNet 1K.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of the zero-shot classification accuracy between VeCLIP <cite class="ltx_cite ltx_citemacro_citep">(Vasu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib55" title="">2024</a>)</cite> and CtrlSynth for CLIP trained on the CC3M. We report top-1 accuracy (%) for the VTAB benchmark <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib64" title="">2020</a>)</cite> across 9 tasks (6 from natural and 3 from specialized sets). We highlight the best numbers in <span class="ltx_text ltx_font_bold" id="S4.T5.2.1">bold</span>.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.3" style="width:389.5pt;height:60.6pt;vertical-align:-6.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-126.4pt,17.7pt) scale(0.606408071482044,0.606408071482044) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.3.1">
<tr class="ltx_tr" id="S4.T5.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6" id="S4.T5.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.2.1">Natural Sets</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T5.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.3.1">Specialized Sets</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.3.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.4.1">Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.3.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.5.1">ImageNet 1K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.1">Caltech101</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.2">CIFAR100</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.3">SVHN</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.4">DTD</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.5">OxPet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.2.6">Flowers102</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.7">EuroSAT</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2.8">RESISC45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.2.9">Camelyon</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.3.1.3.1">CLIP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.2">39.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.3">9.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.4"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.3.4.1">20.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.5">7.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.6">7.44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.3.1.3.7">10.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.8">11.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.9">7.93</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.3.1.3.10">50.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.3.1.3.11">18.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.1.3.12">5.46</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.4.1">VeCLIP</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.2">54.30</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.3">17.74</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.4">18.74</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.5">11.23</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.6"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.4.6.1">10.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.4.7"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.4.7.1">22.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.8">7.35</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.9">16.54</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.4.10"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.4.10.1">52.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.4.11">23.48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4.12">15.98</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.3.1.5.1">CtrlSynth</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.2.1">66.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.3.1">34.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.4">17.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.5"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.5.1">16.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.6">7.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.3.1.5.7">15.55</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.8"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.8.1">20.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.9"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.9.1">24.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.3.1.5.10">50.79</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.3.1.5.11"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.11.1">28.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.1.5.12"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.12.1">23.82</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>We report the zero-shot performance on ImageNet 1K and 15 common downstream datasets for both LaCLIP <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib11" title="">2023</a>)</cite> and CtrlSynth for CLIP trained on CC3M. We highlight the best numbers in <span class="ltx_text ltx_font_bold" id="S4.T6.2.1">bold</span>.
</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3" style="width:389.5pt;height:110.2pt;vertical-align:-9.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.5pt,4.8pt) scale(0.913145575843726,0.913145575843726) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.3.1">
<tr class="ltx_tr" id="S4.T6.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.2.1" style="width:6.9pt;height:40.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.1pt;transform:translate(-16.6pt,-16.6pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.2.1.1">Food-101</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.3">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.3.1" style="width:6.8pt;height:44.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.4pt;transform:translate(-18.81pt,-18.81pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.3.1.1">CIFAR-10</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.4">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.4.1" style="width:6.8pt;height:49.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.4pt;transform:translate(-21.31pt,-21.31pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.4.1.1">CIFAR-100</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.5">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.5.1" style="width:6.8pt;height:35.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.6pt;transform:translate(-14.36pt,-14.36pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.5.1.1">SUN397</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.6">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.6.1" style="width:6.8pt;height:20.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.1pt;transform:translate(-6.63pt,-6.63pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.6.1.1">Cars</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.7">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.7.1" style="width:6.9pt;height:34.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:34.5pt;transform:translate(-13.78pt,-13.78pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.7.1.1">Aircraft</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.8">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.8.1" style="width:6.8pt;height:22.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:22.5pt;transform:translate(-7.83pt,-7.83pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.8.1.1">DTD</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.9">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.9.1" style="width:6.8pt;height:18.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:18.8pt;transform:translate(-5.99pt,-5.99pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.9.1.1">Pets</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.10">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.10.1" style="width:6.9pt;height:51.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.4pt;transform:translate(-22.22pt,-22.22pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.10.1.1">Caltech-101</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.11">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.11.1" style="width:6.9pt;height:33.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:33.3pt;transform:translate(-13.17pt,-13.17pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.11.1.1">Flowers</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.12">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.12.1" style="width:6.8pt;height:32.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.4pt;transform:translate(-12.76pt,-12.76pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.12.1.1">STL-10</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.13">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.13.1" style="width:6.8pt;height:40.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.7pt;transform:translate(-16.94pt,-16.94pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.13.1.1">EuroSAT</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.14">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.14.1" style="width:6.8pt;height:46.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.1pt;transform:translate(-19.64pt,-19.64pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.14.1.1">RESISC45</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.15">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.15.1" style="width:6.8pt;height:35.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.1pt;transform:translate(-14.12pt,-14.12pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.15.1.1">GTSRB</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.16">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.16.1" style="width:8.8pt;height:51.1pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-21.18pt,-18.26pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.16.1.1">Country211</p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.17">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.17.1" style="width:8.8pt;height:35.3pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="width:35.3pt;transform:translate(-13.26pt,-10.35pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.17.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.1.17.1.1.1">Average</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.3.1.1.18">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T6.3.1.1.18.1" style="width:8.8pt;height:42.2pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-16.72pt,-13.81pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T6.3.1.1.18.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.1.18.1.1.1">ImageNet</span></p>
</span></div>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.1.2.1">CLIP</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.2">10.3</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.3">54.9</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.4">21.8</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.5">25.0</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.6">0.8</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.7">1.4</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.8">10.5</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.9">12.8</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.10">43.3</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.11">10.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.12">77.6</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.13">14.1</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.14">19.1</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.15">6.9</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.1.2.16">0.6</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.1.2.17">20.6</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.3.1.2.18">15.8</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.3.1">LaCLIP</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.2">14.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.3">57.1</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.4">27.5</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.5">35.1</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.3.6.1">1.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.7"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.3.7.1">1.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.8">16.6</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.9"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.3.9.1">15.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.10">52.7</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.11">14.7</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.12">86.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.13">15.0</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.14">24.3</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.15">6.4</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.3.1.3.16"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.3.16.1">1.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.3.1.3.17">24.6</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.3.1.3.18">21.5</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.3.1.4.1">CtrlSynth</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.2.1">17.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.3.1">69.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.4.1">34.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.5.1">44.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.6">0.7</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.7">1.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.8"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.8.1">16.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.9">7.8</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.10"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.10.1">66.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.11"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.11.1">15.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.12"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.12.1">88.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.13"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.13.1">20.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.14"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.14.1">24.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.15"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.15.1">10.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.3.1.4.16">0.7</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.3.1.4.17"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.17.1">28.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.3.1.4.18"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.4.18.1">23.8</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Performance on Long-tail Tasks.</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Real-world data often have long-tail distributions. Much recent research <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib50" title="">2024</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib30" title="">2019</a>)</cite> has focused on developing new learning methods for long-tail recognition tasks. Data augmentation remains an important solution, especially when the tail classes only have a few samples. In this section, we evaluate the effectiveness of synthetic samples from CtrlSynth for long-tail tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.4"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.4.1">Setup.</span> We conduct experiments on the ImageNet-LT <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib30" title="">2019</a>)</cite> and Places-LT <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib30" title="">2019</a>)</cite> datasets. ImageNet-LT is a subset of the original ImageNet-2012 <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib7" title="">2009</a>)</cite> and contains 115.8K images from 1000 classes, with 5 to 1280 images per class. Places-LT is even more imbalanced and contains 62.5K images from 365 classes, with 5 to 4980 images per class. The test sets of both datasets are balanced. Following the same setup in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib30" title="">2019</a>)</cite>, we report the overall accuracy as well as the accuracy across the head (<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><gt id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">&gt;</annotation></semantics></math>100 images), medium (20<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mo id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">∼</annotation></semantics></math>100), and tail (<math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mo id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><lt id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">&lt;</annotation></semantics></math>20) classes. We take the same baseline in <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib50" title="">2024</a>)</cite> and fine-tune the classifier head of a pretrained CLIP model (ViT-B/16) for 10 epochs (or the same number of iterations for CtrlSynth). For CtrlSynth synthetic samples, we choose the synthetic path <math alttext="SP(3)" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.2" xref="S4.SS3.p2.4.m4.1.2.cmml"><mi id="S4.SS3.p2.4.m4.1.2.2" xref="S4.SS3.p2.4.m4.1.2.2.cmml">S</mi><mo id="S4.SS3.p2.4.m4.1.2.1" xref="S4.SS3.p2.4.m4.1.2.1.cmml">⁢</mo><mi id="S4.SS3.p2.4.m4.1.2.3" xref="S4.SS3.p2.4.m4.1.2.3.cmml">P</mi><mo id="S4.SS3.p2.4.m4.1.2.1a" xref="S4.SS3.p2.4.m4.1.2.1.cmml">⁢</mo><mrow id="S4.SS3.p2.4.m4.1.2.4.2" xref="S4.SS3.p2.4.m4.1.2.cmml"><mo id="S4.SS3.p2.4.m4.1.2.4.2.1" stretchy="false" xref="S4.SS3.p2.4.m4.1.2.cmml">(</mo><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">3</mn><mo id="S4.SS3.p2.4.m4.1.2.4.2.2" stretchy="false" xref="S4.SS3.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.2.cmml" xref="S4.SS3.p2.4.m4.1.2"><times id="S4.SS3.p2.4.m4.1.2.1.cmml" xref="S4.SS3.p2.4.m4.1.2.1"></times><ci id="S4.SS3.p2.4.m4.1.2.2.cmml" xref="S4.SS3.p2.4.m4.1.2.2">𝑆</ci><ci id="S4.SS3.p2.4.m4.1.2.3.cmml" xref="S4.SS3.p2.4.m4.1.2.3">𝑃</ci><cn id="S4.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">SP(3)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">italic_S italic_P ( 3 )</annotation></semantics></math> to generate synthetic images for the tail classes. We mix the CtrlSynth image samples with the original training set of each dataset. We describe more details in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS2" title="A.2 Datasets Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Key Results.</span> <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T7" title="In 4.3 Performance on Long-tail Tasks. ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows that CtrlSynth improves the tail class accuracy by 21.3% on ImageNet-LT and by 16.2% on Places-LT. Synthetic samples from CtrlSynth also improve the overall and medium class accuracy by 3<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mo id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">∼</annotation></semantics></math>6%, though slightly decrease the head class accuracy.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Long-tail accuracy on the ImagetNet-LT and Places-LT datasets for the baseline and CtrlSynth models.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T7.1" style="width:389.5pt;height:64.2pt;vertical-align:-7.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.7pt,7.8pt) scale(0.783724335291861,0.783724335291861) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T7.1.1">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T7.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="S4.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.2.1">ImageNet-LT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.3.1">Places-LT</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.2">
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.1">Overall</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.2">Tail</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.3">Medium</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T7.1.1.2.4">Head</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.5">Overall</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.6">Tail</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.1.2.7">Medium</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T7.1.1.2.8">Head</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.1.1.3.1">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.2">60.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.3">13.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.4">56.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T7.1.1.3.5"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.3.5.1">82.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.6">34.9</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.7">8.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.1.3.8">31.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T7.1.1.3.9"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.3.9.1">53.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T7.1.1.4.1">CtrlSynth</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.2.1">66.2 (<span class="ltx_text" id="S4.T7.1.1.4.2.1.1" style="color:#008000;">+5.4</span>)</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.3"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.3.1">35.1 (<span class="ltx_text" id="S4.T7.1.1.4.3.1.1" style="color:#008000;">+21.3</span>)</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.4"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.4.1">62.8 (<span class="ltx_text" id="S4.T7.1.1.4.4.1.1" style="color:#008000;">+6.1</span>)</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T7.1.1.4.5">81.4</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.6"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.6.1">38.6 (<span class="ltx_text" id="S4.T7.1.1.4.6.1.1" style="color:#008000;">+3.7</span>)</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.7"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.7.1">24.4 (<span class="ltx_text" id="S4.T7.1.1.4.7.1.1" style="color:#008000;">+16.2</span>)</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.8"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.4.8.1">34.6 (<span class="ltx_text" id="S4.T7.1.1.4.8.1.1" style="color:#008000;">+3.3</span>)</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T7.1.1.4.9">51.2</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<span class="ltx_ERROR undefined" id="S4.SS4.p1.1">\mdfdefinestyle</span>
<p class="ltx_p" id="S4.SS4.p1.2">mdf2innertopmargin=0.2em,innerbottommargin=0.2em,innerleftmargin=0.2em,innerrightmargin=0.2em,roundcorner=2pt,hidealllines=true</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S4.F5.2">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.F5.1">[style=mdf2]

<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="605" id="S4.F5.1.g1" src="x6.png" width="830"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Data efficiency comparison between baseline and CtrlSynth for pretraining CLIP models on CC3M. We normalize the iterations by dividing the total iterations with checkpoint steps.</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data-Efficiency of CtrlSynth in Training CLIP.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">To study the data efficiency of CtrlSynth samples, we plot the top1 zero-shot accuracy of the ImageNet validation set in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.F5" title="In 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> for the baseline and CtrlSynth CLIP models trained on CC3M. CtrlSynth reaches the 20% accuracy with 40% fewer iterations than the baseline, indicating that using CtrlSynth samples is more data-efficient.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Statistics and visualization of CtrlSynth Samples.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">In this section, we provide the statistics for the synthetic samples from CtrlSynth. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.F6" title="In Statistics and visualization of CtrlSynth Samples. ‣ 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows examples of CtrlSynth images and texts compared with the original real samples. We observe that the text samples from CtrlSynth are usually longer and contain richer information about the image. On average, CtrlSynth texts have over 60 words while original captions contain 8 words. We plot the histogram of the number of words in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.F8" title="In CtrlSynth Synthetic Texts. ‣ A.5 More Analysis Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> at <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.SS5" title="A.5 More Analysis Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<div class="ltx_block ltx_figure_panel" id="S4.F6.1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F6.1.1" style="width:8.9pt;height:35.5pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="width:35.6pt;transform:translate(-13.35pt,-10.43pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F6.1.1.1">Original</p>
</span></div>
<figure class="ltx_figure ltx_align_center" id="S4.F6.sf1">
<p class="ltx_p" id="S4.F6.sf1.1">wisteria decorates a black and white timbered cottage</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>*</figcaption>
</figure><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="144" id="S4.F6.g1" src="extracted/5929450/fig/demo/1.jpg" width="144"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2">
<p class="ltx_p" id="S4.F6.sf2.1">fresh red shrimps for sale at a market</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g2" src="extracted/5929450/fig/demo/2.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf3">
<p class="ltx_p" id="S4.F6.sf3.1">view into the living room .</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g3" src="extracted/5929450/fig/demo/3.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf4">
<p class="ltx_p" id="S4.F6.sf4.1">tuscan sun in the landscape</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g4" src="extracted/5929450/fig/demo/4.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div class="ltx_block ltx_figure_panel" id="S4.F6.2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F6.2.1" style="width:8.9pt;height:41.1pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="width:41.1pt;transform:translate(-16.11pt,-13.19pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F6.2.1.1">Synthetic</p>
</span></div>
<figure class="ltx_figure ltx_align_center" id="S4.F6.sf5">
<p class="ltx_p" id="S4.F6.sf5.1">wisteria, with its purple flowers, hangs from the eaves and twines around the wrought iron railing, decorating the small porch of the old, black and white painted cottage.</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>*</figcaption>
</figure><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="144" id="S4.F6.g5" src="extracted/5929450/fig/demo/11.jpg" width="144"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf6">
<p class="ltx_p" id="S4.F6.sf6.1">freshly caught, red shrimp, arranged in a pile at the bustling seafood market, their small black spots visible, overlapping and cooked to perfection, surrounded by a blur of herbs and spices.</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g6" src="extracted/5929450/fig/demo/21.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf7">
<p class="ltx_p" id="S4.F6.sf7.1">a living room, where a large pendant light hangs from the ceiling. on the right, a wooden table is covered with a floral tablecloth, set with a wooden bowl of red tomatoes and a lamp.</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g7" src="extracted/5929450/fig/demo/31.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf8">
<p class="ltx_p" id="S4.F6.sf8.1">tuscan sun casting a warm, orange glow over the serene italian countryside, with tall cypress trees arranged in neat rows along the winding road, the sun setting in the background.</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>*</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="144" id="S4.F6.g8" src="extracted/5929450/fig/demo/41.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Randomly selected CC3M examples of real images and captions (the first row) with their corresponding CtrlSynth synthetic samples (the second row).</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Effects of Self-Filtering.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px3.p1.2">CtrlSynth provides off-the-shelf self-filtering to control the quality of synthetic samples. We study the effects of applying different filtering thresholds <math alttext="p_{f}" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><msub id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.2.cmml">p</mi><mi id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.2">𝑝</ci><ci id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">p_{f}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px3.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> for the synthetic text and image. We set the same filtering thresholds for both synthetic text and image samples. Intuitively, a higher threshold filters out more synthetic samples thus providing better quality samples that align with original real samples. On the contrary, a lower threshold keeps relatively less aligned samples but encourages more diverse samples. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.F7.sf1" title="In Figure 7 ‣ Effects of Self-Filtering. ‣ 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a> plots the zero-shot accuracy numbers of CLIP model on ImageNet under different threshold settings, we show that thresholds 10%<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS4.SSS0.Px3.p1.2.m2.1a"><mo id="S4.SS4.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px3.p1.2.m2.1d">∼</annotation></semantics></math>30% provide similar accuracy numbers and setting the filtering threshold to 20% provides the best accuracy. Thresholds higher than 50% do not provide accuracy gains, likely because the aligned synthetic samples lack diversity and fail to augment the original samples.</p>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_block" id="S4.F7.1">
<figure class="ltx_figure ltx_align_center" id="S4.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="273" id="S4.F7.sf1.g1" src="x7.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Accuracy versus filtering thresholds.</figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S4.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="273" id="S4.F7.sf2.g1" src="x8.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Accuracy versus mixing ratios.</figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Study of different filtering thresholds and mixing ratios of CtrlSynth samples. The accuracy numbers are top1 zero-shot accuracy on the ImageNet-1K validation set. The CLIP models are trained on the CC3M dataset and CtrlSynth samples. </figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Mixing Ratios of Synthetic Samples.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px4.p1.4">To better understand how the synthetic image text samples improve CLIP model training, we study different ratios (<math alttext="p_{r}" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS4.SSS0.Px4.p1.1.m1.1a"><msub id="S4.SS4.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml">p</mi><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2">𝑝</ci><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.1.m1.1c">p_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px4.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>) of mixing CtrlSynth samples with original real ones. During CLIP training, we randomly sample the original sample with probability <math alttext="0&lt;p_{r}&lt;1" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px4.p1.2.m2.1"><semantics id="S4.SS4.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS4.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml">0</mn><mo id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml">&lt;</mo><msub id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.cmml"><mi id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.2.cmml">p</mi><mi id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.3.cmml">r</mi></msub><mo id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.5" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.5.cmml">&lt;</mo><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.6" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"><and id="S4.SS4.SSS0.Px4.p1.2.m2.1.1a.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"></and><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1b.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"><lt id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3"></lt><cn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2">0</cn><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4">subscript</csymbol><ci id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.2">𝑝</ci><ci id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.3">𝑟</ci></apply></apply><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1c.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"><lt id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.5.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.5"></lt><share href="https://arxiv.org/html/2410.11963v1#S4.SS4.SSS0.Px4.p1.2.m2.1.1.4.cmml" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1d.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"></share><cn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.6.cmml" type="integer" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.2.m2.1c">0&lt;p_{r}&lt;1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px4.p1.2.m2.1d">0 &lt; italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT &lt; 1</annotation></semantics></math> and our sample with <math alttext="1-p_{r}" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px4.p1.3.m3.1"><semantics id="S4.SS4.SSS0.Px4.p1.3.m3.1a"><mrow id="S4.SS4.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.cmml"><mn id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.2" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.2.cmml">1</mn><mo id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.1" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.1.cmml">−</mo><msub id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.cmml"><mi id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.2" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.2.cmml">p</mi><mi id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.3" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.3.m3.1b"><apply id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1"><minus id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.1"></minus><cn id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.2">1</cn><apply id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.2">𝑝</ci><ci id="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.3.cmml" xref="S4.SS4.SSS0.Px4.p1.3.m3.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.3.m3.1c">1-p_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px4.p1.3.m3.1d">1 - italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>. <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.F7.sf2" title="In Figure 7 ‣ Effects of Self-Filtering. ‣ 4.4 Analysis ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7(b)</span></a> shows that even adding a small portion (<math alttext="&lt;20" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px4.p1.4.m4.1"><semantics id="S4.SS4.SSS0.Px4.p1.4.m4.1a"><mrow id="S4.SS4.SSS0.Px4.p1.4.m4.1.1" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.2" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.2.cmml"></mi><mo id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.1" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.3" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.4.m4.1b"><apply id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1"><lt id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.1"></lt><csymbol cd="latexml" id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.2">absent</csymbol><cn id="S4.SS4.SSS0.Px4.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS4.SSS0.Px4.p1.4.m4.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.4.m4.1c">&lt;20</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px4.p1.4.m4.1d">&lt; 20</annotation></semantics></math>%) of CtrlSynth samples improves the zero-shot accuracy while mixing with 50% provides best accuracy gains. Further higher mixing ratios show diminishing improvements though still better than the baseline that uses all real data.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In this section, we evaluate the effectiveness of visual tags, the impact of using different pretrained models in the CtrlSynth pipeline, and mixing and filtering effects for CtrlSynth samples. We use the same text and image control policy described in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S3.SS2" title="3.2 Image Text Synthesis in CtrlSynth ‣ 3 CtrlSynth ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> for all settings. We experiment with CC3M dataset for CLIP pretraining and report the accuracy on the SugarCrepe benchmark, zero-shot accuracy of common downstream vision tasks (same tasks in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T3" title="In 4.2 Main Results ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>), and top1 accuracy on the ImageNet 1k validation set.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Evaluation of using different models, visual tags, and synthetic samples in CtrlSynth. ’-’ denotes the same value from the last row (default setting).</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T8.1" style="width:389.5pt;height:129.3pt;vertical-align:-5.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-88.5pt,28.1pt) scale(0.687579444685921,0.687579444685921) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.1.1">
<tr class="ltx_tr" id="S4.T8.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T8.1.1.1.1">Study</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T8.1.1.1.2">Model</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T8.1.1.1.3">Tags</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T8.1.1.1.4">Samples</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.5">Common Tasks</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.6">ImageNet-1K</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T8.1.1.1.7">SugarCrepe</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.2.1" rowspan="3"><span class="ltx_text" id="S4.T8.1.1.2.1.1">Models</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.2.2">Qwen2-7B, SDXL</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.2.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.5">24.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.2.6">23.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T8.1.1.2.7">65.1</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.3">
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.3.1">Qwen2-7B, SD3M</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.3.2">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.3.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.4">26.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.3.5">23.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T8.1.1.3.6">65.2</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.4">
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.4.1">Mistral-Nemo, SD3M</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.4.2">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.4.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.4.4">26.6</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.4.5">25.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T8.1.1.4.6">68.1</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.5.1" rowspan="2"><span class="ltx_text" id="S4.T8.1.1.5.1.1">Tags</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.5.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.5.3">Obj</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.5.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.5.5">26.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.5.6">24.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T8.1.1.5.7">64.3</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.6">
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.6.1">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.6.2">Obj+Attr</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.6.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.6.4">26.2</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.6.5">24.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T8.1.1.6.6">65.4</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.7.1" rowspan="3"><span class="ltx_text" id="S4.T8.1.1.7.1.1">Samples</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.7.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.7.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.1.1.7.4">CtrlSynth-cap, SP(1)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.7.5">26.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.1.7.6">24.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T8.1.1.7.7">67.2</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.8">
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.8.1">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.8.2">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.8.3">CtrlSynth-img, SP(4)</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.8.4">22.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.8.5">21.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T8.1.1.8.6">64.4</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.9">
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.9.1">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.9.2">-</td>
<td class="ltx_td ltx_align_left" id="S4.T8.1.1.9.3">CtrlSynth-capimg, SP(3)</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.9.4">26.5</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.9.5">24.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T8.1.1.9.6">67.5</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.1">CtrlSynth</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.2">Mistral-Nemo, SDXL</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.3">Obj+Attr+Rel</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.4">CtrlSynth-mix</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.5">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.6">25.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.1.1.10.7">68.5</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">Different Pretrained Models.</span> We choose an alternate LLM and a different text-to-image model to understand how different pretrained models affect the quality of synthetic samples. CtrlSynth pipeline is flexible so we can easily swap the pretrained LLM and text-to-image models. Specifically, we use Qwen2-7B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib62" title="">2024a</a>)</cite> for the LLM and Stable Diffusion 3 Medium <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib10" title="">2024</a>)</cite> (SD3M) for the text-to-image model. Comparing the first and last rows in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T8" title="In 4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we find using a smaller LLM like Qwen2-7B degrades the task performance on all three tasks, indicating that using a strong LLM is key to synthesizing high quality texts. The accuracy boost (+3%) on SugarCrepe benchmark shows the LLM is effective in recombining the visual tags in a compositional way to form diverse synthetic texts.
We also point out that using a more recent diffusion model like SD3M provides similar task performance numbers, this is likely because SD3M has fewer (2B versus 3.5B) parameters compared to SDXL, limiting the image generation capability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p3.1.1">Effectiveness of Visual Tags.</span> We study the effects of using different categories of visual tags, <em class="ltx_emph ltx_font_italic" id="S4.SS5.p3.1.2">i.e</em>.<span class="ltx_text" id="S4.SS5.p3.1.3"></span>, using only objects (Obj), objects plus attributes (Obj+Attr), and all categories including relations (Obj+Attr+Rel). In <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.T8" title="In 4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>, comparing the second and last row, we show attributes marginally improve the CLIP performance on compositional reasoning but not much on zero-shot vision tasks. Importantly, visual relations improves the performance on all three tasks, and significantly improves compositional reasoning performance by over 4%.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">CtrlSynth Samples from Different Synthesis Paths.</span> CtrlSynth pipeline supports synthesizing images or texts from different paths, we evaluate their quality by measuring the downstream task accuracy of the CLIP models trained on them. The penultimate and last rows in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a> show all CtrlSynth samples provides performance gains on downstream tasks, except the CtrlSynth-img samples where they do not improve compositional reasoning performance. CtrlSynth-img samples have the least augmentation benefits and are likely due to the original real texts are noisy and thus the generated images are not of high quality. Notably, mixing with synthetic captions (CtrlSynth-cap, CtrlSynth-capimg, and CtrlSynth-mix) provides meaningful augmentation benefits, highlight the importance of using LLMs to recombine the visual tags.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Synthetic data emerges as a viable solution to address challenges in curating high-quality samples from noisy, misaligned, and long-tail web data. However, existing data synthesis pipelines are rigid and the generation process is hard to control and thus being tailored for ad hoc use cases.
We develop CtrlSynth, a new image-text synthesis pipeline that allows users to control the data generation in a fine-grained way. CtrlSynth decomposes the semantics of images and texts into basic elements and uses pretrained foundation models to recompose them based on specified control policies. This way, CtrlSynth provides flexible and diverse image-text samples. Synthetic samples from CtrlSynth improve the long-tail task performance by a large margin. They also significantly boost the zero-shot and compositional capability of CLIP models and enable data-efficient multimodal learning.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Mistral AI.

</span>
<span class="ltx_bibblock">Mistral NeMo, July 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mistral-nemo/" title="">https://mistral.ai/news/mistral-nemo/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bossard et al. (2014)</span>
<span class="ltx_bibblock">
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Food-101 – Mining Discriminative Components with Random Forests.

</span>
<span class="ltx_bibblock">In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Computer Vision – ECCV 2014</em>, pp.  446–461, Cham, 2014. Springer International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-319-10599-4.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-319-10599-4_29</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. (2021)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  3558–3568, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2017)</span>
<span class="ltx_bibblock">
Gong Cheng, Junwei Han, and Xiaoqiang Lu.

</span>
<span class="ltx_bibblock">Remote Sensing Image Scene Classification: Benchmark and State of the Art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE</em>, 105(10):1865–1883, October 2017.

</span>
<span class="ltx_bibblock">ISSN 1558-2256.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/JPROC.2017.2675998</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/7891544" title="">https://ieeexplore.ieee.org/document/7891544</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cimpoi et al. (2014)</span>
<span class="ltx_bibblock">
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Describing Textures in the Wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2014 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  3606–3613, June 2014.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2014.461</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6909856" title="">https://ieeexplore.ieee.org/document/6909856</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coates et al. (2011)</span>
<span class="ltx_bibblock">
Adam Coates, Andrew Ng, and Honglak Lee.

</span>
<span class="ltx_bibblock">An Analysis of Single-Layer Networks in Unsupervised Feature Learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</em>, pp.  215–223. JMLR Workshop and Conference Proceedings, June 2011.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v15/coates11a.html" title="">https://proceedings.mlr.press/v15/coates11a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  248–255. IEEE Computer Society, June 2009.

</span>
<span class="ltx_bibblock">ISBN 978-1-4244-3992-8.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2009.5206848</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.computer.org/csdl/proceedings-article/cvpr/2009/05206848/12OmNxWcH55" title="">https://www.computer.org/csdl/proceedings-article/cvpr/2009/05206848/12OmNxWcH55</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Learning Representations</em>, September 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=YicbFdNTTy" title="">https://openreview.net/forum?id=YicbFdNTTy</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunlap et al. (2023)</span>
<span class="ltx_bibblock">
Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, November 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9wrYfqdrwk" title="">https://openreview.net/forum?id=9wrYfqdrwk</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2024)</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach.

</span>
<span class="ltx_bibblock">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, March 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.03206" title="">http://arxiv.org/abs/2403.03206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2023)</span>
<span class="ltx_bibblock">
Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian.

</span>
<span class="ltx_bibblock">Improving CLIP Training with Language Rewrites, October 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.20088" title="">http://arxiv.org/abs/2305.20088</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei-Fei et al. (2006)</span>
<span class="ltx_bibblock">
Li Fei-Fei, R. Fergus, and P. Perona.

</span>
<span class="ltx_bibblock">One-shot learning of object categories.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 28(4):594–611, April 2006.

</span>
<span class="ltx_bibblock">ISSN 1939-3539.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TPAMI.2006.79</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/1597116" title="">https://ieeexplore.ieee.org/document/1597116</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiger et al. (2013)</span>
<span class="ltx_bibblock">
A Geiger, P Lenz, C Stiller, and R Urtasun.

</span>
<span class="ltx_bibblock">Vision meets robotics: The KITTI dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The International Journal of Robotics Research</em>, 32(11):1231–1237, September 2013.

</span>
<span class="ltx_bibblock">ISSN 0278-3649.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1177/0278364913491297</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/0278364913491297" title="">https://doi.org/10.1177/0278364913491297</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Helber et al. (2018)</span>
<span class="ltx_bibblock">
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.

</span>
<span class="ltx_bibblock">Introducing Eurosat: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium</em>, pp.  204–207, July 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/IGARSS.2018.8519248</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8519248" title="">https://ieeexplore.ieee.org/document/8519248</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021a)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.

</span>
<span class="ltx_bibblock">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  8340–8349, 2021a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/ICCV2021/html/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.html" title="">https://openaccess.thecvf.com/content/ICCV2021/html/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021b)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.

</span>
<span class="ltx_bibblock">Natural Adversarial Examples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  15262–15271, 2021b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com//content/CVPR2021/html/Hendrycks_Natural_Adversarial_Examples_CVPR_2021_paper.html" title="">https://openaccess.thecvf.com//content/CVPR2021/html/Hendrycks_Natural_Adversarial_Examples_CVPR_2021_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna.

</span>
<span class="ltx_bibblock">SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, November 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Jsc7WSCZd4&amp;noteId=Ekiryv85Mr" title="">https://openreview.net/forum?id=Jsc7WSCZd4&amp;noteId=Ekiryv85Mr</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Islam et al. (2024)</span>
<span class="ltx_bibblock">
Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood, and Karthik Nandakumar.

</span>
<span class="ltx_bibblock">DiffuseMix: Label-Preserving Data Augmentation with Diffusion Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  27621–27630, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2024/html/Islam_DiffuseMix_Label-Preserving_Data_Augmentation_with_Diffusion_Models_CVPR_2024_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2024/html/Islam_DiffuseMix_Label-Preserving_Data_Augmentation_with_Diffusion_Models_CVPR_2024_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2023)</span>
<span class="ltx_bibblock">
Wooyoung Kang, Jonghwan Mun, Sungjun Lee, and Byungseok Roh.

</span>
<span class="ltx_bibblock">Noise-aware learning from web-crawled image-text data for image captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  2942–2952, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krause et al. (2013)</span>
<span class="ltx_bibblock">
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">3D Object Representations for Fine-Grained Categorization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2013 IEEE International Conference on Computer Vision Workshops</em>, pp.  554–561, December 2013.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCVW.2013.77</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6755945" title="">https://ieeexplore.ieee.org/document/6755945</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky (2009)</span>
<span class="ltx_bibblock">
A. Krizhevsky.

</span>
<span class="ltx_bibblock">Learning Multiple Layers of Features from Tiny Images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Technical report</em>. University of Toronto, 2009.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" title="">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient Memory Management for Large Language Model Serving with PagedAttention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, SOSP ’23, pp.  611–626, New York, NY, USA, October 2023. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400702297.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3600006.3613165</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3600006.3613165" title="">https://dl.acm.org/doi/10.1145/3600006.3613165</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2024)</span>
<span class="ltx_bibblock">
Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao.

</span>
<span class="ltx_bibblock">VeCLIP: Improving CLIP Training via Visual-enriched Captions, March 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.07699" title="">http://arxiv.org/abs/2310.07699</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Xianhang Li, Zeyu Wang, and Cihang Xie.

</span>
<span class="ltx_bibblock">CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</em>, December 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=0hTtit3AAm" title="">https://openreview.net/forum?id=0hTtit3AAm</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Xianhang Li, Zeyu Wang, and Cihang Xie.

</span>
<span class="ltx_bibblock">An Inverse Scaling Law for CLIP Training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, November 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=LMU2RNwdh2" title="">https://openreview.net/forum?id=LMU2RNwdh2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie.

</span>
<span class="ltx_bibblock">What If We Recaption Billions of Web Images with LLaMA-3?, June 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2406.08478" title="">http://arxiv.org/abs/2406.08478</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.

</span>
<span class="ltx_bibblock">Scaling Language-Image Pre-Training via Masking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  23390–23400, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al. (2023)</span>
<span class="ltx_bibblock">
Long Lian, Boyi Li, Adam Yala, and Trevor Darrell.

</span>
<span class="ltx_bibblock">LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Transactions on Machine Learning Research</em>, October 2023.

</span>
<span class="ltx_bibblock">ISSN 2835-8856.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=hFALpTb4fR" title="">https://openreview.net/forum?id=hFALpTb4fR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Computer Vision – ECCV 2014</em>, Lecture Notes in Computer Science, pp.  740–755, Cham, 2014. Springer International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-319-10602-1.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-319-10602-1_48</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu.

</span>
<span class="ltx_bibblock">Large-Scale Long-Tailed Recognition in an Open World.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  2537–2546, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.html" title="">https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2018)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">International Conference on Learning Representations</em>, September 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2022)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">SGDR: Stochastic Gradient Descent with Warm Restarts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International Conference on Learning Representations</em>, July 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Skq89Scxx" title="">https://openreview.net/forum?id=Skq89Scxx</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maji et al. (2013)</span>
<span class="ltx_bibblock">
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Fine-Grained Visual Classification of Aircraft, June 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1306.5151" title="">http://arxiv.org/abs/1306.5151</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2022)</span>
<span class="ltx_bibblock">
Sachin Mehta, Farzad Abdolhosseini, and Mohammad Rastegari.

</span>
<span class="ltx_bibblock">CVNets: High Performance Library for Computer Vision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 30th ACM International Conference on Multimedia</em>, MM ’22, pp.  7327–7330, New York, NY, USA, October 2022. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 978-1-4503-9203-7.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3503161.3548540</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3503161.3548540" title="">https://dl.acm.org/doi/10.1145/3503161.3548540</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2024a)</span>
<span class="ltx_bibblock">
Sachin Mehta, Farzad Abdolhosseini, and Mohammad Rastegari.

</span>
<span class="ltx_bibblock">apple/corenet, September 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/apple/corenet" title="">https://github.com/apple/corenet</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2024b)</span>
<span class="ltx_bibblock">
Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, and Mohammad Rastegari.

</span>
<span class="ltx_bibblock">CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data, April 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.15653" title="">http://arxiv.org/abs/2404.15653</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al. (2022)</span>
<span class="ltx_bibblock">
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.

</span>
<span class="ltx_bibblock">SLIP: Self-supervision Meets Language-Image Pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVI</em>, pp.  529–544, Berlin, Heidelberg, October 2022. Springer-Verlag.

</span>
<span class="ltx_bibblock">ISBN 978-3-031-19808-3.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-031-19809-0_30</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-19809-0_30" title="">https://doi.org/10.1007/978-3-031-19809-0_30</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Netzer et al. (2011)</span>
<span class="ltx_bibblock">
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.

</span>
<span class="ltx_bibblock">Reading digits in natural images with unsupervised feature learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</em>, 2011.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf" title="">http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilsback &amp; Zisserman (2008)</span>
<span class="ltx_bibblock">
Maria-Elena Nilsback and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Automated Flower Classification over a Large Number of Classes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</em>, pp.  722–729, December 2008.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICVGIP.2008.47</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/4756141" title="">https://ieeexplore.ieee.org/document/4756141</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chatgpt.com" title="">https://chatgpt.com</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parkhi et al. (2012)</span>
<span class="ltx_bibblock">
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar.

</span>
<span class="ltx_bibblock">Cats and dogs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">2012 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  3498–3505, June 2012.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2012.6248092</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6248092" title="">https://ieeexplore.ieee.org/document/6248092</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2015)</span>
<span class="ltx_bibblock">
Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2015 IEEE International Conference on Computer Vision (ICCV)</em>, pp.  2641–2649, December 2015.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV.2015.303</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Podell et al. (2024)</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">SDXL: Improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=di52zR8xgf" title="">https://openreview.net/forum?id=di52zR8xgf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 38th International Conference on Machine Learning</em>, pp.  8748–8763. PMLR, July 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v139/radford21a.html" title="">https://proceedings.mlr.press/v139/radford21a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Recht et al. (2019)</span>
<span class="ltx_bibblock">
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.

</span>
<span class="ltx_bibblock">Do ImageNet Classifiers Generalize to ImageNet?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 36th International Conference on Machine Learning</em>, pp.  5389–5400. PMLR, May 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v97/recht19a.html" title="">https://proceedings.mlr.press/v97/recht19a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-Resolution Image Synthesis With Latent Diffusion Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  10684–10695, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schramowski et al. (2023)</span>
<span class="ltx_bibblock">
Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting.

</span>
<span class="ltx_bibblock">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  22522–22531, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">LAION-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=M3Y74vmsMcY" title="">https://openreview.net/forum?id=M3Y74vmsMcY</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.

</span>
<span class="ltx_bibblock">In Iryna Gurevych and Yusuke Miyao (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  2556–2565, Melbourne, Australia, July 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P18-1238</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P18-1238" title="">https://aclanthology.org/P18-1238</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
Jiang-Xin Shi, Tong Wei, Zhi Zhou, Jie-Jing Shao, Xin-Yan Han, and Yu-Feng Li.

</span>
<span class="ltx_bibblock">Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, pp.  45014–45039. PMLR, July 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v235/shi24g.html" title="">https://proceedings.mlr.press/v235/shi24g.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stallkamp et al. (2011)</span>
<span class="ltx_bibblock">
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.

</span>
<span class="ltx_bibblock">The German Traffic Sign Recognition Benchmark: A multi-class classification competition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">The 2011 International Joint Conference on Neural Networks</em>, pp.  1453–1460, July 2011.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/IJCNN.2011.6033395</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6033395" title="">https://ieeexplore.ieee.org/document/6033395</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.09288" title="">http://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trabucco et al. (2023)</span>
<span class="ltx_bibblock">
Brandon Trabucco, Kyle Doherty, Max A. Gurinas, and Ruslan Salakhutdinov.

</span>
<span class="ltx_bibblock">Effective Data Augmentation With Diffusion Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">The Twelfth International Conference on Learning Representations</em>, October 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=ZWzUA9zeAg" title="">https://openreview.net/forum?id=ZWzUA9zeAg</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Udandarao et al. (2024)</span>
<span class="ltx_bibblock">
Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge.

</span>
<span class="ltx_bibblock">No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance, April 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.04125" title="">http://arxiv.org/abs/2404.04125</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasu et al. (2024)</span>
<span class="ltx_bibblock">
Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, and Oncel Tuzel.

</span>
<span class="ltx_bibblock">CLIP with Quality Captions: A Strong Pretraining for Vision Tasks, May 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.08911" title="">http://arxiv.org/abs/2405.08911</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veeling et al. (2018)</span>
<span class="ltx_bibblock">
Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.

</span>
<span class="ltx_bibblock">Rotation Equivariant CNNs for Digital Pathology.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Medical Image Computing and Computer Assisted Intervention – MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II</em>, pp.  210–218, Berlin, Heidelberg, September 2018. Springer-Verlag.

</span>
<span class="ltx_bibblock">ISBN 978-3-030-00933-5.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-030-00934-2_24</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-030-00934-2_24" title="">https://doi.org/10.1007/978-3-030-00934-2_24</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Platen et al. (2022)</span>
<span class="ltx_bibblock">
Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Diffusers: State-of-the-art diffusion models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/diffusers" title="">https://github.com/huggingface/diffusers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.

</span>
<span class="ltx_bibblock">Learning Robust Global Representations by Penalizing Local Predictive Power.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Neural Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html" title="">https://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-Art Natural Language Processing.

</span>
<span class="ltx_bibblock">In Qun Liu and David Schlangen (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pp.  38–45, Online, October 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-demos.6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-demos.6" title="">https://aclanthology.org/2020.emnlp-demos.6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2024)</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan.

</span>
<span class="ltx_bibblock">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  4818–4829, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2010)</span>
<span class="ltx_bibblock">
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba.

</span>
<span class="ltx_bibblock">SUN database: Large-scale scene recognition from abbey to zoo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, pp.  3485–3492, June 2010.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2010.5539970</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/5539970" title="">https://ieeexplore.ieee.org/document/5539970</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024a)</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan.

</span>
<span class="ltx_bibblock">Qwen2 Technical Report, July 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.10671v4" title="">https://arxiv.org/abs/2407.10671v4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024b)</span>
<span class="ltx_bibblock">
Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui.

</span>
<span class="ltx_bibblock">Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, pp.  56704–56721. PMLR, July 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v235/yang24ai.html" title="">https://proceedings.mlr.press/v235/yang24ai.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2020)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby.

</span>
<span class="ltx_bibblock">A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark, February 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1910.04867" title="">http://arxiv.org/abs/1910.04867</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2022)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">LiT: Zero-Shot Transfer With Locked-Image Text Tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  18123–18133, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html" title="">https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Control Policies</h3>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text Prompt Templates.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">We provide example control policies for text synthesis as predefined prompt templates, the first five templates do not include original text:</p>
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">"Create a detailed and high-quality caption using phrases that represent the entities or objects, their unique attributes, and the visual relationships in the scene depicted. Phrases: {phrases}."</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">"Compose a rich and immersive caption by incorporating a set of phrases that illustrate the entities or objects, their defining attributes, and the interconnections presented within the image. Phrases: {phrases}."</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">"Formulate an articulate and informative caption by using a series of phrases that outline the entities, their attributes, and their visual relationships depicted in an image. Phrases: {phrases}."</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">"Using a set of phrases that highlight the entities, attributes, and their visual associations in an image, craft a detailed and expressive caption. Phrases: {phrases}."</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">"Construct a comprehensive and expressive caption by integrating phrases that detail the entities, their features, and the spatial or thematic relationships in an image. Phrases: {phrases}."</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.2">The following five templates include the original text, which is useful for maintaining the original meaning:</p>
<ol class="ltx_enumerate" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">"Create a comprehensive caption that faithfully represents the objects, attributes, and their relationships contained within the provided sentence and phrases. Given sentence: {caption}. Given phrases: {phrases}. If the original caption specifies particular give phrases, maintain their integrity while using the phrases to enhance the description."</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">"Write a faithful caption by integrating the given phrases with the original sentence. Given sentence: {caption}. Given phrases: {phrases}. Ensure any objects or specific nouns from the original caption are preserved while elaborating on the visual relationships and attributes provided in the phrases to create a more detailed depiction."</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">"Provide a faithful and informative image caption using a given sentence and few phrases. Sentence: {caption}, phrases: {phrases}. Consider the initial sentence as a base for the overall context and ensure that specific objects or nouns such as numbers, car models, animals, etc., are preserved in the new caption. Integrate the given phrases, which describe entities, attributes, or visual relationships, to enrich and elaborate on the original meaning. Maintain fidelity to the original content while enhancing descriptive quality."</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">"Make a detailed caption based on the given phrases and a given sentence. Given phrases: {phrases}. Given sentence: {caption}. The sentence serves as a foundation, while the phrases elaborate on elements depicted in the image, like objects, their characteristics, and interactions. Preserve any pivotal information concerning objects, attributes, and their relations present in the sentence."</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1">"Write a new faithful and high-quality caption based on the given phrases and a given sentence. The given sentence is the original caption and the phrases are entities or objects, attributes, and their visual relationships in an image. Given sentence: {caption}. Given phrases: {phrases}. If the sentence contains objects or nouns (e.g. digits, car models, planes, pets, animals, etc.), the new caption should be faithful and keep this information. Otherwise, use the phrases to create the new caption."</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Image Prompt Templates.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">We provide five image prompt templates:</p>
<ol class="ltx_enumerate" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1">"real": "a real photo. {prompt}. 35mm photograph, film, bokeh, professional, 4k, highly detailed",</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1">"nocap": "a real photo showing {prompt}. highly detailed"</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p" id="A1.I3.i3.p1.1">"isometric": "isometric style {prompt} . vibrant, beautiful, crisp, detailed, ultra detailed, intricate"</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I3.i4.p1">
<p class="ltx_p" id="A1.I3.i4.p1.1">"enhance": "breathtaking {prompt}. award-winning, professional, highly detailed"</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="A1.I3.i5.p1">
<p class="ltx_p" id="A1.I3.i5.p1.1">"quality": "masterpiece, best quality, ultra detailed, {prompt}. intricate details"</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Datasets Details</h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Datasets.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">We list the vision datasets for evaluation in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.T9" title="In Evaluation Datasets. ‣ A.2 Datasets Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Details of evaluation datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T9.1">
<tr class="ltx_tr" id="A1.T9.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.1.1.1">Dataset</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.1.1.2">Metric</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="A1.T9.1.1.3">Classes</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt" id="A1.T9.1.1.4">Test Set Size</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.1.2.1">CIFAR-10 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib21" title="">2009</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.1.2.2">Accuracy</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T9.1.2.3">10</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A1.T9.1.2.4">10000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.3">
<td class="ltx_td ltx_align_left" id="A1.T9.1.3.1">CIFAR-100 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib21" title="">2009</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.3.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.3.3">100</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.3.4">10000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.4">
<td class="ltx_td ltx_align_left" id="A1.T9.1.4.1">CLEVR Counts</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.4.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.4.3">8</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.4.4">15000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.5">
<td class="ltx_td ltx_align_left" id="A1.T9.1.5.1">CLEVR Distance</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.5.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.5.3">6</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.5.4">15000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.6">
<td class="ltx_td ltx_align_left" id="A1.T9.1.6.1">Caltech-101 <cite class="ltx_cite ltx_citemacro_citep">(Fei-Fei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib12" title="">2006</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.6.2">Mean Per Class Recall</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.6.3">102</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.6.4">6085</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.7">
<td class="ltx_td ltx_align_left" id="A1.T9.1.7.1">Country211 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib44" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.7.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.7.3">211</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.7.4">21100</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.8">
<td class="ltx_td ltx_align_left" id="A1.T9.1.8.1">DTD <cite class="ltx_cite ltx_citemacro_citep">(Cimpoi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib5" title="">2014</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.8.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.8.3">47</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.8.4">1880</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.9">
<td class="ltx_td ltx_align_left" id="A1.T9.1.9.1">EuroSAT <cite class="ltx_cite ltx_citemacro_citep">(Helber et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib14" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.9.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.9.3">10</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.9.4">5400</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.10">
<td class="ltx_td ltx_align_left" id="A1.T9.1.10.1">FGVC Aircraft <cite class="ltx_cite ltx_citemacro_citep">(Maji et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib33" title="">2013</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.10.2">Mean Per Class Recall</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.10.3">100</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.10.4">3333</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.11">
<td class="ltx_td ltx_align_left" id="A1.T9.1.11.1">Food-101 <cite class="ltx_cite ltx_citemacro_citep">(Bossard et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib2" title="">2014</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.11.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.11.3">101</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.11.4">25250</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.12">
<td class="ltx_td ltx_align_left" id="A1.T9.1.12.1">GTSRB  <cite class="ltx_cite ltx_citemacro_citep">(Stallkamp et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib51" title="">2011</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.12.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.12.3">43</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.12.4">12630</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.13">
<td class="ltx_td ltx_align_left" id="A1.T9.1.13.1">KITTI <cite class="ltx_cite ltx_citemacro_citep">(Geiger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib13" title="">2013</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.13.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.13.3">4</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.13.4">711</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.14">
<td class="ltx_td ltx_align_left" id="A1.T9.1.14.1">Oxford Flowers-102 <cite class="ltx_cite ltx_citemacro_citep">(Nilsback &amp; Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib39" title="">2008</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.14.2">Mean Per Class Recall</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.14.3">102</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.14.4">6149</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.15">
<td class="ltx_td ltx_align_left" id="A1.T9.1.15.1">Oxford-IIIT Pet <cite class="ltx_cite ltx_citemacro_citep">(Parkhi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib41" title="">2012</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.15.2">Mean Per Class Recall</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.15.3">37</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.15.4">3669</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.16">
<td class="ltx_td ltx_align_left" id="A1.T9.1.16.1">PatchCamelyon <cite class="ltx_cite ltx_citemacro_citep">(Veeling et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib56" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.16.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.16.3">2</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.16.4">32768</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.17">
<td class="ltx_td ltx_align_left" id="A1.T9.1.17.1">RESISC45 <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib4" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.17.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.17.3">45</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.17.4">6300</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.18">
<td class="ltx_td ltx_align_left" id="A1.T9.1.18.1">STL-10 <cite class="ltx_cite ltx_citemacro_citep">(Coates et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib6" title="">2011</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.18.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.18.3">10</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.18.4">8000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.19">
<td class="ltx_td ltx_align_left" id="A1.T9.1.19.1">SUN397 <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib61" title="">2010</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.19.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.19.3">397</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.19.4">108754</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.20">
<td class="ltx_td ltx_align_left" id="A1.T9.1.20.1">SVHN <cite class="ltx_cite ltx_citemacro_citep">(Netzer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib38" title="">2011</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.20.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.20.3">10</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.20.4">26032</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.21">
<td class="ltx_td ltx_align_left" id="A1.T9.1.21.1">Stanford Cars <cite class="ltx_cite ltx_citemacro_citep">(Krause et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib20" title="">2013</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.21.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.21.3">196</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.21.4">8041</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.22">
<td class="ltx_td ltx_align_left" id="A1.T9.1.22.1">ImageNet-1K <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib7" title="">2009</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.22.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.22.3">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.22.4">50000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.23">
<td class="ltx_td ltx_align_left" id="A1.T9.1.23.1">ImageNet-V2 <cite class="ltx_cite ltx_citemacro_citep">(Recht et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib45" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.23.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.23.3">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.23.4">10000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.24">
<td class="ltx_td ltx_align_left" id="A1.T9.1.24.1">ImageNet-S <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib58" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.24.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.24.3">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.24.4">50889</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.25">
<td class="ltx_td ltx_align_left" id="A1.T9.1.25.1">ImageNet-A <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib16" title="">2021b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.25.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.25.3">200</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.25.4">7500</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.26">
<td class="ltx_td ltx_align_left" id="A1.T9.1.26.1">ImageNet-O <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib16" title="">2021b</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.26.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.26.3">200</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.26.4">2000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.27">
<td class="ltx_td ltx_align_left" id="A1.T9.1.27.1">ImageNet-R <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib15" title="">2021a</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.27.2">Accuracy</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.27.3">200</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.27.4">30000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.28">
<td class="ltx_td ltx_align_left" id="A1.T9.1.28.1">Flickr <cite class="ltx_cite ltx_citemacro_citep">(Plummer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib42" title="">2015</a>)</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T9.1.28.2">Mean Recall@1</td>
<td class="ltx_td ltx_align_right" id="A1.T9.1.28.3">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T9.1.28.4">1000</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.29">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.1.29.1">MSCOCO  <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib29" title="">2014</a>)</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.1.29.2">Mean Recall@1</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T9.1.29.3">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A1.T9.1.29.4">5000</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Long-tail Datasets.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">For the tail classes in ImageNet-LT and Places-LT, we generate synthetic images using the “real” style of image prompt template, and we generate 7 samples per tail class so that we roughly double the size of the original real datasets. We obtain 80.4k synthetic samples for ImageNet-LT and 55.2K for Places-LT.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Training Details</h3>
<figure class="ltx_table" id="A1.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Training hyper-parameters.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="A1.T10.st1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span>Pretraining CLIP on CC3M and CC12M.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T10.st1.1" style="width:56.4pt;height:60.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.4pt,86.9pt) scale(0.257371755746695,0.257371755746695) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T10.st1.1.1">
<tr class="ltx_tr" id="A1.T10.st1.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T10.st1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="A1.T10.st1.1.1.2.1.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.st1.1.1.2.2"><span class="ltx_text ltx_font_bold" id="A1.T10.st1.1.1.2.2.1">CC3M</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.st1.1.1.2.3"><span class="ltx_text ltx_font_bold" id="A1.T10.st1.1.1.2.3.1">CC12M</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T10.st1.1.1.3.1">Total iterations</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.st1.1.1.3.2">56,429</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.st1.1.1.3.3">55,429</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.4">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.4.1">Warmup iterations</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.4.2">2822</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.4.3">2771</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.5">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.5.1">Image size</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.5.2">224</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.5.3">224</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.6">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.6.1">LR scheduler</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.6.2">Cosine</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.6.3">Cosine</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.7">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.7.1">Max. LR</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.7.2">0.002</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.7.3">0.002</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.8">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.8.1">Min. LR</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.8.2">0.00002</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.8.3">0.00002</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.9">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.9.1">Optimizer</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.9.2">AdamW</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.9.3">AdamW</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.1">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.1.1">AdamW <math alttext="\beta" class="ltx_Math" display="inline" id="A1.T10.st1.1.1.1.1.m1.1"><semantics id="A1.T10.st1.1.1.1.1.m1.1a"><mi id="A1.T10.st1.1.1.1.1.m1.1.1" xref="A1.T10.st1.1.1.1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A1.T10.st1.1.1.1.1.m1.1b"><ci id="A1.T10.st1.1.1.1.1.m1.1.1.cmml" xref="A1.T10.st1.1.1.1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.st1.1.1.1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="A1.T10.st1.1.1.1.1.m1.1d">italic_β</annotation></semantics></math>’s</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.1.2">(0.9, 0.98)</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.1.3">(0.9, 0.98)</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.10">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.10.1">Weight decay</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.10.2">0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.10.3">0.2</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.11">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.11.1">Batch size per GPU</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.11.2">256</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.11.3">256</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.12">
<td class="ltx_td ltx_align_left" id="A1.T10.st1.1.1.12.1"># A100 GPUs</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.12.2">8</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st1.1.1.12.3">32</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st1.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T10.st1.1.1.13.1">A100 GPU Memory</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.st1.1.1.13.2">40 GB</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.st1.1.1.13.3">40 GB</td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="A1.T10.st2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span>Finetuning CLIP on Places-LT and ImageNet-LT.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T10.st2.1" style="width:64.2pt;height:60.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-92.6pt,86.9pt) scale(0.257371755746695,0.257371755746695) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T10.st2.1.1">
<tr class="ltx_tr" id="A1.T10.st2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T10.st2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.st2.1.1.1.1.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.st2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T10.st2.1.1.1.2.1">Places-LT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T10.st2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T10.st2.1.1.1.3.1">ImageNet-LT</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T10.st2.1.1.2.1">Total Iterations</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.st2.1.1.2.2">56,429</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.st2.1.1.2.3">55,429</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.3">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.3.1">Warmup Iterations</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.3.2">2822</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.3.3">2771</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.4">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.4.1">Image size</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.4.2">224</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.4.3">224</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.5">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.5.1">Loss type</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.5.2">CrossEntropy</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.5.3">CrossEntropy</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.6">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.6.1">LR scheduler</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.6.2">Cosine</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.6.3">Cosine</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.7">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.7.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.7.2">0.01</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.7.3">0.01</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.8">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.8.1">Optimizer</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.8.2">SGD</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.8.3">SGD</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.9">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.9.1">Momentum</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.9.2">0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.9.3">0.9</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.10">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.10.1">Weight decay</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.10.2">5e-4</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.10.3">5e-4</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.11">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.11.1">Batch size per GPU</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.11.2">128</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.11.3">128</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.12">
<td class="ltx_td ltx_align_left" id="A1.T10.st2.1.1.12.1"># A100 GPUs</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.12.2">1</td>
<td class="ltx_td ltx_align_center" id="A1.T10.st2.1.1.12.3">1</td>
</tr>
<tr class="ltx_tr" id="A1.T10.st2.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T10.st2.1.1.13.1">A100 GPU Memory</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.st2.1.1.13.2">40 GB</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.st2.1.1.13.3">40 GB</td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Hyper-parameters.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px1.p1.6">We pretrain the CLIP for the same number of iterations for both the baseline and CtrlSynth. For example, suppose we train for <math alttext="E" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="A1.SS3.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.1.m1.1d">italic_E</annotation></semantics></math> epochs, if the original dataset has <math alttext="N" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="A1.SS3.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.2.m2.1d">italic_N</annotation></semantics></math> samples, CtrlSynth has generated <math alttext="N^{\prime}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS3.SSS0.Px1.p1.3.m3.1a"><msup id="A1.SS3.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml">N</mi><mo id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.3" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><ci id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2">𝑁</ci><ci id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.3.m3.1c">N^{\prime}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.3.m3.1d">italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> samples (<math alttext="N^{\prime}&lt;=N" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS3.SSS0.Px1.p1.4.m4.1a"><mrow id="A1.SS3.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml"><msup id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml"><mi id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.2" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.2.cmml">N</mi><mo id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.3" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.3.cmml">′</mo></msup><mo id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml">&lt;=</mo><mi id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.3" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.4.m4.1b"><apply id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1"><leq id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1"></leq><apply id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2">superscript</csymbol><ci id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.2.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.2">𝑁</ci><ci id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.3.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.3">′</ci></apply><ci id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.4.m4.1c">N^{\prime}&lt;=N</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.4.m4.1d">italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT &lt; = italic_N</annotation></semantics></math> due to filtering), then the total samples are <math alttext="E*N" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS3.SSS0.Px1.p1.5.m5.1a"><mrow id="A1.SS3.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml">E</mi><mo id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml">∗</mo><mi id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.3" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.5.m5.1b"><apply id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1"><times id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1"></times><ci id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2">𝐸</ci><ci id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.5.m5.1c">E*N</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.5.m5.1d">italic_E ∗ italic_N</annotation></semantics></math>, we train CtrlSynth models for <math alttext="\frac{E*N}{N+N^{\prime}}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS3.SSS0.Px1.p1.6.m6.1a"><mfrac id="A1.SS3.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mrow id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml"><mi id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.2" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.2.cmml">E</mi><mo id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.1.cmml">∗</mo><mi id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.3" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.3.cmml">N</mi></mrow><mrow id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml"><mi id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.2" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.2.cmml">N</mi><mo id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.1" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">+</mo><msup id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.cmml"><mi id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.2" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.2.cmml">N</mi><mo id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.3" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.3.cmml">′</mo></msup></mrow></mfrac><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1"><divide id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1"></divide><apply id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2"><times id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.1.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.1"></times><ci id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.2.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.2">𝐸</ci><ci id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.3.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.2.3">𝑁</ci></apply><apply id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3"><plus id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.1.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.1"></plus><ci id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.2.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.2">𝑁</ci><apply id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.1.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3">superscript</csymbol><ci id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.2.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.2">𝑁</ci><ci id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.3.cmml" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.6.m6.1c">\frac{E*N}{N+N^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.6.m6.1d">divide start_ARG italic_E ∗ italic_N end_ARG start_ARG italic_N + italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math> epochs. This guarantees that the baseline and CtrlSynth CLIP models have seen the same number of data samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS3.SSS0.Px1.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.T10" title="In A.3 Training Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a> lists the hyper-parameters used for pretraining on CC3M and CC12m. We use AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib31" title="">2018</a>)</cite> with default <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.1.m1.1"><semantics id="A1.SS3.SSS0.Px1.p2.1.m1.1a"><mi id="A1.SS3.SSS0.Px1.p2.1.m1.1.1" xref="A1.SS3.SSS0.Px1.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p2.1.m1.1b"><ci id="A1.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p2.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p2.1.m1.1d">italic_β</annotation></semantics></math> values as an optimizer and binary cross-entropy loss as an objective function. We use cosine learning rate schedule <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib32" title="">2022</a>)</cite>. We use the CoreNet library <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib35" title="">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib34" title="">2022</a>)</cite> for all pretraining experiments. We adapt the LIFT codebase <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib50" title="">2024</a>)</cite> for fine-tuning long-tail tasks, main modifications include adding support for iteration-based training and data loader for multiple datasets.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>CtrlSynth Inference Details</h3>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">VTM.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS4.SSS0.Px1.p1.1">We use a hybrid tagging model consisting of two stages. We first run the ViT-Huge variant of CatLIP <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib36" title="">2024b</a>)</cite> for each image and output top20 classes based on the sigmoid score of prediction logits, then we convert the class indices to actual word labels. The vocabulary size of CatLIP is 24320. Most of the vocabulary words are nouns and single-word attributes.
We then run the Florence-large <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib60" title="">2024</a>)</cite> for each image to extract detailed captions using the task prompt <span class="ltx_text ltx_font_typewriter" id="A1.SS4.SSS0.Px1.p1.1.1">&lt;MORE_DETAILED_CAPTION&gt;</span>. After that, we run Qwen2-7B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib62" title="">2024a</a>)</cite> to extract objects, attributes, and relations from the Florence captions. We then merge the objects field with CatLIP-predicted labels. The extraction instruction contains a 2-shot example and we list the prompt template below:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.SS4.SSS0.Px1.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,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">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.1">For</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.7">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.10">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.12">identify</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.14">all</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.16">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.18">attributes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.19">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.21">objects</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.23">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.25">entities</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.26">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.28">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.30">visual</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.32">relationships</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.34">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.36">actions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.38">that</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.39"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.40">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.41"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.42">phrases</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.43">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.45">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.47">phrases</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.49">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.51">only</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.53">come</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.55">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.57">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.59">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.60">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.61"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.62">Separate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.63"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.64">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.65"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.66">phrases</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.67"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.68">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.69"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.70">comma</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.71"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.72">without</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.74">formatting</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.75">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.76"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.77">Output</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.78"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.79">three</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.80"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.81">lines</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.82">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.1">attributes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.4">phrases</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.1">objects</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.4">phrases</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.1">relations</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.4">phrases</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.1">Examples</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.2">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.4">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.6">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.8">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.10">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.12">close</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.13">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.14">up</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.16">portrait</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.18">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.20">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.22">middle</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.23">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.24">aged</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.26">man</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.28">wearing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.30">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.32">white</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.34">cowboy</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.36">hat</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.37">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.39">He</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.41">appears</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.43">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.45">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.47">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.49">his</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.51">late</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.52"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.53">60</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.54">s</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.55"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.56">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.57"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.58">early</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.59"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.60">70</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.61">s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.62">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.63"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.64">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.65"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.66">gray</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.67"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.68">hair</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.69"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.70">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.71"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.72">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.74">serious</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.75"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.76">expression</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.77"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.78">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.79"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.80">his</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.81"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.82">face</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.83">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.84"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.85">He</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.86"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.87">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.88"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.89">wearing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.90"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.91">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.92"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.93">dark</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.94"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.95">suit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.96"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.97">jacket</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.98"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.99">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.100"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.101">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.102"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.103">light</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.104"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.105">blue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.106"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.107">collared</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.108"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.109">shirt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.110">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.111"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.112">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.113"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.114">background</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.115"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.116">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.117"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.118">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.119"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.120">clear</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.121"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.122">blue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.123"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.124">sky</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.125"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.126">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.127"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.128">trees</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.129"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.130">visible</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.131"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.132">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.133"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.134">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.135"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.136">distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.137">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.138"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.139">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.140"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.141">man</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.142"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.143">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.144"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.145">looking</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.146"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.147">off</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.148"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.149">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.150"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.151">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.152"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.153">side</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.154"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.155">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.156"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.157">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.158"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.159">slight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.160"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.161">smile</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.162"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.163">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.164"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.165">his</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.166"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.167">lips</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.168">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1">attributes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.4">close</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.5">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.6">up</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.9">middle</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.10">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.11">aged</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.12">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.14">white</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.16">cowboy</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.18">hat</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.19">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.21">gray</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.23">hair</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.24">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.26">serious</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.28">expression</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.29">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.31">light</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.33">blue</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1">objects</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.4">portrait</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.5">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.7">man</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.10">hat</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.11">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.13">face</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.14">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.16">dark</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.18">suit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.20">jacket</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.21">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.23">shirt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.24">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.26">blue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.28">sky</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.29">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.31">trees</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.32">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.34">lips</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.1">relations</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.4">wearing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.6">a</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.9">visible</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.11">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.13">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.15">distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.16">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.18">looking</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.20">off</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.22">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.24">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.26">side</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.27">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.29">slight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.31">smile</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.33">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.35">his</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.37">lips</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.1">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.4">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.6">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.8">shows</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.10">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.12">female</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.14">singer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.16">performing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.18">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.20">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.22">stage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.23">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.25">She</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.27">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.29">standing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.31">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.33">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.35">set</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.37">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.39">stairs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.41">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.43">her</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.45">legs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.47">spread</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.49">apart</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.51">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.53">holding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.55">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.57">microphone</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.59">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.60"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.61">her</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.62"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.63">hand</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.64">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.65"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.66">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.67"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.68">stage</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.69"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.70">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.71"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.72">lit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.74">up</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.75"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.76">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.77"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.78">red</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.79"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.80">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.81"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.82">blue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.83"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.84">lights</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.85"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.86">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.87"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.88">there</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.89"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.90">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.91"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.92">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.93"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.94">large</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.95"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.96">circular</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.97"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.98">screen</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.99"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.100">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.101"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.102">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.103"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.104">background</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.105">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.106"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.107">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.108"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.109">singer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.110"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.111">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.112"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.113">wearing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.114"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.115">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.116"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.117">black</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.118"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.119">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.120"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.121">white</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.122"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.123">patterned</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.124"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.125">outfit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.126"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.127">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.128"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.129">high</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.130"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.131">heels</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.132">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.133"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.134">She</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.135"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.136">appears</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.137"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.138">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.139"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.140">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.141"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.142">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.143"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.144">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.145"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.146">middle</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.147"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.148">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.149"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.150">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.151"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.152">song</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.153"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.154">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.155"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.156">performance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.157">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">attributes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.4">female</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.6">singer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.9">stage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.10">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.12">set</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.14">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.16">stairs</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.17">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.19">red</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.21">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.23">blue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.25">lights</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.26">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.28">large</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.30">circular</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.32">screen</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.33">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.35">black</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.37">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.39">white</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.41">patterned</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.43">outfit</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.44">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.45"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.46">high</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.47"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.48">heels</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.1">objects</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.4">female</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.6">singer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.9">stage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.10">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.12">set</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.14">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.16">stairs</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.17">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.19">legs</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.20">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.22">microphone</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.23">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.25">screen</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.26">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.28">outfit</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.29">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.31">high</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.33">heels</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.34">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.36">song</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.37">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.39">performance</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1">relations</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.4">performing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.6">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.8">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.10">stage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.11">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.13">standing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.15">on</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.16">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.18">her</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.20">legs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.22">spread</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.24">apart</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.25">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.27">holding</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.28">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.30">lit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.32">up</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.33">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.35">background</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.36">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.38">wearing</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.39">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.41">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.43">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.45">middle</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.47">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.49">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.51">song</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.1">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.5">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.6">}</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS4.SSS0.Px1.p2.1">CatLIP is available in CoreNet so we use it directly for inference and we wrap the Florence Transformers <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib59" title="">2020</a>)</cite> code into the CoreNet inference pipeline for easier integration.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">LLM.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS4.SSS0.Px2.p1.1">We use the vLLM engine <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib22" title="">2023</a>)</cite> for offline inference in Qwen2 and Mistral-Nemo. We use greedy decoding for the generation.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Text-to-image Model.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS4.SSS0.Px3.p1.1">We use the diffusers <cite class="ltx_cite ltx_citemacro_citep">(von Platen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#bib.bib57" title="">2022</a>)</cite> library for diffusion model inference. For both SDXL and SD3M models, we use float16 dtype with a guidance scale of 7.0 and set the diffusion steps to 28.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>More Analysis Details</h3>
<section class="ltx_paragraph" id="A1.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">CtrlSynth Samples.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS5.SSS0.Px1.p1.1">For CC3M, the original dataset has 2.8 million image-caption pairs, CtrlSynth-cap contains 2.6 million captions, CtrlSynth-img contains 2.4 million images, and CtrlSynth-mix contains 5.1 million image-caption pairs. Original CC12M has 11.3 million image-caption samples, CtrlSynth-cap consists of 10.2 million captions, CtrlSynth-img contains 9.5 million images, and CtrlSynth-mix has 19.7 million image-caption pairs.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS5.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">CtrlSynth Synthetic Texts.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS5.SSS0.Px2.p1.1">We plot the number of words for synthetic texts generated by CtrlSynth and compare them with original real texts in <a class="ltx_ref" href="https://arxiv.org/html/2410.11963v1#A1.F8" title="In CtrlSynth Synthetic Texts. ‣ A.5 More Analysis Details ‣ Appendix A Appendix ‣ CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="302" id="A1.F8.g1" src="x9.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Number of words for the original captions and CtrlSynth synthetic texts on CC3M.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 15 18:08:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
