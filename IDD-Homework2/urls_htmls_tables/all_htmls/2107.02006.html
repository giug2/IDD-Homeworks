<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.02006] Towards Node Liability in Federated Learning: Computational Cost and Network Overhead</title><meta property="og:description" content="Many machine learning (ML) techniques suffer from the drawback that their output (e.g., a classification decision)
is not clearly and intuitively connected to their input (e.g., an image). To cope with this issue,
seve…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Node Liability in Federated Learning: Computational Cost and Network Overhead">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Node Liability in Federated Learning: Computational Cost and Network Overhead">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.02006">

<!--Generated on Tue Mar 12 08:36:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Node Liability in Federated Learning:
<br class="ltx_break">Computational Cost and Network Overhead</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Malandrino, Carla Fabiana Chiasserini
</span><span class="ltx_author_notes">This work was supported through the EU 5Growth project (Grant No. 856709).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Many machine learning (ML) techniques suffer from the drawback that their output (e.g., a classification decision)
is not clearly and intuitively connected to their input (e.g., an image). To cope with this issue,
several <span id="id1.id1.1" class="ltx_text ltx_font_italic">explainable ML</span> techniques have been proposed to, e.g., identify which pixels of an input image
had the strongest influence on its classification. However, in distributed scenarios, it is often more important
to connect decisions with the information used for the model training and the nodes supplying such
information. To this end, in this paper we focus on federated learning and present a new methodology, named
<span id="id1.id1.2" class="ltx_text ltx_font_italic">node liability in federated learning</span> (NL-FL),
which permits to identify the source of the training information that most contributed to a given decision.
After discussing NL-FL’s cost in terms of extra computation, storage, and network latency, we demonstrate its
usefulness in an edge-based scenario. We find that NL-FL is able to swiftly identify misbehaving nodes and
to exclude them from the training process, thereby improving learning accuracy.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Originally introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, federated learning (FL) is one of the most relevant
approaches to the task of distributed machine learning (ML). Its main advantage is that the participating
<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">learning nodes</span> can cooperatively train a single ML model (usually a deep neural network, DNN)
through an iterative procedure,
without sharing their local data, which may be private and/or sensitive.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As summarized in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, each iteration of the FL paradigm, also called epoch,
consists of the following main steps. First,
each learning node trains its local model using its own data (step 1 in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and sends the local model
parameters to a <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">learning server</span> (step 2),
with the latter being often located at an edge host in a mobile network scenario <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
The learning server averages the received parameters (step 3), and sends
the averaged model back to the learning nodes (step 4), which use it in the subsequent epoch.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">FL is a very popular option in scenarios where nodes
belong to different individuals and/or administrative entities, as it is the case for user
personal devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> or fog nodes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
In such cases, nodes often exchange the data messages depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> through wireless
networks belonging to a third party.
However, limited and/or unreliable connectivity may negatively impact learning performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Typically, a decision made through the trained model during the inference phase,
e.g., the classification of an image,
contains no indication on how it was obtained, e.g., (i) based on which pixels of the image itself, and/or (ii)
which images from the training set have most contributed to such decision.
Analyzing the latter aspect allows establishing a link between decisions (e.g., a self-driving car failing to recognize a highway barrier, or
Microsoft’s Tay chatbot bringing up conspiracy theories) and the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">source</span> of the training data determining
such decisions (e.g., which crowd-sourced images or tweets).
The same issue is present
in all cooperative learning scenarios, where it is of paramount importance to identify the learning nodes
that, due to malicious behavior
or simple malfunctioning, inject incorrect information in the learning process.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we tackle this issue by introducing the methodology of <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">node liability in federated learning</span> (NL-FL).
NL-FL aims at identifying
the learning nodes that provided, during the <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">training phase</span>, the information that had the
strongest influence over a given decision, e.g., a wrong manuever by a self-driving car.
To this end, NL-FL leverages results from layer-wise
relevance propagation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and techniques from
model weighting and client selection in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Once the misbehaving learning nodes are identified, they can be removed from the learning process;
further action (e.g., a criminal investigation) can be taken if warranted.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In order to be suitable to fog networking scenarios, with fleeting connectivity and energy constrained
nodes, NL-FL neither leads to additional network overhead compared to traditional FL,
nor requires additional operations at the learning nodes. Indeed, all tasks are performed by the learning server, which is best equipped to perform them.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2107.02006/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
The main steps performed at each training epoch in FL.

</figcaption>
</figure>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of this paper is organized as follows.
First, Sec. <a href="#S2" title="II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reviews state-of-the-art works
in the areas of FL client selection, explainable ML, and layer-wise relevance propagation.
Then Sec. <a href="#S3" title="III Node liability in federated learning ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> introduces NL-FL,
describing its underlying principles and implementation details. To showcase the effectiveness of NL-FL,
we assess its ability to identify the misbehaving nodes in a realistic FL scenario,
as detailed in Sec. <a href="#S4" title="IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
Finally, in Sec. <a href="#S5" title="V Discussion and open challenges ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> we discuss the outcome of our experiments and sketch open challenges calling
for further research, before concluding the paper in Sec. <a href="#S6" title="VI Conclusion ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2107.02006/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Relevance computation during the inference phase: Schematic view of how a simple DNN processes input images to reach a decision (top, green arrows),
and how relevance is propagated across layers (bottom, red arrows).
Inputs and outputs of fully-connected layers (right) are one-dimensional (“flat”) tensors, hence,
they are represented by their empirical pdf.

</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Current issues in FL management</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Managing a large-scale, distributed learning process like FL is challenging for several reasons,
including (i) the need to select the learning nodes to rely upon and properly weight the information
they provide, (ii) the need to <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">explain</span> the decisions reached,
and (iii) the need to establish the <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">relevance</span> of different parts of the input
on the overall decisions. Below, we discuss how each of these aspects has been dealt
with in the literature.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Client selection and model weighting in FL</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As DNNs require large datasets to be trained properly,
the ability of individual learning nodes to contribute to the global learning
task strongly depends upon
the quantity of data at their disposal. To account for this, the learning server
can make the weights assigned to local models
in the averaging phase
(step 3 of Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) proportional to the size of local datasets, as done
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Along with the quantity of data available to learning nodes, its quality,
e.g., whether or not it is i.i.d.,
has a very important impact on the
training performance.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Three main approaches exist to tackle low-quality local data: (i) data augmentation, e.g., combining local and
remote data samples to obtain new samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>; (ii) selecting i.i.d. subsets
from local datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; (iii) weighting local models according to a measure of data quality,
e.g., entropy.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Besides their local datasets, learning nodes can have different computational capabilities and/or different ways
to connect to the edge server.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">One approach, followed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, is to assign more resources (e.g., radio resource blocks)
to the nodes that need them the most (e.g., experience connectivity issues), so as to avoid performance bottlenecks.
Another option is simply to drop overly-slow nodes (“stragglers”) from the learning process,
thus making individual iterations faster <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">Finally, not all clients
always behave correctly. A node may create multiple identities to
influence the learning process (Sybil attack), or it may
send incorrect updates to the learning server. As discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>,
such nodes can be identified by leveraging the notion of <span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_italic">distance</span> between local and global models:
consistently high distances suggest malfunctioning, while too-close local models coming from different nodes can reveal a Sybil attach.
An alternative approach is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, leveraging blockchain technology to build a reputation system for learning nodes.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Explainable ML</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The high-level goal of Explainable ML (XML) is to make the decisions of ML systems understandable by humans. This is often achieved by spelling out which elements of the input have had the most significant impact on the decision itself, e.g., “the application for a credit card was denied because the income was lower than a threshold”.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">ML techniques based on decisions trees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> naturally lend themselves to XML, and
have long been the most popular option whenever the right to explanation has to be guaranteed.
Similar to flow charts, the root and intermediate nodes of decision trees represent conditions against which
input data can be checked (e.g., whether the amount of a transaction is above a threshold);
the leaves correspond to the ML decision, e.g., whether the transaction is deemed a fraud.
Decision tree learning algorithms seek to optimize the order and content of the rules, so as to maximize the
learning quality.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">On the negative side, not all types of data are suitable for decision trees,
and many ML applications leverage DNNs instead.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Due to their relevance and popularity, several techniques have been developed in order to explain DNN decisions;
among such techniques, the most promising one is layer-wise relevance propagation, discussed next.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Layer-wise relevance propagation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The high-level goal of layer-wise relevance, introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, is to explain DNN decisions. Considering as an example the image classification task exemplified in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we want to associate a <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">relevance score</span> (step 5) to each pixel of the original image (step 1), expressing how important each pixel was in reaching the decision. In the figure, we can observe that pixels corresponding to <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">transitions</span> between dark and light parts of the image tend to have high relevance, while pixels on the background have low relevance: recalling that shapes are defined by transitions between light and dark areas of an image, it makes intuitive sense that those areas matter the most for shape classification.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Relevance values are computed according to the rules laid out in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>:
we start from the neuron in the output layer associated with the highest score (i.e., corresponding to the final decision),
and traverse the DNN back towards the input. At each step, layer-wise relevance values
express how strongly each element of the previous layer has influenced each element of the current one.
Importantly, layer-wise relevance values (steps 4a–4c in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) have the same shape as the corresponding
intermediate outputs (steps 2a–2c in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
This holds also for the one-dimensional tensors that are inputs and outputs of the fully-connected layers on the right,
which are represented by their probability density functions (pdf).
As a result, the final relevance scores (step 5 in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) can be mapped to individual pixels of the input image.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Crucially, relevance values are computed during the <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">inference</span> phase, and do not require any change to the
training phase.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Motivation and novelty</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">NL-FL is a variant of the FL paradigm, and it works exactly like ordinary FL when no issues are present, i.e., when all nodes behave correctly. When misbehaving nodes are present, NL-FL seeks to identify and exclude them in a similar way to the client selection techniques described in Sec. <a href="#S2.SS1" title="II-A Client selection and model weighting in FL ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>; however, it is able to only focus on a subset of <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">specific decisions</span>, e.g., racist tweets by Microsoft Tay. This allows NL-FL to better deal with sophisticate attackers, aiming to only sway some decisions as opposed to sabotaging the whole learning process. At the same time, nodes providing different updates due to their local datasets are not suspected.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The purpose of NL-FL is similar to the XML techniques discussed in Sec. <a href="#S2.SS2" title="II-B Explainable ML ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>, in that both seek to explain ML decisions. However, XML focuses on which parts of the input (e.g., which pixels of an image) influenced a given decision (“this guard-rail was identified as a road line because of these white traces on it”). Conversely, NL-FL aims at identifying the <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">training-time</span> data such decisions are based upon, and their source (“user CrackMonkey74 tagged these photos of guard-rails as road lanes”).</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">NL-FL reaches its goal by combining the relevance techniques reviewed in Sec. <a href="#S2.SS3" title="II-C Layer-wise relevance propagation ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a> with information on the difference between local and global models, as set forth next.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2107.02006/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Operations during the training (green) and inference (blue) phases of the FL paradigm,
and of the NL-FL (red).

</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Node liability in federated learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We now introduce the proposed NL-FL technique that seeks to establish how much influence individual
learning nodes (and their data sets) have on a given decision during the inference phase.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Fig. <a href="#S2.F3" title="Figure 3 ‣ II-D Motivation and novelty ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the operations carried out by the FL paradigm and the
additional ones envisioned by our proposed NL-FL. Specifically, the green box in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-D Motivation and novelty ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>
corresponds to the training phase of FL, where learning nodes send local updates to the learning server,
and the latter computes the average (global) model (see also Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The blue box denotes the inference phase, where the trained (global) model
is used to make decisions (e.g., classification) over individual samples.
The red box includes the operations performed as a part of our NL-FL methodology.
Numbers in the box identify the chronological order of each step.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">During the training phase, the learning server
leverages the information sent by learning nodes as a part of the FL methodology to keep
a log of how similar the updates coming
from learning nodes at each epoch are to the average model. In particular,
the learning server computes the cosine distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> between the local
updates and the average model for each epoch,
node, and layer, and
locally
stores such information in a three-dimensional tensor.
Whenever it is necessary to review an inference-time decision, the server combines distance information with the layer-wise relevance values it can compute as per Sec. <a href="#S2.SS3" title="II-C Layer-wise relevance propagation ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>,
and uses them to compute
a new metric, that we named <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">relevance-aware distance</span> (<span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span>).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">More specifically, for each sample processed through the trained model during the inference phase,
the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric is obtained by multiplying the aforementioned three-dimensional tensor by a
column vector with as many elements as the number of layers, containing the layer-wise relevance
corresponding to that sample.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The result is a matrix, containing the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> values for each epoch and node.
If the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric for a specific node, averaged over the epochs,
is significantly
(e.g., by a factor <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p5.1.m1.1a"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\alpha</annotation></semantics></math>) larger than the average computed over the nodes and the epochs,
that node is identified as misbehaving,
and additional action can be taken – e.g., re-training the model excluding the misbehaving node.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Thanks to the way it is defined
and to the fact that it leverages information from <span id="S3.p6.1.1" class="ltx_text ltx_font_italic">both the training and inference phases</span> of FL,
the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric is able to achieve the two main objectives of NL-FL,
namely (i) quantifying the influence of <span id="S3.p6.1.2" class="ltx_text ltx_font_italic">individual</span> learners (and their datasets)
on the training process (unlike layer-wise relevance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>),
and (ii) focusing on <span id="S3.p6.1.3" class="ltx_text ltx_font_italic">specific</span> decisions, as opposed to considering all parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">We stress that NL-FL differs from XML techniques in that it does not
explain the decision in terms of the present, inference-phase input (step 5 of Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>),
but rather in terms of past, training-phase contributions from learning nodes.
At the same time,
an important feature of NL-FL is that it holds learning nodes liable for <span id="S3.p7.1.1" class="ltx_text ltx_font_italic">specific</span> decisions concerning
<span id="S3.p7.1.2" class="ltx_text ltx_font_italic">individual</span> input samples, e.g., the classification of a given image. This is in contrast with
similarity-based techniques to identify misbehaving learning nodes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>,
which account for the overall similarity between parameter updates.
Thanks to this feature, NL-FL can identify misbehaving nodes also in scenarios where they only mis-label
a small fraction of their local data.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">Finally, it is worth highlighting that
computing the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric – and, in general, achieving the NL-FL vision –
entails <span id="S3.p8.1.1" class="ltx_text ltx_font_italic">a modest cost</span> in terms of additional data storage and computation.
Indeed, we first need to compute the relevance values during the inference phase.
As per <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>,
the cost of computing the relevance values for a sample is similar to that of performing
a round of back-propagation during training – actually smaller, as there is no optimization to perform.
Then, as per Fig. <a href="#S2.F3" title="Figure 3 ‣ II-D Motivation and novelty ‣ II Current issues in FL management ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the learning server needs to store distance and relevance information.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">The former only takes up a small extra space at the learning server,
while the latter can be quite significant, comparable to the size of the items being classified.
For this reason, as discussed in Sec. <a href="#S5" title="V Discussion and open challenges ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, it is often preferable to compute relevance values on
an as-needed basis.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">Notice that all such extra requirements brought by NL-FL affect the learning server:
<span id="S3.p10.1.1" class="ltx_text ltx_font_italic">no extra load</span> is placed on the learning nodes, which are often resource-constrained devices.
Also, no additional operations are necessary during the <span id="S3.p10.1.2" class="ltx_text ltx_font_italic">training</span> phase, which is usually
the most resource-intensive one, and no additional data is transmitted over the network.
In particular, NL-FL does not require learning nodes to share any additional information,
thus the accountability brought by NL-FL does not come at
the cost of jeopardizing FL’s privacy properties.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment design and results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now describe the behavior of the learning nodes in our experiments,
the data at their disposal, and the DNN they run, and we present our results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Reference scenario and benchmarks</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We consider a typical medium-scale edge scenario <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, with 10 learning nodes connected with, and coordinated by, an edge-based learning server. Each learning node has a local dataset of 4,000 images coming from the EMNIST dataset, representing handwritten digits or letters. Using the EMNIST dataset allows us to obtain meaningful, easy-to-generalize results, without the need to implement overly complex DNNs.
To evaluate <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> in a more challenging scenario, we make local datasets non-i.i.d., by assigning to each learning node a randomly-chosen symbol (digit or letter) that is ten times more frequent than the others.
The learning nodes perform a classification task, i.e., associating each image with the character it represents.
To this end, as in the original EMNIST paper, nodes run a DNN with four layers:
two convolutional ones followed by two fully-connected ones.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">Nine out of the ten learning nodes are correct, i.e., all their local data are truthfully labeled.
A tenth node misbehaves; specifically, it labels all occurrences of character <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">x</annotation></semantics></math> as digit <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">9</annotation></semantics></math>.
Such a pattern can be observed in two very different and equally relevant real-world situations:
(i) a sophisticate attack, performed by a malicious node interested in swaying the behavior of the classifier for only some of the classes, or
(ii) a honest but malfunctioning node, subject to a failure of its equipment.
Intuitively, we expect a misbehaving node mislabeling only some of its local data to be harder to detect than a node
sending bogus, e.g., random updates. Indeed, in our case, the parameters of the misbehaving node will be very close to those of the correct nodes, hence, naive distance metrics such as those used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> may not be sufficient.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">All nodes participate in a FL task, sending parameter updates to the centralized learning server after each epoch;
the training process lasts 50 epochs in total. The server uses a simple averaging strategy to determine the global
parameters. Further, all nodes correctly follow the FL protocol, i.e., send their (true) local parameters and dutifully replace them with the global ones upon receiving them.
We further set <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\alpha</annotation></semantics></math> to 2, leaving further study on the effect of such a parameter for future work.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2107.02006/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Accuracy for all symbols (yellow bars) and for the symbol <math id="S4.F4.2.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.F4.2.m1.1b"><mi id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><ci id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">x</annotation></semantics></math> (blue bars) when all nodes are correct (left), one node misbehaves (center), and the misbehaving node is removed (right).

</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Experimental results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">A first question we seek to answer is to which extent a single misbehaving node mislabeling a single character can affect
the overall learning process. To this end, in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Reference scenario and benchmarks ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we plot the classification accuracy for the whole dataset (yellow bars) and for the symbol <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">x</annotation></semantics></math> (blue bars) in three scenarios, namely: (i) when all nodes behave correctly; (ii) when one node misbehaves as detailed above, and (iii) when the misbehaving node is removed, and only the remaining nine take part in the learning.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">The average accuracy is not significantly affected by the presence of the misbehaving node; after all, only a small number of images is misclassified.
Focusing instead on the occurrences of letter <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">x</annotation></semantics></math> (blue bars) and comparing the first and second groups of bars,
it is possible to see a visible drop in the accuracy, which is even more serious because letter <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">x</annotation></semantics></math> is not very well
classified in the first place. Notice how such a significant effect is obtained by a single misbehaving node,
in spite of the nine correct ones. Finally, the rightmost group of bars shows that identifying and removing
the misbehaving node is sufficient to essentially restore the original accuracy.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2107.02006/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Scores for correct (gray lines) and misbehaving (red lines) nodes, under three scoring metrics: cosine distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (dotted lines), reputation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (dashed lines), <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> (solid lines).

</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We now seek to understand the usefulness of the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric in identifying such a node.
To this end, Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-B Experimental results ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the normalized <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">score</span> for correct (gray) and misbehaving (red) nodes,
where “score” is defined as:
(i) cosine distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (dotted lines);
(ii) a reputation system similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (dashed lines), where the reputation of a node reflects its ability to properly classify its local dataset;
(iii) <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> (solid lines).
The difference is very clear: dotted and dashed lines tend to lie close to each other, as both cosine distance and reputation tend to be swayed by the fact that even the misbehaving node behaves correctly <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">most of the times</span>, and that even correct nodes may occasionally provide wrong results.
On the other hand, when moving to the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric (solid lines), the misbehaving node emerges as having a significantly score distance than the others, for all epochs. In other words, relevance values act like a magnifying glass over the differences between parameters coming from correct and misbehaving nodes, allowing such differences to clearly emerge and drive decisions about client selection in FL.
This, in turn, allows us to neutralize the effect of the misbehaving nodes on the resulting accuracy, as shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Reference scenario and benchmarks ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>: specifically, leveraging the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric we can move from the second to the third group of bars in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Reference scenario and benchmarks ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
Overhead associated with NL-FL

</figcaption>
<table id="S4.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<th id="S4.T1.2.3.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.3.1.1.1.1" class="ltx_p"><span id="S4.T1.2.3.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Metric</span></span>
</span>
</th>
<th id="S4.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.2.3.1.2.1" class="ltx_text" style="font-size:80%;">FL</span></th>
<th id="S4.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.2.3.1.3.1" class="ltx_text" style="font-size:80%;">NL-FL</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.4.1" class="ltx_tr">
<th id="S4.T1.2.4.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">
<span id="S4.T1.2.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.4.1.1.1.1" class="ltx_p"><span id="S4.T1.2.4.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Training time</span></span>
</span>
</th>
<td id="S4.T1.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2"><span id="S4.T1.2.4.1.2.1" class="ltx_text" style="font-size:80%;">27 ms/sample</span></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.3.1.1" class="ltx_p"><span id="S4.T1.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Inference time</span></span>
</span>
</th>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.1.1" class="ltx_text" style="font-size:80%;">130 </span><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mi mathsize="80%" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\mu</annotation></semantics></math><span id="S4.T1.1.1.1.2" class="ltx_text" style="font-size:80%;">s/sample</span>
</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">153 </span><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mi mathsize="80%" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\mu</annotation></semantics></math><span id="S4.T1.2.2.2.2" class="ltx_text" style="font-size:80%;">s/sample</span>
</td>
</tr>
<tr id="S4.T1.2.5.2" class="ltx_tr">
<th id="S4.T1.2.5.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.5.2.1.1.1" class="ltx_p"><span id="S4.T1.2.5.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Model size</span></span>
</span>
</th>
<td id="S4.T1.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T1.2.5.2.2.1" class="ltx_text" style="font-size:80%;">3.41 MByte</span></td>
</tr>
<tr id="S4.T1.2.6.3" class="ltx_tr">
<th id="S4.T1.2.6.3.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.6.3.1.1.1" class="ltx_p"><span id="S4.T1.2.6.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Data transfer rate</span></span>
</span>
</th>
<td id="S4.T1.2.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T1.2.6.3.2.1" class="ltx_text" style="font-size:80%;">52.5 kbit/s</span></td>
</tr>
<tr id="S4.T1.2.7.4" class="ltx_tr">
<th id="S4.T1.2.7.4.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.7.4.1.1.1" class="ltx_p"><span id="S4.T1.2.7.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Similarity info. size</span></span>
</span>
</th>
<td id="S4.T1.2.7.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.7.4.2.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T1.2.7.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.7.4.3.1" class="ltx_text" style="font-size:80%;">7.2 kByte/epoch/node</span></td>
</tr>
<tr id="S4.T1.2.8.5" class="ltx_tr">
<th id="S4.T1.2.8.5.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.2.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.8.5.1.1.1" class="ltx_p"><span id="S4.T1.2.8.5.1.1.1.1" class="ltx_text" style="font-size:80%;">Relevance info. size</span></span>
</span>
</th>
<td id="S4.T1.2.8.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.8.5.2.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T1.2.8.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.8.5.3.1" class="ltx_text" style="font-size:80%;">4 kByte/sample</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Last, Tab. <a href="#S4.T1" title="TABLE I ‣ IV-B Experimental results ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the overhead of NL-FL in terms of computational time, additional storage, and network latency.
As discussed in Sec. <a href="#S3" title="III Node liability in federated learning ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we can observe that NL-FL comes at a modest cost in terms of inference time, and the learning
server has to store additional information concerning how close updates from different nodes are, and relevance values for
each sample.
The space taken by similarity information grows linearly with the number of learning nodes, e.g.,
a scenario including 1,000 learning nodes executing 100 epochs would result in
720 MByte
of similarity information – an acceptable overhead for a server with sufficient capabilities
to coordinate 1,000 nodes. The size of relevance information
can become significant for very large-scale datasets. As discussed later, its impact can be reduced by activating NL-FL only when needed.
Importantly, relevance and similarity information are created and stored locally at the coordinator, hence, neither contributes to the network overhead.
In Tab. <a href="#S4.T1" title="TABLE I ‣ IV-B Experimental results ‣ IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, it is also important to observe the quantities that do <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">not</span> change across columns.
Specifically, NL-FL changes neither the training performance (which is usually the most time-consuming part of learning),
nor the network overhead. Furthermore, it places no additional burden (i.e., no extra computation or storage requirements)
on the learning nodes.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion and open challenges</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Through the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric, we have been able to establish a link between wrong
decisions made during the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">inference phase</span> and the nodes providing the wrong
information during the <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">training phase</span>
driving such decisions. In other words, we have been able to hold the misbehaving node liable
for the wrong classification decisions it caused, fulfilling the basic task of NL-FL.
At the same time, several important challenges remain open before the full NL-FL vision can be realized.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">A first challenge concerns the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">integration</span> of NL-FL within the wider task of
client selection and model weighting in FL.
As an example, it is important to study the effect of parameter <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\alpha</annotation></semantics></math> on the performance of NL-FL, balancing the usual trade-off between false positives and false negatives. Furthermore, we need to decide
how wrong decisions shall be reported, and which wrong decisions trigger a liability investigation.
Once one or more learning nodes are associated with the decisions being investigated, it is important to decide what to do with them.
As discussed earlier, removing those nodes from the learning process and repeating the training is the default action; however, if
the misbehavior is due to malfunctioning, the affected nodes can be fixed. At the same time,
some forms of misbehavior may be ground for civil or criminal liability, and need to be reported accordingly.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Another relevant issue is whether, and how, parameters coming from nodes that are unintentionally misbehaving can be
<span id="S5.p3.1.1" class="ltx_text ltx_font_italic">recovered</span> and still used for the training. Consider the scenario of Sec. <a href="#S4" title="IV Experiment design and results ‣ Towards Node Liability in Federated Learning: Computational Cost and Network Overhead" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>: we are throwing away all
information from the misbehaving node, which still classifies images correctly in the vast majority of cases.
It is possible that, using the <span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter ltx_ref_self">RAdist</span> metric as a guidance, parameter updates from misbehaving nodes could be amended
and used in the learning process.
Similarly, it would be highly desirable to remove the influence from misbehaving nodes on the global model <span id="S5.p3.1.2" class="ltx_text ltx_font_italic">without</span> repeating the whole training.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Finally, given the modest but nonzero overhead,
there is the issue of deciding <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">when</span>
to activate NL-FL. Specifically, instead of storing relevance values for all samples
being classified, one can:
(i) store such information for a limited time, and/or (ii) require the misclassified sample when the anomaly is reported.
In the latter case, relevance values can be obtained as easily and swiftly as needed.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper introduced the concept of node liability in federated learning (NL-FL), which
allows classification decisions to be associated with the information used at training time and their sources.
We evaluated the NL-FL performance and overhead in an edge-based scenario,
finding that NL-FL is much more effective than state-of-the-art solutions in
identifying misbehaving learning nodes,
at the cost of a modest increase in computational and storage requirements for the learning server.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Konečný, B. McMahan, and D. Ramage, “Federated optimization: Distributed
optimization beyond the datacenter,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv:1511.03575</em>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
“Adaptive federated learning in resource constrained edge computing
systems,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE JSAC</em>, vol. 37, no. 6, pp. 1205–1221, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. Nishio and R. Yonetani, “Client Selection for Federated Learning with
Heterogeneous Resources in Mobile Edge,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE ICC 2019</em>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
N. H. Tran, W. Bao <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated learning over wireless networks:
Optimization model design and analysis,” in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">IEEE INFOCOM</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Tu, Y. Ruan, S. Wagle, C. G. Brinton, and C. Joe-Wong,
“Network-Aware Optimization of Distributed Learning for Fog Computing,”
in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim,
“Communication-efficient on-device machine learning: Federated distillation
and augmentation under non-iid private data,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1811.11479</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Bach <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “On pixel-wise explanations for non-linear classifier
decisions by layer-wise relevance propagation,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">PloS one</em>, vol. 10,
no. 7, p. e0130140, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Shin, C. Hwang, J. Kim, J. Park, M. Bennis, and S.-L. Kim, “XOR Mixup:
Privacy-Preserving Data Augmentation for One-Shot Federated Learning,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv:2006.05148</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing federated learning on
non-iid data with reinforcement learning,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
G. Neglia <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The role of network topology for distributed
machine learning,” in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">IEEE INFOCOM</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C. Fung, C. J. Yoon, and I. Beschastnikh, “The limitations of federated
learning in sybil settings,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">USENIX RAID</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani, “Reliable
federated learning for mobile networks,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Wireless
Communications</em>, vol. 27, no. 2, pp. 72–80, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. R. Quinlan, “Decision trees and decision-making,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Systems, Man, and Cybernetics</em>, vol. 20, no. 2, pp. 339–346, 1990.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
G. Montavon <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Layer-wise relevance propagation: An overview,”
in <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning</em>.   Springer, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X. Wang, Y. Han <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “In-Edge AI: Intelligentizing Mobile Edge
Computing, Caching and Communication by Federated Learning,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">IEEE
Network</em>, vol. 33, no. 5, pp. 156–165, 2019.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Francesco Malandrino</span>  (M’09, SM’19)
is a researcher at the National Research Council of Italy (CNR-IEIIT).
His research interests include the architecture and management of wireless, cellular, and
vehicular networks.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Carla Fabiana Chiasserini</span>  (M’98, SM’09, F’18)
is Full Professor with Politecnico di Torino and a Research Associate with CNR-IEIIT.
Her research interests include architectures, protocols, and performance analysis of wireless networks.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.02005" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.02006" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.02006">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.02006" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.02007" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 12 08:36:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
