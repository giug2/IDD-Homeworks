<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Guided Profile Generation Improves Personalization with LLMs</title>
<!--Generated on Thu Sep 19 21:26:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13093v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S1" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S2" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S3" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Guided Profile Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S3.SS1" title="In 3 Guided Profile Generation ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>High-level Workflow</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S4" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation Tasks and Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S4.SS1" title="In 4 Evaluation Tasks and Metrics ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task of Preference Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S4.SS2" title="In 4 Evaluation Tasks and Metrics ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Task of Text Paraphrasing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S4.SS3" title="In 4 Evaluation Tasks and Metrics ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Task of Dialogue Response Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5.SS1" title="In 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Baselines.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5.SS2" title="In 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_typewriter">GPG</span> Specifications.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5.SS3" title="In 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Experimental Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Analysis and Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6.SS1" title="In 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Ablation Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6.SS2" title="In 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Error analysis and Observations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6.SS3" title="In 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Limitation and Future Works</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S7" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#A1" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Examples of Three Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#A2" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Challenges in Open-Ended Personalization Tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#A3" title="In Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Statistics of Three Tasks.</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Guided Profile Generation Improves Personalization with LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiarui Zhang 
<br class="ltx_break"/>University of Southern California 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">jzhang37@usc.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">In modern commercial systems, including Recommendation, Ranking, and E-Commerce platforms, there is a trend towards improving customer experiences by incorporating Personalization context as input into Large Language Models (LLMs). However, LLMs often struggle to effectively parse and utilize sparse and complex personal context without additional processing or contextual enrichment, underscoring the need for more sophisticated context understanding mechanisms. In this work, we propose Guided Profile Generation (<span class="ltx_text ltx_font_typewriter" id="id1.1.1">GPG</span>), a general method designed to generate personal profiles in natural language. As is observed, intermediate guided profile generation enables LLMs to summarize, and extract the important, distinctive features from the personal context into concise, descriptive sentences, precisely tailoring their generation more closely to an individual’s unique habits and preferences. Our experimental results show that <span class="ltx_text ltx_font_typewriter" id="id1.1.2">GPG</span> improves LLM’s personalization ability across different tasks, for example, it increases <math alttext="37\%" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">37</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn id="id1.1.m1.1.1.2.cmml" type="integer" xref="id1.1.m1.1.1.2">37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">37\%</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">37 %</annotation></semantics></math> accuracy in predicting personal preference compared to directly feeding the LLMs with raw personal context.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Guided Profile Generation Improves Personalization with LLMs</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Jiarui Zhang</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">University of Southern California</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">jzhang37@usc.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="402" id="S0.F1.g1" src="x1.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A motivating example. The model is given a personal context reflecting the person’s writing style, and the task is to paraphrase a new tweet for the user. We show <span class="ltx_text ltx_font_typewriter" id="S0.F1.2.1">gpt-3.5-turbo-1106</span>’s response under different input conditions. The result shows that generating a descriptive personal profile with proper guidance helps the model finish the personalization better.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Within the context of personalization tasks, personal profiling has been extensively employed. Conventional methodologies typically rely on substantial datasets such as graph-based similarities. These profiles often exhibit ‘neighborhoods’ and ‘relationships’ within the data, posing challenges for immediate interpretability without supplementary processing.
Recently, LLMs have demonstrated robust capabilities in tasks related to reasoning and generation, leading to a growing interest in leveraging LLMs for personalization services.
However, distinguished from other Naturual Language Processing (NLP) tasks, we identify two primary challenges in personalization with LLMs.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The first challenge is the complexity of personal contexts and the sparsity of their key information. For example, a person’s distinctive writing style may only be discernible in a small portion of their writing, whereas the remainder of the writing style tends to be more generic. As is shown in recent studies <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib24" title="">2023b</a>)</cite>, LLMs have challenges in capturing comprehensive information within lengthy contexts, making it easy to overlook the smaller portions that contain distinctive writing styles. Previous studies <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib19" title="">2020</a>); Salemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib34" title="">2023</a>)</cite> have attempted to address this challenge by context retrieval. However, context retrievers frequently rely on surface-level ranking strategies, such as keyword similarity. Such an approach, while straightforward, may not always align with the nuanced needs of personalization tasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The second challenge lies in the balance between generalization and personalization. While LLMs have demonstrated considerable performance on general tasks, they still struggle to generate output that fully aligns with users’ desired behaviors and directions <cite class="ltx_cite ltx_citemacro_cite">Bang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib2" title="">2023</a>)</cite>.
Rather, they prioritize imitating the majority of their training sets  <cite class="ltx_cite ltx_citemacro_cite">Karpathy (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib17" title="">2023</a>)</cite>.
Figure  <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S0.F1" title="Figure 1 ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates a personalized task involving the paraphrasing of a tweet to match someone’s distinctive writing style. From the personal context, it is noticeable that the individual tends to use block letters to emphasize actions and feelings. However, the model closely mirrors the original question input when receiving the personal context and question directly, which can be reachable even without personal context. When we instruct LLM to describe the person’s writing style, rather than noticing the spatial use of capitalization, it pays attention to the emotion, and content, which are not our desired ‘writing styles’.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Steering LLM outputs precisely is always a challenge. To address it, previous work has attempted to apply reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib31" title="">2022</a>)</cite>. However, this is a resource-intensive process that might be financially burdensome and impractical for some service providers. Other works tried to train compact models <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib21" title="">2023b</a>)</cite> from the feedback of comparison between LLM’s output and ground truth labels. However, no certain true label is available for independent profile generation tasks.
Prompt optimization, involving both manual and automated efforts in designing and selecting suitable prompts for various tasks, stands out as a promising and widely adopted alternative.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The majority of recent studies on prompt optimization indicate that LLMs can benefit from digesting intermediate generated prompts to successfully complete complex tasks <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib38" title="">2022</a>); Kojima et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib18" title="">2022</a>); Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib41" title="">2022</a>)</cite>. In personalization, formulating a personal profile serves as a crucial intermediate step that enhances task performance in terms of both accuracy and efficiency.
Most existing profile modeling techniques depend on substantial datasets. While these approaches are effective for structured analysis, they often yield profiles that require additional interpretation. Additionally, these profiles tend to be restricted to a limited range of data types, limiting the inclusion of more diverse perspectives. In contrast, natural language is not only inherently understandable and easily diagnosable, but it also enables the expansion of the scope of data types that can be effectively integrated into the modeling process.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, we propose a general method leveraging LLMs for personalization, named <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Guided Profile Generation (<span class="ltx_text ltx_font_typewriter" id="S1.p6.1.1.1">GPG</span>)</span>, whose goal is to augment LLMs’ capacity for interpreting raw personal contexts and to generate high-quality natural language personal profiles. In <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.2">GPG</span>, the process begins with personal context digestion, where we pose specific questions in predetermined directions on personal context. Then the model will generate descriptive natural language personal profiles, steered by the output of last stage. The resulting personal profile will be subsequently employed to respond to the request with downstream models.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.2">We conduct extensive experiments to evaluate the efficacy of <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.1">GPG</span> with  <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.2">gpt-3.5</span> on the task of purchase preference prediction, text paraphrasing, and dialogue response generation and benchmark the performance of <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.3">GPG</span> with several baselines. Our result shows that <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.4">GPG</span> consistently enhances the personalization performance across various tasks.
In preference prediction of online purchase, <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.5">GPG</span> improve <math alttext="37\%" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mrow id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml"><mn id="S1.p7.1.m1.1.1.2" xref="S1.p7.1.m1.1.1.2.cmml">37</mn><mo id="S1.p7.1.m1.1.1.1" xref="S1.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><apply id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1"><csymbol cd="latexml" id="S1.p7.1.m1.1.1.1.cmml" xref="S1.p7.1.m1.1.1.1">percent</csymbol><cn id="S1.p7.1.m1.1.1.2.cmml" type="integer" xref="S1.p7.1.m1.1.1.2">37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">37\%</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">37 %</annotation></semantics></math> accuracy in predicting personal preference of product purchasing compared with direct prediction with raw context.
In text paraphrasing on Tweet, <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.6">GPG</span> improves METEOR score by <math alttext="2.24" class="ltx_Math" display="inline" id="S1.p7.2.m2.1"><semantics id="S1.p7.2.m2.1a"><mn id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">2.24</mn><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><cn id="S1.p7.2.m2.1.1.cmml" type="float" xref="S1.p7.2.m2.1.1">2.24</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">2.24</annotation><annotation encoding="application/x-llamapun" id="S1.p7.2.m2.1d">2.24</annotation></semantics></math> by digesting the writing style with the recognition of the most significant writing features.
Furthermore, we conduct ablation studies to evaluate the impact of various components within the <span class="ltx_text ltx_font_typewriter" id="S1.p7.2.7">GPG</span> framework and undertake further analysis to comprehend the limitations of our methods, aiming to pave the way for future directions in this research.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLMs have demonstrated robust performance through scaling up, in-context learning <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib3" title="">2020</a>)</cite>, reinforcement learning from human feedback <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib31" title="">2022</a>)</cite>, and instruction tuning <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib37" title="">2021</a>)</cite>, making them capable of complex reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib12" title="">2020</a>); Srivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib35" title="">2022</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib14" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib16" title="">2024</a>)</cite>. The performance of the model is sensitive to input and output manners, making prompt optimization <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib41" title="">2022</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib38" title="">2022</a>); Kojima et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib18" title="">2022</a>); Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib13" title="">2024</a>)</cite> a popular topic.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">There has been a growing interest in using LLMs for personalization. LLM-Rec <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib26" title="">2023</a>)</cite> utilizes LLMs as recommenders by prompting them with recommendation instructions and employing graph-based engagements. However, this approach lacks emphasis on the crafting of user profiles. LAMP <cite class="ltx_cite ltx_citemacro_cite">Salemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib34" title="">2023</a>)</cite> attempts to integrate a context retriever to avoid the need for feeding the entire personal context to LLMs, but the retrieved personal context still proves challenging for LLMs to easily comprehend. PALR <cite class="ltx_cite ltx_citemacro_cite">Chen (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib6" title="">2023</a>)</cite> uses LLMs to generate user profiles for personalized recommendation and fine-tuned llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib36" title="">2023</a>)</cite> to generate ranking. However, the exploration of more effective methods for crafting user profiles in natural language based on personal contexts with diverse structures remains underexplored. Other studies also explore the use of LLMs to augment graph-based recommendation system <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib26" title="">2023</a>)</cite>, support human writing creativity <cite class="ltx_cite ltx_citemacro_cite">Chakrabarty et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib4" title="">2023</a>)</cite>, personalized writing education <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib20" title="">2023a</a>)</cite>, dialogue systems <cite class="ltx_cite ltx_citemacro_cite">Fan and Jiang (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib8" title="">2023</a>)</cite> and healthcare assistant <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib25" title="">2023c</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">For datasets, LAMP <cite class="ltx_cite ltx_citemacro_cite">Salemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib34" title="">2023</a>)</cite> introduces seven language tasks that necessitate personalization. These tasks include tweet paraphrasing and email subject generation, among others. Notably, tweet paraphrasing serves as a comprehensive test bed for evaluating personalized writing style imitation using LLMs. Amazon review <cite class="ltx_cite ltx_citemacro_cite">He and McAuley (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib11" title="">2016</a>)</cite> provides abundant online purchase history and shopping reviews, enabling the creation of a preference prediction dataset for product purchasing. PER-CHAT <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib40" title="">2021</a>)</cite> is an open-domain single-turn dialogue dataset collected from Reddit. In PER-CHAT, each dialogue response is paired with related comment history from the same user, enabling personal profile crafting. Other datasets like MovieLens <cite class="ltx_cite ltx_citemacro_cite">Harper and Konstan (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib10" title="">2015</a>)</cite>, Recipe <cite class="ltx_cite ltx_citemacro_cite">Majumder et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib27" title="">2019</a>)</cite>, PERSONA-CHAT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib44" title="">2018</a>)</cite> are also widely used. We evaluate <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.1">GPG</span> by personalized preference prediction, tweet paraphrasing, and dialogue generation sets in this paper.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Guided Profile Generation</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="298" id="S3.F2.g1" src="x2.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of <span class="ltx_text ltx_font_typewriter" id="S3.F2.4.1">GPG</span> described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S3" title="3 Guided Profile Generation ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">3</span></a>: Given a personal context, we instruct LLM to generate a descriptive personal profile via self-guidance. The personal profile is then used to complete the personal task. <span class="ltx_text ltx_font_typewriter" id="S3.F2.5.2">GPG</span> enables LLM to generate high-quality personal profiles, improving their performance on personalization. Note that our experiments are conducted in <span class="ltx_text ltx_font_bold" id="S3.F2.6.3">textual domain</span>, images are for illustrative purposes.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Given a personal context <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.1">PC</span>, and a task <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.2">T</span>, the objective of personalization is to align with the individual’s behavior and successfully accomplish the task. In contemporary commercial systems, personal profile crafting proves advantages for both accuracy and efficiency, achieved by providing a clear reflection of a person’s behavior and ensuring reusability without the need to process the raw context again. Given the impressive capabilities of LLMs, there is a natural inclination to leverage them for integrating raw <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.3">PC</span> and generating personal profiles. However, our early investigation indicates that these approaches may not achieve the expected performance (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S0.F1" title="Figure 1 ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>). Moreover, the lack of human-annotated data for intermediate personal profiles makes direct optimization through fine-tuning a challenging option.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We propose <span class="ltx_text ltx_font_typewriter" id="S3.p2.1.1">GPG</span>, a general method for personalization with LLMs through personal profile generation. The proposed method of <span class="ltx_text ltx_font_typewriter" id="S3.p2.1.2">GPG</span> is presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S3.F2" title="Figure 2 ‣ 3 Guided Profile Generation ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. Different from joint learning with downstream personalization tasks for LLM, which adopts Reinforcement Learning from Human Feedback (RLHF), we adopt a much more cost-effective yet efficient method. This method focuses on generating a readable, descriptive personal profile based on personal context.
Our method consists of the following steps:</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>High-level Workflow</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The first step is Personal Context Digestion. In this step, we pose task-specific questions to the LLM, guiding it to digest <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">PC</span> in our desired direction. For instance, in the scenario of predicting a customer’s preferred product based on their purchase history, we prompt the model to sequentially generate product categories.
The main purpose of this step is to get direction and key information for the next step.
Note that differentiated from few-shot prompting which needs a large amount of in-context corpus crafted by humans, in <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">GPG</span>, only one specific question is designed for each task.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The second step is Guided Profile Generation. The response of the previous steps serves as guidance for the generation of the personal profile. Similar to <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib21" title="">2023b</a>)</cite>,</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">we concatenate the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.1">PC</span> and guidance as input. We instruct the LLM to generate descriptive sentences serving as the personal profile. In contrast to high-dimensional representations, our profile is explainable, enabling easy diagnosis of inadequacies. Moreover, our profile is language model orthogonal, facilitating broader applications and seamless future development.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The final step is Response Generation. The generated personal profile is used to finish the final task. To provide sufficient information, we do not exclude the raw personal context in our main experiment. In <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5" title="5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we conduct a detailed experiment study of the effect of the inclusion of <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p4.1.1">PC</span> and guidance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation Tasks and Metrics</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our proposed method can be applied to a wide range of personalization tasks to overcome the challenge given by raw personal contexts. In this work, we mainly focus on the task of personalized preference prediction, text paraphrasing, and dialogue continuation.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task of Preference Prediction</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In commercial systems, accurately predicting a user’s preference is one of the most crucial tasks. This prediction holds the potential to benefit various downstream tasks (e.g., personal recommendation). However, reliance on large databases and specific models, like assessing the similarity between different users, poses limitations. The design of these models often restricts access to additional information, such as the full name and detailed product information on the Internet.
Furthermore, these large databases are not always readily accessible for common use. In contrast, LLMs exhibit the capability to process any textual data, providing a means to overcome the aforementioned limitation. In this section, we delve into their ability to predict user preferences relying solely on textual data. Specifically, we choose user-based online purchase history as our focus due to the distinctive personal behaviors evident in this domain.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Specifically, to construct the test bed for user preference prediction, we leverage the Amazon Product Review <cite class="ltx_cite ltx_citemacro_cite">He and McAuley (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib11" title="">2016</a>); McAuley et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib28" title="">2015</a>)</cite> dataset collected from the Amazon website. The dataset provides the purchase history for each of product with categories and users. We extract the purchase history for each of user and keep the product categories. Then we filter out all of the users who have purchased less than 5 categories of product, who are considered as being lack of personal context. For the remaining users, we randomly select one of the purchased product categories with at least 2 products. Then one of the products is selected as a question. To sample the distractors, we randomly select 3 products from the category that this person has never purchased before. We consider the product name to be enough information to identify the person’s purchase preference, to the end, we exclude all of the review information in the dataset for simplicity.
In the resulting dataset, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.1">PC</span> is defined as purchasing history, which is a list of products that the person has purchased before, and the task is to identify the product that is most likely to be purchased by the person, and select the product from four candidate options.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Metrics.</span> Since the dataset is in the form of multiple choice questions, and is designed to be in a balanced set, we take the accuracy as the only metric for this task. Lastly, it is worth noting that the constructed preference prediction dataset mostly serves as a diagnosis purpose, evaluating how we can better utilize LLMs predicting user’s preference based on raw context. As is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.T1" title="Table 1 ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, a single semantic-level comparison algorithm can reach the highest performance in such data, but will not generalize well when facing different formats of datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task of Text Paraphrasing</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Though simple for humans, it is underexplored whether LLMs can detect and imitate the text-writing styles for different individuals. Such capability is crucial since in recent times, LLMs have been widely used as writing assistants. In this section, we explore how well can LLMs imitate a person’s writing style given the raw <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">PC</span>. Compared to formal writing, such as news reports or research articles, Twitter is a platform where every individual can express their thoughts freely. Hence, we select the text on Twitter as our study focus due to the frequently personalized writing on it, such as punctuation, and abbreviations. Specifically, we use LAMP-7 <cite class="ltx_cite ltx_citemacro_cite">Salemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib34" title="">2023</a>)</cite>, a user-based Twitter collection based on sentiment140 <cite class="ltx_cite ltx_citemacro_cite">Go et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib9" title="">2009</a>)</cite> dataset. In LAMP-7, one of a user’s tweets is selected as the source of task input. Then, this input is fed into an LLM for neutralizing the writing style.
In the resulting dataset, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.2">PC</span> is defined as the collection of all past tweets that this person had before excluding the selected one. The task is to reconstruct the tweet following this person’s writing style based on the neutralized tweet and all other tweets.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Metrics.</span> We consider the word and phrase level usage similarity, including BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib32" title="">2002</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib1" title="">2005</a>)</cite> and ROUGE <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib22" title="">2004</a>)</cite>. Since the task is style reconstruction without semantic-level personalization, we do not evaluate the semantic-level (embedding) similarity.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Task of Dialogue Response Generation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Besides writing style imitation discussed in <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S4.SS2" title="4.2 Task of Text Paraphrasing ‣ 4 Evaluation Tasks and Metrics ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>, the ability of AI assistants to accurately reflect an individual’s opinion is also crucial. This task is particularly challenging due to the opinions are often implicit and multifaceted in a raw personal context, and should be selectively employed based on the requirements of different tasks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">We focus on dialogue continuation in practice. In particular, we leverage PER-CHAT <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib40" title="">2021</a>)</cite> collected from open-domain discussions on Reddit. PER-CHAT collects each individual’s comment history, and the task is to use the history as a signal of personal preference and help the individual answer the question. We do not include the retrieved personal profile from the paper for simplicity. To improve the relevance between the comment history and target response, we measure their semantic similarities based on sentence-transformer <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib33" title="">2019</a>)</cite>, and select a subset having a maximum similarity larger than 0.4. We also exclude instances with max similarities larger than 0.6 to avoid overlap between comment history and target response.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Metrics.</span>
We consider semantic level similarity metric based on sentence-transformer and BERT-Score <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib33" title="">2019</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib45" title="">2019</a>)</cite> as our main metric for evaluation. Since the posted questions are mostly open-ended discussions without definite answers, we do not include metrics for direct string, word or phrase-level comparison.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We use OpenAI’s <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.1">gpt-3.5-turbo-1106</span> as our major LLM all through the tasks; during inference, we keep the temperature at 0 (greedy decoding) to gain a deterministic result and set max_tokens to 100. We report the result with a single run due to the greedy decoding.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy comparison of different prompting strategies on amazon preference prediction dataset. Where <span class="ltx_text ltx_font_typewriter" id="S5.T1.3.1">DG</span> denotes direct generation, <span class="ltx_text ltx_font_typewriter" id="S5.T1.4.2">PG</span> denotes profile generation directly with language instructions.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.1.2.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.5.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.5.2.1.1.1">Random</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.2.1.2">25.00</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.5.3.2.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.5.3.2.1.1">DG w/o PC</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.3.2.2">31.65</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.5.4.3.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.5.4.3.1.1">DG w/ PC</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.5.4.3.2">47.55</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.5.5.4.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.5.5.4.1.1">PG</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.4.2">54.98</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.5.6.5.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.5.6.5.1.1">GPG</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.5.6.5.2"><span class="ltx_text ltx_font_bold" id="S5.T1.5.6.5.2.1">65.08</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="S5.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Text paraphrasing on Twitter performance of <span class="ltx_text ltx_font_typewriter" id="S5.F3.5.1">GPG</span> in comparison with direct generation without personal context (<span class="ltx_text ltx_font_typewriter" id="S5.F3.6.2">DG w/o PC</span>), direct generation with personal context (<span class="ltx_text ltx_font_typewriter" id="S5.F3.7.3">DG w/ PC</span>) and Profile Generation (<span class="ltx_text ltx_font_typewriter" id="S5.F3.8.4">PG</span>).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of different prompting strategies on our selected subset of PER-CHAT data, where <span class="ltx_text ltx_font_bold" id="S5.T2.3.1">ST</span> denotes sentence transformer and <span class="ltx_text ltx_font_bold" id="S5.T2.4.2">BS</span> denotes Bert-Score.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.5.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.1.1.2.1">ST</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.5.1.1.3.1">BS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.5.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.5.2.1.1.1">DG w/o PC</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.2.1.2">29.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.2.1.3">83.09</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.5.3.2.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.5.3.2.1.1">DG w/ PC</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.5.3.2.2">32.31</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.3.2.3">83.54</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.5.4.3.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.5.4.3.1.1">PG</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.5.4.3.2">32.66</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.4.3.3">83.47</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.5.5.4.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.5.5.4.1.1">GPG</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.4.2">32.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.4.3">83.43</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines.</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For the comparison purpose, we present the following baselines to illustrate the effectiveness of <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.1">GPG</span>:</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Direct Generation without Personal Context.</span> (<span class="ltx_text ltx_font_typewriter" id="S5.I1.i1.p1.1.2">DG w/o PC</span>) We consider the LLMs’ native response to the question since they have been trained on numerous corpus. For example, LLMs could have knowledge about the general tweet writing style, thus having the ability to reshape a sentence to such a style. The input is formalized as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i1.p1.1.3">{Q}</span>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Direct Generation with Personal Context.</span> (<span class="ltx_text ltx_font_typewriter" id="S5.I1.i2.p1.1.2">DG w/ PC</span>) In this baseline, we feed the <span class="ltx_text ltx_font_typewriter" id="S5.I1.i2.p1.1.3">PC</span> to LLM and ask them directly to generate the answer to our question. The input is formalized as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i2.p1.1.4">{PC}{Q}</span>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Unguided Profile Generation.</span> (<span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.2">PG</span>) In this baseline, we ask LLMs to generate the profile of a person according to <span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.3">PC</span> without further instructions. Then we use the generated profile to finish the personalized task. The input is formalized as <span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.4">{PC}{PP}{Q}</span>, where <span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.5">PP</span> is the profile generated from <span class="ltx_text ltx_font_typewriter" id="S5.I1.i3.p1.1.6">PC</span> by instructing LLM.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_typewriter" id="S5.SS2.1.1">GPG</span> Specifications.</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In the task of Preference Prediction, we guide the LLM to generate the personal profile by providing the product categories. To this end, we first ask the LLM <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">“Provide the product category of above one by one, each of them use less than 10 words, split by a comma:”</span>. The resulting list of categories serves as the guidance for LLM in generating the personal profile. After the generation of the personal profile, we concatenate the raw <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.2">PC</span>, and the personal profile as the final input of LLM, predicting the final answer. We do not include the raw guidance, i.e. purchase category to reduce redundant information. We will discuss the effect of the inclusion of each component in detail in <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6.SS1" title="6.1 Ablation Studies ‣ 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">In the Text Paraphrasing task, the LLMs are guided by a unique aspect of the writing style of the tweets when generating the personal profiles. We identify 4 key aspects of paraphrasing: <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">Capitalization, Emoji, Abbreviation, Punctuation</span>. Then we instruct LLM to select the most distinctive features in the personal context, specifically our instruction is: <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">Among the usage of 1. Capitalization, 2. Emoji, 3. Abbreviation, 4. Punctuation, which is the most distinctive feature of the above tweets?</span>. Then LLM will generate the profile based on the self-selected category and use the generated profile together with the guidance to finish the task.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">In the Dialogue Response Generation Task, We expect the generated personal profile to be a summary of these texting habits and personal opinions. Inspired by the original paper, we instruct LLM to generate the basic personal information from their comment history, the aspects include: <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.1">“pets”, “family”, “residence”, “favorites”, “partner”, “possessions”, “gender”, “self-description”</span>. Then the above aspects are used to craft the personal profile.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Experimental Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5.T1" title="Table 1 ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance on our Amazon preference prediction dataset of different prompting strategies. LLM improves its performance by <math alttext="50.23\%" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">50.23</mn><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S5.SS3.p1.1.m1.1.1.2.cmml" type="float" xref="S5.SS3.p1.1.m1.1.1.2">50.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">50.23\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">50.23 %</annotation></semantics></math> when adding the personal context to its input. Furthermore, this improvement can be further enhanced to <math alttext="73.71\%" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">73.71</mn><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS3.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS3.p1.2.m2.1.1.2">73.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">73.71\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">73.71 %</annotation></semantics></math> by using a self-generated personal profile. Our <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.3.1">GPG</span>, reaching an improvement of <math alttext="105.62\%" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">105.62</mn><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1">percent</csymbol><cn id="S5.SS3.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS3.p1.3.m3.1.1.2">105.62</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">105.62\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">105.62 %</annotation></semantics></math> through self-guidance.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The result of tweet paraphrasing is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.F3" title="Figure 3 ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.
Firstly, the inclusion of personal context improves the performance across all metrics, clearly showing the usefulness of personal context in reshaping the users’ writing styles.
Generating an unguided personal profile further improves the performance compared to direct generation, providing guidance could double such benefit. Such a result indicates the effectiveness of generating a self-guided intermediate profile for personalization of text paraphrasing with LLMs.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">On dialogue generation, the inclusion of raw <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p3.1.1">PC</span> has a positive impact on the performance, as is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.T2" title="Table 2 ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. However, profile generation, either guided or unguided does not help much in such a task. To understand this phenomenon better, we will look deeper into the generations in <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S6.SS1" title="6.1 Ablation Studies ‣ 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The ablation study on amazon preference prediction (<span class="ltx_text ltx_font_bold" id="S5.T3.6.1">P-P</span>) and text paraphrasing (<span class="ltx_text ltx_font_bold" id="S5.T3.7.2">T-P</span>) tasks. We consider the inclusion of raw personal context (<span class="ltx_text ltx_font_typewriter" id="S5.T3.8.3">PC</span>), guidance (<span class="ltx_text ltx_font_typewriter" id="S5.T3.9.4">G</span>, context digestion), and descriptive personal profile (<span class="ltx_text ltx_font_typewriter" id="S5.T3.10.5">PP</span>). The best performances are in bold.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.11.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T3.11.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T3.11.1.1.2">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.11.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.11.1.1.3.1">P-P</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S5.T3.11.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.11.1.1.4.1">T-P</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.11.2.2">
<th class="ltx_td ltx_th ltx_th_column" id="S5.T3.11.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.2">w/ <span class="ltx_text ltx_font_typewriter" id="S5.T3.11.2.2.2.1">PC</span>?</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.3">w/ <span class="ltx_text ltx_font_typewriter" id="S5.T3.11.2.2.3.1">G</span>?</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.11.2.2.4">w/ <span class="ltx_text ltx_font_typewriter" id="S5.T3.11.2.2.4.1">PP</span>?</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.11.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.5.1">Acc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.6.1">ROUGE-1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.7.1">ROUGE-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.8.1">METEOR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.9"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.9.1">ROUGE-L</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.11.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T3.11.2.2.10.1">BLEU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.11.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="S5.T3.11.3.1.1.1">DG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.3.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.3.1.5">47.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.6">35.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.7">14.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.8">42.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.9">32.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.3.1.10">10.43</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.4.2">
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.1">✗</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.4.2.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.4.2.4">31.65</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.5">33.40</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.6">12.74</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.7">40.76</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.8">30.86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.4.2.9">9.27</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.5.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="S5.T3.11.5.3.1.1">PG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.5.3.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.5.3.5">54.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.6">35.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.7">14.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.8">43.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.9">33.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.5.3.10">11.09</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.6.4">
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.1">✗</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.6.4.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.6.4.4">51.86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.5">34.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.6">13.57</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.7">42.04</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.8">31.65</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.6.4.9">9.95</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.7.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.7.5.1" rowspan="6"><span class="ltx_text ltx_font_typewriter" id="S5.T3.11.7.5.1.1">GPG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.7.5.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.7.5.5"><span class="ltx_text ltx_font_bold" id="S5.T3.11.7.5.5.1">65.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.6">36.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.7">15.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.8">43.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.9">33.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.7.5.10">11.23</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.8.6">
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.1">✗</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.8.6.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.8.6.4">58.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.5">33.96</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.6">13.43</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.7">43.50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.8">31.41</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.8.6.9">10.10</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.1">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.9.7.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.9.7.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.11.9.7.4.1">61.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.5"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.7.5.1">36.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.6"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.7.6.1">15.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.7"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.7.7.1">44.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.8"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.7.8.1">33.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.7.9"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.7.9.1">11.37</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.10.8">
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.1">✗</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.10.8.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.11.10.8.4">59.14</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.5">35.90</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.6">14.62</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.7">44.45</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.8">33.32</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.10.8.9">10.81</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.11.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.1">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.11.9.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.11.9.4">51.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.5">35.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.6">14.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.7">43.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.8">33.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.9.9">10.79</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.12.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.1">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.11.12.10.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.11.12.10.4">48.44</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.5">35.04</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.6">13.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.7">42.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.8">32.42</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.11.12.10.9">10.10</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis and Discussions</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Ablation Studies</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We conduct an ablation study to better understand the benefit of each component of <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p1.1.1">GPG</span>, on preference prediction and text paraphrasing tasks. the result is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.T3" title="Table 3 ‣ 5.3 Experimental Results ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
Specifically, we analyze the impact of incorporating personal context (<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p1.1.2">PC</span>), guidance(<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p1.1.3">G</span>, context digestion), and personal profile (<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p1.1.4">PP</span>) during the generation of <span class="ltx_text ltx_font_bold" id="S6.SS1.p1.1.5">final response</span>. Next, we will provide a detailed analysis based on the result.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.2.1">Can we exclude raw personal context when generating an answer?</span>
In our experiment, we initially incorporated the personal context as part of the input to mitigate the risk of information loss. However, in practice, it is inefficient to keep the personal context as input during every run. To this end, we remove the personal context during the final task generation.
Compared with the direct generation, <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.2.2">GPG</span> improve the performance by <math alttext="17.53\%" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mn id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">17.53</mn><mo id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1">percent</csymbol><cn id="S6.SS1.p2.1.m1.1.1.2.cmml" type="float" xref="S6.SS1.p2.1.m1.1.1.2">17.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">17.53\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">17.53 %</annotation></semantics></math> (absolute) in predicting purchase preference, generations without raw personal context (sixth-row in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.T3" title="Table 3 ‣ 5.3 Experimental Results ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 3</span></a>) could approximate <math alttext="61.04\%" class="ltx_Math" display="inline" id="S6.SS1.p2.2.m2.1"><semantics id="S6.SS1.p2.2.m2.1a"><mrow id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mn id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">61.04</mn><mo id="S6.SS1.p2.2.m2.1.1.1" xref="S6.SS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1.1">percent</csymbol><cn id="S6.SS1.p2.2.m2.1.1.2.cmml" type="float" xref="S6.SS1.p2.2.m2.1.1.2">61.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">61.04\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.2.m2.1d">61.04 %</annotation></semantics></math> of such improvement, indicating a considerable trade-off between the expense and performance. However, in text paraphrasing, the performance after removing the raw personal context is worse than a direct generation, underlining the higher importance of personal context in text paraphrasing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Can personal context digestion directly benefit the downstream tasks?</span>
As is shown by our result, personal context digestion can help LLMs generate better descriptive personal profiles. Thus, we are curious whether such a benefit is directly applicable to the final task generation. To this end, we skip the generation of descriptive personal profiles and directly perform downstream tasks after context digestion, the result is shown in the last two rows of <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S5.T3" title="Table 3 ‣ 5.3 Experimental Results ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. Surprisingly, the guidance itself is functioning even worse than an unguided personal profile (third row) in both of the tasks, suggesting: <span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.2">1.</span> Despite being beneficial in enhancing the generation of personal profiles, the guidance itself is not immediately effective for improving the performance of the final task. <span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.3">2.</span> A descriptive personal profile helps the model be better at personalization.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Error analysis and Observations.</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">Profile Generation helps LLM be more certain about making selections.</span>
We find LLMs frequently opt to abstain from responding when faced with uncertain information.
To better understand this behavior of LLMs, we select all of the ‘abstain’ answers and report the ratios of correct, incorrect, and abstained answers in the preference prediction dataset. Specifically, the answer is recognized as abstained if the word ‘sorry’ is found in the answer. From the result shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#S6.T4" title="Table 4 ‣ 6.2 Error analysis and Observations. ‣ 6 Analysis and Discussions ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 4</span></a>, we find that the primary improvement of <span class="ltx_text ltx_font_typewriter" id="S6.SS2.p1.1.2">GPG</span> on preference prediction data is from helping the model reduce the ratio of answer abstaining rather than correcting their failures.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The ratio of correct, incorrect, and abstain answers in the amazon preference prediction dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.2.1">Correct</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.3.1">Incorrect</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.4.1">Abstain</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S6.T4.1.2.1.1.1">DG w/o PC</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.2">27.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.3">55.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.4">16.94</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.1.3.2.1"><span class="ltx_text ltx_font_typewriter" id="S6.T4.1.3.2.1.1">DG w/ PC</span></th>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.2">41.46</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.3">32.39</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.4">26.15</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.1.4.3.1"><span class="ltx_text ltx_font_typewriter" id="S6.T4.1.4.3.1.1">PG</span></th>
<td class="ltx_td ltx_align_center" id="S6.T4.1.4.3.2">52.30</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.4.3.3">34.92</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.4.3.4">12.78</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.1.5.4.1"><span class="ltx_text ltx_font_typewriter" id="S6.T4.1.5.4.1.1">GPG</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.5.4.2">64.04</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.5.4.3">31.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.5.4.4">4.75</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Limitation and Future Works</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p1.1.1">Integrating multiple aspects personalization.</span>
Our experiments are conducted on a single source of personal context.
In practice, the complete profile of an individual should be drawn from multiple aspects. For example, a person’s purchase preference can be related to their gender, age, habit, or even the weather where they live.
Due to the difficulty of cross-platform data collection, most of the off-the-shelf personalization data are from a single source. Constructing datasets containing personal contexts from multiple sources for each individual could be interesting. In addition, it is also challenging to integrate data from multiple aspects. While wisely designed mechanisms like graph contrastive learning <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib5" title="">2023</a>)</cite> could potentially incorporate different types of information, unifying graph information into natural language is a lightweight alternative <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib42" title="">2022</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib15" title="">2023b</a>)</cite>, obtaining better explainability at the same time. We believe our findings bring useful insight into this future direction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Multimodal personalization.</span>
Recently, multimodal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib7" title="">Dai et al. </a>; Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib23" title="">2023a</a>)</cite> have shown promising capabilities in various tasks. Such advancement opens the possibility of multimodal personalization. For example, an individual’s preference for clothes could be highly related to the designs, which are not easily described by text. In such studies, the undesired and generic MLLM outputs could be a problem, applying a visual crop <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib43" title="">2023</a>)</cite> directed by visual search <cite class="ltx_cite ltx_citemacro_cite">Wu and Xie (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib39" title="">2023</a>)</cite> as a ‘guidance’ would be interesting. In addition, other modalities such as sound <cite class="ltx_cite ltx_citemacro_cite">Meta AI Research (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib29" title="">2023</a>)</cite>, and sensor data like heart rates <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib30" title="">2019</a>)</cite> are also considerable.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we present Guided Profile Generation <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.1">GPG</span>, a novel method leveraging LLMs for personalization tasks through profile generation and context digestion. We conduct extensive experiments on various personalization tasks, including preference prediction, text paraphrasing, and dialogue continuation.
Despite the superior performance, <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.2">GPG</span> generates a personal profile in pure natural descriptive language, which is interpretable and easily diagnosable. Moreover, we reveal why and how the guidance and descriptive personal profile improve the performance. We hope our research can pave the way for personalization applications with AI models in the future.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, pages 65–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al. (2023)</span>
<span class="ltx_bibblock">
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023.

</span>
<span class="ltx_bibblock">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2302.04023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakrabarty et al. (2023)</span>
<span class="ltx_bibblock">
Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, and Smaranda Muresan. 2023.

</span>
<span class="ltx_bibblock">Creativity support in the age of large language models: An empirical study involving emerging writers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2309.12570</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, Yong Xu, and Ronghua Luo. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous graph contrastive learning for recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em>, pages 544–552.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2023)</span>
<span class="ltx_bibblock">
Zheng Chen. 2023.

</span>
<span class="ltx_bibblock">Palr: Personalization aware llms for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2305.07622</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2305.06500</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan and Jiang (2023)</span>
<span class="ltx_bibblock">
Yaxin Fan and Feng Jiang. 2023.

</span>
<span class="ltx_bibblock">Uncovering the potential of chatgpt for discourse analysis in dialogue: An empirical study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2305.08391</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Go et al. (2009)</span>
<span class="ltx_bibblock">
Alec Go, Richa Bhayani, and Lei Huang. 2009.

</span>
<span class="ltx_bibblock">Twitter sentiment classification using distant supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CS224N project report, Stanford</em>, 1(12):2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)</span>
<span class="ltx_bibblock">
F Maxwell Harper and Joseph A Konstan. 2015.

</span>
<span class="ltx_bibblock">The movielens datasets: History and context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Acm transactions on interactive intelligent systems (tiis)</em>, 5(4):1–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and McAuley (2016)</span>
<span class="ltx_bibblock">
Ruining He and Julian McAuley. 2016.

</span>
<span class="ltx_bibblock">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">proceedings of the 25th international conference on world wide web</em>, pages 507–517.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Tenghao Huang, Dongwon Jung, and Muhao Chen. 2024.

</span>
<span class="ltx_bibblock">Planning and editing what you retrieve for enhanced tool learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2404.00450</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Yifan Jiang, Filip Ilievski, and Kaixin Ma. 2023a.

</span>
<span class="ltx_bibblock">Brainteaser: Lateral thinking puzzles for large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2310.05057</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Jiang, Filip Ilievski, and Kaixin Ma. 2023b.

</span>
<span class="ltx_bibblock">Transferring procedural knowledge across commonsense tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ECAI 2023</em>, pages 1156–1163. IOS Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Yifan Jiang, Filip Ilievski, and Kaixin Ma. 2024.

</span>
<span class="ltx_bibblock">Semeval-2024 task 9: Brainteaser: A novel task defying common sense.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2404.16068</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy (2023)</span>
<span class="ltx_bibblock">
Andrej Karpathy. 2023.

</span>
<span class="ltx_bibblock">State of gpt: Analyzing and improving the training of large language models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://karpathy.ai/stateofgpt.pdf" title="">https://karpathy.ai/stateofgpt.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in neural information processing systems</em>, 35:22199–22213.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. 2023a.

</span>
<span class="ltx_bibblock">Teach llms to personalize–an approach inspired by writing education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2308.07968</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2023b.

</span>
<span class="ltx_bibblock">Guiding large language models via directional stimulus prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2302.11520</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Text summarization branches out</em>, pages 74–81.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2310.03744</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2307.03172</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. 2023c.

</span>
<span class="ltx_bibblock">Large language models are few-shot health learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2305.15525</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2023)</span>
<span class="ltx_bibblock">
Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023.

</span>
<span class="ltx_bibblock">Llm-rec: Personalized recommendation via prompting large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2307.15780</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumder et al. (2019)</span>
<span class="ltx_bibblock">
Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2019.

</span>
<span class="ltx_bibblock">Generating personalized recipes from historical user preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1909.00105</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McAuley et al. (2015)</span>
<span class="ltx_bibblock">
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015.

</span>
<span class="ltx_bibblock">Image-based recommendations on styles and substitutes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</em>, pages 43–52.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta AI Research (2023)</span>
<span class="ltx_bibblock">
Meta AI Research. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://ai.meta.com/research/seamless-communication/" title="">Seamless communication</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-01-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2019)</span>
<span class="ltx_bibblock">
Jianmo Ni, Larry Muhlstein, and Julian McAuley. 2019.

</span>
<span class="ltx_bibblock">Modeling heart rate and activity data for personalized fitness recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">The World Wide Web Conference</em>, pages 1343–1353.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, 35:27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1908.10084</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi et al. (2023)</span>
<span class="ltx_bibblock">
Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Lamp: When large language models meet personalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2304.11406</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2206.04615</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in Neural Information Processing Systems</em>, 35:24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Xie (2023)</span>
<span class="ltx_bibblock">
Penghao Wu and Saining Xie. 2023.

</span>
<span class="ltx_bibblock">V*: Guided visual search as a core mechanism in multimodal llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2312.14135</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Yuwei Wu, Xuezhe Ma, and Diyi Yang. 2021.

</span>
<span class="ltx_bibblock">Personalized response generation via generative split memory network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1956–1970.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2210.03629</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Jiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, and Alessandro Oltramari. 2022.

</span>
<span class="ltx_bibblock">A study of zero-shot adaptation with commonsense knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">AKBC</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. 2023.

</span>
<span class="ltx_bibblock">Visual cropping improves zero-shot question answering of multimodal large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018.

</span>
<span class="ltx_bibblock">Personalizing dialogue agents: I have a dog, do you have pets too?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:1801.07243</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:1904.09675</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Examples of Three Tasks</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#A1.F4" title="Figure 4 ‣ Appendix A Examples of Three Tasks ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, we present examples of the three tasks under our test, we include raw personal context, personal context digestion, and personal profile in each example. The prompts for generating personal context digestion and personal profiles can be found in <a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#S5.SS2" title="5.2 GPG Specifications. ‣ 5 Experiments ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="734" id="A1.F4.g1" src="x4.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of personal context, personal context digestion, and personal profile of three tasks under our test. We select only part of the personal context due to their length.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Challenges in Open-Ended Personalization Tasks.</h2>
<figure class="ltx_table" id="A2.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>One Example from dialogue continuation task, where LLMs face a challenge in closely aligning the personal preference and texting habits.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.1.1.1">
<span class="ltx_p" id="A2.T5.1.1.1.1.1.1" style="width:213.4pt;"><span class="ltx_ERROR undefined" id="A2.T5.1.1.1.1.1.1.1">\tabucline</span>[1.1pt]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.2.2.1.1">
<span class="ltx_p" id="A2.T5.1.2.2.1.1.1" style="width:213.4pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.1.2.2.1.1.1.1">Profile:</span>
This person seems to have strong opinions and</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.3.3.1.1">
<span class="ltx_p" id="A2.T5.1.3.3.1.1.1" style="width:213.4pt;">enjoys giving advice. They also appear to be direct</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.4.4.1.1">
<span class="ltx_p" id="A2.T5.1.4.4.1.1.1" style="width:213.4pt;">and straightforward in their communication style,</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.5.5.1.1">
<span class="ltx_p" id="A2.T5.1.5.5.1.1.1" style="width:213.4pt;">often using humor and sarcasm.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.6.6.1.1">
<span class="ltx_p" id="A2.T5.1.6.6.1.1.1" style="width:213.4pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.1.6.6.1.1.1.1">Question (task):</span>
How do you talk to a girl irl after only talking online?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.7.7.1.1">
<span class="ltx_p" id="A2.T5.1.7.7.1.1.1" style="width:213.4pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.1.7.7.1.1.1.1">Human (ground truth):</span>
always through mutual acquaintances. Anything else is creepy and stalker like .</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.8.8.1.1">
<span class="ltx_p" id="A2.T5.1.8.8.1.1.1" style="width:213.4pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.1.8.8.1.1.1.1">LLM response:</span>
Well, first off, be yourself and don’t try to be someone you’re not. Keep the conversation light and fun, and don’t be afraid to throw in some humor. And remember, confidence is key! Good luck!</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.9.9.1.1">
<span class="ltx_p" id="A2.T5.1.9.9.1.1.1" style="width:213.4pt;"><span class="ltx_ERROR undefined" id="A2.T5.1.9.9.1.1.1.1">\tabucline</span>[1.1pt]</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">When addressing open-ended tasks like dialogue continuation, LLMs encounter more challenges in aligning with personal preferences and texting habits. One example is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#A2.T5" title="Table 5 ‣ Appendix B Challenges in Open-Ended Personalization Tasks. ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 5</span></a>, where LLMs are trying to give a generic response to the question rather than a personalized one. This tendency aligns with findings reported in <cite class="ltx_cite ltx_citemacro_cite">Karpathy (<a class="ltx_ref" href="https://arxiv.org/html/2409.13093v1#bib.bib17" title="">2023</a>)</cite> that LLM would prioritize imitating the majority of their training data. While such a phenomenon is not bad in itself as it helps LLMs leverage huge amounts of data and obtain impressive capabilities, it is not a desired behavior in personalization.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Statistics of Three Tasks.</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.13093v1#A3.T6" title="Table 6 ‣ Appendix C Statistics of Three Tasks. ‣ Guided Profile Generation Improves Personalization with LLMs"><span class="ltx_text ltx_ref_tag">Table 6</span></a> presents the statistics of three included tasks. We report the total count of data instances (# Data) and the average number of user activities (# Activities) within each personal context. Specifically, in the Preference Prediction task, # Activities represents the average number of products a user has purchased before. In Text Paraphrasing, it represents the average number of history Tweets. In Dialogue Response Generation, it represents the average number of dialogue responses within the personal context.</p>
</div>
<figure class="ltx_table" id="A3.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Statistics of preference prediction (<span class="ltx_text ltx_font_bold" id="A3.T6.4.1">P-P</span>), text paraphrasing (<span class="ltx_text ltx_font_bold" id="A3.T6.5.2">T-P</span>) and Dialogue Response Generation (<span class="ltx_text ltx_font_bold" id="A3.T6.6.3">D-G</span>). We report the total number of data (# data) and the average number of user activities (# Activities) per personal context.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T6.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T6.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T6.7.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T6.7.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T6.7.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T6.7.1.1.2.1">P-P</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T6.7.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T6.7.1.1.3.1">T-P</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T6.7.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T6.7.1.1.4.1">D-G</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T6.7.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T6.7.2.1.1"><span class="ltx_text ltx_font_typewriter" id="A3.T6.7.2.1.1.1"># Data</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.7.2.1.2">673</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.7.2.1.3">1500</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.7.2.1.4">607</td>
</tr>
<tr class="ltx_tr" id="A3.T6.7.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T6.7.3.2.1"><span class="ltx_text ltx_font_typewriter" id="A3.T6.7.3.2.1.1"># Activities</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.7.3.2.2">6.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.7.3.2.3">17.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.7.3.2.4">10.00</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 21:26:07 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
