<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1812.11737] The meaning of “most” for visual question answering models</title><meta property="og:description" content="The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of “most”, we discuss two strategies which rely on fundamentally different…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The meaning of “most” for visual question answering models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The meaning of “most” for visual question answering models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1812.11737">

<!--Generated on Mon Mar  4 22:15:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The meaning of “most” for visual question answering models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Alexander Kuhnle 
<br class="ltx_break">Department of Computer Science 
<br class="ltx_break">and Technology 
<br class="ltx_break">University of Cambridge 
<br class="ltx_break">Cambridge, United Kingdom 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">aok25@cam.ac.uk</span> 
<br class="ltx_break">&amp;Ann Copestake 
<br class="ltx_break">Department of Computer Science 
<br class="ltx_break">and Technology 
<br class="ltx_break">University of Cambridge 
<br class="ltx_break">Cambridge, United Kingdom 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">aac10@cam.ac.uk</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of <span id="id3.id1.1" class="ltx_text ltx_font_italic">“most”</span>, we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber’s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning methods have been very successful in many natural language processing tasks, ranging from syntactic parsing to machine translation to image captioning. However, despite significantly raised performance scores on benchmark datasets, researchers increasingly worry about interpretability and indeed quality of model decisions. We see two distinct research endeavors here, one being more pragmatic, forward-oriented, and guided by the question <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">“Can a system solve this task?”</span>, the other being more analytic, reflective, and motivated by the question <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">“How does a system solve this task?”</span>. In other words, the former aspires to improve performance, while the latter aims to increase our understanding of deep learning models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">By ‘understanding’ here we mean observing a reasoning mechanism that, if not human-like, at least is cognitively plausible. This is by no means necessary for practically solving a task, however, we highlight two reasons why being able to explain model behavior is nonetheless important: On the one hand, cognitive plausibility increases confidence in the abilities of a system – one is generally more willing to rely on a reasonable than an incomprehensible mechanism. On the other hand, pointing out systematic shortcomings inspires systematic improvements and hence can guide progress. Moreover, particularly in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S1.F1.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:130.1pt;">
<p id="S1.F1.1.1" class="ltx_p">paired</p>
<img src="/html/1812.11737/assets/paired1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S1.F1.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:130.1pt;">
<p id="S1.F1.2.1" class="ltx_p">random<span id="S1.F1.2.1.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">p</span></span></p>
<img src="/html/1812.11737/assets/random1.png" id="S1.F1.2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S1.F1.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:130.1pt;">
<p id="S1.F1.3.1" class="ltx_p">partitioned</p>
<img src="/html/1812.11737/assets/partitioned1.png" id="S1.F1.3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.F1.4" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S1.F1.4.1" class="ltx_text ltx_font_italic">“More than half the shapes are red shapes?”</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Three types of spatial arrangement of objects which may or may not affect the performance of a mechanism for verifying <span id="S1.F1.6.1" class="ltx_text ltx_font_italic">“most”</span> statements. Going from left to right, a strategy based on pairing entities of each set and identifying the remainder presumably gets more difficult, while a strategy based on comparing set cardinalities does not.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we are inspired by experimental practice in psycholinguistics to shed light on the question how deep learning models for visual question answering (VQA) learn to interpret statements involving the quantifier <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">“most”</span>. We follow <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite> in designing abstract visual scenes where we control the ratio of the objects quantified over and their spatial arrangement, to identify whether VQA models exhibit a preferred strategy of verifying whether <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">“most”</span> applies. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates how visual scenes can be configured to favor one over another mechanism.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We want to emphasize the experimental approach and its difference to mainstream machine learning practice. For different verification strategies, conditions are identified that should or should not affect their performance, and test instances are designed accordingly. By comparing the accuracy of subjects on various instance patterns, predictions about a subject’s performance for these mechanisms can be verified and the most likely explanation identified. Note that our advocated evaluation methodology is entirely extrinsic and does not constrain the system in any way (like requiring attention maps) or require a specific framework (like being probabilistic).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Psychology as a discipline has focused entirely on questions around how humans process situations and arrive at decisions, and consequently has the potential to inspire a lot of experiments (like ours) for investigating the same questions in the context of machine learning. Similar to psychology, we advocate the preference of an artificial experimentation environment which can be controlled in detail, over the importance of data originating from the real world, to arrive at more convincing and thus meaningful results.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">It is less common recently to evaluate deep learning models on artificial data tailored to a specific problem, as opposed to big real-world datasets. However, artificial data has a history in deep learning of establishing new techniques – most prominently, LSTMs were introduced by showing their ability to handle various formal grammars <cite class="ltx_cite ltx_citemacro_cite">Gers and Schmidhuber (<a href="#bib.bib4" title="" class="ltx_ref">2001</a>)</cite> – and our higher-level goal with this paper is to demonstrate the potential for more informative evaluation of machine learning models in general. This is motivated by our belief that, in the long term, true progress can only be made if we do not just rely on the narrative of neural networks <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">“learning to understand/solve”</span> a task, but can actually confirm our theories experimentally. Taking inspiration from psychology seems particularly appropriate in the context of powerful deep learning models, which recently are not infrequently described by anthropomorphizing words like <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">“understanding”</span>, and compared to <span id="S1.p6.1.3" class="ltx_text ltx_font_italic">“human-level”</span> performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The meaning of “most”</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section we will discuss two mechanisms of interpreting <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">“most”</span> and introduce relevant cognitive concepts.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generalized quantifiers and “most”</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.5" class="ltx_p"><span id="S2.SS1.p1.5.1" class="ltx_text ltx_font_italic">“Most”</span> has a special status in linguistics due to the fact that it is the most prominent example of a quantifier whose semantics cannot be expressed in first-order logic, while other simple natural language quantifiers like <span id="S2.SS1.p1.5.2" class="ltx_text ltx_font_italic">“some”</span>, <span id="S2.SS1.p1.5.3" class="ltx_text ltx_font_italic">“every”</span> or <span id="S2.SS1.p1.5.4" class="ltx_text ltx_font_italic">“no”</span> directly correspond to the quantifier primitives <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\exists" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">∃</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><exists id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"></exists></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\exists</annotation></semantics></math> and <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\forall" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mo id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">∀</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">for-all</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\forall</annotation></semantics></math> (plus logical operators <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\wedge" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mo id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">∧</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><and id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\wedge</annotation></semantics></math>, <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\vee" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mo id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">∨</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><or id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"></or></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\vee</annotation></semantics></math> and <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\neg" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mo id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">¬</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><not id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"></not></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\neg</annotation></semantics></math>). This situation is not just a matter of introducing further appropriate primitives, but requires a fundamental extension of the logic system and its expressivity.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.7" class="ltx_p">In the following, by <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">x</annotation></semantics></math> we denote an entity, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">A</annotation></semantics></math> and <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">B</annotation></semantics></math> denote predicates (<span id="S2.SS1.p2.7.1" class="ltx_text ltx_font_italic">“square”</span>, <span id="S2.SS1.p2.7.2" class="ltx_text ltx_font_italic">“red”</span>), <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="A(x)" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.2" xref="S2.SS1.p2.4.m4.1.2.cmml"><mi id="S2.SS1.p2.4.m4.1.2.2" xref="S2.SS1.p2.4.m4.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.1.2.1" xref="S2.SS1.p2.4.m4.1.2.1.cmml">​</mo><mrow id="S2.SS1.p2.4.m4.1.2.3.2" xref="S2.SS1.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.1.2.3.2.1" xref="S2.SS1.p2.4.m4.1.2.cmml">(</mo><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p2.4.m4.1.2.3.2.2" xref="S2.SS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.2.cmml" xref="S2.SS1.p2.4.m4.1.2"><times id="S2.SS1.p2.4.m4.1.2.1.cmml" xref="S2.SS1.p2.4.m4.1.2.1"></times><ci id="S2.SS1.p2.4.m4.1.2.2.cmml" xref="S2.SS1.p2.4.m4.1.2.2">𝐴</ci><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">A(x)</annotation></semantics></math> is true if and only if <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">x</annotation></semantics></math> satisfies <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">A</annotation></semantics></math>, and <math id="S2.SS1.p2.7.m7.3" class="ltx_Math" alttext="S_{A}=\{x:A(x)\}" display="inline"><semantics id="S2.SS1.p2.7.m7.3a"><mrow id="S2.SS1.p2.7.m7.3.3" xref="S2.SS1.p2.7.m7.3.3.cmml"><msub id="S2.SS1.p2.7.m7.3.3.3" xref="S2.SS1.p2.7.m7.3.3.3.cmml"><mi id="S2.SS1.p2.7.m7.3.3.3.2" xref="S2.SS1.p2.7.m7.3.3.3.2.cmml">S</mi><mi id="S2.SS1.p2.7.m7.3.3.3.3" xref="S2.SS1.p2.7.m7.3.3.3.3.cmml">A</mi></msub><mo id="S2.SS1.p2.7.m7.3.3.2" xref="S2.SS1.p2.7.m7.3.3.2.cmml">=</mo><mrow id="S2.SS1.p2.7.m7.3.3.1.1" xref="S2.SS1.p2.7.m7.3.3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.7.m7.3.3.1.1.2" xref="S2.SS1.p2.7.m7.3.3.1.2.1.cmml">{</mo><mi id="S2.SS1.p2.7.m7.2.2" xref="S2.SS1.p2.7.m7.2.2.cmml">x</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.7.m7.3.3.1.1.3" xref="S2.SS1.p2.7.m7.3.3.1.2.1.cmml">:</mo><mrow id="S2.SS1.p2.7.m7.3.3.1.1.1" xref="S2.SS1.p2.7.m7.3.3.1.1.1.cmml"><mi id="S2.SS1.p2.7.m7.3.3.1.1.1.2" xref="S2.SS1.p2.7.m7.3.3.1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.7.m7.3.3.1.1.1.1" xref="S2.SS1.p2.7.m7.3.3.1.1.1.1.cmml">​</mo><mrow id="S2.SS1.p2.7.m7.3.3.1.1.1.3.2" xref="S2.SS1.p2.7.m7.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.7.m7.3.3.1.1.1.3.2.1" xref="S2.SS1.p2.7.m7.3.3.1.1.1.cmml">(</mo><mi id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p2.7.m7.3.3.1.1.1.3.2.2" xref="S2.SS1.p2.7.m7.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS1.p2.7.m7.3.3.1.1.4" xref="S2.SS1.p2.7.m7.3.3.1.2.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.3b"><apply id="S2.SS1.p2.7.m7.3.3.cmml" xref="S2.SS1.p2.7.m7.3.3"><eq id="S2.SS1.p2.7.m7.3.3.2.cmml" xref="S2.SS1.p2.7.m7.3.3.2"></eq><apply id="S2.SS1.p2.7.m7.3.3.3.cmml" xref="S2.SS1.p2.7.m7.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m7.3.3.3.1.cmml" xref="S2.SS1.p2.7.m7.3.3.3">subscript</csymbol><ci id="S2.SS1.p2.7.m7.3.3.3.2.cmml" xref="S2.SS1.p2.7.m7.3.3.3.2">𝑆</ci><ci id="S2.SS1.p2.7.m7.3.3.3.3.cmml" xref="S2.SS1.p2.7.m7.3.3.3.3">𝐴</ci></apply><apply id="S2.SS1.p2.7.m7.3.3.1.2.cmml" xref="S2.SS1.p2.7.m7.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p2.7.m7.3.3.1.2.1.cmml" xref="S2.SS1.p2.7.m7.3.3.1.1.2">conditional-set</csymbol><ci id="S2.SS1.p2.7.m7.2.2.cmml" xref="S2.SS1.p2.7.m7.2.2">𝑥</ci><apply id="S2.SS1.p2.7.m7.3.3.1.1.1.cmml" xref="S2.SS1.p2.7.m7.3.3.1.1.1"><times id="S2.SS1.p2.7.m7.3.3.1.1.1.1.cmml" xref="S2.SS1.p2.7.m7.3.3.1.1.1.1"></times><ci id="S2.SS1.p2.7.m7.3.3.1.1.1.2.cmml" xref="S2.SS1.p2.7.m7.3.3.1.1.1.2">𝐴</ci><ci id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.3c">S_{A}=\{x:A(x)\}</annotation></semantics></math> is the corresponding set of entities satisfying this predicate (<span id="S2.SS1.p2.7.3" class="ltx_text ltx_font_italic">“squares”</span>). Thus we can define the semantics of <span id="S2.SS1.p2.7.4" class="ltx_text ltx_font_italic">“some”</span> and <span id="S2.SS1.p2.7.5" class="ltx_text ltx_font_italic">“every”</span>:</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex1.m1.2" class="ltx_Math" alttext="\displaystyle\text{some}(A,B)" display="inline"><semantics id="S2.Ex1.m1.2a"><mrow id="S2.Ex1.m1.2.3" xref="S2.Ex1.m1.2.3.cmml"><mtext id="S2.Ex1.m1.2.3.2" xref="S2.Ex1.m1.2.3.2a.cmml">some</mtext><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.3.1" xref="S2.Ex1.m1.2.3.1.cmml">​</mo><mrow id="S2.Ex1.m1.2.3.3.2" xref="S2.Ex1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.3.3.2.1" xref="S2.Ex1.m1.2.3.3.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">A</mi><mo id="S2.Ex1.m1.2.3.3.2.2" xref="S2.Ex1.m1.2.3.3.1.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">B</mi><mo stretchy="false" id="S2.Ex1.m1.2.3.3.2.3" xref="S2.Ex1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.2b"><apply id="S2.Ex1.m1.2.3.cmml" xref="S2.Ex1.m1.2.3"><times id="S2.Ex1.m1.2.3.1.cmml" xref="S2.Ex1.m1.2.3.1"></times><ci id="S2.Ex1.m1.2.3.2a.cmml" xref="S2.Ex1.m1.2.3.2"><mtext id="S2.Ex1.m1.2.3.2.cmml" xref="S2.Ex1.m1.2.3.2">some</mtext></ci><interval closure="open" id="S2.Ex1.m1.2.3.3.1.cmml" xref="S2.Ex1.m1.2.3.3.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝐴</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝐵</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.2c">\displaystyle\text{some}(A,B)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex1.m2.1" class="ltx_math_unparsed" alttext="\displaystyle\Leftrightarrow\exists x:A(x)\wedge B(x)" display="inline"><semantics id="S2.Ex1.m2.1a"><mrow id="S2.Ex1.m2.1b"><mo stretchy="false" id="S2.Ex1.m2.1.1">⇔</mo><mo rspace="0.167em" id="S2.Ex1.m2.1.2">∃</mo><mi id="S2.Ex1.m2.1.3">x</mi><mo lspace="0.278em" rspace="0.278em" id="S2.Ex1.m2.1.4">:</mo><mi id="S2.Ex1.m2.1.5">A</mi><mrow id="S2.Ex1.m2.1.6"><mo stretchy="false" id="S2.Ex1.m2.1.6.1">(</mo><mi id="S2.Ex1.m2.1.6.2">x</mi><mo stretchy="false" id="S2.Ex1.m2.1.6.3">)</mo></mrow><mo id="S2.Ex1.m2.1.7">∧</mo><mi id="S2.Ex1.m2.1.8">B</mi><mrow id="S2.Ex1.m2.1.9"><mo stretchy="false" id="S2.Ex1.m2.1.9.1">(</mo><mi id="S2.Ex1.m2.1.9.2">x</mi><mo stretchy="false" id="S2.Ex1.m2.1.9.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle\Leftrightarrow\exists x:A(x)\wedge B(x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2.m1.2" class="ltx_Math" alttext="\displaystyle\text{every}(A,B)" display="inline"><semantics id="S2.Ex2.m1.2a"><mrow id="S2.Ex2.m1.2.3" xref="S2.Ex2.m1.2.3.cmml"><mtext id="S2.Ex2.m1.2.3.2" xref="S2.Ex2.m1.2.3.2a.cmml">every</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.2.3.1" xref="S2.Ex2.m1.2.3.1.cmml">​</mo><mrow id="S2.Ex2.m1.2.3.3.2" xref="S2.Ex2.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.2.3.3.2.1" xref="S2.Ex2.m1.2.3.3.1.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">A</mi><mo id="S2.Ex2.m1.2.3.3.2.2" xref="S2.Ex2.m1.2.3.3.1.cmml">,</mo><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">B</mi><mo stretchy="false" id="S2.Ex2.m1.2.3.3.2.3" xref="S2.Ex2.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.2b"><apply id="S2.Ex2.m1.2.3.cmml" xref="S2.Ex2.m1.2.3"><times id="S2.Ex2.m1.2.3.1.cmml" xref="S2.Ex2.m1.2.3.1"></times><ci id="S2.Ex2.m1.2.3.2a.cmml" xref="S2.Ex2.m1.2.3.2"><mtext id="S2.Ex2.m1.2.3.2.cmml" xref="S2.Ex2.m1.2.3.2">every</mtext></ci><interval closure="open" id="S2.Ex2.m1.2.3.3.1.cmml" xref="S2.Ex2.m1.2.3.3.2"><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝐴</ci><ci id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2">𝐵</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.2c">\displaystyle\text{every}(A,B)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2.m2.1" class="ltx_math_unparsed" alttext="\displaystyle\Leftrightarrow\forall x:A(x)\Rightarrow B(x)" display="inline"><semantics id="S2.Ex2.m2.1a"><mrow id="S2.Ex2.m2.1b"><mo stretchy="false" id="S2.Ex2.m2.1.1">⇔</mo><mo rspace="0.167em" id="S2.Ex2.m2.1.2">∀</mo><mi id="S2.Ex2.m2.1.3">x</mi><mo lspace="0.278em" rspace="0.278em" id="S2.Ex2.m2.1.4">:</mo><mi id="S2.Ex2.m2.1.5">A</mi><mrow id="S2.Ex2.m2.1.6"><mo stretchy="false" id="S2.Ex2.m2.1.6.1">(</mo><mi id="S2.Ex2.m2.1.6.2">x</mi><mo stretchy="false" id="S2.Ex2.m2.1.6.3">)</mo></mrow><mo stretchy="false" id="S2.Ex2.m2.1.7">⇒</mo><mi id="S2.Ex2.m2.1.8">B</mi><mrow id="S2.Ex2.m2.1.9"><mo stretchy="false" id="S2.Ex2.m2.1.9.1">(</mo><mi id="S2.Ex2.m2.1.9.2">x</mi><mo stretchy="false" id="S2.Ex2.m2.1.9.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.Ex2.m2.1c">\displaystyle\Leftrightarrow\forall x:A(x)\Rightarrow B(x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.8" class="ltx_p">Importantly, these definitions do not involve the concept of set cardinality and indeed can be formulated without involving sets. This is not possible for <span id="S2.SS1.p2.8.1" class="ltx_text ltx_font_italic">“most”</span>, which is commonly defined in one of the following ways:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex3.m1.2" class="ltx_Math" alttext="\displaystyle\text{most}(A,B)" display="inline"><semantics id="S2.Ex3.m1.2a"><mrow id="S2.Ex3.m1.2.3" xref="S2.Ex3.m1.2.3.cmml"><mtext id="S2.Ex3.m1.2.3.2" xref="S2.Ex3.m1.2.3.2a.cmml">most</mtext><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.2.3.1" xref="S2.Ex3.m1.2.3.1.cmml">​</mo><mrow id="S2.Ex3.m1.2.3.3.2" xref="S2.Ex3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.Ex3.m1.2.3.3.2.1" xref="S2.Ex3.m1.2.3.3.1.cmml">(</mo><mi id="S2.Ex3.m1.1.1" xref="S2.Ex3.m1.1.1.cmml">A</mi><mo id="S2.Ex3.m1.2.3.3.2.2" xref="S2.Ex3.m1.2.3.3.1.cmml">,</mo><mi id="S2.Ex3.m1.2.2" xref="S2.Ex3.m1.2.2.cmml">B</mi><mo stretchy="false" id="S2.Ex3.m1.2.3.3.2.3" xref="S2.Ex3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.2b"><apply id="S2.Ex3.m1.2.3.cmml" xref="S2.Ex3.m1.2.3"><times id="S2.Ex3.m1.2.3.1.cmml" xref="S2.Ex3.m1.2.3.1"></times><ci id="S2.Ex3.m1.2.3.2a.cmml" xref="S2.Ex3.m1.2.3.2"><mtext id="S2.Ex3.m1.2.3.2.cmml" xref="S2.Ex3.m1.2.3.2">most</mtext></ci><interval closure="open" id="S2.Ex3.m1.2.3.3.1.cmml" xref="S2.Ex3.m1.2.3.3.2"><ci id="S2.Ex3.m1.1.1.cmml" xref="S2.Ex3.m1.1.1">𝐴</ci><ci id="S2.Ex3.m1.2.2.cmml" xref="S2.Ex3.m1.2.2">𝐵</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.2c">\displaystyle\text{most}(A,B)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex3.m2.2" class="ltx_Math" alttext="\displaystyle\Leftrightarrow|S_{A\wedge B}|&gt;1/2\cdot|A|" display="inline"><semantics id="S2.Ex3.m2.2a"><mrow id="S2.Ex3.m2.2.2" xref="S2.Ex3.m2.2.2.cmml"><mi id="S2.Ex3.m2.2.2.3" xref="S2.Ex3.m2.2.2.3.cmml"></mi><mo stretchy="false" id="S2.Ex3.m2.2.2.2" xref="S2.Ex3.m2.2.2.2.cmml">⇔</mo><mrow id="S2.Ex3.m2.2.2.1" xref="S2.Ex3.m2.2.2.1.cmml"><mrow id="S2.Ex3.m2.2.2.1.1.1" xref="S2.Ex3.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.Ex3.m2.2.2.1.1.1.2" xref="S2.Ex3.m2.2.2.1.1.2.1.cmml">|</mo><msub id="S2.Ex3.m2.2.2.1.1.1.1" xref="S2.Ex3.m2.2.2.1.1.1.1.cmml"><mi id="S2.Ex3.m2.2.2.1.1.1.1.2" xref="S2.Ex3.m2.2.2.1.1.1.1.2.cmml">S</mi><mrow id="S2.Ex3.m2.2.2.1.1.1.1.3" xref="S2.Ex3.m2.2.2.1.1.1.1.3.cmml"><mi id="S2.Ex3.m2.2.2.1.1.1.1.3.2" xref="S2.Ex3.m2.2.2.1.1.1.1.3.2.cmml">A</mi><mo id="S2.Ex3.m2.2.2.1.1.1.1.3.1" xref="S2.Ex3.m2.2.2.1.1.1.1.3.1.cmml">∧</mo><mi id="S2.Ex3.m2.2.2.1.1.1.1.3.3" xref="S2.Ex3.m2.2.2.1.1.1.1.3.3.cmml">B</mi></mrow></msub><mo stretchy="false" id="S2.Ex3.m2.2.2.1.1.1.3" xref="S2.Ex3.m2.2.2.1.1.2.1.cmml">|</mo></mrow><mo id="S2.Ex3.m2.2.2.1.2" xref="S2.Ex3.m2.2.2.1.2.cmml">&gt;</mo><mrow id="S2.Ex3.m2.2.2.1.3" xref="S2.Ex3.m2.2.2.1.3.cmml"><mrow id="S2.Ex3.m2.2.2.1.3.2" xref="S2.Ex3.m2.2.2.1.3.2.cmml"><mn id="S2.Ex3.m2.2.2.1.3.2.2" xref="S2.Ex3.m2.2.2.1.3.2.2.cmml">1</mn><mo id="S2.Ex3.m2.2.2.1.3.2.1" xref="S2.Ex3.m2.2.2.1.3.2.1.cmml">/</mo><mn id="S2.Ex3.m2.2.2.1.3.2.3" xref="S2.Ex3.m2.2.2.1.3.2.3.cmml">2</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.Ex3.m2.2.2.1.3.1" xref="S2.Ex3.m2.2.2.1.3.1.cmml">⋅</mo><mrow id="S2.Ex3.m2.2.2.1.3.3.2" xref="S2.Ex3.m2.2.2.1.3.3.1.cmml"><mo stretchy="false" id="S2.Ex3.m2.2.2.1.3.3.2.1" xref="S2.Ex3.m2.2.2.1.3.3.1.1.cmml">|</mo><mi id="S2.Ex3.m2.1.1" xref="S2.Ex3.m2.1.1.cmml">A</mi><mo stretchy="false" id="S2.Ex3.m2.2.2.1.3.3.2.2" xref="S2.Ex3.m2.2.2.1.3.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m2.2b"><apply id="S2.Ex3.m2.2.2.cmml" xref="S2.Ex3.m2.2.2"><ci id="S2.Ex3.m2.2.2.2.cmml" xref="S2.Ex3.m2.2.2.2">⇔</ci><csymbol cd="latexml" id="S2.Ex3.m2.2.2.3.cmml" xref="S2.Ex3.m2.2.2.3">absent</csymbol><apply id="S2.Ex3.m2.2.2.1.cmml" xref="S2.Ex3.m2.2.2.1"><gt id="S2.Ex3.m2.2.2.1.2.cmml" xref="S2.Ex3.m2.2.2.1.2"></gt><apply id="S2.Ex3.m2.2.2.1.1.2.cmml" xref="S2.Ex3.m2.2.2.1.1.1"><abs id="S2.Ex3.m2.2.2.1.1.2.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.2"></abs><apply id="S2.Ex3.m2.2.2.1.1.1.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m2.2.2.1.1.1.1.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S2.Ex3.m2.2.2.1.1.1.1.2.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.2">𝑆</ci><apply id="S2.Ex3.m2.2.2.1.1.1.1.3.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.3"><and id="S2.Ex3.m2.2.2.1.1.1.1.3.1.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.3.1"></and><ci id="S2.Ex3.m2.2.2.1.1.1.1.3.2.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.3.2">𝐴</ci><ci id="S2.Ex3.m2.2.2.1.1.1.1.3.3.cmml" xref="S2.Ex3.m2.2.2.1.1.1.1.3.3">𝐵</ci></apply></apply></apply><apply id="S2.Ex3.m2.2.2.1.3.cmml" xref="S2.Ex3.m2.2.2.1.3"><ci id="S2.Ex3.m2.2.2.1.3.1.cmml" xref="S2.Ex3.m2.2.2.1.3.1">⋅</ci><apply id="S2.Ex3.m2.2.2.1.3.2.cmml" xref="S2.Ex3.m2.2.2.1.3.2"><divide id="S2.Ex3.m2.2.2.1.3.2.1.cmml" xref="S2.Ex3.m2.2.2.1.3.2.1"></divide><cn type="integer" id="S2.Ex3.m2.2.2.1.3.2.2.cmml" xref="S2.Ex3.m2.2.2.1.3.2.2">1</cn><cn type="integer" id="S2.Ex3.m2.2.2.1.3.2.3.cmml" xref="S2.Ex3.m2.2.2.1.3.2.3">2</cn></apply><apply id="S2.Ex3.m2.2.2.1.3.3.1.cmml" xref="S2.Ex3.m2.2.2.1.3.3.2"><abs id="S2.Ex3.m2.2.2.1.3.3.1.1.cmml" xref="S2.Ex3.m2.2.2.1.3.3.2.1"></abs><ci id="S2.Ex3.m2.1.1.cmml" xref="S2.Ex3.m2.1.1">𝐴</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m2.2c">\displaystyle\Leftrightarrow|S_{A\wedge B}|&gt;1/2\cdot|A|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\displaystyle\Leftrightarrow|S_{A\wedge B}|&gt;|S_{A\wedge\neg B}|" display="inline"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.4" xref="S2.E1.m1.2.2.4.cmml"></mi><mo stretchy="false" id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">⇔</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">S</mi><mrow id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">A</mi><mo id="S2.E1.m1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.1.1.3.1.cmml">∧</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml">B</mi></mrow></msub><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">&gt;</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.2" xref="S2.E1.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">S</mi><mrow id="S2.E1.m1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.2.cmml">A</mi><mo id="S2.E1.m1.2.2.2.2.1.1.3.1" xref="S2.E1.m1.2.2.2.2.1.1.3.1.cmml">∧</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.cmml"><mo rspace="0.167em" id="S2.E1.m1.2.2.2.2.1.1.3.3.1" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml">¬</mo><mi id="S2.E1.m1.2.2.2.2.1.1.3.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml">B</mi></mrow></mrow></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.3" xref="S2.E1.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">⇔</ci><csymbol cd="latexml" id="S2.E1.m1.2.2.4.cmml" xref="S2.E1.m1.2.2.4">absent</csymbol><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><gt id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3"></gt><apply id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1"><abs id="S2.E1.m1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2"></abs><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝑆</ci><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><and id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.1"></and><ci id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">𝐴</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3">𝐵</ci></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1"><abs id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.2"></abs><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2">𝑆</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3"><and id="S2.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.1"></and><ci id="S2.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.2">𝐴</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3"><not id="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1"></not><ci id="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2">𝐵</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\displaystyle\Leftrightarrow|S_{A\wedge B}|&gt;|S_{A\wedge\neg B}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.9" class="ltx_p">This makes <span id="S2.SS1.p2.9.1" class="ltx_text ltx_font_italic">“most”</span> an example of a <em id="S2.SS1.p2.9.2" class="ltx_emph ltx_font_italic">generalized quantifier</em>, and in fact all generalized quantifiers can be defined in terms of cardinalities, indicating the apparent importance of a cardinality concept to human cognition.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Alternative characterization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">There is another way to define <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">“most”</span> which uses the fact that whether two sets are equinumerous can be determined without a concept of cardinality, but based on the idea of a bijection:</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex4.m1.1" class="ltx_Math" alttext="\displaystyle A\leftrightarrow B" display="inline"><semantics id="S2.Ex4.m1.1a"><mrow id="S2.Ex4.m1.1.1" xref="S2.Ex4.m1.1.1.cmml"><mi id="S2.Ex4.m1.1.1.2" xref="S2.Ex4.m1.1.1.2.cmml">A</mi><mo stretchy="false" id="S2.Ex4.m1.1.1.1" xref="S2.Ex4.m1.1.1.1.cmml">↔</mo><mi id="S2.Ex4.m1.1.1.3" xref="S2.Ex4.m1.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex4.m1.1b"><apply id="S2.Ex4.m1.1.1.cmml" xref="S2.Ex4.m1.1.1"><ci id="S2.Ex4.m1.1.1.1.cmml" xref="S2.Ex4.m1.1.1.1">↔</ci><ci id="S2.Ex4.m1.1.1.2.cmml" xref="S2.Ex4.m1.1.1.2">𝐴</ci><ci id="S2.Ex4.m1.1.1.3.cmml" xref="S2.Ex4.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m1.1c">\displaystyle A\leftrightarrow B</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex4.m2.3" class="ltx_math_unparsed" alttext="\displaystyle:\Leftrightarrow\forall x:A(x)\Leftrightarrow B(x)" display="inline"><semantics id="S2.Ex4.m2.3a"><mrow id="S2.Ex4.m2.3b"><mo rspace="0em" id="S2.Ex4.m2.1.1">:</mo><mo lspace="0em" stretchy="false" id="S2.Ex4.m2.2.2">⇔</mo><mo rspace="0.167em" id="S2.Ex4.m2.3.3">∀</mo><mi id="S2.Ex4.m2.3.4">x</mi><mo lspace="0.278em" rspace="0.278em" id="S2.Ex4.m2.3.5">:</mo><mi id="S2.Ex4.m2.3.6">A</mi><mrow id="S2.Ex4.m2.3.7"><mo stretchy="false" id="S2.Ex4.m2.3.7.1">(</mo><mi id="S2.Ex4.m2.3.7.2">x</mi><mo stretchy="false" id="S2.Ex4.m2.3.7.3">)</mo></mrow><mo stretchy="false" id="S2.Ex4.m2.3.8">⇔</mo><mi id="S2.Ex4.m2.3.9">B</mi><mrow id="S2.Ex4.m2.3.10"><mo stretchy="false" id="S2.Ex4.m2.3.10.1">(</mo><mi id="S2.Ex4.m2.3.10.2">x</mi><mo stretchy="false" id="S2.Ex4.m2.3.10.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.Ex4.m2.3c">\displaystyle:\Leftrightarrow\forall x:A(x)\Leftrightarrow B(x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex5.m1.2" class="ltx_Math" alttext="\displaystyle\;\Leftrightarrow|S_{A}|=|S_{B}|" display="inline"><semantics id="S2.Ex5.m1.2a"><mrow id="S2.Ex5.m1.2.2" xref="S2.Ex5.m1.2.2.cmml"><mi id="S2.Ex5.m1.2.2.4" xref="S2.Ex5.m1.2.2.4.cmml"></mi><mo lspace="0.558em" stretchy="false" id="S2.Ex5.m1.2.2.3" xref="S2.Ex5.m1.2.2.3.cmml">⇔</mo><mrow id="S2.Ex5.m1.2.2.2" xref="S2.Ex5.m1.2.2.2.cmml"><mrow id="S2.Ex5.m1.1.1.1.1.1" xref="S2.Ex5.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex5.m1.1.1.1.1.1.2" xref="S2.Ex5.m1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.Ex5.m1.1.1.1.1.1.1" xref="S2.Ex5.m1.1.1.1.1.1.1.cmml"><mi id="S2.Ex5.m1.1.1.1.1.1.1.2" xref="S2.Ex5.m1.1.1.1.1.1.1.2.cmml">S</mi><mi id="S2.Ex5.m1.1.1.1.1.1.1.3" xref="S2.Ex5.m1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo stretchy="false" id="S2.Ex5.m1.1.1.1.1.1.3" xref="S2.Ex5.m1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.Ex5.m1.2.2.2.3" xref="S2.Ex5.m1.2.2.2.3.cmml">=</mo><mrow id="S2.Ex5.m1.2.2.2.2.1" xref="S2.Ex5.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S2.Ex5.m1.2.2.2.2.1.2" xref="S2.Ex5.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S2.Ex5.m1.2.2.2.2.1.1" xref="S2.Ex5.m1.2.2.2.2.1.1.cmml"><mi id="S2.Ex5.m1.2.2.2.2.1.1.2" xref="S2.Ex5.m1.2.2.2.2.1.1.2.cmml">S</mi><mi id="S2.Ex5.m1.2.2.2.2.1.1.3" xref="S2.Ex5.m1.2.2.2.2.1.1.3.cmml">B</mi></msub><mo stretchy="false" id="S2.Ex5.m1.2.2.2.2.1.3" xref="S2.Ex5.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex5.m1.2b"><apply id="S2.Ex5.m1.2.2.cmml" xref="S2.Ex5.m1.2.2"><ci id="S2.Ex5.m1.2.2.3.cmml" xref="S2.Ex5.m1.2.2.3">⇔</ci><csymbol cd="latexml" id="S2.Ex5.m1.2.2.4.cmml" xref="S2.Ex5.m1.2.2.4">absent</csymbol><apply id="S2.Ex5.m1.2.2.2.cmml" xref="S2.Ex5.m1.2.2.2"><eq id="S2.Ex5.m1.2.2.2.3.cmml" xref="S2.Ex5.m1.2.2.2.3"></eq><apply id="S2.Ex5.m1.1.1.1.1.2.cmml" xref="S2.Ex5.m1.1.1.1.1.1"><abs id="S2.Ex5.m1.1.1.1.1.2.1.cmml" xref="S2.Ex5.m1.1.1.1.1.1.2"></abs><apply id="S2.Ex5.m1.1.1.1.1.1.1.cmml" xref="S2.Ex5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex5.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex5.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex5.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex5.m1.1.1.1.1.1.1.2">𝑆</ci><ci id="S2.Ex5.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex5.m1.1.1.1.1.1.1.3">𝐴</ci></apply></apply><apply id="S2.Ex5.m1.2.2.2.2.2.cmml" xref="S2.Ex5.m1.2.2.2.2.1"><abs id="S2.Ex5.m1.2.2.2.2.2.1.cmml" xref="S2.Ex5.m1.2.2.2.2.1.2"></abs><apply id="S2.Ex5.m1.2.2.2.2.1.1.cmml" xref="S2.Ex5.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.Ex5.m1.2.2.2.2.1.1.1.cmml" xref="S2.Ex5.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S2.Ex5.m1.2.2.2.2.1.1.2.cmml" xref="S2.Ex5.m1.2.2.2.2.1.1.2">𝑆</ci><ci id="S2.Ex5.m1.2.2.2.2.1.1.3.cmml" xref="S2.Ex5.m1.2.2.2.2.1.1.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex5.m1.2c">\displaystyle\;\Leftrightarrow|S_{A}|=|S_{B}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.2" class="ltx_p">The definition of equinumerosity can be generalized to <span id="S2.SS2.p1.2.1" class="ltx_text ltx_font_italic">“more than”</span> (and, correspondingly, <span id="S2.SS2.p1.2.2" class="ltx_text ltx_font_italic">“less than”</span>), which lets us define <span id="S2.SS2.p1.2.3" class="ltx_text ltx_font_italic">“most”</span> as follows:</p>
<table id="S6.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\displaystyle\text{most}(A,B)" display="inline"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.3" xref="S2.E2.m1.2.3.cmml"><mtext id="S2.E2.m1.2.3.2" xref="S2.E2.m1.2.3.2a.cmml">most</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.3.1" xref="S2.E2.m1.2.3.1.cmml">​</mo><mrow id="S2.E2.m1.2.3.3.2" xref="S2.E2.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.3.3.2.1" xref="S2.E2.m1.2.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">A</mi><mo id="S2.E2.m1.2.3.3.2.2" xref="S2.E2.m1.2.3.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">B</mi><mo stretchy="false" id="S2.E2.m1.2.3.3.2.3" xref="S2.E2.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.3.cmml" xref="S2.E2.m1.2.3"><times id="S2.E2.m1.2.3.1.cmml" xref="S2.E2.m1.2.3.1"></times><ci id="S2.E2.m1.2.3.2a.cmml" xref="S2.E2.m1.2.3.2"><mtext id="S2.E2.m1.2.3.2.cmml" xref="S2.E2.m1.2.3.2">most</mtext></ci><interval closure="open" id="S2.E2.m1.2.3.3.1.cmml" xref="S2.E2.m1.2.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝐴</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐵</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\displaystyle\text{most}(A,B)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m2.1" class="ltx_math_unparsed" alttext="\displaystyle\Leftrightarrow\exists S\subsetneq S_{A\wedge B}:S\leftrightarrow S_{A\wedge\neg B}" display="inline"><semantics id="S2.E2.m2.1a"><mrow id="S2.E2.m2.1b"><mo stretchy="false" id="S2.E2.m2.1.1">⇔</mo><mo rspace="0.167em" id="S2.E2.m2.1.2">∃</mo><mi id="S2.E2.m2.1.3">S</mi><mo id="S2.E2.m2.1.4">⊊</mo><msub id="S2.E2.m2.1.5"><mi id="S2.E2.m2.1.5.2">S</mi><mrow id="S2.E2.m2.1.5.3"><mi id="S2.E2.m2.1.5.3.2">A</mi><mo id="S2.E2.m2.1.5.3.1">∧</mo><mi id="S2.E2.m2.1.5.3.3">B</mi></mrow></msub><mo lspace="0.278em" rspace="0.278em" id="S2.E2.m2.1.6">:</mo><mi id="S2.E2.m2.1.7">S</mi><mo stretchy="false" id="S2.E2.m2.1.8">↔</mo><msub id="S2.E2.m2.1.9"><mi id="S2.E2.m2.1.9.2">S</mi><mrow id="S2.E2.m2.1.9.3"><mi id="S2.E2.m2.1.9.3.2">A</mi><mo id="S2.E2.m2.1.9.3.1">∧</mo><mrow id="S2.E2.m2.1.9.3.3"><mo rspace="0.167em" id="S2.E2.m2.1.9.3.3.1">¬</mo><mi id="S2.E2.m2.1.9.3.3.2">B</mi></mrow></mrow></msub></mrow><annotation encoding="application/x-tex" id="S2.E2.m2.1c">\displaystyle\Leftrightarrow\exists S\subsetneq S_{A\wedge B}:S\leftrightarrow S_{A\wedge\neg B}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.3" class="ltx_p">Although, at a first glance, this definition looks similar to the one above, it can be seen as suggesting a different algorithmic approach to verifying <span id="S2.SS2.p1.3.1" class="ltx_text ltx_font_italic">“most”</span>, as we will discuss below.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Two interpretation strategies</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">These two characterizations are of course truth-conditionally equivalent, that is, every situation in which one of them holds, the other holds, and vice versa. In particular, if we are just interested in solving a task involving <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">“most”</span> statements, we can be agnostic about which definition our system prefers. Nevertheless, the subtle differences between these two characterizations suggest different algorithmic mechanisms of verifying or falsifying such statements, meaning that a system processes a visual scene differently to come to the (same) conclusion about a statement’s truth.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Characterization (<a href="#S2.E1" title="In 2.1 Generalized quantifiers and “most” ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) represents the <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">cardinality-based strategy</span> of interpreting <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">“most”</span>:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Estimate the number of entities satisfying both predicates (<span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">“red squares”</span>) and the number satisfying one predicate but not the other (<span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">“non-red squares”</span>).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Compare these number estimates and check whether the former is greater than the latter.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">We want to add that, actually, the two definitions in (<a href="#S2.E1" title="In 2.1 Generalized quantifiers and “most” ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) already suggest a minor variation of this mechanism – see <cite class="ltx_cite ltx_citemacro_citet">Hackl (<a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite> for a discussion on <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">“most”</span> versus <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">“more than half”</span>. However, we do not focus on this detail here, and assume the second variant in (<a href="#S2.E1" title="In 2.1 Generalized quantifiers and “most” ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) to be ‘strictly’ simpler in the sense that both involve estimating and comparing cardinalities, but the first variant additionally involves the rather complex operation of halving one number estimates.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Characterization (<a href="#S2.E2" title="In 2.2 Alternative characterization ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) utilizes the concept of a bijection, which is a comparatively simple pairing mechanism and as such could be imagined to be a primitive cognitive operation. This gives us the <span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">pairing-based strategy</span> of verifying <span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_italic">“most”</span>:</p>
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Successively match entities satisfying both predicates (<span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">“red squares”</span>) uniquely with entities satisfying one predicate but not the other (<span id="S2.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">“non-red squares”</span>).</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">The remaining entities are all of one type, so pick one and check whether it is of the first type (<span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">“red square”</span>).</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Cognitive implications</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Finding evidence for one strategy over the other has substantial implications with respect to the ‘cognitive abilities’ of a neural network model. In particular, evidence for a cardinality-based processing of <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">“most”</span> suggests the existence of an <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_bold">approximate number system</span> (ANS), which is able to simultaneously estimate the number of objects in two sets, and perform higher-level operations on the resulting number representations themselves, like the comparison operation here. Explicit counting would be an even more accurate mechanism here, but neither available to the subjects in the experiments of <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite> due to very short scene display time, nor likely to be learned by the ‘one-glance’ feed-forward-style neural network we evaluate in this work<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>By <span id="footnote1.1" class="ltx_text ltx_font_italic">“one-glance feed-forward-style networks”</span> we refer to the predominant type of network architecture which, by design, consists of a fixed sequence of computation steps before arriving at a decision. In particular, such models do not have the ability to interact with their input dynamically depending on the complexity of an instance, or perform more general recursive computations beyond the fixed recurrent modules built into their design. Important for the discussion here is the fact that precise – in contrast to approximate or subitizing-style – counting is by definition a recursive ability, thus impossible to learn for such models.</span></span></span>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The ANS (see appendix in <cite class="ltx_cite ltx_citemacro_citet">Lidz et al. (<a href="#bib.bib11" title="" class="ltx_ref">2011</a>)</cite> for a summary) is an evolutionary comparatively old mechanism which is shared between many different species throughout the animal world. It emerges without explicit training and produces approximate representations of the number of objects of some type. They are approximate in the sense that their number judgment is not ‘sharp’, but resulting behavior exhibits variance – like interpreting <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">“most”</span> statements with a cardinality-based strategy, as described above. This variance follows <span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_bold">Weber’s law</span> which states that the discriminability of two quantities is a function of their ratio<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We want to emphasize that there is evidence for Weber’s Law in a range of other approximate systems, some of them non-discrete and thus rendering a pairing-based strategy impossible. While this does not rule out such a strategy when observing performance decline as predicted by Weber’s Law (which is probably not possible based on extrinsic evaluation alone), it strongly suggests that similar and thus non-pairing-based mechanisms are at work in all of these situations.</span></span></span>. The precision of the ANS is thus usually indicated by a characteristic value called <span id="S2.SS4.p2.1.3" class="ltx_text ltx_font_bold">Weber fraction</span> which relates quantity and variance. The ANS of a typical adult human is often reported to have a Weber fraction of 1.14 or, more tangibly, it can distinguish a ratio of 7:8 with 75% accuracy. Finding evidence for the emergence of a similar system in deep neural networks indicates that these models can indeed learn more abstract concepts (approximate numbers) than mere superficial pattern matching (<span id="S2.SS4.p2.1.4" class="ltx_text ltx_font_italic">“red squares”</span> etc).</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Both mechanisms to interpret <span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_italic">“most”</span> suggest conditions in which they should perform well or badly. For the cardinality-based one, the difference in numbers of the two sets in question is expected to be essential: smaller differences, or greater numbers for the same absolute difference, require more accurate number estimations and hence make this comparison harder, according to Weber’s law. The pairing-based mechanism, on the other hand, is likely affected by the spatial arrangement of the objects in question: if the objects are more clustered within one set, pairing them with objects from the other set becomes harder. Importantly, these conditions are orthogonal, so each mechanism should not substantially be affected by the other condition, respectively. By constructing (artificial) scenes where one of the conditions dominates the configuration, and measuring the accuracy of being able to correctly interpret propositions involving <span id="S2.SS4.p3.1.2" class="ltx_text ltx_font_italic">“most”</span>, the expected difficulties can be confirmed (or refuted) and thus indicate which mechanism is actually at work.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">Using this methodology, <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite> show that humans exhibit a default strategy of interpreting <span id="S2.SS4.p4.1.1" class="ltx_text ltx_font_italic">“most”</span>, at least when only given 200ms to look at the scene and hence having to rely on an immediate subconscious judgment. This strategy is based on the approximate number system and the cardinality-based mechanism. Moreover, the behavior is shown to be sub-optimal in some situations where humans would, in principle, be able to perform better if deviating from their default strategy. Since machine learning models are trained by optimizing parameters for the task at hand, it is far from obvious whether they learn a similarly stable default mechanism, or instead follow a potentially superior adaptive strategy depending on the situation. While the latter is likely more efficient in solving at least a narrowly defined task, the former would instead suggest that the system is able to acquire and utilize core concepts like an approximate number system.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">We may speculate about the innate preference of modern network architectures for either of the strategies: Most of the visual processing is based on convolutions which, being an inherently local computation, we assume would favor the pairing-based strategy via locally matching and ‘cancelling out’ entities of the two predicates. On the other hand, the tensors resulting from the sequence of convolution operations are globally fused into a final embedding vector, which in turn would support the more globally aggregating cardinality-based strategy. However, the type of computations and representations learned by deep neural networks are poorly understood, making such speculations fallacious. We thus emphasize again that the higher-level motivation for this paper is to demonstrate how we need not rely on such speculative ‘narratives’, but can experimentally substantiate our claims.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S2.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:52.0pt;">
<img src="/html/1812.11737/assets/example1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S2.F2.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:160.4pt;">
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text" style="color:#990000;">Exactly two squares are yellow.</span></p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.8pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text" style="color:#009900;">Exactly no square is red.</span></p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.8pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p"><span id="S2.I3.i3.p1.1.1" class="ltx_text" style="color:#990000;">More than half the red shapes are squares.</span></p>
</div>
</li>
<li id="S2.I3.i4" class="ltx_item" style="list-style-type:none;padding-top:-2.8pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i4.p1" class="ltx_para">
<p id="S2.I3.i4.p1.1" class="ltx_p"><span id="S2.I3.i4.p1.1.1" class="ltx_text" style="color:#009900;">More than a third of the shapes are cyan.</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S2.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:52.0pt;">
<img src="/html/1812.11737/assets/example2.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S2.F2.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:160.4pt;">
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p"><span id="S2.I4.i1.p1.1.1" class="ltx_text" style="color:#009900;">Less than half the shapes are green.</span></p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p"><span id="S2.I4.i2.p1.1.1" class="ltx_text" style="color:#990000;">Exactly all magenta shapes are squares.</span></p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p"><span id="S2.I4.i3.p1.1.1" class="ltx_text" style="color:#009900;">At most five shapes are magenta.</span></p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p id="S2.I4.i4.p1.1" class="ltx_p"><span id="S2.I4.i4.p1.1.1" class="ltx_text" style="color:#009900;">At least one triangle is gray.</span></p>
</div>
</li>
</ul>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Two example images with in-/correct captions, taken from the Q-full dataset (all quantifiers/numbers).</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The setup in this paper closely resembles the psychological experiments conducted by <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>, but aimed at a state-of-the-art VQA model and its interpretation of <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">“most”</span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training and evaluation data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the ShapeWorld framework <cite class="ltx_cite ltx_citemacro_cite">Kuhnle and Copestake (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> as starting point to generate appropriate data. ShapeWorld is a configurable generation system for abstract, visually grounded language data. A data point consists of an image, an accompanying caption, and an agreement value indicating whether the caption is true given the image. The underlying task, image caption agreement, essentially corresponds to yes/no questions and as such is a type of visual question answering. Internally, the system samples an abstract world description from which a semantic caption representation is extracted. Both are then turned into ‘natural’ (but still abstract) representations as image and natural language statement, respectively. The latter transformation is based on a semantic grammar formalism (see the paper for details).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We use the pre-implemented quantifier captioner component, both in its unrestricted version and one with available quantifiers restricted to <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">“more than half”</span> and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">“less than half”<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><span id="footnote3.5" class="ltx_text ltx_font_upright">We use these two instead of </span><span id="footnote3.6" class="ltx_text">“most”</span><span id="footnote3.7" class="ltx_text ltx_font_upright"> since ShapeWorld generates them by default. The VQA model is trained from scratch on this data, so we do not expect any of the differences between </span><span id="footnote3.8" class="ltx_text">“most”</span><span id="footnote3.9" class="ltx_text ltx_font_upright"> and </span><span id="footnote3.10" class="ltx_text">“more than half”</span><span id="footnote3.11" class="ltx_text ltx_font_upright"> one observes with humans </span><cite class="ltx_cite ltx_citemacro_cite">Hackl <span id="footnote3.12.1.1.1" class="ltx_text ltx_font_upright">(</span><a href="#bib.bib6" title="" class="ltx_ref">2009</a><span id="footnote3.13.2.2.1" class="ltx_text ltx_font_upright">)</span></cite><span id="footnote3.14" class="ltx_text ltx_font_upright"> to matter.</span></span></span></span></span>. The former contains various additional (generalized) quantifiers (<span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">“no”</span>, <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_italic">“a/three quarter(s)”</span>, <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_italic">“a/two third(s)”</span>, <span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_italic">“all”</span>) and numbers (ranging from <span id="S3.SS1.p2.1.7" class="ltx_text ltx_font_italic">“zero”</span> to <span id="S3.SS1.p2.1.8" class="ltx_text ltx_font_italic">“five”</span>), each in combination with a comparing modifier (<span id="S3.SS1.p2.1.9" class="ltx_text ltx_font_italic">“less than”</span>, <span id="S3.SS1.p2.1.10" class="ltx_text ltx_font_italic">“at most”</span>, <span id="S3.SS1.p2.1.11" class="ltx_text ltx_font_italic">“exactly”</span>, <span id="S3.SS1.p2.1.12" class="ltx_text ltx_font_italic">“at least”</span>, <span id="S3.SS1.p2.1.13" class="ltx_text ltx_font_italic">“more than”</span>, <span id="S3.SS1.p2.1.14" class="ltx_text ltx_font_italic">“not”</span>). We refer to the unrestricted version as Q-full, the other one as Q-half. Figure <a href="#S2.F2" title="Figure 2 ‣ 2.4 Cognitive implications ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows two images together with potential Q-full captions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We also use the default world generator to produce training data (up to 15 randomly positioned objects, as seen in figure <a href="#S2.F2" title="Figure 2 ‣ 2.4 Cognitive implications ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). However, all of the pre-implemented generator modules are too generic for our evaluation purposes, since they do not allow to control attributes and positioning of objects to the desired degree. We thus implemented our own custom generator module with the following functionality to produce test data.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_12.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_23.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_34.png" id="S3.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_45.png" id="S3.F3.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_56.png" id="S3.F3.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_67.png" id="S3.F3.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/1812.11737/assets/random_78.png" id="S3.F3.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>From left to right, the ratio between the two attributes is increasingly balanced.</figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<dl id="S3.I1" class="ltx_description">
<dt id="S3.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Attribute contrast:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">For each instance, either the attribute ‘shape’ or ‘color’ is picked<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Note that we chose the examples in figures to always vary in color only, for clarity.</span></span></span>, and subsequently two values for this attribute and one value for the other is randomly chosen. This means that the only relevant difference between objects in every image is either one of two shape or color values (for instance, red vs blue squares, or red squares vs circles).</p>
</div>
</dd>
<dt id="S3.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Contrast ratios:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p">A list of valid ratios between the contrasted attributes can be specified, from which one will randomly be chosen per instance. For instance, a ratio of 2:3 means that there are 50% more objects with the second than the first attribute. We look at values close to 1:1, that is, 1:2, 2:3, 3:4, 4:5, etc. The increasing difficulty (for humans) resulting from closer ratios is illustrated in figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Training and evaluation data ‣ 3 Experimental setup ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Multiples of the smaller-valued ratios are also generated (e.g., 2:4 or 6:9), within the limit of up to 15 objects overall.</p>
</div>
</dd>
<dt id="S3.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Area-controlled (vs size-controlled):</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p">If this option is set, object sizes are not chosen uniformly across the entire valid range, but size ranges for the two contrasting object types are adapted to the given contrast ratio and size of the chosen shape(s), so that both attributes cover the same image area on average. This means that the more numerous attribute will generally be represented by smaller objects, and the difference in covered area between, for instance, squares and triangles is taken into account.</p>
</div>
</dd>
</dl>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">While objects are still positioned randomly in the basic version of this new generator module, we define two modes which control this aspect as well. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in the introduction illustrates the different modes.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<dl id="S3.I2" class="ltx_description">
<dt id="S3.I2.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I2.ix1.1.1.1" class="ltx_text ltx_font_bold">Partitioned positioning:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I2.ix1.p1" class="ltx_para">
<p id="S3.I2.ix1.p1.1" class="ltx_p">An angle is randomly chosen for each image, and objects of the contrasting attributes are consistently placed either on one side or the other.</p>
</div>
</dd>
<dt id="S3.I2.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I2.ix2.1.1.1" class="ltx_text ltx_font_bold">Paired positioning:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I2.ix2.p1" class="ltx_para">
<p id="S3.I2.ix2.p1.1" class="ltx_p">If there are objects of the contrasted attribute which are not yet paired, one of them is randomly chosen and the new object is placed next to it.</p>
</div>
</dd>
</dl>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The captions of these evaluation instances are always of the form <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_italic">“More/less than half the shapes are X”.</span> with <span id="S3.SS1.p7.1.2" class="ltx_text ltx_font_italic">“X”</span> being the attribute in question, for instance, <span id="S3.SS1.p7.1.3" class="ltx_text ltx_font_italic">“squares”</span> or <span id="S3.SS1.p7.1.4" class="ltx_text ltx_font_italic">“red shapes”</span>. Note that this is an even more constrained captioner than the one used for Q-half. We also emphasize that, in contrast to this new evaluation generator module, the default generator configuration of the ‘quantification’ dataset pre-specified in ShapeWorld is used to generate the training instances in Q-half and Q-full. So these images generally contain many more than just two contrasted attributes, and ratios between attributes tend to be accordingly smaller. The examples in figure <a href="#S2.F2" title="Figure 2 ‣ 2.4 Cognitive implications ‣ 2 The meaning of “most” ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are chosen to illustrate this fact: the second example contains a <span id="S3.SS1.p7.1.5" class="ltx_text ltx_font_italic">“half”</span> statement with ratio 7:8, and the first contains one about a 0:4 ratio, while the image would also allow for a more ‘interesting’ 3:4 ratio (color of semicircles).</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">While we generally try to stay close to the experimental setup of <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>, in the following we point out some differences. Most importantly, instead of just using yellow and blue dots, we use all eight shapes and seven colors that ShapeWorld provides. This increases the visual variety of the instances and thus encourages the system to actually learn the fact that shape and color are attributes that can be combined in any way, instead of just straightforward binary pattern matching. Note that the humans in the psychological experiments have learned language in even more complex situations, which we cannot hope to approximate here. Moreover, our data does not contain yes/no questions but true/false captions, and <span id="S3.SS1.p8.1.1" class="ltx_text ltx_font_italic">“most”</span>-equivalent variations <span id="S3.SS1.p8.1.2" class="ltx_text ltx_font_italic">“more/less than half”</span>. Since the model is trained from scratch on such data, this should not affect results.</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">We do not implement the ‘column pairs mixed/sorted’ modes since they would require comparatively big and mostly empty images, hence require bigger networks and might cause practical learning problems due to sparseness, which we do not want to address here. In contrast, our ‘partitioned’ mode is more difficult than the ones investigated by <cite class="ltx_cite ltx_citemacro_citet">Pietroski et al. (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>, at least for a pairing-based mechanism.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We focus on the FiLM model <cite class="ltx_cite ltx_citemacro_cite">Perez et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite> here since it showed close-to-perfect accuracy on the CLEVR dataset <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib8" title="" class="ltx_ref">2017a</a>)</cite>. We interpret the ShapeWorld captions and agreement values as questions and answer, respectively. The image is processed using either a pre-trained CNN or a four-layer CNN trained from scratch on the task. The question is processed by a GRU. In a sequence of four residual blocks, the image information is processed with its features linearly modulated (scale, offset) conditioned on the processed question embedding. Finally, the classifier module produces the answer, true or false. We use the code made available by the authors of the FiLM model, without changing any parameters. The only aspect we adapt is the trainable four-layer CNN, which uses a kernel size of 3, batch normalization and a stride of 2 in the second and fourth layer.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We considered investigating other models as well: The PG+EE model <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017b</a>)</cite> is openly available and achieved very good performance on CLEVR, however, it relies on the ‘program tree’ provided by CLEVR, and while there exists a basic conversion of ShapeWorld caption models to CLEVR program trees, first, the CLEVR-specific modules do not cover quantifiers like <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">“most”</span> and, second, these program trees encode the interpretation strategy, which would defeat the purpose of our investigation to analyze precisely this mechanism as learned from data. The RelationNet architecture <cite class="ltx_cite ltx_citemacro_cite">Santoro et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> explicitly implements a pairing-based mechanism and hence we considered its evaluation less interesting than FiLM. For similar reasons, we did not focus on the VQA model of <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, whose architecture includes an explicit counting component. While our aim is to investigate the strategy for understanding <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">“most”</span> learned from data, it would be interesting to examine in both cases whether their architectural prior does indeed have the expected effect. Finally, we only learned about the MAC model <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> after we started this project and so decided to leave it for future work, but we definitely consider it one of the most interesting candidate models to evaluate, since its architecture does not suggest an obvious preference for either strategy.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The training set for both Q-full and Q-half consists of around 100k (25x 4096) images with 5 captions per image, so overall around 500k instances. The model is trained for 100k iterations with a batch size of 64. Training performance is measured on an additional validation set of 20k instances. Moreover, we produced 1024 instances for each of the overall 48 evaluation configurations, to investigate the trained model in more detail.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training.</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We train two versions of the FiLM model, with CNN trained from scratch on the task: one on the Q-full dataset which contains all available quantifier and number caption types, the other on the Q-half dataset which is restricted to captions involving the quantifier <span id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">“half”</span> only. Performance of the system over the course of the 100k training iterations is shown in figure <a href="#S4.F4" title="Figure 4 ‣ Evaluation. ‣ 4 Results ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The two models, referred to by Q-full and Q-half below, learn to solve the task quasi-perfectly, with a final accuracy of 98.9% and 99.4% respectively. Not surprisingly, the system trained on the more diverse Q-full training set takes longer to reach this level of performance, but nevertheless plateaus after around 70k iterations.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">For the sake of completeness, we also include the performance of other models in this figure, which failed to show clear improvement over the first 50k iterations. This includes the FiLM model with pre-trained instead of trainable CNN module (Q-full-pre, Q-half-pre), and an earlier trial on Q-half (Q-half-coll) where we did not constrain the data generation to not produce object collisions (the default in ShapeWorld is to allow up to 25% area overlap). We note, however, that we have not done any hyperparameter search which might alleviate these learning problems.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation.</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S4.F5" title="Figure 5 ‣ More balanced ratios. ‣ 4 Results ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents a detailed breakdown of system performance on the evaluation settings. Before discussing the results in detail, we want to reiterate three key differences between the evaluation data and the training data:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The visual scenes here do all exhibit close-to-balanced contrast ratios, while this is not the case for the training instances.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The evaluation scenes only contain objects of two different attribute pairs, and consequently the numbers to compare are generally greater than in the training instances, where more attributes are likely present in a scene.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Q-full contains not just statements involving <span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">“half”</span> – in fact, a random sample of 100 images / 500 captions suggests that they constitute only around 8% of the dataset (and this includes combinations with modifiers beyond <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">“more/less than”</span>).</p>
</div>
</li>
</ul>
<p id="S4.SS0.SSS0.Px2.p1.2" class="ltx_p">Considering that, the relatively high accuracy on test instances throughout indicates a remarkable degree of generalization.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><svg id="S4.F4.pic1" class="ltx_picture" height="201.44" overflow="visible" version="1.1" width="582.21"><g transform="translate(0,201.44) matrix(1 0 0 -1 0 0) translate(29.49,0) translate(0,18.42) matrix(1.0 0.0 0.0 1.0 -29.49 -18.42)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(29.49,0) translate(0,14.09)"><g stroke-width="0.4pt" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#BFBFBF"><path d="M 0 4.33 L 537.73 4.33 M 0 39.12 L 537.73 39.12 M 0 73.91 L 537.73 73.91 M 0 108.7 L 537.73 108.7 M 0 143.5 L 537.73 143.5 M 0 178.29 L 537.73 178.29" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 4.33 L 0 10.24 M 107.55 4.33 L 107.55 10.24 M 215.09 4.33 L 215.09 10.24 M 322.64 4.33 L 322.64 10.24 M 430.19 4.33 L 430.19 10.24 M 537.73 4.33 L 537.73 10.24 M 0 178.29 L 0 172.38 M 107.55 178.29 L 107.55 172.38 M 215.09 178.29 L 215.09 172.38 M 322.64 178.29 L 322.64 172.38 M 430.19 178.29 L 430.19 172.38 M 537.73 178.29 L 537.73 172.38" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 4.33 L 5.91 4.33 M 0 39.12 L 5.91 39.12 M 0 73.91 L 5.91 73.91 M 0 108.7 L 5.91 108.7 M 0 143.5 L 5.91 143.5 M 0 178.29 L 5.91 178.29 M 537.73 4.33 L 531.83 4.33 M 537.73 39.12 L 531.83 39.12 M 537.73 73.91 L 531.83 73.91 M 537.73 108.7 L 531.83 108.7 M 537.73 143.5 L 531.83 143.5 M 537.73 178.29 L 531.83 178.29" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M 0 4.33 L 0 178.29 L 537.73 178.29 L 537.73 4.33 L 0 4.33 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 100.63 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 208.17 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 315.72 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 423.27 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">80</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 527.36 -9.47)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">100</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 -0.13)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">0.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 34.66)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.6" display="inline"><semantics id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 69.46)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">0.7</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 104.25)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">0.8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 139.04)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">0.9</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -11.81 173.83)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M 0 4.33 L 537.73 4.33 L 537.73 178.29 L 0 178.29 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 0 4.33 L 26.89 68.83 L 53.77 71.91 L 80.66 74.58 L 107.55 75.34 L 134.43 79.03 L 161.32 83.38 L 188.21 116.94 L 215.09 133.17 L 241.98 136.65 L 268.87 155.68 L 295.75 161.57 L 322.64 165.58 L 349.53 169.76 L 376.41 171.27 L 403.3 170.95 L 430.19 172.75 L 457.07 172.39 L 483.96 173.31 L 510.85 171.61 L 537.73 174.7" style="fill:none"></path></g><g></g><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 0 4.33 L 26.89 72.96 L 53.77 86.3 L 80.66 107.45 L 107.55 143.97 L 134.43 157.8 L 161.32 166.16 L 188.21 169.45 L 215.09 172.77 L 241.98 172.09 L 268.87 173.28 L 295.75 174.48 L 322.64 174.26 L 349.53 173.94 L 376.41 173 L 403.3 174.14 L 430.19 174.57 L 457.07 173.68 L 483.96 173.89 L 510.85 175.89 L 537.73 169.15" style="fill:none"></path></g><g></g><g stroke="#FF5757" fill="#FF5757" stroke-width="1.6pt" color="#FF5757"><path d="M 0 4.33 L 26.89 40.6 L 53.77 50 L 80.66 50.49 L 107.55 23.17 L 134.43 56.43 L 161.32 15.92 L 188.21 58.52 L 215.09 5.39 L 241.98 22.97 L 268.87 53.04" style="fill:none"></path></g><g></g><g stroke="#8080FF" fill="#8080FF" stroke-width="1.6pt" color="#8080FF"><path d="M 0 4.33 L 26.89 20.13 L 53.77 7.73 L 80.66 24.9 L 107.55 21.37 L 134.43 37.22 L 161.32 23.53 L 188.21 41.43 L 215.09 35.4 L 241.98 38.73 L 268.87 44.98" style="fill:none"></path></g><g></g><g stroke="#57FF57" fill="#57FF57" stroke-width="1.6pt" color="#57FF57"><path d="M 0 4.33 L 26.89 19.88 L 53.77 44.3 L 80.66 18.79 L 107.55 44.32 L 134.43 6.01 L 161.32 0 L 188.21 43.79 L 215.09 23.85 L 241.98 32.58 L 268.87 44.37" style="fill:none"></path></g><g></g></g><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M -2.77 4.33 L 2.77 4.33 M 0 7.1 L 0 1.56" style="fill:none"></path><path d="M 24.12 68.83 L 29.65 68.83 M 26.89 71.6 L 26.89 66.07" style="fill:none"></path><path d="M 51.01 71.91 L 56.54 71.91 M 53.77 74.68 L 53.77 69.14" style="fill:none"></path><path d="M 77.89 74.58 L 83.43 74.58 M 80.66 77.34 L 80.66 71.81" style="fill:none"></path><path d="M 104.78 75.34 L 110.31 75.34 M 107.55 78.11 L 107.55 72.57" style="fill:none"></path><path d="M 131.67 79.03 L 137.2 79.03 M 134.43 81.79 L 134.43 76.26" style="fill:none"></path><path d="M 158.55 83.38 L 164.09 83.38 M 161.32 86.14 L 161.32 80.61" style="fill:none"></path><path d="M 185.44 116.94 L 190.97 116.94 M 188.21 119.71 L 188.21 114.18" style="fill:none"></path><path d="M 212.33 133.17 L 217.86 133.17 M 215.09 135.93 L 215.09 130.4" style="fill:none"></path><path d="M 239.21 136.65 L 244.75 136.65 M 241.98 139.42 L 241.98 133.88" style="fill:none"></path><path d="M 266.1 155.68 L 271.63 155.68 M 268.87 158.44 L 268.87 152.91" style="fill:none"></path><path d="M 292.99 161.57 L 298.52 161.57 M 295.75 164.34 L 295.75 158.8" style="fill:none"></path><path d="M 319.87 165.58 L 325.41 165.58 M 322.64 168.35 L 322.64 162.81" style="fill:none"></path><path d="M 346.76 169.76 L 352.29 169.76 M 349.53 172.53 L 349.53 166.99" style="fill:none"></path><path d="M 373.65 171.27 L 379.18 171.27 M 376.41 174.04 L 376.41 168.5" style="fill:none"></path><path d="M 400.53 170.95 L 406.07 170.95 M 403.3 173.72 L 403.3 168.18" style="fill:none"></path><path d="M 427.42 172.75 L 432.95 172.75 M 430.19 175.52 L 430.19 169.98" style="fill:none"></path><path d="M 454.31 172.39 L 459.84 172.39 M 457.07 175.16 L 457.07 169.62" style="fill:none"></path><path d="M 481.19 173.31 L 486.73 173.31 M 483.96 176.08 L 483.96 170.54" style="fill:none"></path><path d="M 508.08 171.61 L 513.61 171.61 M 510.85 174.38 L 510.85 168.84" style="fill:none"></path><path d="M 534.97 174.7 L 540.5 174.7 M 537.73 177.47 L 537.73 171.93" style="fill:none"></path></g><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M -2.77 4.33 L 2.77 4.33 M 0 7.1 L 0 1.56" style="fill:none"></path><path d="M 24.12 72.96 L 29.65 72.96 M 26.89 75.73 L 26.89 70.2" style="fill:none"></path><path d="M 51.01 86.3 L 56.54 86.3 M 53.77 89.07 L 53.77 83.53" style="fill:none"></path><path d="M 77.89 107.45 L 83.43 107.45 M 80.66 110.22 L 80.66 104.68" style="fill:none"></path><path d="M 104.78 143.97 L 110.31 143.97 M 107.55 146.74 L 107.55 141.2" style="fill:none"></path><path d="M 131.67 157.8 L 137.2 157.8 M 134.43 160.57 L 134.43 155.03" style="fill:none"></path><path d="M 158.55 166.16 L 164.09 166.16 M 161.32 168.92 L 161.32 163.39" style="fill:none"></path><path d="M 185.44 169.45 L 190.97 169.45 M 188.21 172.22 L 188.21 166.69" style="fill:none"></path><path d="M 212.33 172.77 L 217.86 172.77 M 215.09 175.53 L 215.09 170" style="fill:none"></path><path d="M 239.21 172.09 L 244.75 172.09 M 241.98 174.85 L 241.98 169.32" style="fill:none"></path><path d="M 266.1 173.28 L 271.63 173.28 M 268.87 176.04 L 268.87 170.51" style="fill:none"></path><path d="M 292.99 174.48 L 298.52 174.48 M 295.75 177.25 L 295.75 171.71" style="fill:none"></path><path d="M 319.87 174.26 L 325.41 174.26 M 322.64 177.03 L 322.64 171.49" style="fill:none"></path><path d="M 346.76 173.94 L 352.29 173.94 M 349.53 176.71 L 349.53 171.17" style="fill:none"></path><path d="M 373.65 173 L 379.18 173 M 376.41 175.77 L 376.41 170.24" style="fill:none"></path><path d="M 400.53 174.14 L 406.07 174.14 M 403.3 176.91 L 403.3 171.37" style="fill:none"></path><path d="M 427.42 174.57 L 432.95 174.57 M 430.19 177.33 L 430.19 171.8" style="fill:none"></path><path d="M 454.31 173.68 L 459.84 173.68 M 457.07 176.45 L 457.07 170.92" style="fill:none"></path><path d="M 481.19 173.89 L 486.73 173.89 M 483.96 176.65 L 483.96 171.12" style="fill:none"></path><path d="M 508.08 175.89 L 513.61 175.89 M 510.85 178.66 L 510.85 173.12" style="fill:none"></path><path d="M 534.97 169.15 L 540.5 169.15 M 537.73 171.91 L 537.73 166.38" style="fill:none"></path></g><g stroke="#FF5757" fill="#FF5757" stroke-width="1.6pt" color="#FF5757"><path d="M -2.77 4.33 L 2.77 4.33 M 0 7.1 L 0 1.56" style="fill:none"></path><path d="M 24.12 40.6 L 29.65 40.6 M 26.89 43.37 L 26.89 37.83" style="fill:none"></path><path d="M 51.01 50 L 56.54 50 M 53.77 52.76 L 53.77 47.23" style="fill:none"></path><path d="M 77.89 50.49 L 83.43 50.49 M 80.66 53.26 L 80.66 47.72" style="fill:none"></path><path d="M 104.78 23.17 L 110.31 23.17 M 107.55 25.94 L 107.55 20.4" style="fill:none"></path><path d="M 131.67 56.43 L 137.2 56.43 M 134.43 59.2 L 134.43 53.67" style="fill:none"></path><path d="M 158.55 15.92 L 164.09 15.92 M 161.32 18.68 L 161.32 13.15" style="fill:none"></path><path d="M 185.44 58.52 L 190.97 58.52 M 188.21 61.29 L 188.21 55.76" style="fill:none"></path><path d="M 212.33 5.39 L 217.86 5.39 M 215.09 8.15 L 215.09 2.62" style="fill:none"></path><path d="M 239.21 22.97 L 244.75 22.97 M 241.98 25.73 L 241.98 20.2" style="fill:none"></path><path d="M 266.1 53.04 L 271.63 53.04 M 268.87 55.8 L 268.87 50.27" style="fill:none"></path></g><g stroke="#8080FF" fill="#8080FF" stroke-width="1.6pt" color="#8080FF"><path d="M -2.77 4.33 L 2.77 4.33 M 0 7.1 L 0 1.56" style="fill:none"></path><path d="M 24.12 20.13 L 29.65 20.13 M 26.89 22.9 L 26.89 17.36" style="fill:none"></path><path d="M 51.01 7.73 L 56.54 7.73 M 53.77 10.5 L 53.77 4.96" style="fill:none"></path><path d="M 77.89 24.9 L 83.43 24.9 M 80.66 27.67 L 80.66 22.14" style="fill:none"></path><path d="M 104.78 21.37 L 110.31 21.37 M 107.55 24.14 L 107.55 18.6" style="fill:none"></path><path d="M 131.67 37.22 L 137.2 37.22 M 134.43 39.99 L 134.43 34.45" style="fill:none"></path><path d="M 158.55 23.53 L 164.09 23.53 M 161.32 26.3 L 161.32 20.76" style="fill:none"></path><path d="M 185.44 41.43 L 190.97 41.43 M 188.21 44.2 L 188.21 38.67" style="fill:none"></path><path d="M 212.33 35.4 L 217.86 35.4 M 215.09 38.17 L 215.09 32.64" style="fill:none"></path><path d="M 239.21 38.73 L 244.75 38.73 M 241.98 41.5 L 241.98 35.96" style="fill:none"></path><path d="M 266.1 44.98 L 271.63 44.98 M 268.87 47.75 L 268.87 42.22" style="fill:none"></path></g><g stroke="#57FF57" fill="#57FF57" stroke-width="1.6pt" color="#57FF57"><path d="M -2.77 4.33 L 2.77 4.33 M 0 7.1 L 0 1.56" style="fill:none"></path><path d="M 24.12 19.88 L 29.65 19.88 M 26.89 22.64 L 26.89 17.11" style="fill:none"></path><path d="M 51.01 44.3 L 56.54 44.3 M 53.77 47.07 L 53.77 41.54" style="fill:none"></path><path d="M 77.89 18.79 L 83.43 18.79 M 80.66 21.56 L 80.66 16.02" style="fill:none"></path><path d="M 104.78 44.32 L 110.31 44.32 M 107.55 47.09 L 107.55 41.55" style="fill:none"></path><path d="M 131.67 6.01 L 137.2 6.01 M 134.43 8.78 L 134.43 3.25" style="fill:none"></path><path d="M 185.44 43.79 L 190.97 43.79 M 188.21 46.56 L 188.21 41.03" style="fill:none"></path><path d="M 212.33 23.85 L 217.86 23.85 M 215.09 26.62 L 215.09 21.08" style="fill:none"></path><path d="M 239.21 32.58 L 244.75 32.58 M 241.98 35.35 L 241.98 29.82" style="fill:none"></path><path d="M 266.1 44.37 L 271.63 44.37 M 268.87 47.14 L 268.87 41.6" style="fill:none"></path></g><g fill="#FFFFFF" stroke="#000000"><path d="M 418.23 9.83 h 103.09 v 91.63 h -103.09 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 422.38 12.59)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 77.49)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#FF0000" stroke="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 40.06 0) translate(20.26,0) matrix(1.0 0.0 0.0 1.0 -17.49 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="34.98" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.13.13.13.13.13.1.1.1.1.1" class="ltx_text">Q-full</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#0000FF" stroke="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 38.52 0) translate(21.79,0) matrix(1.0 0.0 0.0 1.0 -19.03 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="38.05" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.14.14.14.14.14.2.2.1.1.1" class="ltx_text">Q-half</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 43.05)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#FF5757" stroke="#FF5757" stroke-width="1.6pt" color="#FF5757"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 28.12 0) translate(32.19,0) matrix(1.0 0.0 0.0 1.0 -29.42 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="58.85" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.15.15.15.15.15.3.3.1.1.1" class="ltx_text">Q-full-pre</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 60.27)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#8080FF" stroke="#8080FF" stroke-width="1.6pt" color="#8080FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 26.59 0) translate(33.73,0) matrix(1.0 0.0 0.0 1.0 -30.96 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="61.92" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.16.16.16.16.16.4.4.1.1.1" class="ltx_text">Q-half-pre</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 77.49)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#57FF57" stroke="#57FF57" stroke-width="1.6pt" color="#57FF57"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.84 0) translate(34.48,0) matrix(1.0 0.0 0.0 1.0 -31.71 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="63.42" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.17.17.17.17.17.5.5.1.1.1" class="ltx_text">Q-half-coll</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Training performance (iterations in 1000). <span id="S4.F4.6.1" class="ltx_text ltx_font_italic">Q-full</span>: unconstrained dataset; <span id="S4.F4.7.2" class="ltx_text ltx_font_italic">Q-half</span>: dataset restricted to <span id="S4.F4.8.3" class="ltx_text ltx_font_italic">“less/more than half”</span>; <span id="S4.F4.9.4" class="ltx_text ltx_font_italic">-pre</span>: using pre-trained CNN module; <span id="S4.F4.10.5" class="ltx_text ltx_font_italic">-coll</span>: allowing object overlap.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">More balanced ratios.</h5>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">The most consistent effect is that more balanced ratios of contrasted attributes cause performance to decrease. This is certainly affected by the tendency of the training data to not include many examples of almost balanced ratios. However, if this were the only reason, one would expect a much more sudden and less uniformly linear decrease. More importantly, since Q-full generally contains fewer <span id="S4.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">“half”</span> statements, the decline should be more pronounced here. We do not observe either of these effects, and thus conclude that both models may actually have developed an approximate number system. This is further discussed at the end of this section.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div id="S4.F5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:127.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.7pt,8.2pt) scale(0.886595909263948,0.886595909263948) ;">
<table id="S4.F5.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F5.1.1.1.1" class="ltx_tr">
<td id="S4.F5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.F5.1.1.1.1.1.1" class="ltx_text">train</span></td>
<td id="S4.F5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" rowspan="2"><span id="S4.F5.1.1.1.1.2.1" class="ltx_text">mode</span></td>
<td id="S4.F5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" colspan="8">size-controlled</td>
<td id="S4.F5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="8">area-controlled</td>
</tr>
<tr id="S4.F5.1.1.2.2" class="ltx_tr">
<td id="S4.F5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">all</td>
<td id="S4.F5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1:2</td>
<td id="S4.F5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2:3</td>
<td id="S4.F5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3:4</td>
<td id="S4.F5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4:5</td>
<td id="S4.F5.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5:6</td>
<td id="S4.F5.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6:7</td>
<td id="S4.F5.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">7:8</td>
<td id="S4.F5.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">all</td>
<td id="S4.F5.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1:2</td>
<td id="S4.F5.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2:3</td>
<td id="S4.F5.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3:4</td>
<td id="S4.F5.1.1.2.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4:5</td>
<td id="S4.F5.1.1.2.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5:6</td>
<td id="S4.F5.1.1.2.2.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6:7</td>
<td id="S4.F5.1.1.2.2.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7:8</td>
</tr>
<tr id="S4.F5.1.1.3.3" class="ltx_tr">
<td id="S4.F5.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.F5.1.1.3.3.1.1" class="ltx_text">Q-full</span></td>
<td id="S4.F5.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">random</td>
<td id="S4.F5.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">97</td>
<td id="S4.F5.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">94</td>
<td id="S4.F5.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">91</td>
<td id="S4.F5.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFD9B3;">88</td>
<td id="S4.F5.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="background-color:#FFA6A6;">85</td>
<td id="S4.F5.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">97</td>
<td id="S4.F5.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">91</td>
<td id="S4.F5.1.1.3.3.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFD9B3;">86</td>
<td id="S4.F5.1.1.3.3.18" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFA6A6;">82</td>
</tr>
<tr id="S4.F5.1.1.4.4" class="ltx_tr">
<td id="S4.F5.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_rr">paired</td>
<td id="S4.F5.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">96</td>
<td id="S4.F5.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">90</td>
<td id="S4.F5.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">88</td>
<td id="S4.F5.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_rr" style="background-color:#FFA6A6;">82</td>
<td id="S4.F5.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.4.4.13" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">96</td>
<td id="S4.F5.1.1.4.4.14" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">91</td>
<td id="S4.F5.1.1.4.4.15" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">87</td>
<td id="S4.F5.1.1.4.4.16" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFA6A6;">84</td>
<td id="S4.F5.1.1.4.4.17" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF7373;">80</td>
</tr>
<tr id="S4.F5.1.1.5.5" class="ltx_tr">
<td id="S4.F5.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_rr">part.</td>
<td id="S4.F5.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">89</td>
<td id="S4.F5.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">90</td>
<td id="S4.F5.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFA6A6;">81</td>
<td id="S4.F5.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF7373;">77</td>
<td id="S4.F5.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_rr" style="background-color:#FF4040;">72</td>
<td id="S4.F5.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">89</td>
<td id="S4.F5.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">98</td>
<td id="S4.F5.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.5.5.14" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">88</td>
<td id="S4.F5.1.1.5.5.15" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFA6A6;">82</td>
<td id="S4.F5.1.1.5.5.16" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF7373;">78</td>
<td id="S4.F5.1.1.5.5.17" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF4040;">72</td>
</tr>
<tr id="S4.F5.1.1.6.6" class="ltx_tr">
<td id="S4.F5.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.F5.1.1.6.6.1.1" class="ltx_text">Q-half</span></td>
<td id="S4.F5.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">random</td>
<td id="S4.F5.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">98</td>
<td id="S4.F5.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFD9B3;">88</td>
<td id="S4.F5.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFD9B3;">88</td>
<td id="S4.F5.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="background-color:#FFD9B3;">87</td>
<td id="S4.F5.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">93</td>
<td id="S4.F5.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.6.6.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#73FF73;">97</td>
<td id="S4.F5.1.1.6.6.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.6.6.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFD9B3;">86</td>
<td id="S4.F5.1.1.6.6.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFA6A6;">85</td>
<td id="S4.F5.1.1.6.6.18" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFA6A6;">82</td>
</tr>
<tr id="S4.F5.1.1.7.7" class="ltx_tr">
<td id="S4.F5.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_rr">paired</td>
<td id="S4.F5.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">96</td>
<td id="S4.F5.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">90</td>
<td id="S4.F5.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">86</td>
<td id="S4.F5.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFA6A6;">84</td>
<td id="S4.F5.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_rr" style="background-color:#FF7373;">79</td>
<td id="S4.F5.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#BFFFBF;">92</td>
<td id="S4.F5.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#73FF73;">96</td>
<td id="S4.F5.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFD9B3;">87</td>
<td id="S4.F5.1.1.7.7.15" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFA6A6;">84</td>
<td id="S4.F5.1.1.7.7.16" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF7373;">79</td>
<td id="S4.F5.1.1.7.7.17" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FF7373;">76</td>
</tr>
<tr id="S4.F5.1.1.8.8" class="ltx_tr">
<td id="S4.F5.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">part.</td>
<td id="S4.F5.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#BFFFBF;">91</td>
<td id="S4.F5.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#73FF73;">96</td>
<td id="S4.F5.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFD9B3;">86</td>
<td id="S4.F5.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFA6A6;">83</td>
<td id="S4.F5.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFA6A6;">83</td>
<td id="S4.F5.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="background-color:#FF7373;">80</td>
<td id="S4.F5.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#BFFFBF;">91</td>
<td id="S4.F5.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#73FF73;">100</td>
<td id="S4.F5.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#73FF73;">99</td>
<td id="S4.F5.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#BFFFBF;">94</td>
<td id="S4.F5.1.1.8.8.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFD9B3;">89</td>
<td id="S4.F5.1.1.8.8.15" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFA6A6;">83</td>
<td id="S4.F5.1.1.8.8.16" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FFA6A6;">83</td>
<td id="S4.F5.1.1.8.8.17" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#FF7373;">80</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy in percent of the models trained on Q-full and Q-half for the various evaluation configurations.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Random vs paired vs partitioned.</h5>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">There is a clear negative effect of the partitioned configuration on performance for the model trained on Q-full, which suggests that the learned mechanism is not robust to a high degree of per-attribute clustering. This indicates at most a weak preference towards a pairing-based strategy for Q-full, though, since otherwise the model would not be expected to perform best on the random configuration. Interestingly, the results for Q-half even suggest slightly better performance on the area-controlled partitioned configuration. Overall, no clear preference for either the perfectly clustered partitioned or the perfectly mixed paired arrangement is apparent. We note, however, that the random mode instances are most similar to the random placement of objects in the training data, which might cause this effect.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Size- vs area-controlled.</h5>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.1" class="ltx_p">The performance in both cases is comparable, showing that the models do not (solely) learn to rely on comparing the overall covered area, which would only work well in the size-controlled mode. Nevertheless, we note a tendency for area-controlled instances to be somewhat more difficult in random and paired mode, more so for Q-half, which suggests that the model(s) learn to use covered area as a feature to inform a correct decision in some cases.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Q-full vs Q-half.</h5>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px6.p1.1" class="ltx_p">There seems to be a tendency of the system trained on Q-full to perform marginally better, except for the partitioned mode discussed before. The fact that this model performs at least on a par with the one trained on Q-half, while only seeing a fraction of directly relevant training captions, indicates that the learning process is not ‘distracted’ by the variety of captions, and indeed might profit from it.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><svg id="S4.F6.pic1" class="ltx_picture ltx_centering ltx_figure_panel" height="181.75" overflow="visible" version="1.1" width="267.5"><g transform="translate(0,181.75) matrix(1 0 0 -1 0 0) translate(29.49,0) translate(0,18.42) matrix(1.0 0.0 0.0 1.0 -29.49 -18.42)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(29.49,0) translate(0,18.42)"><g stroke-width="0.4pt" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#BFBFBF"><path d="M 0 0 L 237.73 0 M 0 30.85 L 237.73 30.85 M 0 61.71 L 237.73 61.71 M 0 92.56 L 237.73 92.56 M 0 123.41 L 237.73 123.41 M 0 154.27 L 237.73 154.27" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 21.61 0 L 21.61 5.91 M 43.22 0 L 43.22 5.91 M 64.84 0 L 64.84 5.91 M 86.45 0 L 86.45 5.91 M 108.06 0 L 108.06 5.91 M 129.67 0 L 129.67 5.91 M 151.28 0 L 151.28 5.91 M 172.9 0 L 172.9 5.91 M 194.51 0 L 194.51 5.91 M 216.12 0 L 216.12 5.91 M 21.61 154.27 L 21.61 148.36 M 43.22 154.27 L 43.22 148.36 M 64.84 154.27 L 64.84 148.36 M 86.45 154.27 L 86.45 148.36 M 108.06 154.27 L 108.06 148.36 M 129.67 154.27 L 129.67 148.36 M 151.28 154.27 L 151.28 148.36 M 172.9 154.27 L 172.9 148.36 M 194.51 154.27 L 194.51 148.36 M 216.12 154.27 L 216.12 148.36" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0 L 5.91 0 M 0 30.85 L 5.91 30.85 M 0 61.71 L 5.91 61.71 M 0 92.56 L 5.91 92.56 M 0 123.41 L 5.91 123.41 M 0 154.27 L 5.91 154.27 M 237.73 0 L 231.83 0 M 237.73 30.85 L 231.83 30.85 M 237.73 61.71 L 231.83 61.71 M 237.73 92.56 L 231.83 92.56 M 237.73 123.41 L 231.83 123.41 M 237.73 154.27 L 231.83 154.27" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 154.27 L 237.73 154.27 L 237.73 0 L 0 0 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 18.15 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 39.76 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 61.38 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">3</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 82.99 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 104.6 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 126.21 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 147.83 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">7</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 169.44 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 191.05 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">9</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 209.2 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">0.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 26.39)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.6" display="inline"><semantics id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 57.25)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1c">0.7</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 88.1)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1c">0.8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 118.95)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.m1.1c">0.9</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -11.81 149.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp2"><path d="M 0 0 L 237.73 0 L 237.73 154.27 L 0 154.27 Z"></path></clipPath><g clip-path="url(#pgfcp2)"><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 21.61 154.27 L 43.22 151.18 L 64.84 145.01 L 86.45 135.75 L 108.06 126.5 L 129.67 117.24 L 151.28 107.99 L 172.9 89.47 L 194.51 83.3 L 216.12 70.96" style="fill:none"></path></g><g></g><g stroke="#00FF00" fill="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 21.61 151.18 L 43.22 151.18 L 64.84 141.93 L 86.45 132.67 L 108.06 123.41 L 129.67 117.24 L 151.28 98.73 L 172.9 86.39 L 194.51 86.39 L 216.12 70.96" style="fill:none"></path></g><g></g><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 21.61 154.27 L 43.22 151.18 L 64.84 129.58 L 86.45 123.41 L 108.06 95.65 L 129.67 83.3 L 151.28 67.88 L 172.9 58.62 L 194.51 43.19 L 216.12 33.94" style="fill:none"></path></g><g></g></g><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 18.84 154.27 L 24.38 154.27 M 21.61 157.03 L 21.61 151.5" style="fill:none"></path><path d="M 40.46 151.18 L 45.99 151.18 M 43.22 153.95 L 43.22 148.41" style="fill:none"></path><path d="M 62.07 145.01 L 67.6 145.01 M 64.84 147.78 L 64.84 142.24" style="fill:none"></path><path d="M 83.68 135.75 L 89.22 135.75 M 86.45 138.52 L 86.45 132.99" style="fill:none"></path><path d="M 105.29 126.5 L 110.83 126.5 M 108.06 129.27 L 108.06 123.73" style="fill:none"></path><path d="M 126.91 117.24 L 132.44 117.24 M 129.67 120.01 L 129.67 114.48" style="fill:none"></path><path d="M 148.52 107.99 L 154.05 107.99 M 151.28 110.75 L 151.28 105.22" style="fill:none"></path><path d="M 170.13 89.47 L 175.66 89.47 M 172.9 92.24 L 172.9 86.71" style="fill:none"></path><path d="M 191.74 83.3 L 197.28 83.3 M 194.51 86.07 L 194.51 80.54" style="fill:none"></path><path d="M 213.35 70.96 L 218.89 70.96 M 216.12 73.73 L 216.12 68.2" style="fill:none"></path></g><g stroke="#00FF00" fill="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 18.84 151.18 L 24.38 151.18 M 21.61 153.95 L 21.61 148.41" style="fill:none"></path><path d="M 40.46 151.18 L 45.99 151.18 M 43.22 153.95 L 43.22 148.41" style="fill:none"></path><path d="M 62.07 141.93 L 67.6 141.93 M 64.84 144.69 L 64.84 139.16" style="fill:none"></path><path d="M 83.68 132.67 L 89.22 132.67 M 86.45 135.44 L 86.45 129.9" style="fill:none"></path><path d="M 105.29 123.41 L 110.83 123.41 M 108.06 126.18 L 108.06 120.65" style="fill:none"></path><path d="M 126.91 117.24 L 132.44 117.24 M 129.67 120.01 L 129.67 114.48" style="fill:none"></path><path d="M 148.52 98.73 L 154.05 98.73 M 151.28 101.5 L 151.28 95.96" style="fill:none"></path><path d="M 170.13 86.39 L 175.66 86.39 M 172.9 89.16 L 172.9 83.62" style="fill:none"></path><path d="M 191.74 86.39 L 197.28 86.39 M 194.51 89.16 L 194.51 83.62" style="fill:none"></path><path d="M 213.35 70.96 L 218.89 70.96 M 216.12 73.73 L 216.12 68.2" style="fill:none"></path></g><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 18.84 154.27 L 24.38 154.27 M 21.61 157.03 L 21.61 151.5" style="fill:none"></path><path d="M 40.46 151.18 L 45.99 151.18 M 43.22 153.95 L 43.22 148.41" style="fill:none"></path><path d="M 62.07 129.58 L 67.6 129.58 M 64.84 132.35 L 64.84 126.82" style="fill:none"></path><path d="M 83.68 123.41 L 89.22 123.41 M 86.45 126.18 L 86.45 120.65" style="fill:none"></path><path d="M 105.29 95.65 L 110.83 95.65 M 108.06 98.41 L 108.06 92.88" style="fill:none"></path><path d="M 126.91 83.3 L 132.44 83.3 M 129.67 86.07 L 129.67 80.54" style="fill:none"></path><path d="M 148.52 67.88 L 154.05 67.88 M 151.28 70.64 L 151.28 65.11" style="fill:none"></path><path d="M 170.13 58.62 L 175.66 58.62 M 172.9 61.39 L 172.9 55.85" style="fill:none"></path><path d="M 191.74 43.19 L 197.28 43.19 M 194.51 45.96 L 194.51 40.43" style="fill:none"></path><path d="M 213.35 33.94 L 218.89 33.94 M 216.12 36.71 L 216.12 31.17" style="fill:none"></path></g><g fill="#FFFFFF" stroke="#000000"><path d="M 7.41 4.9 h 85.83 v 56.09 h -85.83 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 11.56 7.67)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 42.5)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#0000FF" stroke="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.84 0) translate(25.85,0) matrix(1.0 0.0 0.0 1.0 -23.08 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="46.16" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic1.17.17.17.17.17.1.1.1.1.1" class="ltx_text">random</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#00FF00" stroke="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 30.06 0) translate(21.62,0) matrix(1.0 0.0 0.0 1.0 -18.85 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="37.71" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic1.18.18.18.18.18.2.2.1.1.1" class="ltx_text">paired</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 42.5)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#FF0000" stroke="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 34.29 0) translate(17.39,0) matrix(1.0 0.0 0.0 1.0 -14.62 -3.22)" fill="#000000" stroke="#000000"><foreignObject width="29.25" height="11.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic1.19.19.19.19.19.3.3.1.1.1" class="ltx_text">part.</span></foreignObject></g></g></g></g></g></g></g></svg></div>
<div class="ltx_flex_cell ltx_flex_size_2"><svg id="S4.F6.pic2" class="ltx_picture ltx_centering ltx_figure_panel" height="181.76" overflow="visible" version="1.1" width="281.83"><g transform="translate(0,181.76) matrix(1 0 0 -1 0 0) translate(29.49,0) translate(0,18.42) matrix(1.0 0.0 0.0 1.0 -29.49 -18.42)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(29.49,0) translate(0,-135.85)"><g stroke-width="0.4pt" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#BFBFBF"><path d="M 0 154.27 L 237.73 154.27 M 0 185.12 L 237.73 185.12 M 0 215.98 L 237.73 215.98 M 0 246.83 L 237.73 246.83 M 0 277.68 L 237.73 277.68 M 0 308.54 L 237.73 308.54" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 154.27 L 0 160.17 M 52.3 154.27 L 52.3 160.17 M 76.07 154.27 L 76.07 160.17 M 118.87 154.27 L 118.87 160.17 M 156.9 154.27 L 156.9 160.17 M 237.73 154.27 L 237.73 160.17 M 0 308.54 L 0 302.63 M 52.3 308.54 L 52.3 302.63 M 76.07 308.54 L 76.07 302.63 M 118.87 308.54 L 118.87 302.63 M 156.9 308.54 L 156.9 302.63 M 237.73 308.54 L 237.73 302.63" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 154.27 L 5.91 154.27 M 0 185.12 L 5.91 185.12 M 0 215.98 L 5.91 215.98 M 0 246.83 L 5.91 246.83 M 0 277.68 L 5.91 277.68 M 0 308.54 L 5.91 308.54 M 237.73 154.27 L 231.83 154.27 M 237.73 185.12 L 231.83 185.12 M 237.73 215.98 L 231.83 215.98 M 237.73 246.83 L 231.83 246.83 M 237.73 277.68 L 231.83 277.68 M 237.73 308.54 L 231.83 308.54" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M 0 154.27 L 0 308.54 L 237.73 308.54 L 237.73 154.27 L 0 154.27 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 140.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 38.85 140.46)" fill="#000000" stroke="#000000"><foreignObject width="26.91" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1.11" display="inline"><semantics id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1.11</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">1.11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">1.11</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 62.62 140.46)" fill="#000000" stroke="#000000"><foreignObject width="26.91" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1.16" display="inline"><semantics id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1.16</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">1.16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">1.16</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 105.41 140.46)" fill="#000000" stroke="#000000"><foreignObject width="26.91" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1.25" display="inline"><semantics id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1.25</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">1.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">1.25</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 143.45 140.46)" fill="#000000" stroke="#000000"><foreignObject width="26.91" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1.33" display="inline"><semantics id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1.33</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">1.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">1.33</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 227.74 140.46)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">1.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 149.81)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">0.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 180.66)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.6" display="inline"><semantics id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 211.52)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">0.7</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 242.37)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">0.8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -24.88 273.22)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">0.9</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -11.81 304.08)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic2.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp3"><path d="M 0 154.27 L 237.73 154.27 L 237.73 308.54 L 0 308.54 Z"></path></clipPath><g clip-path="url(#pgfcp3)"><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 237.73 305.45 L 158.47 299.28 L 118.87 290.02 L 95.09 280.77 L 79.21 271.51 L 67.94 262.26 L 59.43 243.74 L 52.82 237.57 L 47.55 225.23" style="fill:none"></path></g><g></g><g stroke="#00FF00" fill="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 237.73 305.45 L 158.47 296.2 L 118.87 286.94 L 95.09 277.68 L 79.21 271.51 L 67.94 253 L 59.43 240.66 L 52.82 240.66 L 47.55 225.23" style="fill:none"></path></g><g></g><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 237.73 305.45 L 158.47 283.85 L 118.87 277.68 L 95.09 249.92 L 79.21 237.57 L 67.94 222.15 L 59.43 212.89 L 52.82 197.46 L 47.55 188.21" style="fill:none"></path></g><g></g><g stroke="#FFBF80" fill="#FFBF80" stroke-width="1.6pt" color="#FFBF80"><path d="M 0 154.27 L 2.4 158.25 L 4.8 162.22 L 7.2 166.15 L 9.61 170.05 L 12.01 173.92 L 14.41 177.74 L 16.81 181.53 L 19.21 185.26 L 21.61 188.95 L 24.01 192.59 L 26.41 196.18 L 28.82 199.71 L 31.22 203.18 L 33.62 206.59 L 36.02 209.94 L 38.42 213.22 L 40.82 216.44 L 43.22 219.6 L 45.62 222.68 L 48.03 225.7 L 50.43 228.64 L 52.83 231.52 L 55.23 234.33 L 57.63 237.07 L 60.03 239.73 L 62.43 242.33 L 64.84 244.85 L 67.24 247.31 L 69.64 249.69 L 72.04 252 L 74.44 254.25 L 76.84 256.43 L 79.24 258.54 L 81.64 260.58 L 84.05 262.55 L 86.45 264.47 L 88.85 266.31 L 91.25 268.1 L 93.65 269.82 L 96.05 271.48 L 98.45 273.09 L 100.85 274.64 L 103.26 276.13 L 105.66 277.56 L 108.06 278.94 L 110.46 280.27 L 112.86 281.54 L 115.26 282.77 L 117.66 283.95 L 120.07 285.08 L 122.47 286.17 L 124.87 287.21 L 127.27 288.21 L 129.67 289.16 L 132.07 290.08 L 134.47 290.96 L 136.87 291.8 L 139.28 292.61 L 141.68 293.37 L 144.08 294.11 L 146.48 294.82 L 148.88 295.49 L 151.28 296.13 L 153.68 296.74 L 156.08 297.33 L 158.49 297.89 L 160.89 298.42 L 163.29 298.93 L 165.69 299.41 L 168.09 299.87 L 170.49 300.31 L 172.89 300.73 L 175.29 301.13 L 177.7 301.51 L 180.1 301.87 L 182.5 302.22 L 184.9 302.54 L 187.3 302.85 L 189.7 303.15 L 192.1 303.43 L 194.51 303.7 L 196.91 303.95 L 199.31 304.19 L 201.71 304.42 L 204.11 304.64 L 206.51 304.84 L 208.91 305.04 L 211.31 305.23 L 213.72 305.4 L 216.12 305.57 L 218.52 305.73 L 220.92 305.88 L 223.32 306.02 L 225.72 306.16 L 228.12 306.28 L 230.52 306.41 L 232.93 306.52 L 235.33 306.63 L 237.73 306.73" style="fill:none"></path></g><g></g><g stroke="#FFBF80" fill="#FFBF80" stroke-width="1.6pt" color="#FFBF80"><path d="M 0 154.27 L 2.4 157.01 L 4.8 159.74 L 7.2 162.44 L 9.61 165.13 L 12.01 167.81 L 14.41 170.46 L 16.81 173.09 L 19.21 175.7 L 21.61 178.29 L 24.01 180.85 L 26.41 183.39 L 28.82 185.9 L 31.22 188.38 L 33.62 190.84 L 36.02 193.27 L 38.42 195.67 L 40.82 198.05 L 43.22 200.39 L 45.62 202.7 L 48.03 204.99 L 50.43 207.24 L 52.83 209.46 L 55.23 211.65 L 57.63 213.8 L 60.03 215.93 L 62.43 218.02 L 64.84 220.08 L 67.24 222.1 L 69.64 224.1 L 72.04 226.06 L 74.44 227.98 L 76.84 229.88 L 79.24 231.74 L 81.64 233.56 L 84.05 235.36 L 86.45 237.12 L 88.85 238.85 L 91.25 240.54 L 93.65 242.21 L 96.05 243.84 L 98.45 245.44 L 100.85 247.01 L 103.26 248.54 L 105.66 250.05 L 108.06 251.52 L 110.46 252.96 L 112.86 254.38 L 115.26 255.76 L 117.66 257.11 L 120.07 258.44 L 122.47 259.73 L 124.87 261 L 127.27 262.23 L 129.67 263.44 L 132.07 264.63 L 134.47 265.78 L 136.87 266.91 L 139.28 268.01 L 141.68 269.09 L 144.08 270.14 L 146.48 271.17 L 148.88 272.17 L 151.28 273.15 L 153.68 274.1 L 156.08 275.03 L 158.49 275.94 L 160.89 276.83 L 163.29 277.69 L 165.69 278.53 L 168.09 279.35 L 170.49 280.15 L 172.89 280.93 L 175.29 281.69 L 177.7 282.43 L 180.1 283.15 L 182.5 283.85 L 184.9 284.54 L 187.3 285.21 L 189.7 285.85 L 192.1 286.49 L 194.51 287.1 L 196.91 287.7 L 199.31 288.28 L 201.71 288.85 L 204.11 289.4 L 206.51 289.94 L 208.91 290.46 L 211.31 290.97 L 213.72 291.47 L 216.12 291.95 L 218.52 292.42 L 220.92 292.87 L 223.32 293.32 L 225.72 293.75 L 228.12 294.17 L 230.52 294.57 L 232.93 294.97 L 235.33 295.36 L 237.73 295.73" style="fill:none"></path></g><g></g><g stroke="#808080" fill="#808080" stroke-width="0.8pt" color="#808080"><path d="M 0 231.4 L 475.47 231.4" style="fill:none"></path></g><g></g><g stroke="#808080" fill="#808080" stroke-width="0.8pt" color="#808080"><path d="M 52.3 231.4 L 52.3 0" style="fill:none"></path></g><g></g><g stroke="#808080" fill="#808080" stroke-width="0.8pt" color="#808080"><path d="M 76.07 231.4 L 76.07 0" style="fill:none"></path></g><g></g></g><g stroke="#0000FF" fill="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 234.97 305.45 L 240.5 305.45 M 237.73 308.22 L 237.73 302.68" style="fill:none"></path><path d="M 155.71 299.28 L 161.24 299.28 M 158.47 302.05 L 158.47 296.51" style="fill:none"></path><path d="M 116.1 290.02 L 121.63 290.02 M 118.87 292.79 L 118.87 287.26" style="fill:none"></path><path d="M 92.33 280.77 L 97.86 280.77 M 95.09 283.54 L 95.09 278" style="fill:none"></path><path d="M 76.45 271.51 L 81.98 271.51 M 79.21 274.28 L 79.21 268.75" style="fill:none"></path><path d="M 65.18 262.26 L 70.71 262.26 M 67.94 265.02 L 67.94 259.49" style="fill:none"></path><path d="M 56.67 243.74 L 62.2 243.74 M 59.43 246.51 L 59.43 240.98" style="fill:none"></path><path d="M 50.06 237.57 L 55.59 237.57 M 52.82 240.34 L 52.82 234.81" style="fill:none"></path><path d="M 44.78 225.23 L 50.31 225.23 M 47.55 228 L 47.55 222.46" style="fill:none"></path></g><g stroke="#00FF00" fill="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 234.97 305.45 L 240.5 305.45 M 237.73 308.22 L 237.73 302.68" style="fill:none"></path><path d="M 155.71 296.2 L 161.24 296.2 M 158.47 298.96 L 158.47 293.43" style="fill:none"></path><path d="M 116.1 286.94 L 121.63 286.94 M 118.87 289.71 L 118.87 284.17" style="fill:none"></path><path d="M 92.33 277.68 L 97.86 277.68 M 95.09 280.45 L 95.09 274.92" style="fill:none"></path><path d="M 76.45 271.51 L 81.98 271.51 M 79.21 274.28 L 79.21 268.75" style="fill:none"></path><path d="M 65.18 253 L 70.71 253 M 67.94 255.77 L 67.94 250.23" style="fill:none"></path><path d="M 56.67 240.66 L 62.2 240.66 M 59.43 243.43 L 59.43 237.89" style="fill:none"></path><path d="M 50.06 240.66 L 55.59 240.66 M 52.82 243.43 L 52.82 237.89" style="fill:none"></path><path d="M 44.78 225.23 L 50.31 225.23 M 47.55 228 L 47.55 222.46" style="fill:none"></path></g><g stroke="#FF0000" fill="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 234.97 305.45 L 240.5 305.45 M 237.73 308.22 L 237.73 302.68" style="fill:none"></path><path d="M 155.71 283.85 L 161.24 283.85 M 158.47 286.62 L 158.47 281.09" style="fill:none"></path><path d="M 116.1 277.68 L 121.63 277.68 M 118.87 280.45 L 118.87 274.92" style="fill:none"></path><path d="M 92.33 249.92 L 97.86 249.92 M 95.09 252.68 L 95.09 247.15" style="fill:none"></path><path d="M 76.45 237.57 L 81.98 237.57 M 79.21 240.34 L 79.21 234.81" style="fill:none"></path><path d="M 65.18 222.15 L 70.71 222.15 M 67.94 224.91 L 67.94 219.38" style="fill:none"></path><path d="M 56.67 212.89 L 62.2 212.89 M 59.43 215.66 L 59.43 210.12" style="fill:none"></path><path d="M 50.06 197.46 L 55.59 197.46 M 52.82 200.23 L 52.82 194.7" style="fill:none"></path><path d="M 44.78 188.21 L 50.31 188.21 M 47.55 190.98 L 47.55 185.44" style="fill:none"></path></g><g fill="#FFFFFF" stroke="#000000"><path d="M 144.49 159.17 h 85.83 v 73.31 h -85.83 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 148.64 161.94)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 59.17)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#0000FF" stroke="#0000FF" stroke-width="1.6pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.84 0) translate(25.85,0) matrix(1.0 0.0 0.0 1.0 -23.08 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="46.16" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic2.13.13.13.13.13.1.1.1.1.1" class="ltx_text">random</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#00FF00" stroke="#00FF00" stroke-width="1.6pt" color="#00FF00"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 30.06 0) translate(21.62,0) matrix(1.0 0.0 0.0 1.0 -18.85 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="37.71" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic2.14.14.14.14.14.2.2.1.1.1" class="ltx_text">paired</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 42.5)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#FF0000" stroke="#FF0000" stroke-width="1.6pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 0 L 14.58 0 M 11.81 2.77 L 11.81 -2.77" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 34.29 0) translate(17.39,0) matrix(1.0 0.0 0.0 1.0 -14.62 -3.22)" fill="#000000" stroke="#000000"><foreignObject width="29.25" height="11.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic2.15.15.15.15.15.3.3.1.1.1" class="ltx_text">part.</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 59.17)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(1.11,0)" fill="#FFBF80" stroke="#FFBF80" stroke-width="1.6pt" color="#FFBF80"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 30.66 0) translate(21.02,0) matrix(1.0 0.0 0.0 1.0 -18.26 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="36.51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F6.pic2.16.16.16.16.16.4.4.1.1.1" class="ltx_text">model</span></foreignObject></g></g></g></g></g></g></g></svg></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.3.1" class="ltx_text ltx_font_italic">Left:</span> Q-full model performance for increasingly balanced ratios (x-axis indicates ratio via n:n+1).
<br class="ltx_break"><span id="S4.F6.4.2" class="ltx_text ltx_font_italic">Right:</span> Performance as a function of the actual ratio fraction (n+1)/n, with Weber fraction (75%) highlighted.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Ratios and Weber fraction.</h5>

<div id="S4.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p1.1" class="ltx_p">We generated evaluation sets of even more balanced ratios (8:9, 9:10, 10:11, increasing the overall number of objects accordingly to 17/19/21), and in figure <a href="#S4.F6" title="Figure 6 ‣ Q-full vs Q-half. ‣ 4 Results ‣ The meaning of “most” for visual question answering models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> plotted the accuracy of the Q-full model on increasingly balanced sets for all three spatial configuration modes, not controlling for area (which for greater numbers only has a negligible effect anyway). The figure also contains a diagram with accuracy plotted against ratio fraction, which is more common in the context of Weber’s law. The characteristic Weber fraction can be read off directly as the ratio at which a subject is able to distinguish two values with 75% accuracy. We observe around 1.11 for random/paired and 1.16 for partitioned, which corresponds to 9:10 and 6:7 as closest integer ratios. These values are in the same region as the average human Weber fraction, which is often reported as being 1.14, or 7:8.</p>
</div>
<div id="S4.SS0.SSS0.Px7.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p2.1" class="ltx_p">We emphasize that these curves align well with the trend predicted by Weber’s law, even for the ratios with more than 15 objects overall, where such situations have never been encountered during training. All this strongly suggests that the model learns a mechanism similar to an ANS, which is able to produce representations that can (at least) be utilized for identifying the more numerous set. It can in particular be concluded that the system does not actually learn to explicitly count, since we would then not expect to observe such fuzziness characteristic to an ANS.</p>
</div>
<div id="S4.SS0.SSS0.Px7.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p3.1" class="ltx_p">Moreover, since performance is affected somewhat by the partitioned and the area-controlled modes, the interpretation of <span id="S4.SS0.SSS0.Px7.p3.1.1" class="ltx_text ltx_font_italic">“most”</span> seems to be informed by other features as well. As we noted earlier, since the model is trained to optimize this task, an adaptive strategy is not unexpected. On the contrary, more surprising is the fact that an ANS-like system emerges as a dominating ‘backbone’ mechanism, with additional factors acting as less influential ‘secondary’ features.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Visual question answering (VQA) is the general task of answering questions about visual scenes. Since the introduction of the VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>, this dataset was widely used as evaluation benchmark for multimodal deep learning. It provides a shallow categorization of questions, including basic count questions, however, these categories are far too coarse for our purposes.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Motivated by various problems with the VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017</a>); Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>, a range of artificial abstract datasets have been introduced recently. CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib8" title="" class="ltx_ref">2017a</a>)</cite> consists of rendered images of geometric objects and questions generated based on templates, covering some abilities like number or attribute comparison in more detail, but still in a fixed categorization. NLVR <cite class="ltx_cite ltx_citemacro_cite">Suhr et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> contains crowdsourced statements about abstract images, but does not sort them according to some criteria. Recently, the COG dataset <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> was introduced, which most explicitly focuses on replicating psychological experiments for deep learning models, hence most related to our work. However, their dataset does not contain any number or quantifier statements.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">There is some work on investigating deep neural networks which look at numerosity from a more psychologically inspired viewpoint. <cite class="ltx_cite ltx_citemacro_citet">Stoianov and Zorzi (<a href="#bib.bib19" title="" class="ltx_ref">2012</a>)</cite> find that visual numerosity emerges from unsupervised learning on abstract image data. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib23" title="" class="ltx_ref">2015</a>)</cite> look at salient object subitizing in real-world images, formulated as a classification task over five classes ranging from “0” to “4 or more”. In a more general number-per-category classification setup, <cite class="ltx_cite ltx_citemacro_citet">Chattopadhyay et al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> investigate different methods of obtaining counts per object category, including one which is inspired by subitizing. Moving beyond explicit number classification, <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> recently introduced a dedicated counting module for visual question answering.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Other work looks at a similar classification task, but for proper quantifiers like “no”, “few”, “most”, “all”, first on abstract images of circles <cite class="ltx_cite ltx_citemacro_cite">Sorodoc et al. (<a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>, then on natural scenes <cite class="ltx_cite ltx_citemacro_cite">Sorodoc et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>. Recently, <cite class="ltx_cite ltx_citemacro_citet">Pezzelle et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> investigated a hierarchy of quantifier-related classification abilities, from comparatives via quantifiers like the ones above to fine-grained proportions. <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>, besides investigating precise numerosity via number classification as above, also look at approximate numerosity as binary greater/smaller decision, which closely corresponds to our experiments. However, on the one hand, their focus is on the subitizing ability, not the approximate number system. On the other hand, their experiments follow a different methodology in that they already train models on specifically designed datasets, while we deliberately leverage such targeted data only for evaluation.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">On a methodological level, our proposal of inspiring experimental setup and evaluation practice for deep learning by cognitive psychology is in line with that of <cite class="ltx_cite ltx_citemacro_citet">Ritter et al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> and their shape bias investigation for modern vision architectures.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We identify two strategies of algorithmically interpreting <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">“most”</span> in a visual context, with different implications on cognitive concepts. Following experimental practice of similar investigations with humans in psycholinguistics, we design experiments and data to shed light on the question whether the state-of-the-art FiLM VQA model shows preference for one strategy over the other. Performance on various specifically designed instances does indeed indicate that a form of approximate number system is learned, which generalizes to more difficult scenes as predicted by Weber’s law. The results further suggest that additional features influence the interpretation process, which are affected by the spatial arrangement and relative size of objects in a scene. There are many opportunities for future work from here, from strengthening the finding of an approximate number system and further analyzing confounding factors to investigating the relation to more explicit counting tasks.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">We thank the anonymous reviewers for their constructive feedback. AK is grateful for being supported by a Qualcomm Research Studentship and an EPSRC Doctoral Training Studentship.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2016)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing</em>, EMNLP 2016, pages 1955–1960.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, ICCV 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chattopadhyay et al. (2017)</span>
<span class="ltx_bibblock">
Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R. Selvaraju, Dhruv
Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Counting everyday objects in everyday scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, CVPR 2017, pages 4428–4437.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gers and Schmidhuber (2001)</span>
<span class="ltx_bibblock">
Felix A. Gers and Jürgen Schmidhuber. 2001.

</span>
<span class="ltx_bibblock">LSTM recurrent networks learn simple context-free and
context-sensitive languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions on Neural Networks</em>, 12(6):1333–1340.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, CVPR 2017, pages 6325–6334.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hackl (2009)</span>
<span class="ltx_bibblock">
Martin Hackl. 2009.

</span>
<span class="ltx_bibblock">On the grammar and processing of proportional quantifiers: most
versus more than half.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Natural Language Semantics</em>, 17(1):63–98.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2018)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock">Compositional attention networks for machine reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations</em>, ICLR 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017a)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C. Lawrence Zitnick, and Ross Girshick. 2017a.

</span>
<span class="ltx_bibblock">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, CVPR 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017b)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017b.

</span>
<span class="ltx_bibblock">Inferring and executing programs for visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, ICCV 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhnle and Copestake (2017)</span>
<span class="ltx_bibblock">
Alexander Kuhnle and Ann Copestake. 2017.

</span>
<span class="ltx_bibblock">ShapeWorld - A new test methodology for multimodal language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints 1704.04517</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lidz et al. (2011)</span>
<span class="ltx_bibblock">
Jeffrey Lidz, Paul Pietroski, Justin Halberda, and Tim Hunter. 2011.

</span>
<span class="ltx_bibblock">Interface transparency and the psychosemantics of most.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Natural Language Semantics</em>, 19(3):227–256.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al. (2018)</span>
<span class="ltx_bibblock">
Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C.
Courville. 2018.

</span>
<span class="ltx_bibblock">FiLM: Visual reasoning with a general conditioning layer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pezzelle et al. (2018)</span>
<span class="ltx_bibblock">
Sandro Pezzelle, Ionut-Teodor Sorodoc, and Raffaella Bernardi. 2018.

</span>
<span class="ltx_bibblock">Comparatives, quantifiers, proportions: A multi-task model for the
learning of quantities from vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference of the North American Chapter
of the Association for Computational Linguistics</em>, NAACL 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pietroski et al. (2009)</span>
<span class="ltx_bibblock">
Paul Pietroski, Jeffrey Lidz, Tim Hunter, and Justin Halberda. 2009.

</span>
<span class="ltx_bibblock">The meaning of ’most’: Semantics, numerosity and psychology.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Mind and Language</em>, 24(5):554–585.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ritter et al. (2017)</span>
<span class="ltx_bibblock">
Samuel Ritter, David G. T. Barrett, Adam Santoro, and Matt M. Botvinick. 2017.

</span>
<span class="ltx_bibblock">Cognitive psychology for deep neural networks: A shape bias case
study.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine
Learning</em>, ICML 2017, pages 2940–2949.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santoro et al. (2017)</span>
<span class="ltx_bibblock">
Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan
Pascanu, Peter Battaglia, and Timothy P. Lillicrap. 2017.

</span>
<span class="ltx_bibblock">A simple neural network module for relational reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems</em>, NIPS 2017, pages 4974–4983.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorodoc et al. (2016)</span>
<span class="ltx_bibblock">
Ionut Sorodoc, Angeliki Lazaridou, Gemma Boleda, Aurélie Herbelot, Sandro
Pezzelle, and Raffaella Bernardi. 2016.

</span>
<span class="ltx_bibblock">“Look, some green circles!”: Learning to quantify from images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Vision and Language</em>,
Berlin, Germany.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorodoc et al. (2018)</span>
<span class="ltx_bibblock">
Ionut Sorodoc, Sandro Pezzelle, Aurélie Herbelot, Mariella Dimiccoli, and
Raffaella Bernardi. 2018.

</span>
<span class="ltx_bibblock">Learning quantification from images: A structured neural
architecture.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Natural Language Engineering</em>, page 1–30.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stoianov and Zorzi (2012)</span>
<span class="ltx_bibblock">
Ivilin Stoianov and Marco Zorzi. 2012.

</span>
<span class="ltx_bibblock">Emergence of a ‘visual number sense’ in hierarchical generative
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Nature Neuroscience</em>, 15(194):194––196.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. (2017)</span>
<span class="ltx_bibblock">
Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017.

</span>
<span class="ltx_bibblock">A corpus of natural language for visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics</em>, ACL 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2018)</span>
<span class="ltx_bibblock">
Xiaolin Wu, Xi Zhang, and Xiao Shu. 2018.

</span>
<span class="ltx_bibblock">On numerosity of deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints 1802.05160</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Guangyu Robert Yang, Igor Ganichev, Xiao-Jing Wang, Jonathon Shlens, and David
Sussillo. 2018.

</span>
<span class="ltx_bibblock">A dataset and architecture for visual reasoning with a working
memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints 1803.06092</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2015)</span>
<span class="ltx_bibblock">
Jianming Zhang, Shuga Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe
Lin, Xiaohui Shen, Brian Price, and Radomír Měch. 2015.

</span>
<span class="ltx_bibblock">Salient object subitizing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, CVPR 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Yan Zhang, Jonathon Hare, and Adam Prügel-Bennett. 2018.

</span>
<span class="ltx_bibblock">Learning to count objects in natural images for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations</em>, ICLR 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1812.11736" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1812.11737" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1812.11737">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1812.11737" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1812.11738" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar  4 22:15:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
