<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation</title>
<!--Generated on Tue Jan  2 20:02:39 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.01419v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Related Work ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS0.SSS0.Px1" title="Translation Divergence ‣ 2 Related Work ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Translation Divergence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS0.SSS0.Px2" title="Diverse Machine Translation ‣ 2 Related Work ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Diverse Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS0.SSS0.Px3" title="Algorithmic Bias ‣ 2 Related Work ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Algorithmic Bias</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS0.SSS0.Px1" title="Types of Morphosyntactic Divergence ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Types of Morphosyntactic Divergence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS0.SSS0.Px2" title="Data ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS0.SSS0.Px3" title="Models ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS0.SSS0.Px4" title="Annotations ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Annotations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Comparative Analysis of MT vs HT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>MT is Less Morphosyntactically Diverse Than HT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px1" title="Preliminaries ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px2" title="Aggregate Finding ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Aggregate Finding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px3" title="Finding by Source Pattern ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Finding by Source Pattern</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="4.2 MT is More Convergent Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>MT is More Convergent Than HT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS2.SSS0.Px1" title="Preliminaries ‣ 4.2 MT is More Convergent Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS2.SSS0.Px2" title="Aggregate Finding ‣ 4.2 MT is More Convergent Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Aggregate Finding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS2.SSS0.Px3" title="Finding by Source Pattern ‣ 4.2 MT is More Convergent Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Finding by Source Pattern</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="4.3 MT Looks Less Like HT For Less Frequent Patterns ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>MT Looks Less Like HT For Less Frequent Patterns</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px1" title="Preliminaries ‣ 4.3 MT Looks Less Like HT For Less Frequent Patterns ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px2" title="Finding ‣ 4.3 MT Looks Less Like HT For Less Frequent Patterns ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Finding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS4" title="4.4 Beyond One-to-one Alignments ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Beyond One-to-one Alignments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS4.SSS0.Px1" title="Preliminaries ‣ 4.4 Beyond One-to-one Alignments ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS4.SSS0.Px2" title="Finding ‣ 4.4 Beyond One-to-one Alignments ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Finding</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Understanding the Discrepancy</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S5.SS0.SSS0.Px1" title="Decoding Algorithms ‣ 5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Decoding Algorithms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S5.SS0.SSS0.Px2" title="Beam Search is Biased Against Diversity and Divergence ‣ 5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Beam Search is Biased Against Diversity and Divergence</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Divergence and MT Quality</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS0.SSS0.Px1" title="Preliminaries ‣ 6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS0.SSS0.Px2" title="Findings ‣ 6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title">Findings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="7 Conclusion ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A1" title="Appendix A Analysis Subset Filtered Using LaBSE Embeddings ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Analysis Subset Filtered Using LaBSE Embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A2" title="Appendix B Percentage of Content Words and Their Alignments ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Percentage of Content Words and Their Alignments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: mfirstuc</li>
<li>failed: suffix</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.01419v1 [cs.CL] 02 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\WithSuffix</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[2]<span class="ltx_text ltx_font_italic" id="p1.1.1" style="color:#000000;">*#2</span>
<span class="ltx_ERROR undefined" id="p1.1.2">\WithSuffix</span>[1]<span class="ltx_text ltx_font_italic" id="p1.1.3" style="color:#000000;">*orange</span>#1

<span class="ltx_ERROR undefined" id="p1.1.4">\WithSuffix</span>[1]<span class="ltx_text ltx_font_italic" id="p1.1.5" style="color:#000000;">*red</span>#1


<span class="ltx_ERROR undefined" id="p1.1.6">\WithSuffix</span><span class="ltx_text ltx_font_italic" id="p1.1.7" style="color:#FF0000;">**</span>Fill in.























</p>
</div>
<h1 class="ltx_title ltx_title_document">To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiaming Luo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Colin Cherry
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">George Foster

<br class="ltx_break"/> 
<br class="ltx_break"/>Google Translate Research

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{jmluo,colincherry,fosterg}.google.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">We conduct a large-scale fine-grained comparative analysis
of machine translations (MT) against human translations (HT) through
the lens of morphosyntactic divergence.
Across three language pairs and two types of divergence
defined as the structural difference between the source and the target,
MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments.
Through analysis on different decoding algorithms,
we attribute this discrepancy to the use of beam search
that biases MT towards more convergent patterns.
This bias is most amplified when the convergent pattern appears around 50% of the time in training data.
Lastly, we show that for a majority of morphosyntactic divergences,
their presence in HT is
correlated with decreased MT performance, presenting a greater challenge for MT systems.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_flex_size_1 ltx_align_middle" id="S1.F1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.F1.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S1.F1.3.1.1.1">
<span class="ltx_text" id="S1.F1.3.1.1.1.1" style="font-size:70%;">Readers are </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.1.1.1.2" style="font-size:70%;">cautioned</span><span class="ltx_text" id="S1.F1.3.1.1.1.3" style="font-size:70%;"> not to </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.1.1.1.4" style="font-size:70%;">place</span><span class="ltx_text" id="S1.F1.3.1.1.1.5" style="font-size:70%;"> undue reliance on …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.2.2">
<td class="ltx_td ltx_align_left" id="S1.F1.3.2.2.1">
<span class="ltx_text ltx_framed_underline" id="S1.F1.3.2.2.1.1" style="font-size:70%;">Il</span><span class="ltx_text" id="S1.F1.3.2.2.1.2" style="font-size:70%;"> est recommandé aux </span><span class="ltx_text ltx_framed_underline" id="S1.F1.3.2.2.1.3" style="font-size:70%;">lecteurs</span><span class="ltx_text" id="S1.F1.3.2.2.1.4" style="font-size:70%;"> de ne pas accorder…</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.F1.3.3.3.1">
<span class="ltx_text" id="S1.F1.3.3.3.1.1" style="font-size:70%;">PCO has </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.3.3.1.2" style="font-size:70%;">continued</span><span class="ltx_text" id="S1.F1.3.3.3.1.3" style="font-size:70%;"> to </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.3.3.1.4" style="font-size:70%;">assist</span><span class="ltx_text" id="S1.F1.3.3.3.1.5" style="font-size:70%;"> …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.4.4">
<td class="ltx_td ltx_align_left" id="S1.F1.3.4.4.1">
<span class="ltx_text" id="S1.F1.3.4.4.1.1" style="font-size:70%;">Le BCP a </span><span class="ltx_text ltx_framed_underline" id="S1.F1.3.4.4.1.2" style="font-size:70%;">constamment</span><span class="ltx_text" id="S1.F1.3.4.4.1.3" style="font-size:70%;"> épaulé …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.F1.3.5.5.1">
<span class="ltx_text" id="S1.F1.3.5.5.1.1" style="font-size:70%;">These meetings </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.5.5.1.2" style="font-size:70%;">seek</span><span class="ltx_text" id="S1.F1.3.5.5.1.3" style="font-size:70%;"> to </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.5.5.1.4" style="font-size:70%;">strengthen</span><span class="ltx_text" id="S1.F1.3.5.5.1.5" style="font-size:70%;"> …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.6.6">
<td class="ltx_td ltx_align_left" id="S1.F1.3.6.6.1">
<span class="ltx_text" id="S1.F1.3.6.6.1.1" style="font-size:70%;">Ces réunions </span><span class="ltx_text ltx_framed_underline" id="S1.F1.3.6.6.1.2" style="font-size:70%;">ont pour but</span><span class="ltx_text" id="S1.F1.3.6.6.1.3" style="font-size:70%;"> de renforcer …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.F1.3.7.7.1">
<span class="ltx_text" id="S1.F1.3.7.7.1.1" style="font-size:70%;">…weaknesses will </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.7.7.1.2" style="font-size:70%;">become</span><span class="ltx_text" id="S1.F1.3.7.7.1.3" style="font-size:70%;"> more </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.7.7.1.4" style="font-size:70%;">apparent</span><span class="ltx_text" id="S1.F1.3.7.7.1.5" style="font-size:70%;"> …</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.3.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.F1.3.8.8.1">
<span class="ltx_text" id="S1.F1.3.8.8.1.1" style="font-size:70%;">Les faiblesses </span><span class="ltx_text ltx_framed_underline" id="S1.F1.3.8.8.1.2" style="font-size:70%;">apparaîtront</span><span class="ltx_text" id="S1.F1.3.8.8.1.3" style="font-size:70%;">…</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="307" id="S1.F1.g1" src="x1.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.22.1">Top table</span>: Examples of divergences
in HT for En<math alttext="\to" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mo id="S1.F1.2.m1.1.1" stretchy="false" xref="S1.F1.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">→</annotation></semantics></math>Fr WMT15 training data <cite class="ltx_cite ltx_citemacro_citep">(Bojar et al., <a class="ltx_ref" href="#bib.bib6" title="">2015</a>)</cite>, with relevant fragments of the source/target shown in the first/second rows.
The English control constructions are bolded including both the finite root verb and the controlled word,
while the French phrases of interest are underlined.
<span class="ltx_text ltx_font_bold" id="S1.F1.23.2">Bottom figure</span>: Percentages of target patterns for HT and MT, with obligatory control finite verbs as the source pattern.
<span class="ltx_text ltx_font_typewriter" id="S1.F1.24.3">o2o:conv</span>: one-to-one convergent patterns where the target phrase uses a similar control construction to the source;
<span class="ltx_text ltx_font_typewriter" id="S1.F1.25.4">o2o:div</span>: one-to-one divergent patterns where the target differs structurally from the source;
<span class="ltx_text ltx_font_typewriter" id="S1.F1.26.5">null</span>: no target word is aligned;
<span class="ltx_text ltx_font_typewriter" id="S1.F1.27.6">others</span>: other less frequent patterns (e.g., one-to-many alignments).
The percentages of all four categories sum up to 100%.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Translation divergences occur when the translations differ structurally from the source sentences,
typically as a result of either inherent crosslingual differences
or idiosyncratic preferences of translators.
These divergences happen naturally in the translation process and can be readily found in human translations (HT), including those used for training machine translation (MT) systems (see the table in Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">1</span></a> for some examples).
Their existence in HT has long been regarded
as a key challenge for MT <cite class="ltx_cite ltx_citemacro_cite">Dorr (<a class="ltx_ref" href="#bib.bib14" title="">1994</a>)</cite>
and more recent empirical studies have demonstrated the abundance of translation divergences in
HT  <cite class="ltx_cite ltx_citemacro_cite">Deng and Xue (<a class="ltx_ref" href="#bib.bib12" title="">2017</a>); Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>.
</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In contrast to HT, MT outputs tend to be less diverse and more literal (i.e., absence of translation divergence), exhibiting the features of <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">translationese</em> <cite class="ltx_cite ltx_citemacro_citep">(Gellerstam, <a class="ltx_ref" href="#bib.bib26" title="">1986</a>)</cite>.
This <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">qualitative</em> difference between HT and MT has inspired a rich body of work attempting to narrow the gap, such as automatic
detection of machine translated texts in the training data <cite class="ltx_cite ltx_citemacro_citep">(Kurokawa et al., <a class="ltx_ref" href="#bib.bib34" title="">2009</a>; Lembersky et al., <a class="ltx_ref" href="#bib.bib36" title="">2012</a>; Aharoni et al., <a class="ltx_ref" href="#bib.bib1" title="">2014</a>; Riley et al., <a class="ltx_ref" href="#bib.bib49" title="">2020</a>; Freitag et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>,
training MT systems on more diverse translations <cite class="ltx_cite ltx_citemacro_citep">(Khayrallah et al., <a class="ltx_ref" href="#bib.bib32" title="">2020</a>; Bao et al., <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite>, and carefully reordering the examples to reduce the degree of divergence between the source and the target <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib66" title="">2007</a>; Zhang and Zong, <a class="ltx_ref" href="#bib.bib71" title="">2016</a>; Zhou et al., <a class="ltx_ref" href="#bib.bib73" title="">2019</a>)</cite>.
The challenges that translation divergences present do not just concern training MT systems, but also their evaluation <cite class="ltx_cite ltx_citemacro_citep">(Koppel and Ordan, <a class="ltx_ref" href="#bib.bib33" title="">2011</a>; Freitag et al., <a class="ltx_ref" href="#bib.bib22" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Nonetheless, even  as we gain deepened understanding of how to address these challenges, it remains unclear
<em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">how quantitatively different</em> MT and HT are in terms of divergences.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use the term MT to mean the version of MT tested in this project’s experiments: bilingual encoder-decoder Transformer-base networks with beam search decoding (see Models in Section <a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</span></span></span>
Control verbs,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Control_(linguistics)" title="">https://en.wikipedia.org/wiki/Control_(linguistics)</a>. They are coded as <span class="ltx_text ltx_font_typewriter" id="footnote2.1">xcomp</span> in Universal Dependencies (see <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://universaldependencies.org/u/dep/all.html#xcomp-open-clausal-complement" title="">https://universaldependencies.org/u/dep/all.html#xcomp-open-clausal-complement</a>).</span></span></span> for instance, provide a great case study
to showcase this difference.
There is much uncertainty when translating them from English to French, and human translators employ a wide variety of constructions including many divergent patterns (Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">1</span></a>).
In comparison,
MT is much more likely to preserve the source structure, with the convergent pattern comprising about 20% more of all translations of control verbs.
This difference exemplifies MT’s undesirable tendency to
produce translationese that is too literal and lacks structural diversity <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="#bib.bib21" title="">2019</a>); Bizzoni et al. (<a class="ltx_ref" href="#bib.bib5" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we seek to systematically investigate this difference
by conducting a <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">large-scale</em> <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">fine-grained</em> <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">comparative</em> analysis
on the distribution of translation divergences for HT and MT,
all through the lens of morphosyntax.
More specifically, we aim to answer the following research questions:
1) How are MT and HT quantitatively different in terms of morphosyntactic divergence?
2) How do we explain or understand this difference?
3) How do translation divergences in HT affect MT quality? In other words, do MT systems have
more difficulty translating source sentences that exhibit divergences in HT?</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Through extensive analyses based on three language pairs and two types of morphosyntactic divergence
using the annotational framework of Universal Dependencies <cite class="ltx_cite ltx_citemacro_cite">Nivre et al. (<a class="ltx_ref" href="#bib.bib44" title="">2016</a>)</cite>,
we make the following empirical observations:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">MT is more <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">conservative</em> than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">MT is morphosyntactically less similar to HT for less frequent source patterns.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The distributional difference can be largely attributed to the use of beam search, which
is biased towards convergent patterns. This bias is most amplified when the convergent
target patterns appear around 50% of the time out of all translations of the same source pattern in the training data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">A majority of the most frequent divergent patterns are correlated with decreased MT performance. This correlation cannot be fully explained by the lower frequencies
of the relevant divergences.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To the best of our knowledge, this is the first work to present the comparative perspective of HT vs MT in such fine granularity covering thousands of morphosyntactic constructions. In the remaining sections, we first briefly describe related work in Section <a class="ltx_ref" href="#S2" title="2 Related Work ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
The experimental setup is described in detail in Section <a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a>.
We demonstrate the quantitative difference between MT and HT in Section <a class="ltx_ref" href="#S4" title="4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">4</span></a>,
and seek to understand this discrepancy in Section <a class="ltx_ref" href="#S5" title="5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">5</span></a>.
Lastly, we explore the correlation between the presence of divergences in HT with MT performance
in Section <a class="ltx_ref" href="#S6" title="6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">6</span></a> and make conclusions in Section <a class="ltx_ref" href="#S7" title="7 Conclusion ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">7</span></a>.

</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Translation Divergence</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Systematic and theoretical treatment of translation divergences started in the
early 1990s, focusing on European languages <cite class="ltx_cite ltx_citemacro_cite">Dorr (<a class="ltx_ref" href="#bib.bib15" title="">1992</a>, <a class="ltx_ref" href="#bib.bib16" title="">1993</a>, <a class="ltx_ref" href="#bib.bib14" title="">1994</a>)</cite>.
Later work has expanded into more languages,
and focused on the automatic detection of divergences <cite class="ltx_cite ltx_citemacro_cite">Gupta and Chatterjee (<a class="ltx_ref" href="#bib.bib28" title="">2001</a>, <a class="ltx_ref" href="#bib.bib29" title="">2003</a>); Sinha et al. (<a class="ltx_ref" href="#bib.bib57" title="">2005</a>); Mishra and Mishra (<a class="ltx_ref" href="#bib.bib41" title="">2009</a>); Saboor and Khan (<a class="ltx_ref" href="#bib.bib52" title="">2010</a>)</cite>
or their empirical distributions in human
translations <cite class="ltx_cite ltx_citemacro_cite">Wong et al. (<a class="ltx_ref" href="#bib.bib68" title="">2017</a>); Deng and Xue (<a class="ltx_ref" href="#bib.bib12" title="">2017</a>); Wein and Schneider (<a class="ltx_ref" href="#bib.bib67" title="">2021</a>)</cite>.
Relatedly, <cite class="ltx_cite ltx_citemacro_citet">Carpuat et al. (<a class="ltx_ref" href="#bib.bib11" title="">2017</a>); Vyas et al. (<a class="ltx_ref" href="#bib.bib65" title="">2018</a>); Briakou and Carpuat (<a class="ltx_ref" href="#bib.bib8" title="">2020</a>)</cite> focused on identifying semantic divergences that manifest in translations not entirely semantically equivalent to the original sources.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">The closest work to ours is from <cite class="ltx_cite ltx_citemacro_citet">Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite> who
proposed to investigate fine-grained crosslingual morphosyntactic divergence
based on Universal Dependencies.
They augmented a subset of the Parallel Universal Dependencies (PUD) corpus <cite class="ltx_cite ltx_citemacro_cite">Zeman et al. (<a class="ltx_ref" href="#bib.bib70" title="">2017</a>)</cite>
with human-annotated word alignments for five language pairs and focused
exclusively on content words.
While our work shares a similar conceptional and methodological
foundation to theirs, our goal is to conduct a comparative analysis between HT and MT.
In addition, we rely on a dependency parser and a word aligner (see Section <a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a> for more details) to reach a sufficiently large scale
to enable the investigation of more fine-grained divergences.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Diverse Machine Translation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">MT systems tend to produce less diverse outputs in general <cite class="ltx_cite ltx_citemacro_cite">Gimpel et al. (<a class="ltx_ref" href="#bib.bib27" title="">2013</a>); Ott et al. (<a class="ltx_ref" href="#bib.bib45" title="">2018</a>)</cite>,
which is particularly harmful for back translation <cite class="ltx_cite ltx_citemacro_cite">Edunov et al. (<a class="ltx_ref" href="#bib.bib18" title="">2018</a>); Soto et al. (<a class="ltx_ref" href="#bib.bib58" title="">2020</a>); Burchell et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>.
To address this issue, various techniques have been proposed in the literature,
including modified decoding algorithms <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="#bib.bib38" title="">2016</a>); Sun et al. (<a class="ltx_ref" href="#bib.bib59" title="">2020</a>); Li et al. (<a class="ltx_ref" href="#bib.bib37" title="">2021</a>)</cite>,
mixtures of experts <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a class="ltx_ref" href="#bib.bib55" title="">2019</a>)</cite>,
Bayesian models <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="#bib.bib69" title="">2020</a>)</cite>, additional codes (syntax or latent) <cite class="ltx_cite ltx_citemacro_cite">Shu et al. (<a class="ltx_ref" href="#bib.bib56" title="">2019</a>); Lachaux et al. (<a class="ltx_ref" href="#bib.bib35" title="">2020</a>)</cite>
and training with simulated multi-reference corpora <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>.
In all aforementioned works, the emphasis is on the lack of diversity in MT outputs rather than comparing
them systematically against HT.
Notable exceptions include <cite class="ltx_cite ltx_citemacro_citet">Roberts et al. (<a class="ltx_ref" href="#bib.bib51" title="">2020</a>)</cite> who investigated the distributional
differences between MT and HT in terms of n-grams, sentence length, punctuation, and copy rates.
<cite class="ltx_cite ltx_citemacro_citet">Marchisio et al. (<a class="ltx_ref" href="#bib.bib40" title="">2022</a>)</cite> compared translations from supervised MT and unsupervised MT and noted their systematic style differences based on
similarity and monotonicity in their POS sequences.
In contrast, our work goes beyond surface features and focuses
on fine-grained morphosyntactic divergences.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Algorithmic Bias</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Another closely related line of work studies algorithmic biases
of current NLP systems,
with particular emphasis on gender and racial biases <cite class="ltx_cite ltx_citemacro_cite">Bolukbasi et al. (<a class="ltx_ref" href="#bib.bib7" title="">2016</a>); Caliskan et al. (<a class="ltx_ref" href="#bib.bib10" title="">2017</a>); Zhao et al. (<a class="ltx_ref" href="#bib.bib72" title="">2017</a>); Garg et al. (<a class="ltx_ref" href="#bib.bib24" title="">2018</a>)</cite>.
Specifically for MT,
researchers have focused on lexical diversity by comparing
HT against post-editese <cite class="ltx_cite ltx_citemacro_cite">Toral (<a class="ltx_ref" href="#bib.bib60" title="">2019</a>)</cite>
or MT outputs directly <cite class="ltx_cite ltx_citemacro_cite">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib62" title="">2019</a>)</cite>;
<cite class="ltx_cite ltx_citemacro_citet">Bizzoni et al. (<a class="ltx_ref" href="#bib.bib5" title="">2020</a>)</cite> have compared HT, MT and simultaneous interpreting
in terms of translationese using POS perplexity and dependency length.
Most related to our work, <cite class="ltx_cite ltx_citemacro_citet">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib61" title="">2021</a>)</cite> have conducted an extensive comparison
between HT and MT based on a suite of lexical and morphological diversity metrics.
While our study reaches a similar conclusion that MT is less diverse than HT,
we explore morphosyntactic patterns on a more fine-grained level,
and also reveal the bias of MT (and more specifically beam search) towards
convergent structures.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Types of Morphosyntactic Divergence</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">In this study, we experiment with
two types of translation patterns
based on the annotational scheme of Universal Dependencies:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(A)</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.ix1.p1.1.1">Word-based</span>: POS tags for the aligned word pair.
We additionally include their parent and child syntactic dependencies for more granularity.
Order of the children dependencies is ignored.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(B)</span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.ix2.p1.1.1">Arc-based</span>: The source dependency arc, and the
target path between the aligned words of the arc’s head and tail.
Directionality of the target dependencies is ignored.
We additionally include the POS tags of both the head and the tail for more granularity.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.2">These types are largely based on the proposal of <cite class="ltx_cite ltx_citemacro_citet">Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>,
with modifications to accommodate more granularity.
With either type, the translation pattern is a <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.2.1">convergence</em>
if the source and the target sides have the same structure (word-based or arc-based), and otherwise a <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.2.2">divergence</em>.
Notationally, we use tildes to connect the various parts of the pattern in a fixed order.
For instance, for the control verb <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.2.3">“cautioned”</em>
in Figure <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ Types of Morphosyntactic Divergence ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">2</span></a>,
its word-based divergence has <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.4">root~VERB~nsubj+xcomp</span> on the source side, where <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.5">VERB</span> corresponds
to its POS tag, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.6">root</span> its parent dependency, and
<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.7">nsubj</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.8">xcomp</span> its two child dependencies. Similarly, we have
<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.9">root~VERB~nsubj+obl+xcomp</span> on the target side.
With regard to an arc-based divergence, for the source arc between the words <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.2.10">“cautioned”</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.2.11">“readers”</em>, we denote it as <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.12">VERB~nsubj~NOUN</span>,
where <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.13">nsubj</span> is the dependency relation of the arc, and <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.14">VERB</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.15">NOUN</span> the
POS tags of the head and the tail, respectively. Similarly, we denote the aligned target pattern as
<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.2.16">VERB~obl~NOUN</span>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="464" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustration of the two types of morphosyntactic
divergence. See Section <a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a> for details.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Data</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.3">We conduct experiments for three language pairs using WMT datasets <cite class="ltx_cite ltx_citemacro_cite">Bojar et al. (<a class="ltx_ref" href="#bib.bib6" title="">2015</a>); Barrault et al. (<a class="ltx_ref" href="#bib.bib3" title="">2019</a>)</cite>: En<math alttext="\to" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>Zh (WMT19),
En<math alttext="\to" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a"><mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" stretchy="false" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.2.m2.1d">→</annotation></semantics></math>Fr (WMT15) and En<math alttext="\to" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS0.SSS0.Px2.p1.3.m3.1a"><mo id="S3.SS0.SSS0.Px2.p1.3.m3.1.1" stretchy="false" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.3.m3.1d">→</annotation></semantics></math>De (WMT19).
All training datasets are lightly filtered based on length, length ratio and language ID,
and deduplicated.
For each language pair, one million sentences
are held out from the training split to form an analysis subset. All analyses in our study are based on this subset to eliminate potential confounding effects from domain mismatch.
Table <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ Data ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of distinct
source or target patterns found in the analysis set for each language pair.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.3.4.1.1">Language pair</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.4.1.2">Source</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.4.1.3">Target</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.1">En<math alttext="\to" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">→</annotation></semantics></math>De</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2">17 055</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3">15 040</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.1">En<math alttext="\to" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><mo id="S3.T1.2.2.1.m1.1.1" stretchy="false" xref="S3.T1.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><ci id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">→</annotation></semantics></math>Zh</th>
<td class="ltx_td ltx_align_right" id="S3.T1.2.2.2">14 816</td>
<td class="ltx_td ltx_align_right" id="S3.T1.2.2.3">19 471</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.3.3.1">En<math alttext="\to" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mo id="S3.T1.3.3.1.m1.1.1" stretchy="false" xref="S3.T1.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><ci id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">→</annotation></semantics></math>Fr</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.3.3.2">18 321</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.3.3.3">12 212</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of distinct source or target patterns found in the analysis set (1M sentences from WMT).</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Models</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">We train a bilingual Transformer base model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="#bib.bib63" title="">2017</a>)</cite>
for each language pair using the <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px3.p1.1.1">T5X</span> framework <cite class="ltx_cite ltx_citemacro_cite">Roberts et al. (<a class="ltx_ref" href="#bib.bib50" title="">2022</a>)</cite>.
All models are trained
with <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px3.p1.1.2">Adafactor</span> optimizer <cite class="ltx_cite ltx_citemacro_cite">Shazeer and Stern (<a class="ltx_ref" href="#bib.bib54" title="">2018</a>)</cite>
for 2M steps with 0.1 dropout rate, 1024 batch size, and 0.1 label smoothing.
We use an inverse square root learning rate schedule with a base rate of 2.0.
As summarized in Table <a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ Models ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">2</span></a> part (i), all models achieve similar BLEU
scores<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>All reported BLEU scores for our models are obtained through <span class="ltx_text ltx_font_typewriter" id="footnote3.1">SacreBLEU</span> <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="#bib.bib47" title="">2018</a>)</cite>.</span></span></span>
on the development set as reported in the literature with a comparable setup.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_1 ltx_align_middle" id="S3.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="4" id="S3.T2.3.4.1.1">
<span class="ltx_text" id="S3.T2.3.4.1.1.1" style="font-size:90%;">(i) </span><span class="ltx_text ltx_font_smallcaps" id="S3.T2.3.4.1.1.2" style="font-size:90%;">Translation</span><span class="ltx_text" id="S3.T2.3.4.1.1.3" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T2.3.5.2.1"><span class="ltx_text" id="S3.T2.3.5.2.1.1" style="font-size:90%;">Target</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T2.3.5.2.2"><span class="ltx_text" id="S3.T2.3.5.2.2.1" style="font-size:90%;">Dev dataset</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="S3.T2.3.5.2.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.3.5.2.3.1"><span class="ltx_text" id="S3.T2.3.5.2.3.1.1" style="font-size:90%;">BLEU</span></p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="S3.T2.3.5.2.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.3.5.2.4.1"><span class="ltx_text" id="S3.T2.3.5.2.4.1.1" style="font-size:90%;">Reported</span></p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.2"><span class="ltx_text" id="S3.T2.1.1.2.1" style="font-size:90%;">Fr</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.3"><span class="ltx_text" id="S3.T2.1.1.3.1" style="font-size:90%;">newstest2014</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.1.1.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.1.1.4.1"><span class="ltx_text" id="S3.T2.1.1.4.1.1" style="font-size:90%;">39.9</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.1.1.1">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.1.1.1.1.1"><span class="ltx_text" id="S3.T2.1.1.1.1.1.2" style="font-size:90%;">38.1</span><span class="ltx_text" id="S3.T2.1.1.1.1.1.1" style="font-size:90%;width:0.0pt;"><sup class="ltx_sup" id="S3.T2.1.1.1.1.1.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.1.1.1.m1.1d">†</annotation></semantics></math></sup></span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.2.2"><span class="ltx_text" id="S3.T2.2.2.2.1" style="font-size:90%;">De</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.2.3"><span class="ltx_text" id="S3.T2.2.2.3.1" style="font-size:90%;">newstest2018</span></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.2.2.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.2.2.4.1"><span class="ltx_text" id="S3.T2.2.2.4.1.1" style="font-size:90%;">46.3</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.2.2.1">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.2.2.1.1.1"><span class="ltx_text" id="S3.T2.2.2.1.1.1.2" style="font-size:90%;">46.4</span><span class="ltx_text" id="S3.T2.2.2.1.1.1.1" style="font-size:90%;width:0.0pt;"><sup class="ltx_sup" id="S3.T2.2.2.1.1.1.1.1"><math alttext="\ddagger" class="ltx_Math" display="inline" id="S3.T2.2.2.1.1.1.1.1.m1.1"><semantics id="S3.T2.2.2.1.1.1.1.1.m1.1a"><mo id="S3.T2.2.2.1.1.1.1.1.m1.1.1" xref="S3.T2.2.2.1.1.1.1.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.1.1.1.1.m1.1b"><ci id="S3.T2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.1.1.1.1.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.1.1.1.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.1.1.1.1.1.m1.1d">‡</annotation></semantics></math></sup></span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.3.2"><span class="ltx_text" id="S3.T2.3.3.2.1" style="font-size:90%;">Zh</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.3.3"><span class="ltx_text" id="S3.T2.3.3.3.1" style="font-size:90%;">newstest2018</span></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.3.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.3.3.4.1"><span class="ltx_text" id="S3.T2.3.3.4.1.1" style="font-size:90%;">34.4</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.3.1">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.3.3.1.1.1"><span class="ltx_text" id="S3.T2.3.3.1.1.1.2" style="font-size:90%;">34.8</span><span class="ltx_text" id="S3.T2.3.3.1.1.1.1" style="font-size:90%;width:0.0pt;"><sup class="ltx_sup" id="S3.T2.3.3.1.1.1.1.1"><math alttext="\dagger\dagger" class="ltx_Math" display="inline" id="S3.T2.3.3.1.1.1.1.1.m1.2"><semantics id="S3.T2.3.3.1.1.1.1.1.m1.2a"><mrow id="S3.T2.3.3.1.1.1.1.1.m1.2.3.2" xref="S3.T2.3.3.1.1.1.1.1.m1.2.3.1.cmml"><mo id="S3.T2.3.3.1.1.1.1.1.m1.1.1" xref="S3.T2.3.3.1.1.1.1.1.m1.1.1.cmml">†</mo><mo id="S3.T2.3.3.1.1.1.1.1.m1.2.3.2.1" lspace="0em" xref="S3.T2.3.3.1.1.1.1.1.m1.2.3.1.cmml">⁣</mo><mo id="S3.T2.3.3.1.1.1.1.1.m1.2.2" xref="S3.T2.3.3.1.1.1.1.1.m1.2.2.cmml">†</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.1.1.1.1.m1.2b"><list id="S3.T2.3.3.1.1.1.1.1.m1.2.3.1.cmml" xref="S3.T2.3.3.1.1.1.1.1.m1.2.3.2"><ci id="S3.T2.3.3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.3.3.1.1.1.1.1.m1.1.1">†</ci><ci id="S3.T2.3.3.1.1.1.1.1.m1.2.2.cmml" xref="S3.T2.3.3.1.1.1.1.1.m1.2.2">†</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.1.1.1.1.m1.2c">\dagger\dagger</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.1.1.1.1.1.m1.2d">† †</annotation></semantics></math></sup></span></p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_1 ltx_align_middle" id="S3.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="5" id="S3.T2.6.1.1.1">
<span class="ltx_text" id="S3.T2.6.1.1.1.1" style="font-size:90%;">(ii) </span><span class="ltx_text ltx_font_smallcaps" id="S3.T2.6.1.1.1.2" style="font-size:90%;">Dependency Parsing</span><span class="ltx_text" id="S3.T2.6.1.1.1.3" style="font-size:90%;"></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.6.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.6.2.1.1"><span class="ltx_text" id="S3.T2.6.2.1.1.1" style="font-size:90%;">Language</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.6.2.1.2"><span class="ltx_text" id="S3.T2.6.2.1.2.1" style="font-size:90%;">Dataset</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.2.1.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.2.1.3.1"><span class="ltx_text" id="S3.T2.6.2.1.3.1.1" style="font-size:90%;">UPOS</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.2.1.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.2.1.4.1"><span class="ltx_text" id="S3.T2.6.2.1.4.1.1" style="font-size:90%;">UAS</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.2.1.5">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.2.1.5.1"><span class="ltx_text" id="S3.T2.6.2.1.5.1.1" style="font-size:90%;">LAS</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.6.3.2.1"><span class="ltx_text" id="S3.T2.6.3.2.1.1" style="font-size:90%;">En</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.6.3.2.2"><span class="ltx_text" id="S3.T2.6.3.2.2.1" style="font-size:90%;">EWT</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.3.2.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.3.2.3.1"><span class="ltx_text" id="S3.T2.6.3.2.3.1.1" style="font-size:90%;">95.56</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.3.2.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.3.2.4.1"><span class="ltx_text" id="S3.T2.6.3.2.4.1.1" style="font-size:90%;">89.55</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.6.3.2.5">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.3.2.5.1"><span class="ltx_text" id="S3.T2.6.3.2.5.1.1" style="font-size:90%;">96.67</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.4.3.1"><span class="ltx_text" id="S3.T2.6.4.3.1.1" style="font-size:90%;">Fr</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.4.3.2"><span class="ltx_text" id="S3.T2.6.4.3.2.1" style="font-size:90%;">GSD</span></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.4.3.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.4.3.3.1"><span class="ltx_text" id="S3.T2.6.4.3.3.1.1" style="font-size:90%;">97.77</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.4.3.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.4.3.4.1"><span class="ltx_text" id="S3.T2.6.4.3.4.1.1" style="font-size:90%;">93.20</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.4.3.5">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.4.3.5.1"><span class="ltx_text" id="S3.T2.6.4.3.5.1.1" style="font-size:90%;">90.90</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.5.4.1"><span class="ltx_text" id="S3.T2.6.5.4.1.1" style="font-size:90%;">De</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.5.4.2"><span class="ltx_text" id="S3.T2.6.5.4.2.1" style="font-size:90%;">GSD</span></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.5.4.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.5.4.3.1"><span class="ltx_text" id="S3.T2.6.5.4.3.1.1" style="font-size:90%;">94.80</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.5.4.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.5.4.4.1"><span class="ltx_text" id="S3.T2.6.5.4.4.1.1" style="font-size:90%;">87.87</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.5.4.5">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.5.4.5.1"><span class="ltx_text" id="S3.T2.6.5.4.5.1.1" style="font-size:90%;">83.25</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.6.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.6.5.1"><span class="ltx_text" id="S3.T2.6.6.5.1.1" style="font-size:90%;">Zh</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.6.6.5.2"><span class="ltx_text" id="S3.T2.6.6.5.2.1" style="font-size:90%;">GSD</span></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.6.5.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.6.5.3.1"><span class="ltx_text" id="S3.T2.6.6.5.3.1.1" style="font-size:90%;">94.58</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.6.5.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.6.5.4.1"><span class="ltx_text" id="S3.T2.6.6.5.4.1.1" style="font-size:90%;">86.41</span></p>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T2.6.6.5.5">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.6.6.5.5.1"><span class="ltx_text" id="S3.T2.6.6.5.5.1.1" style="font-size:90%;">80.70</span></p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_1 ltx_align_middle" id="S3.T2.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4" id="S3.T2.7.1.1.1">
<span class="ltx_text" id="S3.T2.7.1.1.1.1" style="font-size:90%;">(iii) </span><span class="ltx_text ltx_font_smallcaps" id="S3.T2.7.1.1.1.2" style="font-size:90%;">Word Alignment</span><span class="ltx_text" id="S3.T2.7.1.1.1.3" style="font-size:90%;"></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.7.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.7.2.1.1"><span class="ltx_text" id="S3.T2.7.2.1.1.1" style="font-size:90%;">Target</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.2.1.2">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.2.1.2.1"><span class="ltx_text" id="S3.T2.7.2.1.2.1.1" style="font-size:90%;">Precision</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.2.1.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.2.1.3.1"><span class="ltx_text" id="S3.T2.7.2.1.3.1.1" style="font-size:90%;">Recall</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.2.1.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.2.1.4.1"><span class="ltx_text" id="S3.T2.7.2.1.4.1.1" style="font-size:90%;">F1</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.7.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.7.3.2.1"><span class="ltx_text" id="S3.T2.7.3.2.1.1" style="font-size:90%;">Fr</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.3.2.2">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.3.2.2.1"><span class="ltx_text" id="S3.T2.7.3.2.2.1.1" style="font-size:90%;">85.6</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.3.2.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.3.2.3.1"><span class="ltx_text" id="S3.T2.7.3.2.3.1.1" style="font-size:90%;">81.7</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.7.3.2.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.3.2.4.1"><span class="ltx_text" id="S3.T2.7.3.2.4.1.1" style="font-size:90%;">83.6</span></p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.7.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.7.4.3.1"><span class="ltx_text" id="S3.T2.7.4.3.1.1" style="font-size:90%;">Zh</span></th>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S3.T2.7.4.3.2">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.4.3.2.1"><span class="ltx_text" id="S3.T2.7.4.3.2.1.1" style="font-size:90%;">85.5</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S3.T2.7.4.3.3">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.4.3.3.1"><span class="ltx_text" id="S3.T2.7.4.3.3.1.1" style="font-size:90%;">81.9</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S3.T2.7.4.3.4">
<p class="ltx_p ltx_align_center ltx_align_top" id="S3.T2.7.4.3.4.1"><span class="ltx_text" id="S3.T2.7.4.3.4.1.1" style="font-size:90%;">83.7</span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance for (i) MT (ii) dependency parsing and
(iii) word alignment. No human annotations for En<math alttext="\to" class="ltx_Math" display="inline" id="S3.T2.5.m1.1"><semantics id="S3.T2.5.m1.1b"><mo id="S3.T2.5.m1.1.1" stretchy="false" xref="S3.T2.5.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.m1.1c"><ci id="S3.T2.5.m1.1.1.cmml" xref="S3.T2.5.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.m1.1e">→</annotation></semantics></math>De are provided by <cite class="ltx_cite ltx_citemacro_citet">Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Annotations</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">We rely on two automatic tools to conduct a large-scale
analysis: a dependency parser and a word aligner.
More specifically, the dependency parser is an implementation of <cite class="ltx_cite ltx_citemacro_citet">Dozat and Manning (<a class="ltx_ref" href="#bib.bib17" title="">2017</a>)</cite> based on mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="#bib.bib13" title="">2019</a>)</cite>.
The neural word aligner is based on AMBER <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>
and fine-tuned on human-annotated alignments.
We follow <cite class="ltx_cite ltx_citemacro_citet">Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>
to keep the content words<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Content words are words with semantic content, used in various “contentful” positions such as subjects, objects and adjectival modifiers. We identify
content words by matching their parent dependencies against a manually selected set,
as defined in footnote 10 of the original paper <cite class="ltx_cite ltx_citemacro_citep">(Nikolaev et al., <a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>.
This criterion kept around 40%-50% of all the tokens for all three language pairs in our experiments. Please see Appendix <a class="ltx_ref" href="#A2" title="Appendix B Percentage of Content Words and Their Alignments ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">B</span></a> for a more detailed analysis.
</span></span></span> and their dependencies and alignments only,
and focus on one-to-one alignments unless otherwise noted.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p2.1">As reported in Table <a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ Models ‣ 3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">2</span></a> part (ii) and (iii),
we validate that both tools have high accuracy on public datasets:
UD test sets for parsing and human-annotated PUD datasets <cite class="ltx_cite ltx_citemacro_cite">Nikolaev et al. (<a class="ltx_ref" href="#bib.bib43" title="">2020</a>)</cite>
for word alignment.
We will release the automatic annotations to the public.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Comparative Analysis of MT vs HT</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.3">We proceed to conduct a comparative analysis of MT vs HT based on the fine-grained morphosyntactic patterns defined in the previous section.
For any given source pattern <math alttext="p" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_p</annotation></semantics></math> according to the word-based or arc-based definition as detailed in the previous section, we study the distribution of its aligned target patterns, i.e.,
<math alttext="\Pr_{\text{HT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1b"><msub id="S4.p1.2.m2.1.1"><mi id="S4.p1.2.m2.1.1.2">Pr</mi><mtext id="S4.p1.2.m2.1.1.3">HT</mtext></msub><mrow id="S4.p1.2.m2.1.2"><mo id="S4.p1.2.m2.1.2.1" stretchy="false">(</mo><mo id="S4.p1.2.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.p1.2.m2.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.p1.2.m2.1.2.4">p</mi><mo id="S4.p1.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\Pr_{\text{HT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">roman_Pr start_POSTSUBSCRIPT HT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> and <math alttext="\Pr_{\text{MT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mrow id="S4.p1.3.m3.1b"><msub id="S4.p1.3.m3.1.1"><mi id="S4.p1.3.m3.1.1.2">Pr</mi><mtext id="S4.p1.3.m3.1.1.3">MT</mtext></msub><mrow id="S4.p1.3.m3.1.2"><mo id="S4.p1.3.m3.1.2.1" stretchy="false">(</mo><mo id="S4.p1.3.m3.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.p1.3.m3.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.p1.3.m3.1.2.4">p</mi><mo id="S4.p1.3.m3.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\Pr_{\text{MT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">roman_Pr start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math>, along two major dimensions:
diversity/uncertainty as measured by entropy of the target pattern, and convergence/divergence rate. Figure <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows that there is considerable
variance in how the most frequent source patterns in HT are distributed along these two axes, and that each
dimension captures a different property of the distribution.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Through analyses on both the aggregate level and the individual pattern level, we conclude that MT is more <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">conservative</em> than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments.
We also observe that MT tends to be less similar to HT for the less frequent source patterns.
The analyses in this section are based on the held-out subset consisting of one million sentence pairs. We refer readers to Appendix <a class="ltx_ref" href="#A1" title="Appendix A Analysis Subset Filtered Using LaBSE Embeddings ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">A</span></a> for similar results on a subset that is further filtered using LaBSE crosslingual embeddings <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite> with a remarkably similar trend, which we include to show that it does not change our conclusions when we test on data that has been filtered to improve its cross-lingual equivalence.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="527" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Plot of convergence rate vs entropy for the most frequent word-based source patterns in En<math alttext="\to" class="ltx_Math" display="inline" id="S4.F3.2.m1.1"><semantics id="S4.F3.2.m1.1b"><mo id="S4.F3.2.m1.1.1" stretchy="false" xref="S4.F3.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F3.2.m1.1c"><ci id="S4.F3.2.m1.1.1.cmml" xref="S4.F3.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S4.F3.2.m1.1e">→</annotation></semantics></math>Fr human translations, three of which are highlighted in black:
(1) <span class="ltx_text ltx_font_typewriter" id="S4.F3.11.1">amod~ADJ~leaf</span> (high convergence rate, low entropy): the most common cases of adjectival modifiers; (2) <span class="ltx_text ltx_font_typewriter" id="S4.F3.12.2">acl~VERB~nsubj</span> (low convergence rate, high entropy): object relative clauses without a relative pronoun, or subject relative clauses. The high entropy reflects a major difference between English and French, where the relative pronoun <span class="ltx_text ltx_font_italic" id="S4.F3.13.3">que</span> is obligatory in French but not in English.
(3) <span class="ltx_text ltx_font_typewriter" id="S4.F3.14.4">amod~PROPN~leaf</span> (low convergence rate, low entropy): adjectives as part of a proper nouns. Adjectives in official institutions and titles are typically capitalized and annotated as <span class="ltx_text ltx_font_typewriter" id="S4.F3.15.5">PROPN</span> in English (e.g., <span class="ltx_text ltx_font_italic" id="S4.F3.16.6">Secretary General</span>) but lowercased and annotated as <span class="ltx_text ltx_font_typewriter" id="S4.F3.17.7">ADJ</span> in French (e.g., <span class="ltx_text ltx_font_italic" id="S4.F3.18.8">secrétaire général</span>).
</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>MT is Less Morphosyntactically Diverse Than HT</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.2">We define diversity score as the conditional entropy of target patterns given
source patterns, which reflects the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.2.1">aggregate</em> level of uncertainty
when translating a morphosyntactic pattern.
More formally, let <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">italic_P</annotation></semantics></math> and <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.2.m2.1d">italic_Q</annotation></semantics></math> denote the categorical random variables
for source patterns and their aligned target patterns, respectively.
The aggregate diversity score is defined as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx1">
<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle H(Q\mid P)=\sum_{p}\Pr(p)\cdot H(Q\mid P=p)," class="ltx_Math" display="inline" id="S4.E1.m1.3"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.3.cmml">H</mi><mo id="S4.E1.m1.3.3.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml">Q</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml">∣</mo><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml">P</mi></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.3" xref="S4.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S4.E1.m1.3.3.1.1.2" xref="S4.E1.m1.3.3.1.1.2.cmml"><mstyle displaystyle="true" id="S4.E1.m1.3.3.1.1.2.2" xref="S4.E1.m1.3.3.1.1.2.2.cmml"><munder id="S4.E1.m1.3.3.1.1.2.2a" xref="S4.E1.m1.3.3.1.1.2.2.cmml"><mo id="S4.E1.m1.3.3.1.1.2.2.2" movablelimits="false" xref="S4.E1.m1.3.3.1.1.2.2.2.cmml">∑</mo><mi id="S4.E1.m1.3.3.1.1.2.2.3" xref="S4.E1.m1.3.3.1.1.2.2.3.cmml">p</mi></munder></mstyle><mrow id="S4.E1.m1.3.3.1.1.2.1" xref="S4.E1.m1.3.3.1.1.2.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.2.1.3" xref="S4.E1.m1.3.3.1.1.2.1.3.cmml"><mrow id="S4.E1.m1.3.3.1.1.2.1.3.2.2" xref="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml"><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">Pr</mi><mo id="S4.E1.m1.3.3.1.1.2.1.3.2.2a" xref="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml">⁡</mo><mrow id="S4.E1.m1.3.3.1.1.2.1.3.2.2.1" xref="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml"><mo id="S4.E1.m1.3.3.1.1.2.1.3.2.2.1.1" stretchy="false" xref="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml">(</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">p</mi><mo id="S4.E1.m1.3.3.1.1.2.1.3.2.2.1.2" rspace="0.055em" stretchy="false" xref="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.2.1.3.1" rspace="0.222em" xref="S4.E1.m1.3.3.1.1.2.1.3.1.cmml">⋅</mo><mi id="S4.E1.m1.3.3.1.1.2.1.3.3" xref="S4.E1.m1.3.3.1.1.2.1.3.3.cmml">H</mi></mrow><mo id="S4.E1.m1.3.3.1.1.2.1.2" xref="S4.E1.m1.3.3.1.1.2.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.2.1.1.1" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.cmml"><mo id="S4.E1.m1.3.3.1.1.2.1.1.1.2" stretchy="false" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.1.1.2.1.1.1.1" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.cmml"><mi id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.2" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.2.cmml">Q</mi><mo id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.1" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.1.cmml">∣</mo><mi id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.3" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S4.E1.m1.3.3.1.1.2.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.1.cmml">=</mo><mi id="S4.E1.m1.3.3.1.1.2.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml">p</mi></mrow><mo id="S4.E1.m1.3.3.1.1.2.1.1.1.3" stretchy="false" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1"><eq id="S4.E1.m1.3.3.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.3"></eq><apply id="S4.E1.m1.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><times id="S4.E1.m1.3.3.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.2"></times><ci id="S4.E1.m1.3.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3">𝐻</ci><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2">𝑄</ci><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3">𝑃</ci></apply></apply><apply id="S4.E1.m1.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2"><apply id="S4.E1.m1.3.3.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.2.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2.2">subscript</csymbol><sum id="S4.E1.m1.3.3.1.1.2.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2.2.2"></sum><ci id="S4.E1.m1.3.3.1.1.2.2.3.cmml" xref="S4.E1.m1.3.3.1.1.2.2.3">𝑝</ci></apply><apply id="S4.E1.m1.3.3.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1"><times id="S4.E1.m1.3.3.1.1.2.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2.1.2"></times><apply id="S4.E1.m1.3.3.1.1.2.1.3.cmml" xref="S4.E1.m1.3.3.1.1.2.1.3"><ci id="S4.E1.m1.3.3.1.1.2.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.3.1">⋅</ci><apply id="S4.E1.m1.3.3.1.1.2.1.3.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.3.2.2"><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">Pr</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑝</ci></apply><ci id="S4.E1.m1.3.3.1.1.2.1.3.3.cmml" xref="S4.E1.m1.3.3.1.1.2.1.3.3">𝐻</ci></apply><apply id="S4.E1.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1"><eq id="S4.E1.m1.3.3.1.1.2.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.1"></eq><apply id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.1">conditional</csymbol><ci id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.2">𝑄</ci><ci id="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.2.3">𝑃</ci></apply><ci id="S4.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.2.1.1.1.1.3">𝑝</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\displaystyle H(Q\mid P)=\sum_{p}\Pr(p)\cdot H(Q\mid P=p),</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.3d">italic_H ( italic_Q ∣ italic_P ) = ∑ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT roman_Pr ( italic_p ) ⋅ italic_H ( italic_Q ∣ italic_P = italic_p ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.3">where <math alttext="p" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.3.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.3.m1.1a"><mi id="S4.SS1.SSS0.Px1.p1.3.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m1.1b"><ci id="S4.SS1.SSS0.Px1.p1.3.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.3.m1.1d">italic_p</annotation></semantics></math> is any specific source pattern that occurs in the corpus.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.3">In addition, for any given source pattern <math alttext="p" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p2.1.m1.1a"><mi id="S4.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p2.1.m1.1b"><ci id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p2.1.m1.1d">italic_p</annotation></semantics></math>,
we define a <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px1.p2.3.1">source pattern-specific</em> diversity score
as the entropy of the target patterns aligned to that source pattern <math alttext="p" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p2.2.m2.1a"><mi id="S4.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p2.2.m2.1b"><ci id="S4.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p2.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p2.2.m2.1d">italic_p</annotation></semantics></math>. This score corresponds to the term <math alttext="H(Q\mid P=p)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.3.m3.1"><semantics id="S4.SS1.SSS0.Px1.p2.3.m3.1a"><mrow id="S4.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.3" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml">H</mi><mo id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.2" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml"><mo id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.2" stretchy="false" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml"><mrow id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.2" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.2.cmml">Q</mi><mo id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.1" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.1.cmml">∣</mo><mi id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.3" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.1" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.1.cmml">=</mo><mi id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.3" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.3.cmml">p</mi></mrow><mo id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.3" stretchy="false" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p2.3.m3.1b"><apply id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1"><times id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.2"></times><ci id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.3">𝐻</ci><apply id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1"><eq id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.1">conditional</csymbol><ci id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.2">𝑄</ci><ci id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.2.3">𝑃</ci></apply><ci id="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p2.3.m3.1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p2.3.m3.1c">H(Q\mid P=p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p2.3.m3.1d">italic_H ( italic_Q ∣ italic_P = italic_p )</annotation></semantics></math> in Equation (<a class="ltx_ref" href="#S4.E1" title="1 ‣ Preliminaries ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">1</span></a>).
</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Aggregate Finding</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.5">As summarized in Table <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ Aggregate Finding ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a> part (i), MT is less morphosyntactically diverse than
HT in aggregate, across three language pairs and two types of divergence.
The relative reduction in diversity for MT compared to HT ranges from 5.9%
for En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>Zh (2.77 vs 2.95 with word-based patterns) to 22.2%
for En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" stretchy="false" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.2.m2.1d">→</annotation></semantics></math>Fr (1.75 vs 2.24 with arc-based patterns).
Interestingly, En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS1.SSS0.Px2.p1.3.m3.1a"><mo id="S4.SS1.SSS0.Px2.p1.3.m3.1.1" stretchy="false" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.3.m3.1b"><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.3.m3.1d">→</annotation></semantics></math>Zh has noticeably higher diversity scores than
En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS1.SSS0.Px2.p1.4.m4.1a"><mo id="S4.SS1.SSS0.Px2.p1.4.m4.1.1" stretchy="false" xref="S4.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.4.m4.1b"><ci id="S4.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.4.m4.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.4.m4.1d">→</annotation></semantics></math>Fr and En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS1.SSS0.Px2.p1.5.m5.1a"><mo id="S4.SS1.SSS0.Px2.p1.5.m5.1.1" stretchy="false" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.5.m5.1b"><ci id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.5.m5.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p1.5.m5.1d">→</annotation></semantics></math>De but lower overall reduction.
This may be attributed to the larger linguistic difference between Chinese and English.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Note that, however, our setup is
not entirely comparable across language pairs
since the data is not multi-way parallel.</span></span></span>
</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.2.3.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.2.3.1.1.1" style="font-size:90%;">Target</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.2.3.1.2"><span class="ltx_text" id="S4.T3.2.3.1.2.1" style="font-size:90%;">Word-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.2.3.1.3"><span class="ltx_text" id="S4.T3.2.3.1.3.1" style="font-size:90%;">Arc-based</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.2.3"><span class="ltx_text" id="S4.T3.2.2.3.1" style="font-size:90%;">HT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.2.4"><span class="ltx_text" id="S4.T3.2.2.4.1" style="font-size:90%;">MT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.1.1.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:90%;">%</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.2.5"><span class="ltx_text" id="S4.T3.2.2.5.1" style="font-size:90%;">HT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.2.6"><span class="ltx_text" id="S4.T3.2.2.6.1" style="font-size:90%;">MT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.2.2">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mi id="S4.T3.2.2.2.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.2.2.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:90%;">%</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S4.T3.2.4.2.1">
<span class="ltx_text" id="S4.T3.2.4.2.1.1" style="font-size:90%;">(i) </span><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.4.2.1.2" style="font-size:90%;">Diversity</span><span class="ltx_text" id="S4.T3.2.4.2.1.3" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.5.3.1"><span class="ltx_text" id="S4.T3.2.5.3.1.1" style="font-size:90%;">Fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.2"><span class="ltx_text" id="S4.T3.2.5.3.2.1" style="font-size:90%;">2.23</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.3"><span class="ltx_text" id="S4.T3.2.5.3.3.1" style="font-size:90%;">1.84</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.4">
<span class="ltx_text" id="S4.T3.2.5.3.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.5.3.4.2" style="font-size:90%;">17.6</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.5"><span class="ltx_text" id="S4.T3.2.5.3.5.1" style="font-size:90%;">2.24</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.6"><span class="ltx_text" id="S4.T3.2.5.3.6.1" style="font-size:90%;">1.75</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.5.3.7">
<span class="ltx_text" id="S4.T3.2.5.3.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.5.3.7.2" style="font-size:90%;">22.2</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.4">
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.1"><span class="ltx_text" id="S4.T3.2.6.4.1.1" style="font-size:90%;">De</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.2"><span class="ltx_text" id="S4.T3.2.6.4.2.1" style="font-size:90%;">2.23</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.3"><span class="ltx_text" id="S4.T3.2.6.4.3.1" style="font-size:90%;">1.90</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.4">
<span class="ltx_text" id="S4.T3.2.6.4.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.6.4.4.2" style="font-size:90%;">15.0</span>
</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.5"><span class="ltx_text" id="S4.T3.2.6.4.5.1" style="font-size:90%;">2.38</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.6"><span class="ltx_text" id="S4.T3.2.6.4.6.1" style="font-size:90%;">1.96</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.6.4.7">
<span class="ltx_text" id="S4.T3.2.6.4.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.6.4.7.2" style="font-size:90%;">17.8</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.5">
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.1"><span class="ltx_text" id="S4.T3.2.7.5.1.1" style="font-size:90%;">Zh</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.2"><span class="ltx_text" id="S4.T3.2.7.5.2.1" style="font-size:90%;">2.95</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.3"><span class="ltx_text" id="S4.T3.2.7.5.3.1" style="font-size:90%;">2.77</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.4">
<span class="ltx_text" id="S4.T3.2.7.5.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.7.5.4.2" style="font-size:90%;">5.9</span>
</td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.5"><span class="ltx_text" id="S4.T3.2.7.5.5.1" style="font-size:90%;">3.79</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.6"><span class="ltx_text" id="S4.T3.2.7.5.6.1" style="font-size:90%;">3.46</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.7.5.7">
<span class="ltx_text" id="S4.T3.2.7.5.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="S4.T3.2.7.5.7.2" style="font-size:90%;">8.6</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S4.T3.2.8.6.1">
<span class="ltx_text" id="S4.T3.2.8.6.1.1" style="font-size:90%;">(ii) </span><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.8.6.1.2" style="font-size:90%;">Convergence Rate</span><span class="ltx_text" id="S4.T3.2.8.6.1.3" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.9.7.1"><span class="ltx_text" id="S4.T3.2.9.7.1.1" style="font-size:90%;">Fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.2"><span class="ltx_text" id="S4.T3.2.9.7.2.1" style="font-size:90%;">37.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.3"><span class="ltx_text" id="S4.T3.2.9.7.3.1" style="font-size:90%;">44.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.4"><span class="ltx_text" id="S4.T3.2.9.7.4.1" style="font-size:90%;">18.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.5"><span class="ltx_text" id="S4.T3.2.9.7.5.1" style="font-size:90%;">46.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.6"><span class="ltx_text" id="S4.T3.2.9.7.6.1" style="font-size:90%;">53.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.2.9.7.7"><span class="ltx_text" id="S4.T3.2.9.7.7.1" style="font-size:90%;">15.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.8">
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.1"><span class="ltx_text" id="S4.T3.2.10.8.1.1" style="font-size:90%;">De</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.2"><span class="ltx_text" id="S4.T3.2.10.8.2.1" style="font-size:90%;">45.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.3"><span class="ltx_text" id="S4.T3.2.10.8.3.1" style="font-size:90%;">51.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.4"><span class="ltx_text" id="S4.T3.2.10.8.4.1" style="font-size:90%;">13.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.5"><span class="ltx_text" id="S4.T3.2.10.8.5.1" style="font-size:90%;">51.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.6"><span class="ltx_text" id="S4.T3.2.10.8.6.1" style="font-size:90%;">57.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.2.10.8.7"><span class="ltx_text" id="S4.T3.2.10.8.7.1" style="font-size:90%;">11.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.2.11.9.1"><span class="ltx_text" id="S4.T3.2.11.9.1.1" style="font-size:90%;">Zh</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.2"><span class="ltx_text" id="S4.T3.2.11.9.2.1" style="font-size:90%;">21.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.3"><span class="ltx_text" id="S4.T3.2.11.9.3.1" style="font-size:90%;">22.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.4"><span class="ltx_text" id="S4.T3.2.11.9.4.1" style="font-size:90%;">6.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.5"><span class="ltx_text" id="S4.T3.2.11.9.5.1" style="font-size:90%;">23.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.6"><span class="ltx_text" id="S4.T3.2.11.9.6.1" style="font-size:90%;">25.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.2.11.9.7"><span class="ltx_text" id="S4.T3.2.11.9.7.1" style="font-size:90%;">7.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Aggregate diversity scores and convergence rates. The <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T3.4.m1.1"><semantics id="S4.T3.4.m1.1b"><mi id="S4.T3.4.m1.1.1" mathvariant="normal" xref="S4.T3.4.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.4.m1.1c"><ci id="S4.T3.4.m1.1.1.cmml" xref="S4.T3.4.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.m1.1e">roman_Δ</annotation></semantics></math>% columns show the relative change in percentage from HT to MT.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="770" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Stacked histogram of the relative differences in source pattern-specific diversity score.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Finding by Source Pattern</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">On the level of individual source patterns, we observe that the reduction of diversity
among their aligned target patterns is <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.1">across-the-board</em> but
<em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.2">unevenly distributed</em>.
Figure <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ Aggregate Finding ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">4</span></a> plots a stacked histogram of the relative differences in diversity score (MT vs HT)
for the most frequent source patterns with at least 1000 occurrences, and it shows that the vast majority of them see a drop of diversity (i.e., negative difference).
This reduction varies from pattern to pattern,
ranging from 0% to 60%.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>MT is More Convergent Than HT</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We tally divergences and convergences according to the two types detailed in
Section <a class="ltx_ref" href="#S3" title="3 Experimental Setup ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a>.
We then define the convergence or divergence rate as the percentage of convergent or divergent patterns out of all translation patterns.
Similar to diversity, we can compute convergence/divergence rates for both the entire corpus in aggregate and individual source patterns.
For the latter case, we tally all the aligned target patterns for a specific source pattern and calculate the rates accordingly.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Aggregate Finding</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.5">As summarized in Table <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ Aggregate Finding ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a> part (ii), we observe a consistent increase
of convergence rate for all three language pairs and two types of
divergence.
This increase is most pronounced for En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>Fr and En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px2.p1.2.m2.1.1" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.2.m2.1d">→</annotation></semantics></math>De,
whereas En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px2.p1.3.m3.1a"><mo id="S4.SS2.SSS0.Px2.p1.3.m3.1.1" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.3.m3.1b"><ci id="S4.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.3.m3.1d">→</annotation></semantics></math>Zh has a less noticeable although still consistent
increase and starts with a much lower convergence rate for HT:
the highest rate for En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS2.SSS0.Px2.p1.4.m4.1a"><mo id="S4.SS2.SSS0.Px2.p1.4.m4.1.1" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S4.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.4.m4.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.4.m4.1d">→</annotation></semantics></math>Zh is 23.4%, whereas En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS2.SSS0.Px2.p1.5.m5.1a"><mo id="S4.SS2.SSS0.Px2.p1.5.m5.1.1" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.5.m5.1b"><ci id="S4.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.5.m5.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.5.m5.1d">→</annotation></semantics></math>De can reach 57.8%.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Finding by Source Pattern</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.4">On a more granular level, we again notice a consistent increase of convergent patterns
for MT among the top source patterns (Figure <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ Finding by Source Pattern ‣ 4.2 MT is More Convergent Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">5</span></a>).
For the vast majority of top source patterns, MT has produced more convergent
translations than HT, and this discrepancy ranges from a negligible amount
(~0%) for most patterns to more than 20%.
This discrepancy is distributed differently for the three languages:
En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math>Fr and En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px3.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px3.p1.2.m2.1.1" stretchy="false" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.2.m2.1d">→</annotation></semantics></math>De have seen more patterns with increased convergence
rate while En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px3.p1.3.m3.1a"><mo id="S4.SS2.SSS0.Px3.p1.3.m3.1.1" stretchy="false" xref="S4.SS2.SSS0.Px3.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.3.m3.1b"><ci id="S4.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.3.m3.1d">→</annotation></semantics></math>Zh has most patterns barely changed and clustered around 0%.
As we later show in Figure <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‣ Beam Search is Biased Against Diversity and Divergence ‣ 5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">9</span></a>, this trend is unsurprising
given the much lower convergence rates for En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.4.m4.1"><semantics id="S4.SS2.SSS0.Px3.p1.4.m4.1a"><mo id="S4.SS2.SSS0.Px3.p1.4.m4.1.1" stretchy="false" xref="S4.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.4.m4.1b"><ci id="S4.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.4.m4.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.4.m4.1d">→</annotation></semantics></math>Zh in general.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="769" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Stacked histogram of the absolute differences in source pattern-specific convergence rate.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>MT Looks Less Like HT For Less Frequent Patterns</h3>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.4">Both diversity score and convergence rate are properties of translations produced by one system, <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.4.1">either</em> MT <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.4.2">or</em> HT.
To directly measure the distributional difference between MT and HT, we resort to Wasserstein distance (WD) between the two conditional distributions<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Recall that we treat both source patterns and target patterns as categorical random variables where every unique source or target pattern is treated as a distinct value that the random variables can take.</span></span></span> <math alttext="\Pr_{\text{MT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><msub id="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">Pr</mi><mtext id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3">MT</mtext></msub><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.2"><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.2.1" stretchy="false">(</mo><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.2.4">p</mi><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">\Pr_{\text{MT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.1.m1.1d">roman_Pr start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> and <math alttext="\Pr_{\text{HT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1b"><msub id="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2">Pr</mi><mtext id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3">HT</mtext></msub><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1.2"><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.2.1" stretchy="false">(</mo><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.2.4">p</mi><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.2.m2.1c">\Pr_{\text{HT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.2.m2.1d">roman_Pr start_POSTSUBSCRIPT HT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> using a unit cost matrix.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>
In which diagonal/off-diagonal entries are 0/1.</span></span></span>
This metric can be intuitively interpreted as the minimal amount of probability mass that has to be moved from <math alttext="\Pr_{\text{MT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS3.SSS0.Px1.p1.3.m3.1b"><msub id="S4.SS3.SSS0.Px1.p1.3.m3.1.1"><mi id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2">Pr</mi><mtext id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3">MT</mtext></msub><mrow id="S4.SS3.SSS0.Px1.p1.3.m3.1.2"><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.2.1" stretchy="false">(</mo><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.SS3.SSS0.Px1.p1.3.m3.1.2.4">p</mi><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.3.m3.1c">\Pr_{\text{MT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.3.m3.1d">roman_Pr start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> to match <math alttext="\Pr_{\text{HT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="S4.SS3.SSS0.Px1.p1.4.m4.1a"><mrow id="S4.SS3.SSS0.Px1.p1.4.m4.1b"><msub id="S4.SS3.SSS0.Px1.p1.4.m4.1.1"><mi id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2">Pr</mi><mtext id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3">HT</mtext></msub><mrow id="S4.SS3.SSS0.Px1.p1.4.m4.1.2"><mo id="S4.SS3.SSS0.Px1.p1.4.m4.1.2.1" stretchy="false">(</mo><mo id="S4.SS3.SSS0.Px1.p1.4.m4.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.SS3.SSS0.Px1.p1.4.m4.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.SS3.SSS0.Px1.p1.4.m4.1.2.4">p</mi><mo id="S4.SS3.SSS0.Px1.p1.4.m4.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.4.m4.1c">\Pr_{\text{HT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.4.m4.1d">roman_Pr start_POSTSUBSCRIPT HT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math>, with an upper
bound of 1 (i.e., sum of all probability mass).<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We note that other metrics such as KL-divergence can also be used to measure distributional difference, but we eventually chose WD for its interpretability.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Finding</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">As Figure <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ Finding ‣ 4.3 MT Looks Less Like HT For Less Frequent Patterns ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">6</span></a> shows,
there is a negative correlation between WD and the source pattern frequency:
MT matches HT more closely for the more frequent source patterns while having difficulty
in reproducing the HT distribution for the less frequent ones.
This trend persists for all tested settings, and points to a potential weakness of MT systems when it comes to learning the distributions of the less common structures.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="651" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Wasserstein distance with a unit cost matrix between <math alttext="\Pr_{\text{MT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.F6.4.m1.1"><semantics id="S4.F6.4.m1.1b"><mrow id="S4.F6.4.m1.1c"><msub id="S4.F6.4.m1.1.1"><mi id="S4.F6.4.m1.1.1.2">Pr</mi><mtext id="S4.F6.4.m1.1.1.3">MT</mtext></msub><mrow id="S4.F6.4.m1.1.2"><mo id="S4.F6.4.m1.1.2.1" stretchy="false">(</mo><mo id="S4.F6.4.m1.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.F6.4.m1.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.F6.4.m1.1.2.4">p</mi><mo id="S4.F6.4.m1.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.F6.4.m1.1d">\Pr_{\text{MT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.F6.4.m1.1e">roman_Pr start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> and <math alttext="\Pr_{\text{HT}}(\cdot\mid p)" class="ltx_math_unparsed" display="inline" id="S4.F6.5.m2.1"><semantics id="S4.F6.5.m2.1b"><mrow id="S4.F6.5.m2.1c"><msub id="S4.F6.5.m2.1.1"><mi id="S4.F6.5.m2.1.1.2">Pr</mi><mtext id="S4.F6.5.m2.1.1.3">HT</mtext></msub><mrow id="S4.F6.5.m2.1.2"><mo id="S4.F6.5.m2.1.2.1" stretchy="false">(</mo><mo id="S4.F6.5.m2.1.2.2" lspace="0em" rspace="0em">⋅</mo><mo id="S4.F6.5.m2.1.2.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S4.F6.5.m2.1.2.4">p</mi><mo id="S4.F6.5.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.F6.5.m2.1d">\Pr_{\text{HT}}(\cdot\mid p)</annotation><annotation encoding="application/x-llamapun" id="S4.F6.5.m2.1e">roman_Pr start_POSTSUBSCRIPT HT end_POSTSUBSCRIPT ( ⋅ ∣ italic_p )</annotation></semantics></math> for any given source pattern <math alttext="p" class="ltx_Math" display="inline" id="S4.F6.6.m3.1"><semantics id="S4.F6.6.m3.1b"><mi id="S4.F6.6.m3.1.1" xref="S4.F6.6.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.F6.6.m3.1c"><ci id="S4.F6.6.m3.1.1.cmml" xref="S4.F6.6.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.6.m3.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.F6.6.m3.1e">italic_p</annotation></semantics></math>.
Patterns are binned by frequency on a log scale, and both the means (lines) and the 95% confidence intervals (shaded areas) are shown.
The plot shows a negative correlation between WD and the source pattern frequency.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Beyond One-to-one Alignments</h3>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">One-to-one alignments constitute a majority of all detected alignments, but they
fail to account for translation patterns involving deletions and insertions.
To investigate the quantitative differences between HT and MT on those special
patterns, we conduct additional analyses on the distribution of all categories
of alignments based on the word-based definition.
Besides deletions (<span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS0.Px1.p1.1.1">src2null</span>) and
insertions (<span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS0.Px1.p1.1.2">null2tgt</span>), the remaining alignments
are collapsed into the <span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS0.Px1.p1.1.3">other</span> category (e.g., one-to-many mapping).</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="825" id="S4.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Distribution for all types of alignments. Percentages are defined
relative to the total number of source content words.
<span class="ltx_text ltx_font_typewriter" id="S4.F7.5.1">o2o</span>: one-to-one;
<span class="ltx_text ltx_font_typewriter" id="S4.F7.6.2">src2null</span>: deletions; <span class="ltx_text ltx_font_typewriter" id="S4.F7.7.3">null2tgt</span>: insertions; <span class="ltx_text ltx_font_typewriter" id="S4.F7.8.4">other</span>: other types such as
one-to-many.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Finding</h4>
<div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">Figure <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ Preliminaries ‣ 4.4 Beyond One-to-one Alignments ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes the distribution of all alignment categories,<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>The percentages are computed
in terms of source words. By definition, <span class="ltx_text ltx_font_typewriter" id="footnote9.1">src2null</span>, <span class="ltx_text ltx_font_typewriter" id="footnote9.2">o2o</span>
and <span class="ltx_text ltx_font_typewriter" id="footnote9.3">other</span> add up to 100%. Since <span class="ltx_text ltx_font_typewriter" id="footnote9.4">null2tgt</span> alignments do not have
aligned source words, their percentages indicate how many target content words are inserted
for each content word on the source side.</span></span></span>
which demonstrates a significant and consistent difference between HT and MT.
More specifically, MT produces fewer deletions (green), fewer insertions (red),
and more one-to-one translations (blue).
En<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS4.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS4.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S4.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>Fr again exhibits the biggest discrepancy with 9.6% less deletions (10.8% vs 20.4%)
and 14.8% less insertions (13.0% vs 26.8%), both around 50% relative reduction.
This trend contributes to the overall conservative nature of MT predictions, favoring one-to-one alignments at the expense of the other (more uncertain) categories.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Understanding the Discrepancy</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we seek to understand the source of
discrepancy between HT and MT as demonstrated in the previous section.
By investigating different decoding algorithms, we attribute this discrepancy
to the use of beam search, echoing the thesis laid out by
previous work <cite class="ltx_cite ltx_citemacro_cite">Edunov et al. (<a class="ltx_ref" href="#bib.bib18" title="">2018</a>); Eikema and Aziz (<a class="ltx_ref" href="#bib.bib19" title="">2020</a>)</cite>.
More specifically in our experiments, we show that beam search is biased towards less diverse and more
convergent translations, even when the learned model distribution actually resembles HT. This bias is most prominent when the convergent
patterns appear around 50% of the time in training data.
Moreover, frequencies of convergent patterns in MT are increased even when they are uncommon in HT, suggesting perhaps a more inherent structural bias in current MT architectures.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Decoding Algorithms</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Besides beam search, we additionally obtain translations through
two sampling methods.
More specifically, to make fair comparison with single-reference HT,
we sample one translation using ancestral sampling or nucleus
sampling with <math alttext="p=0.95" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">p=0.95</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">italic_p = 0.95</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Holtzman et al. (<a class="ltx_ref" href="#bib.bib30" title="">2020</a>)</cite> for each source sentence.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Beam Search is Biased Against Diversity and Divergence</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">As Figure <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‣ Beam Search is Biased Against Diversity and Divergence ‣ 5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates, for all three language pairs and
two types of divergence,
translations obtained through beam search are
significantly less diverse and more convergent compared to either sampling method.
Indeed, ancestral sampling consistently produces higher diversity scores and lower
convergence rates than even HT.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We hypothesize that the increased diversity score
and the higher divergence rate for
ancestral sampling compared to HT are attributable to the use of
label smoothing during training. <cite class="ltx_cite ltx_citemacro_citet">Roberts et al. (<a class="ltx_ref" href="#bib.bib51" title="">2020</a>)</cite> have also demonstrated the effect of label smoothing on various diversity diagnostics.</span></span></span>
Since ancestral sampling is an unbiased estimator of the model distribution,
this suggests that on the aggregate distribution level,
the model learns to be as least as morphosyntactically diverse and divergent as HT.
</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="768" id="S5.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Convergence rates (Y axis) and diversity scores (X axis) on the aggregate level for translations through different sampling methods
and HT. Sampling methods consistently obtain higher diversity score and lower
convergence rate than beam search.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p2.1">A further breakdown of most frequent<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>With at least 1000 occurrences.</span></span></span> individual source patterns reveals that beam search’s bias
towards convergent translations is a function of the relative frequencies of the
convergent patterns.
As Figure <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‣ Beam Search is Biased Against Diversity and Divergence ‣ 5 Understanding the Discrepancy ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">9</span></a> demonstrates,
the increase of convergence rate for beam search compared to ancestral sampling
seems to be quadratically correlated with the convergence rate for ancestral sampling:
Peak difference is reached at around 40-50%.
This suggests that beam search favors the convergent pattern more when the pattern appears around 50% of the time in training data. This could be because the model has seen the pattern enough to assign it substantial probability mass, but there is still enough uncertainty that humans will frequently choose other patterns.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1085" id="S5.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Plot of difference in convergence rate (beam search vs HT)
against convergence rate of HT. The plot is similar when comparing beam search against ancestral sampling.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p3.1">We additionally note that convergence rate increases
for the overwhelming majority of the most frequent source patterns
<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p3.1.1">even when the
convergence patterns are uncommon in HT</em>.
This strongly suggests an inherent bias of beam search towards convergent patterns,<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>We do not observe a similar trend when comparing ancestral sampling against HT.</span></span></span>
and that this bias is distinct from the typical
bias amplification due to data exposure, e.g., “cooking” is more likely to co-occur with “women” than “men” in the training data <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="#bib.bib72" title="">2017</a>)</cite>.
We suspect that this bias towards convergence is due to the architectural design of MT systems, but we leave the subject matter for future work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Divergence and MT Quality</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In our final analysis, we investigate how the presence of morphosyntactic divergence
in HT might affect MT quality.
In contrast to the previous sections analyzing conditional distributions given a source pattern, we focus instead on individual divergences/convergences.
The potential connection between divergence and MT quality is motivated by
second-language acquisition research
that describes language inference
from their first languages (i.e., negative transfer)
as one source of difficulty for learners <cite class="ltx_cite ltx_citemacro_cite">Gass et al. (<a class="ltx_ref" href="#bib.bib25" title="">2020</a>)</cite>,
which can happen when the two languages diverge structurally.
Do MT systems have similar problems with divergences?</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">To answer this question, we conduct an analysis on the presence (or absence) of
a word-based morphosyntactic divergence in HT and the corresponding
MT quality as measured by BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="#bib.bib46" title="">2002</a>)</cite>
and BLEURT <cite class="ltx_cite ltx_citemacro_cite">Sellam et al. (<a class="ltx_ref" href="#bib.bib53" title="">2020</a>)</cite>.
The basic idea is to construct two contrastive groups of source sentences (called the experiment group and the control group) and compare the MT performance on each group. The HT references of the experiment group contain a given divergent pattern, corresponding to sentences that are perhaps more challenging to translate, whereas those of the control group do not.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.8">More specifically, for a given divergence with source pattern <math alttext="p" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="S6.SS0.SSS0.Px1.p2.1.m1.1a"><mi id="S6.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.1.m1.1b"><ci id="S6.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.1.m1.1d">italic_p</annotation></semantics></math> and target pattern <math alttext="q" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.2.m2.1"><semantics id="S6.SS0.SSS0.Px1.p2.2.m2.1a"><mi id="S6.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S6.SS0.SSS0.Px1.p2.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.2.m2.1b"><ci id="S6.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.2.m2.1d">italic_q</annotation></semantics></math> <math alttext="(p\neq q)" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.3.m3.1"><semantics id="S6.SS0.SSS0.Px1.p2.3.m3.1a"><mrow id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.cmml"><mo id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.2" stretchy="false" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.cmml"><mi id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.2" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.2.cmml">p</mi><mo id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.1" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml">≠</mo><mi id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.3" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.3.cmml">q</mi></mrow><mo id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.3" stretchy="false" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.3.m3.1b"><apply id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1"><neq id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.1"></neq><ci id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.2">𝑝</ci><ci id="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.3.cmml" xref="S6.SS0.SSS0.Px1.p2.3.m3.1.1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.3.m3.1c">(p\neq q)</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.3.m3.1d">( italic_p ≠ italic_q )</annotation></semantics></math>, its control group consists of source sentences for which HT translates every source <math alttext="p" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.4.m4.1"><semantics id="S6.SS0.SSS0.Px1.p2.4.m4.1a"><mi id="S6.SS0.SSS0.Px1.p2.4.m4.1.1" xref="S6.SS0.SSS0.Px1.p2.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.4.m4.1b"><ci id="S6.SS0.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.4.m4.1d">italic_p</annotation></semantics></math> into target <math alttext="p" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.5.m5.1"><semantics id="S6.SS0.SSS0.Px1.p2.5.m5.1a"><mi id="S6.SS0.SSS0.Px1.p2.5.m5.1.1" xref="S6.SS0.SSS0.Px1.p2.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.5.m5.1b"><ci id="S6.SS0.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.5.m5.1d">italic_p</annotation></semantics></math> (i.e., a convergent pattern), and its experiment group consists of source sentences for which HT translates every source <math alttext="p" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.6.m6.1"><semantics id="S6.SS0.SSS0.Px1.p2.6.m6.1a"><mi id="S6.SS0.SSS0.Px1.p2.6.m6.1.1" xref="S6.SS0.SSS0.Px1.p2.6.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.6.m6.1b"><ci id="S6.SS0.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.6.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.6.m6.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.6.m6.1d">italic_p</annotation></semantics></math> into <math alttext="p" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.7.m7.1"><semantics id="S6.SS0.SSS0.Px1.p2.7.m7.1a"><mi id="S6.SS0.SSS0.Px1.p2.7.m7.1.1" xref="S6.SS0.SSS0.Px1.p2.7.m7.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.7.m7.1b"><ci id="S6.SS0.SSS0.Px1.p2.7.m7.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.7.m7.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.7.m7.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.7.m7.1d">italic_p</annotation></semantics></math> except for one that is translated into <math alttext="q" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.8.m8.1"><semantics id="S6.SS0.SSS0.Px1.p2.8.m8.1a"><mi id="S6.SS0.SSS0.Px1.p2.8.m8.1.1" xref="S6.SS0.SSS0.Px1.p2.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p2.8.m8.1b"><ci id="S6.SS0.SSS0.Px1.p2.8.m8.1.1.cmml" xref="S6.SS0.SSS0.Px1.p2.8.m8.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p2.8.m8.1c">q</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p2.8.m8.1d">italic_q</annotation></semantics></math>. For an simplified example, if we are interested the divergence that translates nouns into verbs, the corresponding control group contains source sentences for which HT translates every noun into a noun, whereas its experiment group contains source sentences for which exactly one noun is translated into a verb and the rest of nouns into nouns.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p3.1">We then collect the MT outputs for both groups and compute the differences in BLEU
and BLEURT. This procedure is repeated for every divergence pattern
for which both groups have at least 100 sentences.</p>
</div>
<figure class="ltx_figure" id="S6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="807" id="S6.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Kernel density estimation for the difference in BLEU or BLEURT scores
between the experiment group and the control group. Negative values indicate that the
experiment group has lower score than the control group.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Findings</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">We treat each difference in BLEU or BLEURT as one data point and plot their estimated probability density function. As illustrated in Figure <a class="ltx_ref" href="#S6.F10" title="Figure 10 ‣ Preliminaries ‣ 6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">10</span></a>, divergences are more often
associated with significantly lower BLEU scores (i.e.,
negative differences),
with a fairly large amount of variance.
Trends for BLEURT scores are similar, but with En<math alttext="\to" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S6.SS0.SSS0.Px2.p1.1.m1.1a"><mo id="S6.SS0.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S6.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>De showing less drastic differences compared to BLEU.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>We also note that ngram overlap-based metrics such as BLEU are more likely to penalize diverse translations <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite>.</span></span></span>
On the other hand, a substantial number of divergent patterns have either
virtual no change or an increase of BLEU or BLEURT scores.
This suggests that being a divergence pattern in itself is not associated
with decreased MT performance.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.4">What could explain this variance?
Why are some divergent patterns associated with worse MT performance while others aren’t?
One obvious hypothesis is that these patterns are seen less frequently
during training. However, a closer inspection seems to suggest that frequency
of divergent patterns alone is not an adequate predictor.
More specifically, we use the absolute or relative frequency<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Here, relative frequency is defined as the ratio of the number of training examples with the divergence over that with the convergence. It is a way to counterbalance the fact that some extremely common source patterns will have a lot more frequent divergences.</span></span></span> of the divergent pattern, with or without taking a log of the number, and correlate it with BLEU or BLEURT scores. Even with the best option (log of relative frequency) presented in Table <a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ Findings ‣ 6 Divergence and MT Quality ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, there is only weak correlation (Pearson or Kendall <math alttext="\tau" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p2.1.m1.1"><semantics id="S6.SS0.SSS0.Px2.p2.1.m1.1a"><mi id="S6.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S6.SS0.SSS0.Px2.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p2.1.m1.1b"><ci id="S6.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px2.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px2.p2.1.m1.1d">italic_τ</annotation></semantics></math>) for En<math alttext="\to" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p2.2.m2.1"><semantics id="S6.SS0.SSS0.Px2.p2.2.m2.1a"><mo id="S6.SS0.SSS0.Px2.p2.2.m2.1.1" stretchy="false" xref="S6.SS0.SSS0.Px2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p2.2.m2.1b"><ci id="S6.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p2.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px2.p2.2.m2.1d">→</annotation></semantics></math>Fr and En<math alttext="\to" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p2.3.m3.1"><semantics id="S6.SS0.SSS0.Px2.p2.3.m3.1a"><mo id="S6.SS0.SSS0.Px2.p2.3.m3.1.1" stretchy="false" xref="S6.SS0.SSS0.Px2.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p2.3.m3.1b"><ci id="S6.SS0.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px2.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p2.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px2.p2.3.m3.1d">→</annotation></semantics></math>De, and no correlation for En<math alttext="\to" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p2.4.m4.1"><semantics id="S6.SS0.SSS0.Px2.p2.4.m4.1a"><mo id="S6.SS0.SSS0.Px2.p2.4.m4.1.1" stretchy="false" xref="S6.SS0.SSS0.Px2.p2.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p2.4.m4.1b"><ci id="S6.SS0.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px2.p2.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p2.4.m4.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px2.p2.4.m4.1d">→</annotation></semantics></math>Zh.
It is unclear what aspects of divergent patterns make them
more difficult to translate, or whether they are merely co-occurring with
those elements that are the true cause of difficulty.
We leave it to future work to investigate the underlying cause.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.2"><span class="ltx_text" id="S6.T4.1.1.2.1" style="font-size:80%;">Target</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.3"><span class="ltx_text" id="S6.T4.1.1.3.1" style="font-size:80%;">Metric</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T4.1.1.4"><span class="ltx_text" id="S6.T4.1.1.4.1" style="font-size:80%;">Pearson</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S6.T4.1.1.1">
<span class="ltx_text" id="S6.T4.1.1.1.1" style="font-size:80%;">Kendall </span><math alttext="\tau" class="ltx_Math" display="inline" id="S6.T4.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.m1.1a"><mi id="S6.T4.1.1.1.m1.1.1" mathsize="80%" xref="S6.T4.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.m1.1d">italic_τ</annotation></semantics></math><span class="ltx_text" id="S6.T4.1.1.1.2" style="font-size:80%;"></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.1.1" rowspan="2"><span class="ltx_text" id="S6.T4.1.2.1.1.1" style="font-size:80%;">Zh</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.1.2"><span class="ltx_text" id="S6.T4.1.2.1.2.1" style="font-size:80%;">BLEURT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.2.1.3"><span class="ltx_text" id="S6.T4.1.2.1.3.1" style="font-size:80%;">-0.072</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.1.4"><span class="ltx_text" id="S6.T4.1.2.1.4.1" style="font-size:50%;color:#404040;">0.32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.2.1.5"><span class="ltx_text" id="S6.T4.1.2.1.5.1" style="font-size:80%;">0.030</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.1.6"><span class="ltx_text" id="S6.T4.1.2.1.6.1" style="font-size:50%;color:#404040;">0.54</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.3.2">
<td class="ltx_td ltx_align_left" id="S6.T4.1.3.2.1"><span class="ltx_text" id="S6.T4.1.3.2.1.1" style="font-size:80%;">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.3.2.2"><span class="ltx_text" id="S6.T4.1.3.2.2.1" style="font-size:80%;">-0.080</span></td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.3.2.3"><span class="ltx_text" id="S6.T4.1.3.2.3.1" style="font-size:50%;color:#404040;">0.27</span></td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.3.2.4"><span class="ltx_text" id="S6.T4.1.3.2.4.1" style="font-size:80%;">-0.026</span></td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.3.2.5"><span class="ltx_text" id="S6.T4.1.3.2.5.1" style="font-size:50%;color:#404040;">0.59</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.4.3.1" rowspan="2"><span class="ltx_text" id="S6.T4.1.4.3.1.1" style="font-size:80%;">Fr</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.4.3.2"><span class="ltx_text" id="S6.T4.1.4.3.2.1" style="font-size:80%;">BLEURT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.4.3.3"><span class="ltx_text" id="S6.T4.1.4.3.3.1" style="font-size:80%;">0.319</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.4.3.4"><span class="ltx_text" id="S6.T4.1.4.3.4.1" style="font-size:50%;color:#404040;">1.4e-16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.4.3.5"><span class="ltx_text" id="S6.T4.1.4.3.5.1" style="font-size:80%;">0.240</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.4.3.6"><span class="ltx_text" id="S6.T4.1.4.3.6.1" style="font-size:50%;color:#404040;">1.0e-19</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.5.4">
<td class="ltx_td ltx_align_left" id="S6.T4.1.5.4.1"><span class="ltx_text" id="S6.T4.1.5.4.1.1" style="font-size:80%;">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.5.4.2"><span class="ltx_text" id="S6.T4.1.5.4.2.1" style="font-size:80%;">0.206</span></td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.5.4.3"><span class="ltx_text" id="S6.T4.1.5.4.3.1" style="font-size:50%;color:#404040;">1.6e-7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.5.4.4"><span class="ltx_text" id="S6.T4.1.5.4.4.1" style="font-size:80%;">0.161</span></td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.5.4.5"><span class="ltx_text" id="S6.T4.1.5.4.5.1" style="font-size:50%;color:#404040;">1.2e-9</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S6.T4.1.6.5.1" rowspan="2"><span class="ltx_text" id="S6.T4.1.6.5.1.1" style="font-size:80%;">De</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.6.5.2"><span class="ltx_text" id="S6.T4.1.6.5.2.1" style="font-size:80%;">BLEURT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.6.5.3"><span class="ltx_text" id="S6.T4.1.6.5.3.1" style="font-size:80%;">0.289</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.6.5.4"><span class="ltx_text" id="S6.T4.1.6.5.4.1" style="font-size:50%;color:#404040;">1.4e-11</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.6.5.5"><span class="ltx_text" id="S6.T4.1.6.5.5.1" style="font-size:80%;">0.253</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.6.5.6"><span class="ltx_text" id="S6.T4.1.6.5.6.1" style="font-size:50%;color:#404040;">3.6e-18</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T4.1.7.6.1"><span class="ltx_text" id="S6.T4.1.7.6.1.1" style="font-size:80%;">BLEU</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T4.1.7.6.2"><span class="ltx_text" id="S6.T4.1.7.6.2.1" style="font-size:80%;">0.159</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T4.1.7.6.3"><span class="ltx_text" id="S6.T4.1.7.6.3.1" style="font-size:50%;color:#404040;">2.5e-4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T4.1.7.6.4"><span class="ltx_text" id="S6.T4.1.7.6.4.1" style="font-size:80%;">0.171</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T4.1.7.6.5"><span class="ltx_text" id="S6.T4.1.7.6.5.1" style="font-size:50%;color:#404040;">4.4e-9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Correlation between the difference in BLEURT score and ratio of frequencies (i.e.,
the number of training examples with divergences over that with convergences).
<math alttext="p" class="ltx_Math" display="inline" id="S6.T4.3.m1.1"><semantics id="S6.T4.3.m1.1b"><mi id="S6.T4.3.m1.1.1" xref="S6.T4.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.T4.3.m1.1c"><ci id="S6.T4.3.m1.1.1.cmml" xref="S6.T4.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S6.T4.3.m1.1e">italic_p</annotation></semantics></math>-values are displayed in gray.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We conduct a large-scale fine-grained comparative investigation between HT and MT outputs, through the
lens of morphosyntactic divergence.
Based on extensive analyses on three language pairs, we demonstrate that MT is less morphosyntactic diverse
and more convergent than HT.
We further attribute to this difference to the use of beam search that biases MT outputs towards
less diverse and less divergent patterns.
Finally, we show that the presence of divergent patterns in HT has overall an adverse effect on MT quality.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In future work, we are interested in applying the same analysis to large language model (LLM)-based MT systems.
Recent studies have noted that LLM-based systems tend to produce less literal translations, compared to the traditional
encoder-decoder models <cite class="ltx_cite ltx_citemacro_cite">Vilar et al. (<a class="ltx_ref" href="#bib.bib64" title="">2023</a>); Raunak et al. (<a class="ltx_ref" href="#bib.bib48" title="">2023</a>)</cite>.
It would be interested to see whether and to what extent the LLM translations might differ from those produced by traditional models
when viewed from a morphological lens.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Julia Kreutzer, Eleftheria Briakou, Markus Freitag and Macduff Hughes for providing useful feedback that helped shape this paper. We would also like to thank the anonymous reviewers and the action editor for their constructive comments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2014)</span>
<span class="ltx_bibblock">
Roee Aharoni, Moshe Koppel, and Yoav Goldberg. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P14-2048" title="">Automatic detection of
machine translated text and translation quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers)</em>, pages 289–295,
Baltimore, Maryland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. (2023)</span>
<span class="ltx_bibblock">
Guangsheng Bao, Zhiyang Teng, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.599" title="">Target-side
augmentation for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 10725–10742,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2019)</span>
<span class="ltx_bibblock">
Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian
Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp
Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt
Post, and Marcos Zampieri. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5301" title="">Findings of the 2019
conference on machine translation (WMT19)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1)</em>, pages 1–61, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden et al. (2019)</span>
<span class="ltx_bibblock">
Rachel Bawden, Nikolay Bogoychev, Ulrich Germann, Roman Grundkiewicz, Faheem
Kirefu, Antonio Valerio Miceli Barone, and Alexandra Birch. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5304" title="">The University of
Edinburgh’s submissions to the WMT19 news translation task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1)</em>, pages 103–115, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bizzoni et al. (2020)</span>
<span class="ltx_bibblock">
Yuri Bizzoni, Tom S Juzek, Cristina España-Bonet, Koel Dutta Chowdhury,
Josef van Genabith, and Elke Teich. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.iwslt-1.34" title="">How human is
machine translationese? comparing human and machine translations of text and
speech</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 17th International Conference on Spoken
Language Translation</em>, pages 280–290, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2015)</span>
<span class="ltx_bibblock">
Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow,
Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi.
2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W15-3001" title="">Findings of the 2015
workshop on statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Tenth Workshop on Statistical Machine
Translation</em>, pages 1–46, Lisbon, Portugal. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al. (2016)</span>
<span class="ltx_bibblock">
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T
Kalai. 2016.

</span>
<span class="ltx_bibblock">Man is to computer programmer as woman is to homemaker? debiasing
word embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in neural information processing systems</em>, 29.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briakou and Carpuat (2020)</span>
<span class="ltx_bibblock">
Eleftheria Briakou and Marine Carpuat. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.121" title="">Detecting
Fine-Grained Cross-Lingual Semantic Divergences without
Supervision by Learning to Rank</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1563–1580, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burchell et al. (2022)</span>
<span class="ltx_bibblock">
Laurie Burchell, Alexandra Birch, and Kenneth Heafield. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.deeplo-1.8" title="">Exploring
diversity in back translation for low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Third Workshop on Deep Learning for
Low-Resource Natural Language Processing</em>, pages 67–79, Hybrid. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caliskan et al. (2017)</span>
<span class="ltx_bibblock">
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language corpora contain
human-like biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Science</em>, 356(6334):183–186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carpuat et al. (2017)</span>
<span class="ltx_bibblock">
Marine Carpuat, Yogarshi Vyas, and Xing Niu. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-3209" title="">Detecting cross-lingual
semantic divergence for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the First Workshop on Neural Machine
Translation</em>, pages 69–79, Vancouver. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng and Xue (2017)</span>
<span class="ltx_bibblock">
D Deng and Nianwen Xue. 2017.

</span>
<span class="ltx_bibblock">Translation divergences in chinese–english machine translation: An
empirical investigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Computational Linguistics</em>, 43:521–565.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorr (1994)</span>
<span class="ltx_bibblock">
B. Dorr. 1994.

</span>
<span class="ltx_bibblock">Machine translation divergences: A formal description and proposed
solution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Comput. Linguistics</em>, 20:597–633.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorr (1992)</span>
<span class="ltx_bibblock">
Bonnie J Dorr. 1992.

</span>
<span class="ltx_bibblock">The use of lexical semantics in interlingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Machine Translation</em>, 7(3):135–193.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorr (1993)</span>
<span class="ltx_bibblock">
Bonnie J Dorr. 1993.

</span>
<span class="ltx_bibblock">Interlingual machine translation a parameterized approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Artificial Intelligence</em>, 63(1-2):429–492.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dozat and Manning (2017)</span>
<span class="ltx_bibblock">
Timothy Dozat and Christopher D. Manning. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Hk95PK9le" title="">Deep biaffine
attention for neural dependency parsing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1045" title="">Understanding
back-translation at scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eikema and Aziz (2020)</span>
<span class="ltx_bibblock">
Bryan Eikema and Wilker Aziz. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.398" title="">Is MAP
decoding all you need? the inadequacy of the mode in neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 28th International Conference on
Computational Linguistics</em>, pages 4506–4520, Barcelona, Spain (Online).
International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.62" title="">Language-agnostic BERT sentence embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878–891,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2019)</span>
<span class="ltx_bibblock">
Markus Freitag, Isaac Caswell, and Scott Roy. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5204" title="">APE at scale and its
implications on MT evaluation biases</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 34–44, Florence, Italy. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2020)</span>
<span class="ltx_bibblock">
Markus Freitag, David Grangier, and Isaac Caswell. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.5" title="">BLEU might
be guilty but references are not innocent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 61–71, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2022)</span>
<span class="ltx_bibblock">
Markus Freitag, David Vilar, David Grangier, Colin Cherry, and George Foster.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.263" title="">A natural
diet: Towards improving naturalness of machine translation output</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 3340–3353, Dublin, Ireland. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garg et al. (2018)</span>
<span class="ltx_bibblock">
Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Y. Zou. 2018.

</span>
<span class="ltx_bibblock">Word embeddings quantify 100 years of gender and ethnic stereotypes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the National Academy of Sciences</em>, 115:E3635 –
E3644.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gass et al. (2020)</span>
<span class="ltx_bibblock">
Susan M Gass, Jennifer Behney, and Luke Plonsky. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Second language acquisition: An introductory course</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gellerstam (1986)</span>
<span class="ltx_bibblock">
Martin Gellerstam. 1986.

</span>
<span class="ltx_bibblock">Translationese in swedish novels translated from english.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Translation studies in Scandinavia</em>, 1:88–95.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gimpel et al. (2013)</span>
<span class="ltx_bibblock">
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory Shakhnarovich. 2013.

</span>
<span class="ltx_bibblock">A systematic exploration of diversity in machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1100–1111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Chatterjee (2001)</span>
<span class="ltx_bibblock">
Deepa Gupta and Niladri Chatterjee. 2001.

</span>
<span class="ltx_bibblock">Study of divergence for example based english-hindi machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">STRANS-2001, IIT Kanpur</em>, pages 43–51.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Chatterjee (2003)</span>
<span class="ltx_bibblock">
Deepa Gupta and Niladri Chatterjee. 2003.

</span>
<span class="ltx_bibblock">Identification of divergence for english to hindi ebmt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">MTSUMMIT</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al. (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=rygGQyrFvH" title="">The curious case
of neural text degeneration</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, and Graham Neubig.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.284" title="">Explicit
alignment objectives for multilingual bidirectional encoders</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 3633–3643, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah et al. (2020)</span>
<span class="ltx_bibblock">
Huda Khayrallah, Brian Thompson, Matt Post, and Philipp Koehn. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.7" title="">Simulated
multiple reference training improves low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 82–89, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koppel and Ordan (2011)</span>
<span class="ltx_bibblock">
Moshe Koppel and Noam Ordan. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P11-1132" title="">Translationese and its
dialects</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages
1318–1326, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurokawa et al. (2009)</span>
<span class="ltx_bibblock">
David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2009.mtsummit-papers.9" title="">Automatic
detection of translated text and its impact on machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of Machine Translation Summit XII: Papers</em>,
Ottawa, Canada.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lachaux et al. (2020)</span>
<span class="ltx_bibblock">
Marie-Anne Lachaux, Armand Joulin, and Guillaume Lample. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.256" title="">Target
conditioning for one-to-many generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 2853–2862, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lembersky et al. (2012)</span>
<span class="ltx_bibblock">
Gennadi Lembersky, Noam Ordan, and Shuly Wintner. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/E12-1026" title="">Adapting translation
models to translationese improves SMT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 13th Conference of the European Chapter
of the Association for Computational Linguistics</em>, pages 255–265, Avignon,
France. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Jicheng Li, Pengzhi Gao, Xuanfu Wu, Yang Feng, Zhongjun He, Hua Wu, and Haifeng
Wang. 2021.

</span>
<span class="ltx_bibblock">Mixup decoding for diverse machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 312–320.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2016)</span>
<span class="ltx_bibblock">
Jiwei Li, Will Monroe, and Dan Jurafsky. 2016.

</span>
<span class="ltx_bibblock">A simple, fast diverse decoding algorithm for neural generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">ArXiv</em>, abs/1611.08562.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Huan Lin, Baosong Yang, Liang Yao, Dayiheng Liu, Haibo Zhang, Jun Xie, Min
Zhang, and Jinsong Su. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-naacl.200" title="">Bridging
the gap between training and inference: Multi-candidate optimization for
diverse neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Findings of the Association for Computational Linguistics:
NAACL 2022</em>, pages 2622–2632, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marchisio et al. (2022)</span>
<span class="ltx_bibblock">
Kelly Marchisio, Markus Freitag, and David Grangier. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.161" title="">On
systematic style differences between unsupervised and supervised MT and an
application for high-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 2214–2225, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra and Mishra (2009)</span>
<span class="ltx_bibblock">
Vimal Mishra and Ravi Bhushan Mishra. 2009.

</span>
<span class="ltx_bibblock">Divergence patterns between english and sanskrit machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">INFOCOMP Journal of Computer Science</em>, 8:62–71.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al. (2019)</span>
<span class="ltx_bibblock">
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov.
2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5333" title="">Facebook FAIR’s
WMT19 news translation task submission</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1)</em>, pages 314–319, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolaev et al. (2020)</span>
<span class="ltx_bibblock">
Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika Mitnik,
Lilja Saeboe, and Omri Abend. 2020.

</span>
<span class="ltx_bibblock">Fine-grained analysis of cross-linguistic syntactic divergences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nivre et al. (2016)</span>
<span class="ltx_bibblock">
Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan
Hajic, Christopher D Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016.

</span>
<span class="ltx_bibblock">Universal dependencies v1: A multilingual treebank collection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC’16)</em>, pages 1659–1666.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2018)</span>
<span class="ltx_bibblock">
Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. 2018.

</span>
<span class="ltx_bibblock">Analyzing uncertainty in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">International Conference on Machine Learning</em>, pages
3956–3965. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aclweb.org/anthology/W18-6319" title="">A call for clarity
in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186–191, Belgium, Brussels. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raunak et al. (2023)</span>
<span class="ltx_bibblock">
Vikas Raunak, Arul Menezes, Matt Post, and Hany Hassan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.acl-short.90" title="">Do GPTs produce
less literal translations?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers)</em>, pages 1041–1050,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riley et al. (2020)</span>
<span class="ltx_bibblock">
Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.691" title="">Translationese
as a language in “multilingual” NMT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7737–7746, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. (2022)</span>
<span class="ltx_bibblock">
Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury,
Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,
Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin,
Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo
Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan
Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick,
Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea
Gesmundo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2203.17189" title="">Scaling up models and data
with <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="bib.bib50.3.3.1">t5x</span> and <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="bib.bib50.4.4.2">seqio</span></a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.5.1">arXiv preprint arXiv:2203.17189</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. (2020)</span>
<span class="ltx_bibblock">
Nicholas Roberts, Davis Liang, Graham Neubig, and Zachary Lipton. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.amazon.science/publications/decoding-and-diversity-in-machine-translation" title="">Decoding and diversity in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">NeurIPS 2020 Workshop on Resistance AI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saboor and Khan (2010)</span>
<span class="ltx_bibblock">
Abdus Saboor and Mohammad Abid Khan. 2010.

</span>
<span class="ltx_bibblock">Lexical-semantic divergence in urdu-to-english example based machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">2010 6th International Conference on Emerging Technologies
(ICET)</em>, pages 316–320.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.704" title="">BLEURT:
Learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7881–7892, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer and Stern (2018)</span>
<span class="ltx_bibblock">
Noam Shazeer and Mitchell Stern. 2018.

</span>
<span class="ltx_bibblock">Adafactor: Adaptive learning rates with sublinear memory cost.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">International Conference on Machine Learning</em>, pages
4596–4604. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2019)</span>
<span class="ltx_bibblock">
Tianxiao Shen, Myle Ott, Michael Auli, and Marc’Aurelio Ranzato. 2019.

</span>
<span class="ltx_bibblock">Mixture models for diverse machine translation: Tricks of the trade.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">International conference on machine learning</em>, pages
5719–5728. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al. (2019)</span>
<span class="ltx_bibblock">
Raphael Shu, Hideki Nakayama, and Kyunghyun Cho. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1177" title="">Generating diverse
translations with sentence codes</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1823–1827, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha et al. (2005)</span>
<span class="ltx_bibblock">
K. Sinha, R. Mahesh, and Anil Thakur. 2005.

</span>
<span class="ltx_bibblock">Translation divergence in english-hindi mt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">EAMT</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soto et al. (2020)</span>
<span class="ltx_bibblock">
Xabier Soto, Dimitar Shterionov, Alberto Poncelas, and Andy Way. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.359" title="">Selecting
backtranslated data from multiple sources for improved neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3898–3908, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2020)</span>
<span class="ltx_bibblock">
Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai, and Jiajun Chen. 2020.

</span>
<span class="ltx_bibblock">Generating diverse translation by manipulating multi-head attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 8976–8983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral (2019)</span>
<span class="ltx_bibblock">
Antonio Toral. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W19-6627" title="">Post-editese: an
exacerbated translationese</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of Machine Translation Summit XVII: Research
Track</em>, pages 273–281, Dublin, Ireland. European Association for Machine
Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2021)</span>
<span class="ltx_bibblock">
Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.188" title="">Machine
translationese: Effects of algorithmic bias on linguistic complexity in
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume</em>, pages
2203–2213, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2019)</span>
<span class="ltx_bibblock">
Eva Vanmassenhove, Dimitar Shterionov, and Andy Way. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W19-6622" title="">Lost in translation: Loss
and decay of linguistic richness in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of Machine Translation Summit XVII: Research
Track</em>, pages 222–232, Dublin, Ireland. European Association for Machine
Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al. (2023)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and
George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.acl-long.859" title="">Prompting
PaLM for translation: Assessing strategies and performance</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15406–15427,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vyas et al. (2018)</span>
<span class="ltx_bibblock">
Yogarshi Vyas, Xing Niu, and Marine Carpuat. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1136" title="">Identifying semantic
divergences in parallel text without annotations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1503–1515, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2007)</span>
<span class="ltx_bibblock">
Chao Wang, Michael Collins, and Philipp Koehn. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D07-1077" title="">Chinese syntactic
reordering for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL)</em>, pages 737–745, Prague, Czech Republic.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wein and Schneider (2021)</span>
<span class="ltx_bibblock">
Shira Wein and Nathan Schneider. 2021.

</span>
<span class="ltx_bibblock">Classifying divergences in cross-lingual amr pairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of The Joint 15th Linguistic Annotation Workshop
(LAW) and 3rd Designing Meaning Representations (DMR) Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong et al. (2017)</span>
<span class="ltx_bibblock">
Tak-sum Wong, Kim Gerdes, Herman Leung, and John SY Lee. 2017.

</span>
<span class="ltx_bibblock">Quantitative comparative syntax on the cantonese-mandarin parallel
dependency treebank.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the fourth international conference on
Dependency Linguistics (Depling 2017)</em>, pages 266–275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020)</span>
<span class="ltx_bibblock">
Xuanfu Wu, Yang Feng, and Chenze Shao. 2020.

</span>
<span class="ltx_bibblock">Generating diverse translation from model distribution with dropout.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1088–1097.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeman et al. (2017)</span>
<span class="ltx_bibblock">
Daniel Zeman, Martin Popel, Milan Straka, Jan Hajič, Joakim Nivre, Filip
Ginter, Juhani Luotolahti, Sampo Pyysalo, Slav Petrov, Martin Potthast,
Francis Tyers, Elena Badmaeva, Memduh Gokirmak, Anna Nedoluzhko, Silvie
Cinková, Jan Hajič jr., Jaroslava Hlaváčová,
Václava Kettnerová, Zdeňka Urešová, Jenna Kanerva,
Stina Ojala, Anna Missilä, Christopher D. Manning, Sebastian Schuster,
Siva Reddy, Dima Taji, Nizar Habash, Herman Leung, Marie-Catherine
de Marneffe, Manuela Sanguinetti, Maria Simi, Hiroshi Kanayama, Valeria
de Paiva, Kira Droganova, Héctor Martínez Alonso,
Çağrı Çöltekin, Umut Sulubacak, Hans Uszkoreit,
Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg
Rehm, Tolga Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler,
Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde,
Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko
Shimada, Sookyoung Kwak, Gustavo Mendonça, Tatiana Lando, Rattima
Nitisaroj, and Josie Li. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/K17-3001" title="">CoNLL 2017 shared
task: Multilingual parsing from raw text to Universal Dependencies</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies</em>, pages 1–19, Vancouver,
Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Zong (2016)</span>
<span class="ltx_bibblock">
Jiajun Zhang and Chengqing Zong. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D16-1160" title="">Exploiting source-side
monolingual data in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1535–1545, Austin, Texas. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2017)</span>
<span class="ltx_bibblock">
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.
2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D17-1323" title="">Men also like shopping:
Reducing gender bias amplification using corpus-level constraints</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2979–2989, Copenhagen, Denmark.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2019)</span>
<span class="ltx_bibblock">
Chunting Zhou, Xuezhe Ma, Junjie Hu, and Graham Neubig. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1143" title="">Handling syntactic
divergence in low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1388–1394, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Analysis Subset Filtered Using LaBSE Embeddings</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The main results of the paper are obtained on a held-out subset of the WMT data. To remove some of the noise due to the automatic extraction pipeline that produced the WMT data, we resort to LaBSE embeddings <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite> to further filter the original held-out subset.
More specifically, we use the LaBSE model to derive the crosslingual embeddings for the source and the target of
any sentence pair, and sort all pairs based on the cosine distance between the source and the target embeddings.
The top half (i.e., lowest distance) is kept for analysis, resulting in 500K sentence pairs for
each language pair.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T5.2.3.1.1" rowspan="2"><span class="ltx_text" id="A1.T5.2.3.1.1.1" style="font-size:90%;">Target</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T5.2.3.1.2"><span class="ltx_text" id="A1.T5.2.3.1.2.1" style="font-size:90%;">Word-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T5.2.3.1.3"><span class="ltx_text" id="A1.T5.2.3.1.3.1" style="font-size:90%;">Arc-based</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.2">
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.2.3"><span class="ltx_text" id="A1.T5.2.2.3.1" style="font-size:90%;">HT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.2.4"><span class="ltx_text" id="A1.T5.2.2.4.1" style="font-size:90%;">MT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.1.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T5.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.m1.1a"><mi id="A1.T5.1.1.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="A1.T5.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="A1.T5.1.1.1.1" style="font-size:90%;">%</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.2.5"><span class="ltx_text" id="A1.T5.2.2.5.1" style="font-size:90%;">HT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.2.6"><span class="ltx_text" id="A1.T5.2.2.6.1" style="font-size:90%;">MT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.2.2">
<math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T5.2.2.2.m1.1"><semantics id="A1.T5.2.2.2.m1.1a"><mi id="A1.T5.2.2.2.m1.1.1" mathsize="90%" mathvariant="normal" xref="A1.T5.2.2.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.m1.1b"><ci id="A1.T5.2.2.2.m1.1.1.cmml" xref="A1.T5.2.2.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="A1.T5.2.2.2.1" style="font-size:90%;">%</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A1.T5.2.4.2.1">
<span class="ltx_text" id="A1.T5.2.4.2.1.1" style="font-size:90%;">(i) </span><span class="ltx_text ltx_font_smallcaps" id="A1.T5.2.4.2.1.2" style="font-size:90%;">Diversity</span><span class="ltx_text" id="A1.T5.2.4.2.1.3" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.2.5.3.1"><span class="ltx_text" id="A1.T5.2.5.3.1.1" style="font-size:90%;">Fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.2"><span class="ltx_text" id="A1.T5.2.5.3.2.1" style="font-size:90%;">2.22</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.3"><span class="ltx_text" id="A1.T5.2.5.3.3.1" style="font-size:90%;">1.85</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.4">
<span class="ltx_text" id="A1.T5.2.5.3.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.5.3.4.2" style="font-size:90%;">16.8</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.5"><span class="ltx_text" id="A1.T5.2.5.3.5.1" style="font-size:90%;">2.25</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.6"><span class="ltx_text" id="A1.T5.2.5.3.6.1" style="font-size:90%;">1.77</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.5.3.7">
<span class="ltx_text" id="A1.T5.2.5.3.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.5.3.7.2" style="font-size:90%;">21.6</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.6.4">
<td class="ltx_td ltx_align_left" id="A1.T5.2.6.4.1"><span class="ltx_text" id="A1.T5.2.6.4.1.1" style="font-size:90%;">De</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.2"><span class="ltx_text" id="A1.T5.2.6.4.2.1" style="font-size:90%;">2.24</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.3"><span class="ltx_text" id="A1.T5.2.6.4.3.1" style="font-size:90%;">1.95</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.4">
<span class="ltx_text" id="A1.T5.2.6.4.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.6.4.4.2" style="font-size:90%;">12.9</span>
</td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.5"><span class="ltx_text" id="A1.T5.2.6.4.5.1" style="font-size:90%;">2.39</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.6"><span class="ltx_text" id="A1.T5.2.6.4.6.1" style="font-size:90%;">2.01</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.6.4.7">
<span class="ltx_text" id="A1.T5.2.6.4.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.6.4.7.2" style="font-size:90%;">16.0</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.7.5">
<td class="ltx_td ltx_align_left" id="A1.T5.2.7.5.1"><span class="ltx_text" id="A1.T5.2.7.5.1.1" style="font-size:90%;">Zh</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.2"><span class="ltx_text" id="A1.T5.2.7.5.2.1" style="font-size:90%;">2.92</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.3"><span class="ltx_text" id="A1.T5.2.7.5.3.1" style="font-size:90%;">2.76</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.4">
<span class="ltx_text" id="A1.T5.2.7.5.4.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.7.5.4.2" style="font-size:90%;">5.5</span>
</td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.5"><span class="ltx_text" id="A1.T5.2.7.5.5.1" style="font-size:90%;">3.78</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.6"><span class="ltx_text" id="A1.T5.2.7.5.6.1" style="font-size:90%;">3.47</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.7.5.7">
<span class="ltx_text" id="A1.T5.2.7.5.7.1" style="font-size:90%;width:0.0pt;">-</span><span class="ltx_text" id="A1.T5.2.7.5.7.2" style="font-size:90%;">8.3</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A1.T5.2.8.6.1">
<span class="ltx_text" id="A1.T5.2.8.6.1.1" style="font-size:90%;">(ii) </span><span class="ltx_text ltx_font_smallcaps" id="A1.T5.2.8.6.1.2" style="font-size:90%;">Convergence Rate</span><span class="ltx_text" id="A1.T5.2.8.6.1.3" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.9.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.2.9.7.1"><span class="ltx_text" id="A1.T5.2.9.7.1.1" style="font-size:90%;">Fr</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.2"><span class="ltx_text" id="A1.T5.2.9.7.2.1" style="font-size:90%;">37.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.3"><span class="ltx_text" id="A1.T5.2.9.7.3.1" style="font-size:90%;">43.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.4"><span class="ltx_text" id="A1.T5.2.9.7.4.1" style="font-size:90%;">17.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.5"><span class="ltx_text" id="A1.T5.2.9.7.5.1" style="font-size:90%;">45.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.6"><span class="ltx_text" id="A1.T5.2.9.7.6.1" style="font-size:90%;">52.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.2.9.7.7"><span class="ltx_text" id="A1.T5.2.9.7.7.1" style="font-size:90%;">14.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.10.8">
<td class="ltx_td ltx_align_left" id="A1.T5.2.10.8.1"><span class="ltx_text" id="A1.T5.2.10.8.1.1" style="font-size:90%;">De</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.2"><span class="ltx_text" id="A1.T5.2.10.8.2.1" style="font-size:90%;">44.4</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.3"><span class="ltx_text" id="A1.T5.2.10.8.3.1" style="font-size:90%;">49.4</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.4"><span class="ltx_text" id="A1.T5.2.10.8.4.1" style="font-size:90%;">11.3</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.5"><span class="ltx_text" id="A1.T5.2.10.8.5.1" style="font-size:90%;">49.6</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.6"><span class="ltx_text" id="A1.T5.2.10.8.6.1" style="font-size:90%;">54.7</span></td>
<td class="ltx_td ltx_align_right" id="A1.T5.2.10.8.7"><span class="ltx_text" id="A1.T5.2.10.8.7.1" style="font-size:90%;">10.3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.11.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T5.2.11.9.1"><span class="ltx_text" id="A1.T5.2.11.9.1.1" style="font-size:90%;">Zh</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.2"><span class="ltx_text" id="A1.T5.2.11.9.2.1" style="font-size:90%;">20.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.3"><span class="ltx_text" id="A1.T5.2.11.9.3.1" style="font-size:90%;">22.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.4"><span class="ltx_text" id="A1.T5.2.11.9.4.1" style="font-size:90%;">5.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.5"><span class="ltx_text" id="A1.T5.2.11.9.5.1" style="font-size:90%;">23.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.6"><span class="ltx_text" id="A1.T5.2.11.9.6.1" style="font-size:90%;">24.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T5.2.11.9.7"><span class="ltx_text" id="A1.T5.2.11.9.7.1" style="font-size:90%;">6.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Aggregate diversity scores and convergence rates for the LaBSE-filtered subset. The <math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T5.4.m1.1"><semantics id="A1.T5.4.m1.1b"><mi id="A1.T5.4.m1.1.1" mathvariant="normal" xref="A1.T5.4.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T5.4.m1.1c"><ci id="A1.T5.4.m1.1.1.cmml" xref="A1.T5.4.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.m1.1e">roman_Δ</annotation></semantics></math>% columns show the relative change in percentage from HT to MT.</figcaption>
</figure>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Table <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ Appendix A Analysis Subset Filtered Using LaBSE Embeddings ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the aggregate diversity scores and convergence rates.
The relative changes are slightly smaller than those in Table <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ Aggregate Finding ‣ 4.1 MT is Less Morphosyntactically Diverse Than HT ‣ 4 Comparative Analysis of MT vs HT ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, but the overall trend is remarkably similar: For both word-based and arc-based divergences, MT produces less diverse outputs with more convergent patterns.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Percentage of Content Words and Their Alignments</h2>
<figure class="ltx_table" id="A2.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A2.T6.1.1.1.1"><span class="ltx_text" id="A2.T6.1.1.1.1.1" style="font-size:90%;">Lang</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.2"><span class="ltx_text" id="A2.T6.1.1.1.2.1" style="font-size:90%;">Source content words</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.3"><span class="ltx_text" id="A2.T6.1.1.1.3.1" style="font-size:90%;">Target content words</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.4"><span class="ltx_text" id="A2.T6.1.1.1.4.1" style="font-size:90%;">Alignments</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.2.1.1"><span class="ltx_text" id="A2.T6.1.2.1.1.1" style="font-size:90%;">Fr</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.2.1.2"><span class="ltx_text" id="A2.T6.1.2.1.2.1" style="font-size:90%;">13.7M / 28.7M = 47.9%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.2.1.3"><span class="ltx_text" id="A2.T6.1.2.1.3.1" style="font-size:90%;">14.8M / 33.7M = 44.0%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.2.1.4"><span class="ltx_text" id="A2.T6.1.2.1.4.1" style="font-size:90%;">11.3M / 27.2M = 41.7%</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.3.2.1"><span class="ltx_text" id="A2.T6.1.3.2.1.1" style="font-size:90%;">De</span></th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.3.2.2"><span class="ltx_text" id="A2.T6.1.3.2.2.1" style="font-size:90%;">10.2M / 21.3M = 48.1%</span></td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.3.2.3"><span class="ltx_text" id="A2.T6.1.3.2.3.1" style="font-size:90%;">8.8M / 20.1M = 43.8%</span></td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.3.2.4"><span class="ltx_text" id="A2.T6.1.3.2.4.1" style="font-size:90%;">7.9M / 18.5M = 42.9%</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T6.1.4.3.1"><span class="ltx_text" id="A2.T6.1.4.3.1.1" style="font-size:90%;">Zh</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T6.1.4.3.2"><span class="ltx_text" id="A2.T6.1.4.3.2.1" style="font-size:90%;">12.6M / 26.2M = 48.2%</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T6.1.4.3.3"><span class="ltx_text" id="A2.T6.1.4.3.3.1" style="font-size:90%;">12.8M / 24.5M = 52.4%</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T6.1.4.3.4"><span class="ltx_text" id="A2.T6.1.4.3.4.1" style="font-size:90%;">10.1M / 22.3M = 45.3%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Percentage of content words and their alignments for the held-out analysis subset.</figcaption>
</figure>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Table <a class="ltx_ref" href="#A2.T6" title="Table 6 ‣ Appendix B Percentage of Content Words and Their Alignments ‣ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the percentage of content words and their alignments based on the held-out analysis subset. We only keep the alignments for the main results if both the source token and the target token are content words. The statistics show that around 40%-50% of the tokens (either on the source or the target side) are considered content words, and a similar percentage of alignments pass our criterion.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jan  2 20:02:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
