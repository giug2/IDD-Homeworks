<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.15339] Deep Learning for Economists</title><meta property="og:description" content="Deep learning provides powerful methods to impute structured information from large-scale, unstructured text and image datasets. For example, economists might wish to detect the presence of economic activity in satelli…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning for Economists">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Deep Learning for Economists">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.15339">

<!--Generated on Mon Aug  5 14:18:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Deep Learning for Economists</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Melissa Dell
</span><span class="ltx_author_notes">Department of Economics, Harvard University and NBER, melissadell@fas.harvard.edu. I would like to thank four anonymous referees, the editor, and Jake Carlson for their helpful comments and suggestions. Yiyang Chen provided excellent research assistance.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep learning provides powerful methods to impute structured information from large-scale, unstructured text and image datasets. For example, economists might wish to detect the presence of economic activity in satellite images, or to measure the topics or entities mentioned in social media, the congressional record, or firm filings. This review introduces deep neural networks, covering methods such as classifiers, regression models, generative AI, and embedding models. Applications include classification, document digitization, record linkage, and methods for data exploration in massive scale text and image corpora. When suitable methods are used, deep learning models can be cheap to tune and can scale affordably to problems involving millions or billions of data points.. The review is accompanied by a companion website, EconDL, with user-friendly demo notebooks, software resources, and a knowledge base that provides technical details and additional applications.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">Deep neural networks have led to many recent scientific achievements – ranging from landing a rover on rugged Martian terrain to creating capable chatbots to transforming the diagnosis of disease.
Deep neural networks typically map <span id="p1.1.1" class="ltx_text ltx_font_italic">unstructured</span> data - such as text, document image scans, satellite and other imagery, videos, and audio - to a continuous vector space.
In the above examples, these vectors are used to compute the instructions to steer the spacecraft, autoregressively predict what word comes next given a prompt, or identify whether an image contains a tumor. Analogously, an economist might use neural networks to detect the presence of informal vendors in street view images, or to measure the topics or people mentioned in firm filings or government documents.</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">At its core, deep learning is an approach for learning representations of data from empirical examples <cite class="ltx_cite ltx_citemacro_citep">(LeCun, Bengio and Hinton, <a href="#bib.bib81" title="" class="ltx_ref">2015</a>)</cite>. These representations simplify high dimensional unstructured data into continuous vectors.
Deep neural networks learn representations at multiple layers of abstraction, combining non-linear neural network modules that each transform the representation in the previous layer of the neural network into a slightly more abstract representation, using learned weights (the term “deep” signifies these many layers of transformation). These weights are estimated by minimizing a loss function that compares model predictions for some task to ground truth examples.</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">Why would one use a neural network to transform the raw data into these vector representations, versus just working directly with raw texts or images? First, deep neural networks don’t just learn from the problem at hand. Rather, they incorporate relevant information in their parameters from exposure to massive-scale data. During pre-training, a modern language model, or vision model, will have been exposed to many millions of texts or images, learning the basic structures of language or vision. Exposure to vast amounts of data is essential to strong performance when processing unstructured data, because human language and vision are remarkably complex. This principle is called <span id="p3.1.1" class="ltx_text ltx_font_italic">transfer learning</span> and is core to the success of deep neural methods.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2407.15339/assets/figures/topic-classification-flowchart.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Flowchart for approaching classification.</figcaption>
</figure>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">Moreover, raw pixels or words lack <span id="p4.1.1" class="ltx_text ltx_font_italic">context</span>, which is necessary for interpreting their meaning. Deep neural networks provide a powerful method for computing <span id="p4.1.2" class="ltx_text ltx_font_italic">contextualized representations</span>.
They map terms or pixels to vectors that depend on other nearby terms or pixels, with parameters learned mostly through massive-scale pre-training.</p>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">Finally, raw texts and images are computationally unwieldy. In contrast, there are extremely optimized tools for continuous vector computations. For example, <cite class="ltx_cite ltx_citemacro_citet">Silcock et al. (<a href="#bib.bib124" title="" class="ltx_ref">2023</a>)</cite> make <math id="p5.1.m1.1" class="ltx_Math" alttext="10^{14}" display="inline"><semantics id="p5.1.m1.1a"><msup id="p5.1.m1.1.1" xref="p5.1.m1.1.1.cmml"><mn id="p5.1.m1.1.1.2" xref="p5.1.m1.1.1.2.cmml">10</mn><mn id="p5.1.m1.1.1.3" xref="p5.1.m1.1.1.3.cmml">14</mn></msup><annotation-xml encoding="MathML-Content" id="p5.1.m1.1b"><apply id="p5.1.m1.1.1.cmml" xref="p5.1.m1.1.1"><csymbol cd="ambiguous" id="p5.1.m1.1.1.1.cmml" xref="p5.1.m1.1.1">superscript</csymbol><cn type="integer" id="p5.1.m1.1.1.2.cmml" xref="p5.1.m1.1.1.2">10</cn><cn type="integer" id="p5.1.m1.1.1.3.cmml" xref="p5.1.m1.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p5.1.m1.1c">10^{14}</annotation></semantics></math> exact vector similarity calculations on a single mid-range GPU in 3 hours. This means that data can be analyzed at an <span id="p5.1.1" class="ltx_text ltx_font_italic">unprecedented scale</span>. Theories are tested with data, and while more data won’t solve challenges of causality, in general, it will provide economists with more fine-grained information for testing various hypotheses.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">This review aims to bridge the gap between state-of-the-art deep learning research and economic applications. It focuses on imputing low-dimensional structured data from unstructured texts or images, in contexts where the ground truth is uncontroversial but extraction needs to be automated due to the massive scale of the problem. This structured data is then used for causal or descriptive analyses, whether as an outcome, endogenous variable, instrument, or control. Tasks that economists already perform by hand or with traditional methods (<span id="p6.1.1" class="ltx_text ltx_font_italic">e.g.,</span> record linkage, text classification, document scan digitization) can be more accurately automated at scale, and deep learning also facilitates the extraction of novel data. Like a prior review by <cite class="ltx_cite ltx_citemacro_citet">Gentzkow, Kelly and Taddy (<a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>, this review emphasizes text as data, but with methods developed since the publication of that article.</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">Many of the applications in this review fall under the broad umbrella of <span id="p7.1.1" class="ltx_text ltx_font_italic">classification</span>: mapping high-dimensional unstructured data into discrete classes. The classes could identify types of objects present in a satellite image or numbers and words in a document image scan. Alternatively, classes could identify the topics of texts, the underlying source they were reproduced from, or unique individuals or locations referenced in them. A language model can be used to encode the raw text into lower-dimensional dense vector representations, one for the full text and one for each individual term (where “dense” means that the vector has a non-zero value in every position). The researcher can use these vector representations to predict whether the text is about a given topic, which locations are referenced, etc., by adding a classifier layer to the language model. Image classification works analogously. Generative AI models can also be prompted to impute these classifications. Alternatively, one can work directly with the dense vector representations, which are referred to as <span id="p7.1.2" class="ltx_text ltx_font_italic">embeddings</span>.</p>
</div>
<div id="p8" class="ltx_para">
<p id="p8.1" class="ltx_p">Figure <a href="#S0.F1" title="Figure 1 ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a flow chart for approaching classification. The first question to ask is: “Are the classes enumerated ex ante?” Sometimes, classes are not known, or the researcher may wish to add classes when applying the model to new settings without re-training it. A classifier computes a score for each class using the last layer of the language or vision model. Therefore, it can only be estimated when the classes are specified and seen in training. If classes are not specified, the researcher will need to work directly with the embeddings. If there are many classes, <span id="p8.1.1" class="ltx_text ltx_font_italic">e.g.,</span> as with record linkage, where each unique entity can be thought of as a class, the researcher will again need to work with embeddings due to computational constraints in estimating a classifier.</p>
</div>
<div id="p9" class="ltx_para">
<p id="p9.1" class="ltx_p">When classes are specified ex ante and modest in number, either a classifier or generative AI may be well suited to the problem. If applications diverge from the data used to pre-train the neural network—common when working with historical data, document scans, or certain specialized settings—there is significant <span id="p9.1.1" class="ltx_text ltx_font_italic">domain shift</span> from the pre-training corpus, and it may be necessary to tune a customized classifier to achieve strong performance. The nuance of the class definitions is also important. For straightforward tasks, framing classification as text generation using an off-the-shelf generative AI model like OpenAI’s GPT may work well. For more nuanced tasks, a custom-trained classifier can better capture that nuance by being exposed to fine-grained examples. If in doubt, a researcher can try an off-the-shelf method and switch to a customized classifier if performance is unsatisfactory. This review shows that while custom-trained classifiers most often outperform GPT on text classification tasks, generative AI and custom classifiers both perform well on straightforward tasks. The review also considers the costs of these approaches.</p>
</div>
<figure id="S0.T1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Applications</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S0.T1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:665.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.9pt,-125.6pt) scale(1.60698934721479,1.60698934721479) ;">
<table id="S0.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.T1.1.1.1.1" class="ltx_tr">
<th id="S0.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S0.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Problem</span></th>
<th id="S0.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S0.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Modality</span></th>
<th id="S0.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S0.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Application(s)</span></th>
<th id="S0.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Section</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.T1.1.1.2.1" class="ltx_tr">
<td id="S0.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" colspan="3"><span id="S0.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Classifiers and GenAI</span></td>
<td id="S0.T1.1.1.2.1.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S0.T1.1.1.3.2" class="ltx_tr">
<td id="S0.T1.1.1.3.2.1" class="ltx_td ltx_align_left">Sequence</td>
<td id="S0.T1.1.1.3.2.2" class="ltx_td ltx_align_left">Text</td>
<td id="S0.T1.1.1.3.2.3" class="ltx_td ltx_align_left">Classify news</td>
<td id="S0.T1.1.1.3.2.4" class="ltx_td ltx_align_left"><a href="#S6.SS3" title="6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a></td>
</tr>
<tr id="S0.T1.1.1.4.3" class="ltx_tr">
<td id="S0.T1.1.1.4.3.1" class="ltx_td ltx_align_left">    classification</td>
<td id="S0.T1.1.1.4.3.2" class="ltx_td"></td>
<td id="S0.T1.1.1.4.3.3" class="ltx_td ltx_align_left">article topics</td>
<td id="S0.T1.1.1.4.3.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.5.4" class="ltx_tr">
<td id="S0.T1.1.1.5.4.1" class="ltx_td ltx_align_left">Token</td>
<td id="S0.T1.1.1.5.4.2" class="ltx_td ltx_align_left">Text</td>
<td id="S0.T1.1.1.5.4.3" class="ltx_td ltx_align_left">Tag people,</td>
<td id="S0.T1.1.1.5.4.4" class="ltx_td ltx_align_left"><a href="#S6.SS4" title="6.4 Token classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a></td>
</tr>
<tr id="S0.T1.1.1.6.5" class="ltx_tr">
<td id="S0.T1.1.1.6.5.1" class="ltx_td ltx_align_left">    classification</td>
<td id="S0.T1.1.1.6.5.2" class="ltx_td"></td>
<td id="S0.T1.1.1.6.5.3" class="ltx_td ltx_align_left">locations, orgs</td>
<td id="S0.T1.1.1.6.5.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.7.6" class="ltx_tr">
<td id="S0.T1.1.1.7.6.1" class="ltx_td ltx_align_left">Paired text</td>
<td id="S0.T1.1.1.7.6.2" class="ltx_td ltx_align_left">Text</td>
<td id="S0.T1.1.1.7.6.3" class="ltx_td ltx_align_left">Text b</td>
<td id="S0.T1.1.1.7.6.4" class="ltx_td ltx_align_left"><a href="#S6.SS5" title="6.5 Relationships between texts ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a></td>
</tr>
<tr id="S0.T1.1.1.8.7" class="ltx_tr">
<td id="S0.T1.1.1.8.7.1" class="ltx_td ltx_align_left">    classification</td>
<td id="S0.T1.1.1.8.7.2" class="ltx_td"></td>
<td id="S0.T1.1.1.8.7.3" class="ltx_td ltx_align_left">entails a?</td>
<td id="S0.T1.1.1.8.7.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.9.8" class="ltx_tr">
<td id="S0.T1.1.1.9.8.1" class="ltx_td ltx_align_left" colspan="2"><span id="S0.T1.1.1.9.8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Embedding Models</span></td>
<td id="S0.T1.1.1.9.8.2" class="ltx_td"></td>
<td id="S0.T1.1.1.9.8.3" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.10.9" class="ltx_tr">
<td id="S0.T1.1.1.10.9.1" class="ltx_td ltx_align_left">Link</td>
<td id="S0.T1.1.1.10.9.2" class="ltx_td ltx_align_left">Text,</td>
<td id="S0.T1.1.1.10.9.3" class="ltx_td ltx_align_left">Link</td>
<td id="S0.T1.1.1.10.9.4" class="ltx_td ltx_align_left"><a href="#S7.SS2" title="7.2 Record linkage with structured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a></td>
</tr>
<tr id="S0.T1.1.1.11.10" class="ltx_tr">
<td id="S0.T1.1.1.11.10.1" class="ltx_td ltx_align_left">    structured</td>
<td id="S0.T1.1.1.11.10.2" class="ltx_td ltx_align_left">Images</td>
<td id="S0.T1.1.1.11.10.3" class="ltx_td ltx_align_left">firms, products,</td>
<td id="S0.T1.1.1.11.10.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.12.11" class="ltx_tr">
<td id="S0.T1.1.1.12.11.1" class="ltx_td ltx_align_left">    data</td>
<td id="S0.T1.1.1.12.11.2" class="ltx_td"></td>
<td id="S0.T1.1.1.12.11.3" class="ltx_td ltx_align_left">locations</td>
<td id="S0.T1.1.1.12.11.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.13.12" class="ltx_tr">
<td id="S0.T1.1.1.13.12.1" class="ltx_td ltx_align_left">Link</td>
<td id="S0.T1.1.1.13.12.2" class="ltx_td ltx_align_left">Text</td>
<td id="S0.T1.1.1.13.12.3" class="ltx_td ltx_align_left">Link people</td>
<td id="S0.T1.1.1.13.12.4" class="ltx_td ltx_align_left"><a href="#S7.SS3" title="7.3 Linking unstructured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a></td>
</tr>
<tr id="S0.T1.1.1.14.13" class="ltx_tr">
<td id="S0.T1.1.1.14.13.1" class="ltx_td ltx_align_left">    unstructured</td>
<td id="S0.T1.1.1.14.13.2" class="ltx_td"></td>
<td id="S0.T1.1.1.14.13.3" class="ltx_td ltx_align_left">mentions to</td>
<td id="S0.T1.1.1.14.13.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.15.14" class="ltx_tr">
<td id="S0.T1.1.1.15.14.1" class="ltx_td ltx_align_left">    data</td>
<td id="S0.T1.1.1.15.14.2" class="ltx_td"></td>
<td id="S0.T1.1.1.15.14.3" class="ltx_td ltx_align_left">Wikipedia</td>
<td id="S0.T1.1.1.15.14.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.16.15" class="ltx_tr">
<td id="S0.T1.1.1.16.15.1" class="ltx_td ltx_align_left">Classification</td>
<td id="S0.T1.1.1.16.15.2" class="ltx_td ltx_align_left">Text</td>
<td id="S0.T1.1.1.16.15.3" class="ltx_td ltx_align_left">Track content</td>
<td id="S0.T1.1.1.16.15.4" class="ltx_td ltx_align_left"><a href="#S7.SS4" title="7.4 Classification when categories are unknown ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a></td>
</tr>
<tr id="S0.T1.1.1.17.16" class="ltx_tr">
<td id="S0.T1.1.1.17.16.1" class="ltx_td ltx_align_left">    w/ unknown</td>
<td id="S0.T1.1.1.17.16.2" class="ltx_td ltx_align_left">Images</td>
<td id="S0.T1.1.1.17.16.3" class="ltx_td ltx_align_left">dissemination;</td>
<td id="S0.T1.1.1.17.16.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.18.17" class="ltx_tr">
<td id="S0.T1.1.1.18.17.1" class="ltx_td ltx_align_left">     categories</td>
<td id="S0.T1.1.1.18.17.2" class="ltx_td"></td>
<td id="S0.T1.1.1.18.17.3" class="ltx_td ltx_align_left">data exploration</td>
<td id="S0.T1.1.1.18.17.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.19.18" class="ltx_tr">
<td id="S0.T1.1.1.19.18.1" class="ltx_td ltx_align_left">Retrieval</td>
<td id="S0.T1.1.1.19.18.2" class="ltx_td ltx_align_left">Images</td>
<td id="S0.T1.1.1.19.18.3" class="ltx_td ltx_align_left">Optical character</td>
<td id="S0.T1.1.1.19.18.4" class="ltx_td ltx_align_left"><a href="#S7.SS5" title="7.5 Optical character recognition ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.5</span></a></td>
</tr>
<tr id="S0.T1.1.1.20.19" class="ltx_tr">
<td id="S0.T1.1.1.20.19.1" class="ltx_td"></td>
<td id="S0.T1.1.1.20.19.2" class="ltx_td"></td>
<td id="S0.T1.1.1.20.19.3" class="ltx_td ltx_align_left">recognition</td>
<td id="S0.T1.1.1.20.19.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.21.20" class="ltx_tr">
<td id="S0.T1.1.1.21.20.1" class="ltx_td ltx_align_left"><span id="S0.T1.1.1.21.20.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Regression</span></td>
<td id="S0.T1.1.1.21.20.2" class="ltx_td"></td>
<td id="S0.T1.1.1.21.20.3" class="ltx_td"></td>
<td id="S0.T1.1.1.21.20.4" class="ltx_td"></td>
</tr>
<tr id="S0.T1.1.1.22.21" class="ltx_tr">
<td id="S0.T1.1.1.22.21.1" class="ltx_td ltx_align_left">Object</td>
<td id="S0.T1.1.1.22.21.2" class="ltx_td ltx_align_left">Images</td>
<td id="S0.T1.1.1.22.21.3" class="ltx_td ltx_align_left">Detect document</td>
<td id="S0.T1.1.1.22.21.4" class="ltx_td ltx_align_left"><a href="#S8" title="8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a></td>
</tr>
<tr id="S0.T1.1.1.23.22" class="ltx_tr">
<td id="S0.T1.1.1.23.22.1" class="ltx_td ltx_align_left ltx_border_bb">    detection</td>
<td id="S0.T1.1.1.23.22.2" class="ltx_td ltx_border_bb"></td>
<td id="S0.T1.1.1.23.22.3" class="ltx_td ltx_align_left ltx_border_bb">layouts</td>
<td id="S0.T1.1.1.23.22.4" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S0.T1.2" class="ltx_ERROR ltx_figure_panel undefined">{tablenotes}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S0.T1.3" class="ltx_p ltx_figure_panel">Applications covered in this review.</p>
</div>
</div>
</figure>
<div id="p10" class="ltx_para">
<p id="p10.1" class="ltx_p">Table <a href="#S0.T1" title="Table 1 ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes this review’s applications. Most can be framed as classification problems, and the article also reviews regression, where a neural network is used to impute continuous values from text or images.</p>
</div>
<div id="p11" class="ltx_para">
<p id="p11.1" class="ltx_p">In the pre-deep learning era, problems in different domains were approached in very different ways, using rules heavily engineered to the specific features of a given language or a particular type of image, etc.; whereas deep learning has a remarkable capacity to generalize. Natural language processing (NLP), computer vision, and audio processing all use the same state-of-the-art neural network architecture, for instance. This generalizability is apparent in the diverse applications discussed in this review.</p>
</div>
<div id="p12" class="ltx_para">
<p id="p12.1" class="ltx_p">A variety of neural network applications are beyond the scope of this review. It does not cover how language models can be used more generally to enhance economists’ productivity, as discussed by <cite class="ltx_cite ltx_citemacro_citet">Korinek (<a href="#bib.bib78" title="" class="ltx_ref">2023</a>)</cite>. It also does not cover machine learning methods outside of deep learning, such as those applied to structured data (which typically use shallower networks), as summarized in a review by <cite class="ltx_cite ltx_citemacro_citet">Athey and Imbens (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>. Additionally, it does not examine using deep neural networks to compute approximate solutions to combinatorial optimization and high-dimensional DSGE problems. Approximating these solutions requires learning a neural network to map the original problem to a continuous vector space that preserves the essential properties of the problem. This is useful because computing an approximate solution in this space is dramatically faster than with traditional methods, allowing much larger problems to be approximated. There are many parallels with the methods covered in this review, but the applications are different enough that they necessitate their own treatment. Readers may wish to consult courses by <cite class="ltx_cite ltx_citemacro_citet">Fernández-Villaverde (<a href="#bib.bib43" title="" class="ltx_ref">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Vitercik (<a href="#bib.bib133" title="" class="ltx_ref">2023</a>)</cite>. Finally, a small literature directly uses deep neural networks in a causal framework. For instance, <cite class="ltx_cite ltx_citemacro_citet">Lynn, Kummerfeld and Mihalcea (<a href="#bib.bib89" title="" class="ltx_ref">2020</a>)</cite> use classifiers and experiments to examine how variations in texts causally influence decision-making. While there is a role for this when experimentally manipulating text, often a researcher would like to extract low-dimensional representations from high-dimensional unstructured data (<span id="p12.1.1" class="ltx_text ltx_font_italic">e.g.,</span> a text’s topic, objects in a satellite image, numbers in a table scan, which textual records refer to the same firm) and use these—not the unstructured data—in the causal estimating equation. Hence, the focus here is on predicting these low-dimensional characteristics. The review does not attempt to summarize how these predictions have been used in economic studies, as this literature is new and rapidly evolving.</p>
</div>
<div id="p13" class="ltx_para">
<p id="p13.1" class="ltx_p">The reader may be wondering how quickly this review will become outdated. It is helpful to consider the popular metaphor that neural networks are like Legos: different neural network components can be configured in various ways to achieve different ends, or to achieve a more state-of-the-art version of the same end. This review focuses on frameworks where it is straightforward to swap in new neural network components as the literature advances, <span id="p13.1.1" class="ltx_text ltx_font_italic">e.g.,</span> replacing an older convolutional neural network with a vision transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, or updating a BERT language model backbone <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> with the most recent language model. Technical and implementation details—those most likely to change as the literature advances—are provided on the accompanying EconDL website: <a target="_blank" href="https://econdl.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://econdl.github.io/</a>. It provides a knowledge base organized into core topics, as well as links to open-source packages geared towards economists and pipelines that construct large-scale datasets with deep learning. Interested readers will find lecture notes and links to blog posts, textbook treatments, open courseware, and original papers. EconDL also links to demo notebooks for many of the applications in this review. The website will be updated on a continual basis, and some of the packages explicitly support swapping in new neural networks as the literature advances.</p>
</div>
<div id="p14" class="ltx_para">
<p id="p14.1" class="ltx_p">This article is organized as follows: Section <a href="#S1" title="1 An Overview of Deep Learning ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of deep learning, and Section <a href="#S2" title="2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces foundational architectures.
Section <a href="#S3" title="3 Training Data ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> discusses data requirements of deep learning, Section <a href="#S4" title="4 Bias and Uncertainty Quantification ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> considers bias and uncertainty quantification, and Section <a href="#S5" title="5 Reusability and Reproducibility in Deep Learning ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> addresses reusability and reproducibility.
Next we turn to applications. Section <a href="#S6" title="6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> introduces classification problems where the classes are defined ex ante and there are not too many classes, comparing classifiers and generative AI.
Next, Section <a href="#S7" title="7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> delves into embedding models, which are useful when the number of classes is large or the classes are not specified ex ante.
Section <a href="#S8" title="8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> considers regression problems. There are other ways to approach the applications covered in this review. Section <a href="#S9" title="9 Alternative methods ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> highlights why the methods emphasized are most likely to be suitable to the constraints faced by academic researchers.
Section <a href="#S10" title="10 Conclusion ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> concludes.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>An Overview of Deep Learning</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>What is deep learning?</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Deep neural networks learn representations of raw data that extract information useful for specific tasks. Deep learning uses neural networks with many layers to map raw data to these representations, simplifying high-dimensional unstructured data into continuous vectors.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">To represent data meaningfully for a given task, nodes (the numbers in a vector representation) in one layer of the neural network are transformed into nodes in the next by combining them with a non-linear function whose weights are learned parameters. These parameters—numbering millions to billions—are estimated by minimizing a cost function that compares model predictions on some task (<span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> predicting masked terms in text) to ground truth examples. For those unfamiliar with neural networks, I recommend the introductory videos by <cite class="ltx_cite ltx_citemacro_citet">Sanderson (<a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">The development of novel architectures and methods has made it feasible to optimize neural networks with millions to billions of parameters. These advances, while largely beyond the scope of this relatively broad review, are discussed in the EconDL knowledge base. In particular, many pioneering contributions in estimating deep networks were made in the literature on convolutional neural networks and are discussed in that post in the knowledge base.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Training a deep neural network from scratch requires a massive amount of data, and a couple of large-scale datasets are mainstays of the literature: ImageNet—a 14 million image dataset for image classification and related tasks <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib37" title="" class="ltx_ref">2009</a>)</cite>—and crawl corpora (<span id="S1.SS1.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> Cleaned Colossal Common Crawl <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib105" title="" class="ltx_ref">2019</a>; Dodge et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>)—massive public domain text datasets that essentially take a snapshot of the internet. Commercial models behind an API may also license proprietary training data. Training a deep neural model from scratch can require up to millions of dollars in compute, but fortunately, this is rarely necessary.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">Deep learning has transformed many fields because of the power of <span id="S1.SS1.p5.1.1" class="ltx_text ltx_font_italic">transfer learning</span>: deep networks trained in one domain can be adapted to many other domains with far fewer empirical training examples (often a few hundred to a few thousand) than would be required to train a model from scratch. For example, a researcher who needed to train a topic classifier could go to Hugging Face—the central hub for NLP (Section <a href="#S3" title="3 Training Data ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)—and download a pre-trained state-of-the-art language model made publicly available by entities such as Google, Meta, and Microsoft. The language model was trained on a colossal corpus to predict tokens (words or sub-words) randomly masked from text. From this training, it learned to produce meaningful contextualized vector representations of texts. The researcher could add a classifier layer to the pre-trained language model and fine-tune the resulting neural network on their classification task using a relatively modest amount of labeled data. Most of the millions of model parameters would remain unchanged, as the model’s basic understanding of language doesn’t need updating, but the parameters that are of greatest relevance to the task at hand will update to improve model predictions <cite class="ltx_cite ltx_citemacro_citep">(Merchant et al., <a href="#bib.bib93" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2407.15339/assets/bow.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Classifying text.</figcaption>
</figure>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.6" class="ltx_p">One of the striking findings that has emerged from deep learning in recent years is that returns from increasing model size can continue to accrue even with very large models (e.g., billions of parameters) <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib106" title="" class="ltx_ref">2020</a>)</cite>. Human vision and language are complex, and a rich expressive model is needed to capture that complexity.
For example, suppose we wish to perform the simple task of classifying which of ten objects (a horse, car, etc) is contained in an image. The inputs to a classifier are the RGB values of each pixel <math id="S1.SS1.p6.1.m1.8" class="ltx_Math" alttext="x_{1,1},x_{2,1}...x_{n,n}" display="inline"><semantics id="S1.SS1.p6.1.m1.8a"><mrow id="S1.SS1.p6.1.m1.8.8.2" xref="S1.SS1.p6.1.m1.8.8.3.cmml"><msub id="S1.SS1.p6.1.m1.7.7.1.1" xref="S1.SS1.p6.1.m1.7.7.1.1.cmml"><mi id="S1.SS1.p6.1.m1.7.7.1.1.2" xref="S1.SS1.p6.1.m1.7.7.1.1.2.cmml">x</mi><mrow id="S1.SS1.p6.1.m1.2.2.2.4" xref="S1.SS1.p6.1.m1.2.2.2.3.cmml"><mn id="S1.SS1.p6.1.m1.1.1.1.1" xref="S1.SS1.p6.1.m1.1.1.1.1.cmml">1</mn><mo id="S1.SS1.p6.1.m1.2.2.2.4.1" xref="S1.SS1.p6.1.m1.2.2.2.3.cmml">,</mo><mn id="S1.SS1.p6.1.m1.2.2.2.2" xref="S1.SS1.p6.1.m1.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S1.SS1.p6.1.m1.8.8.2.3" xref="S1.SS1.p6.1.m1.8.8.3.cmml">,</mo><mrow id="S1.SS1.p6.1.m1.8.8.2.2" xref="S1.SS1.p6.1.m1.8.8.2.2.cmml"><msub id="S1.SS1.p6.1.m1.8.8.2.2.2" xref="S1.SS1.p6.1.m1.8.8.2.2.2.cmml"><mi id="S1.SS1.p6.1.m1.8.8.2.2.2.2" xref="S1.SS1.p6.1.m1.8.8.2.2.2.2.cmml">x</mi><mrow id="S1.SS1.p6.1.m1.4.4.2.4" xref="S1.SS1.p6.1.m1.4.4.2.3.cmml"><mn id="S1.SS1.p6.1.m1.3.3.1.1" xref="S1.SS1.p6.1.m1.3.3.1.1.cmml">2</mn><mo id="S1.SS1.p6.1.m1.4.4.2.4.1" xref="S1.SS1.p6.1.m1.4.4.2.3.cmml">,</mo><mn id="S1.SS1.p6.1.m1.4.4.2.2" xref="S1.SS1.p6.1.m1.4.4.2.2.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p6.1.m1.8.8.2.2.1" xref="S1.SS1.p6.1.m1.8.8.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S1.SS1.p6.1.m1.8.8.2.2.3" xref="S1.SS1.p6.1.m1.8.8.2.2.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S1.SS1.p6.1.m1.8.8.2.2.1a" xref="S1.SS1.p6.1.m1.8.8.2.2.1.cmml">​</mo><msub id="S1.SS1.p6.1.m1.8.8.2.2.4" xref="S1.SS1.p6.1.m1.8.8.2.2.4.cmml"><mi id="S1.SS1.p6.1.m1.8.8.2.2.4.2" xref="S1.SS1.p6.1.m1.8.8.2.2.4.2.cmml">x</mi><mrow id="S1.SS1.p6.1.m1.6.6.2.4" xref="S1.SS1.p6.1.m1.6.6.2.3.cmml"><mi id="S1.SS1.p6.1.m1.5.5.1.1" xref="S1.SS1.p6.1.m1.5.5.1.1.cmml">n</mi><mo id="S1.SS1.p6.1.m1.6.6.2.4.1" xref="S1.SS1.p6.1.m1.6.6.2.3.cmml">,</mo><mi id="S1.SS1.p6.1.m1.6.6.2.2" xref="S1.SS1.p6.1.m1.6.6.2.2.cmml">n</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.1.m1.8b"><list id="S1.SS1.p6.1.m1.8.8.3.cmml" xref="S1.SS1.p6.1.m1.8.8.2"><apply id="S1.SS1.p6.1.m1.7.7.1.1.cmml" xref="S1.SS1.p6.1.m1.7.7.1.1"><csymbol cd="ambiguous" id="S1.SS1.p6.1.m1.7.7.1.1.1.cmml" xref="S1.SS1.p6.1.m1.7.7.1.1">subscript</csymbol><ci id="S1.SS1.p6.1.m1.7.7.1.1.2.cmml" xref="S1.SS1.p6.1.m1.7.7.1.1.2">𝑥</ci><list id="S1.SS1.p6.1.m1.2.2.2.3.cmml" xref="S1.SS1.p6.1.m1.2.2.2.4"><cn type="integer" id="S1.SS1.p6.1.m1.1.1.1.1.cmml" xref="S1.SS1.p6.1.m1.1.1.1.1">1</cn><cn type="integer" id="S1.SS1.p6.1.m1.2.2.2.2.cmml" xref="S1.SS1.p6.1.m1.2.2.2.2">1</cn></list></apply><apply id="S1.SS1.p6.1.m1.8.8.2.2.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2"><times id="S1.SS1.p6.1.m1.8.8.2.2.1.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.1"></times><apply id="S1.SS1.p6.1.m1.8.8.2.2.2.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.2"><csymbol cd="ambiguous" id="S1.SS1.p6.1.m1.8.8.2.2.2.1.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.2">subscript</csymbol><ci id="S1.SS1.p6.1.m1.8.8.2.2.2.2.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.2.2">𝑥</ci><list id="S1.SS1.p6.1.m1.4.4.2.3.cmml" xref="S1.SS1.p6.1.m1.4.4.2.4"><cn type="integer" id="S1.SS1.p6.1.m1.3.3.1.1.cmml" xref="S1.SS1.p6.1.m1.3.3.1.1">2</cn><cn type="integer" id="S1.SS1.p6.1.m1.4.4.2.2.cmml" xref="S1.SS1.p6.1.m1.4.4.2.2">1</cn></list></apply><ci id="S1.SS1.p6.1.m1.8.8.2.2.3.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.3">…</ci><apply id="S1.SS1.p6.1.m1.8.8.2.2.4.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.4"><csymbol cd="ambiguous" id="S1.SS1.p6.1.m1.8.8.2.2.4.1.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.4">subscript</csymbol><ci id="S1.SS1.p6.1.m1.8.8.2.2.4.2.cmml" xref="S1.SS1.p6.1.m1.8.8.2.2.4.2">𝑥</ci><list id="S1.SS1.p6.1.m1.6.6.2.3.cmml" xref="S1.SS1.p6.1.m1.6.6.2.4"><ci id="S1.SS1.p6.1.m1.5.5.1.1.cmml" xref="S1.SS1.p6.1.m1.5.5.1.1">𝑛</ci><ci id="S1.SS1.p6.1.m1.6.6.2.2.cmml" xref="S1.SS1.p6.1.m1.6.6.2.2">𝑛</ci></list></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.1.m1.8c">x_{1,1},x_{2,1}...x_{n,n}</annotation></semantics></math>.
Suppose we estimate a linear classifier using these inputs. The score for each class <math id="S1.SS1.p6.2.m2.1" class="ltx_Math" alttext="j=1...10" display="inline"><semantics id="S1.SS1.p6.2.m2.1a"><mrow id="S1.SS1.p6.2.m2.1.1" xref="S1.SS1.p6.2.m2.1.1.cmml"><mi id="S1.SS1.p6.2.m2.1.1.2" xref="S1.SS1.p6.2.m2.1.1.2.cmml">j</mi><mo id="S1.SS1.p6.2.m2.1.1.1" xref="S1.SS1.p6.2.m2.1.1.1.cmml">=</mo><mrow id="S1.SS1.p6.2.m2.1.1.3" xref="S1.SS1.p6.2.m2.1.1.3.cmml"><mn id="S1.SS1.p6.2.m2.1.1.3.2" xref="S1.SS1.p6.2.m2.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S1.SS1.p6.2.m2.1.1.3.1" xref="S1.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S1.SS1.p6.2.m2.1.1.3.3" xref="S1.SS1.p6.2.m2.1.1.3.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S1.SS1.p6.2.m2.1.1.3.1a" xref="S1.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mn id="S1.SS1.p6.2.m2.1.1.3.4" xref="S1.SS1.p6.2.m2.1.1.3.4.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.2.m2.1b"><apply id="S1.SS1.p6.2.m2.1.1.cmml" xref="S1.SS1.p6.2.m2.1.1"><eq id="S1.SS1.p6.2.m2.1.1.1.cmml" xref="S1.SS1.p6.2.m2.1.1.1"></eq><ci id="S1.SS1.p6.2.m2.1.1.2.cmml" xref="S1.SS1.p6.2.m2.1.1.2">𝑗</ci><apply id="S1.SS1.p6.2.m2.1.1.3.cmml" xref="S1.SS1.p6.2.m2.1.1.3"><times id="S1.SS1.p6.2.m2.1.1.3.1.cmml" xref="S1.SS1.p6.2.m2.1.1.3.1"></times><cn type="integer" id="S1.SS1.p6.2.m2.1.1.3.2.cmml" xref="S1.SS1.p6.2.m2.1.1.3.2">1</cn><ci id="S1.SS1.p6.2.m2.1.1.3.3.cmml" xref="S1.SS1.p6.2.m2.1.1.3.3">…</ci><cn type="integer" id="S1.SS1.p6.2.m2.1.1.3.4.cmml" xref="S1.SS1.p6.2.m2.1.1.3.4">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.2.m2.1c">j=1...10</annotation></semantics></math> is <math id="S1.SS1.p6.3.m3.15" class="ltx_Math" alttext="\beta_{j,1,1}x_{1,1}+\beta_{j,1,2}x_{1,2}\cdots+\beta_{j,n,n}x_{n,n}+\gamma_{j}" display="inline"><semantics id="S1.SS1.p6.3.m3.15a"><mrow id="S1.SS1.p6.3.m3.15.16" xref="S1.SS1.p6.3.m3.15.16.cmml"><mrow id="S1.SS1.p6.3.m3.15.16.2" xref="S1.SS1.p6.3.m3.15.16.2.cmml"><msub id="S1.SS1.p6.3.m3.15.16.2.2" xref="S1.SS1.p6.3.m3.15.16.2.2.cmml"><mi id="S1.SS1.p6.3.m3.15.16.2.2.2" xref="S1.SS1.p6.3.m3.15.16.2.2.2.cmml">β</mi><mrow id="S1.SS1.p6.3.m3.3.3.3.5" xref="S1.SS1.p6.3.m3.3.3.3.4.cmml"><mi id="S1.SS1.p6.3.m3.1.1.1.1" xref="S1.SS1.p6.3.m3.1.1.1.1.cmml">j</mi><mo id="S1.SS1.p6.3.m3.3.3.3.5.1" xref="S1.SS1.p6.3.m3.3.3.3.4.cmml">,</mo><mn id="S1.SS1.p6.3.m3.2.2.2.2" xref="S1.SS1.p6.3.m3.2.2.2.2.cmml">1</mn><mo id="S1.SS1.p6.3.m3.3.3.3.5.2" xref="S1.SS1.p6.3.m3.3.3.3.4.cmml">,</mo><mn id="S1.SS1.p6.3.m3.3.3.3.3" xref="S1.SS1.p6.3.m3.3.3.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p6.3.m3.15.16.2.1" xref="S1.SS1.p6.3.m3.15.16.2.1.cmml">​</mo><msub id="S1.SS1.p6.3.m3.15.16.2.3" xref="S1.SS1.p6.3.m3.15.16.2.3.cmml"><mi id="S1.SS1.p6.3.m3.15.16.2.3.2" xref="S1.SS1.p6.3.m3.15.16.2.3.2.cmml">x</mi><mrow id="S1.SS1.p6.3.m3.5.5.2.4" xref="S1.SS1.p6.3.m3.5.5.2.3.cmml"><mn id="S1.SS1.p6.3.m3.4.4.1.1" xref="S1.SS1.p6.3.m3.4.4.1.1.cmml">1</mn><mo id="S1.SS1.p6.3.m3.5.5.2.4.1" xref="S1.SS1.p6.3.m3.5.5.2.3.cmml">,</mo><mn id="S1.SS1.p6.3.m3.5.5.2.2" xref="S1.SS1.p6.3.m3.5.5.2.2.cmml">1</mn></mrow></msub></mrow><mo id="S1.SS1.p6.3.m3.15.16.1" xref="S1.SS1.p6.3.m3.15.16.1.cmml">+</mo><mrow id="S1.SS1.p6.3.m3.15.16.3" xref="S1.SS1.p6.3.m3.15.16.3.cmml"><msub id="S1.SS1.p6.3.m3.15.16.3.2" xref="S1.SS1.p6.3.m3.15.16.3.2.cmml"><mi id="S1.SS1.p6.3.m3.15.16.3.2.2" xref="S1.SS1.p6.3.m3.15.16.3.2.2.cmml">β</mi><mrow id="S1.SS1.p6.3.m3.8.8.3.5" xref="S1.SS1.p6.3.m3.8.8.3.4.cmml"><mi id="S1.SS1.p6.3.m3.6.6.1.1" xref="S1.SS1.p6.3.m3.6.6.1.1.cmml">j</mi><mo id="S1.SS1.p6.3.m3.8.8.3.5.1" xref="S1.SS1.p6.3.m3.8.8.3.4.cmml">,</mo><mn id="S1.SS1.p6.3.m3.7.7.2.2" xref="S1.SS1.p6.3.m3.7.7.2.2.cmml">1</mn><mo id="S1.SS1.p6.3.m3.8.8.3.5.2" xref="S1.SS1.p6.3.m3.8.8.3.4.cmml">,</mo><mn id="S1.SS1.p6.3.m3.8.8.3.3" xref="S1.SS1.p6.3.m3.8.8.3.3.cmml">2</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p6.3.m3.15.16.3.1" xref="S1.SS1.p6.3.m3.15.16.3.1.cmml">​</mo><msub id="S1.SS1.p6.3.m3.15.16.3.3" xref="S1.SS1.p6.3.m3.15.16.3.3.cmml"><mi id="S1.SS1.p6.3.m3.15.16.3.3.2" xref="S1.SS1.p6.3.m3.15.16.3.3.2.cmml">x</mi><mrow id="S1.SS1.p6.3.m3.10.10.2.4" xref="S1.SS1.p6.3.m3.10.10.2.3.cmml"><mn id="S1.SS1.p6.3.m3.9.9.1.1" xref="S1.SS1.p6.3.m3.9.9.1.1.cmml">1</mn><mo id="S1.SS1.p6.3.m3.10.10.2.4.1" xref="S1.SS1.p6.3.m3.10.10.2.3.cmml">,</mo><mn id="S1.SS1.p6.3.m3.10.10.2.2" xref="S1.SS1.p6.3.m3.10.10.2.2.cmml">2</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p6.3.m3.15.16.3.1a" xref="S1.SS1.p6.3.m3.15.16.3.1.cmml">​</mo><mi mathvariant="normal" id="S1.SS1.p6.3.m3.15.16.3.4" xref="S1.SS1.p6.3.m3.15.16.3.4.cmml">⋯</mi></mrow><mo id="S1.SS1.p6.3.m3.15.16.1a" xref="S1.SS1.p6.3.m3.15.16.1.cmml">+</mo><mrow id="S1.SS1.p6.3.m3.15.16.4" xref="S1.SS1.p6.3.m3.15.16.4.cmml"><msub id="S1.SS1.p6.3.m3.15.16.4.2" xref="S1.SS1.p6.3.m3.15.16.4.2.cmml"><mi id="S1.SS1.p6.3.m3.15.16.4.2.2" xref="S1.SS1.p6.3.m3.15.16.4.2.2.cmml">β</mi><mrow id="S1.SS1.p6.3.m3.13.13.3.5" xref="S1.SS1.p6.3.m3.13.13.3.4.cmml"><mi id="S1.SS1.p6.3.m3.11.11.1.1" xref="S1.SS1.p6.3.m3.11.11.1.1.cmml">j</mi><mo id="S1.SS1.p6.3.m3.13.13.3.5.1" xref="S1.SS1.p6.3.m3.13.13.3.4.cmml">,</mo><mi id="S1.SS1.p6.3.m3.12.12.2.2" xref="S1.SS1.p6.3.m3.12.12.2.2.cmml">n</mi><mo id="S1.SS1.p6.3.m3.13.13.3.5.2" xref="S1.SS1.p6.3.m3.13.13.3.4.cmml">,</mo><mi id="S1.SS1.p6.3.m3.13.13.3.3" xref="S1.SS1.p6.3.m3.13.13.3.3.cmml">n</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p6.3.m3.15.16.4.1" xref="S1.SS1.p6.3.m3.15.16.4.1.cmml">​</mo><msub id="S1.SS1.p6.3.m3.15.16.4.3" xref="S1.SS1.p6.3.m3.15.16.4.3.cmml"><mi id="S1.SS1.p6.3.m3.15.16.4.3.2" xref="S1.SS1.p6.3.m3.15.16.4.3.2.cmml">x</mi><mrow id="S1.SS1.p6.3.m3.15.15.2.4" xref="S1.SS1.p6.3.m3.15.15.2.3.cmml"><mi id="S1.SS1.p6.3.m3.14.14.1.1" xref="S1.SS1.p6.3.m3.14.14.1.1.cmml">n</mi><mo id="S1.SS1.p6.3.m3.15.15.2.4.1" xref="S1.SS1.p6.3.m3.15.15.2.3.cmml">,</mo><mi id="S1.SS1.p6.3.m3.15.15.2.2" xref="S1.SS1.p6.3.m3.15.15.2.2.cmml">n</mi></mrow></msub></mrow><mo id="S1.SS1.p6.3.m3.15.16.1b" xref="S1.SS1.p6.3.m3.15.16.1.cmml">+</mo><msub id="S1.SS1.p6.3.m3.15.16.5" xref="S1.SS1.p6.3.m3.15.16.5.cmml"><mi id="S1.SS1.p6.3.m3.15.16.5.2" xref="S1.SS1.p6.3.m3.15.16.5.2.cmml">γ</mi><mi id="S1.SS1.p6.3.m3.15.16.5.3" xref="S1.SS1.p6.3.m3.15.16.5.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.3.m3.15b"><apply id="S1.SS1.p6.3.m3.15.16.cmml" xref="S1.SS1.p6.3.m3.15.16"><plus id="S1.SS1.p6.3.m3.15.16.1.cmml" xref="S1.SS1.p6.3.m3.15.16.1"></plus><apply id="S1.SS1.p6.3.m3.15.16.2.cmml" xref="S1.SS1.p6.3.m3.15.16.2"><times id="S1.SS1.p6.3.m3.15.16.2.1.cmml" xref="S1.SS1.p6.3.m3.15.16.2.1"></times><apply id="S1.SS1.p6.3.m3.15.16.2.2.cmml" xref="S1.SS1.p6.3.m3.15.16.2.2"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.2.2.1.cmml" xref="S1.SS1.p6.3.m3.15.16.2.2">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.2.2.2.cmml" xref="S1.SS1.p6.3.m3.15.16.2.2.2">𝛽</ci><list id="S1.SS1.p6.3.m3.3.3.3.4.cmml" xref="S1.SS1.p6.3.m3.3.3.3.5"><ci id="S1.SS1.p6.3.m3.1.1.1.1.cmml" xref="S1.SS1.p6.3.m3.1.1.1.1">𝑗</ci><cn type="integer" id="S1.SS1.p6.3.m3.2.2.2.2.cmml" xref="S1.SS1.p6.3.m3.2.2.2.2">1</cn><cn type="integer" id="S1.SS1.p6.3.m3.3.3.3.3.cmml" xref="S1.SS1.p6.3.m3.3.3.3.3">1</cn></list></apply><apply id="S1.SS1.p6.3.m3.15.16.2.3.cmml" xref="S1.SS1.p6.3.m3.15.16.2.3"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.2.3.1.cmml" xref="S1.SS1.p6.3.m3.15.16.2.3">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.2.3.2.cmml" xref="S1.SS1.p6.3.m3.15.16.2.3.2">𝑥</ci><list id="S1.SS1.p6.3.m3.5.5.2.3.cmml" xref="S1.SS1.p6.3.m3.5.5.2.4"><cn type="integer" id="S1.SS1.p6.3.m3.4.4.1.1.cmml" xref="S1.SS1.p6.3.m3.4.4.1.1">1</cn><cn type="integer" id="S1.SS1.p6.3.m3.5.5.2.2.cmml" xref="S1.SS1.p6.3.m3.5.5.2.2">1</cn></list></apply></apply><apply id="S1.SS1.p6.3.m3.15.16.3.cmml" xref="S1.SS1.p6.3.m3.15.16.3"><times id="S1.SS1.p6.3.m3.15.16.3.1.cmml" xref="S1.SS1.p6.3.m3.15.16.3.1"></times><apply id="S1.SS1.p6.3.m3.15.16.3.2.cmml" xref="S1.SS1.p6.3.m3.15.16.3.2"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.3.2.1.cmml" xref="S1.SS1.p6.3.m3.15.16.3.2">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.3.2.2.cmml" xref="S1.SS1.p6.3.m3.15.16.3.2.2">𝛽</ci><list id="S1.SS1.p6.3.m3.8.8.3.4.cmml" xref="S1.SS1.p6.3.m3.8.8.3.5"><ci id="S1.SS1.p6.3.m3.6.6.1.1.cmml" xref="S1.SS1.p6.3.m3.6.6.1.1">𝑗</ci><cn type="integer" id="S1.SS1.p6.3.m3.7.7.2.2.cmml" xref="S1.SS1.p6.3.m3.7.7.2.2">1</cn><cn type="integer" id="S1.SS1.p6.3.m3.8.8.3.3.cmml" xref="S1.SS1.p6.3.m3.8.8.3.3">2</cn></list></apply><apply id="S1.SS1.p6.3.m3.15.16.3.3.cmml" xref="S1.SS1.p6.3.m3.15.16.3.3"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.3.3.1.cmml" xref="S1.SS1.p6.3.m3.15.16.3.3">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.3.3.2.cmml" xref="S1.SS1.p6.3.m3.15.16.3.3.2">𝑥</ci><list id="S1.SS1.p6.3.m3.10.10.2.3.cmml" xref="S1.SS1.p6.3.m3.10.10.2.4"><cn type="integer" id="S1.SS1.p6.3.m3.9.9.1.1.cmml" xref="S1.SS1.p6.3.m3.9.9.1.1">1</cn><cn type="integer" id="S1.SS1.p6.3.m3.10.10.2.2.cmml" xref="S1.SS1.p6.3.m3.10.10.2.2">2</cn></list></apply><ci id="S1.SS1.p6.3.m3.15.16.3.4.cmml" xref="S1.SS1.p6.3.m3.15.16.3.4">⋯</ci></apply><apply id="S1.SS1.p6.3.m3.15.16.4.cmml" xref="S1.SS1.p6.3.m3.15.16.4"><times id="S1.SS1.p6.3.m3.15.16.4.1.cmml" xref="S1.SS1.p6.3.m3.15.16.4.1"></times><apply id="S1.SS1.p6.3.m3.15.16.4.2.cmml" xref="S1.SS1.p6.3.m3.15.16.4.2"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.4.2.1.cmml" xref="S1.SS1.p6.3.m3.15.16.4.2">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.4.2.2.cmml" xref="S1.SS1.p6.3.m3.15.16.4.2.2">𝛽</ci><list id="S1.SS1.p6.3.m3.13.13.3.4.cmml" xref="S1.SS1.p6.3.m3.13.13.3.5"><ci id="S1.SS1.p6.3.m3.11.11.1.1.cmml" xref="S1.SS1.p6.3.m3.11.11.1.1">𝑗</ci><ci id="S1.SS1.p6.3.m3.12.12.2.2.cmml" xref="S1.SS1.p6.3.m3.12.12.2.2">𝑛</ci><ci id="S1.SS1.p6.3.m3.13.13.3.3.cmml" xref="S1.SS1.p6.3.m3.13.13.3.3">𝑛</ci></list></apply><apply id="S1.SS1.p6.3.m3.15.16.4.3.cmml" xref="S1.SS1.p6.3.m3.15.16.4.3"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.4.3.1.cmml" xref="S1.SS1.p6.3.m3.15.16.4.3">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.4.3.2.cmml" xref="S1.SS1.p6.3.m3.15.16.4.3.2">𝑥</ci><list id="S1.SS1.p6.3.m3.15.15.2.3.cmml" xref="S1.SS1.p6.3.m3.15.15.2.4"><ci id="S1.SS1.p6.3.m3.14.14.1.1.cmml" xref="S1.SS1.p6.3.m3.14.14.1.1">𝑛</ci><ci id="S1.SS1.p6.3.m3.15.15.2.2.cmml" xref="S1.SS1.p6.3.m3.15.15.2.2">𝑛</ci></list></apply></apply><apply id="S1.SS1.p6.3.m3.15.16.5.cmml" xref="S1.SS1.p6.3.m3.15.16.5"><csymbol cd="ambiguous" id="S1.SS1.p6.3.m3.15.16.5.1.cmml" xref="S1.SS1.p6.3.m3.15.16.5">subscript</csymbol><ci id="S1.SS1.p6.3.m3.15.16.5.2.cmml" xref="S1.SS1.p6.3.m3.15.16.5.2">𝛾</ci><ci id="S1.SS1.p6.3.m3.15.16.5.3.cmml" xref="S1.SS1.p6.3.m3.15.16.5.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.3.m3.15c">\beta_{j,1,1}x_{1,1}+\beta_{j,1,2}x_{1,2}\cdots+\beta_{j,n,n}x_{n,n}+\gamma_{j}</annotation></semantics></math>, where the <math id="S1.SS1.p6.4.m4.1" class="ltx_Math" alttext="\mathbf{\beta}_{j}" display="inline"><semantics id="S1.SS1.p6.4.m4.1a"><msub id="S1.SS1.p6.4.m4.1.1" xref="S1.SS1.p6.4.m4.1.1.cmml"><mi id="S1.SS1.p6.4.m4.1.1.2" xref="S1.SS1.p6.4.m4.1.1.2.cmml">β</mi><mi id="S1.SS1.p6.4.m4.1.1.3" xref="S1.SS1.p6.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.4.m4.1b"><apply id="S1.SS1.p6.4.m4.1.1.cmml" xref="S1.SS1.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S1.SS1.p6.4.m4.1.1.1.cmml" xref="S1.SS1.p6.4.m4.1.1">subscript</csymbol><ci id="S1.SS1.p6.4.m4.1.1.2.cmml" xref="S1.SS1.p6.4.m4.1.1.2">𝛽</ci><ci id="S1.SS1.p6.4.m4.1.1.3.cmml" xref="S1.SS1.p6.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.4.m4.1c">\mathbf{\beta}_{j}</annotation></semantics></math> and <math id="S1.SS1.p6.5.m5.1" class="ltx_Math" alttext="\gamma_{j}" display="inline"><semantics id="S1.SS1.p6.5.m5.1a"><msub id="S1.SS1.p6.5.m5.1.1" xref="S1.SS1.p6.5.m5.1.1.cmml"><mi id="S1.SS1.p6.5.m5.1.1.2" xref="S1.SS1.p6.5.m5.1.1.2.cmml">γ</mi><mi id="S1.SS1.p6.5.m5.1.1.3" xref="S1.SS1.p6.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.5.m5.1b"><apply id="S1.SS1.p6.5.m5.1.1.cmml" xref="S1.SS1.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S1.SS1.p6.5.m5.1.1.1.cmml" xref="S1.SS1.p6.5.m5.1.1">subscript</csymbol><ci id="S1.SS1.p6.5.m5.1.1.2.cmml" xref="S1.SS1.p6.5.m5.1.1.2">𝛾</ci><ci id="S1.SS1.p6.5.m5.1.1.3.cmml" xref="S1.SS1.p6.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.5.m5.1c">\gamma_{j}</annotation></semantics></math> are estimated parameters. The classifier predicts assignment to the class with the highest score.
The weight parameters for a class will be larger for pixels where the object in that class tends to be located.
This is inherently brittle, because for each class there is only one parameter per pixel, but the horse could vary in its pose, its size, its position in the image, its color or build, etc.
In the plot of the <math id="S1.SS1.p6.6.m6.1" class="ltx_Math" alttext="\mathbf{\beta_{horse}}" display="inline"><semantics id="S1.SS1.p6.6.m6.1a"><msub id="S1.SS1.p6.6.m6.1.1" xref="S1.SS1.p6.6.m6.1.1.cmml"><mi id="S1.SS1.p6.6.m6.1.1.2" xref="S1.SS1.p6.6.m6.1.1.2.cmml">β</mi><mi id="S1.SS1.p6.6.m6.1.1.3" xref="S1.SS1.p6.6.m6.1.1.3.cmml">𝐡𝐨𝐫𝐬𝐞</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p6.6.m6.1b"><apply id="S1.SS1.p6.6.m6.1.1.cmml" xref="S1.SS1.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S1.SS1.p6.6.m6.1.1.1.cmml" xref="S1.SS1.p6.6.m6.1.1">subscript</csymbol><ci id="S1.SS1.p6.6.m6.1.1.2.cmml" xref="S1.SS1.p6.6.m6.1.1.2">𝛽</ci><ci id="S1.SS1.p6.6.m6.1.1.3.cmml" xref="S1.SS1.p6.6.m6.1.1.3">𝐡𝐨𝐫𝐬𝐞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p6.6.m6.1c">\mathbf{\beta_{horse}}</annotation></semantics></math>, one may see a horse standing in the middle of the image with two heads facing in either direction, as the linear classifier struggles to assign high values to pixels where horses are empirically likely to be located.
A large neural network effectively allows for many such “filters” in predicting whether a horse is in the image.</p>
</div>
<div id="S1.SS1.p7" class="ltx_para">
<p id="S1.SS1.p7.1" class="ltx_p">Alternatively, suppose we would like to analyze whether statements (<span id="S1.SS1.p7.1.1" class="ltx_text ltx_font_italic">e.g.,</span> in survey data) have a positive, negative, or neutral sentiment. A traditional approach, commonly used in the economics literature, is the bag-of-words method. The researcher looks up the sentiment of each word in a lookup table and aggregates these together to measure the sentiment of the sentence (Figure <a href="#S1.F2" title="Figure 2 ‣ 1.1 What is deep learning? ‣ 1 An Overview of Deep Learning ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S1.SS1.p8" class="ltx_para">
<p id="S1.SS1.p8.1" class="ltx_p">It is straightforward to see the limitations of this approach. Consider the following sentences: ’I love this article,’ ’I don’t love this article,’ ’I don’t hate this article,’ ’There’s nothing that I don’t love about this article.’ One cannot capture these different sentiments—even in these very simple sentences—by adding up independent representations of each word. Instead, we need to model nonlinear combinations of words, and neural networks are the state-of-the-art tool for approximating complicated nonlinear functions.</p>
</div>
<div id="S1.SS1.p9" class="ltx_para">
<p id="S1.SS1.p9.1" class="ltx_p">When processing text with a modern language model (bottom panel of Figure <a href="#S1.F2" title="Figure 2 ‣ 1.1 What is deep learning? ‣ 1 An Overview of Deep Learning ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), a tokenizer first maps each word in the input to a number assigned by a lookup dictionary (if the word isn’t in the dictionary, it gets split into subwords that are). These numbers get converted into vectors using learned parameters and are then passed through the neural network, which transforms them incrementally at each layer into semantically rich representations of the input tokens. Vision models are broadly analogous, taking pixels or patches of images as inputs.</p>
</div>
<div id="S1.SS1.p10" class="ltx_para">
<p id="S1.SS1.p10.1" class="ltx_p">The main alternative to neural networks is to use human-engineered features. In other words, the researcher pre-specifies rules for processing raw information. For example, table digitization could be automated by writing rules to detect the connected white space that separates rows and columns. With deep learning, the model is instead shown annotated examples of table layouts. The deep learning revolution has illustrated over and over—across many different tasks—that learning from empirical examples greatly outperforms human-engineered feature extraction in processing unstructured data. Some of this evidence is discussed in the EconDL knowledge base.</p>
</div>
<div id="S1.SS1.p11" class="ltx_para">
<p id="S1.SS1.p11.1" class="ltx_p">Deep learning is likely to outperform feature engineering in many economic applications as well. The information that economists would like to process is frequently complex and noisy. For example, noise is introduced into document scans through aging, scanning, and historical printing techniques; alternatively, text data may contain OCR errors or typos. Human language is complex, with many different ways to express the same sentiment and words that can change meaning significantly depending on the context. Noise and complexity create exceptions to human-engineered rules, which must also be hard-coded, and likewise there are exceptions to the exceptions. What initially seems like a simple task can quickly become convoluted as the researcher tries to hard-code these exceptions. Even if the results are satisfactory, the human-engineered system is likely to be heavily tailored to the case at hand and will not translate well to other data with different types of complexity and noise.</p>
</div>
<div id="S1.SS1.p12" class="ltx_para">
<p id="S1.SS1.p12.1" class="ltx_p">Another potential advantage of deep learning is that recipes for training and implementing neural networks are standard and reproducible, whereas significant discretion is inherent in human-engineered feature extraction. Even leaving aside researcher degrees of freedom, significant domain knowledge is required to engineer rules. For example, in statistical machine translation, large numbers of researchers worked over decades to engineer complicated statistical rules for machine translation. These systems were outperformed by a neural network that a few researchers developed over a few months. Subsequent advances in neural translation led to the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib132" title="" class="ltx_ref">2017</a>)</cite>, which has since revolutionized natural language processing, computer vision, audio, and other domains.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Foundational Deep Learning Architectures</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides a brief introduction to neural network architectures. For readers who are not familiar, I recommend consulting the EconDL knowledge base and the resources available there for a more detailed treatment.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Basics of Neural Networks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A neural network consists of layers of interconnected nodes, which are called <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">neurons</span>. Each neuron holds a value. The value of a neuron is computed by combining values from neurons in the previous layer using an activation function and learned weights. These many layers transform the input (<span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> tokenized text) into vectors that are useful for performing a desired task.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">An example activation function is rectified linear unit (ReLU): <math id="S2.SS1.p2.1.m1.3" class="ltx_Math" alttext="f(x)=max(0,x)" display="inline"><semantics id="S2.SS1.p2.1.m1.3a"><mrow id="S2.SS1.p2.1.m1.3.4" xref="S2.SS1.p2.1.m1.3.4.cmml"><mrow id="S2.SS1.p2.1.m1.3.4.2" xref="S2.SS1.p2.1.m1.3.4.2.cmml"><mi id="S2.SS1.p2.1.m1.3.4.2.2" xref="S2.SS1.p2.1.m1.3.4.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.3.4.2.1" xref="S2.SS1.p2.1.m1.3.4.2.1.cmml">​</mo><mrow id="S2.SS1.p2.1.m1.3.4.2.3.2" xref="S2.SS1.p2.1.m1.3.4.2.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.3.4.2.3.2.1" xref="S2.SS1.p2.1.m1.3.4.2.cmml">(</mo><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p2.1.m1.3.4.2.3.2.2" xref="S2.SS1.p2.1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p2.1.m1.3.4.1" xref="S2.SS1.p2.1.m1.3.4.1.cmml">=</mo><mrow id="S2.SS1.p2.1.m1.3.4.3" xref="S2.SS1.p2.1.m1.3.4.3.cmml"><mi id="S2.SS1.p2.1.m1.3.4.3.2" xref="S2.SS1.p2.1.m1.3.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.3.4.3.1" xref="S2.SS1.p2.1.m1.3.4.3.1.cmml">​</mo><mi id="S2.SS1.p2.1.m1.3.4.3.3" xref="S2.SS1.p2.1.m1.3.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.3.4.3.1a" xref="S2.SS1.p2.1.m1.3.4.3.1.cmml">​</mo><mi id="S2.SS1.p2.1.m1.3.4.3.4" xref="S2.SS1.p2.1.m1.3.4.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.3.4.3.1b" xref="S2.SS1.p2.1.m1.3.4.3.1.cmml">​</mo><mrow id="S2.SS1.p2.1.m1.3.4.3.5.2" xref="S2.SS1.p2.1.m1.3.4.3.5.1.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.3.4.3.5.2.1" xref="S2.SS1.p2.1.m1.3.4.3.5.1.cmml">(</mo><mn id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml">0</mn><mo id="S2.SS1.p2.1.m1.3.4.3.5.2.2" xref="S2.SS1.p2.1.m1.3.4.3.5.1.cmml">,</mo><mi id="S2.SS1.p2.1.m1.3.3" xref="S2.SS1.p2.1.m1.3.3.cmml">x</mi><mo stretchy="false" id="S2.SS1.p2.1.m1.3.4.3.5.2.3" xref="S2.SS1.p2.1.m1.3.4.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.3b"><apply id="S2.SS1.p2.1.m1.3.4.cmml" xref="S2.SS1.p2.1.m1.3.4"><eq id="S2.SS1.p2.1.m1.3.4.1.cmml" xref="S2.SS1.p2.1.m1.3.4.1"></eq><apply id="S2.SS1.p2.1.m1.3.4.2.cmml" xref="S2.SS1.p2.1.m1.3.4.2"><times id="S2.SS1.p2.1.m1.3.4.2.1.cmml" xref="S2.SS1.p2.1.m1.3.4.2.1"></times><ci id="S2.SS1.p2.1.m1.3.4.2.2.cmml" xref="S2.SS1.p2.1.m1.3.4.2.2">𝑓</ci><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑥</ci></apply><apply id="S2.SS1.p2.1.m1.3.4.3.cmml" xref="S2.SS1.p2.1.m1.3.4.3"><times id="S2.SS1.p2.1.m1.3.4.3.1.cmml" xref="S2.SS1.p2.1.m1.3.4.3.1"></times><ci id="S2.SS1.p2.1.m1.3.4.3.2.cmml" xref="S2.SS1.p2.1.m1.3.4.3.2">𝑚</ci><ci id="S2.SS1.p2.1.m1.3.4.3.3.cmml" xref="S2.SS1.p2.1.m1.3.4.3.3">𝑎</ci><ci id="S2.SS1.p2.1.m1.3.4.3.4.cmml" xref="S2.SS1.p2.1.m1.3.4.3.4">𝑥</ci><interval closure="open" id="S2.SS1.p2.1.m1.3.4.3.5.1.cmml" xref="S2.SS1.p2.1.m1.3.4.3.5.2"><cn type="integer" id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2">0</cn><ci id="S2.SS1.p2.1.m1.3.3.cmml" xref="S2.SS1.p2.1.m1.3.3">𝑥</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.3c">f(x)=max(0,x)</annotation></semantics></math>, where</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="x=w_{1}\cdot i_{1}+w_{2}\cdot i_{2}+\ldots+w_{n}\cdot i_{n}+b" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">x</mi><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><msub id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.cmml">w</mi><mn id="S2.E1.m1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.2.3.2" xref="S2.E1.m1.1.1.3.2.3.2.cmml">i</mi><mn id="S2.E1.m1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><msub id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">w</mi><mn id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.cmml">2</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.3.3.1" xref="S2.E1.m1.1.1.3.3.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml">i</mi><mn id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.cmml">2</mn></msub></mrow><mo id="S2.E1.m1.1.1.3.1a" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S2.E1.m1.1.1.3.4" xref="S2.E1.m1.1.1.3.4.cmml">…</mi><mo id="S2.E1.m1.1.1.3.1b" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.5" xref="S2.E1.m1.1.1.3.5.cmml"><msub id="S2.E1.m1.1.1.3.5.2" xref="S2.E1.m1.1.1.3.5.2.cmml"><mi id="S2.E1.m1.1.1.3.5.2.2" xref="S2.E1.m1.1.1.3.5.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.3.5.2.3" xref="S2.E1.m1.1.1.3.5.2.3.cmml">n</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.3.5.1" xref="S2.E1.m1.1.1.3.5.1.cmml">⋅</mo><msub id="S2.E1.m1.1.1.3.5.3" xref="S2.E1.m1.1.1.3.5.3.cmml"><mi id="S2.E1.m1.1.1.3.5.3.2" xref="S2.E1.m1.1.1.3.5.3.2.cmml">i</mi><mi id="S2.E1.m1.1.1.3.5.3.3" xref="S2.E1.m1.1.1.3.5.3.3.cmml">n</mi></msub></mrow><mo id="S2.E1.m1.1.1.3.1c" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.6" xref="S2.E1.m1.1.1.3.6.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑥</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><ci id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1">⋅</ci><apply id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2">𝑤</ci><cn type="integer" id="S2.E1.m1.1.1.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.3">1</cn></apply><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><ci id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1">⋅</ci><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.2">𝑤</ci><cn type="integer" id="S2.E1.m1.1.1.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3">2</cn></apply><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3">2</cn></apply></apply><ci id="S2.E1.m1.1.1.3.4.cmml" xref="S2.E1.m1.1.1.3.4">…</ci><apply id="S2.E1.m1.1.1.3.5.cmml" xref="S2.E1.m1.1.1.3.5"><ci id="S2.E1.m1.1.1.3.5.1.cmml" xref="S2.E1.m1.1.1.3.5.1">⋅</ci><apply id="S2.E1.m1.1.1.3.5.2.cmml" xref="S2.E1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.5.2.1.cmml" xref="S2.E1.m1.1.1.3.5.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.5.2.2.cmml" xref="S2.E1.m1.1.1.3.5.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.5.2.3.cmml" xref="S2.E1.m1.1.1.3.5.2.3">𝑛</ci></apply><apply id="S2.E1.m1.1.1.3.5.3.cmml" xref="S2.E1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.5.3.1.cmml" xref="S2.E1.m1.1.1.3.5.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.5.3.2.cmml" xref="S2.E1.m1.1.1.3.5.3.2">𝑖</ci><ci id="S2.E1.m1.1.1.3.5.3.3.cmml" xref="S2.E1.m1.1.1.3.5.3.3">𝑛</ci></apply></apply><ci id="S2.E1.m1.1.1.3.6.cmml" xref="S2.E1.m1.1.1.3.6">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">x=w_{1}\cdot i_{1}+w_{2}\cdot i_{2}+\ldots+w_{n}\cdot i_{n}+b</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.3" class="ltx_p"><math id="S2.SS1.p4.1.m1.4" class="ltx_Math" alttext="w_{1},w_{2},\ldots,w_{n}" display="inline"><semantics id="S2.SS1.p4.1.m1.4a"><mrow id="S2.SS1.p4.1.m1.4.4.3" xref="S2.SS1.p4.1.m1.4.4.4.cmml"><msub id="S2.SS1.p4.1.m1.2.2.1.1" xref="S2.SS1.p4.1.m1.2.2.1.1.cmml"><mi id="S2.SS1.p4.1.m1.2.2.1.1.2" xref="S2.SS1.p4.1.m1.2.2.1.1.2.cmml">w</mi><mn id="S2.SS1.p4.1.m1.2.2.1.1.3" xref="S2.SS1.p4.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p4.1.m1.4.4.3.4" xref="S2.SS1.p4.1.m1.4.4.4.cmml">,</mo><msub id="S2.SS1.p4.1.m1.3.3.2.2" xref="S2.SS1.p4.1.m1.3.3.2.2.cmml"><mi id="S2.SS1.p4.1.m1.3.3.2.2.2" xref="S2.SS1.p4.1.m1.3.3.2.2.2.cmml">w</mi><mn id="S2.SS1.p4.1.m1.3.3.2.2.3" xref="S2.SS1.p4.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p4.1.m1.4.4.3.5" xref="S2.SS1.p4.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">…</mi><mo id="S2.SS1.p4.1.m1.4.4.3.6" xref="S2.SS1.p4.1.m1.4.4.4.cmml">,</mo><msub id="S2.SS1.p4.1.m1.4.4.3.3" xref="S2.SS1.p4.1.m1.4.4.3.3.cmml"><mi id="S2.SS1.p4.1.m1.4.4.3.3.2" xref="S2.SS1.p4.1.m1.4.4.3.3.2.cmml">w</mi><mi id="S2.SS1.p4.1.m1.4.4.3.3.3" xref="S2.SS1.p4.1.m1.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.4b"><list id="S2.SS1.p4.1.m1.4.4.4.cmml" xref="S2.SS1.p4.1.m1.4.4.3"><apply id="S2.SS1.p4.1.m1.2.2.1.1.cmml" xref="S2.SS1.p4.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p4.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p4.1.m1.2.2.1.1.2.cmml" xref="S2.SS1.p4.1.m1.2.2.1.1.2">𝑤</ci><cn type="integer" id="S2.SS1.p4.1.m1.2.2.1.1.3.cmml" xref="S2.SS1.p4.1.m1.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.p4.1.m1.3.3.2.2.cmml" xref="S2.SS1.p4.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.3.3.2.2.1.cmml" xref="S2.SS1.p4.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p4.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p4.1.m1.3.3.2.2.2">𝑤</ci><cn type="integer" id="S2.SS1.p4.1.m1.3.3.2.2.3.cmml" xref="S2.SS1.p4.1.m1.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">…</ci><apply id="S2.SS1.p4.1.m1.4.4.3.3.cmml" xref="S2.SS1.p4.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.4.4.3.3.1.cmml" xref="S2.SS1.p4.1.m1.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p4.1.m1.4.4.3.3.2.cmml" xref="S2.SS1.p4.1.m1.4.4.3.3.2">𝑤</ci><ci id="S2.SS1.p4.1.m1.4.4.3.3.3.cmml" xref="S2.SS1.p4.1.m1.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.4c">w_{1},w_{2},\ldots,w_{n}</annotation></semantics></math> and <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">b</annotation></semantics></math> are the learned weights and bias terms and <math id="S2.SS1.p4.3.m3.4" class="ltx_Math" alttext="i_{1},i_{2},\ldots,i_{n}" display="inline"><semantics id="S2.SS1.p4.3.m3.4a"><mrow id="S2.SS1.p4.3.m3.4.4.3" xref="S2.SS1.p4.3.m3.4.4.4.cmml"><msub id="S2.SS1.p4.3.m3.2.2.1.1" xref="S2.SS1.p4.3.m3.2.2.1.1.cmml"><mi id="S2.SS1.p4.3.m3.2.2.1.1.2" xref="S2.SS1.p4.3.m3.2.2.1.1.2.cmml">i</mi><mn id="S2.SS1.p4.3.m3.2.2.1.1.3" xref="S2.SS1.p4.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p4.3.m3.4.4.3.4" xref="S2.SS1.p4.3.m3.4.4.4.cmml">,</mo><msub id="S2.SS1.p4.3.m3.3.3.2.2" xref="S2.SS1.p4.3.m3.3.3.2.2.cmml"><mi id="S2.SS1.p4.3.m3.3.3.2.2.2" xref="S2.SS1.p4.3.m3.3.3.2.2.2.cmml">i</mi><mn id="S2.SS1.p4.3.m3.3.3.2.2.3" xref="S2.SS1.p4.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p4.3.m3.4.4.3.5" xref="S2.SS1.p4.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">…</mi><mo id="S2.SS1.p4.3.m3.4.4.3.6" xref="S2.SS1.p4.3.m3.4.4.4.cmml">,</mo><msub id="S2.SS1.p4.3.m3.4.4.3.3" xref="S2.SS1.p4.3.m3.4.4.3.3.cmml"><mi id="S2.SS1.p4.3.m3.4.4.3.3.2" xref="S2.SS1.p4.3.m3.4.4.3.3.2.cmml">i</mi><mi id="S2.SS1.p4.3.m3.4.4.3.3.3" xref="S2.SS1.p4.3.m3.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.4b"><list id="S2.SS1.p4.3.m3.4.4.4.cmml" xref="S2.SS1.p4.3.m3.4.4.3"><apply id="S2.SS1.p4.3.m3.2.2.1.1.cmml" xref="S2.SS1.p4.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.2.2.1.1.1.cmml" xref="S2.SS1.p4.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p4.3.m3.2.2.1.1.2.cmml" xref="S2.SS1.p4.3.m3.2.2.1.1.2">𝑖</ci><cn type="integer" id="S2.SS1.p4.3.m3.2.2.1.1.3.cmml" xref="S2.SS1.p4.3.m3.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.p4.3.m3.3.3.2.2.cmml" xref="S2.SS1.p4.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.3.3.2.2.1.cmml" xref="S2.SS1.p4.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p4.3.m3.3.3.2.2.2.cmml" xref="S2.SS1.p4.3.m3.3.3.2.2.2">𝑖</ci><cn type="integer" id="S2.SS1.p4.3.m3.3.3.2.2.3.cmml" xref="S2.SS1.p4.3.m3.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">…</ci><apply id="S2.SS1.p4.3.m3.4.4.3.3.cmml" xref="S2.SS1.p4.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.4.4.3.3.1.cmml" xref="S2.SS1.p4.3.m3.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p4.3.m3.4.4.3.3.2.cmml" xref="S2.SS1.p4.3.m3.4.4.3.3.2">𝑖</ci><ci id="S2.SS1.p4.3.m3.4.4.3.3.3.cmml" xref="S2.SS1.p4.3.m3.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.4c">i_{1},i_{2},\ldots,i_{n}</annotation></semantics></math> are the input values to the neuron from the previous layer in the network. When we feed data into a neural network, the input values are transformed by the activation functions at each layer. Nodes in the final layer are the output.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Activation functions are an important component of neural networks, because they introduce non-linearity, enabling the network to capture non-linear relationships in the data. The Convolutional Neural Networks post in the knowledge base provides further introduction to activation functions.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">To optimize the neural network, the output is compared to ground truth labels using a loss function. What these labels measure depends on the objective: <span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_italic">e.g.,</span> predicting masked terms for a language model or predicting image classes for an image model.
As with any optimization problem, we need to know the gradient of the loss with respect to each weight to minimize the function.
For each layer starting from the output layer and moving backward through the network to the input layer, we compute the gradient of the loss with respect to the weights in that layer. This requires using the chain rule. The chain rule allows us to compute the derivative of the loss with respect to any weight in the network by multiplying together derivatives computed layer by layer. This is known as <span id="S2.SS1.p6.1.2" class="ltx_text ltx_font_italic">backpropagation.</span> Weights are adjusted using a gradient descent algorithm.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p">Readers who are not familiar with backpropagation are encouraged to consult <cite class="ltx_cite ltx_citemacro_citet">Sanderson (<a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite> for a high-production-value, graphical introduction. Readers wishing to gain a deeper understanding may enjoy <cite class="ltx_cite ltx_citemacro_citet">Karpathy (<a href="#bib.bib71" title="" class="ltx_ref">2022</a>)</cite>, an advanced backpropagation tutorial. <cite class="ltx_cite ltx_citemacro_citet">Nielsen (<a href="#bib.bib99" title="" class="ltx_ref">2015</a>)</cite> provides a textbook treatment for those with no prior familiarity with neural networks. <cite class="ltx_cite ltx_citemacro_citet">Goodfellow, Bengio and Courville (<a href="#bib.bib51" title="" class="ltx_ref">2016</a>)</cite> offers a textbook treatment for those already familiar who would like an in-depth review, and <cite class="ltx_cite ltx_citemacro_citet">Stevens, Antiga and Viehmann (<a href="#bib.bib126" title="" class="ltx_ref">2020</a>)</cite> is aimed at those who would like to learn key concepts through hands-on implementations in PyTorch.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p">In a vanilla feedforward neural network, all neurons in one layer connect to all neurons in the next layer. Deep fully connected networks are rarely used in practice. Rather a few types of neural networks have dominated the deep learning literature. This review focuses on convolutional neural networks (CNNs; Section <a href="#S2.SS2" title="2.2 Convolutional Neural Networks ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>), recurrent neural networks (RNNs; Section <a href="#S2.SS3" title="2.3 Recurrent Neural Networks ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>), and the transformer (Sections <a href="#S2.SS4" title="2.4 The Transformer ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>, <a href="#S2.SS5" title="2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>, and <a href="#S2.SS6" title="2.6 Vision and Audio Transformers ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.6</span></a>).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Convolutional Neural Networks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">CNNs leverage the spatial structure present in images and played a central role in ushering in the deep learning revolution. Despite the advent of a newer architecture for image processing—the vision transformer (Section <a href="#S2.SS6" title="2.6 Vision and Audio Transformers ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.6</span></a>)—they remain widely used, can obtain near-state-of-the-art performance when appropriately modernized, and can be lighter weight and easier to tune than vision transformers. This section provides a brief introduction. I recommend that those unfamiliar with CNNs consult the short graphical introduction to convolution by <cite class="ltx_cite ltx_citemacro_citet">Sanderson (<a href="#bib.bib114" title="" class="ltx_ref">2020</a>)</cite>, as a visual introduction to the concepts described below is particularly helpful. Additional resources are on the ‘Convolutional Neural Network’ page of the EconDL knowledge base.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.3" class="ltx_p">Vision problems start with an image of a given height (in pixels), width (in pixels), and depth (<span id="S2.SS2.p2.3.1" class="ltx_text ltx_font_italic">e.g.,</span> 3 for an RGB image). Convolutional layers are the core building block of a CNN. The layer’s parameters consist of a set of learnable filters, <span id="S2.SS2.p2.3.2" class="ltx_text ltx_font_italic">e.g.</span>, 3 <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mo id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><times id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\times</annotation></semantics></math> 3, 5 <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mo id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><times id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\times</annotation></semantics></math> 5, or 7 <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mo id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><times id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\times</annotation></semantics></math> 7 weight matrices. These filters are only applied to the nodes immediately surrounding a given node when computing the output for the next layer and extend through the full depth of the input. Each filter is convolved (moved) across the input, producing an activation at each spatial location. Using the same weights for different spatial locations drastically reduces the number of parameters compared to fully connected layers.
Moreover, parameter sharing ensures that features can be detected regardless of their position in the image. This gives CNNs a degree of <span id="S2.SS2.p2.3.3" class="ltx_text ltx_font_italic">translation invariance</span>, desirable since <span id="S2.SS2.p2.3.4" class="ltx_text ltx_font_italic">e.g.,</span> a horse is a horse regardless of its position in the image.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The locality bias inherent in small convolutional filters is logical because the interpretation of a pixel is more influenced by its neighboring pixels than by distant ones. Despite the localized nature of these filters, a CNN still achieves an extensive receptive field through the depth of the network. CNNs are adept at learning hierarchical features: lower layers, which have a more limited receptive field, capture simple patterns such as edges, while deeper layers capture increasingly complex structures.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.2" class="ltx_p">In addition to convolutional layers, CNNs also use pooling layers.
If <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">M</annotation></semantics></math> different convolutional filters are applied to a neural network layer, the depth of the next layer will be <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">M</annotation></semantics></math>, since each filter produces an activation for each spatial location. Pooling layers reduce this depth, preventing the number of parameters from becoming infeasibly large. Often, a CNN consists of alternating convolutional and pooling layers.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">The central challenge to estimating neural networks with many layers is the vanishing gradient problem <cite class="ltx_cite ltx_citemacro_citep">(Bengio, Simard and Frasconi, <a href="#bib.bib17" title="" class="ltx_ref">1994</a>)</cite>. Backpropagation computes the gradient of the cost function with respect to each weight in the network. This requires applying the chain rule to find the gradient of the loss with respect to the output of each layer, and then the gradient of the output of each layer with respect to its input. Derivatives can become very small for extreme values of their inputs. Backpropagation multiplies small gradients together. Hence, the gradient may become exponentially smaller as it flows back to the earlier layers. If the gradient becomes extremely small for the initial layers, learning will be very slow or stop altogether. The post on ‘Convolutional Neural Networks’ on EconDL examines the evolution of CNN architectures, including key innovations that allowed for the optimization of much deeper, more expressive networks, circumventing the vanishing gradient problem <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky, Sutskever and Hinton, <a href="#bib.bib79" title="" class="ltx_ref">2012</a>; Simonyan and Zisserman, <a href="#bib.bib125" title="" class="ltx_ref">2014</a>; Szegedy et al., <a href="#bib.bib127" title="" class="ltx_ref">2015</a>; He et al., <a href="#bib.bib61" title="" class="ltx_ref">2016</a>; Xie et al., <a href="#bib.bib139" title="" class="ltx_ref">2017</a>; Howard et al., <a href="#bib.bib67" title="" class="ltx_ref">2019</a>; Liu et al., <a href="#bib.bib88" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Recurrent Neural Networks</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">CNNs require fixed-size inputs, as neural networks are initialized with weight matrices of fixed dimensions (variably-sized images need to be resized or padded). In contrast, <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">recurrent neural networks</span> (RNNs) are designed to process variably-sized inputs and outputs. They historically played an important role in NLP <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a href="#bib.bib65" title="" class="ltx_ref">1997</a>; Greff et al., <a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite>, though they have since been superseded by the transformer. While researchers generally should use a transformer for NLP applications, I introduce RNNs as a point of comparison to the transformer.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">RNNs process a sequence of inputs—<span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> tokens in a text—iteratively. At each time step, they maintain a state that captures historical information about the input sequence. This state is updated iteratively as the network processes each element in the sequence, allowing the network to ‘remember’ previous elements in the variable-length input.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Long-range dependencies are important to human language. To use a prominent example from <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a href="#bib.bib132" title="" class="ltx_ref">2017</a>)</cite>, ‘The animal didn’t cross the road because it was too tired’ versus ‘The animal didn’t cross the road because it was too wide.’ Does ‘it’ refer to the animal or the road? This depends on dependencies between ‘it’ and other tokens in the input. The most prominent RNN is the bi-directional LSTM (Long Short-Term Memory) <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a href="#bib.bib65" title="" class="ltx_ref">1997</a>)</cite>. Bi-directionality captures dependencies in both directions by feeding the input sequence forwards and backwards. Readers can find a more detailed introduction to LSTMs in the EconDL knowledge base.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>The Transformer</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">transformer</span> <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib132" title="" class="ltx_ref">2017</a>)</cite> has revolutionized NLP and made inroads in nearly all areas of deep learning, including vision, audio, graphs, and reinforcement learning. For readers who are not familiar, I recommend the Illustrated Transformer blog post by <cite class="ltx_cite ltx_citemacro_citep">(Alammar, <a href="#bib.bib4" title="" class="ltx_ref">2018<span class="ltx_text ltx_font_italic">b</span></a>)</cite>, widely recognized as the most accessible introduction. An annotation of the original paper by <cite class="ltx_cite ltx_citemacro_citet">Rush (<a href="#bib.bib111" title="" class="ltx_ref">2018</a>)</cite> is also a classic reference.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The original transformer was a neural translation model whose key ingredient is <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">attention</span>. All tokens (words or sub-words) in a sequence are fed into the model in parallel, and the model attends to all other tokens in the context to create contextualized representations for each token. Contextualized representations contrast with traditional static representations of words <cite class="ltx_cite ltx_citemacro_citep">(Mikolov et al., <a href="#bib.bib94" title="" class="ltx_ref">2013</a>; Pennington, Socher and Manning, <a href="#bib.bib102" title="" class="ltx_ref">2014</a>; Olah, <a href="#bib.bib100" title="" class="ltx_ref">2014</a>)</cite>, where a given word in a training corpus always has the same representation. The transformer solves the locality bias of RNNs—where information is lost as the hidden state is passed along to each sequential token—because it can attend to any token in the sequence, whether nearby or not.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Self-attention is quadratic, which limits the length of text that can be passed into the model at one time (the context window). A typical context window length in open-source models is 512 tokens. For many problems, this is sufficient (<span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_italic">e.g.,</span> the texts can be chunked, or the first 512 tokens are sufficient to form a meaningful document representation). There are also models with sparse attention mechanisms that allow for long context windows.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">Inputs are fed into the transformer in parallel, rather than sequentially as in an RNN, allowing training to fully leverage the parallel computing power of GPUs. This makes it computationally feasible to train much larger models on more data for longer, all of which improve performance <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib106" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Transformer Large Language Models</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Most modern NLP applications use transformer-based large language models (LLMs). For those unfamiliar with LLMs or needing a refresher, I highly recommend Jay Alammar’s ‘Illustrated GPT-2/3’ <cite class="ltx_cite ltx_citemacro_citep">(Alammar, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> and ‘Illustrated BERT’ <cite class="ltx_cite ltx_citemacro_citep">(Alammar, <a href="#bib.bib3" title="" class="ltx_ref">2018<span class="ltx_text ltx_font_italic">a</span></a>)</cite> blog posts, which provide an intuitive graphical introduction to transformer LLM architectures.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">There are two main types of transformer LLMs. <span id="S2.SS5.p2.1.1" class="ltx_text ltx_font_italic">Generative (decoder) models</span> predict the next word in a sequence <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib103" title="" class="ltx_ref">2019</a>; Brown et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. They are typically used for text generation. Since they are trained by predicting the next word, they can only attend to prior tokens when creating contextualized representations for a given token. This is called <span id="S2.SS5.p2.1.2" class="ltx_text ltx_font_italic">causal attention</span>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2407.15339/assets/figures/bert.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="427" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Tasks performed with a transformer encoder language model.</figcaption>
</figure>
<div id="S2.SS5.p3" class="ltx_para">
<p id="S2.SS5.p3.1" class="ltx_p">In contrast, <span id="S2.SS5.p3.1.1" class="ltx_text ltx_font_italic">masked (encoder) language models</span> are bidirectional: in creating contextualized representations of words in a sequence, they can attend to all words in the sequence (<span id="S2.SS5.p3.1.2" class="ltx_text ltx_font_italic">masked attention</span>). The model is trained by predicting masked tokens <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Liu et al., <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Sanh et al., <a href="#bib.bib116" title="" class="ltx_ref">2019<span class="ltx_text ltx_font_italic">a</span></a>; Lan et al., <a href="#bib.bib80" title="" class="ltx_ref">2019</a>; He et al., <a href="#bib.bib63" title="" class="ltx_ref">2020</a>)</cite>. Encoder models are typically used when a researcher aims to create representations of text that entail feeding an entire text to a model. Bidirectionality is helpful for such tasks, because the context both before and after a word is useful for creating semantically meaningful representations of it. Language models can also combine encoder and decoder transformer blocks, <span id="S2.SS5.p3.1.3" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Raffel et al. (<a href="#bib.bib105" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.SS5.p4" class="ltx_para">
<p id="S2.SS5.p4.1" class="ltx_p">With the transformer architecture, the same pre-trained language model can be used as the “backbone” for a wide variety of tasks, facilitating transfer learning. This is illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which is adapted from an illustration in the original BERT paper <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>.
A transformer language model produces a vector representation for each token (word or sub-word) in its input, as well as a <span id="S2.SS5.p4.1.1" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> representation that represents the entire input text.
The first panel shows that full text sequences can be classified by adding a classifier head to the <span id="S2.SS5.p4.1.2" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> token. The classifier is a feedforward neural network that aggregates the nodes in the <span id="S2.SS5.p4.1.3" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> vector of the final transformer layer into a score for each class, using learned weights.
Alternatively, two texts can be embedded jointly - separated by the special token <span id="S2.SS5.p4.1.4" class="ltx_text ltx_font_typewriter">&lt;sep&gt;</span> - and then a classifier can be added to the <span id="S2.SS5.p4.1.5" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> token to classify the relationship between texts (panel b). Or, each individual token can be classified (<span id="S2.SS5.p4.1.6" class="ltx_text ltx_font_italic">e.g.,</span>tagging whether it refers to an individuals, location, etc.) by adding classifier heads to each of the token embeddings (panel c). Spans of text can also be identified (<span id="S2.SS5.p4.1.7" class="ltx_text ltx_font_italic">e.g.,</span> the answer to a question; panel d).</p>
</div>
<div id="S2.SS5.p5" class="ltx_para">
<p id="S2.SS5.p5.1" class="ltx_p">A variety of different transformer pre-trained language models are detailed in the EconDL knowledge base post on ‘Transformer Language Models.’</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Vision and Audio Transformers</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">The transformer has transformed many other areas of deep learning, including computer vision <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib128" title="" class="ltx_ref">2021</a>; Grill et al., <a href="#bib.bib53" title="" class="ltx_ref">2020</a>; Caron et al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Ali et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>; He et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Chen, Xie and He, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>. Vision Transformers (ViTs) use the same transformer architecture as transformer language models, with some adaptations to make them suitable for images. Unlike in NLP, where transformer language models have greatly outpaced the prior technology, in vision the gains from the transformer relative to CNNs are more modest <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib88" title="" class="ltx_ref">2022</a>)</cite>. An appropriately modernized CNN will often be competitive with a similarly sized ViT. Moreover, the smallest CNNs (<span id="S2.SS6.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_cite">Howard et al. (<a href="#bib.bib67" title="" class="ltx_ref">2019</a>)</cite>) are smaller than lightweight ViTs (<span id="S2.SS6.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_cite">Mehta and Rastegari (<a href="#bib.bib92" title="" class="ltx_ref">2021</a>)</cite>) at present and can also perform well on straightforward tasks. Practically, I recommend starting with a lightweight CNN—which will be easier and significantly cheaper to train and deploy—and examining the performance of a larger CNN or ViT model if the lightweight model is inadequate. The EconDL post on Vision Transformers provides more details about ViT architectures.</p>
</div>
<div id="S2.SS6.p2" class="ltx_para">
<p id="S2.SS6.p2.1" class="ltx_p">The extent to which the transformer can be used to process highly diverse types of unstructured data is strikingly illustrated by its application to audio. State-of-the-art performance was achieved by applying a ViT to the spectrogram image of the audio <cite class="ltx_cite ltx_citemacro_citep">(Gong, Chung and Glass, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>. Even more strikingly, performance was maximized by pre-training on ImageNet—the main benchmark for vision that consists of over 14 million natural images (<span id="S2.SS6.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> of dogs, food, etc.)—a powerful illustration of transfer learning even across modalities.</p>
</div>
</section>
<section id="S2.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.7 </span>Optimizing Neural Networks</h3>

<div id="S2.SS7.p1" class="ltx_para">
<p id="S2.SS7.p1.1" class="ltx_p">Being able to optimize a neural network is clearly central to using them in research. The optimizer (see <span id="S2.SS7.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Kingma and Ba (<a href="#bib.bib75" title="" class="ltx_ref">2014</a>); Goh (<a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>), initialization <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib60" title="" class="ltx_ref">2015</a>)</cite>, and normalization <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a href="#bib.bib68" title="" class="ltx_ref">2015</a>; Santurkar et al., <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite> are all important. To estimate deep neural models, the researcher must also select various hyperparameters (<span id="S2.SS7.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib84" title="" class="ltx_ref">2017</a>); Falkner, Klein and Hutter (<a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite>). Interested readers are referred to the post on ‘Basics of Training and Optimizing Neural Networks’ in the EconDL companion knowledge base. The packages on EconDL choose reasonable defaults for various hyperparameters in an effort to make training neural networks more user-friendly.</p>
</div>
<div id="S2.SS7.p2" class="ltx_para">
<p id="S2.SS7.p2.1" class="ltx_p">While there are various details at play, my main practical takeaway from guiding many students in optimizing neural networks is that when performance is unexpectedly poor, it is most often due to either a poorly chosen learning rate or incorrectly formatted input data. The learning rate determines the size of the steps taken by the optimizer while adjusting weights during training. If it is too low, the model won’t update; if it is too high, the model weights will oscillate wildly. Moreover, data are expected in a specific format, and transformations may be performed on the fly (for instance, many neural networks require fixed-size inputs). Neural networks learn from empirical examples, and unexpectedly poor performance is often the result of feeding them misformatted examples.</p>
</div>
<div id="S2.SS7.p3" class="ltx_para">
<p id="S2.SS7.p3.1" class="ltx_p">I also recommend that those new to deep learning train and deploy models using a cloud server specifically designed for this purpose. The EconDL tutorials use Google Colab. Installing deep learning packages locally requires resolving extensive dependencies, which may be challenging for those with limited experience. For more experienced users who heavily utilize deep learning in their research, purchasing their own GPUs can often provide significant cost savings.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">High-quality training and evaluation data are integral to the utility of deep learning. In <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">supervised learning</span>, data are partitioned into <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">labeled</span> and <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">unlabeled</span> sets. The unlabeled set, which consists of all the data that the deep learning model will be applied to, is typically much larger than the labeled set. Deep learning is <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">self-supervised</span> when relevant labels are gleaned automatically from the data itself. For example, language models can be pre-trained by predicting words that have been randomly masked from a massive text corpus. Analogously, a masking strategy can be applied to images for the self-supervised pre-training of vision models <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>. Self-supervised learning is most commonly used in pre-training, and then the pre-trained model is transferred to another domain and applied to unlabeled data (potentially following additional supervised tuning on a modest amount of data from the target domain). Finally, in <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">unsupervised learning</span>, there are no ground truth labels, as the goal is to discover underlying structures in the data, grouping them according to similarities. Embeddings can be clustered, for example, to discover these relationships.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Supervised methods are common in economic applications, as the goal is often to use neural networks to extract some characteristics from unlabeled data.
Economists might also continue self-supervised pre-training. This is most common when the domain of their application shifts considerably from the domain that the model was pre-trained on <cite class="ltx_cite ltx_citemacro_cite">Gururangan et al. (<a href="#bib.bib55" title="" class="ltx_ref">2020</a>)</cite>. For instance, an economic historian analyzing 18th century legal texts might first continue pre-training the language model on that database (by predicting masked tokens), in order to impart better understanding of 18th century legal jargon.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Unsupervised learning is most useful for data exploration. In empirical economics, the norm is to specify a narrowly defined hypothesis and test it statistically, which often lends itself to supervised applications. However, methodical data exploration using unsupervised methods can be a powerful tool for gleaning stylized facts from novel unstructured data.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">When conducting supervised learning, labeled data are further divided into <span id="S3.p4.1.1" class="ltx_text ltx_font_italic">training data</span>—used to train the model, <span id="S3.p4.1.2" class="ltx_text ltx_font_italic">validation data</span>—used to tune model hyperparameters or select prompts, and <span id="S3.p4.1.3" class="ltx_text ltx_font_italic">test data</span>—used only to compute the model evaluations that will be reported in the findings. The researcher should always have a high-quality, representative test set to evaluate model performance. If the data used to evaluate model performance are not representative of the unlabeled dataset—and in particular, if some appreciable portion of the unlabeled data has no support in the labeled data—model performance on the evaluation data may diverge widely from model performance on the unlabeled data, which is the underlying object of interest.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In an ideal scenario, representative test sets can be created through random sampling. However, this is not always possible, particularly when classes that the researcher would like to measure are highly imbalanced. Suppose that a researcher needs to extract texts on a topic of interest from a massive web corpus, and the relevant topic appears only once in every ten thousand texts. The labeling requirements for sampling enough positives randomly are clearly infeasible. This scenario is common in social science, where researchers frequently need to classify relevant information from a massive corpus—<span id="S3.p5.1.1" class="ltx_text ltx_font_italic">e.g.,</span> media or government documents—where only a tiny share of content is about the topic of interest.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">While strategies to draw the most informative samples to annotate have generated a large machine learning literature on <span id="S3.p6.1.1" class="ltx_text ltx_font_italic">active learning</span> (EconDL provides a detailed discussion in the context of text classification), there is little work on selecting representative samples for training, evaluation, or debiasing when class imbalance is severe. Discriminative active learning <cite class="ltx_cite ltx_citemacro_citep">(Gissin and Shalev-Shwartz, <a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite> selects samples to label that maximize the difficulty of distinguishing between the labeled and unlabeled data and can work well with relatively balanced data. It does not work well with severe class imbalance because it fails to sufficiently sample the rare class(es). Other active learning approaches seek to sample near the decision boundary of a classifier, which will sample the rare class(es) and can maximize predictive accuracy. However, this will provide an unrepresentative sample.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">Social scientists, instead, frequently use the presence of certain keyword(s) to choose content to label. However, by construction, this fails to place positive sampling probability on all instances, increasing the odds that some types of unlabeled data have no support in the labeled data. This can generate prediction bias that is systematically correlated with the error term in the downstream causal estimating equation—where the researcher intends to use the deep learning model predictions—since semantics and omitted variables often both vary across space and time.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">Embedding models (Section <a href="#S7" title="7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>)—deep neural networks that create a space where distances between vector representations of texts or images are meaningful—provide a metric that can be used for stratified sampling of data to label for training or evaluation. The closer a text/image is to a set of queries about a class (<span id="S3.p8.1.1" class="ltx_text ltx_font_italic">e.g.,</span> ‘this article is about tax policy’), the higher the probability that it comes from that class, making distances in this space useful for stratified sampling. A stratified sampling approach can also provide informative negatives for training: samples that a pre-trained model place near a query but that are not related to that query in the way the researcher intends. This is an active area of research, where economists have the potential to make important contributions. The EconDL site will update on this literature as it advances.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">Training data need not be drawn from the same distribution as the unlabeled data—given the power of transfer learning—although predictive accuracy will typically decline with the magnitude of the <span id="S3.p9.1.1" class="ltx_text ltx_font_italic">domain shift</span> between the target and training data. Oftentimes, datasets that already exist or can be extracted from web texts allow for the cheap creation of a much larger training set than the researcher could label by hand. High performance on a target dataset is then ensured by further tuning on a much smaller set of hand-crafted labels from the target data.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p"><span id="S3.p10.1.1" class="ltx_text ltx_font_italic">Congruence labeling</span>—when two (or more) annotators label the same data points—is important for ensuring the quality of training and evaluation data. Even seemingly simple tasks are often messier than expected once taken to real-world unstructured data. Congruence labeling also ensures that annotators have understood the task and are producing high-quality labels. In challenging labeling tasks, researchers may wish for all labeled data to be double-annotated, resolving discrepancies by hand. In more straightforward cases, congruence labeling may only be necessary for a smaller subset of the data, to ensure that the task is well-defined and annotators have properly understood the instructions. In machine learning papers, the researcher is typically expected to report the congruence between annotators, as well as to publish annotator instructions, and this can be useful in economic applications as well.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Bias and Uncertainty Quantification</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">There are many limitations to using deep learning to solve social or economic problems (see, for instance, papers from the ACM Conference on Fairness, Accountability, and Transparency <a target="_blank" href="https://facctconference.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://facctconference.org/</a> and <cite class="ltx_cite ltx_citemacro_citet">Cui and Athey (<a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>). Here, our focus is much narrower: to impute or explore features of unstructured data that humans are likely to agree on, in contexts where the size of the raw dataset is orders of magnitude too large to extract features manually.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Caution is necessary when applying deep learning to contexts that require subjective judgment. For example, researchers have shown that the self-identified political orientations of annotators influences their incongruence on political sentiment labeling <cite class="ltx_cite ltx_citemacro_citep">(Shen and Rose, <a href="#bib.bib120" title="" class="ltx_ref">2021</a>)</cite>. Sentiment classification in the deep learning literature has been designed largely around laptop, restaurant, and movie reviews, contexts where there is typically an explicit sentiment about the product that can be validated by stars. In many applications that economists care about—such as sentiment in media data, political speeches, corporate reports, etc.—sentiment can be much more implicit, and humans may not agree on it. If a model is fed annotations that reflect the subjective biases of the annotator—versus a well-defined ground truth—or if it simply does not have enough examples because the distinctions to be made are complex, it will make inaccurate predictions that may be systematically biased. Models can also inherit biases from pre-training, and there are large literatures on bias and fairness in AI <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al., <a href="#bib.bib91" title="" class="ltx_ref">2021</a>)</cite>. These challenges can be mitigated by sticking to straightforward tasks with a clearly defined ground truth.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Economists can make valuable contributions on uncertainty quantification, which is uncommon in much of the deep learning literature. Conformal inference can provide uncertainty quantification for prediction tasks. Facilitated by the collection of a ground truth calibration dataset, conformal methods produce prediction sets with marginal coverage guarantees under mild conditions. A canonical tutorial is <cite class="ltx_cite ltx_citemacro_citet">Shafer and Vovk (<a href="#bib.bib119" title="" class="ltx_ref">2008</a>)</cite>; see <cite class="ltx_cite ltx_citemacro_citet">Chernozhukov, Wüthrich and Zhu (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Cattaneo et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Lei and Candès (<a href="#bib.bib82" title="" class="ltx_ref">2020</a>)</cite> for recent contributions.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Asymptotically motivated inference usually requires that estimates of model parameters be unbiased, which poses a problem for ‘black box’ machine learning predictors that typically trade off bias and variance to produce predictions with low mean squared error. A long literature in semi-parametric inference (<span id="S4.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Robins, Rotnitzky and Zhao (<a href="#bib.bib110" title="" class="ltx_ref">1994</a>)</cite>) has worked to remedy these issues, culminating in a large, recent econometrics literature on debiased machine learning (<span id="S4.p4.1.2" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Chernozhukov et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>); Chernozhukov, Newey and Singh (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>).</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">There are many parallels between the debiased machine learning literature in econometrics and a literature on ’prediction powered inference’ in the deep learning space (<span id="S4.p5.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_cite">Angelopoulos et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>); Zrnic and Candès (<a href="#bib.bib142" title="" class="ltx_ref">2023</a>)</cite>). Broadly speaking, imputing structured characteristics from unstructured data (the focus of the deep learning literature) and causal inference (the focus of the econometrics literature) are special cases of a more general problem of imputing missing data. In causal inference, potential outcomes are missing, whereas in many deep learning prediction applications, low-dimensional structured characteristics are missing because it is prohibitively costly to extract them from high-dimensional unstructured data manually. The prediction powered inference literature examines how deep learning predictions can be debiased using a high-quality auxiliary sample of ground truth labels for the population of interest. This information is used to measure the bias induced by imputation, which is then corrected, ultimately allowing the researcher to perform valid inference without sacrificing the information available from using a model pre-trained on a larger dataset that makes biased predictions. The deep learning model is treated as a black box. One can show that the prediction powered inference framework is equivalent to debiased machine learning.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Reusability and Reproducibility in Deep Learning</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Deep learning has been built to a remarkable degree upon open science and open data, although recent years have seen a pronounced shift towards proprietary models and data as the commercial potential of the technology has become increasingly clear. Nevertheless, the amount of open resources is staggering, and deep learning would not have made the strides it has without widespread sharing of models and datasets. Given the centrality of transfer learning and massive-scale pre-training, the field as we know it would not exist without open science.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The more economics can create an open science culture around big data, whenever data privacy concerns allow, the more we can benefit as a profession from the positive externalities of transfer learning. For example, deep learning researchers are often incentivized to share their code on GitHub as soon as possible, as a way of staking claim to their contribution, or to release a dataset as soon as it is constructed so that a fast-moving literature will use it for longer. Moreover, publication venues in deep learning often require compliance with agreed-upon metadata and ethical frameworks for data and code release <cite class="ltx_cite ltx_citemacro_citep">(Gebru et al., <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; MLCommons, <a href="#bib.bib96" title="" class="ltx_ref">2024</a>; Mitchell et al., <a href="#bib.bib95" title="" class="ltx_ref">2019</a>; Holland et al., <a href="#bib.bib66" title="" class="ltx_ref">2018</a>)</cite>. While I would not advocate that economics wholesale adopt these standards, it is worth considering whether there are standards for model and dataset release that could facilitate reproducibility and reusability of deep neural models in economics.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The largest hub for deep learning models and data is Hugging Face. A wealth of language models and text data can be found there, some examples of which are examined in the demo notebooks linked through EconDL. Hugging Face recently acquired timm, a central repository for vision models, making Hugging Face a one-stop shop for many language and vision tasks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Classifiers</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Having provided an introduction to deep learning, I now turn to applications. Classification is frequently integral to economic analyses. In the era of big data, a researcher may first need to extract relevant data using classification. For instance, they might start with a massive-scale corpus of news, social media posts, earnings calls, or legislative records and need to extract only coverage about interest rates, immigration, or higher education out of millions or even billions of texts in the full corpus. This much more limited corpus is then used to extract the measure(s) to be used in some downstream causal estimating equation. While this step often receives scant attention, biased classification will result in selection bias into the sample used in the downstream causal estimating equation, potentially significantly biasing the conclusions. Alternatively, a researcher might impute structured data—<span id="S6.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> geographic locations mentioned in texts, their sentiments or topics, or what type of objects appear in a satellite image—using classification.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">This section first introduces classifiers (Section <a href="#S6.SS1" title="6.1 An Introduction to classifiers ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>), as well as describing the use of generative AI for classification (Section <a href="#S6.SS2" title="6.2 Generative AI for classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>).
Then, it introduces <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">sequence classification</span>, in which a class label is imputed for a sequence of text: <span id="S6.p2.1.2" class="ltx_text ltx_font_italic">e.g.,</span> a sentence, paragraph, or document (Section <a href="#S6.SS3" title="6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>). It compares the performance of custom-trained classifiers to generative AI on 19 different text classification tasks.
Classification can also be applied to individual terms in a text (Section <a href="#S6.SS4" title="6.4 Token classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>).
Finally, a classifier can be used to compare texts to each other (Section <a href="#S6.SS5" title="6.5 Relationships between texts ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>).
I focus on text classification for ease of exposition. Image classification–of a full image or pixels or objects within an image–is analogous, using a CNN or vision transformer rather than a language transformer. It is covered in depth in the EconDL knowledge base post titled ‘Convolutional Neural Networks.’</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>An Introduction to classifiers</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In traditional classification, a neural network predicts a score for each of <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mi id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><ci id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">N</annotation></semantics></math> classes, and the input is assigned the class with the highest score. For those unfamiliar with classifiers, <cite class="ltx_cite ltx_citemacro_citet">Sanderson (<a href="#bib.bib113" title="" class="ltx_ref">2017</a>)</cite> provides an excellent graphical treatment of classification in the context of classifying images of digits.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Recall the analogy that neural networks are like Legos. Central to the power of transformer models is the ability to use the same pre-trained language model as the backbone for a wide variety of classification tasks. This is illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
A transformer language model produces a vector representation for each token (word or sub-word) in its input, as well as the <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> representation that summarizes the entire input text.
The text sequence can be classified by adding a classifier head to the <span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> representation (panel a). The classifier is a feedforward neural network that aggregates the nodes in the <span id="S6.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> vector into a score for each class using learned weights.
As shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, panel c, individual tokens can likewise be classified by adding classifier heads to their vector representations. Alternatively, two texts can be jointly embedded and then a classifier can be added to the <span id="S6.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> representation to classify the relationship between them (panel b).</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Training a classifier is one of the most straightforward tasks in deep learning. The open-source package <span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> can be used to train text sequence classifiers, with a demo available via EconDL. While the base transformer language model could be frozen when training a classifier, and various layers of the transformer could be used as inputs to the classifier, typically all parameters are allowed to update, with the classifier layer attached to the final layer of the transformer.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Classifier training is a supervised task, and the model must see a sufficient number of examples from each class during training in order to perform well on unlabeled data. When creating labels for classification, the labeled data should be relatively balanced across classes (<span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> positive and negative samples in the case of binary classification).</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">To train a classifier, we also need an appropriate loss function. The two most common losses for classification are Support Vector Machine (SVM) loss, also referred to as hinge loss, and cross-entropy loss.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.2" class="ltx_p">Given a sample with true label <math id="S6.SS1.p6.1.m1.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S6.SS1.p6.1.m1.1a"><msub id="S6.SS1.p6.1.m1.1.1" xref="S6.SS1.p6.1.m1.1.1.cmml"><mi id="S6.SS1.p6.1.m1.1.1.2" xref="S6.SS1.p6.1.m1.1.1.2.cmml">y</mi><mi id="S6.SS1.p6.1.m1.1.1.3" xref="S6.SS1.p6.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.1.m1.1b"><apply id="S6.SS1.p6.1.m1.1.1.cmml" xref="S6.SS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p6.1.m1.1.1.1.cmml" xref="S6.SS1.p6.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p6.1.m1.1.1.2.cmml" xref="S6.SS1.p6.1.m1.1.1.2">𝑦</ci><ci id="S6.SS1.p6.1.m1.1.1.3.cmml" xref="S6.SS1.p6.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.1.m1.1c">y_{i}</annotation></semantics></math> and the score vector <math id="S6.SS1.p6.2.m2.1" class="ltx_Math" alttext="\mathbf{p}" display="inline"><semantics id="S6.SS1.p6.2.m2.1a"><mi id="S6.SS1.p6.2.m2.1.1" xref="S6.SS1.p6.2.m2.1.1.cmml">𝐩</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.2.m2.1b"><ci id="S6.SS1.p6.2.m2.1.1.cmml" xref="S6.SS1.p6.2.m2.1.1">𝐩</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.2.m2.1c">\mathbf{p}</annotation></semantics></math> for the class scores produced by the neural network, the SVM loss is:</p>
</div>
<div id="S6.SS1.p7" class="ltx_para">
<table id="S6.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E2.m1.3" class="ltx_Math" alttext="L_{i}=\sum_{j\neq y_{i}}\max(0,p_{j}-p_{y_{i}}+1)" display="block"><semantics id="S6.E2.m1.3a"><mrow id="S6.E2.m1.3.3" xref="S6.E2.m1.3.3.cmml"><msub id="S6.E2.m1.3.3.3" xref="S6.E2.m1.3.3.3.cmml"><mi id="S6.E2.m1.3.3.3.2" xref="S6.E2.m1.3.3.3.2.cmml">L</mi><mi id="S6.E2.m1.3.3.3.3" xref="S6.E2.m1.3.3.3.3.cmml">i</mi></msub><mo rspace="0.111em" id="S6.E2.m1.3.3.2" xref="S6.E2.m1.3.3.2.cmml">=</mo><mrow id="S6.E2.m1.3.3.1" xref="S6.E2.m1.3.3.1.cmml"><munder id="S6.E2.m1.3.3.1.2" xref="S6.E2.m1.3.3.1.2.cmml"><mo movablelimits="false" id="S6.E2.m1.3.3.1.2.2" xref="S6.E2.m1.3.3.1.2.2.cmml">∑</mo><mrow id="S6.E2.m1.3.3.1.2.3" xref="S6.E2.m1.3.3.1.2.3.cmml"><mi id="S6.E2.m1.3.3.1.2.3.2" xref="S6.E2.m1.3.3.1.2.3.2.cmml">j</mi><mo id="S6.E2.m1.3.3.1.2.3.1" xref="S6.E2.m1.3.3.1.2.3.1.cmml">≠</mo><msub id="S6.E2.m1.3.3.1.2.3.3" xref="S6.E2.m1.3.3.1.2.3.3.cmml"><mi id="S6.E2.m1.3.3.1.2.3.3.2" xref="S6.E2.m1.3.3.1.2.3.3.2.cmml">y</mi><mi id="S6.E2.m1.3.3.1.2.3.3.3" xref="S6.E2.m1.3.3.1.2.3.3.3.cmml">i</mi></msub></mrow></munder><mrow id="S6.E2.m1.3.3.1.1.1" xref="S6.E2.m1.3.3.1.1.2.cmml"><mi id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml">max</mi><mo id="S6.E2.m1.3.3.1.1.1a" xref="S6.E2.m1.3.3.1.1.2.cmml">⁡</mo><mrow id="S6.E2.m1.3.3.1.1.1.1" xref="S6.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S6.E2.m1.3.3.1.1.1.1.2" xref="S6.E2.m1.3.3.1.1.2.cmml">(</mo><mn id="S6.E2.m1.2.2" xref="S6.E2.m1.2.2.cmml">0</mn><mo id="S6.E2.m1.3.3.1.1.1.1.3" xref="S6.E2.m1.3.3.1.1.2.cmml">,</mo><mrow id="S6.E2.m1.3.3.1.1.1.1.1" xref="S6.E2.m1.3.3.1.1.1.1.1.cmml"><mrow id="S6.E2.m1.3.3.1.1.1.1.1.2" xref="S6.E2.m1.3.3.1.1.1.1.1.2.cmml"><msub id="S6.E2.m1.3.3.1.1.1.1.1.2.2" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2.cmml"><mi id="S6.E2.m1.3.3.1.1.1.1.1.2.2.2" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml">p</mi><mi id="S6.E2.m1.3.3.1.1.1.1.1.2.2.3" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S6.E2.m1.3.3.1.1.1.1.1.2.1" xref="S6.E2.m1.3.3.1.1.1.1.1.2.1.cmml">−</mo><msub id="S6.E2.m1.3.3.1.1.1.1.1.2.3" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.cmml"><mi id="S6.E2.m1.3.3.1.1.1.1.1.2.3.2" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.2.cmml">p</mi><msub id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.cmml"><mi id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.2" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.2.cmml">y</mi><mi id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.3" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.3.cmml">i</mi></msub></msub></mrow><mo id="S6.E2.m1.3.3.1.1.1.1.1.1" xref="S6.E2.m1.3.3.1.1.1.1.1.1.cmml">+</mo><mn id="S6.E2.m1.3.3.1.1.1.1.1.3" xref="S6.E2.m1.3.3.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S6.E2.m1.3.3.1.1.1.1.4" xref="S6.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.3b"><apply id="S6.E2.m1.3.3.cmml" xref="S6.E2.m1.3.3"><eq id="S6.E2.m1.3.3.2.cmml" xref="S6.E2.m1.3.3.2"></eq><apply id="S6.E2.m1.3.3.3.cmml" xref="S6.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.3.1.cmml" xref="S6.E2.m1.3.3.3">subscript</csymbol><ci id="S6.E2.m1.3.3.3.2.cmml" xref="S6.E2.m1.3.3.3.2">𝐿</ci><ci id="S6.E2.m1.3.3.3.3.cmml" xref="S6.E2.m1.3.3.3.3">𝑖</ci></apply><apply id="S6.E2.m1.3.3.1.cmml" xref="S6.E2.m1.3.3.1"><apply id="S6.E2.m1.3.3.1.2.cmml" xref="S6.E2.m1.3.3.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.1.2.1.cmml" xref="S6.E2.m1.3.3.1.2">subscript</csymbol><sum id="S6.E2.m1.3.3.1.2.2.cmml" xref="S6.E2.m1.3.3.1.2.2"></sum><apply id="S6.E2.m1.3.3.1.2.3.cmml" xref="S6.E2.m1.3.3.1.2.3"><neq id="S6.E2.m1.3.3.1.2.3.1.cmml" xref="S6.E2.m1.3.3.1.2.3.1"></neq><ci id="S6.E2.m1.3.3.1.2.3.2.cmml" xref="S6.E2.m1.3.3.1.2.3.2">𝑗</ci><apply id="S6.E2.m1.3.3.1.2.3.3.cmml" xref="S6.E2.m1.3.3.1.2.3.3"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.1.2.3.3.1.cmml" xref="S6.E2.m1.3.3.1.2.3.3">subscript</csymbol><ci id="S6.E2.m1.3.3.1.2.3.3.2.cmml" xref="S6.E2.m1.3.3.1.2.3.3.2">𝑦</ci><ci id="S6.E2.m1.3.3.1.2.3.3.3.cmml" xref="S6.E2.m1.3.3.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S6.E2.m1.3.3.1.1.2.cmml" xref="S6.E2.m1.3.3.1.1.1"><max id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"></max><cn type="integer" id="S6.E2.m1.2.2.cmml" xref="S6.E2.m1.2.2">0</cn><apply id="S6.E2.m1.3.3.1.1.1.1.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1"><plus id="S6.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.1"></plus><apply id="S6.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2"><minus id="S6.E2.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.1"></minus><apply id="S6.E2.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S6.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2.2">𝑝</ci><ci id="S6.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.2.3">𝑗</ci></apply><apply id="S6.E2.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.1.1.1.1.1.2.3.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S6.E2.m1.3.3.1.1.1.1.1.2.3.2.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.2">𝑝</ci><apply id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.1.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.2.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.2">𝑦</ci><ci id="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.3.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S6.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S6.E2.m1.3.3.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.3c">L_{i}=\sum_{j\neq y_{i}}\max(0,p_{j}-p_{y_{i}}+1)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S6.SS1.p8" class="ltx_para">
<p id="S6.SS1.p8.1" class="ltx_p">The loss sums over the incorrect classes, imposing a penalty if the score of the correct class is not at least some threshold amount above the score(s) for the incorrect class(es). The threshold can be set to one without loss of generality, as it just scales the learned weights.</p>
</div>
<div id="S6.SS1.p9" class="ltx_para">
<p id="S6.SS1.p9.8" class="ltx_p">Cross-entropy loss measures the dissimilarity between the predicted score distribution and the true distribution. Consider a neural network used for a multi-class classification problem with <math id="S6.SS1.p9.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S6.SS1.p9.1.m1.1a"><mi id="S6.SS1.p9.1.m1.1.1" xref="S6.SS1.p9.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.1.m1.1b"><ci id="S6.SS1.p9.1.m1.1.1.cmml" xref="S6.SS1.p9.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.1.m1.1c">C</annotation></semantics></math> classes. Let <math id="S6.SS1.p9.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.p9.2.m2.1a"><mi id="S6.SS1.p9.2.m2.1.1" xref="S6.SS1.p9.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.2.m2.1b"><ci id="S6.SS1.p9.2.m2.1.1.cmml" xref="S6.SS1.p9.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.2.m2.1c">y</annotation></semantics></math> be the true label of a sample, represented as a one-hot encoded vector. For a sample belonging to class <math id="S6.SS1.p9.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S6.SS1.p9.3.m3.1a"><mi id="S6.SS1.p9.3.m3.1.1" xref="S6.SS1.p9.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.3.m3.1b"><ci id="S6.SS1.p9.3.m3.1.1.cmml" xref="S6.SS1.p9.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.3.m3.1c">i</annotation></semantics></math>, <math id="S6.SS1.p9.4.m4.1" class="ltx_Math" alttext="y_{i}=1" display="inline"><semantics id="S6.SS1.p9.4.m4.1a"><mrow id="S6.SS1.p9.4.m4.1.1" xref="S6.SS1.p9.4.m4.1.1.cmml"><msub id="S6.SS1.p9.4.m4.1.1.2" xref="S6.SS1.p9.4.m4.1.1.2.cmml"><mi id="S6.SS1.p9.4.m4.1.1.2.2" xref="S6.SS1.p9.4.m4.1.1.2.2.cmml">y</mi><mi id="S6.SS1.p9.4.m4.1.1.2.3" xref="S6.SS1.p9.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S6.SS1.p9.4.m4.1.1.1" xref="S6.SS1.p9.4.m4.1.1.1.cmml">=</mo><mn id="S6.SS1.p9.4.m4.1.1.3" xref="S6.SS1.p9.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.4.m4.1b"><apply id="S6.SS1.p9.4.m4.1.1.cmml" xref="S6.SS1.p9.4.m4.1.1"><eq id="S6.SS1.p9.4.m4.1.1.1.cmml" xref="S6.SS1.p9.4.m4.1.1.1"></eq><apply id="S6.SS1.p9.4.m4.1.1.2.cmml" xref="S6.SS1.p9.4.m4.1.1.2"><csymbol cd="ambiguous" id="S6.SS1.p9.4.m4.1.1.2.1.cmml" xref="S6.SS1.p9.4.m4.1.1.2">subscript</csymbol><ci id="S6.SS1.p9.4.m4.1.1.2.2.cmml" xref="S6.SS1.p9.4.m4.1.1.2.2">𝑦</ci><ci id="S6.SS1.p9.4.m4.1.1.2.3.cmml" xref="S6.SS1.p9.4.m4.1.1.2.3">𝑖</ci></apply><cn type="integer" id="S6.SS1.p9.4.m4.1.1.3.cmml" xref="S6.SS1.p9.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.4.m4.1c">y_{i}=1</annotation></semantics></math> and <math id="S6.SS1.p9.5.m5.1" class="ltx_Math" alttext="y_{j}=0" display="inline"><semantics id="S6.SS1.p9.5.m5.1a"><mrow id="S6.SS1.p9.5.m5.1.1" xref="S6.SS1.p9.5.m5.1.1.cmml"><msub id="S6.SS1.p9.5.m5.1.1.2" xref="S6.SS1.p9.5.m5.1.1.2.cmml"><mi id="S6.SS1.p9.5.m5.1.1.2.2" xref="S6.SS1.p9.5.m5.1.1.2.2.cmml">y</mi><mi id="S6.SS1.p9.5.m5.1.1.2.3" xref="S6.SS1.p9.5.m5.1.1.2.3.cmml">j</mi></msub><mo id="S6.SS1.p9.5.m5.1.1.1" xref="S6.SS1.p9.5.m5.1.1.1.cmml">=</mo><mn id="S6.SS1.p9.5.m5.1.1.3" xref="S6.SS1.p9.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.5.m5.1b"><apply id="S6.SS1.p9.5.m5.1.1.cmml" xref="S6.SS1.p9.5.m5.1.1"><eq id="S6.SS1.p9.5.m5.1.1.1.cmml" xref="S6.SS1.p9.5.m5.1.1.1"></eq><apply id="S6.SS1.p9.5.m5.1.1.2.cmml" xref="S6.SS1.p9.5.m5.1.1.2"><csymbol cd="ambiguous" id="S6.SS1.p9.5.m5.1.1.2.1.cmml" xref="S6.SS1.p9.5.m5.1.1.2">subscript</csymbol><ci id="S6.SS1.p9.5.m5.1.1.2.2.cmml" xref="S6.SS1.p9.5.m5.1.1.2.2">𝑦</ci><ci id="S6.SS1.p9.5.m5.1.1.2.3.cmml" xref="S6.SS1.p9.5.m5.1.1.2.3">𝑗</ci></apply><cn type="integer" id="S6.SS1.p9.5.m5.1.1.3.cmml" xref="S6.SS1.p9.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.5.m5.1c">y_{j}=0</annotation></semantics></math> for <math id="S6.SS1.p9.6.m6.1" class="ltx_Math" alttext="j\neq i" display="inline"><semantics id="S6.SS1.p9.6.m6.1a"><mrow id="S6.SS1.p9.6.m6.1.1" xref="S6.SS1.p9.6.m6.1.1.cmml"><mi id="S6.SS1.p9.6.m6.1.1.2" xref="S6.SS1.p9.6.m6.1.1.2.cmml">j</mi><mo id="S6.SS1.p9.6.m6.1.1.1" xref="S6.SS1.p9.6.m6.1.1.1.cmml">≠</mo><mi id="S6.SS1.p9.6.m6.1.1.3" xref="S6.SS1.p9.6.m6.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.6.m6.1b"><apply id="S6.SS1.p9.6.m6.1.1.cmml" xref="S6.SS1.p9.6.m6.1.1"><neq id="S6.SS1.p9.6.m6.1.1.1.cmml" xref="S6.SS1.p9.6.m6.1.1.1"></neq><ci id="S6.SS1.p9.6.m6.1.1.2.cmml" xref="S6.SS1.p9.6.m6.1.1.2">𝑗</ci><ci id="S6.SS1.p9.6.m6.1.1.3.cmml" xref="S6.SS1.p9.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.6.m6.1c">j\neq i</annotation></semantics></math>. Let <math id="S6.SS1.p9.7.m7.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S6.SS1.p9.7.m7.1a"><mi id="S6.SS1.p9.7.m7.1.1" xref="S6.SS1.p9.7.m7.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.7.m7.1b"><ci id="S6.SS1.p9.7.m7.1.1.cmml" xref="S6.SS1.p9.7.m7.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.7.m7.1c">\mathbf{z}</annotation></semantics></math> be the raw numbers (often referred to as logits) produced by the neural network for that sample. A classification layer will produce one score for each class. The predicted score of class <math id="S6.SS1.p9.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S6.SS1.p9.8.m8.1a"><mi id="S6.SS1.p9.8.m8.1.1" xref="S6.SS1.p9.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p9.8.m8.1b"><ci id="S6.SS1.p9.8.m8.1.1.cmml" xref="S6.SS1.p9.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p9.8.m8.1c">i</annotation></semantics></math>, obtained using the softmax function, is:</p>
</div>
<div id="S6.SS1.p10" class="ltx_para">
<table id="S6.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E3.m1.1" class="ltx_Math" alttext="p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{C}e^{z_{j}}}" display="block"><semantics id="S6.E3.m1.1a"><mrow id="S6.E3.m1.1.1" xref="S6.E3.m1.1.1.cmml"><msub id="S6.E3.m1.1.1.2" xref="S6.E3.m1.1.1.2.cmml"><mi id="S6.E3.m1.1.1.2.2" xref="S6.E3.m1.1.1.2.2.cmml">p</mi><mi id="S6.E3.m1.1.1.2.3" xref="S6.E3.m1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.1.1.1" xref="S6.E3.m1.1.1.1.cmml">=</mo><mfrac id="S6.E3.m1.1.1.3" xref="S6.E3.m1.1.1.3.cmml"><msup id="S6.E3.m1.1.1.3.2" xref="S6.E3.m1.1.1.3.2.cmml"><mi id="S6.E3.m1.1.1.3.2.2" xref="S6.E3.m1.1.1.3.2.2.cmml">e</mi><msub id="S6.E3.m1.1.1.3.2.3" xref="S6.E3.m1.1.1.3.2.3.cmml"><mi id="S6.E3.m1.1.1.3.2.3.2" xref="S6.E3.m1.1.1.3.2.3.2.cmml">z</mi><mi id="S6.E3.m1.1.1.3.2.3.3" xref="S6.E3.m1.1.1.3.2.3.3.cmml">i</mi></msub></msup><mrow id="S6.E3.m1.1.1.3.3" xref="S6.E3.m1.1.1.3.3.cmml"><msubsup id="S6.E3.m1.1.1.3.3.1" xref="S6.E3.m1.1.1.3.3.1.cmml"><mo id="S6.E3.m1.1.1.3.3.1.2.2" xref="S6.E3.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S6.E3.m1.1.1.3.3.1.2.3" xref="S6.E3.m1.1.1.3.3.1.2.3.cmml"><mi id="S6.E3.m1.1.1.3.3.1.2.3.2" xref="S6.E3.m1.1.1.3.3.1.2.3.2.cmml">j</mi><mo id="S6.E3.m1.1.1.3.3.1.2.3.1" xref="S6.E3.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S6.E3.m1.1.1.3.3.1.2.3.3" xref="S6.E3.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S6.E3.m1.1.1.3.3.1.3" xref="S6.E3.m1.1.1.3.3.1.3.cmml">C</mi></msubsup><msup id="S6.E3.m1.1.1.3.3.2" xref="S6.E3.m1.1.1.3.3.2.cmml"><mi id="S6.E3.m1.1.1.3.3.2.2" xref="S6.E3.m1.1.1.3.3.2.2.cmml">e</mi><msub id="S6.E3.m1.1.1.3.3.2.3" xref="S6.E3.m1.1.1.3.3.2.3.cmml"><mi id="S6.E3.m1.1.1.3.3.2.3.2" xref="S6.E3.m1.1.1.3.3.2.3.2.cmml">z</mi><mi id="S6.E3.m1.1.1.3.3.2.3.3" xref="S6.E3.m1.1.1.3.3.2.3.3.cmml">j</mi></msub></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.1b"><apply id="S6.E3.m1.1.1.cmml" xref="S6.E3.m1.1.1"><eq id="S6.E3.m1.1.1.1.cmml" xref="S6.E3.m1.1.1.1"></eq><apply id="S6.E3.m1.1.1.2.cmml" xref="S6.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.2.2">𝑝</ci><ci id="S6.E3.m1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.1.1.3.cmml" xref="S6.E3.m1.1.1.3"><divide id="S6.E3.m1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.3"></divide><apply id="S6.E3.m1.1.1.3.2.cmml" xref="S6.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.2.1.cmml" xref="S6.E3.m1.1.1.3.2">superscript</csymbol><ci id="S6.E3.m1.1.1.3.2.2.cmml" xref="S6.E3.m1.1.1.3.2.2">𝑒</ci><apply id="S6.E3.m1.1.1.3.2.3.cmml" xref="S6.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.2.3.1.cmml" xref="S6.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S6.E3.m1.1.1.3.2.3.2.cmml" xref="S6.E3.m1.1.1.3.2.3.2">𝑧</ci><ci id="S6.E3.m1.1.1.3.2.3.3.cmml" xref="S6.E3.m1.1.1.3.2.3.3">𝑖</ci></apply></apply><apply id="S6.E3.m1.1.1.3.3.cmml" xref="S6.E3.m1.1.1.3.3"><apply id="S6.E3.m1.1.1.3.3.1.cmml" xref="S6.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.1.1.cmml" xref="S6.E3.m1.1.1.3.3.1">superscript</csymbol><apply id="S6.E3.m1.1.1.3.3.1.2.cmml" xref="S6.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.1.2.1.cmml" xref="S6.E3.m1.1.1.3.3.1">subscript</csymbol><sum id="S6.E3.m1.1.1.3.3.1.2.2.cmml" xref="S6.E3.m1.1.1.3.3.1.2.2"></sum><apply id="S6.E3.m1.1.1.3.3.1.2.3.cmml" xref="S6.E3.m1.1.1.3.3.1.2.3"><eq id="S6.E3.m1.1.1.3.3.1.2.3.1.cmml" xref="S6.E3.m1.1.1.3.3.1.2.3.1"></eq><ci id="S6.E3.m1.1.1.3.3.1.2.3.2.cmml" xref="S6.E3.m1.1.1.3.3.1.2.3.2">𝑗</ci><cn type="integer" id="S6.E3.m1.1.1.3.3.1.2.3.3.cmml" xref="S6.E3.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S6.E3.m1.1.1.3.3.1.3.cmml" xref="S6.E3.m1.1.1.3.3.1.3">𝐶</ci></apply><apply id="S6.E3.m1.1.1.3.3.2.cmml" xref="S6.E3.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.2.1.cmml" xref="S6.E3.m1.1.1.3.3.2">superscript</csymbol><ci id="S6.E3.m1.1.1.3.3.2.2.cmml" xref="S6.E3.m1.1.1.3.3.2.2">𝑒</ci><apply id="S6.E3.m1.1.1.3.3.2.3.cmml" xref="S6.E3.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.2.3.1.cmml" xref="S6.E3.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S6.E3.m1.1.1.3.3.2.3.2.cmml" xref="S6.E3.m1.1.1.3.3.2.3.2">𝑧</ci><ci id="S6.E3.m1.1.1.3.3.2.3.3.cmml" xref="S6.E3.m1.1.1.3.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.1c">p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{C}e^{z_{j}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S6.SS1.p11" class="ltx_para">
<p id="S6.SS1.p11.1" class="ltx_p">While the class scores are frequently referred to in the literature as ‘probabilities’, they are not probabilities in a statistical sense. How peaky they are will depend on the regularization of the neural network.</p>
</div>
<div id="S6.SS1.p12" class="ltx_para">
<p id="S6.SS1.p12.1" class="ltx_p">The cross-entropy loss between the true label and the predicted distribution is:</p>
<table id="S6.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E4.m1.4" class="ltx_Math" alttext="\text{CE}(y,p)=-\sum_{i=1}^{C}y_{i}\log(p_{i})" display="block"><semantics id="S6.E4.m1.4a"><mrow id="S6.E4.m1.4.4" xref="S6.E4.m1.4.4.cmml"><mrow id="S6.E4.m1.4.4.3" xref="S6.E4.m1.4.4.3.cmml"><mtext id="S6.E4.m1.4.4.3.2" xref="S6.E4.m1.4.4.3.2a.cmml">CE</mtext><mo lspace="0em" rspace="0em" id="S6.E4.m1.4.4.3.1" xref="S6.E4.m1.4.4.3.1.cmml">​</mo><mrow id="S6.E4.m1.4.4.3.3.2" xref="S6.E4.m1.4.4.3.3.1.cmml"><mo stretchy="false" id="S6.E4.m1.4.4.3.3.2.1" xref="S6.E4.m1.4.4.3.3.1.cmml">(</mo><mi id="S6.E4.m1.1.1" xref="S6.E4.m1.1.1.cmml">y</mi><mo id="S6.E4.m1.4.4.3.3.2.2" xref="S6.E4.m1.4.4.3.3.1.cmml">,</mo><mi id="S6.E4.m1.2.2" xref="S6.E4.m1.2.2.cmml">p</mi><mo stretchy="false" id="S6.E4.m1.4.4.3.3.2.3" xref="S6.E4.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S6.E4.m1.4.4.2" xref="S6.E4.m1.4.4.2.cmml">=</mo><mrow id="S6.E4.m1.4.4.1" xref="S6.E4.m1.4.4.1.cmml"><mo id="S6.E4.m1.4.4.1a" xref="S6.E4.m1.4.4.1.cmml">−</mo><mrow id="S6.E4.m1.4.4.1.1" xref="S6.E4.m1.4.4.1.1.cmml"><munderover id="S6.E4.m1.4.4.1.1.2" xref="S6.E4.m1.4.4.1.1.2.cmml"><mo movablelimits="false" id="S6.E4.m1.4.4.1.1.2.2.2" xref="S6.E4.m1.4.4.1.1.2.2.2.cmml">∑</mo><mrow id="S6.E4.m1.4.4.1.1.2.2.3" xref="S6.E4.m1.4.4.1.1.2.2.3.cmml"><mi id="S6.E4.m1.4.4.1.1.2.2.3.2" xref="S6.E4.m1.4.4.1.1.2.2.3.2.cmml">i</mi><mo id="S6.E4.m1.4.4.1.1.2.2.3.1" xref="S6.E4.m1.4.4.1.1.2.2.3.1.cmml">=</mo><mn id="S6.E4.m1.4.4.1.1.2.2.3.3" xref="S6.E4.m1.4.4.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S6.E4.m1.4.4.1.1.2.3" xref="S6.E4.m1.4.4.1.1.2.3.cmml">C</mi></munderover><mrow id="S6.E4.m1.4.4.1.1.1" xref="S6.E4.m1.4.4.1.1.1.cmml"><msub id="S6.E4.m1.4.4.1.1.1.3" xref="S6.E4.m1.4.4.1.1.1.3.cmml"><mi id="S6.E4.m1.4.4.1.1.1.3.2" xref="S6.E4.m1.4.4.1.1.1.3.2.cmml">y</mi><mi id="S6.E4.m1.4.4.1.1.1.3.3" xref="S6.E4.m1.4.4.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.167em" rspace="0em" id="S6.E4.m1.4.4.1.1.1.2" xref="S6.E4.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S6.E4.m1.4.4.1.1.1.1.1" xref="S6.E4.m1.4.4.1.1.1.1.2.cmml"><mi id="S6.E4.m1.3.3" xref="S6.E4.m1.3.3.cmml">log</mi><mo id="S6.E4.m1.4.4.1.1.1.1.1a" xref="S6.E4.m1.4.4.1.1.1.1.2.cmml">⁡</mo><mrow id="S6.E4.m1.4.4.1.1.1.1.1.1" xref="S6.E4.m1.4.4.1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E4.m1.4.4.1.1.1.1.1.1.2" xref="S6.E4.m1.4.4.1.1.1.1.2.cmml">(</mo><msub id="S6.E4.m1.4.4.1.1.1.1.1.1.1" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S6.E4.m1.4.4.1.1.1.1.1.1.1.2" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S6.E4.m1.4.4.1.1.1.1.1.1.1.3" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S6.E4.m1.4.4.1.1.1.1.1.1.3" xref="S6.E4.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E4.m1.4b"><apply id="S6.E4.m1.4.4.cmml" xref="S6.E4.m1.4.4"><eq id="S6.E4.m1.4.4.2.cmml" xref="S6.E4.m1.4.4.2"></eq><apply id="S6.E4.m1.4.4.3.cmml" xref="S6.E4.m1.4.4.3"><times id="S6.E4.m1.4.4.3.1.cmml" xref="S6.E4.m1.4.4.3.1"></times><ci id="S6.E4.m1.4.4.3.2a.cmml" xref="S6.E4.m1.4.4.3.2"><mtext id="S6.E4.m1.4.4.3.2.cmml" xref="S6.E4.m1.4.4.3.2">CE</mtext></ci><interval closure="open" id="S6.E4.m1.4.4.3.3.1.cmml" xref="S6.E4.m1.4.4.3.3.2"><ci id="S6.E4.m1.1.1.cmml" xref="S6.E4.m1.1.1">𝑦</ci><ci id="S6.E4.m1.2.2.cmml" xref="S6.E4.m1.2.2">𝑝</ci></interval></apply><apply id="S6.E4.m1.4.4.1.cmml" xref="S6.E4.m1.4.4.1"><minus id="S6.E4.m1.4.4.1.2.cmml" xref="S6.E4.m1.4.4.1"></minus><apply id="S6.E4.m1.4.4.1.1.cmml" xref="S6.E4.m1.4.4.1.1"><apply id="S6.E4.m1.4.4.1.1.2.cmml" xref="S6.E4.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S6.E4.m1.4.4.1.1.2.1.cmml" xref="S6.E4.m1.4.4.1.1.2">superscript</csymbol><apply id="S6.E4.m1.4.4.1.1.2.2.cmml" xref="S6.E4.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S6.E4.m1.4.4.1.1.2.2.1.cmml" xref="S6.E4.m1.4.4.1.1.2">subscript</csymbol><sum id="S6.E4.m1.4.4.1.1.2.2.2.cmml" xref="S6.E4.m1.4.4.1.1.2.2.2"></sum><apply id="S6.E4.m1.4.4.1.1.2.2.3.cmml" xref="S6.E4.m1.4.4.1.1.2.2.3"><eq id="S6.E4.m1.4.4.1.1.2.2.3.1.cmml" xref="S6.E4.m1.4.4.1.1.2.2.3.1"></eq><ci id="S6.E4.m1.4.4.1.1.2.2.3.2.cmml" xref="S6.E4.m1.4.4.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S6.E4.m1.4.4.1.1.2.2.3.3.cmml" xref="S6.E4.m1.4.4.1.1.2.2.3.3">1</cn></apply></apply><ci id="S6.E4.m1.4.4.1.1.2.3.cmml" xref="S6.E4.m1.4.4.1.1.2.3">𝐶</ci></apply><apply id="S6.E4.m1.4.4.1.1.1.cmml" xref="S6.E4.m1.4.4.1.1.1"><times id="S6.E4.m1.4.4.1.1.1.2.cmml" xref="S6.E4.m1.4.4.1.1.1.2"></times><apply id="S6.E4.m1.4.4.1.1.1.3.cmml" xref="S6.E4.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S6.E4.m1.4.4.1.1.1.3.1.cmml" xref="S6.E4.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S6.E4.m1.4.4.1.1.1.3.2.cmml" xref="S6.E4.m1.4.4.1.1.1.3.2">𝑦</ci><ci id="S6.E4.m1.4.4.1.1.1.3.3.cmml" xref="S6.E4.m1.4.4.1.1.1.3.3">𝑖</ci></apply><apply id="S6.E4.m1.4.4.1.1.1.1.2.cmml" xref="S6.E4.m1.4.4.1.1.1.1.1"><log id="S6.E4.m1.3.3.cmml" xref="S6.E4.m1.3.3"></log><apply id="S6.E4.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S6.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S6.E4.m1.4.4.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E4.m1.4c">\text{CE}(y,p)=-\sum_{i=1}^{C}y_{i}\log(p_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S6.SS1.p13" class="ltx_para">
<p id="S6.SS1.p13.1" class="ltx_p">As <math id="S6.SS1.p13.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.p13.1.m1.1a"><mi id="S6.SS1.p13.1.m1.1.1" xref="S6.SS1.p13.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p13.1.m1.1b"><ci id="S6.SS1.p13.1.m1.1.1.cmml" xref="S6.SS1.p13.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p13.1.m1.1c">y</annotation></semantics></math> are one hot vectors, this simplifies to:</p>
</div>
<div id="S6.SS1.p14" class="ltx_para">
<table id="S6.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E5.m1.4" class="ltx_Math" alttext="\text{CE}(y,p)=-\log(p_{\text{true class}})" display="block"><semantics id="S6.E5.m1.4a"><mrow id="S6.E5.m1.4.4" xref="S6.E5.m1.4.4.cmml"><mrow id="S6.E5.m1.4.4.3" xref="S6.E5.m1.4.4.3.cmml"><mtext id="S6.E5.m1.4.4.3.2" xref="S6.E5.m1.4.4.3.2a.cmml">CE</mtext><mo lspace="0em" rspace="0em" id="S6.E5.m1.4.4.3.1" xref="S6.E5.m1.4.4.3.1.cmml">​</mo><mrow id="S6.E5.m1.4.4.3.3.2" xref="S6.E5.m1.4.4.3.3.1.cmml"><mo stretchy="false" id="S6.E5.m1.4.4.3.3.2.1" xref="S6.E5.m1.4.4.3.3.1.cmml">(</mo><mi id="S6.E5.m1.1.1" xref="S6.E5.m1.1.1.cmml">y</mi><mo id="S6.E5.m1.4.4.3.3.2.2" xref="S6.E5.m1.4.4.3.3.1.cmml">,</mo><mi id="S6.E5.m1.2.2" xref="S6.E5.m1.2.2.cmml">p</mi><mo stretchy="false" id="S6.E5.m1.4.4.3.3.2.3" xref="S6.E5.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S6.E5.m1.4.4.2" xref="S6.E5.m1.4.4.2.cmml">=</mo><mrow id="S6.E5.m1.4.4.1" xref="S6.E5.m1.4.4.1.cmml"><mo rspace="0.167em" id="S6.E5.m1.4.4.1a" xref="S6.E5.m1.4.4.1.cmml">−</mo><mrow id="S6.E5.m1.4.4.1.1.1" xref="S6.E5.m1.4.4.1.1.2.cmml"><mi id="S6.E5.m1.3.3" xref="S6.E5.m1.3.3.cmml">log</mi><mo id="S6.E5.m1.4.4.1.1.1a" xref="S6.E5.m1.4.4.1.1.2.cmml">⁡</mo><mrow id="S6.E5.m1.4.4.1.1.1.1" xref="S6.E5.m1.4.4.1.1.2.cmml"><mo stretchy="false" id="S6.E5.m1.4.4.1.1.1.1.2" xref="S6.E5.m1.4.4.1.1.2.cmml">(</mo><msub id="S6.E5.m1.4.4.1.1.1.1.1" xref="S6.E5.m1.4.4.1.1.1.1.1.cmml"><mi id="S6.E5.m1.4.4.1.1.1.1.1.2" xref="S6.E5.m1.4.4.1.1.1.1.1.2.cmml">p</mi><mtext id="S6.E5.m1.4.4.1.1.1.1.1.3" xref="S6.E5.m1.4.4.1.1.1.1.1.3a.cmml">true class</mtext></msub><mo stretchy="false" id="S6.E5.m1.4.4.1.1.1.1.3" xref="S6.E5.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E5.m1.4b"><apply id="S6.E5.m1.4.4.cmml" xref="S6.E5.m1.4.4"><eq id="S6.E5.m1.4.4.2.cmml" xref="S6.E5.m1.4.4.2"></eq><apply id="S6.E5.m1.4.4.3.cmml" xref="S6.E5.m1.4.4.3"><times id="S6.E5.m1.4.4.3.1.cmml" xref="S6.E5.m1.4.4.3.1"></times><ci id="S6.E5.m1.4.4.3.2a.cmml" xref="S6.E5.m1.4.4.3.2"><mtext id="S6.E5.m1.4.4.3.2.cmml" xref="S6.E5.m1.4.4.3.2">CE</mtext></ci><interval closure="open" id="S6.E5.m1.4.4.3.3.1.cmml" xref="S6.E5.m1.4.4.3.3.2"><ci id="S6.E5.m1.1.1.cmml" xref="S6.E5.m1.1.1">𝑦</ci><ci id="S6.E5.m1.2.2.cmml" xref="S6.E5.m1.2.2">𝑝</ci></interval></apply><apply id="S6.E5.m1.4.4.1.cmml" xref="S6.E5.m1.4.4.1"><minus id="S6.E5.m1.4.4.1.2.cmml" xref="S6.E5.m1.4.4.1"></minus><apply id="S6.E5.m1.4.4.1.1.2.cmml" xref="S6.E5.m1.4.4.1.1.1"><log id="S6.E5.m1.3.3.cmml" xref="S6.E5.m1.3.3"></log><apply id="S6.E5.m1.4.4.1.1.1.1.1.cmml" xref="S6.E5.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E5.m1.4.4.1.1.1.1.1.1.cmml" xref="S6.E5.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S6.E5.m1.4.4.1.1.1.1.1.2.cmml" xref="S6.E5.m1.4.4.1.1.1.1.1.2">𝑝</ci><ci id="S6.E5.m1.4.4.1.1.1.1.1.3a.cmml" xref="S6.E5.m1.4.4.1.1.1.1.1.3"><mtext mathsize="70%" id="S6.E5.m1.4.4.1.1.1.1.1.3.cmml" xref="S6.E5.m1.4.4.1.1.1.1.1.3">true class</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E5.m1.4c">\text{CE}(y,p)=-\log(p_{\text{true class}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S6.SS1.p14.1" class="ltx_p">where <math id="S6.SS1.p14.1.m1.1" class="ltx_Math" alttext="p_{\text{true class}}" display="inline"><semantics id="S6.SS1.p14.1.m1.1a"><msub id="S6.SS1.p14.1.m1.1.1" xref="S6.SS1.p14.1.m1.1.1.cmml"><mi id="S6.SS1.p14.1.m1.1.1.2" xref="S6.SS1.p14.1.m1.1.1.2.cmml">p</mi><mtext id="S6.SS1.p14.1.m1.1.1.3" xref="S6.SS1.p14.1.m1.1.1.3a.cmml">true class</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p14.1.m1.1b"><apply id="S6.SS1.p14.1.m1.1.1.cmml" xref="S6.SS1.p14.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p14.1.m1.1.1.1.cmml" xref="S6.SS1.p14.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p14.1.m1.1.1.2.cmml" xref="S6.SS1.p14.1.m1.1.1.2">𝑝</ci><ci id="S6.SS1.p14.1.m1.1.1.3a.cmml" xref="S6.SS1.p14.1.m1.1.1.3"><mtext mathsize="70%" id="S6.SS1.p14.1.m1.1.1.3.cmml" xref="S6.SS1.p14.1.m1.1.1.3">true class</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p14.1.m1.1c">p_{\text{true class}}</annotation></semantics></math> is the predicted probability of the correct class.</p>
</div>
<div id="S6.SS1.p15" class="ltx_para">
<p id="S6.SS1.p15.1" class="ltx_p">With SVM loss, once the score for the correct class surpasses a threshold, it is indifferent to elevating the score of the correct class further. On the other hand, the cross-entropy loss pushes the correct class towards 1. This means that during the early stages of training, accuracy might increase suddenly without a significant change in the loss. In real-world scenarios, both losses typically yield comparable results.</p>
</div>
<div id="S6.SS1.p16" class="ltx_para">
<p id="S6.SS1.p16.1" class="ltx_p">Binary classifiers are evaluated using the F1 score, a metric that combines recall (true positives divided by true positives plus false negatives) and precision (true positives divided by true positives plus false positives).</p>
</div>
<div id="S6.SS1.p17" class="ltx_para">
<table id="S6.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E6.m1.1" class="ltx_Math" alttext="F1=2\times\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}" display="block"><semantics id="S6.E6.m1.1a"><mrow id="S6.E6.m1.1.1" xref="S6.E6.m1.1.1.cmml"><mrow id="S6.E6.m1.1.1.2" xref="S6.E6.m1.1.1.2.cmml"><mi id="S6.E6.m1.1.1.2.2" xref="S6.E6.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S6.E6.m1.1.1.2.1" xref="S6.E6.m1.1.1.2.1.cmml">​</mo><mn id="S6.E6.m1.1.1.2.3" xref="S6.E6.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S6.E6.m1.1.1.1" xref="S6.E6.m1.1.1.1.cmml">=</mo><mrow id="S6.E6.m1.1.1.3" xref="S6.E6.m1.1.1.3.cmml"><mn id="S6.E6.m1.1.1.3.2" xref="S6.E6.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S6.E6.m1.1.1.3.1" xref="S6.E6.m1.1.1.3.1.cmml">×</mo><mfrac id="S6.E6.m1.1.1.3.3" xref="S6.E6.m1.1.1.3.3.cmml"><mrow id="S6.E6.m1.1.1.3.3.2" xref="S6.E6.m1.1.1.3.3.2.cmml"><mtext id="S6.E6.m1.1.1.3.3.2.2" xref="S6.E6.m1.1.1.3.3.2.2a.cmml">precision</mtext><mo lspace="0.222em" rspace="0.222em" id="S6.E6.m1.1.1.3.3.2.1" xref="S6.E6.m1.1.1.3.3.2.1.cmml">×</mo><mtext id="S6.E6.m1.1.1.3.3.2.3" xref="S6.E6.m1.1.1.3.3.2.3a.cmml">recall</mtext></mrow><mrow id="S6.E6.m1.1.1.3.3.3" xref="S6.E6.m1.1.1.3.3.3.cmml"><mtext id="S6.E6.m1.1.1.3.3.3.2" xref="S6.E6.m1.1.1.3.3.3.2a.cmml">precision</mtext><mo id="S6.E6.m1.1.1.3.3.3.1" xref="S6.E6.m1.1.1.3.3.3.1.cmml">+</mo><mtext id="S6.E6.m1.1.1.3.3.3.3" xref="S6.E6.m1.1.1.3.3.3.3a.cmml">recall</mtext></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E6.m1.1b"><apply id="S6.E6.m1.1.1.cmml" xref="S6.E6.m1.1.1"><eq id="S6.E6.m1.1.1.1.cmml" xref="S6.E6.m1.1.1.1"></eq><apply id="S6.E6.m1.1.1.2.cmml" xref="S6.E6.m1.1.1.2"><times id="S6.E6.m1.1.1.2.1.cmml" xref="S6.E6.m1.1.1.2.1"></times><ci id="S6.E6.m1.1.1.2.2.cmml" xref="S6.E6.m1.1.1.2.2">𝐹</ci><cn type="integer" id="S6.E6.m1.1.1.2.3.cmml" xref="S6.E6.m1.1.1.2.3">1</cn></apply><apply id="S6.E6.m1.1.1.3.cmml" xref="S6.E6.m1.1.1.3"><times id="S6.E6.m1.1.1.3.1.cmml" xref="S6.E6.m1.1.1.3.1"></times><cn type="integer" id="S6.E6.m1.1.1.3.2.cmml" xref="S6.E6.m1.1.1.3.2">2</cn><apply id="S6.E6.m1.1.1.3.3.cmml" xref="S6.E6.m1.1.1.3.3"><divide id="S6.E6.m1.1.1.3.3.1.cmml" xref="S6.E6.m1.1.1.3.3"></divide><apply id="S6.E6.m1.1.1.3.3.2.cmml" xref="S6.E6.m1.1.1.3.3.2"><times id="S6.E6.m1.1.1.3.3.2.1.cmml" xref="S6.E6.m1.1.1.3.3.2.1"></times><ci id="S6.E6.m1.1.1.3.3.2.2a.cmml" xref="S6.E6.m1.1.1.3.3.2.2"><mtext id="S6.E6.m1.1.1.3.3.2.2.cmml" xref="S6.E6.m1.1.1.3.3.2.2">precision</mtext></ci><ci id="S6.E6.m1.1.1.3.3.2.3a.cmml" xref="S6.E6.m1.1.1.3.3.2.3"><mtext id="S6.E6.m1.1.1.3.3.2.3.cmml" xref="S6.E6.m1.1.1.3.3.2.3">recall</mtext></ci></apply><apply id="S6.E6.m1.1.1.3.3.3.cmml" xref="S6.E6.m1.1.1.3.3.3"><plus id="S6.E6.m1.1.1.3.3.3.1.cmml" xref="S6.E6.m1.1.1.3.3.3.1"></plus><ci id="S6.E6.m1.1.1.3.3.3.2a.cmml" xref="S6.E6.m1.1.1.3.3.3.2"><mtext id="S6.E6.m1.1.1.3.3.3.2.cmml" xref="S6.E6.m1.1.1.3.3.3.2">precision</mtext></ci><ci id="S6.E6.m1.1.1.3.3.3.3a.cmml" xref="S6.E6.m1.1.1.3.3.3.3"><mtext id="S6.E6.m1.1.1.3.3.3.3.cmml" xref="S6.E6.m1.1.1.3.3.3.3">recall</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E6.m1.1c">F1=2\times\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S6.SS1.p18" class="ltx_para">
<p id="S6.SS1.p18.1" class="ltx_p">Perfect precision and recall yield an F1 score of 1, whereas the worst score is 0. F1 is a harmonic mean of precision and recall and thus tends to be closer to the smaller of these two metrics. If either the precision or the recall is low, the F1 score will also be low. F1 is preferred over accuracy because if classes are imbalanced, accuracy can be high simply by always predicting the majority class.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Generative AI for classification</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Large generative AI models like GPT, Claude, or Llama (commonly referred to in the literature as <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">foundation models</span>) use a decoder transformer architecture (Section <a href="#S2.SS5" title="2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>) to autoregressively generate text given a prompt. In practice, they might also be connected to an external database (<span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> the internet) via a retrieval-augmented language modeling (RALM) setup (the post on retrieval in EconDL contains more information about RALM). In a fundamental sense, these models are performing classification, autoregressively predicting the most likely next token in a discrete vocabulary at each time step. By default, models like GPT are stochastic; they predict the next token from a distribution of the most probable tokens.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>At the time of writing, setting <span id="footnote1.1" class="ltx_text ltx_font_typewriter">top_p</span> to 0 makes GPT deterministic.</span></span></span></p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">To perform classification tasks using generative AI, the user needs to prompt the model. Prompting a generative language model is, in many ways, less straightforward than tuning a classifier via gradient descent, as the space of discrete prompts is infinite and prompting has generated a large and unwieldy literature.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">A few clear insights are though worth emphasizing. Centrally, prompt tuning should be done on a validation set, never on the test set used to evaluate performance. The latter may overfit the prompt to the idiosyncrasies of the test set, making performance on it unrepresentative of performance on the unlabeled data.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">A literature on chain-of-thought prompting suggests breaking tasks down into simple steps, making them more digestible <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib136" title="" class="ltx_ref">2022</a>)</cite>. This aligns closely with my experience, where simple prompts work much better than lengthier and more detailed ones. If a problem requires lengthy instructions, try to break it down into multiple problems, prompting the model at each step. There is also literature on demonstrating tasks for generative LLMs (<span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a href="#bib.bib73" title="" class="ltx_ref">2022</a>)</cite>). Whether this is useful will depend on the nature of your task, and I recommend checking whether demonstration helps using a validation set. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib85" title="" class="ltx_ref">2023</a>)</cite> provide a review of prompt engineering. For autoregressive models like GPT, they recommend <span id="S6.SS2.p4.1.2" class="ltx_text ltx_font_italic">prefix</span> prompts; <span id="S6.SS2.p4.1.3" class="ltx_text ltx_font_italic">e.g.,</span> “I love this class. What’s the sentiment of this review?” This contrasts with cloze prompts: “I love this class, it is a <math id="S6.SS2.p4.1.m1.1" class="ltx_Math" alttext="[z]" display="inline"><semantics id="S6.SS2.p4.1.m1.1a"><mrow id="S6.SS2.p4.1.m1.1.2.2" xref="S6.SS2.p4.1.m1.1.2.1.cmml"><mo stretchy="false" id="S6.SS2.p4.1.m1.1.2.2.1" xref="S6.SS2.p4.1.m1.1.2.1.1.cmml">[</mo><mi id="S6.SS2.p4.1.m1.1.1" xref="S6.SS2.p4.1.m1.1.1.cmml">z</mi><mo stretchy="false" id="S6.SS2.p4.1.m1.1.2.2.2" xref="S6.SS2.p4.1.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.1.m1.1b"><apply id="S6.SS2.p4.1.m1.1.2.1.cmml" xref="S6.SS2.p4.1.m1.1.2.2"><csymbol cd="latexml" id="S6.SS2.p4.1.m1.1.2.1.1.cmml" xref="S6.SS2.p4.1.m1.1.2.2.1">delimited-[]</csymbol><ci id="S6.SS2.p4.1.m1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.1.m1.1c">[z]</annotation></semantics></math> class.”</p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p">This paper examines the performance of GPT-3.5 and GPT-4o on topic classification of historical newspaper articles as of June 2024. Over the past year, I have performed this exercise with older models, as well as GPT-4 and GPT-4 Turbo. GPT-4 and GPT-4o perform similarly, edging out Turbo. I haven’t seen any systematic improvements with new releases. Your mileage may vary, as there undoubtedly are tasks where the larger, newer (and hence more expensive) models will perform better.</p>
</div>
<div id="S6.SS2.p6" class="ltx_para">
<p id="S6.SS2.p6.1" class="ltx_p">I also examined two other leading AI models, from Anthropic: Claude Haiku and Claude Opus. These led to significantly worse performance (with F1 scores typically 10-40 points lower than GPT) and are not reported due to space constraints. The drivers of this lower performance are twofold. First, Claude refused to produce an output for texts that it assessed as harmful. A distinguishing feature of Claude is its ‘Constitutional AI’ framework, which sets out certain ethical principles (<span id="S6.SS2.p6.1.1" class="ltx_text ltx_font_italic">e.g.,</span> harmlessness necessitates that responses should be peaceful, ethical, and avoid content that might be considered offensive in non-Western cultures). Some articles on past conflicts (mostly objectively reporting on events) and a range of other topics (<span id="S6.SS2.p6.1.2" class="ltx_text ltx_font_italic">e.g.,</span> content about the introduction of contraception written in the 1960s) were considered harmful. Moreover, Claude didn’t always generate the desired Yes/No format, making it impossible to extract whether the article was on-topic. Neither of these behaviors arose with GPT. Perhaps this could be fixed with more prompt tuning, or it may change with a future update.</p>
</div>
<div id="S6.SS2.p7" class="ltx_para">
<p id="S6.SS2.p7.1" class="ltx_p">Regardless, with a classifier it is often fairly straightforward to interpret why it makes particular errors and how to fix them (by adding more training data for the types of instances it is confusing), whereas GenAI at present can feel more like a black box. There is more going on under the hood, with models trained with reinforcement learning to produce responses deemed desirable by the commercial entities training the models. This may or may not pose a problem for a given academic application and underscores the importance of rigorous evaluation with a test set.</p>
</div>
<div id="S6.SS2.p8" class="ltx_para">
<p id="S6.SS2.p8.1" class="ltx_p">The pros of generative AI for classification are: 1) startup costs are low, requiring minimal programming expertise or understanding of what is going on under the hood, and 2) it can be used <span id="S6.SS2.p8.1.1" class="ltx_text ltx_font_italic">zero-shot</span> (without the user providing training data), whereas tuning a classifier requires training data. We will see that, if anything, custom-trained classifiers tend to have a performance edge on text classification, though the most straightforward tasks can be performed very well with generative AI. This is consistent with the broad consensus that human-level performance can generally be attained on supervised tasks given adequate, high-quality training data.</p>
</div>
<div id="S6.SS2.p9" class="ltx_para">
<p id="S6.SS2.p9.1" class="ltx_p">This brings us to potential disadvantages of generative AI. Using a large model behind an API does not provide the same fine-grained control as training a classifier. While models such as GPT allow users to expose the model to empirical examples, for the experiments in this paper, this did not lead to improvements in performance. It is not fully understood how demonstration through prompting conditions these models, whereas it is clear how providing training examples updates a classifier through gradient descent. Not all models learn equally when exposed to the same amount of training data (as demonstrated empirically in Section <a href="#S9" title="9 Alternative methods ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), and lightweight models like those used with customized classifiers tend to update very efficiently. A custom classifier also has an edge when it comes to interpretability and reproducibility. Results from a commercial API may no longer be reproducible if a model is deprecated, and as discussed above, commercial GenAI models can be more of a black box. These concerns could be mitigated by using an open-source foundation model, such as Meta AI’s Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib129" title="" class="ltx_ref">2023</a>)</cite>. The startup costs and hardware requirements, however, negate the ease-of-use advantage. Finally, classifiers using a lightweight backbone such as RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite> are cheap to deploy over a massive number of texts, whereas commercial models at present can be very expensive. This may change if competition increases and research on cheap deployment advances.</p>
</div>
<div id="S6.SS2.p10" class="ltx_para">
<p id="S6.SS2.p10.1" class="ltx_p">To decide whether a classifier or generative AI is most suitable for a task, I would recommend first doing a back-of-the-envelope calculation to ensure that generative AI is within budget. If so, create test and validation sets, tune prompts, and evaluate its performance. If performance is not adequate, a training set to tune a custom classifier will be necessary. If the user knows ex ante that guaranteeing reproducibility is imperative, that there is substantial domain shift from web texts, or that the task requires fine-grained control, they might go straight to training a custom classifier. Data privacy requirements can add an additional layer to consider for those working with confidential data.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Sequence classification</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Economists might wish to impute a variety of structured information at the level of a text: <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> its topic, the type of content it contains, or its sentiment. To illustrate text sequence classification, this section trains 19 different binary topic classifiers, applied to massive-scale databases of historical news <cite class="ltx_cite ltx_citemacro_citep">(Dell et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Silcock et al., <a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, and compares them to generative AI. It would have been very difficult for annotators to keep 19 different topic definitions in mind to create multi-class labels; hence, binary classification is used. Binary classifiers cannot be combined into a multi-class classifier, as negatives for one topic may be positives for another. Keeping prompts simple for generative AI also suggests binary classes.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">The annotated data were congruence-labeled by highly skilled annotators, with discrepancies resolved by hand. Congruence labeling ensured high-quality data and facilitated the development of a well-specified definition. For example, in the case of the crime classifier, annotators disagreed on whether articles about Watergate should be considered on-topic, and a zero-shot model showed a massive spike in crime coverage in 1974 due to Watergate. While potentially a reasonable definition, I did not want crime coverage to be skewed by political scandals (which receive massive coverage), and the classifier quickly learned this with a modest number of labels.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">A frequent question is how many labels are required. This will vary. Topics that are more diverse or require learning a more complex definition will require more labels. Topics that were seen many times in the pre-training of the language model may require fewer labels. Fortunately, training a classifier is compute-efficient. If, after the first round of training, results are unsatisfactory, it is straightforward to add more labels and retrain. An error analysis may give a sense of what types of texts require more training examples. Since labeling is costly, we recommend starting with fewer labels and adding more if needed.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">Table <a href="#S6.T2" title="Table 2 ‣ 6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides the split statistics for the various topic classification tasks examined in this section.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The politics classifier is taken from a published paper <cite class="ltx_cite ltx_citemacro_citep">(Dell et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> with different aims, and hence the split shares and overall number of annotations differ somewhat.</span></span></span> Labeled data are randomly split into training, validation, and test data. Validation data are used to select hyperparameters, select the model checkpoint (when to stop training), and to tune prompts, whereas the test data were used only to compute Table <a href="#S6.T2" title="Table 2 ‣ 6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The prompts for Table <a href="#S6.T2" title="Table 2 ‣ 6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are listed in the supplemental materials.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.2" class="ltx_p">The classifiers were trained with <span id="S6.SS3.p5.2.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span>, which supports using any base language model available on Hugging Face. We used DistilRoBERTa (82M parameters) <cite class="ltx_cite ltx_citemacro_citep">(Sanh et al., <a href="#bib.bib117" title="" class="ltx_ref">2019<span class="ltx_text ltx_font_italic">b</span></a>)</cite> and RoBERTa large (335M parameters). RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite> is a widely used, improved version of BERT. Distilled language models are smaller models that are trained to match the performance of a larger model. The distilled version runs faster but often with a performance loss. We used a consistent set of hyperparameters across classification tasks, which appear to work well more generally (a learning rate of <math id="S6.SS3.p5.1.m1.1" class="ltx_Math" alttext="1e-6" display="inline"><semantics id="S6.SS3.p5.1.m1.1a"><mrow id="S6.SS3.p5.1.m1.1.1" xref="S6.SS3.p5.1.m1.1.1.cmml"><mrow id="S6.SS3.p5.1.m1.1.1.2" xref="S6.SS3.p5.1.m1.1.1.2.cmml"><mn id="S6.SS3.p5.1.m1.1.1.2.2" xref="S6.SS3.p5.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.1.m1.1.1.2.1" xref="S6.SS3.p5.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS3.p5.1.m1.1.1.2.3" xref="S6.SS3.p5.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S6.SS3.p5.1.m1.1.1.1" xref="S6.SS3.p5.1.m1.1.1.1.cmml">−</mo><mn id="S6.SS3.p5.1.m1.1.1.3" xref="S6.SS3.p5.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.1.m1.1b"><apply id="S6.SS3.p5.1.m1.1.1.cmml" xref="S6.SS3.p5.1.m1.1.1"><minus id="S6.SS3.p5.1.m1.1.1.1.cmml" xref="S6.SS3.p5.1.m1.1.1.1"></minus><apply id="S6.SS3.p5.1.m1.1.1.2.cmml" xref="S6.SS3.p5.1.m1.1.1.2"><times id="S6.SS3.p5.1.m1.1.1.2.1.cmml" xref="S6.SS3.p5.1.m1.1.1.2.1"></times><cn type="integer" id="S6.SS3.p5.1.m1.1.1.2.2.cmml" xref="S6.SS3.p5.1.m1.1.1.2.2">1</cn><ci id="S6.SS3.p5.1.m1.1.1.2.3.cmml" xref="S6.SS3.p5.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S6.SS3.p5.1.m1.1.1.3.cmml" xref="S6.SS3.p5.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.1.m1.1c">1e-6</annotation></semantics></math> or <math id="S6.SS3.p5.2.m2.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="S6.SS3.p5.2.m2.1a"><mrow id="S6.SS3.p5.2.m2.1.1" xref="S6.SS3.p5.2.m2.1.1.cmml"><mrow id="S6.SS3.p5.2.m2.1.1.2" xref="S6.SS3.p5.2.m2.1.1.2.cmml"><mn id="S6.SS3.p5.2.m2.1.1.2.2" xref="S6.SS3.p5.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.SS3.p5.2.m2.1.1.2.1" xref="S6.SS3.p5.2.m2.1.1.2.1.cmml">​</mo><mi id="S6.SS3.p5.2.m2.1.1.2.3" xref="S6.SS3.p5.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S6.SS3.p5.2.m2.1.1.1" xref="S6.SS3.p5.2.m2.1.1.1.cmml">−</mo><mn id="S6.SS3.p5.2.m2.1.1.3" xref="S6.SS3.p5.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.2.m2.1b"><apply id="S6.SS3.p5.2.m2.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1"><minus id="S6.SS3.p5.2.m2.1.1.1.cmml" xref="S6.SS3.p5.2.m2.1.1.1"></minus><apply id="S6.SS3.p5.2.m2.1.1.2.cmml" xref="S6.SS3.p5.2.m2.1.1.2"><times id="S6.SS3.p5.2.m2.1.1.2.1.cmml" xref="S6.SS3.p5.2.m2.1.1.2.1"></times><cn type="integer" id="S6.SS3.p5.2.m2.1.1.2.2.cmml" xref="S6.SS3.p5.2.m2.1.1.2.2">1</cn><ci id="S6.SS3.p5.2.m2.1.1.2.3.cmml" xref="S6.SS3.p5.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S6.SS3.p5.2.m2.1.1.3.cmml" xref="S6.SS3.p5.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.2.m2.1c">1e-5</annotation></semantics></math> and a batch size of eight).</p>
</div>
<figure id="S6.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>F1 on test set and summary of train-test split</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S6.T2.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:375.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.7pt,7.5pt) scale(0.961497072933577,0.961497072933577) ;">
<table id="S6.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.1.1.1.1" class="ltx_tr">
<th id="S6.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:8.61108pt;">
<span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span>
<span id="S6.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Topic</span>
</th>
<th id="S6.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:8.61108pt;" colspan="5"><span id="S6.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">F1 on test set</span></th>
<th id="S6.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:8.61108pt;" colspan="3"><span id="S6.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold"># of labels</span></th>
</tr>
<tr id="S6.T2.1.1.2.2" class="ltx_tr">
<th id="S6.T2.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r" style="padding-bottom:12.91663pt;"></th>
<th id="S6.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">GPT-3.5</th>
<th id="S6.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">GPT-4</th>
<th id="S6.T2.1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">
<span id="S6.T2.1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.2.2.4.1.1" class="ltx_p" style="width:52.0pt;">GPT-3.5 Trained Model<sup id="S6.T2.1.1.2.2.4.1.1.1" class="ltx_sup ltx_centering">†</sup></span>
</span>
</th>
<th id="S6.T2.1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">
<span id="S6.T2.1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.2.2.5.1.1" class="ltx_p" style="width:43.4pt;">Distil RoBERTa</span>
</span>
</th>
<th id="S6.T2.1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r" style="padding-bottom:12.91663pt;">
<span id="S6.T2.1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.2.2.6.1.1" class="ltx_p" style="width:43.4pt;">RoBERTa Large</span>
</span>
</th>
<th id="S6.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">Train</th>
<th id="S6.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-bottom:12.91663pt;">Eval</th>
<th id="S6.T2.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-bottom:12.91663pt;">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.1.1.3.1" class="ltx_tr">
<td id="S6.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">advice</td>
<td id="S6.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.72</td>
<td id="S6.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.85</td>
<td id="S6.T2.1.1.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T2.1.1.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.3.1.4.1.1" class="ltx_p" style="width:52.0pt;">0.55</span>
</span>
</td>
<td id="S6.T2.1.1.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T2.1.1.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.3.1.5.1.1" class="ltx_p" style="width:43.4pt;">0.87</span>
</span>
</td>
<td id="S6.T2.1.1.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S6.T2.1.1.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.3.1.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.3.1.6.1.1.1" class="ltx_text ltx_font_bold">0.97</span></span>
</span>
</td>
<td id="S6.T2.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">319</td>
<td id="S6.T2.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">68</td>
<td id="S6.T2.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68</td>
</tr>
<tr id="S6.T2.1.1.4.2" class="ltx_tr">
<td id="S6.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">antitrust</td>
<td id="S6.T2.1.1.4.2.2" class="ltx_td ltx_align_center">0.85</td>
<td id="S6.T2.1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.4.2.3.1" class="ltx_text ltx_font_bold">0.94</span></td>
<td id="S6.T2.1.1.4.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.4.2.4.1.1" class="ltx_p" style="width:52.0pt;">0.84</span>
</span>
</td>
<td id="S6.T2.1.1.4.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.4.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.4.2.5.1.1" class="ltx_p" style="width:43.4pt;">0.92</span>
</span>
</td>
<td id="S6.T2.1.1.4.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.4.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.4.2.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.4.2.6.1.1.1" class="ltx_text ltx_font_bold">0.94</span></span>
</span>
</td>
<td id="S6.T2.1.1.4.2.7" class="ltx_td ltx_align_center">329</td>
<td id="S6.T2.1.1.4.2.8" class="ltx_td ltx_align_center">70</td>
<td id="S6.T2.1.1.4.2.9" class="ltx_td ltx_align_center ltx_border_r">70</td>
</tr>
<tr id="S6.T2.1.1.5.3" class="ltx_tr">
<td id="S6.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">bible</td>
<td id="S6.T2.1.1.5.3.2" class="ltx_td ltx_align_center">0.52</td>
<td id="S6.T2.1.1.5.3.3" class="ltx_td ltx_align_center">0.81</td>
<td id="S6.T2.1.1.5.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.5.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.5.3.4.1.1" class="ltx_p" style="width:52.0pt;">0.10</span>
</span>
</td>
<td id="S6.T2.1.1.5.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.5.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.5.3.5.1.1" class="ltx_p" style="width:43.4pt;">0.85</span>
</span>
</td>
<td id="S6.T2.1.1.5.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.5.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.5.3.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.5.3.6.1.1.1" class="ltx_text ltx_font_bold">0.87</span></span>
</span>
</td>
<td id="S6.T2.1.1.5.3.7" class="ltx_td ltx_align_center">314</td>
<td id="S6.T2.1.1.5.3.8" class="ltx_td ltx_align_center">67</td>
<td id="S6.T2.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_r">67</td>
</tr>
<tr id="S6.T2.1.1.6.4" class="ltx_tr">
<td id="S6.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">civil rights</td>
<td id="S6.T2.1.1.6.4.2" class="ltx_td ltx_align_center">0.59</td>
<td id="S6.T2.1.1.6.4.3" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.6.4.3.1" class="ltx_text ltx_font_bold">0.87</span></td>
<td id="S6.T2.1.1.6.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.6.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.6.4.4.1.1" class="ltx_p" style="width:52.0pt;">0.54</span>
</span>
</td>
<td id="S6.T2.1.1.6.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.6.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.6.4.5.1.1" class="ltx_p" style="width:43.4pt;">0.85</span>
</span>
</td>
<td id="S6.T2.1.1.6.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.6.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.6.4.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.6.4.6.1.1.1" class="ltx_text ltx_font_bold">0.87</span></span>
</span>
</td>
<td id="S6.T2.1.1.6.4.7" class="ltx_td ltx_align_center">943</td>
<td id="S6.T2.1.1.6.4.8" class="ltx_td ltx_align_center">202</td>
<td id="S6.T2.1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_r">202</td>
</tr>
<tr id="S6.T2.1.1.7.5" class="ltx_tr">
<td id="S6.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">contraception</td>
<td id="S6.T2.1.1.7.5.2" class="ltx_td ltx_align_center">0.83</td>
<td id="S6.T2.1.1.7.5.3" class="ltx_td ltx_align_center">0.91</td>
<td id="S6.T2.1.1.7.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.7.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.7.5.4.1.1" class="ltx_p" style="width:52.0pt;">0.72</span>
</span>
</td>
<td id="S6.T2.1.1.7.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.7.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.7.5.5.1.1" class="ltx_p" style="width:43.4pt;">0.88</span>
</span>
</td>
<td id="S6.T2.1.1.7.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.7.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.7.5.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.7.5.6.1.1.1" class="ltx_text ltx_font_bold">0.97</span></span>
</span>
</td>
<td id="S6.T2.1.1.7.5.7" class="ltx_td ltx_align_center">597</td>
<td id="S6.T2.1.1.7.5.8" class="ltx_td ltx_align_center">127</td>
<td id="S6.T2.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_r">127</td>
</tr>
<tr id="S6.T2.1.1.8.6" class="ltx_tr">
<td id="S6.T2.1.1.8.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">crime</td>
<td id="S6.T2.1.1.8.6.2" class="ltx_td ltx_align_center">0.85</td>
<td id="S6.T2.1.1.8.6.3" class="ltx_td ltx_align_center">0.80</td>
<td id="S6.T2.1.1.8.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.8.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.8.6.4.1.1" class="ltx_p" style="width:52.0pt;">0.85</span>
</span>
</td>
<td id="S6.T2.1.1.8.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.8.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.8.6.5.1.1" class="ltx_p" style="width:43.4pt;">0.85</span>
</span>
</td>
<td id="S6.T2.1.1.8.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.8.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.8.6.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.8.6.6.1.1.1" class="ltx_text ltx_font_bold">0.90</span></span>
</span>
</td>
<td id="S6.T2.1.1.8.6.7" class="ltx_td ltx_align_center">463</td>
<td id="S6.T2.1.1.8.6.8" class="ltx_td ltx_align_center">98</td>
<td id="S6.T2.1.1.8.6.9" class="ltx_td ltx_align_center ltx_border_r">98</td>
</tr>
<tr id="S6.T2.1.1.9.7" class="ltx_tr">
<td id="S6.T2.1.1.9.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">horoscope</td>
<td id="S6.T2.1.1.9.7.2" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.9.7.2.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S6.T2.1.1.9.7.3" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.9.7.3.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S6.T2.1.1.9.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.9.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.9.7.4.1.1" class="ltx_p" style="width:52.0pt;">0.92</span>
</span>
</td>
<td id="S6.T2.1.1.9.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.9.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.9.7.5.1.1" class="ltx_p" style="width:43.4pt;">0.96</span>
</span>
</td>
<td id="S6.T2.1.1.9.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.9.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.9.7.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.9.7.6.1.1.1" class="ltx_text ltx_font_bold">1.00</span></span>
</span>
</td>
<td id="S6.T2.1.1.9.7.7" class="ltx_td ltx_align_center">288</td>
<td id="S6.T2.1.1.9.7.8" class="ltx_td ltx_align_center">61</td>
<td id="S6.T2.1.1.9.7.9" class="ltx_td ltx_align_center ltx_border_r">61</td>
</tr>
<tr id="S6.T2.1.1.10.8" class="ltx_tr">
<td id="S6.T2.1.1.10.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">labor movement</td>
<td id="S6.T2.1.1.10.8.2" class="ltx_td ltx_align_center">0.77</td>
<td id="S6.T2.1.1.10.8.3" class="ltx_td ltx_align_center">0.90</td>
<td id="S6.T2.1.1.10.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.10.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.10.8.4.1.1" class="ltx_p" style="width:52.0pt;">0.79</span>
</span>
</td>
<td id="S6.T2.1.1.10.8.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.10.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.10.8.5.1.1" class="ltx_p" style="width:43.4pt;">0.89</span>
</span>
</td>
<td id="S6.T2.1.1.10.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.10.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.10.8.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.10.8.6.1.1.1" class="ltx_text ltx_font_bold">0.94</span></span>
</span>
</td>
<td id="S6.T2.1.1.10.8.7" class="ltx_td ltx_align_center">253</td>
<td id="S6.T2.1.1.10.8.8" class="ltx_td ltx_align_center">54</td>
<td id="S6.T2.1.1.10.8.9" class="ltx_td ltx_align_center ltx_border_r">54</td>
</tr>
<tr id="S6.T2.1.1.11.9" class="ltx_tr">
<td id="S6.T2.1.1.11.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">obituaries</td>
<td id="S6.T2.1.1.11.9.2" class="ltx_td ltx_align_center">0.98</td>
<td id="S6.T2.1.1.11.9.3" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.11.9.3.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S6.T2.1.1.11.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.11.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.11.9.4.1.1" class="ltx_p" style="width:52.0pt;"><span id="S6.T2.1.1.11.9.4.1.1.1" class="ltx_text ltx_font_bold">1.00</span></span>
</span>
</td>
<td id="S6.T2.1.1.11.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.11.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.11.9.5.1.1" class="ltx_p" style="width:43.4pt;">0.96</span>
</span>
</td>
<td id="S6.T2.1.1.11.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.11.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.11.9.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.11.9.6.1.1.1" class="ltx_text ltx_font_bold">1.00</span></span>
</span>
</td>
<td id="S6.T2.1.1.11.9.7" class="ltx_td ltx_align_center">272</td>
<td id="S6.T2.1.1.11.9.8" class="ltx_td ltx_align_center">57</td>
<td id="S6.T2.1.1.11.9.9" class="ltx_td ltx_align_center ltx_border_r">57</td>
</tr>
<tr id="S6.T2.1.1.12.10" class="ltx_tr">
<td id="S6.T2.1.1.12.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">pesticide</td>
<td id="S6.T2.1.1.12.10.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S6.T2.1.1.12.10.3" class="ltx_td ltx_align_center">0.91</td>
<td id="S6.T2.1.1.12.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.12.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.12.10.4.1.1" class="ltx_p" style="width:52.0pt;">0.71</span>
</span>
</td>
<td id="S6.T2.1.1.12.10.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.12.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.12.10.5.1.1" class="ltx_p" style="width:43.4pt;">0.89</span>
</span>
</td>
<td id="S6.T2.1.1.12.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.12.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.12.10.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.12.10.6.1.1.1" class="ltx_text ltx_font_bold">0.98</span></span>
</span>
</td>
<td id="S6.T2.1.1.12.10.7" class="ltx_td ltx_align_center">873</td>
<td id="S6.T2.1.1.12.10.8" class="ltx_td ltx_align_center">187</td>
<td id="S6.T2.1.1.12.10.9" class="ltx_td ltx_align_center ltx_border_r">187</td>
</tr>
<tr id="S6.T2.1.1.13.11" class="ltx_tr">
<td id="S6.T2.1.1.13.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">polio vaccine</td>
<td id="S6.T2.1.1.13.11.2" class="ltx_td ltx_align_center">0.92</td>
<td id="S6.T2.1.1.13.11.3" class="ltx_td ltx_align_center"><span id="S6.T2.1.1.13.11.3.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S6.T2.1.1.13.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.13.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.13.11.4.1.1" class="ltx_p" style="width:52.0pt;">0.94</span>
</span>
</td>
<td id="S6.T2.1.1.13.11.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.13.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.13.11.5.1.1" class="ltx_p" style="width:43.4pt;">0.96</span>
</span>
</td>
<td id="S6.T2.1.1.13.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.13.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.13.11.6.1.1" class="ltx_p" style="width:43.4pt;">0.97</span>
</span>
</td>
<td id="S6.T2.1.1.13.11.7" class="ltx_td ltx_align_center">350</td>
<td id="S6.T2.1.1.13.11.8" class="ltx_td ltx_align_center">74</td>
<td id="S6.T2.1.1.13.11.9" class="ltx_td ltx_align_center ltx_border_r">74</td>
</tr>
<tr id="S6.T2.1.1.14.12" class="ltx_tr">
<td id="S6.T2.1.1.14.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">politics</td>
<td id="S6.T2.1.1.14.12.2" class="ltx_td ltx_align_center">0.67<sup id="S6.T2.1.1.14.12.2.1" class="ltx_sup">*</sup>
</td>
<td id="S6.T2.1.1.14.12.3" class="ltx_td ltx_align_center">0.62<sup id="S6.T2.1.1.14.12.3.1" class="ltx_sup">*</sup>
</td>
<td id="S6.T2.1.1.14.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.14.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.14.12.4.1.1" class="ltx_p" style="width:52.0pt;">0.74</span>
</span>
</td>
<td id="S6.T2.1.1.14.12.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.14.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.14.12.5.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.14.12.5.1.1.1" class="ltx_text ltx_font_bold">0.86</span></span>
</span>
</td>
<td id="S6.T2.1.1.14.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.14.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.14.12.6.1.1" class="ltx_p" style="width:43.4pt;">0.85</span>
</span>
</td>
<td id="S6.T2.1.1.14.12.7" class="ltx_td ltx_align_center">2,418</td>
<td id="S6.T2.1.1.14.12.8" class="ltx_td ltx_align_center">498</td>
<td id="S6.T2.1.1.14.12.9" class="ltx_td ltx_align_center ltx_border_r">1,473</td>
</tr>
<tr id="S6.T2.1.1.15.13" class="ltx_tr">
<td id="S6.T2.1.1.15.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">protests</td>
<td id="S6.T2.1.1.15.13.2" class="ltx_td ltx_align_center">0.74</td>
<td id="S6.T2.1.1.15.13.3" class="ltx_td ltx_align_center">0.81</td>
<td id="S6.T2.1.1.15.13.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.15.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.15.13.4.1.1" class="ltx_p" style="width:52.0pt;">0.79</span>
</span>
</td>
<td id="S6.T2.1.1.15.13.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.15.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.15.13.5.1.1" class="ltx_p" style="width:43.4pt;">0.90</span>
</span>
</td>
<td id="S6.T2.1.1.15.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.15.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.15.13.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.15.13.6.1.1.1" class="ltx_text ltx_font_bold">0.91</span></span>
</span>
</td>
<td id="S6.T2.1.1.15.13.7" class="ltx_td ltx_align_center">351</td>
<td id="S6.T2.1.1.15.13.8" class="ltx_td ltx_align_center">75</td>
<td id="S6.T2.1.1.15.13.9" class="ltx_td ltx_align_center ltx_border_r">75</td>
</tr>
<tr id="S6.T2.1.1.16.14" class="ltx_tr">
<td id="S6.T2.1.1.16.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Red Scare</td>
<td id="S6.T2.1.1.16.14.2" class="ltx_td ltx_align_center">0.81</td>
<td id="S6.T2.1.1.16.14.3" class="ltx_td ltx_align_center">0.86</td>
<td id="S6.T2.1.1.16.14.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.16.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.16.14.4.1.1" class="ltx_p" style="width:52.0pt;">0.79</span>
</span>
</td>
<td id="S6.T2.1.1.16.14.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.16.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.16.14.5.1.1" class="ltx_p" style="width:43.4pt;">0.90</span>
</span>
</td>
<td id="S6.T2.1.1.16.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.16.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.16.14.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.16.14.6.1.1.1" class="ltx_text ltx_font_bold">0.91</span></span>
</span>
</td>
<td id="S6.T2.1.1.16.14.7" class="ltx_td ltx_align_center">1,852</td>
<td id="S6.T2.1.1.16.14.8" class="ltx_td ltx_align_center">396</td>
<td id="S6.T2.1.1.16.14.9" class="ltx_td ltx_align_center ltx_border_r">396</td>
</tr>
<tr id="S6.T2.1.1.17.15" class="ltx_tr">
<td id="S6.T2.1.1.17.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">schedules</td>
<td id="S6.T2.1.1.17.15.2" class="ltx_td ltx_align_center">0.79</td>
<td id="S6.T2.1.1.17.15.3" class="ltx_td ltx_align_center">0.95</td>
<td id="S6.T2.1.1.17.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.17.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.17.15.4.1.1" class="ltx_p" style="width:52.0pt;">0.81</span>
</span>
</td>
<td id="S6.T2.1.1.17.15.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.17.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.17.15.5.1.1" class="ltx_p" style="width:43.4pt;">0.95</span>
</span>
</td>
<td id="S6.T2.1.1.17.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.17.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.17.15.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.17.15.6.1.1.1" class="ltx_text ltx_font_bold">0.96</span></span>
</span>
</td>
<td id="S6.T2.1.1.17.15.7" class="ltx_td ltx_align_center">346</td>
<td id="S6.T2.1.1.17.15.8" class="ltx_td ltx_align_center">74</td>
<td id="S6.T2.1.1.17.15.9" class="ltx_td ltx_align_center ltx_border_r">74</td>
</tr>
<tr id="S6.T2.1.1.18.16" class="ltx_tr">
<td id="S6.T2.1.1.18.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">sports</td>
<td id="S6.T2.1.1.18.16.2" class="ltx_td ltx_align_center">0.80</td>
<td id="S6.T2.1.1.18.16.3" class="ltx_td ltx_align_center">0.92</td>
<td id="S6.T2.1.1.18.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.18.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.18.16.4.1.1" class="ltx_p" style="width:52.0pt;">0.88</span>
</span>
</td>
<td id="S6.T2.1.1.18.16.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.18.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.18.16.5.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.18.16.5.1.1.1" class="ltx_text ltx_font_bold">0.94</span></span>
</span>
</td>
<td id="S6.T2.1.1.18.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.18.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.18.16.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.18.16.6.1.1.1" class="ltx_text ltx_font_bold">0.94</span></span>
</span>
</td>
<td id="S6.T2.1.1.18.16.7" class="ltx_td ltx_align_center">339</td>
<td id="S6.T2.1.1.18.16.8" class="ltx_td ltx_align_center">72</td>
<td id="S6.T2.1.1.18.16.9" class="ltx_td ltx_align_center ltx_border_r">72</td>
</tr>
<tr id="S6.T2.1.1.19.17" class="ltx_tr">
<td id="S6.T2.1.1.19.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Vietnam War</td>
<td id="S6.T2.1.1.19.17.2" class="ltx_td ltx_align_center">0.91</td>
<td id="S6.T2.1.1.19.17.3" class="ltx_td ltx_align_center">0.94</td>
<td id="S6.T2.1.1.19.17.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.19.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.19.17.4.1.1" class="ltx_p" style="width:52.0pt;">0.98</span>
</span>
</td>
<td id="S6.T2.1.1.19.17.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.19.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.19.17.5.1.1" class="ltx_p" style="width:43.4pt;">0.98</span>
</span>
</td>
<td id="S6.T2.1.1.19.17.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.19.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.19.17.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.19.17.6.1.1.1" class="ltx_text ltx_font_bold">0.99</span></span>
</span>
</td>
<td id="S6.T2.1.1.19.17.7" class="ltx_td ltx_align_center">738</td>
<td id="S6.T2.1.1.19.17.8" class="ltx_td ltx_align_center">157</td>
<td id="S6.T2.1.1.19.17.9" class="ltx_td ltx_align_center ltx_border_r">157</td>
</tr>
<tr id="S6.T2.1.1.20.18" class="ltx_tr">
<td id="S6.T2.1.1.20.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">weather</td>
<td id="S6.T2.1.1.20.18.2" class="ltx_td ltx_align_center">0.94</td>
<td id="S6.T2.1.1.20.18.3" class="ltx_td ltx_align_center">0.92</td>
<td id="S6.T2.1.1.20.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.20.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.20.18.4.1.1" class="ltx_p" style="width:52.0pt;">0.94</span>
</span>
</td>
<td id="S6.T2.1.1.20.18.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T2.1.1.20.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.20.18.5.1.1" class="ltx_p" style="width:43.4pt;">0.94</span>
</span>
</td>
<td id="S6.T2.1.1.20.18.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S6.T2.1.1.20.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.20.18.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.20.18.6.1.1.1" class="ltx_text ltx_font_bold">0.95</span></span>
</span>
</td>
<td id="S6.T2.1.1.20.18.7" class="ltx_td ltx_align_center">569</td>
<td id="S6.T2.1.1.20.18.8" class="ltx_td ltx_align_center">57</td>
<td id="S6.T2.1.1.20.18.9" class="ltx_td ltx_align_center ltx_border_r">57</td>
</tr>
<tr id="S6.T2.1.1.21.19" class="ltx_tr">
<td id="S6.T2.1.1.21.19.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">World War I</td>
<td id="S6.T2.1.1.21.19.2" class="ltx_td ltx_align_center ltx_border_b">0.72</td>
<td id="S6.T2.1.1.21.19.3" class="ltx_td ltx_align_center ltx_border_b">0.74</td>
<td id="S6.T2.1.1.21.19.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S6.T2.1.1.21.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.21.19.4.1.1" class="ltx_p" style="width:52.0pt;">0.51</span>
</span>
</td>
<td id="S6.T2.1.1.21.19.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S6.T2.1.1.21.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.21.19.5.1.1" class="ltx_p" style="width:43.4pt;">0.89</span>
</span>
</td>
<td id="S6.T2.1.1.21.19.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S6.T2.1.1.21.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T2.1.1.21.19.6.1.1" class="ltx_p" style="width:43.4pt;"><span id="S6.T2.1.1.21.19.6.1.1.1" class="ltx_text ltx_font_bold">0.92</span></span>
</span>
</td>
<td id="S6.T2.1.1.21.19.7" class="ltx_td ltx_align_center ltx_border_b">690</td>
<td id="S6.T2.1.1.21.19.8" class="ltx_td ltx_align_center ltx_border_b">164</td>
<td id="S6.T2.1.1.21.19.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">192</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S6.T2.2" class="ltx_ERROR ltx_figure_panel undefined">{tablenotes}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S6.T2.3" class="ltx_p ltx_figure_panel">†This column reports the F1 for trained models (based on either DistilRoBERTa or RoBERTa-Large, whichever works better) using labels generated by GPT-3.5. 
<br class="ltx_break"><sup id="S6.T2.3.1" class="ltx_sup">*</sup> The results with asterisks were produced on a random sample of 500 out of total 1,473 articles in the test set.</p>
</div>
</div>
</figure>
<div id="S6.SS3.p6" class="ltx_para">
<p id="S6.SS3.p6.1" class="ltx_p">In most cases—across a diversity of topics—the tuned classifier tends to outperform or equal the performance of GPT, though for the more straightforward tasks, GPT’s performance can be very good, particularly in the case of GPT-4o (GPT-4 performed similarly). The training data used to produce these classifiers are high quality. With lower-quality labels, such as those created with online annotation platforms where quality is notoriously poor, a custom-trained classifier may well be consistently worse than GPT. These comparisons may also change in the future.</p>
</div>
<div id="S6.SS3.p7" class="ltx_para">
<p id="S6.SS3.p7.1" class="ltx_p">More generally, generative AI performs best on straightforward topics that it was likely extensively exposed to during pre-training. The further the domain shifts from training data—primarily modern web texts—the more performance deteriorates. For horoscopes, obituaries, and articles on the polio vaccine—all extremely straightforward—GPT performs nearly perfectly (as do the classifiers). However, there are also topics for which GPT performs poorly; for example, politics, a topic that is challenging because it is broad and diverse, with content drawn from the late 19th and early 20th centuries and including both local and national politics. World War I has an F1 score from both GPT models in the low 70s, much worse than the Vietnam War, which likely has greater representation in the training corpus. Moreover, language has changed more since World War I, translating into greater domain shift. Yet with minimal labels, the RoBERTa classifiers can adjust to this domain shift.</p>
</div>
<div id="S6.SS3.p8" class="ltx_para">
<p id="S6.SS3.p8.1" class="ltx_p">At present, GPT is likely to be well beyond the budget of most social science researchers for large corpora, though I do not cite figures here as prices fluctuate and could change considerably depending on competition and technological advancement. In contrast, training a RoBERTa classifier on the number of labels shown here is very cheap (at the time of writing, it could be done within minutes on a $9.99/month Google Colab plan or a mid-range Nvidia GPU card). I have also had students, with patience, train similar models on a laptop, though getting access to a decent GPU through cloud compute or dedicated hardware is preferable. Deploying the classifiers, even across millions of articles, is also cheap, and can be done either with cloud CPUs or on a mid-range GPU card in hours. One can circumvent the expense of generative AI by training a classifier on labels predicted by GPT. Table <a href="#S6.T2" title="Table 2 ‣ 6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows this can work when GPT produces very high-quality labels. However, where GPT performs less well, training on noisy data can magnify errors.</p>
</div>
<div id="S6.SS3.p9" class="ltx_para">
<p id="S6.SS3.p9.1" class="ltx_p">Figure <a href="#S6.F4" title="Figure 4 ‣ 6.3 Sequence classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, drawn from <cite class="ltx_cite ltx_citemacro_citet">Silcock et al. (<a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, takes binary classifiers that apply across time and deploys them to a dataset of 2.7 million unique newswire articles published between 1878 and 1977. Various trends are evident, such as the Prohibition-related crime coverage spike in the 1920s or the surge of Civil Rights and protests coverage in the 1960s.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2407.15339/assets/figures/topic_article_perc.png" id="S6.F4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="289" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Share of newswire articles with a given binary topic tag.</figcaption>
</figure>
<div id="S6.SS3.p10" class="ltx_para">
<p id="S6.SS3.p10.5" class="ltx_p">It is worth saying a word about how neural methods compare to sparse methods for text classification, which (with varying degrees of sophistication) rely keywords. For example, one common sparse method is TF-IDF: Term Frequency (TF) is the raw count of term <math id="S6.SS3.p10.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S6.SS3.p10.1.m1.1a"><mi id="S6.SS3.p10.1.m1.1.1" xref="S6.SS3.p10.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p10.1.m1.1b"><ci id="S6.SS3.p10.1.m1.1.1.cmml" xref="S6.SS3.p10.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p10.1.m1.1c">t</annotation></semantics></math> in document <math id="S6.SS3.p10.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S6.SS3.p10.2.m2.1a"><mi id="S6.SS3.p10.2.m2.1.1" xref="S6.SS3.p10.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p10.2.m2.1b"><ci id="S6.SS3.p10.2.m2.1.1.cmml" xref="S6.SS3.p10.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p10.2.m2.1c">d</annotation></semantics></math>. Inverse Document Frequency (IDF) measures the importance of a term in the entire corpus. If a word appears in many documents, it’s not a good identifier of a given document. The IDF of a term <math id="S6.SS3.p10.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S6.SS3.p10.3.m3.1a"><mi id="S6.SS3.p10.3.m3.1.1" xref="S6.SS3.p10.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p10.3.m3.1b"><ci id="S6.SS3.p10.3.m3.1.1.cmml" xref="S6.SS3.p10.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p10.3.m3.1c">t</annotation></semantics></math> for a corpus <math id="S6.SS3.p10.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S6.SS3.p10.4.m4.1a"><mi id="S6.SS3.p10.4.m4.1.1" xref="S6.SS3.p10.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p10.4.m4.1b"><ci id="S6.SS3.p10.4.m4.1.1.cmml" xref="S6.SS3.p10.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p10.4.m4.1c">D</annotation></semantics></math> is given by <math id="S6.SS3.p10.5.m5.1" class="ltx_Math" alttext="\log\frac{\text{Total number of docs in }D}{\text{Number of docs with term }t\text{ in }D}" display="inline"><semantics id="S6.SS3.p10.5.m5.1a"><mrow id="S6.SS3.p10.5.m5.1.1" xref="S6.SS3.p10.5.m5.1.1.cmml"><mi id="S6.SS3.p10.5.m5.1.1.1" xref="S6.SS3.p10.5.m5.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S6.SS3.p10.5.m5.1.1a" xref="S6.SS3.p10.5.m5.1.1.cmml">⁡</mo><mfrac id="S6.SS3.p10.5.m5.1.1.2" xref="S6.SS3.p10.5.m5.1.1.2.cmml"><mrow id="S6.SS3.p10.5.m5.1.1.2.2" xref="S6.SS3.p10.5.m5.1.1.2.2.cmml"><mtext id="S6.SS3.p10.5.m5.1.1.2.2.2" xref="S6.SS3.p10.5.m5.1.1.2.2.2a.cmml">Total number of docs in </mtext><mo lspace="0em" rspace="0em" id="S6.SS3.p10.5.m5.1.1.2.2.1" xref="S6.SS3.p10.5.m5.1.1.2.2.1.cmml">​</mo><mi id="S6.SS3.p10.5.m5.1.1.2.2.3" xref="S6.SS3.p10.5.m5.1.1.2.2.3.cmml">D</mi></mrow><mrow id="S6.SS3.p10.5.m5.1.1.2.3" xref="S6.SS3.p10.5.m5.1.1.2.3.cmml"><mtext id="S6.SS3.p10.5.m5.1.1.2.3.2" xref="S6.SS3.p10.5.m5.1.1.2.3.2a.cmml">Number of docs with term </mtext><mo lspace="0em" rspace="0em" id="S6.SS3.p10.5.m5.1.1.2.3.1" xref="S6.SS3.p10.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S6.SS3.p10.5.m5.1.1.2.3.3" xref="S6.SS3.p10.5.m5.1.1.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p10.5.m5.1.1.2.3.1a" xref="S6.SS3.p10.5.m5.1.1.2.3.1.cmml">​</mo><mtext id="S6.SS3.p10.5.m5.1.1.2.3.4" xref="S6.SS3.p10.5.m5.1.1.2.3.4a.cmml"> in </mtext><mo lspace="0em" rspace="0em" id="S6.SS3.p10.5.m5.1.1.2.3.1b" xref="S6.SS3.p10.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S6.SS3.p10.5.m5.1.1.2.3.5" xref="S6.SS3.p10.5.m5.1.1.2.3.5.cmml">D</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p10.5.m5.1b"><apply id="S6.SS3.p10.5.m5.1.1.cmml" xref="S6.SS3.p10.5.m5.1.1"><log id="S6.SS3.p10.5.m5.1.1.1.cmml" xref="S6.SS3.p10.5.m5.1.1.1"></log><apply id="S6.SS3.p10.5.m5.1.1.2.cmml" xref="S6.SS3.p10.5.m5.1.1.2"><divide id="S6.SS3.p10.5.m5.1.1.2.1.cmml" xref="S6.SS3.p10.5.m5.1.1.2"></divide><apply id="S6.SS3.p10.5.m5.1.1.2.2.cmml" xref="S6.SS3.p10.5.m5.1.1.2.2"><times id="S6.SS3.p10.5.m5.1.1.2.2.1.cmml" xref="S6.SS3.p10.5.m5.1.1.2.2.1"></times><ci id="S6.SS3.p10.5.m5.1.1.2.2.2a.cmml" xref="S6.SS3.p10.5.m5.1.1.2.2.2"><mtext mathsize="70%" id="S6.SS3.p10.5.m5.1.1.2.2.2.cmml" xref="S6.SS3.p10.5.m5.1.1.2.2.2">Total number of docs in </mtext></ci><ci id="S6.SS3.p10.5.m5.1.1.2.2.3.cmml" xref="S6.SS3.p10.5.m5.1.1.2.2.3">𝐷</ci></apply><apply id="S6.SS3.p10.5.m5.1.1.2.3.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3"><times id="S6.SS3.p10.5.m5.1.1.2.3.1.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.1"></times><ci id="S6.SS3.p10.5.m5.1.1.2.3.2a.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.2"><mtext mathsize="70%" id="S6.SS3.p10.5.m5.1.1.2.3.2.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.2">Number of docs with term </mtext></ci><ci id="S6.SS3.p10.5.m5.1.1.2.3.3.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.3">𝑡</ci><ci id="S6.SS3.p10.5.m5.1.1.2.3.4a.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.4"><mtext mathsize="70%" id="S6.SS3.p10.5.m5.1.1.2.3.4.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.4"> in </mtext></ci><ci id="S6.SS3.p10.5.m5.1.1.2.3.5.cmml" xref="S6.SS3.p10.5.m5.1.1.2.3.5">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p10.5.m5.1c">\log\frac{\text{Total number of docs in }D}{\text{Number of docs with term }t\text{ in }D}</annotation></semantics></math>. The TF-IDF score for a term is simply the product of its TF and IDF scores. The higher the TF-IDF score, the more important a term is to a specific document relative to the context of the entire corpus. To rank documents from a corpus based on their similarity to a query using TF-IDF, each document and query is represented as a sparse, high-dimensional vector, with each dimension corresponding to a unique term from the corpus. The weight of each term in the vector is its TF-IDF score. The angle between any two vectors captures the similarity between the texts. One can think of this as akin to a keyword search that downweights terms that appear across the corpus.</p>
</div>
<div id="S6.SS3.p11" class="ltx_para">
<p id="S6.SS3.p11.1" class="ltx_p">We refer to methods like TF-IDF as sparse because each term in the corpus forms a dimension in the vector space. Most terms in the vocabulary will not be present in a single document, leading most entries in the term vector to be zero.</p>
</div>
<div id="S6.SS3.p12" class="ltx_para">
<p id="S6.SS3.p12.1" class="ltx_p">Sparse methods are useful when exact term overlap is highly informative. However, relying on term overlap is often a major shortcoming, as language is complex. There are many ways to say the same thing, and the same term can have different meanings. Moreover, noise (<span id="S6.SS3.p12.1.1" class="ltx_text ltx_font_italic">e.g.,</span> typos, OCR errors, abbreviations) is ubiquitous. Semantics vary across time and space, as do many omitted variables. This could result in correlation between prediction error and the error term in the causal estimating equation where the keyword predictions are used. Moreover, while terms can be mined, more often they are simply chosen, creating a researcher degrees of freedom problem.</p>
</div>
<div id="S6.SS3.p13" class="ltx_para">
<p id="S6.SS3.p13.1" class="ltx_p">Neural methods address these shortcomings by using a large language model to map texts to a dense vector representation, <span id="S6.SS3.p13.1.1" class="ltx_text ltx_font_italic">e.g,</span> a 768-dimensional vector composed of non-zero terms. The dimensionality of this vector depends on the base language model. The pre-trained language model is imbued with language understanding, and hence dense methods account for contextual and semantic similarities. This allows them to generalize over synonyms and semantically similar phrases, and to be more robust to other noise.</p>
</div>
<div id="S6.SS3.p14" class="ltx_para">
<p id="S6.SS3.p14.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Dell et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> compare the neural classifier for politics, shown above, to mined keywords as well as keywords suggested by ChatGPT. Neural methods lead to significantly more accurate predictions.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Token classification</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Researchers may need to extract information about individual terms in a text, rather than the text as a whole.
This problem is analogous to sequence classification, except that classifier heads are added to the representations for each token in the final layer of the transformer, rather than only to the <span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> representation (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.5 Transformer Large Language Models ‣ 2 Foundational Deep Learning Architectures ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, panel c).</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">This section develops an example of token classification, named entity recognition (NER), which detects named entities in texts.
These entities can be defined however the researcher desires, as long as a clear, consistent definition and sufficient labels for training exist.
For example, the researcher may want to identify locations referred to in social media posts. Alternatively, a researcher extracting family relationships from obituary data might wish to tag the relationships of individuals to the deceased (child, parent, sibling, officiant, etc). Or, a researcher wishing to convert biographical texts into a structured dataset might tag birthplace, mother, father, university, spouse, and employer.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">NER is a classic task that has generated a very large literature, and there are many open-source pre-trained models and datasets on Hugging Face. In this literature, entity classes typically include person, location, and organization. CoNLL is a prominent benchmark <cite class="ltx_cite ltx_citemacro_citep">(Sang and De Meulder, <a href="#bib.bib115" title="" class="ltx_ref">2003</a>)</cite> used to pre-train various models on Hugging Face.
WNUT is another <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a href="#bib.bib97" title="" class="ltx_ref">2020</a>)</cite>, which focuses on noisy user-generated texts (tweets). A researcher will need to create labels if they depart from the entity types emphasized by the benchmarks. NER typically uses BIO labeling - the first token in an entity is labeled B (for begin), the following tokens are labeled I (for interior), and tokens that are not of interest are labeled O. If the entity types of interest are people (P) and locations (L), tags would be B-P, B-L, I-P, I-L, and O.</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p id="S6.SS4.p4.1" class="ltx_p">Figure <a href="#S6.F5" title="Figure 5 ‣ 6.4 Token classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents results from applying NER to historical newswire articles, plotting the shares of over 27 million entities that fall into four categories: person, location, organization, and miscellaneous.
The results are sensible, for example showing a spike in location and miscellaneous named entities (<span id="S6.SS4.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> aircraft names) during World War II.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2407.15339/assets/figures/entities.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="442" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Shares of entity types in newswire articles.</figcaption>
</figure>
<div id="S6.SS4.p5" class="ltx_para">
<p id="S6.SS4.p5.1" class="ltx_p">One can also ask generative AI to recognize entities in text and convert the output into a table.
As with sequence classification, researchers can test whether performance is adequate for their needs by constructing representative validation and test sets.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Relationships between texts</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">There are a variety of contexts in which we would like to measure whether two texts are related in some pre-specified way. For example, we could phrase topic classification as a task in which we would like to classify whether one statement entails another: does the article text entail the statement “this article is about monetary policy”? Alternatively, are two texts noisy duplicates of each other? Do they take the same stance on a political issue? Does one follow the other?</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">Figure <a href="#S6.F6" title="Figure 6 ‣ 6.5 Relationships between texts ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates two approaches to comparing texts. The top shows a <span id="S6.SS5.p2.1.1" class="ltx_text ltx_font_italic">cross-encoder</span>: two texts are concatenated, with a <span id="S6.SS5.p2.1.2" class="ltx_text ltx_font_typewriter">&lt;sep&gt;</span> token between them. These texts are jointly passed into the transformer, and a classifier head classifies how they are related. This approach allows for full cross-attention between all tokens in each of the texts. The bottom figure illustrates a <span id="S6.SS5.p2.1.3" class="ltx_text ltx_font_italic">bi-encoder</span> approach: texts are embedded separately, and then we compute the similarity between them using some distance metric, such as cosine similarity. Bi-encoders are very useful, and we elaborate on them in Section <a href="#S7" title="7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S6.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.15339/assets/figures/cross_encoder.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="419" height="346" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S6.F6.1" class="ltx_p ltx_figure_panel ltx_align_center">Cross-encoder
<br class="ltx_break"><img src="/html/2407.15339/assets/figures/biencoder.png" id="S6.F6.1.g1" class="ltx_graphics ltx_img_square" width="479" height="416" alt="Refer to caption">
<br class="ltx_break">Bi-encoder</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Architectures to compare texts.</figcaption>
</figure>
<div id="S6.SS5.p3" class="ltx_para">
<p id="S6.SS5.p3.8" class="ltx_p">A cross-encoder allows for full cross-attention between terms. Because they are jointly embedded, the terms in the texts being compared attend flexibly to each other when creating representations. In contrast, a bi-encoder computes a single representation for each text separately, and then these representations are compared. Hence, cross-encoders tend to have higher accuracy. However, they also have significant drawbacks. Most centrally, if we need to compare <math id="S6.SS5.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S6.SS5.p3.1.m1.1a"><mi id="S6.SS5.p3.1.m1.1.1" xref="S6.SS5.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.1.m1.1b"><ci id="S6.SS5.p3.1.m1.1.1.cmml" xref="S6.SS5.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.1.m1.1c">M</annotation></semantics></math> texts to <math id="S6.SS5.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S6.SS5.p3.2.m2.1a"><mi id="S6.SS5.p3.2.m2.1.1" xref="S6.SS5.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.2.m2.1b"><ci id="S6.SS5.p3.2.m2.1.1.cmml" xref="S6.SS5.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.2.m2.1c">N</annotation></semantics></math> other texts, this would require embedding <math id="S6.SS5.p3.3.m3.1" class="ltx_Math" alttext="M\times N" display="inline"><semantics id="S6.SS5.p3.3.m3.1a"><mrow id="S6.SS5.p3.3.m3.1.1" xref="S6.SS5.p3.3.m3.1.1.cmml"><mi id="S6.SS5.p3.3.m3.1.1.2" xref="S6.SS5.p3.3.m3.1.1.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S6.SS5.p3.3.m3.1.1.1" xref="S6.SS5.p3.3.m3.1.1.1.cmml">×</mo><mi id="S6.SS5.p3.3.m3.1.1.3" xref="S6.SS5.p3.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.3.m3.1b"><apply id="S6.SS5.p3.3.m3.1.1.cmml" xref="S6.SS5.p3.3.m3.1.1"><times id="S6.SS5.p3.3.m3.1.1.1.cmml" xref="S6.SS5.p3.3.m3.1.1.1"></times><ci id="S6.SS5.p3.3.m3.1.1.2.cmml" xref="S6.SS5.p3.3.m3.1.1.2">𝑀</ci><ci id="S6.SS5.p3.3.m3.1.1.3.cmml" xref="S6.SS5.p3.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.3.m3.1c">M\times N</annotation></semantics></math> texts. This quadratic cost quickly becomes infeasible, since each text is passed through a neural network with hundreds of millions of parameters.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Moreover, if we ask a cross-encoder to classify if <math id="footnote3.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="footnote3.m1.1b"><mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">a</annotation></semantics></math> is the same as <math id="footnote3.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="footnote3.m2.1b"><mi id="footnote3.m2.1.1" xref="footnote3.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="footnote3.m2.1c"><ci id="footnote3.m2.1.1.cmml" xref="footnote3.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m2.1d">b</annotation></semantics></math>, <math id="footnote3.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="footnote3.m3.1b"><mi id="footnote3.m3.1.1" xref="footnote3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="footnote3.m3.1c"><ci id="footnote3.m3.1.1.cmml" xref="footnote3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m3.1d">b</annotation></semantics></math> is the same as <math id="footnote3.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="footnote3.m4.1b"><mi id="footnote3.m4.1.1" xref="footnote3.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="footnote3.m4.1c"><ci id="footnote3.m4.1.1.cmml" xref="footnote3.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m4.1d">c</annotation></semantics></math>, and <math id="footnote3.m5.1" class="ltx_Math" alttext="a" display="inline"><semantics id="footnote3.m5.1b"><mi id="footnote3.m5.1.1" xref="footnote3.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="footnote3.m5.1c"><ci id="footnote3.m5.1.1.cmml" xref="footnote3.m5.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m5.1d">a</annotation></semantics></math> is the same as <math id="footnote3.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="footnote3.m6.1b"><mi id="footnote3.m6.1.1" xref="footnote3.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="footnote3.m6.1c"><ci id="footnote3.m6.1.1.cmml" xref="footnote3.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m6.1d">c</annotation></semantics></math>, intransitivities may result.</span></span></span> In contrast, for the bi-encoder to compare <math id="S6.SS5.p3.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S6.SS5.p3.4.m4.1a"><mi id="S6.SS5.p3.4.m4.1.1" xref="S6.SS5.p3.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.4.m4.1b"><ci id="S6.SS5.p3.4.m4.1.1.cmml" xref="S6.SS5.p3.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.4.m4.1c">M</annotation></semantics></math> texts to <math id="S6.SS5.p3.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S6.SS5.p3.5.m5.1a"><mi id="S6.SS5.p3.5.m5.1.1" xref="S6.SS5.p3.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.5.m5.1b"><ci id="S6.SS5.p3.5.m5.1.1.cmml" xref="S6.SS5.p3.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.5.m5.1c">N</annotation></semantics></math> other texts, only <math id="S6.SS5.p3.6.m6.1" class="ltx_Math" alttext="M+N" display="inline"><semantics id="S6.SS5.p3.6.m6.1a"><mrow id="S6.SS5.p3.6.m6.1.1" xref="S6.SS5.p3.6.m6.1.1.cmml"><mi id="S6.SS5.p3.6.m6.1.1.2" xref="S6.SS5.p3.6.m6.1.1.2.cmml">M</mi><mo id="S6.SS5.p3.6.m6.1.1.1" xref="S6.SS5.p3.6.m6.1.1.1.cmml">+</mo><mi id="S6.SS5.p3.6.m6.1.1.3" xref="S6.SS5.p3.6.m6.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.6.m6.1b"><apply id="S6.SS5.p3.6.m6.1.1.cmml" xref="S6.SS5.p3.6.m6.1.1"><plus id="S6.SS5.p3.6.m6.1.1.1.cmml" xref="S6.SS5.p3.6.m6.1.1.1"></plus><ci id="S6.SS5.p3.6.m6.1.1.2.cmml" xref="S6.SS5.p3.6.m6.1.1.2">𝑀</ci><ci id="S6.SS5.p3.6.m6.1.1.3.cmml" xref="S6.SS5.p3.6.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.6.m6.1c">M+N</annotation></semantics></math> embeddings are required, making this approach highly scalable. To get the best of both worlds, the literature often uses a bi-encoder to get the <math id="S6.SS5.p3.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.SS5.p3.7.m7.1a"><mi id="S6.SS5.p3.7.m7.1.1" xref="S6.SS5.p3.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.7.m7.1b"><ci id="S6.SS5.p3.7.m7.1.1.cmml" xref="S6.SS5.p3.7.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.7.m7.1c">n</annotation></semantics></math> most similar texts to a query text (for some small <math id="S6.SS5.p3.8.m8.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.SS5.p3.8.m8.1a"><mi id="S6.SS5.p3.8.m8.1.1" xref="S6.SS5.p3.8.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS5.p3.8.m8.1b"><ci id="S6.SS5.p3.8.m8.1.1.cmml" xref="S6.SS5.p3.8.m8.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p3.8.m8.1c">n</annotation></semantics></math>), and then re-ranks these matches with a cross-encoder.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Embedding models</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">To estimate a classifier, the classes must be specified ex ante, since the number of classes determines the number of parameters in the neural network, and classes must be seen in training. Prompting generative AI for classification tasks also entails specifying what the classes of interest are. However, there are a variety of problems where the classes are not known ex ante or where the researcher would like to add new classes later without having to retrain the model. Moreover, if the number of classes is large, it can become computationally intractable to compute the softmax over all the classes for the loss function.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">These common scenarios can be addressed by working directly with the embeddings from the final layer of the transformer or CNN, rather than estimating an additional neural network layer (the classifier) that maps embeddings to class scores. This avoids the pre-specification of classes. Moreover, vector similarity calculations are highly optimized, allowing for problems at the scale of millions or even billions of classes.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">This section introduces a variety of applications that can be approached with embedding models. Record linkage (<span id="S7.p3.1.1" class="ltx_text ltx_font_italic">e.g.,</span> of individuals, firms, locations, or products across datasets) is a common task that can be framed as a classification problem with many classes (<span id="S7.p3.1.2" class="ltx_text ltx_font_italic">e.g.,</span> each individual, firm, etc.). It is particularly amenable to embedding methods (Section <a href="#S7.SS2" title="7.2 Record linkage with structured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>).
These methods can handle settings—like multilingual linkage or linkage with multiple, noisy text descriptions—that are very difficult to tackle with traditional string matching methods.
Similar methods can be used to link mentions of individuals, firms, etc. in unstructured texts (<span id="S7.p3.1.3" class="ltx_text ltx_font_italic">e.g.,</span> social or print media, firm filings, biographies, government documents) to an external knowledge base such as Wikipedia (Section <a href="#S7.SS3" title="7.3 Linking unstructured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>).
Tracking the spread of text or images through media is another application of potential interest, where the classes are often unknown ex ante (Section <a href="#S7.SS4" title="7.4 Classification when categories are unknown ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>).
In some cases, the aim may be exploratory, to uncover the stylized facts in a massive novel text or image dataset, and embedding methods are well-suited to such descriptive analyses.
Finally, optical character recognition (OCR) can be framed as an image classification task, where the researcher might wish to add characters or words subsequently without retraining the model, suggesting embedding methods (Section <a href="#S7.SS5" title="7.5 Optical character recognition ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.5</span></a>).</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Working directly with embeddings requires distances between vector representations to be meaningful. The geometric properties of pre-trained transformer language models are not well-suited to this task. For example, representations of low-frequency words are pushed outwards on the hypersphere. The sparsity of low-frequency words violates convexity, and the distance between embeddings is correlated with lexical similarity. This leads to poor alignment between the embeddings of semantically similar texts and poor performance when individual term representations are pooled to create an average representation for text sequences <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh, <a href="#bib.bib41" title="" class="ltx_ref">2019</a>; Reimers and Gurevych, <a href="#bib.bib108" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Mathematically, the problem is that the embedding space created by a pre-trained transformer model is not <span id="S7.p5.1.1" class="ltx_text ltx_font_italic">isotropic</span>, meaning that the representations are not evenly distributed. When embeddings are isotropic, no particular direction is favored. This uniform distribution ensures that the distances between vectors accurately reflect their relationships, making the space more effective for tasks that depend on these distances. Contrastive learning is a widely used method that improves isotropy and is discussed before turning to embedding model applications.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Contrastive learning</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Contrastive learning aims to learn similar representations for semantically similar inputs and dissimilar representations for semantically different inputs, where the definition of similarity is given by empirical training examples. The contrastive loss function encourages the model to reduce the distance in embedding space between positive examples (<span id="S7.SS1.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> similar texts or images) and increase the distance between negative examples (<span id="S7.SS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> dissimilar texts or images). Contrastive training reduces anisotropy <cite class="ltx_cite ltx_citemacro_citep">(Wang and Liu, <a href="#bib.bib135" title="" class="ltx_ref">2021</a>)</cite>, significantly improves pooled representations of text sequences <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib108" title="" class="ltx_ref">2019</a>)</cite>, and improves alignment between representations of semantically similar texts.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">Contrastive learning follows the bi-encoder setup shown in Figure <a href="#S6.F6" title="Figure 6 ‣ 6.5 Relationships between texts ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Bi-encoders form a single representation for each instance, by passing it through a transformer and pooling (averaging) term level embeddings.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Representations are pooled rather than using the <span id="footnote4.1" class="ltx_text ltx_font_typewriter">&lt;cls&gt;</span> token because this has been shown to have better performance <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib108" title="" class="ltx_ref">2019</a>)</cite>.</span></span></span> Representations can be compared by computing their vector similarity. In practice cosine similarity is frequently used since representations are on a unit hypersphere.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">There are different options for the loss function depending on the training data. Contrastive loss <cite class="ltx_cite ltx_citemacro_citep">(Chopra, Hadsell and LeCun, <a href="#bib.bib33" title="" class="ltx_ref">2005</a>)</cite> uses positive and negative pairs, incentivizing positives to have the same representation and negatives to be above a threshold distance apart. A cosine loss uses a continuous measure of the difference between instances. With triplet loss <cite class="ltx_cite ltx_citemacro_citep">(Hermans, Beyer and Leibe, <a href="#bib.bib64" title="" class="ltx_ref">2017</a>)</cite>, training data consist of triplets: an anchor, one positive example for the anchor, and one negative example. Embeddings of positives are incentivized to be more similar to the anchor than negatives. With the InfoNCE loss <cite class="ltx_cite ltx_citemacro_citep">(Oord, Li and Vinyals, <a href="#bib.bib101" title="" class="ltx_ref">2018</a>)</cite>, multiple negative examples are compared to a single positive example. Supervised contrastive loss <cite class="ltx_cite ltx_citemacro_citep">(Khosla et al., <a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite> generalizes InfoNCE to allow for multiple positive and negative examples. More details, including mathematical formulations, are provided in the ‘Contrastive Learning’ post in the EconDL knowledge base.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p">The metric space created by a neural network should be interpreted based on the contrastive loss function used to train the network. For instance, with contrastive loss, instances in the same class are incentivized to have similar representations, whereas different instances are incentivized to be above a threshold distance apart. Hence, local distances are meaningful, but global distances are not, since being more dissimilar beyond the threshold does not affect the loss.</p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.1" class="ltx_p">When contrastively training on paired data, the weights of the two encoders used to embed each of the instances can be the same (a symmetric encoder) or different (an asymmetric encoder, as in <cite class="ltx_cite ltx_citemacro_citet">Karpukhin et al. (<a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite>). Symmetric encoders provide a more parsimonious model, and hence require less data and compute to train. In practice, they can perform well even when two distinct types of instances are being encoded (<span id="S7.SS1.p5.1.1" class="ltx_text ltx_font_italic">e.g.,</span> search queries and documents that contain their answers).</p>
</div>
<div id="S7.SS1.p6" class="ltx_para">
<p id="S7.SS1.p6.1" class="ltx_p">Selecting informative negative examples is important for contrastive learning. If the negatives are too ‘easy’—<span id="S7.SS1.p6.1.1" class="ltx_text ltx_font_italic">e.g.,</span> they are dissimilar in the embedding space of the pre-trained model used to initialize training—little is learned. In some contexts, the researcher can use prior knowledge to select ‘hard’ negatives for training. In other contexts, they can be mined by using a pre-trained model to choose negative examples with similar embeddings. This can work well if the researcher knows ex ante which instances are negative, as would be the case when training on synthetically generated data. When negatives are drawn from unlabeled data, however, mining hard negatives without a human in the loop risks inadvertently selecting positives. Sometimes training on random negatives may be sufficient, although this often requires large batch sizes (<span id="S7.SS1.p6.1.2" class="ltx_text ltx_font_italic">e.g.,</span> GPU cards with a lot of memory), which is not amenable to academic compute budgets. This section’s applications consider how negatives are selected.</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="/html/2407.15339/assets/figures/ComparativeAgendas.png" id="S7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Embedding similarities within and across topics.</figcaption>
</figure>
<div id="S7.SS1.p7" class="ltx_para">
<p id="S7.SS1.p7.1" class="ltx_p">Embeddings are also available off-the-shelf. Sentence-BERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib108" title="" class="ltx_ref">2019</a>)</cite> is a prominent and well-supported open-source model. (In this literature, the term ‘sentence’ is used to refer to any text sequence, which could be a phrase, sentence, or entire document.) Moreover, OpenAI sells quite affordable sentence embeddings.</p>
</div>
<div id="S7.SS1.p8" class="ltx_para">
<p id="S7.SS1.p8.1" class="ltx_p">While there has been considerable interest in developing all-purpose embedding models that can excel zero-shot on any task <cite class="ltx_cite ltx_citemacro_citep">(Cao, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>—and a larger model will on average outperform a smaller model zero-shot—fine-tuned lightweight embedding models have important advantages over zero-shot embeddings. Intuitively, embeddings provide a single representation for each text (or image). An off-the-shelf representation will capture lots of different information about the text/image. However, the researcher is typically interested in some narrowly defined aspects of it. Fine-tuning will accentuate the relevant dimensions, creating better separation between classes in embedding space.</p>
</div>
<div id="S7.SS1.p9" class="ltx_para">
<p id="S7.SS1.p9.1" class="ltx_p">Consider the following empirical example. I take the comparative agendas dataset on U.S. legislation <cite class="ltx_cite ltx_citemacro_citep">(Wilkerson et al., <a href="#bib.bib137" title="" class="ltx_ref">2023</a>)</cite>, which assigns topic tags to congressional bills, and calculate pairwise similarities between the embeddings of the legislative descriptions using three different models: off-the-shelf lightweight S-BERT embeddings (Figure <a href="#S7.F7" title="Figure 7 ‣ 7.1 Contrastive learning ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, panel a), OpenAI large embeddings (panel b), and embeddings produced by tuning S-BERT on paired positives and (random) negative bills from a training split of these data (panel c). The blue distribution plots cosine similarities within topics, and the red line plots cosine similarities between topics. Identical representations have a similarity of 1.</p>
</div>
<div id="S7.SS1.p10" class="ltx_para">
<p id="S7.SS1.p10.1" class="ltx_p">With off-the-shelf models, embeddings are indeed more similar within than across topics (<span id="S7.SS1.p10.1.1" class="ltx_text ltx_font_italic">e.g.,</span> the blue distribution is shifted to the right of the red distribution), but the differences are not stark. SBERT and OpenAI perform similarly. In contrast, once the model has been tuned on target data, embeddings are much more similar within topic than across topic, as the contrastive training accentuates the importance of topic in determining how the language model maps texts to embedding space. Much of the overlap in distributions comes from edge cases where articles fall between topics or cover multiple themes. Panel (d) uses the model tuned on U.S. bills to compare embeddings of UK Acts of Parliament within and across topics. While there is some domain shift, there continues to be marked separation, showing the fine-tuned model’s ability to generalize to similar problems.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Record linkage with structured data</h3>

<figure id="S7.F8" class="ltx_figure"><img src="/html/2407.15339/assets/figures/LTarchitecture.png" id="S7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S7.F8.2.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> architecture.</figcaption>
</figure>
<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Record linkage is central to many economic analyses. A researcher might need to link individuals, locations, firms, organizations, product descriptions, or academic papers across datasets. Traditionally, records have been linked using measures like Levenshtein edit distance—which counts the number of character insertions, deletions, and substitutions to convert one string into another—or Jaccard similarity—which computes the similarity between substring n-gram representations of strings. A recent machine learning literature focused on matching across e-commerce datasets shows the promise of transformer LLMs for improving record linkage. Yet at the time of writing, these methods have not yet made widespread inroads in social science, with rule-based methods continuing to overwhelmingly predominate (<span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> see reviews by <cite class="ltx_cite ltx_citemacro_citet">Binette and Steorts (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>); Abramitzky et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>); Bailey et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>).</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">To make these methods more accessible, <cite class="ltx_cite ltx_citemacro_citet">Arora and Dell (<a href="#bib.bib11" title="" class="ltx_ref">2024</a>)</cite> designed <span id="S7.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span>, a package for using transformer models for record linkage that is geared towards social scientists. The study documents that transformers outperform traditional string matching methods across a variety of tasks and languages, often by a wide margin. Applications include linking 1940 Mexican tariff schedules and linking 1950 Japanese firm-level records using multiple noisy fields, as well as linking modern firms and products across six languages. A multilingual model can link products across languages without the need for translation.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">This work was motivated by a variety of projects that I had to abandon in the pre-deep learning era because sparse methods performed poorly and hand-linking was infeasible. As with any predictive task, it is incumbent upon the researcher to evaluate whether performance is acceptable using a test set.</p>
</div>
<div id="S7.SS2.p4" class="ltx_para">
<p id="S7.SS2.p4.1" class="ltx_p">The <span id="S7.SS2.p4.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model architecture is shown in Figure <a href="#S7.F8" title="Figure 8 ‣ 7.2 Record linkage with structured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The texts that need to be matched are encoded using a transformer language model. For each query, <span id="S7.SS2.p4.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> finds the nearest neighbors in the corpus, as measured by the cosine similarity between the embeddings. This is extremely fast, as it uses the highly optimized FAISS (Facebook Artificial Intelligence Similarity Search) backend <cite class="ltx_cite ltx_citemacro_citep">(Johnson, Douze and Jégou, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>. <span id="S7.SS2.p4.1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span> returns a ranking as well as the cosine similarity scores, which can be used for 1-1, 1-many, or many-many merges, including no matches (captured when the similarity to the nearest record is below some threshold).</p>
</div>
<div id="S7.SS2.p5" class="ltx_para">
<p id="S7.SS2.p5.1" class="ltx_p">Just as traditional sparse methods like edit distance return distances between records, <span id="S7.SS2.p5.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> computes a distance metric powered by all the semantic knowledge embodied in the pre-trained language model, as well as any additional knowledge gained through contrastive training. For example, ABC Corporation, ABC Co., and ABCC are very similar semantically—and hence nearby in embedding space—since ‘Co.’ and ‘C’ represent ‘Corporation,’ but these strings have a high Levenshtein edit distance. Examples like this are common in record linkage tasks, given the prevalence of abbreviations, different ways to describe the same product or firm name, OCR errors, and typos. Embedding similarity can be used analogously to how researchers use string distance metrics.</p>
</div>
<div id="S7.SS2.p6" class="ltx_para">
<p id="S7.SS2.p6.1" class="ltx_p"><span id="S7.SS2.p6.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> seamlessly supports linking with multiple fields, serializing fields by concatenating them with a <span id="S7.SS2.p6.1.2" class="ltx_text ltx_font_typewriter">&lt;sep&gt;</span> token that the package automatically selects to be compatible with the base language model tokenizer. The study provides an example of linking 1950s Japanese firms across different large-scale, noisy databases using the firm name, location, products, shareholders, and banks. This type of linkage problem would be highly convoluted with string matching methods, as fields are noisy (<span id="S7.SS2.p6.1.3" class="ltx_text ltx_font_italic">e.g.,</span> products are described in different ways across datasets, different subsets of managers and shareholders are listed, etc.). A large language model can handle these challenges with ease because it captures semantic similarity.</p>
</div>
<div id="S7.SS2.p7" class="ltx_para">
<p id="S7.SS2.p7.1" class="ltx_p"><span id="S7.SS2.p7.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> allows users to employ Sentence Transformer models, OpenAI embeddings, models tuned on the target task, or any transformer language model available on Hugging Face. The general picture that emerges, across 20 diverse linking tasks, is that models custom trained on a modest number of labels tend to perform best, followed by off-the-shelf embeddings from OpenAI, and then off-the-shelf Sentence Transformer models (though there is some variation from task to task). The results are consistent with the discussion of off-the-shelf versus customized embedding models above.</p>
</div>
<div id="S7.SS2.p8" class="ltx_para">
<p id="S7.SS2.p8.1" class="ltx_p"><span id="S7.SS2.p8.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> also provides APIs to use transformer large language models for other data processing tasks, <span id="S7.SS2.p8.1.2" class="ltx_text ltx_font_italic">e.g.</span>, classification, aggregation, and de-duplication, as outlined in tutorial notebooks linked on EconDL. Users can also find a ‘Train Your Own <span id="S7.SS2.p8.1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span> Model’ tutorial, for when customization is necessary. Contrastive training requires both positive and negative pairs (in this case, linked records and distinct records). The user can provide only positives—in which case <span id="S7.SS2.p8.1.4" class="ltx_text ltx_font_typewriter">LinkTransformer</span> chooses negatives at random—or can provide both positives and negatives, if hard negatives are available. To promote reusability, reproducibility, and extensibility, models can be shared to the Hugging Face hub with a single line of code.</p>
</div>
<div id="S7.SS2.p9" class="ltx_para">
<p id="S7.SS2.p9.1" class="ltx_p">When the task is to link scanned documents (as in much of economic history), computer vision may also be useful.
In the vision only record linkage models developed in <cite class="ltx_cite ltx_citemacro_citet">Arora et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, record linkage is OCR-free, using only the image crops of the firm names to be linked. This is explored in a challenging setting, linking firms between historical Japanese publications where one publication is written horizontally and the other vertically. In general, vision-only linkage leads to reasonably high accuracy.
However, there are some matches that a vision model cannot resolve because a firm can write its name in different ways.</p>
</div>
<div id="S7.SS2.p10" class="ltx_para">
<p id="S7.SS2.p10.1" class="ltx_p">Embedding models can combine vision and language transformers, leveraging an understanding of both semantic and visual similarity. <cite class="ltx_cite ltx_citemacro_citet">Arora et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> show that a multimodal model leads to extremely accurate linkage of OCR’ed Japanese firm-level records on customers and suppliers, whereas less accurate string distance metrics produce a different supply chain network that would likely lead to biased downstream economic analyses.</p>
</div>
<div id="S7.SS2.p11" class="ltx_para">
<p id="S7.SS2.p11.1" class="ltx_p">Combining text and image embeddings is most straightforward when the embedding spaces are aligned. In other words, image and text representations of the same thing need to have similar embeddings; <span id="S7.SS2.p11.1.1" class="ltx_text ltx_font_italic">e.g.,</span> a picture of an avocado and the text ’avocado’ have similar embeddings when an image and text encoder respectively map them to vector space. <cite class="ltx_cite ltx_citemacro_citet">Arora et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> start with a Japanese version of CLIP, which stands for Contrastive Language-Image Pre-training. CLIP is an OpenAI model contrastively trained to align text and image encoders using 400 million image-caption pairs scraped from the web <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib104" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Arora et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> do further pre-training on synthetically noised pairs of document crops and their corresponding OCR’ed texts. Pooled text-image representations are then used to link firms.</p>
</div>
<div id="S7.SS2.p12" class="ltx_para">
<p id="S7.SS2.p12.1" class="ltx_p">There are other ways to incorporate visual similarity into linkage that may be helpful when OCR errors are present. Approximate string matching methods count the number of edits (insertions, deletions, and substitutions) needed to transform one string into another <cite class="ltx_cite ltx_citemacro_citep">(Levenshtein et al., <a href="#bib.bib83" title="" class="ltx_ref">1966</a>)</cite>. In practice, not all string substitutions are equally probable, and efforts to construct lists that vary their costs date back at least to 1918, when Russell and Odell patented Soundex <cite class="ltx_cite ltx_citemacro_citep">(Russell, <a href="#bib.bib112" title="" class="ltx_ref">1918</a>; Archives and Administration, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, a sound standardization toolkit that accounts for the fact that census enumerators often misspelled names according to their sound. Such methods can significantly improve the accuracy of edit distance linking in the contexts for which they are tailored but are labor-intensive to extend to new settings due to the use of hand-crafted features. This skews research with linked data—necessary to examine many economic questions—towards higher resource settings that are not representative of the diversity of human societies.</p>
</div>
<div id="S7.SS2.p13" class="ltx_para">
<p id="S7.SS2.p13.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023</a>)</cite> develop an extensible, self-supervised method for determining the relative costs of character substitutions in databases created with OCR. OCR often confuses characters with their homoglyphs, which have a similar visual appearance (<span id="S7.SS2.p13.1.1" class="ltx_text ltx_font_italic">e.g.,</span> ’0’ and ’O’). <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023</a>)</cite> augment digital fonts to contrastively learn a metric space where different augmentations of a character (<span id="S7.SS2.p13.1.2" class="ltx_text ltx_font_italic">e.g.,</span> the same character rendered with different fonts) have similar vector representations. The resulting space can be used, with a reference font, to measure the visual similarity across different characters. Examples of characters and their nearest neighbors in homoglyph space are shown in Figure <a href="#S7.F9" title="Figure 9 ‣ 7.2 Record linkage with structured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S7.F9" class="ltx_figure"><img src="/html/2407.15339/assets/figures/char_near.png" id="S7.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="568" height="778" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Character similarity, as measured by vision transformers.</figcaption>
</figure>
<div id="S7.SS2.p14" class="ltx_para">
<p id="S7.SS2.p14.1" class="ltx_p">Using the cosine distance between characters in the homoglyph space as the substitution cost within a Levenshtein edit distance framework <cite class="ltx_cite ltx_citemacro_citep">(Levenshtein et al., <a href="#bib.bib83" title="" class="ltx_ref">1966</a>)</cite> significantly improves linkage of firms and placenames. The study focuses on CJK, as the extremely large number of characters in this script make it completely infeasible to compute homoglyphs by hand, but shows that the method is extensible by computing homoglyphs for ancient Chinese characters and for all of Unicode. The broader takeaway is even when traditional methods—like string distance—are preferred, deep learning may provide a way to cheaply extend the methods to novel settings.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Linking unstructured data</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">There is also a large NLP literature on linking entities mentioned in unstructured texts (<span id="S7.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> news, social media, etc.), a task referred to as <span id="S7.SS3.p1.1.2" class="ltx_text ltx_font_italic">entity disambiguation</span>. Linking entity mentions in raw texts (tagged through NER - Section <a href="#S6.SS4" title="6.4 Token classification ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>) to Wikipedia or other knowledge bases is useful, because these contain information such as structured biographical data. Whether individuals are in an external knowledge base may itself also be of interest.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">Researchers might also wish to coreference entity mentions across documents in a corpus (<span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> find every reference to President John F. Kennedy in historical news). This is referred to as <span id="S7.SS3.p2.1.2" class="ltx_text ltx_font_italic">coreference resolution.</span></p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">Despite increasing digitization, historical documents typically lack cross-document identifiers for individuals mentioned in the texts, as well as identifiers from external knowledge bases like Wikipedia, both of which would make it much easier for economists to extract structured data from these sources.</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite> develop a bi-encoder embedding model for coreferencing entities within texts and disambiguating them to Wikipedia. The model is contrastively trained on over 190 million entity pairs from Wikipedia.
Positive pairs come from contexts (paragraphs) in Wikipedia that contain hyperlinks to the same page (for coreference), or from a context and the first paragraph of the relevant entity that it links to (for disambiguation).
Hard negatives are mined at scale from Wikipedia disambiguation pages, which list entities that have confusable names or aliases. For example, the disambiguation page ”John Kennedy” includes John F. Kennedy the president, John Kennedy (Louisiana politician), John F. Kennedy Jr, and a variety of other John Kennedys. Hard negatives sample contexts mentioning John F. Kennedy (<span id="S7.SS3.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> with hyperlinks to John F. Kennedy’s page) and pair them with contexts mentioning other entities from the John Kennedy disambiguation page.
Hard negatives from families (<span id="S7.SS3.p4.1.2" class="ltx_text ltx_font_italic">e.g.</span>, Henry Ford Jr. and Sr.) are over-represented by mining family members from Wikidata.
It was also necessary to include random negatives, as otherwise the model lost its initial ability to distinguish easy cases, a phenomenon known in the deep learning literature as <span id="S7.SS3.p4.1.3" class="ltx_text ltx_font_italic">catastrophic forgetting</span>.</p>
</div>
<div id="S7.SS3.p5" class="ltx_para">
<p id="S7.SS3.p5.1" class="ltx_p">This illustrates how existing knowledge can be mined to create informative negatives for contrastive training.
More generally, Wikipedia is a useful source of training data (<span id="S7.SS3.p5.1.1" class="ltx_text ltx_font_italic">e.g.,</span> firm aliases to train <span id="S7.SS3.p5.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models were also taken from Wikidata).</p>
</div>
<div id="S7.SS3.p6" class="ltx_para">
<p id="S7.SS3.p6.1" class="ltx_p">Entity mentions are disambiguated by embedding their contexts with the disambiguation model and retrieving their nearest Wikipedia neighbor in embedding space. If they are below a threshold cosine similarity to the nearest Wikipedia embedding, they are marked as not in the knowledge base.
Figure <a href="#S7.F10" title="Figure 10 ‣ 7.3 Linking unstructured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows that entity mentions in 100 years of newswire coverage, spanning 1878-1977, correlate strongly with Wikipedia Qrank, which ranks Wikidata entities by aggregate page views in the Wikiverse <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_cite">Silcock et al. (<a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite> find that the most mentioned entity in these 100 years is Dwight Eisenhower—edging out Adolf Hitler, Richard Nixon, and Harry Truman—and only 4.7% of disambiguated entities in newswire articles are women.</p>
</div>
<figure id="S7.F10" class="ltx_figure"><img src="/html/2407.15339/assets/figures/decile_mean_comparison_scatter_with_fit_full.png" id="S7.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Mentions against Wikipedia Qrank.</figcaption>
</figure>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Classification when categories are unknown</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">The classes that a researcher would like to impute from unstructured data may be unknown ex ante. This is particularly likely when the aim is to describe the stylized facts in a novel unstructured corpus, but can arise more generally. This section provides several examples drawn from media economics: detecting reproduced article texts and images, classifying the biggest news stories historically, and retrieving historical news stories that are semantically similar to modern ones.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">Reproduced content is a fundamental feature of media—both traditional media and in the age of sharing via social media. Media historian Julia <cite class="ltx_cite ltx_citemacro_citet">Guarneri (<a href="#bib.bib54" title="" class="ltx_ref">2017</a>)</cite> writes: “by the 1910s and 1920s, most of the articles that Americans read in their local papers had either been bought or sold on the national news market… This constructed a broadly understood American ‘way of life’ that would become a touchstone of U.S. domestic politics and international relations throughout the twentieth century.” Suppose we would like to be able to identify each unique article and image that was sent out over the newswire, measure how widely reproduced it was, and observe which papers reproduced it. This problem is more challenging than it seems. Texts are often heavily abridged and can contain significant OCR errors. Images are often cropped, and the quality can be extremely low.</p>
</div>
<div id="S7.SS4.p3" class="ltx_para">
<p id="S7.SS4.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Silcock et al. (<a href="#bib.bib124" title="" class="ltx_ref">2023</a>)</cite> show that deep neural methods can significantly outperform non-neural methods for detecting noisily duplicated texts, with applications to historical newswires, modern news, and patent databases. Training and evaluation data are hand-curated by grouping articles across thousands of digitized local newspapers into groups of articles from the same newswire source. Articles from the same wire source are positives. Articles with high <math id="S7.SS4.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S7.SS4.p3.1.m1.1a"><mi id="S7.SS4.p3.1.m1.1.1" xref="S7.SS4.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p3.1.m1.1b"><ci id="S7.SS4.p3.1.m1.1.1.cmml" xref="S7.SS4.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p3.1.m1.1c">n</annotation></semantics></math>-gram overlap but from different sources—often similar articles from different newswire services or article updates—form hard negatives. Random negatives are also used in training.</p>
</div>
<div id="S7.SS4.p4" class="ltx_para">
<p id="S7.SS4.p4.1" class="ltx_p">A bi-encoder embedding model is contrastively trained such that articles from the same wire article source (regardless of noise and abridgement) have similar vector representations, while articles from different sources (even if about the same underlying story) have different representations. These representations can then be clustered with highly efficient single-linkage clustering to quantify which articles are from the same underlying news wire or syndicate article source and which are from different ones. Community detection is used to break the spurious links that are a potential drawback of single-linkage clustering.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Other common clustering methods, like hierarchical agglomerative clustering, do not scale well.</span></span></span> As with record linkage, the Sentence-BERT library <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib108" title="" class="ltx_ref">2019</a>)</cite> was an important resource. The model is initialized with the S-BERT MPNet bi-encoder, a lightweight, high-performing semantic similarity model.</p>
</div>
<div id="S7.SS4.p5" class="ltx_para">
<p id="S7.SS4.p5.1" class="ltx_p">The neural approach outperforms traditional <math id="S7.SS4.p5.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S7.SS4.p5.1.m1.1a"><mi id="S7.SS4.p5.1.m1.1.1" xref="S7.SS4.p5.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p5.1.m1.1b"><ci id="S7.SS4.p5.1.m1.1.1.cmml" xref="S7.SS4.p5.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p5.1.m1.1c">N</annotation></semantics></math>-gram and hashing methods - sparse methods that rely on term overlap to detect noisy duplicates - by a wide margin. As suggested in Section <a href="#S6.SS5" title="6.5 Relationships between texts ‣ 6 Classifiers ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>, modest gains result from adding a re-ranking step that applies a cross-encoder to articles within a threshold bi-encoder distance.</p>
</div>
<div id="S7.SS4.p6" class="ltx_para">
<p id="S7.SS4.p6.1" class="ltx_p">A strength of embedding methods is their scalability. Clustering dense vector representations requires highly optimized similarity search, as traditional clustering libraries don’t scale well. Facebook AI Similarity Search (FAISS) <cite class="ltx_cite ltx_citemacro_citep">(Johnson, Douze and Jégou, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>, an open-source library for computing vector similarity, made <math id="S7.SS4.p6.1.m1.1" class="ltx_Math" alttext="10^{14}" display="inline"><semantics id="S7.SS4.p6.1.m1.1a"><msup id="S7.SS4.p6.1.m1.1.1" xref="S7.SS4.p6.1.m1.1.1.cmml"><mn id="S7.SS4.p6.1.m1.1.1.2" xref="S7.SS4.p6.1.m1.1.1.2.cmml">10</mn><mn id="S7.SS4.p6.1.m1.1.1.3" xref="S7.SS4.p6.1.m1.1.1.3.cmml">14</mn></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.p6.1.m1.1b"><apply id="S7.SS4.p6.1.m1.1.1.cmml" xref="S7.SS4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.p6.1.m1.1.1.1.cmml" xref="S7.SS4.p6.1.m1.1.1">superscript</csymbol><cn type="integer" id="S7.SS4.p6.1.m1.1.1.2.cmml" xref="S7.SS4.p6.1.m1.1.1.2">10</cn><cn type="integer" id="S7.SS4.p6.1.m1.1.1.3.cmml" xref="S7.SS4.p6.1.m1.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p6.1.m1.1c">10^{14}</annotation></semantics></math> exact similarity comparisons—required to cluster 10 million article representations—on a single GPU card in around 3 hours. This could have been sped up significantly, with only a modest hit to accuracy, by using approximate vector search (with appropriately tuned hyperparameters).</p>
</div>
<div id="S7.SS4.p7" class="ltx_para">
<p id="S7.SS4.p7.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Silcock et al. (<a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite> release 2.7 million unique newswire articles spanning 1878-1977 (the end date is due to copyright law changes). It includes topic tags, named entity tags, disambiguation of individuals to Wikipedia, and the counties where articles ran.</p>
</div>
<div id="S7.SS4.p8" class="ltx_para">
<p id="S7.SS4.p8.1" class="ltx_p">Detecting the noisy reproduction of images is analogous to detecting reproduced texts. Rather than training a language model, a vision model can be contrastively trained to map reproduced versions of the same image to similar vector representations and different images to dissimilar representations. A lightweight CNN works well in practice <cite class="ltx_cite ltx_citemacro_citep">(Howard et al., <a href="#bib.bib67" title="" class="ltx_ref">2019</a>)</cite>, with little gain from using a much larger ViT. Training data consist mostly of synthetically augmented images—which simulate the noise present in the actual images. When it is possible to simulate realistic synthetic data, this can save considerable annotation expense, though adding a modest number of labeled examples from target data may still offer a performance boost.</p>
</div>
<div id="S7.SS4.p9" class="ltx_para">
<p id="S7.SS4.p9.1" class="ltx_p">Embedding models are well-suited to assessing stylized facts in unstructured data at scale. Deep learning makes it possible to use a variety of novel unstructured datasets in economic research. While our focus is often on using causal estimation to test precisely defined hypotheses, understanding stylized facts is an important first step to formulating these hypotheses.</p>
</div>
<div id="S7.SS4.p10" class="ltx_para">
<p id="S7.SS4.p10.1" class="ltx_p">Consider an application from <cite class="ltx_cite ltx_citemacro_citet">Dell et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, which constructs a historical newspaper dataset consisting of over 430 million historical U.S. newspaper articles. The aim of the exercise is to determine the biggest news stories of each year without knowing what these stories are ex ante. The study contrastively trains a model on data from AllSides, a modern news website that groups news articles from different sources into stories (often with different perspectives on the same event).<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://www.allsides.com/unbiased-balanced-news" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.allsides.com/unbiased-balanced-news</a></span></span></span> Grouped stories form positive pairs for training, with the model learning what constitutes the “same story” via these empirical examples.
The trained model is used to embed articles, and stories are formed by clustering. Table <a href="#S7.T3" title="Table 3 ‣ 7.4 Classification when categories are unknown ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports the largest cluster for each year <cite class="ltx_cite ltx_citemacro_citep">(Dell et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>. Some interesting stylized facts emerge, particularly the extensive coverage of labor movements. If a researcher wanted to create a measure of labor movements to use in a causal estimating equation, they would likely train a classifier with carefully crafted labels to predict which articles are about labor movements. In contrast, this exercise motivates why labor movements are important to study in the first place.</p>
</div>
<figure id="S7.T3" class="ltx_table">
<div id="S7.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:1420.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(115.2pt,-377.3pt) scale(2.13313656659812,2.13313656659812) ;">
<table id="S7.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T3.1.1.1.1" class="ltx_tr">
<th id="S7.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S7.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S7.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S7.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Biggest story</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T3.1.1.2.1" class="ltx_tr">
<th id="S7.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1885</th>
<td id="S7.T3.1.1.2.1.2" class="ltx_td ltx_align_left">Death of General Grant</td>
</tr>
<tr id="S7.T3.1.1.3.2" class="ltx_tr">
<th id="S7.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1886</th>
<td id="S7.T3.1.1.3.2.2" class="ltx_td ltx_align_left">Southwest Railroad Strike</td>
</tr>
<tr id="S7.T3.1.1.4.3" class="ltx_tr">
<th id="S7.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1887</th>
<td id="S7.T3.1.1.4.3.2" class="ltx_td ltx_align_left">Vatican supports Knights of Labor</td>
</tr>
<tr id="S7.T3.1.1.5.4" class="ltx_tr">
<th id="S7.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1888</th>
<td id="S7.T3.1.1.5.4.2" class="ltx_td ltx_align_left">Rail strikes</td>
</tr>
<tr id="S7.T3.1.1.6.5" class="ltx_tr">
<th id="S7.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1889</th>
<td id="S7.T3.1.1.6.5.2" class="ltx_td ltx_align_left">Samoan Crisis</td>
</tr>
<tr id="S7.T3.1.1.7.6" class="ltx_tr">
<th id="S7.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1890</th>
<td id="S7.T3.1.1.7.6.2" class="ltx_td ltx_align_left">1893 World’s Fair planning</td>
</tr>
<tr id="S7.T3.1.1.8.7" class="ltx_tr">
<th id="S7.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1891</th>
<td id="S7.T3.1.1.8.7.2" class="ltx_td ltx_align_left">New Orleans Lynchings</td>
</tr>
<tr id="S7.T3.1.1.9.8" class="ltx_tr">
<th id="S7.T3.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1892</th>
<td id="S7.T3.1.1.9.8.2" class="ltx_td ltx_align_left">Homestead Steel Strike</td>
</tr>
<tr id="S7.T3.1.1.10.9" class="ltx_tr">
<th id="S7.T3.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1893</th>
<td id="S7.T3.1.1.10.9.2" class="ltx_td ltx_align_left">World’s Fair, Chicago</td>
</tr>
<tr id="S7.T3.1.1.11.10" class="ltx_tr">
<th id="S7.T3.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1894</th>
<td id="S7.T3.1.1.11.10.2" class="ltx_td ltx_align_left">Wilson–Gorman Tariff Act</td>
</tr>
<tr id="S7.T3.1.1.12.11" class="ltx_tr">
<th id="S7.T3.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1895</th>
<td id="S7.T3.1.1.12.11.2" class="ltx_td ltx_align_left">British occupation of Nicaragua</td>
</tr>
<tr id="S7.T3.1.1.13.12" class="ltx_tr">
<th id="S7.T3.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1896</th>
<td id="S7.T3.1.1.13.12.2" class="ltx_td ltx_align_left">Bimetallism Movement</td>
</tr>
<tr id="S7.T3.1.1.14.13" class="ltx_tr">
<th id="S7.T3.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1897</th>
<td id="S7.T3.1.1.14.13.2" class="ltx_td ltx_align_left">Coal Miners’ Strike</td>
</tr>
<tr id="S7.T3.1.1.15.14" class="ltx_tr">
<th id="S7.T3.1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1898</th>
<td id="S7.T3.1.1.15.14.2" class="ltx_td ltx_align_left">Cuban War of Independence</td>
</tr>
<tr id="S7.T3.1.1.16.15" class="ltx_tr">
<th id="S7.T3.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1899</th>
<td id="S7.T3.1.1.16.15.2" class="ltx_td ltx_align_left">Philippine-American War</td>
</tr>
<tr id="S7.T3.1.1.17.16" class="ltx_tr">
<th id="S7.T3.1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1900</th>
<td id="S7.T3.1.1.17.16.2" class="ltx_td ltx_align_left">Anglo-Boer War</td>
</tr>
<tr id="S7.T3.1.1.18.17" class="ltx_tr">
<th id="S7.T3.1.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1901</th>
<td id="S7.T3.1.1.18.17.2" class="ltx_td ltx_align_left">U.S. Steel Recognition Strike</td>
</tr>
<tr id="S7.T3.1.1.19.18" class="ltx_tr">
<th id="S7.T3.1.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1902</th>
<td id="S7.T3.1.1.19.18.2" class="ltx_td ltx_align_left">Anthracite Coal Strike</td>
</tr>
<tr id="S7.T3.1.1.20.19" class="ltx_tr">
<th id="S7.T3.1.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1903</th>
<td id="S7.T3.1.1.20.19.2" class="ltx_td ltx_align_left">Panama Canal Treaty</td>
</tr>
<tr id="S7.T3.1.1.21.20" class="ltx_tr">
<th id="S7.T3.1.1.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1904</th>
<td id="S7.T3.1.1.21.20.2" class="ltx_td ltx_align_left">Russo-Japanese War</td>
</tr>
<tr id="S7.T3.1.1.22.21" class="ltx_tr">
<th id="S7.T3.1.1.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1905</th>
<td id="S7.T3.1.1.22.21.2" class="ltx_td ltx_align_left">Russo-Japanese Peace Process</td>
</tr>
<tr id="S7.T3.1.1.23.22" class="ltx_tr">
<th id="S7.T3.1.1.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1906</th>
<td id="S7.T3.1.1.23.22.2" class="ltx_td ltx_align_left">Hepburn Railroad Rate Bill</td>
</tr>
<tr id="S7.T3.1.1.24.23" class="ltx_tr">
<th id="S7.T3.1.1.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1907</th>
<td id="S7.T3.1.1.24.23.2" class="ltx_td ltx_align_left">Mining accidents</td>
</tr>
<tr id="S7.T3.1.1.25.24" class="ltx_tr">
<th id="S7.T3.1.1.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1908</th>
<td id="S7.T3.1.1.25.24.2" class="ltx_td ltx_align_left">Taft presidential victory</td>
</tr>
<tr id="S7.T3.1.1.26.25" class="ltx_tr">
<th id="S7.T3.1.1.26.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1909</th>
<td id="S7.T3.1.1.26.25.2" class="ltx_td ltx_align_left">Race to the North Pole</td>
</tr>
<tr id="S7.T3.1.1.27.26" class="ltx_tr">
<th id="S7.T3.1.1.27.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1910</th>
<td id="S7.T3.1.1.27.26.2" class="ltx_td ltx_align_left">Rail strikes</td>
</tr>
<tr id="S7.T3.1.1.28.27" class="ltx_tr">
<th id="S7.T3.1.1.28.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1911</th>
<td id="S7.T3.1.1.28.27.2" class="ltx_td ltx_align_left">Canadian Reciprocity Bill</td>
</tr>
<tr id="S7.T3.1.1.29.28" class="ltx_tr">
<th id="S7.T3.1.1.29.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1912</th>
<td id="S7.T3.1.1.29.28.2" class="ltx_td ltx_align_left">Republican National Convention</td>
</tr>
<tr id="S7.T3.1.1.30.29" class="ltx_tr">
<th id="S7.T3.1.1.30.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1913</th>
<td id="S7.T3.1.1.30.29.2" class="ltx_td ltx_align_left">Underwood-Simmons Tariff Act</td>
</tr>
<tr id="S7.T3.1.1.31.30" class="ltx_tr">
<th id="S7.T3.1.1.31.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1914</th>
<td id="S7.T3.1.1.31.30.2" class="ltx_td ltx_align_left">World War I</td>
</tr>
<tr id="S7.T3.1.1.32.31" class="ltx_tr">
<th id="S7.T3.1.1.32.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1915</th>
<td id="S7.T3.1.1.32.31.2" class="ltx_td ltx_align_left">World War I</td>
</tr>
<tr id="S7.T3.1.1.33.32" class="ltx_tr">
<th id="S7.T3.1.1.33.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1916</th>
<td id="S7.T3.1.1.33.32.2" class="ltx_td ltx_align_left">Pancho Villa Expedition</td>
</tr>
<tr id="S7.T3.1.1.34.33" class="ltx_tr">
<th id="S7.T3.1.1.34.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1917</th>
<td id="S7.T3.1.1.34.33.2" class="ltx_td ltx_align_left">World War I</td>
</tr>
<tr id="S7.T3.1.1.35.34" class="ltx_tr">
<th id="S7.T3.1.1.35.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1918</th>
<td id="S7.T3.1.1.35.34.2" class="ltx_td ltx_align_left">World War I</td>
</tr>
<tr id="S7.T3.1.1.36.35" class="ltx_tr">
<th id="S7.T3.1.1.36.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1919</th>
<td id="S7.T3.1.1.36.35.2" class="ltx_td ltx_align_left">Treaty of Versailles</td>
</tr>
<tr id="S7.T3.1.1.37.36" class="ltx_tr">
<th id="S7.T3.1.1.37.36.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1920</th>
<td id="S7.T3.1.1.37.36.2" class="ltx_td ltx_align_left">Rail strikes</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Biggest news stories.</figcaption>
</figure>
<div id="S7.SS4.p11" class="ltx_para">
<p id="S7.SS4.p11.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Franklin et al. (<a href="#bib.bib44" title="" class="ltx_ref">2024</a>)</cite> use this model for additional data exploration, first masking out all named entities (people, organizations, locations, and other miscellaneous proper nouns) and then querying the most similar historical news articles to a modern news article query in embedding space. The resulting <span id="S7.SS4.p11.1.1" class="ltx_text ltx_font_typewriter">News Déjà Vu</span> open-source package and website provide a novel tool for exploring parallels in how people have perceived past and present.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Optical character recognition</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">Optical character recognition (OCR) is an important task for economists, particularly for economic historians. Documents are extremely diverse in terms of character sets, languages, fonts or handwriting, printing technologies, and artifacts from scanning and aging. Off-the-shelf OCR technology is developed largely for small-scale commercial applications in high-resource languages like English, and the architecture it uses is not well-suited to extending OCR to lower-resource languages and settings, as elaborated in Section <a href="#S9" title="9 Alternative methods ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. OCR quality can deteriorate rapidly when moving away from English and a few other high-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Carlson, Bryan and Dell, <a href="#bib.bib26" title="" class="ltx_ref">2024</a>; Hegghammer, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>. For example, on printed Japanese documents from the 1950s, the best-performing existing OCR mis-predicts over half of the characters. Poor performance is widespread, spurring a large post-OCR error-correction literature <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib90" title="" class="ltx_ref">2021</a>; Nguyen et al., <a href="#bib.bib98" title="" class="ltx_ref">2021</a>; van Strien. et al., <a href="#bib.bib131" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S7.SS5.p2" class="ltx_para">
<p id="S7.SS5.p2.1" class="ltx_p">Even in the highest resource settings, off-the-shelf solutions can still fail, especially when accuracy is paramount. This is particularly true when transcribing quantitative data. An OCR error in prose is often straightforward to correct in post-processing or inconsequential. However, with numbers, a similar error (<span id="S7.SS5.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> hallucinating a “1” at the beginning of a number) may severely bias downstream statistical analyses.</p>
</div>
<div id="S7.SS5.p3" class="ltx_para">
<p id="S7.SS5.p3.1" class="ltx_p">Moreover, the scale of document collections to be digitized can be vast. For example, the U.S. National Archives holds approximately 13.28 billion pages of textual records. Bringing big data to economic history requires an OCR technology that is both accurate and cheap to deploy.</p>
</div>
<div id="S7.SS5.p4" class="ltx_para">
<p id="S7.SS5.p4.1" class="ltx_p">If economists, historians, and others rely solely on off-the-shelf commercial technologies, we will end up focusing on economic applications that look a lot like high resource commercial applications (<span id="S7.SS5.p4.1.1" class="ltx_text ltx_font_italic">e.g.,</span> receipts in English). This is indeed what I’ve seen over many years of working with students: they are far more likely to abandon projects in lower resource languages because the OCR quality of any existing off-the-shelf solution is poor. This skews economic knowledge towards settings that look more like high-resource commercial applications, which are not representative of the diversity of human societies.</p>
</div>
<div id="S7.SS5.p5" class="ltx_para">
<p id="S7.SS5.p5.1" class="ltx_p">To address these challenges, <cite class="ltx_cite ltx_citemacro_citet">Carlson, Bryan and Dell (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite> develop a novel, open-source OCR architecture, <span id="S7.SS5.p5.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> (<span id="S7.SS5.p5.1.2" class="ltx_text ltx_font_bold">Eff</span>icient<span id="S7.SS5.p5.1.3" class="ltx_text ltx_font_bold">OCR</span>). <span id="S7.SS5.p5.1.4" class="ltx_text ltx_font_typewriter">EffOCR</span> is designed for researchers and archives seeking a sample-efficient, customizable, scalable OCR solution for diverse documents. Deep learning-based object detection methods (Section <a href="#S8" title="8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) are used to localize individual characters or words in a document image. Recognition models for characters or words are contrastively trained—largely on augmented digital fonts—to map image crops of the same character or word to similar vector representations, regardless of font and other variations. Different characters or words, even if they have a very similar visual appearance, are mapped further apart.</p>
</div>
<div id="S7.SS5.p6" class="ltx_para">
<p id="S7.SS5.p6.1" class="ltx_p">A document is transcribed by embedding word or character crops and retrieving their nearest neighbors in an index that embeds crops rendered with a digital font. New characters or words can be added to the index after training (unlike with a classifier), a useful feature for economic historians since idiosyncratic symbols frequently appear in historical document collections.</p>
</div>
<figure id="S7.F11" class="ltx_figure"><img src="/html/2407.15339/assets/figures/layouts.png" id="S7.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Layouts from historical newspapers.</figcaption>
</figure>
<div id="S7.SS5.p7" class="ltx_para">
<p id="S7.SS5.p7.1" class="ltx_p"><span id="S7.SS5.p7.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> performs very accurately, even when using lightweight models designed for mobile phones that are cheap to train and deploy. For example, it can provide a sample efficient, highly accurate OCR architecture for historical Japanese documents where all current solutions fail. Its blend of accuracy and efficient runtime also makes it attractive for digitizing massive-scale collections in high-resource languages. <cite class="ltx_cite ltx_citemacro_cite">Dell et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> cheaply digitize over 430 million historical newspaper articles from the Library of Congress’s Chronicling America collection with <span id="S7.SS5.p7.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span>. TrOCR, an open-source solution with similar accuracy, would have cost nearly 50 times more to deploy, and commercial solutions were even more cost prohibitive.</p>
</div>
<div id="S7.SS5.p8" class="ltx_para">
<p id="S7.SS5.p8.1" class="ltx_p">EconDL links to a demo notebook for training a custom OCR for polytonic (ancient) Greek, using minimal cloud compute. <cite class="ltx_cite ltx_citemacro_citet">Carlson, Bryan and Dell (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite> show that this model outperforms Google Cloud Vision on the target data. The notebook uses the <span id="S7.SS5.p8.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> package <cite class="ltx_cite ltx_citemacro_citep">(Bryan et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, which allows users to tune their own OCR models and run existing models off-the-shelf.
<span id="S7.SS5.p8.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span> does not focus on handwriting; however, the approach would be analogous. Synthetic handwriting generators, <span id="S7.SS5.p8.1.3" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Bhunia et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, could provide extensive data for pre-training, analogous to the use of digital fonts.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Regression</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Regression is analogous to classification, except that a regression layer added to a neural network predicts a continuous number(s), rather than a set of class scores. For this reason, our treatment here is brief, focusing on a single application: object detection.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Object detection problems, as the name suggests, locate objects in an image <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib109" title="" class="ltx_ref">2017</a>; He et al., <a href="#bib.bib59" title="" class="ltx_ref">2017</a>; Kirillov et al., <a href="#bib.bib77" title="" class="ltx_ref">2019</a>; Cai and Vasconcelos, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>; Redmon et al., <a href="#bib.bib107" title="" class="ltx_ref">2016</a>; Ultralytics, <a href="#bib.bib130" title="" class="ltx_ref">2020</a>; Carion et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>; Liu et al., <a href="#bib.bib87" title="" class="ltx_ref">2021</a>)</cite>. For example, an economist digitizing firm financial records would need to detect the coordinates of different document objects: <span id="S8.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> table headers, column and row headers, table cells, footnotes, etc. Alternatively, an economist wishing to measure informality from street view data would need to localize street vendors in an image. For each object, the neural network outputs four continuous numbers (top-x, top-y, height, and width of the box containing the object)—a regression problem—as well as the class of that object (<span id="S8.p2.1.2" class="ltx_text ltx_font_italic">e.g.,</span> table header, column header, etc.)—a classification problem.</p>
</div>
<figure id="S8.F12" class="ltx_figure"><img src="/html/2407.15339/assets/figures/test-GCV-3.png" id="S8.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="732" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Layouts as detected by Google Cloud Vision.</figcaption>
</figure>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Figure <a href="#S7.F11" title="Figure 11 ‣ 7.5 Optical character recognition ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows how object detection methods can be used to localize and classify document layout objects (<span id="S8.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, articles, headlines, etc.) in scans of historical newspapers, facilitating the creation of structured digital texts that can be analyzed with modern NLP methods.
In contrast, Figure <a href="#S8.F12" title="Figure 12 ‣ 8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> provides an example of how commercial OCR (Google Cloud Vision) reads a newspaper scan like a single column book, failing to detect individual articles, headlines, etc.
All one can do with these scrambled texts is search for keywords, as is typical in the economic literature using historical newspapers (see <cite class="ltx_cite ltx_citemacro_citet">Hanlon and Beach (<a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite> for a review).
Layout detection is also needed to extract structure when digitizing tabular data, as Figure <a href="#S8.F13" title="Figure 13 ‣ 8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows for historical Japanese firm records.</p>
</div>
<figure id="S8.F13" class="ltx_figure"><img src="/html/2407.15339/assets/figures/tk_key.png" id="S8.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Document layouts.</figcaption>
</figure>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">At present, document layout detection typically requires customization.
An off-the-shelf model may work well if the target task is quite close to what it was fine-tuned for.
However, in computer vision, the main pre-training dataset is ImageNet, which consists of natural images, such as different breeds of dogs. Models have not been exposed to massive-scale pre-training on documents, and hence tend to suffer from substantial domain shift when applied to different types of documents. This is particularly true with historical documents, which are heterogeneous.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">While there is a foundation model for object localization, Segment Anything by Meta AI <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al., <a href="#bib.bib76" title="" class="ltx_ref">2023</a>)</cite>, at the time of writing I have not found it to be very useful for document tasks. It can localize objects in an image, but does not classify them into categories. Moreover, localizations on document images are not particularly accurate at present.</p>
</div>
<div id="S8.p6" class="ltx_para">
<p id="S8.p6.1" class="ltx_p">The open-source package <span id="S8.p6.1.1" class="ltx_text ltx_font_typewriter">Layout Parser <cite class="ltx_cite ltx_citemacro_citep"><span id="S8.p6.1.1.1.1" class="ltx_text ltx_font_serif">(</span>Shen et al.<span id="S8.p6.1.1.2.2.1.1" class="ltx_text ltx_font_serif">, </span><a href="#bib.bib122" title="" class="ltx_ref">2021</a><span id="S8.p6.1.1.3.3" class="ltx_text ltx_font_serif">)</span></cite></span> lowers the barriers to detecting document layouts with deep learning. The library—which contains a model zoo for off-the-shelf usage and facilitates tuning customized models—is implemented with simple Python APIs. More information can be found on the EconDL resource page.
The EconDL knowledge base introduces active learning for object detection <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib121" title="" class="ltx_ref">2022</a>)</cite>, which chooses instances to label that the model is most uncertain about to economize on labeling costs.</p>
</div>
<div id="S8.p7" class="ltx_para">
<p id="S8.p7.1" class="ltx_p">Space constraints preclude delving into the architectures of object detection models, but a detailed treatment is provided in the object detection post in the EconDL knowledge base. Resources for satellite images include <span id="S8.p7.1.1" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Aleissaee et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib134" title="" class="ltx_ref">2022</a>); Bandara and Patel (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>); Fuller, Millard and Green (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite> and are linked from the page on processing satellite imagery in EconDL.</p>
</div>
<figure id="S8.F14" class="ltx_figure"><img src="/html/2407.15339/assets/figures/arch_owr.png" id="S8.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>EffOCR and Seq2Seq Model Architectures.</figcaption>
</figure>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Alternative methods</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This review has focused on a set of methods that—while a mainstay in deep learning—are far from comprehensive. If the reader delves into the deep learning literature, they will find other approaches to the above problems and may wonder why those were not covered. I focus on classifiers and embedding models because they are often sample and computationally efficient, meaning that they learn well from limited data and can be cheaply deployed on constrained hardware. They are user-friendly to train and can attain state-of-the-art performance on a variety of tasks.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">This section provides a brief flavor of other ways of approaching two applications, OCR and entity disambiguation. It highlights how problems can be conceptualized differently and underscores some of the advantages of embedding models for academic applications.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Optical Character Recognition</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p id="S9.SS1.p1.1" class="ltx_p">Section <a href="#S7.SS5" title="7.5 Optical character recognition ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.5</span></a> framed OCR as an image retrieval problem, using a contrastively trained vision model. This diverges from the literature, which mostly models OCR as a sequence-to-sequence (seq2seq) problem. Seq2seq models transform one sequence of data into another sequence. They are frequently used when the input and output data are sequences that may differ in length, such as in machine translation.</p>
</div>
<div id="S9.SS1.p2" class="ltx_para">
<p id="S9.SS1.p2.1" class="ltx_p">Figure <a href="#S8.F14" title="Figure 14 ‣ 8 Regression ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> highlights the differences between the <span id="S9.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> and seq2seq architectures for OCR. First, seq2seq OCR typically requires line-level inputs and does not localize individual characters or words. Instead, it divides text line images or their representations into fixed-size patches. In contrast, <span id="S9.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span> uses modern object detection methods <cite class="ltx_cite ltx_citemacro_citep">(Cai and Vasconcelos, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Jocher, <a href="#bib.bib69" title="" class="ltx_ref">2020</a>)</cite> to locate characters or words in the input image.</p>
</div>
<div id="S9.SS1.p3" class="ltx_para">
<p id="S9.SS1.p3.1" class="ltx_p">Second, seq2seq decodes image representations—created by a learned vision model—into text sequentially using a learned language model. Conversely, <span id="S9.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> employs contrastive training <cite class="ltx_cite ltx_citemacro_citep">(Khosla et al., <a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite> to learn a meaningful metric space for OCR. Its vision model projects crops of the same character or word close together, regardless of style, while projecting crops of different characters or words to dissimilar embeddings. With <span id="S9.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span>, the vision embeddings alone are sufficient to infer text, by retrieving their nearest neighbor from an index created by embedding a digital font. In contrast, with seq2seq, the vision representations are decoded into texts with a language model, which requires jointly estimating millions of additional parameters.</p>
</div>
<div id="S9.SS1.p4" class="ltx_para">
<p id="S9.SS1.p4.1" class="ltx_p">A drawback of the seq2seq architecture is that it is challenging to extend and customize to novel settings <cite class="ltx_cite ltx_citemacro_citep">(Hedderich et al., <a href="#bib.bib57" title="" class="ltx_ref">2021</a>)</cite>, because training a joint vision-language model requires a vast collection of labeled image-text pairs and significant compute, particularly when state-of-the-art architectures are used. TrOCR—a transformer seq2seq model created by researchers at Microsoft—was trained using 684 million English synthetic text lines and 32 32GB V100 GPUs, a very costly setup that no academic researcher can replicate.</p>
</div>
<div id="S9.SS1.p5" class="ltx_para">
<p id="S9.SS1.p5.1" class="ltx_p">This drawback can be quantified by sample efficiency, which refers to how well a model can perform following exposure to a limited number of training examples. Some architectures learn more efficiently than others. Bi-encoders tend to learn efficiently, which is important for economists because our compute and annotation budgets are severely limited by deep learning standards.</p>
</div>
<div id="S9.SS1.p6" class="ltx_para">
<p id="S9.SS1.p6.1" class="ltx_p">Figure <a href="#S9.F15" title="Figure 15 ‣ 9.1 Optical Character Recognition ‣ 9 Alternative methods ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, drawn from <cite class="ltx_cite ltx_citemacro_cite">Carlson, Bryan and Dell (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>, examines sample efficiency by training various open-source OCR architectures on the same small training sets.
The x-axis plots the percentage of the <span id="S9.SS1.p6.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> training dataset used in training, and the y-axis plots the character error rate.</p>
</div>
<figure id="S9.F15" class="ltx_figure"><img src="/html/2407.15339/assets/figures/sample_eff_new.png" id="S9.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="412" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Sample Efficiency.</figcaption>
</figure>
<div id="S9.SS1.p7" class="ltx_para">
<p id="S9.SS1.p7.1" class="ltx_p">On just 99 labeled table cells for Japanese tables and 21 labeled rows from U.S. newspapers in the Chronicling America collection, as well as digital fonts (the 5% train splits), <span id="S9.SS1.p7.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span>’s character error rate is around 4%, showing viable few-shot performance.
Other architectures, trained on identical data, remain unusable.
<span id="S9.SS1.p7.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span> performs nearly as well using 20% of the training data as using 70%, where it continues to outperform all other alternatives. TrOCR learns almost nothing from the amount of data we can expose it to (hence why Microsoft trained it on 684 million text lines). In contrast, <span id="S9.SS1.p7.1.3" class="ltx_text ltx_font_typewriter">EffOCR</span> can be trained with a student account in the cloud, or even on a laptop. CRNN is a much lighter weight, older seq2seq architecture, that learns better with limited data but uses an LSTM rather than a transformer, leading to an accuracy hit when fully trained.</p>
</div>
<div id="S9.SS1.p8" class="ltx_para">
<p id="S9.SS1.p8.1" class="ltx_p">Embedding models can also offer a computational advantage over seq2seq models.
<span id="S9.SS1.p8.1.1" class="ltx_text ltx_font_typewriter">EffOCR</span> supports inference parallelization across characters, promoting faster inference, whereas seq2seq requires autoregressive decoding, which is slower. <span id="S9.SS1.p8.1.2" class="ltx_text ltx_font_typewriter">EffOCR</span> runs approximately 50 times faster than TrOCR, the only open-source model comparable in accuracy once fully trained. We created the open-source American Stories dataset consisting of over 430 million historical news articles with a $60,000 budget using <span id="S9.SS1.p8.1.3" class="ltx_text ltx_font_typewriter">EffOCR</span>, and could not have realistically generated any substantial additional funds, let alone 50 times more funds.</p>
</div>
<div id="S9.SS1.p9" class="ltx_para">
<p id="S9.SS1.p9.1" class="ltx_p">A sample and compute efficient architecture makes it possible to achieve high-quality transcriptions in diverse settings and on massive-scale document collections, bringing big data to a diversity of economic history applications. In theory, contextual understanding from the full sequence of representations could lead to better OCR. In practice, state-of-the-art transformer seq2seq models are expensive to train and deploy, and are not available for lower-resource languages, with advances mainly in a few languages. By moving away from seq2seq models, significant improvements in sample and computational efficiency can be achieved. In an academic setting, such advantages are particularly relevant, as our applications are highly diverse and our budgets are usually extremely constrained.</p>
</div>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Entity Disambiguation</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p id="S9.SS2.p1.1" class="ltx_p">Entity disambiguation—linking entity mentions in unstructured texts to an external knowledgebase like Wikipedia—has led to the development of various architectures. These include a masked language model (LUKE, <cite class="ltx_cite ltx_citemacro_cite">Yamada et al. (<a href="#bib.bib140" title="" class="ltx_ref">2022</a>)</cite>) and a neural translation model (GENRE, <cite class="ltx_cite ltx_citemacro_cite">De Cao et al. (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>)—which uses a sequence-to-sequence architecture to translate mentions into Wikipedia ids—as well as the bi-encoder embedding architecture that treats entity disambiguation as a nearest neighbor retrieval problem <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib138" title="" class="ltx_ref">2019</a>)</cite>. The latter architecture is used by this article’s application (Section <a href="#S7.SS3" title="7.3 Linking unstructured data ‣ 7 Embedding models ‣ Deep Learning for Economists" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>).</p>
</div>
<div id="S9.SS2.p2" class="ltx_para">
<p id="S9.SS2.p2.1" class="ltx_p">The masked language model approach masks out entities and predicts their Wikipedia id using a classifier head.
While it is the leader on some benchmarks, in practice it has major limitations.
Language models predict masked tokens through classification.
LUKE is restricted to the top 50K Wikipedia entries, due to computational constraints in calculating the softmax. Many of the top-50K entries are not people, and many people that appear in historical news or government documents are not amongst the top-50K.
Additionally, it does not accommodate out-of-knowledge base entities and requires sparse entity priors to initialize the model. In many applications, not all individuals will be in a knowledge base, and the model needs to be able to predict this.</p>
</div>
<div id="S9.SS2.p3" class="ltx_para">
<p id="S9.SS2.p3.1" class="ltx_p">The neural translation model’s sequence-to-sequence architecture is slow during inference, taking approximately 60 times longer to run than bi-encoder embedding models. <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite> also show that an embedding model achieves higher accuracy disambiguating historical texts. In short, again a sequence-to-sequence architecture is costly to run at scale and doesn’t necessarily offer performance advantages.</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusion</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">Deep learning provides powerful tools for processing unstructured data. On social science tasks ranging from text classification to record linkage, entity disambiguation, and tracing the spread of reproduced content, deep learning can outperform traditional sparse methods by a wide margin (often by 20 points of F1/accuracy or more) <cite class="ltx_cite ltx_citemacro_citep">(Silcock et al., <a href="#bib.bib124" title="" class="ltx_ref">2023</a>; Dell et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Arora and Dell, <a href="#bib.bib11" title="" class="ltx_ref">2024</a>; Arora et al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>, <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S10.p2" class="ltx_para">
<p id="S10.p2.1" class="ltx_p">Deep learning can facilitate novel analyses by providing tools to impute structured information from unstructured data on a massive scale. When working with lightweight, sample-efficient pre-trained models, training and deployment are quite affordable, even for datasets with millions or billions of observations. For some applications, deep learning also offers promise for processing data from low-resource settings, with the potential to make economic research more representative of the diversity of human societies.</p>
</div>
<div id="S10.p3" class="ltx_para">
<p id="S10.p3.1" class="ltx_p">Becoming familiar with deep learning methods entails significant startup costs. This article—along with the accompanying open-source packages, tutorials, and knowledge base—aims to significantly reduce entry barriers for economists who would like to use deep learning in their research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramitzky et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">Abramitzky, Ran, Leah Boustan, Katherine Eriksson, James Feigenbaum, and Santiago Pérez.</span> 2021. “Automated linking of historical data.” <span id="bib.bib2.2.2" class="ltx_text ltx_font_italic">Journal of Economic Literature</span>, 59(3): 865–918.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alammar (2018<span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">a</span>)</span>
<span class="ltx_bibblock">
<span id="bib.bib3.3.1" class="ltx_text ltx_font_bold">Alammar, Jay.</span> 2018<span id="bib.bib3.4.2" class="ltx_text ltx_font_italic">a</span>. “The illustrated Bert, Elmo, and Co. (how NLP cracked transfer learning).” <a target="_blank" href="https://jalammar.github.io/illustrated-bert/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jalammar.github.io/illustrated-bert/</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alammar (2018<span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">b</span>)</span>
<span class="ltx_bibblock">
<span id="bib.bib4.3.1" class="ltx_text ltx_font_bold">Alammar, Jay.</span> 2018<span id="bib.bib4.4.2" class="ltx_text ltx_font_italic">b</span>. “The illustrated Transformer.” <a target="_blank" href="https://jalammar.github.io/illustrated-transformer/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jalammar.github.io/illustrated-transformer/</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alammar (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">Alammar, Jay.</span> 2019. “The illustrated GPT-2 (Visualizing Transformer language models).” <a target="_blank" href="http://jalammar.github.io/illustrated-gpt2/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jalammar.github.io/illustrated-gpt2/</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alammar (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">Alammar, Jay.</span> 2020. “How GPT3 Works - Visualizations and Animations.” <a target="_blank" href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jalammar.github.io/how-gpt3-works-visualizations-animations/</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aleissaee et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">Aleissaee, Abdulaziz Amer, Amandeep Kumar, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal, Gui-Song Xia, et al.</span> 2022. “Transformers in remote sensing: A survey.” <span id="bib.bib7.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.01206</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ali et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al.</span> 2021. “Xcit: Cross-covariance image transformers.” <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34: 20014–20027.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angelopoulos et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">Angelopoulos, Anastasios N., Stephen Bates, Clara Fannjiang, Michael I. Jordan, Tijana Zrnic, and Emmanuel J. Candès.</span> 2023. “Prediction-Powered Inference.” <span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.09633</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Archives and Administration (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">Archives, U.S. National, and Records Administration.</span> 2023. “The Soundex Indexing System.” Accessed: 11/10/2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora and Dell (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">Arora, Abhishek, and Melissa Dell.</span> 2024. “LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models.” <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Association of Computational Linguistics: Systems Demonstration Track</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">Arora, Abhishek, Emily Silcock, Leander Heldring, and Melissa Dell.</span> 2024. “Contrastive Entity Coreference and Disambiguation for Historical Texts.” <span id="bib.bib12.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2406.15576</span>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">Arora, Abhishek, Xinmei Yang, Shao Yu Jheng, and Melissa Dell.</span> 2023. “Linking representations with multimodal contrastive learning.” <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.03464</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Athey and Imbens (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">Athey, Susan, and Guido W Imbens.</span> 2019. “Machine learning methods that economists should know about.” <span id="bib.bib14.2.2" class="ltx_text ltx_font_italic">Annual Review of Economics</span>, 11: 685–725.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bailey et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">Bailey, Martha J, Connor Cole, Morgan Henderson, and Catherine Massey.</span> 2020. “How well do automated linking methods perform? Lessons from US historical data.” <span id="bib.bib15.2.2" class="ltx_text ltx_font_italic">Journal of Economic Literature</span>, 58(4): 997–1044.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bandara and Patel (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">Bandara, Wele Gedara Chaminda, and Vishal M Patel.</span> 2022. “A transformer-based siamese network for change detection.” 207–210, IEEE.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio, Simard and Frasconi (1994)</span>
<span class="ltx_bibblock">
<span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">Bengio, Yoshua, Patrice Simard, and Paolo Frasconi.</span> 1994. “Learning long-term dependencies with gradient descent is difficult.” <span id="bib.bib17.2.2" class="ltx_text ltx_font_italic">IEEE transactions on neural networks</span>, 5(2): 157–166.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhunia et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">Bhunia, Ankan Kumar, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, and Mubarak Shah.</span> 2021. “Handwriting transformers.” 1086–1094.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binette and Steorts (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">Binette, Olivier, and Rebecca C Steorts.</span> 2022. “(Almost) all of entity resolution.” <span id="bib.bib19.2.2" class="ltx_text ltx_font_italic">Science Advances</span>, 8(12): eabi8021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.</span> 2020. “Language models are few-shot learners.” <span id="bib.bib20.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 33: 1877–1901.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bryan et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">Bryan, Tom, Jacob Carlson, Abhishek Arora, and Melissa Dell.</span> 2023. “EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge”.” <span id="bib.bib21.2.2" class="ltx_text ltx_font_italic">Empirical Methods on Natural Language Processing (Systems Demonstrations Track)</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai and Vasconcelos (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">Cai, Zhaowei, and Nuno Vasconcelos.</span> 2018. “Cascade r-cnn: Delving into high quality object detection.” <span id="bib.bib22.2.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 6154–6162.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai and Vasconcelos (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">Cai, Zhaowei, and Nuno Vasconcelos.</span> 2019. “Cascade R-CNN: high quality object detection and instance segmentation.” <span id="bib.bib23.2.2" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, 43(5): 1483–1498.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">Cao, Hongliu.</span> 2023. “Recent advances in universal text embeddings: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark.” Amadeus SAS France.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">Carion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.</span> 2020. “End-to-end object detection with transformers.” 213–229, Springer.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlson, Bryan and Dell (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">Carlson, Jacob., Tom. Bryan, and Melissa. Dell.</span> 2024. “Efficient OCR for Building a Diverse Digital History.” <span id="bib.bib26.2.2" class="ltx_text ltx_font_italic">ACL Anthology</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">Caron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.</span> 2021. “Emerging properties in self-supervised vision transformers.” 9650–9660.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cattaneo et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">Cattaneo, Matias D., Yingjie Feng, Filippo Palomba, and Rocio Titiunik.</span> 2022. “Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption.” <span id="bib.bib28.2.2" class="ltx_text ltx_font_italic">Journal of Econometrics</span>, 228(2): 260–279.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen, Xie and He (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">Chen, Xinlei, Saining Xie, and Kaiming He.</span> 2021. “An empirical study of training self-supervised vision transformers.” 9640–9649.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chernozhukov et al. (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins.</span> 2018. “Double/debiased machine learning for treatment and structural parameters.” <span id="bib.bib30.2.2" class="ltx_text ltx_font_italic">The Econometrics Journal</span>, 21(1): C1–C68.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chernozhukov, Wüthrich and Zhu (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">Chernozhukov, Victor, Kaspar Wüthrich, and Yinchu Zhu.</span> 2021. “An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.” <span id="bib.bib31.2.2" class="ltx_text ltx_font_italic">Journal of the American Statistical Association</span>, 116(536): 1849–1868.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chernozhukov, Newey and Singh (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">Chernozhukov, Victor, Whitney K. Newey, and Rahul Singh.</span> 2022. “Automatic Debiased Machine Learning of Causal and Structural Effects.” <span id="bib.bib32.2.2" class="ltx_text ltx_font_italic">Econometrics Journal</span>, 25(3): C1–C38.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chopra, Hadsell and LeCun (2005)</span>
<span class="ltx_bibblock">
<span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">Chopra, Sumit, Raia Hadsell, and Yann LeCun.</span> 2005. “Learning a similarity metric discriminatively, with application to face verification.” Vol. 1, 539–546, IEEE.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui and Athey (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">Cui, Peng, and Susan Athey.</span> 2022. “Stable learning establishes some common ground between causal inference and machine learning.” <span id="bib.bib34.2.2" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, 4(2): 110–115.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Cao et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">De Cao, Nicola, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.</span> 2020. “Autoregressive entity retrieval.” <span id="bib.bib35.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.00904</span>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dell et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">Dell, Melissa, Jacob Carlson, Tom Bryan, Emily Silcock, Abhishek Arora, Zejiang Shen, Luca D’Amico-Wong, Quan Le, Pablo Querubin, and Leander Heldring.</span> 2023. “American Stories: A Large-Scale Structured Text Dataset of Historical US Newspapers.” <span id="bib.bib36.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information and Processing Systems, Datasets and Benchmarks</span>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
<span id="bib.bib37.1.1" class="ltx_text ltx_font_bold">Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.</span> 2009. “Imagenet: A large-scale hierarchical image database.” 248–255, IEEE.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.</span> 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” 4171–4186, Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">Dodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.</span> 2021. “Documenting large webtext corpora: A case study on the colossal clean crawled corpus.” <span id="bib.bib39.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.08758</span>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.</span> 2020. “An image is worth 16x16 words: Transformers for image recognition at scale.” <span id="bib.bib40.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">Ethayarajh, Kawin.</span> 2019. “How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings.” <span id="bib.bib41.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.00512</span>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falkner, Klein and Hutter (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib42.1.1" class="ltx_text ltx_font_bold">Falkner, Stefan, Aaron Klein, and Frank Hutter.</span> 2018. “BOHB: Robust and efficient hyperparameter optimization at scale.” 1437–1446, PMLR.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernández-Villaverde (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">Fernández-Villaverde, Jesús.</span> 2024. “Deep Learning for Macroeconomists.” <a target="_blank" href="https://www.sas.upenn.edu/~jesusfv/teaching.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sas.upenn.edu/~jesusfv/teaching.html</a>, <a target="_blank" href="https://www.sas.upenn.edu/~jesusfv/teaching.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sas.upenn.edu/~jesusfv/teaching.html</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Franklin et al. (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib44.1.1" class="ltx_text ltx_font_bold">Franklin, Brevin, Emily Silcock, Abhishek Arora, Tom Bryan, and Melissa Dell.</span> 2024. “News Deja Vu: Connecting Past and Present with Semantic Search.” 99–112.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fuller, Millard and Green (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">Fuller, Anthony, Koreen Millard, and James R Green.</span> 2022. “Transfer Learning with Pretrained Remote Sensing Transformers.” <span id="bib.bib45.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.14969</span>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib46.1.1" class="ltx_text ltx_font_bold">Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford.</span> 2021. “Datasheets for datasets.” 1–14.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gentzkow, Kelly and Taddy (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">Gentzkow, Matthew, Bryan Kelly, and Matt Taddy.</span> 2019. “Text as Data.” <span id="bib.bib47.2.2" class="ltx_text ltx_font_italic">Journal of Economic Literature</span>, 57(3): 535–574.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gissin and Shalev-Shwartz (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib48.1.1" class="ltx_text ltx_font_bold">Gissin, Daniel, and Shai Shalev-Shwartz.</span> 2019. “Discriminative active learning.” <span id="bib.bib48.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.06347</span>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goh (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib49.1.1" class="ltx_text ltx_font_bold">Goh, Gabriel.</span> 2017. “Why momentum really works.” <span id="bib.bib49.2.2" class="ltx_text ltx_font_italic">Distill</span>, 2(4): e6.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong, Chung and Glass (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib50.1.1" class="ltx_text ltx_font_bold">Gong, Yuan, Yu-An Chung, and James Glass.</span> 2021. “Ast: Audio spectrogram transformer.” <span id="bib.bib50.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.01778</span>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow, Bengio and Courville (2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib51.1.1" class="ltx_text ltx_font_bold">Goodfellow, Ian, Yoshua Bengio, and Aaron Courville.</span> 2016. <span id="bib.bib51.2.2" class="ltx_text ltx_font_italic">Deep learning.</span> MIT press.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greff et al. (2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib52.1.1" class="ltx_text ltx_font_bold">Greff, Klaus, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber.</span> 2016. “LSTM: A search space odyssey.” <span id="bib.bib52.2.2" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>, 28(10): 2222–2232.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grill et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib53.1.1" class="ltx_text ltx_font_bold">Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al.</span> 2020. “Bootstrap your own latent-a new approach to self-supervised learning.” <span id="bib.bib53.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 33: 21271–21284.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarneri (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib54.1.1" class="ltx_text ltx_font_bold">Guarneri, Julia.</span> 2017. <span id="bib.bib54.2.2" class="ltx_text ltx_font_italic">Newsprint Metropolis.</span> University of Chicago Press.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib55.1.1" class="ltx_text ltx_font_bold">Gururangan, Suchin, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith.</span> 2020. “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.” <span id="bib.bib55.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.10964</span>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanlon and Beach (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib56.1.1" class="ltx_text ltx_font_bold">Hanlon, W Walker, and Brian Beach.</span> 2022. “Historical Newspaper Data: A Researcher’s Guide and Toolkit.”

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hedderich et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib57.1.1" class="ltx_text ltx_font_bold">Hedderich, Michael A., Lukas Lange, Heike Adel, Jannik Strötgen, and Dietrich Klakow.</span> 2021. “A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios.” 2545–2568. Online:Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hegghammer (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib58.1.1" class="ltx_text ltx_font_bold">Hegghammer, Thomas.</span> 2021. “OCR with Tesseract, Amazon Textract, and Google Document AI: A benchmarking experiment.” <span id="bib.bib58.2.2" class="ltx_text ltx_font_italic">Journal of Computational Social Science</span>, 5(1): 861–882.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib59.1.1" class="ltx_text ltx_font_bold">He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.</span> 2017. “Mask R-CNN.” 2961–2969.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib60.1.1" class="ltx_text ltx_font_bold">He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.</span> 2015. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” 1026–1034.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib61.1.1" class="ltx_text ltx_font_bold">He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.</span> 2016. “Deep residual learning for image recognition.” 770–778.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib62.1.1" class="ltx_text ltx_font_bold">He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.</span> 2022. “Masked autoencoders are scalable vision learners.” 16000–16009.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib63.1.1" class="ltx_text ltx_font_bold">He, Pengcheng, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.</span> 2020. “Deberta: Decoding-enhanced bert with disentangled attention.” <span id="bib.bib63.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.03654</span>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermans, Beyer and Leibe (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib64.1.1" class="ltx_text ltx_font_bold">Hermans, Alexander, Lucas Beyer, and Bastian Leibe.</span> 2017. “In defense of the triplet loss for person re-identification.” <span id="bib.bib64.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.07737</span>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
<span id="bib.bib65.1.1" class="ltx_text ltx_font_bold">Hochreiter, Sepp, and Jürgen Schmidhuber.</span> 1997. “Long short-term memory.” <span id="bib.bib65.2.2" class="ltx_text ltx_font_italic">Neural computation</span>, 9(8): 1735–1780.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holland et al. (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib66.1.1" class="ltx_text ltx_font_bold">Holland, Sarah, Ahmed Hosny, Sara Newman, Josh Joseph, and Kevin Chmielinski.</span> 2018. “The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards.” <span id="bib.bib66.2.2" class="ltx_text ltx_font_italic">FAT* ’18</span>, 1–5. New York, NY, USA:Association for Computing Machinery.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib67.1.1" class="ltx_text ltx_font_bold">Howard, Andrew, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al.</span> 2019. “Searching for mobilenetv3.” 1314–1324.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy (2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib68.1.1" class="ltx_text ltx_font_bold">Ioffe, Sergey, and Christian Szegedy.</span> 2015. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” 448–456, PMLR.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jocher (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib69.1.1" class="ltx_text ltx_font_bold">Jocher, Glenn.</span> 2020. “YOLOv5 by Ultralytics.”

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson, Douze and Jégou (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib70.1.1" class="ltx_text ltx_font_bold">Johnson, Jeff, Matthijs Douze, and Hervé Jégou.</span> 2019. “Billion-scale similarity search with gpus.” <span id="bib.bib70.2.2" class="ltx_text ltx_font_italic">IEEE Transactions on Big Data</span>, 7(3): 535–547.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib71.1.1" class="ltx_text ltx_font_bold">Karpathy, Andrej.</span> 2022. “The spelled-out intro to neural networks and backpropagation.” https://www.youtube.com/watch?v=VMj-3S1tku0.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib72.1.1" class="ltx_text ltx_font_bold">Karpukhin, Vladimir, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.</span> 2020. “Dense passage retrieval for open-domain question answering.” <span id="bib.bib72.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.04906</span>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib73.1.1" class="ltx_text ltx_font_bold">Khattab, Omar, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.</span> 2022. “Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP.” <span id="bib.bib73.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.14024</span>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib74.1.1" class="ltx_text ltx_font_bold">Khosla, Prannay, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.</span> 2020. “Supervised contrastive learning.” <span id="bib.bib74.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 33: 18661–18673.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
<span id="bib.bib75.1.1" class="ltx_text ltx_font_bold">Kingma, Diederik P, and Jimmy Ba.</span> 2014. “Adam: A method for stochastic optimization.” <span id="bib.bib75.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib76.1.1" class="ltx_text ltx_font_bold">Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.</span> 2023. “Segment anything.” <span id="bib.bib76.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.02643</span>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib77.1.1" class="ltx_text ltx_font_bold">Kirillov, Alexander, Ross Girshick, Kaiming He, and Piotr Dollár.</span> 2019. “Panoptic feature pyramid networks.” 6399–6408.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korinek (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib78.1.1" class="ltx_text ltx_font_bold">Korinek, Anton.</span> 2023. “Generative AI for Economic Research: Use Cases and Implications for Economists.” <span id="bib.bib78.2.2" class="ltx_text ltx_font_italic">Journal of Economic Literature</span>, 61(4): 1281–1317.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky, Sutskever and Hinton (2012)</span>
<span class="ltx_bibblock">
<span id="bib.bib79.1.1" class="ltx_text ltx_font_bold">Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton.</span> 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” Vol. 25. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib80.1.1" class="ltx_text ltx_font_bold">Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.</span> 2019. “Albert: A lite bert for self-supervised learning of language representations.” <span id="bib.bib80.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.11942</span>.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun, Bengio and Hinton (2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib81.1.1" class="ltx_text ltx_font_bold">LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton.</span> 2015. “Deep learning.” <span id="bib.bib81.2.2" class="ltx_text ltx_font_italic">Nature</span>, 521(7553): 436–444.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei and Candès (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib82.1.1" class="ltx_text ltx_font_bold">Lei, Lihua, and Emmanuel J. Candès.</span> 2020. “Conformal Inference of Counterfactuals and Individual Treatment Effects.” <span id="bib.bib82.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.06138</span>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein et al. (1966)</span>
<span class="ltx_bibblock">
<span id="bib.bib83.1.1" class="ltx_text ltx_font_bold">Levenshtein, Vladimir I, et al.</span> 1966. “Binary codes capable of correcting deletions, insertions, and reversals.” Vol. 10, 707–710, Soviet Union.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib84.1.1" class="ltx_text ltx_font_bold">Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar.</span> 2017. “Hyperband: A novel bandit-based approach to hyperparameter optimization.” <span id="bib.bib84.2.2" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 18(1): 6765–6816.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib85.1.1" class="ltx_text ltx_font_bold">Liu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.</span> 2023. “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.” <span id="bib.bib85.2.2" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, 55(9): 1–35.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib86.1.1" class="ltx_text ltx_font_bold">Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.</span> 2019. “Roberta: A robustly optimized bert pretraining approach.” <span id="bib.bib86.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.11692</span>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib87.1.1" class="ltx_text ltx_font_bold">Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.</span> 2021. “Swin transformer: Hierarchical vision transformer using shifted windows.” 10012–10022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib88.1.1" class="ltx_text ltx_font_bold">Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.</span> 2022. “A convnet for the 2020s.” 11976–11986.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lynn, Kummerfeld and Mihalcea (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib89.1.1" class="ltx_text ltx_font_bold">Lynn, Veronica, Jonathan K. Kummerfeld, and Rada Mihalcea.</span> 2020. “A Causal Framework for Uncovering the Effects of Descriptive Text on Decision Making.” 5276–5294.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib90.1.1" class="ltx_text ltx_font_bold">Lyu, Lijun, Maria Koutraki, Martin Krickl, and Besnik Fetahu.</span> 2021. “Neural OCR Post-Hoc Correction of Historical Corpora.” <span id="bib.bib90.2.2" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>, 9: 479–483.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabi et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib91.1.1" class="ltx_text ltx_font_bold">Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan.</span> 2021. “A Survey on Bias and Fairness in Machine Learning.” <span id="bib.bib91.2.2" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 54(6): 1–35.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta and Rastegari (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib92.1.1" class="ltx_text ltx_font_bold">Mehta, Sachin, and Mohammad Rastegari.</span> 2021. “MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer.” <span id="bib.bib92.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.02178</span>.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merchant et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib93.1.1" class="ltx_text ltx_font_bold">Merchant, Amil, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney.</span> 2020. “What Happens to BERT Embeddings during Fine-tuning?” <span id="bib.bib93.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.14448</span>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
<span id="bib.bib94.1.1" class="ltx_text ltx_font_bold">Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.</span> 2013. “Distributed representations of words and phrases and their compositionality.” <span id="bib.bib94.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 26: 3111–3119.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib95.1.1" class="ltx_text ltx_font_bold">Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.</span> 2019. “Model Cards for Model Reporting.” 220–229.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MLCommons (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib96.1.1" class="ltx_text ltx_font_bold">MLCommons.</span> 2024. “Croissant: A Metadata Framework for ML-Ready Datasets.” <a target="_blank" href="https://github.com/mlcommons/croissant" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mlcommons/croissant</a>, Accessed: 2024-07-09.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib97.1.1" class="ltx_text ltx_font_bold">Nguyen, Dat Quoc, Thanh Vu, Afshin Rahimi, Mai Hoang Dao, Linh The Nguyen, and Long Doan.</span> 2020. “WNUT-2020 task 2: identification of informative COVID-19 english tweets.” <span id="bib.bib97.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.08232</span>.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib98.1.1" class="ltx_text ltx_font_bold">Nguyen, Thi Tuyet Hai, Adam Jatowt, Mickael Coustaty, and Antoine Doucet.</span> 2021. “Survey of Post-OCR Processing Approaches.” <span id="bib.bib98.2.2" class="ltx_text ltx_font_italic">ACM Comput. Surv.</span>, 54(6).

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nielsen (2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib99.1.1" class="ltx_text ltx_font_bold">Nielsen, Michael A.</span> 2015. <span id="bib.bib99.2.2" class="ltx_text ltx_font_italic">Neural Networks and Deep learning.</span> Vol. 25, Determination press San Francisco, CA, USA.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olah (2014)</span>
<span class="ltx_bibblock">
<span id="bib.bib100.1.1" class="ltx_text ltx_font_bold">Olah, Christopher.</span> 2014. “Deep learning, NLP, and Representations.” <span id="bib.bib100.2.2" class="ltx_text ltx_font_italic">GitHub blog, posted on July</span>.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord, Li and Vinyals (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib101.1.1" class="ltx_text ltx_font_bold">Oord, Aaron van den, Yazhe Li, and Oriol Vinyals.</span> 2018. “Representation learning with contrastive predictive coding.” <span id="bib.bib101.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1807.03748</span>.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington, Socher and Manning (2014)</span>
<span class="ltx_bibblock">
<span id="bib.bib102.1.1" class="ltx_text ltx_font_bold">Pennington, Jeffrey, Richard Socher, and Christopher D Manning.</span> 2014. “Glove: Global vectors for word representation.” 1532–1543.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib103.1.1" class="ltx_text ltx_font_bold">Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.</span> 2019. “Language models are unsupervised multitask learners.” <span id="bib.bib103.2.2" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8): 9.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib104.1.1" class="ltx_text ltx_font_bold">Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.</span> 2021. “Learning transferable visual models from natural language supervision.” 8748–8763, PMLR.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib105.1.1" class="ltx_text ltx_font_bold">Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.</span> 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <span id="bib.bib105.2.2" class="ltx_text ltx_font_italic">arXiv e-prints</span>.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib106.1.1" class="ltx_text ltx_font_bold">Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.</span> 2020. “Exploring the limits of transfer learning with a unified text-to-text transformer.” <span id="bib.bib106.2.2" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 21(1): 5485–5551.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon et al. (2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib107.1.1" class="ltx_text ltx_font_bold">Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi.</span> 2016. “You only look once: Unified, real-time object detection.” 779–788.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib108.1.1" class="ltx_text ltx_font_bold">Reimers, Nils, and Iryna Gurevych.</span> 2019. “Sentence-bert: Sentence embeddings using siamese bert-networks.” <span id="bib.bib108.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.10084</span>.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib109.1.1" class="ltx_text ltx_font_bold">Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun.</span> 2017. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” <span id="bib.bib109.2.2" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>, 39(6): 1137–1149.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robins, Rotnitzky and Zhao (1994)</span>
<span class="ltx_bibblock">
<span id="bib.bib110.1.1" class="ltx_text ltx_font_bold">Robins, James M., Andrea Rotnitzky, and Lue Ping Zhao.</span> 1994. “Estimation of regression coefficients when some regressors are not always observed.” <span id="bib.bib110.2.2" class="ltx_text ltx_font_italic">Journal of the American Statistical Association</span>, 89(427): 846–866.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rush (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib111.1.1" class="ltx_text ltx_font_bold">Rush, Alexander M.</span> 2018. “The annotated transformer.” 52–60.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell (1918)</span>
<span class="ltx_bibblock">
<span id="bib.bib112.1.1" class="ltx_text ltx_font_bold">Russell, Robert C.</span> 1918. “U.S. Patent No. US1261167A.” <span id="bib.bib112.2.2" class="ltx_text ltx_font_italic">U.S. Patent and Trademark Office</span>, <a target="_blank" href="https://patents.google.com/patent/US1261167A/en" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://patents.google.com/patent/US1261167A/en</a>.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanderson (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib113.1.1" class="ltx_text ltx_font_bold">Sanderson, Grant.</span> 2017. “Neural Networks.” <a target="_blank" href="https://www.3blue1brown.com/topics/neural-networks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.3blue1brown.com/topics/neural-networks</a>.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanderson (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib114.1.1" class="ltx_text ltx_font_bold">Sanderson, Grant.</span> 2020. “Convolutions in Image Processing.” <a target="_blank" href="https://www.youtube.com/watch?v=8rrHTtUzyZA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=8rrHTtUzyZA</a>.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sang and De Meulder (2003)</span>
<span class="ltx_bibblock">
<span id="bib.bib115.1.1" class="ltx_text ltx_font_bold">Sang, Erik F, and Fien De Meulder.</span> 2003. “Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.” <span id="bib.bib115.2.2" class="ltx_text ltx_font_italic">arXiv preprint cs/0306050</span>.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019<span id="bib.bib116.2.2.1" class="ltx_text ltx_font_italic">a</span>)</span>
<span class="ltx_bibblock">
<span id="bib.bib116.3.1" class="ltx_text ltx_font_bold">Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf.</span> 2019<span id="bib.bib116.4.2" class="ltx_text ltx_font_italic">a</span>. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” <span id="bib.bib116.5.3" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.01108</span>.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019<span id="bib.bib117.2.2.1" class="ltx_text ltx_font_italic">b</span>)</span>
<span class="ltx_bibblock">
<span id="bib.bib117.3.1" class="ltx_text ltx_font_bold">Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf.</span> 2019<span id="bib.bib117.4.2" class="ltx_text ltx_font_italic">b</span>. “DistilRoBERTa: A distilled version of RoBERTa.” <a target="_blank" href="https://github.com/huggingface/transformers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/transformers</a>, Accessed: 2024-07-09.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santurkar et al. (2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib118.1.1" class="ltx_text ltx_font_bold">Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.</span> 2018. “How does batch normalization help optimization?” <span id="bib.bib118.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 31.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shafer and Vovk (2008)</span>
<span class="ltx_bibblock">
<span id="bib.bib119.1.1" class="ltx_text ltx_font_bold">Shafer, Glenn, and Vladimir Vovk.</span> 2008. “A Tutorial on Conformal Prediction.” <span id="bib.bib119.2.2" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 9: 371–421.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen and Rose (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib120.1.1" class="ltx_text ltx_font_bold">Shen, Qinlan, and Carolyn Rose.</span> 2021. “What Sounds “Right” to Me? Experiential Factors in the Perception of Political Ideology.” 1762–1771, Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib121.1.1" class="ltx_text ltx_font_bold">Shen, Zejiang, Jian Zhao, Yaoliang Yu, Weining Li, and Melissa Dell.</span> 2022. “Olala: object-level active learning based layout annotation.” <span id="bib.bib121.2.2" class="ltx_text ltx_font_italic">EMNLP Computational Social Science Workshop</span>.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib122.1.1" class="ltx_text ltx_font_bold">Shen, Zejiang, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, and Weining Li.</span> 2021. “LayoutParser: A unified toolkit for deep learning based document image analysis.” 131–146, Springer.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silcock et al. (2024)</span>
<span class="ltx_bibblock">
<span id="bib.bib123.1.1" class="ltx_text ltx_font_bold">Silcock, Emily, Abhishek Arora, Luca D’Amico-Wong, and Melissa Dell.</span> 2024. “Newswire: A Large-Scale Structured Database of a Century of Historical News.” <span id="bib.bib123.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2406.09490</span>.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silcock et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib124.1.1" class="ltx_text ltx_font_bold">Silcock, Emily, Luca D’Amico-Wong, Jinglin Yang, and Melissa Dell.</span> 2023. “Noise-Robust De-Duplication at Scale.” Vol. 332.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
<span id="bib.bib125.1.1" class="ltx_text ltx_font_bold">Simonyan, Karen, and Andrew Zisserman.</span> 2014. “Very deep convolutional networks for large-scale image recognition.” <span id="bib.bib125.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stevens, Antiga and Viehmann (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib126.1.1" class="ltx_text ltx_font_bold">Stevens, Eli, Luca Antiga, and Thomas Viehmann.</span> 2020. <span id="bib.bib126.2.2" class="ltx_text ltx_font_italic">Deep learning with PyTorch.</span> Manning Publications Company.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib127.1.1" class="ltx_text ltx_font_bold">Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.</span> 2015. “Going deeper with convolutions.” 1–9.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib128.1.1" class="ltx_text ltx_font_bold">Touvron, Hugo, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.</span> 2021. “Training data-efficient image transformers &amp; distillation through attention.” 10347–10357, PMLR.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib129.1.1" class="ltx_text ltx_font_bold">Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.</span> 2023. “LLaMA: Open and Efficient Foundation Language Models.” <span id="bib.bib129.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ultralytics (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib130.1.1" class="ltx_text ltx_font_bold">Ultralytics.</span> 2020. “Ultralytics/yolov5.” https://github.com/ultralytics/yolov5.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Strien. et al. (2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib131.1.1" class="ltx_text ltx_font_bold">van Strien., Daniel, Kaspar Beelen., Mariona Coll Ardanuy., Kasra Hosseini., Barbara McGillivray., and Giovanni Colavizza.</span> 2020. “Assessing the Impact of OCR Quality on Downstream NLP Tasks.” 484–496, INSTICC. SciTePress.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib132.1.1" class="ltx_text ltx_font_bold">Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.</span> 2017. “Attention is all you need.” <span id="bib.bib132.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 30.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vitercik (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib133.1.1" class="ltx_text ltx_font_bold">Vitercik, Ellen.</span> 2023. “Machine Learning for Algorithm Design.” <a target="_blank" href="https://vitercik.github.io/ml4algs/calendar/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vitercik.github.io/ml4algs/calendar/</a>, <a target="_blank" href="https://vitercik.github.io/ml4algs/calendar/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vitercik.github.io/ml4algs/calendar/</a>.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib134.1.1" class="ltx_text ltx_font_bold">Wang, Di, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao.</span> 2022. “An empirical study of remote sensing pretraining.” <span id="bib.bib134.2.2" class="ltx_text ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</span>.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Liu (2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib135.1.1" class="ltx_text ltx_font_bold">Wang, Feng, and Huaping Liu.</span> 2021. “Understanding the behaviour of contrastive loss.” 2495–2504.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib136.1.1" class="ltx_text ltx_font_bold">Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.</span> 2022. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” <span id="bib.bib136.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.11903</span>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkerson et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib137.1.1" class="ltx_text ltx_font_bold">Wilkerson, John, E. Scott Adler, Bryan D. Jones, Frank R. Baumgartner, Guy Freedman, Sean M. Theriault, Alison Craig, Derek A. Epp, Cheyenne Lee, and Miranda E. Sullivan.</span> 2023. “Policy Agendas Project: Congressional Bills.” <a target="_blank" href="https://comparativeagendas.net/datasets_codebooks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://comparativeagendas.net/datasets_codebooks</a>.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib138.1.1" class="ltx_text ltx_font_bold">Wu, Ledell, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer.</span> 2019. “Scalable zero-shot entity linking with dense entity retrieval.” <span id="bib.bib138.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.03814</span>.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib139.1.1" class="ltx_text ltx_font_bold">Xie, Saining, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.</span> 2017. “Aggregated residual transformations for deep neural networks.” 1492–1500.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada et al. (2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib140.1.1" class="ltx_text ltx_font_bold">Yamada, Ikuya, Koki Washio, Hiroyuki Shindo, and Yuji Matsumoto.</span> 2022. “Global entity disambiguation with BERT.” 3264–3271.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib141.1.1" class="ltx_text ltx_font_bold">Yang, Xinmei, Abhishek Arora, Shao Yu Jheng, and Melissa Dell.</span> 2023. “Quantifying Character Similarity with Vision Transformers.” <span id="bib.bib141.2.2" class="ltx_text ltx_font_italic">Empirical Methods on Natural Language Processing</span>.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zrnic and Candès (2023)</span>
<span class="ltx_bibblock">
<span id="bib.bib142.1.1" class="ltx_text ltx_font_bold">Zrnic, Tijana, and Emmanuel J. Candès.</span> 2023. “Cross-Prediction-Powered Inference.” <span id="bib.bib142.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.16598</span>.

</span>
</li>
</ul>
</section>
<figure id="S0.T1a" class="ltx_table ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-1: </span>Prompts used to query OpenAI GPT models</figcaption>
<table id="S0.T1a.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.T1a.1.1.1" class="ltx_tr">
<th id="S0.T1a.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.1.1.1.1.1" class="ltx_p" style="width:108.4pt;"><span id="S0.T1a.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Topic</span></span>
</span>
</th>
<th id="S0.T1a.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.1.1.2.1.1" class="ltx_p" style="width:281.9pt;"><span id="S0.T1a.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.T1a.1.2.1" class="ltx_tr">
<td id="S0.T1a.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.2.1.1.1.1" class="ltx_p" style="width:108.4pt;">advice</span>
</span>
</td>
<td id="S0.T1a.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.2.1.2.1.1" class="ltx_p" style="width:281.9pt;">We would like to classify whether a text is from an advice column. Advice columns answer letters from a reader seeking advice, or give unsolicited advice to readers. Examples include Dear Abby, Ask Ann Landers, Dear Doctor, etc. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T1a.1.3.2" class="ltx_tr">
<td id="S0.T1a.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.3.2.1.1.1" class="ltx_p" style="width:108.4pt;">antitrust</span>
</span>
</td>
<td id="S0.T1a.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.3.2.2.1.1" class="ltx_p" style="width:281.9pt;">We would like to classify whether a text is about antitrust action. An article that is about antitrust action covers business practices that stifle competition, or accusations of such practices. It might involve legal action, government regulation, the breaking up of monopolies, or any plans to do so. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T1a.1.4.3" class="ltx_tr">
<td id="S0.T1a.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.4.3.1.1.1" class="ltx_p" style="width:108.4pt;">bible</span>
</span>
</td>
<td id="S0.T1a.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.4.3.2.1.1" class="ltx_p" style="width:281.9pt;">We would like to classify whether a text reproduces a short religious blurb, like a quote from the Bible, prayer for the day, spiritual thought for the day, without explanations, opinions, interpretations, or discourses. Long sermons, discourse, or longer texts quoting from the Bible won’t count as this ‘short religious blurb.’ This will generally look like ‘X for the day’ - short and crisp with no explanations or opinions at all besides the small blurb. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T1a.1.5.4" class="ltx_tr">
<td id="S0.T1a.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.5.4.1.1.1" class="ltx_p" style="width:108.4pt;">civil rights movement</span>
</span>
</td>
<td id="S0.T1a.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.5.4.2.1.1" class="ltx_p" style="width:281.9pt;">We would like to classify whether a text is about the Civil Rights movement. This includes articles referring to organizations and individuals that protested racism against Black Americans, and articles discussing racial discrimination in the government, social and educational inequality of African Americans, police brutality against African Americans, the use of federal power to protect civil rights, or segregation. Articles about protests and riots are on topic if they stemmed from conflicts over civil rights or occurred after the death of Martin Luther King Jr., but race riots and acts of violence involving African Americans should not be on topic if they do not refer to discrimination. Some articles may refer to states rights or express anger about rioting—these articles are on topic only if race or federal protection of civil rights is mentioned. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T1a.1.6.5" class="ltx_tr">
<td id="S0.T1a.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.6.5.1.1.1" class="ltx_p" style="width:108.4pt;">contraception</span>
</span>
</td>
<td id="S0.T1a.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T1a.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1a.1.6.5.2.1.1" class="ltx_p" style="width:281.9pt;">We would like to classify whether a text is about contraception. Abortion is not contraception. Yes/No:</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S0.T2" class="ltx_table ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-2: </span>Prompts used to query OpenAI GPT models (continued)</figcaption>
<table id="S0.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.T2.1.1.1" class="ltx_tr">
<th id="S0.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.1.1.1.1.1" class="ltx_p" style="width:108.4pt;"><span id="S0.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Topic</span></span>
</span>
</th>
<th id="S0.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.1.1.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="S0.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.T2.1.2.1" class="ltx_tr">
<td id="S0.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.2.1.1.1.1" class="ltx_p" style="width:108.4pt;">crime</span>
</span>
</td>
<td id="S0.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.2.1.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text covers about crime. This includes reports of crimes and investigations, coverage of court proceedings, law enforcement, and discussions of crime prevention and community safety. Violations of international law are not considered crimes. Nor are actions that may be unethical but are not illegal. Articles about Watergate should not be classified as about crime. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T2.1.3.2" class="ltx_tr">
<td id="S0.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.3.2.1.1.1" class="ltx_p" style="width:108.4pt;">horoscope</span>
</span>
</td>
<td id="S0.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.3.2.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is a newspaper horoscope. Horoscopes are articles that make predictions about people’s futures based on astrological signals or signs (sun signs for instance) and are written to entertain and predict future events. They can be of daily frequency, monthly or even yearly. They can also be making astrological predictions for celebrities and known personalities. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T2.1.4.3" class="ltx_tr">
<td id="S0.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.4.3.1.1.1" class="ltx_p" style="width:108.4pt;">labor movement</span>
</span>
</td>
<td id="S0.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.4.3.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text covers any American labor movements. A text that is on topic may cover unions striking, advocating for workers’ rights, lobbying the government, or speaking about the experience of working in their industry. It may also cover statements by employers that express anger at union action. Articles that express support for or criticize the labor movement should both count as on topic. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T2.1.5.4" class="ltx_tr">
<td id="S0.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.5.4.1.1.1" class="ltx_p" style="width:108.4pt;">obituaries</span>
</span>
</td>
<td id="S0.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.5.4.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is an obituary. An obituary is defined as an article which mentions a death of a person and provides their Biographical/Family information or Funeral information. Articles which just mention deaths of individuals in passing and do not summarize information about their life or their funeral services are not obituaries. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T2.1.6.5" class="ltx_tr">
<td id="S0.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.6.5.1.1.1" class="ltx_p" style="width:108.4pt;">pesticide</span>
</span>
</td>
<td id="S0.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.6.5.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether an article is about pesticides. Texts are on topic if they mention any chemical attempt to deter pests (insects, other animals, or fungi) that were on a crop or plant. This does not include herbicides, killing insects not on crops, non-chemical bug killing, or anything else about insects. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T2.1.7.6" class="ltx_tr">
<td id="S0.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.7.6.1.1.1" class="ltx_p" style="width:108.4pt;">polio vaccine</span>
</span>
</td>
<td id="S0.T2.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T2.1.7.6.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is about the polio vaccine. Yes/No:</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S0.T3" class="ltx_table ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-3: </span>Prompts used to query OpenAI GPT models (continued)</figcaption>
<table id="S0.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.T3.1.1.1" class="ltx_tr">
<th id="S0.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.1.1.1.1.1" class="ltx_p" style="width:108.4pt;"><span id="S0.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Topic</span></span>
</span>
</th>
<th id="S0.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.1.1.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="S0.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.T3.1.2.1" class="ltx_tr">
<td id="S0.T3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.2.1.1.1.1" class="ltx_p" style="width:108.4pt;">politics</span>
</span>
</td>
<td id="S0.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.2.1.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is a political news. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T3.1.3.2" class="ltx_tr">
<td id="S0.T3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.3.2.1.1.1" class="ltx_p" style="width:108.4pt;">protests</span>
</span>
</td>
<td id="S0.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.3.2.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is about protests. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T3.1.4.3" class="ltx_tr">
<td id="S0.T3.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.4.3.1.1.1" class="ltx_p" style="width:108.4pt;">Red Scare</span>
</span>
</td>
<td id="S0.T3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.4.3.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is about the Red Scare or McCarthyism. Any text that reports concerns about Communist infiltration into the U.S. government or institutions will be on topic. In addition, any accusations of U.S. citizens display communist sympathies will be on topic. References to the Korean War or the expansion of Communism abroad are not on topic, though fears of espionage in the U.S. by foreign Communists agents are on topic. Articles casting doubt on the extent of Communist influences in the U.S. should also be on topic. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T3.1.5.4" class="ltx_tr">
<td id="S0.T3.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.5.4.1.1.1" class="ltx_p" style="width:108.4pt;">schedules</span>
</span>
</td>
<td id="S0.T3.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.5.4.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is a schedule. This includes TV/Game/Movie/Church and other schedules. Schedules list multiple events or programs on the same medium or by the same organization. They are generally lists of time stamps of events followed by no (or a one line) description about them. A church calendar is a schedule, but a summary of the Sunday service is not. Movie listings are a schedule but a special screening of a particular movie is not. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T3.1.6.5" class="ltx_tr">
<td id="S0.T3.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.6.5.1.1.1" class="ltx_p" style="width:108.4pt;">sports</span>
</span>
</td>
<td id="S0.T3.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T3.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T3.1.6.5.2.1.1" class="ltx_p" style="width:303.5pt;">We would like you to classify whether a text is about sports. This does not include fishing, hunting, or flying. Yes/No:</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S0.T4" class="ltx_table ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-4: </span>Prompts used to query OpenAI GPT models (continued)</figcaption>
<table id="S0.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.T4.1.1.1" class="ltx_tr">
<th id="S0.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.1.1.1.1.1" class="ltx_p" style="width:108.4pt;"><span id="S0.T4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Topic</span></span>
</span>
</th>
<th id="S0.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.1.1.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="S0.T4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.T4.1.2.1" class="ltx_tr">
<td id="S0.T4.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.2.1.1.1.1" class="ltx_p" style="width:108.4pt;">Vietnam War</span>
</span>
</td>
<td id="S0.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.2.1.2.1.1" class="ltx_p" style="width:303.5pt;">We would like you to classify whether a text pertains to (any aspect of) the Vietnam War. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T4.1.3.2" class="ltx_tr">
<td id="S0.T4.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.3.2.1.1.1" class="ltx_p" style="width:108.4pt;">weather</span>
</span>
</td>
<td id="S0.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.3.2.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify texts that contain measurements about the weather. These include the weather forecast as well as texts summarizing what the weather was in the recent past (e.g., temperature, precipitation, visibility). This also includes articles about weather records, like the coldest/hottest days in a decade/year or coldest/hottest places. If the article contains weather measurements but also talks about the consequences of the weather in passing (e.g., damage, events cancelled), we will call it about weather measurements. However, if it just talks about the consequences of the weather (e.g., closures, accidents, damage) without giving the weather measurements, this does not count. As a rule of thumb, if there is precise weather measurement somewhere in the article- call it a weather measurement article. Yes/No:</span>
</span>
</td>
</tr>
<tr id="S0.T4.1.4.3" class="ltx_tr">
<td id="S0.T4.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.4.3.1.1.1" class="ltx_p" style="width:108.4pt;">World War I</span>
</span>
</td>
<td id="S0.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:4pt;padding-bottom:4pt;">
<span id="S0.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T4.1.4.3.2.1.1" class="ltx_p" style="width:303.5pt;">We would like to classify whether a text is about World War I. Yes/No:</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.15337" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.15339" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.15339">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.15339" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.15340" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:18:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
