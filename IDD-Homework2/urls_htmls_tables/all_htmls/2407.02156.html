<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.02156] Towards training music taggers on synthetic data</title><meta property="og:description" content="Most contemporary music tagging systems rely on large volumes of annotated data. As an alternative, we investigate the extent to which synthetically generated music excerpts can improve tagging systems when only small …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards training music taggers on synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards training music taggers on synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.02156">

<!--Generated on Mon Aug  5 18:39:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
music information retrieval,  genre detection,  music tagging,  domain adaptation,  generative music
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards training music taggers on synthetic data
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nadine Kroher
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Time Machine Capital Squared Ltd.</span>
<br class="ltx_break">London, United Kingdom 
<br class="ltx_break">nadine@tmc2.ai
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Manangu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">Time Machine Capital Squared Ltd.</span>
<br class="ltx_break">London, United Kingdom 
<br class="ltx_break">steven@tmc2.ai
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aggelos Pikrakis
</span><span class="ltx_author_notes">Aggelos Pikrakis is also a scientific advisor to Time Machine Capital Squared Ltd.
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Department of Informatics</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">University of Piraeus
<br class="ltx_break"></span>Piraeus, Greece 
<br class="ltx_break">pikrakis@unipi.gr
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Most contemporary music tagging systems rely on large volumes of annotated data. As an alternative, we investigate the extent to which synthetically generated music excerpts can improve tagging systems when only small annotated collections are available. To this end, we release <span id="id5.id1.1" class="ltx_text ltx_font_italic">GTZAN-synth</span>, a synthetic dataset that follows the taxonomy of the well-known <span id="id5.id1.2" class="ltx_text ltx_font_italic">GTZAN</span> dataset while being ten times larger in data volume. We first observe that simply adding this synthetic dataset to the training split of <span id="id5.id1.3" class="ltx_text ltx_font_italic">GTZAN</span> does not result into performance improvements. We then proceed to investigating domain adaptation, transfer learning and fine-tuning strategies for the task at hand and draw the conclusion that the last two options yield an increase in accuracy. Overall, the proposed approach can be considered as a first guide in a promising field for future research.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
music information retrieval, genre detection, music tagging, domain adaptation, generative music

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Systems which can automatically classify or tag music tracks are an essential component of large-scale music and multimedia indexing pipelines. Recent methods commonly rely on deep architectures which are either trained end-to-end on large annotated datasets (i.e. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) or pre-trained on large volumes of real-world audio data and then fine-tuned for various downstream tasks on smaller collections (i.e. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>). Although both approaches have shown promising results, they still rely on large volumes of labelled data to effectively pre-train or train end-to-end deep neural networks, a prerequisite which comes at great cost. Manual labelling is a time-consuming and tedious task and crowd-sourced or user-contributed annotations are often noisy or inconsistent. To alleviate the problem, recent approaches commonly employ data augmentation strategies (see i.e. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>) which create variants of training instances by applying small modifications, i.e. pitch shifting or time stretching. While data augmentation does generally yield improvements, the augmented examples are still strongly correlated with the originals and do not prevent overfitting to the extent that adding new data would.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In an attempt to overcome the dependence on annotated data further, we explore an alternative strategy that makes use of synthetically generated music to train a tagging system. Similar approaches have been adopted by computer vision systems to address tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for which annotated data is hard to assemble, as it is for example the case with defect detection in the steel industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, vessel classification from overhead imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or medical image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In the audio domain, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> recently demonstrated that environmental sound classification can be improved by augmenting real-world datasets with synthetic examples generated with text-to-audio models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In order to explore a similar approach in the music domain, we recently conducted a preliminary proof-of-concept experiment which involved only a small taxonomy of five perceptually easy to distinguish genres <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which showed promising results. Based on these findings, we now extend our work to <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">GTZAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, a small genre classification dataset, and explore various strategies for training on synthetic music data. Specifically, we first create <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">GTZAN-synth</span>, a collection of artificially generated music excerpts that follows the <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">GTZAN</span> taxonomy while being <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn type="integer" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">10</annotation></semantics></math> times larger in data volume. We make all code for reproducing and scaling this collection publicly available to foster further research into this direction.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition, we explore several strategies towards improving the classification of a deep convolutional neural network on the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">GTZAN</span> dataset by incorporating synthetic music excerpts into the training procedure. Our experiments cover a wide spectrum of options, including the simple integration of the synthetically generated data with the training split of <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">GTZAN</span>, the investigation of transfer learning and fine-tuning strategies, and a domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> mechanism to mitigate the distributional shift between real and synthetic data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Music tagging</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Music tagging systems analyse audio content in order to automatically annotate music tracks with high-level descriptors related to mood (i.e. “melancholic” or “upbeat”), genre (i.e. “rock” or “jazz”) or instrumentation (i.e. “string quartet” or “saxophone”). Most recent methods rely on deep neural network architectures, such as convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which are usually trained end-to-end on large annotated music collections like the Million Song Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or the Magna Tag-a-Tune dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Other approaches leverage representations learned in a supervised setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> on even larger datasets and then formulate specific tagging problems as downstream tasks which are solved via transfer learning. In both scenarios, large volumes of annotated data are required, which may not be available for every use-case.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In order to overcome this need, some recent efforts have focused on self-supervised techniques (partially <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or fully <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>) to learn audio representations. While this approach, which is also commonly used in natural language processing, has shown promising results, we explore in this paper the radically different idea of leveraging generative music systems to create artificial training data. To the best of our knowledge, with the exception of our previous exploratory study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, this is the first work aimed at investigating frameworks for training music taggers on synthetically generated music datasets.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Generative music systems</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The recent advancements in generative artificial intelligence for natural language processing and image analysis tasks have also driven rapid improvements of state of the art systems that can generate music excerpts conditioned on text and melody prompts. Prominent generative music models include Jukebox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, MusicLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Jen-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and MusicGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. In this study, we focus on MusicGen, for which pre-trained models are publicly available via the <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">Audiocraft<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span id="footnote1.4" class="ltx_text ltx_font_upright">https://github.com/facebookresearch/audiocraft/</span></span></span></span></span> library.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">While the distribution of synthetically created music via streaming services or its use in movies, TV or social media content poses legal issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and has raised controversial discussions from an ethical point of view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, we believe that there is unexplored potential in leveraging such systems for the creation of synthetic training data for tagging and analysis models.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Domain adaptation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Although generative systems for natural language and image content have become very sophisticated over the recent years and are now capable of producing convincing output, generative music systems, while advancing rapidly, are still limited in their ability to produce realistic music tracks and are highly relevant to the input prompt. Consequently, with a view on training music taggers, it is reasonable to expect a content distribution shift between real and synthetic data.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">This issue has also been addressed in other domains, mainly in computer vision, via the so-called domain adaptation (DA) methods, which essentially introduce additional loss terms during the training stage to ensure that a machine learning model trained partially or fully on synthetic data will generalise its performance to real world data. For a comprehensive review of different methods on various use-cases, training frameworks and domains, we refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In the context of music, existing work on domain adaptation has been limited to specific use-cases that do not involve synthetic data, i.e. cross-cultural emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or the compensation of differences among microphones for the purposes of piano transcription <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">For the music tagging task at hand, we investigate the method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which addresses supervised DA in neural network training via an additional loss term that operates on a bottleneck layer and essentially acts as a contrastive loss that forces intermediate representations from real and synthetic data of the same class to be in close proximity in a Euclidean sense while maintaining large Euclidean distances to instances of different classes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Data</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our experiments make use of two datasets, the well-known <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">GTZAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> genre detection dataset and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">GTZAN-synth</span>, a synthetic dataset which we generated using the MusicGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> generative music system. <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">GTZAN-synth</span> follows the <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">GTZAN</span> class taxonomy but it is ten times larger.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Real music dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">During the early days of music tagging, the <span id="S3.SS1.p1.4.1" class="ltx_text ltx_font_italic">GTZAN</span> dataset was widely used as a benchmark collection to evaluate and compare competing approaches. It contains a total of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">1000</annotation></semantics></math> music excerpts of length <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="30s" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">30</cn><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">30s</annotation></semantics></math> belonging to <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">10</annotation></semantics></math> genres (e.g., “blues”, “classical” or “rock”), with each genre being represented by <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">100</annotation></semantics></math> tracks. The dataset, including audio files and metadata, is publicly available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification</span></span></span> via the Kaggle platform.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In our experimental evaluation, we the use artist-filtered validation splits proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> which are available on GitHub<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/julianofoleiss/gtzan_sturm_filter_3folds_stratified/</span></span></span>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Synthetic music dataset</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">In order to study the suitability of synthetically generated music for training tagging systems and foster future work on this topic, we release the synthetic music dataset <span id="S3.SS2.p1.2.1" class="ltx_text ltx_font_italic">GTZAN-synth</span>. Following the <span id="S3.SS2.p1.2.2" class="ltx_text ltx_font_italic">GTZAN</span> taxonomy, we create genre-specific prompts that are used to condition the medium-sized MusicGen model. Compared to the original <span id="S3.SS2.p1.2.3" class="ltx_text ltx_font_italic">GTZAN</span>, we scale by factor of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">10</annotation></semantics></math>, thus generating <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">1000</annotation></semantics></math> tracks per genre.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">MusicGen is controllable via text prompts and while its generative process is to a certain degree stochastic, e.g. the prompt “a rock song” will give different results if run repeatedly, we observed during our initial experiments that large volumes of excerpts generated with a single prompt do not exhibit sufficient diversity. Consequently, the main challenge in generating a synthetic music dataset lies in the creation of a large volume of genre-specific text prompts that yield sufficient variety while maintaining genre-specific characteristics.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">During initial experimentation, we also observed that MusicGen does not appear to generate vocals, even if prompted to do so. In addition, we noticed that prompts mentioning vocals often lead to output artefacts. As a result, we are restricted to generating instrumental examples.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">To assemble an adequate number of text prompts for the MusicGen synthesis engine, we used a large language model (LLM) of the GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> family which can be accessed via the OpenAI API<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://openai.com/blog/openai-api</span></span></span>. To avoid confusion, we will refer to the text input that drives the GPT-3 LLM as the <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">LLM-prompt</span> and the text guidance input to the MusicGen model as the <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">MusicGen-prompt</span>. Figure <a href="#S3.F1" title="Figure 1 ‣ III-B2 MusicGen-prompt engineering ‣ III-B Synthetic music dataset ‣ III Data ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts our prompt engineering pipeline along with an example.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">We make the recipe for generating <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_italic">LLM-promts</span>, the generated <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mn id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><cn type="integer" id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">10</annotation></semantics></math>k <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_italic">LLM-promts</span>, as well as code for generating the music excerpts with MusicGen publicly available<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>URL will be inserted upon paper acceptance</span></span></span>.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>LLM-prompt engineering</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We used the <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">gpt-3.5-turbo</span> model available via OpenAI’s chat completion API and set the temperature parameter to <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mn id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><cn type="float" id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">0.5</annotation></semantics></math> to ensure sufficient variety over consecutive runs. The model takes an optional system prompt as an additional input which steers the overall behaviour of the model with respect to tone and style of interaction. Here, we used the following short system prompt: <span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">You are a music expert writing short textual descriptions for songs.</span> When engineering the <span id="S3.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">LLM-prompt</span>, we aimed at enforcing the generation of prompts that are limited to instrumental music, that mention the genre specifically and that describe various aspects of a music track. We experimentally found that the following <span id="S3.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_italic">LLM-prompt</span>, where {genre} is a placeholder for the respective genre, yielded stable and convincing results: <span id="S3.SS2.SSS1.p1.1.5" class="ltx_text ltx_font_italic">Write a description for an instrumental </span>{<span id="S3.SS2.SSS1.p1.1.6" class="ltx_text ltx_font_italic">genre</span>}<span id="S3.SS2.SSS1.p1.1.7" class="ltx_text ltx_font_italic"> track. The description is a single sentence. It mentions that it is an instrumental </span>{<span id="S3.SS2.SSS1.p1.1.8" class="ltx_text ltx_font_italic">genre</span>}<span id="S3.SS2.SSS1.p1.1.9" class="ltx_text ltx_font_italic"> track and gives details on tempo and instruments.</span></p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>MusicGen-prompt engineering</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">While most of the generated textual descriptions yielded convincing output when used to guide MusicGen, we observed that simply mentioning the genre once in the prompt did not guarantee a good alignment with the target class. We experimented with different prompt engineering strategies and observed that the best option for forcing the model to generate music excerpts that are prototypical for a particular genre is to simply prepend the genre several times to the prompt.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Based on this observation, we generated the <span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">MusicGen-prompts</span> by concatenating two repetitions of the class name with the <span id="S3.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">LLM-promp</span> as follows: {<span id="S3.SS2.SSS2.p2.1.3" class="ltx_text ltx_font_italic">genre</span>}<span id="S3.SS2.SSS2.p2.1.4" class="ltx_text ltx_font_italic"> </span>{<span id="S3.SS2.SSS2.p2.1.5" class="ltx_text ltx_font_italic">genre</span>}<span id="S3.SS2.SSS2.p2.1.6" class="ltx_text ltx_font_italic"> </span>{<span id="S3.SS2.SSS2.p2.1.7" class="ltx_text ltx_font_italic">LLM-prompt</span>}.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1" class="ltx_p ltx_align_center"><span id="S3.F1.1.1" class="ltx_text"><img src="/html/2407.02156/assets/prompt.png" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="101" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic representation of the proposed prompt engineering pipeline. For a given music genre, we use an LLM to generate genre-specific track descriptions. After some additional processing, these serve as input to MusicGen, a model for text-conditioned music generation.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Music generation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.4" class="ltx_p">Finally, in order to generate synthetic music excerpts, we employ the medium-sized version of the pre-trained MusicGen model, which has around <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="1.5B" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.1.m1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><times id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1"></times><cn type="float" id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">1.5</cn><ci id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">1.5B</annotation></semantics></math> parameters, and condition it on the <span id="S3.SS2.SSS3.p1.4.1" class="ltx_text ltx_font_italic">MusicGen-prompts</span>. The model operates on top of the EnCodec tokenizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> which decodes tokens from <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mn id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">4</annotation></semantics></math> codebooks at a sample rate of <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mn id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">50</annotation></semantics></math> Hz to raw audio with a sampling rate of <math id="S3.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><mn id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><cn type="integer" id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">32</annotation></semantics></math> kHz.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Model Architecture</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Since the aim of this paper is not to propose a novel genre tagging method but rather to explore the potential of using synthetically generated data, our neural network design roughly follows the MusiCNN architecture described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which was intended for end-to-end training of music taggers on large volumes of data.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.8" class="ltx_p">The network operates on <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">96</annotation></semantics></math>-band mel-spectrograms extracted from <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="integer" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">10</annotation></semantics></math>s of audio with a sampling rate of <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn type="integer" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">16</annotation></semantics></math>kHz, a window size of <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.p2.4.m4.1a"><mn id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><cn type="integer" id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">512</annotation></semantics></math> samples and a hop size of <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.p2.5.m5.1a"><mn id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><cn type="integer" id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">256</annotation></semantics></math> samples. The input is then processed by a first layer of parallel convolutional filters of different kernel shapes which are designed to learn different temporal and timbral features. The resulting feature maps are then pooled across the full frequency axis, concatenated and processed by further convolutional layers with residual connections and pooling operations before being passed through a normalised dense layer with <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.p2.6.m6.1a"><mn id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><cn type="integer" id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">512</annotation></semantics></math> units and a final softmax classification layer with <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.7.m7.1a"><mn id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><cn type="integer" id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">10</annotation></semantics></math> units corresponding to the <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p2.8.m8.1a"><mn id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><cn type="integer" id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">10</annotation></semantics></math> genre classes. The architecture is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ IV Model Architecture ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For a detailed description of the motivation behind the design choice the reader is referred to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<p id="S4.F2.1" class="ltx_p ltx_align_center"><span id="S4.F2.1.1" class="ltx_text"><img src="/html/2407.02156/assets/arch.png" id="S4.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="180" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic representation of the model architecture which takes a mel-spectrogram as input and predicts the music genre in a multi-class classification task. <span id="S4.F2.7.1" class="ltx_text ltx_font_bold">2D-Conv</span>: two-dimensional convolutional layer. <span id="S4.F2.8.2" class="ltx_text ltx_font_bold">LN</span>: layer normalization. <span id="S4.F2.9.3" class="ltx_text ltx_font_bold">CC</span>: concatenation. <span id="S4.F2.10.4" class="ltx_text ltx_font_bold">MaxPool</span>: maximum pooling. <span id="S4.F2.11.5" class="ltx_text ltx_font_bold">AvgPool</span>: average pooling.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In our experimental evaluation, we train the model end-to-end and also in transfer learning and fine-tuning settings. In the case of transfer learning, the entire network is pre-trained on a large dataset and then only the penultimate dense layer and the final classification layer are trained from scratch on the smaller dataset with all other layers keeping their weights frozen. In the fine-tuning scenario, the entire network is first trained on a large collection and then training for all weights is resumed on the smaller collection.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Supervised domain adaptation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.7" class="ltx_p">In order to compensate for potential distributional discrepancies between synthetic and real music data, we experiment with the supervised domain adaptation approach proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We can describe the genre classification network as <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="f=g\circ h" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">f</mi><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml"><mi id="S5.p1.1.m1.1.1.3.2" xref="S5.p1.1.m1.1.1.3.2.cmml">g</mi><mo lspace="0.222em" rspace="0.222em" id="S5.p1.1.m1.1.1.3.1" xref="S5.p1.1.m1.1.1.3.1.cmml">∘</mo><mi id="S5.p1.1.m1.1.1.3.3" xref="S5.p1.1.m1.1.1.3.3.cmml">h</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><eq id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></eq><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝑓</ci><apply id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3"><compose id="S5.p1.1.m1.1.1.3.1.cmml" xref="S5.p1.1.m1.1.1.3.1"></compose><ci id="S5.p1.1.m1.1.1.3.2.cmml" xref="S5.p1.1.m1.1.1.3.2">𝑔</ci><ci id="S5.p1.1.m1.1.1.3.3.cmml" xref="S5.p1.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">f=g\circ h</annotation></semantics></math>, where <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="g:\mathcal{X}\rightarrow\mathcal{Z}" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">g</mi><mo lspace="0.278em" rspace="0.278em" id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">:</mo><mrow id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p1.2.m2.1.1.3.2" xref="S5.p1.2.m2.1.1.3.2.cmml">𝒳</mi><mo stretchy="false" id="S5.p1.2.m2.1.1.3.1" xref="S5.p1.2.m2.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S5.p1.2.m2.1.1.3.3" xref="S5.p1.2.m2.1.1.3.3.cmml">𝒵</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><ci id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1">:</ci><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">𝑔</ci><apply id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3"><ci id="S5.p1.2.m2.1.1.3.1.cmml" xref="S5.p1.2.m2.1.1.3.1">→</ci><ci id="S5.p1.2.m2.1.1.3.2.cmml" xref="S5.p1.2.m2.1.1.3.2">𝒳</ci><ci id="S5.p1.2.m2.1.1.3.3.cmml" xref="S5.p1.2.m2.1.1.3.3">𝒵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">g:\mathcal{X}\rightarrow\mathcal{Z}</annotation></semantics></math> takes the mel-spectrogram <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{X}" display="inline"><semantics id="S5.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><ci id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">\mathcal{X}</annotation></semantics></math> as input and maps it to the penultimate dense layer <math id="S5.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S5.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml">𝒵</mi><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><ci id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1">𝒵</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">\mathcal{Z}</annotation></semantics></math>, which we will treat as an intermediate feature representation. The classification layer <math id="S5.p1.5.m5.1" class="ltx_Math" alttext="h:\mathcal{Z}\rightarrow\mathcal{Y}" display="inline"><semantics id="S5.p1.5.m5.1a"><mrow id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml"><mi id="S5.p1.5.m5.1.1.2" xref="S5.p1.5.m5.1.1.2.cmml">h</mi><mo lspace="0.278em" rspace="0.278em" id="S5.p1.5.m5.1.1.1" xref="S5.p1.5.m5.1.1.1.cmml">:</mo><mrow id="S5.p1.5.m5.1.1.3" xref="S5.p1.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p1.5.m5.1.1.3.2" xref="S5.p1.5.m5.1.1.3.2.cmml">𝒵</mi><mo stretchy="false" id="S5.p1.5.m5.1.1.3.1" xref="S5.p1.5.m5.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S5.p1.5.m5.1.1.3.3" xref="S5.p1.5.m5.1.1.3.3.cmml">𝒴</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><apply id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1"><ci id="S5.p1.5.m5.1.1.1.cmml" xref="S5.p1.5.m5.1.1.1">:</ci><ci id="S5.p1.5.m5.1.1.2.cmml" xref="S5.p1.5.m5.1.1.2">ℎ</ci><apply id="S5.p1.5.m5.1.1.3.cmml" xref="S5.p1.5.m5.1.1.3"><ci id="S5.p1.5.m5.1.1.3.1.cmml" xref="S5.p1.5.m5.1.1.3.1">→</ci><ci id="S5.p1.5.m5.1.1.3.2.cmml" xref="S5.p1.5.m5.1.1.3.2">𝒵</ci><ci id="S5.p1.5.m5.1.1.3.3.cmml" xref="S5.p1.5.m5.1.1.3.3">𝒴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">h:\mathcal{Z}\rightarrow\mathcal{Y}</annotation></semantics></math> maps this intermediate representation <math id="S5.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S5.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml">𝒵</mi><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.1b"><ci id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1">𝒵</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.1c">\mathcal{Z}</annotation></semantics></math> to the final classifier output <math id="S5.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><semantics id="S5.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p1.7.m7.1.1" xref="S5.p1.7.m7.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S5.p1.7.m7.1b"><ci id="S5.p1.7.m7.1.1.cmml" xref="S5.p1.7.m7.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.7.m7.1c">\mathcal{Y}</annotation></semantics></math>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.7" class="ltx_p">Without any counter measures, any distributional shift between real <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="X_{r}" display="inline"><semantics id="S5.p2.1.m1.1a"><msub id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mi id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">X</mi><mi id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1">subscript</csymbol><ci id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">𝑋</ci><ci id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">X_{r}</annotation></semantics></math> and synthetic <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="X_{s}" display="inline"><semantics id="S5.p2.2.m2.1a"><msub id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mi id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">X</mi><mi id="S5.p2.2.m2.1.1.3" xref="S5.p2.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1">subscript</csymbol><ci id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">𝑋</ci><ci id="S5.p2.2.m2.1.1.3.cmml" xref="S5.p2.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">X_{s}</annotation></semantics></math> inputs can potentially propagate to the intermediate representation, resulting in discrepancies between <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{Z}_{r}" display="inline"><semantics id="S5.p2.3.m3.1a"><msub id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">𝒵</mi><mi id="S5.p2.3.m3.1.1.3" xref="S5.p2.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1">subscript</csymbol><ci id="S5.p2.3.m3.1.1.2.cmml" xref="S5.p2.3.m3.1.1.2">𝒵</ci><ci id="S5.p2.3.m3.1.1.3.cmml" xref="S5.p2.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\mathcal{Z}_{r}</annotation></semantics></math> and <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{Z}_{s}" display="inline"><semantics id="S5.p2.4.m4.1a"><msub id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">𝒵</mi><mi id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1">subscript</csymbol><ci id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">𝒵</ci><ci id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\mathcal{Z}_{s}</annotation></semantics></math>. In order to drive the network towards learning feature representations which encode commonalities between the two domains, we employ a contrastive loss <math id="S5.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{SA}" display="inline"><semantics id="S5.p2.5.m5.1a"><msub id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">ℒ</mi><mrow id="S5.p2.5.m5.1.1.3" xref="S5.p2.5.m5.1.1.3.cmml"><mi id="S5.p2.5.m5.1.1.3.2" xref="S5.p2.5.m5.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.p2.5.m5.1.1.3.1" xref="S5.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.p2.5.m5.1.1.3.3" xref="S5.p2.5.m5.1.1.3.3.cmml">A</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1">subscript</csymbol><ci id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">ℒ</ci><apply id="S5.p2.5.m5.1.1.3.cmml" xref="S5.p2.5.m5.1.1.3"><times id="S5.p2.5.m5.1.1.3.1.cmml" xref="S5.p2.5.m5.1.1.3.1"></times><ci id="S5.p2.5.m5.1.1.3.2.cmml" xref="S5.p2.5.m5.1.1.3.2">𝑆</ci><ci id="S5.p2.5.m5.1.1.3.3.cmml" xref="S5.p2.5.m5.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">\mathcal{L}_{SA}</annotation></semantics></math> in addition to the classification loss <math id="S5.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{CLS}" display="inline"><semantics id="S5.p2.6.m6.1a"><msub id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.6.m6.1.1.2" xref="S5.p2.6.m6.1.1.2.cmml">ℒ</mi><mrow id="S5.p2.6.m6.1.1.3" xref="S5.p2.6.m6.1.1.3.cmml"><mi id="S5.p2.6.m6.1.1.3.2" xref="S5.p2.6.m6.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.p2.6.m6.1.1.3.1" xref="S5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.p2.6.m6.1.1.3.3" xref="S5.p2.6.m6.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.p2.6.m6.1.1.3.1a" xref="S5.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.p2.6.m6.1.1.3.4" xref="S5.p2.6.m6.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><apply id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p2.6.m6.1.1.1.cmml" xref="S5.p2.6.m6.1.1">subscript</csymbol><ci id="S5.p2.6.m6.1.1.2.cmml" xref="S5.p2.6.m6.1.1.2">ℒ</ci><apply id="S5.p2.6.m6.1.1.3.cmml" xref="S5.p2.6.m6.1.1.3"><times id="S5.p2.6.m6.1.1.3.1.cmml" xref="S5.p2.6.m6.1.1.3.1"></times><ci id="S5.p2.6.m6.1.1.3.2.cmml" xref="S5.p2.6.m6.1.1.3.2">𝐶</ci><ci id="S5.p2.6.m6.1.1.3.3.cmml" xref="S5.p2.6.m6.1.1.3.3">𝐿</ci><ci id="S5.p2.6.m6.1.1.3.4.cmml" xref="S5.p2.6.m6.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">\mathcal{L}_{CLS}</annotation></semantics></math>. <math id="S5.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{L}_{SA}" display="inline"><semantics id="S5.p2.7.m7.1a"><msub id="S5.p2.7.m7.1.1" xref="S5.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.7.m7.1.1.2" xref="S5.p2.7.m7.1.1.2.cmml">ℒ</mi><mrow id="S5.p2.7.m7.1.1.3" xref="S5.p2.7.m7.1.1.3.cmml"><mi id="S5.p2.7.m7.1.1.3.2" xref="S5.p2.7.m7.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.p2.7.m7.1.1.3.1" xref="S5.p2.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.p2.7.m7.1.1.3.3" xref="S5.p2.7.m7.1.1.3.3.cmml">A</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p2.7.m7.1b"><apply id="S5.p2.7.m7.1.1.cmml" xref="S5.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S5.p2.7.m7.1.1.1.cmml" xref="S5.p2.7.m7.1.1">subscript</csymbol><ci id="S5.p2.7.m7.1.1.2.cmml" xref="S5.p2.7.m7.1.1.2">ℒ</ci><apply id="S5.p2.7.m7.1.1.3.cmml" xref="S5.p2.7.m7.1.1.3"><times id="S5.p2.7.m7.1.1.3.1.cmml" xref="S5.p2.7.m7.1.1.3.1"></times><ci id="S5.p2.7.m7.1.1.3.2.cmml" xref="S5.p2.7.m7.1.1.3.2">𝑆</ci><ci id="S5.p2.7.m7.1.1.3.3.cmml" xref="S5.p2.7.m7.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.7.m7.1c">\mathcal{L}_{SA}</annotation></semantics></math> encourages semantic alignment rather than domain-specific alignment of the intermediate feature representation.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.5" class="ltx_p">Given a synthetic instance <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="x_{s}^{i}" display="inline"><semantics id="S5.p3.1.m1.1a"><msubsup id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2.2" xref="S5.p3.1.m1.1.1.2.2.cmml">x</mi><mi id="S5.p3.1.m1.1.1.2.3" xref="S5.p3.1.m1.1.1.2.3.cmml">s</mi><mi id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1">superscript</csymbol><apply id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.2.1.cmml" xref="S5.p3.1.m1.1.1">subscript</csymbol><ci id="S5.p3.1.m1.1.1.2.2.cmml" xref="S5.p3.1.m1.1.1.2.2">𝑥</ci><ci id="S5.p3.1.m1.1.1.2.3.cmml" xref="S5.p3.1.m1.1.1.2.3">𝑠</ci></apply><ci id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">x_{s}^{i}</annotation></semantics></math>, we can compute the contrastive loss by comparing its embedding to that of a real sample <math id="S5.p3.2.m2.1" class="ltx_Math" alttext="x_{r}^{j}" display="inline"><semantics id="S5.p3.2.m2.1a"><msubsup id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml"><mi id="S5.p3.2.m2.1.1.2.2" xref="S5.p3.2.m2.1.1.2.2.cmml">x</mi><mi id="S5.p3.2.m2.1.1.2.3" xref="S5.p3.2.m2.1.1.2.3.cmml">r</mi><mi id="S5.p3.2.m2.1.1.3" xref="S5.p3.2.m2.1.1.3.cmml">j</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.1.cmml" xref="S5.p3.2.m2.1.1">superscript</csymbol><apply id="S5.p3.2.m2.1.1.2.cmml" xref="S5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.2.1.cmml" xref="S5.p3.2.m2.1.1">subscript</csymbol><ci id="S5.p3.2.m2.1.1.2.2.cmml" xref="S5.p3.2.m2.1.1.2.2">𝑥</ci><ci id="S5.p3.2.m2.1.1.2.3.cmml" xref="S5.p3.2.m2.1.1.2.3">𝑟</ci></apply><ci id="S5.p3.2.m2.1.1.3.cmml" xref="S5.p3.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">x_{r}^{j}</annotation></semantics></math> of the same label and to that of a real sample <math id="S5.p3.3.m3.1" class="ltx_Math" alttext="x_{r}^{k}" display="inline"><semantics id="S5.p3.3.m3.1a"><msubsup id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml"><mi id="S5.p3.3.m3.1.1.2.2" xref="S5.p3.3.m3.1.1.2.2.cmml">x</mi><mi id="S5.p3.3.m3.1.1.2.3" xref="S5.p3.3.m3.1.1.2.3.cmml">r</mi><mi id="S5.p3.3.m3.1.1.3" xref="S5.p3.3.m3.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.1b"><apply id="S5.p3.3.m3.1.1.cmml" xref="S5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.cmml" xref="S5.p3.3.m3.1.1">superscript</csymbol><apply id="S5.p3.3.m3.1.1.2.cmml" xref="S5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.2.1.cmml" xref="S5.p3.3.m3.1.1">subscript</csymbol><ci id="S5.p3.3.m3.1.1.2.2.cmml" xref="S5.p3.3.m3.1.1.2.2">𝑥</ci><ci id="S5.p3.3.m3.1.1.2.3.cmml" xref="S5.p3.3.m3.1.1.2.3">𝑟</ci></apply><ci id="S5.p3.3.m3.1.1.3.cmml" xref="S5.p3.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.1c">x_{r}^{k}</annotation></semantics></math> of a different label, where <math id="S5.p3.4.m4.1" class="ltx_Math" alttext="y^{i}=y^{j}" display="inline"><semantics id="S5.p3.4.m4.1a"><mrow id="S5.p3.4.m4.1.1" xref="S5.p3.4.m4.1.1.cmml"><msup id="S5.p3.4.m4.1.1.2" xref="S5.p3.4.m4.1.1.2.cmml"><mi id="S5.p3.4.m4.1.1.2.2" xref="S5.p3.4.m4.1.1.2.2.cmml">y</mi><mi id="S5.p3.4.m4.1.1.2.3" xref="S5.p3.4.m4.1.1.2.3.cmml">i</mi></msup><mo id="S5.p3.4.m4.1.1.1" xref="S5.p3.4.m4.1.1.1.cmml">=</mo><msup id="S5.p3.4.m4.1.1.3" xref="S5.p3.4.m4.1.1.3.cmml"><mi id="S5.p3.4.m4.1.1.3.2" xref="S5.p3.4.m4.1.1.3.2.cmml">y</mi><mi id="S5.p3.4.m4.1.1.3.3" xref="S5.p3.4.m4.1.1.3.3.cmml">j</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.4.m4.1b"><apply id="S5.p3.4.m4.1.1.cmml" xref="S5.p3.4.m4.1.1"><eq id="S5.p3.4.m4.1.1.1.cmml" xref="S5.p3.4.m4.1.1.1"></eq><apply id="S5.p3.4.m4.1.1.2.cmml" xref="S5.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.p3.4.m4.1.1.2.1.cmml" xref="S5.p3.4.m4.1.1.2">superscript</csymbol><ci id="S5.p3.4.m4.1.1.2.2.cmml" xref="S5.p3.4.m4.1.1.2.2">𝑦</ci><ci id="S5.p3.4.m4.1.1.2.3.cmml" xref="S5.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S5.p3.4.m4.1.1.3.cmml" xref="S5.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.p3.4.m4.1.1.3.1.cmml" xref="S5.p3.4.m4.1.1.3">superscript</csymbol><ci id="S5.p3.4.m4.1.1.3.2.cmml" xref="S5.p3.4.m4.1.1.3.2">𝑦</ci><ci id="S5.p3.4.m4.1.1.3.3.cmml" xref="S5.p3.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.4.m4.1c">y^{i}=y^{j}</annotation></semantics></math> and <math id="S5.p3.5.m5.1" class="ltx_Math" alttext="y^{i}\neq y^{k}" display="inline"><semantics id="S5.p3.5.m5.1a"><mrow id="S5.p3.5.m5.1.1" xref="S5.p3.5.m5.1.1.cmml"><msup id="S5.p3.5.m5.1.1.2" xref="S5.p3.5.m5.1.1.2.cmml"><mi id="S5.p3.5.m5.1.1.2.2" xref="S5.p3.5.m5.1.1.2.2.cmml">y</mi><mi id="S5.p3.5.m5.1.1.2.3" xref="S5.p3.5.m5.1.1.2.3.cmml">i</mi></msup><mo id="S5.p3.5.m5.1.1.1" xref="S5.p3.5.m5.1.1.1.cmml">≠</mo><msup id="S5.p3.5.m5.1.1.3" xref="S5.p3.5.m5.1.1.3.cmml"><mi id="S5.p3.5.m5.1.1.3.2" xref="S5.p3.5.m5.1.1.3.2.cmml">y</mi><mi id="S5.p3.5.m5.1.1.3.3" xref="S5.p3.5.m5.1.1.3.3.cmml">k</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.5.m5.1b"><apply id="S5.p3.5.m5.1.1.cmml" xref="S5.p3.5.m5.1.1"><neq id="S5.p3.5.m5.1.1.1.cmml" xref="S5.p3.5.m5.1.1.1"></neq><apply id="S5.p3.5.m5.1.1.2.cmml" xref="S5.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S5.p3.5.m5.1.1.2.1.cmml" xref="S5.p3.5.m5.1.1.2">superscript</csymbol><ci id="S5.p3.5.m5.1.1.2.2.cmml" xref="S5.p3.5.m5.1.1.2.2">𝑦</ci><ci id="S5.p3.5.m5.1.1.2.3.cmml" xref="S5.p3.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S5.p3.5.m5.1.1.3.cmml" xref="S5.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.p3.5.m5.1.1.3.1.cmml" xref="S5.p3.5.m5.1.1.3">superscript</csymbol><ci id="S5.p3.5.m5.1.1.3.2.cmml" xref="S5.p3.5.m5.1.1.3.2">𝑦</ci><ci id="S5.p3.5.m5.1.1.3.3.cmml" xref="S5.p3.5.m5.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.5.m5.1c">y^{i}\neq y^{k}</annotation></semantics></math> as follows:</p>
</div>
<div id="S5.p4" class="ltx_para">
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.1" class="ltx_math_unparsed" alttext="d(g(x_{s}^{i},x_{r}^{j})=\frac{1}{2}||g(x_{s}^{i})-g(x_{r}^{j})||^{2}" display="block"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1b"><mi id="S5.E1.m1.1.1">d</mi><mrow id="S5.E1.m1.1.2"><mo stretchy="false" id="S5.E1.m1.1.2.1">(</mo><mi id="S5.E1.m1.1.2.2">g</mi><mrow id="S5.E1.m1.1.2.3"><mo stretchy="false" id="S5.E1.m1.1.2.3.1">(</mo><msubsup id="S5.E1.m1.1.2.3.2"><mi id="S5.E1.m1.1.2.3.2.2.2">x</mi><mi id="S5.E1.m1.1.2.3.2.2.3">s</mi><mi id="S5.E1.m1.1.2.3.2.3">i</mi></msubsup><mo id="S5.E1.m1.1.2.3.3">,</mo><msubsup id="S5.E1.m1.1.2.3.4"><mi id="S5.E1.m1.1.2.3.4.2.2">x</mi><mi id="S5.E1.m1.1.2.3.4.2.3">r</mi><mi id="S5.E1.m1.1.2.3.4.3">j</mi></msubsup><mo stretchy="false" id="S5.E1.m1.1.2.3.5">)</mo></mrow><mo id="S5.E1.m1.1.2.4">=</mo><mfrac id="S5.E1.m1.1.2.5"><mn id="S5.E1.m1.1.2.5.2">1</mn><mn id="S5.E1.m1.1.2.5.3">2</mn></mfrac><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E1.m1.1.2.6">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E1.m1.1.2.7">|</mo><mi id="S5.E1.m1.1.2.8">g</mi><mrow id="S5.E1.m1.1.2.9"><mo stretchy="false" id="S5.E1.m1.1.2.9.1">(</mo><msubsup id="S5.E1.m1.1.2.9.2"><mi id="S5.E1.m1.1.2.9.2.2.2">x</mi><mi id="S5.E1.m1.1.2.9.2.2.3">s</mi><mi id="S5.E1.m1.1.2.9.2.3">i</mi></msubsup><mo stretchy="false" id="S5.E1.m1.1.2.9.3">)</mo></mrow><mo id="S5.E1.m1.1.2.10">−</mo><mi id="S5.E1.m1.1.2.11">g</mi><mrow id="S5.E1.m1.1.2.12"><mo stretchy="false" id="S5.E1.m1.1.2.12.1">(</mo><msubsup id="S5.E1.m1.1.2.12.2"><mi id="S5.E1.m1.1.2.12.2.2.2">x</mi><mi id="S5.E1.m1.1.2.12.2.2.3">r</mi><mi id="S5.E1.m1.1.2.12.2.3">j</mi></msubsup><mo stretchy="false" id="S5.E1.m1.1.2.12.3">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E1.m1.1.2.13">|</mo><msup id="S5.E1.m1.1.2.14"><mo fence="false" stretchy="false" id="S5.E1.m1.1.2.14.2">|</mo><mn id="S5.E1.m1.1.2.14.3">2</mn></msup></mrow></mrow><annotation encoding="application/x-tex" id="S5.E1.m1.1c">d(g(x_{s}^{i},x_{r}^{j})=\frac{1}{2}||g(x_{s}^{i})-g(x_{r}^{j})||^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">and</p>
</div>
<div id="S5.p6" class="ltx_para">
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.2" class="ltx_math_unparsed" alttext="d(g(x_{s}^{i},x_{r}^{k})=\frac{1}{2}\max(0,m-||g(x_{s}^{i})-g(x_{r}^{j})||^{2})" display="block"><semantics id="S5.E2.m1.2a"><mrow id="S5.E2.m1.2b"><mi id="S5.E2.m1.2.3">d</mi><mrow id="S5.E2.m1.2.4"><mo stretchy="false" id="S5.E2.m1.2.4.1">(</mo><mi id="S5.E2.m1.2.4.2">g</mi><mrow id="S5.E2.m1.2.4.3"><mo stretchy="false" id="S5.E2.m1.2.4.3.1">(</mo><msubsup id="S5.E2.m1.2.4.3.2"><mi id="S5.E2.m1.2.4.3.2.2.2">x</mi><mi id="S5.E2.m1.2.4.3.2.2.3">s</mi><mi id="S5.E2.m1.2.4.3.2.3">i</mi></msubsup><mo id="S5.E2.m1.2.4.3.3">,</mo><msubsup id="S5.E2.m1.2.4.3.4"><mi id="S5.E2.m1.2.4.3.4.2.2">x</mi><mi id="S5.E2.m1.2.4.3.4.2.3">r</mi><mi id="S5.E2.m1.2.4.3.4.3">k</mi></msubsup><mo stretchy="false" id="S5.E2.m1.2.4.3.5">)</mo></mrow><mo id="S5.E2.m1.2.4.4">=</mo><mfrac id="S5.E2.m1.2.4.5"><mn id="S5.E2.m1.2.4.5.2">1</mn><mn id="S5.E2.m1.2.4.5.3">2</mn></mfrac><mi id="S5.E2.m1.1.1">max</mi><mrow id="S5.E2.m1.2.4.6"><mo stretchy="false" id="S5.E2.m1.2.4.6.1">(</mo><mn id="S5.E2.m1.2.2">0</mn><mo id="S5.E2.m1.2.4.6.2">,</mo><mi id="S5.E2.m1.2.4.6.3">m</mi><mo rspace="0em" id="S5.E2.m1.2.4.6.4">−</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E2.m1.2.4.6.5">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E2.m1.2.4.6.6">|</mo><mi id="S5.E2.m1.2.4.6.7">g</mi><mrow id="S5.E2.m1.2.4.6.8"><mo stretchy="false" id="S5.E2.m1.2.4.6.8.1">(</mo><msubsup id="S5.E2.m1.2.4.6.8.2"><mi id="S5.E2.m1.2.4.6.8.2.2.2">x</mi><mi id="S5.E2.m1.2.4.6.8.2.2.3">s</mi><mi id="S5.E2.m1.2.4.6.8.2.3">i</mi></msubsup><mo stretchy="false" id="S5.E2.m1.2.4.6.8.3">)</mo></mrow><mo id="S5.E2.m1.2.4.6.9">−</mo><mi id="S5.E2.m1.2.4.6.10">g</mi><mrow id="S5.E2.m1.2.4.6.11"><mo stretchy="false" id="S5.E2.m1.2.4.6.11.1">(</mo><msubsup id="S5.E2.m1.2.4.6.11.2"><mi id="S5.E2.m1.2.4.6.11.2.2.2">x</mi><mi id="S5.E2.m1.2.4.6.11.2.2.3">r</mi><mi id="S5.E2.m1.2.4.6.11.2.3">j</mi></msubsup><mo stretchy="false" id="S5.E2.m1.2.4.6.11.3">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E2.m1.2.4.6.12">|</mo><msup id="S5.E2.m1.2.4.6.13"><mo fence="false" rspace="0.167em" stretchy="false" id="S5.E2.m1.2.4.6.13.2">|</mo><mn id="S5.E2.m1.2.4.6.13.3">2</mn></msup><mo stretchy="false" id="S5.E2.m1.2.4.6.14">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S5.E2.m1.2c">d(g(x_{s}^{i},x_{r}^{k})=\frac{1}{2}\max(0,m-||g(x_{s}^{i})-g(x_{r}^{j})||^{2})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.2" class="ltx_p">where <math id="S5.p7.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p7.1.m1.1a"><mi id="S5.p7.1.m1.1.1" xref="S5.p7.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p7.1.m1.1b"><ci id="S5.p7.1.m1.1.1.cmml" xref="S5.p7.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p7.1.m1.1c">m</annotation></semantics></math> denotes a margin parameter related to the separability in the embedding space and the semantic alignment loss becomes <math id="S5.p7.2.m2.1" class="ltx_math_unparsed" alttext="\mathcal{L}_{SA}=d(g(x_{s}^{i},x_{r}^{j})+d(g(x_{s}^{i},x_{r}^{k})" display="inline"><semantics id="S5.p7.2.m2.1a"><mrow id="S5.p7.2.m2.1b"><msub id="S5.p7.2.m2.1.1"><mi class="ltx_font_mathcaligraphic" id="S5.p7.2.m2.1.1.2">ℒ</mi><mrow id="S5.p7.2.m2.1.1.3"><mi id="S5.p7.2.m2.1.1.3.2">S</mi><mo lspace="0em" rspace="0em" id="S5.p7.2.m2.1.1.3.1">​</mo><mi id="S5.p7.2.m2.1.1.3.3">A</mi></mrow></msub><mo id="S5.p7.2.m2.1.2">=</mo><mi id="S5.p7.2.m2.1.3">d</mi><mrow id="S5.p7.2.m2.1.4"><mo stretchy="false" id="S5.p7.2.m2.1.4.1">(</mo><mi id="S5.p7.2.m2.1.4.2">g</mi><mrow id="S5.p7.2.m2.1.4.3"><mo stretchy="false" id="S5.p7.2.m2.1.4.3.1">(</mo><msubsup id="S5.p7.2.m2.1.4.3.2"><mi id="S5.p7.2.m2.1.4.3.2.2.2">x</mi><mi id="S5.p7.2.m2.1.4.3.2.2.3">s</mi><mi id="S5.p7.2.m2.1.4.3.2.3">i</mi></msubsup><mo id="S5.p7.2.m2.1.4.3.3">,</mo><msubsup id="S5.p7.2.m2.1.4.3.4"><mi id="S5.p7.2.m2.1.4.3.4.2.2">x</mi><mi id="S5.p7.2.m2.1.4.3.4.2.3">r</mi><mi id="S5.p7.2.m2.1.4.3.4.3">j</mi></msubsup><mo stretchy="false" id="S5.p7.2.m2.1.4.3.5">)</mo></mrow><mo id="S5.p7.2.m2.1.4.4">+</mo><mi id="S5.p7.2.m2.1.4.5">d</mi><mrow id="S5.p7.2.m2.1.4.6"><mo stretchy="false" id="S5.p7.2.m2.1.4.6.1">(</mo><mi id="S5.p7.2.m2.1.4.6.2">g</mi><mrow id="S5.p7.2.m2.1.4.6.3"><mo stretchy="false" id="S5.p7.2.m2.1.4.6.3.1">(</mo><msubsup id="S5.p7.2.m2.1.4.6.3.2"><mi id="S5.p7.2.m2.1.4.6.3.2.2.2">x</mi><mi id="S5.p7.2.m2.1.4.6.3.2.2.3">s</mi><mi id="S5.p7.2.m2.1.4.6.3.2.3">i</mi></msubsup><mo id="S5.p7.2.m2.1.4.6.3.3">,</mo><msubsup id="S5.p7.2.m2.1.4.6.3.4"><mi id="S5.p7.2.m2.1.4.6.3.4.2.2">x</mi><mi id="S5.p7.2.m2.1.4.6.3.4.2.3">r</mi><mi id="S5.p7.2.m2.1.4.6.3.4.3">k</mi></msubsup><mo stretchy="false" id="S5.p7.2.m2.1.4.6.3.5">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S5.p7.2.m2.1c">\mathcal{L}_{SA}=d(g(x_{s}^{i},x_{r}^{j})+d(g(x_{s}^{i},x_{r}^{k})</annotation></semantics></math>. In practice, we did not observe a difference between comparing to a single randomly selected target instance versus computing the average distance over all target samples as originally suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The second option is computationally very inefficient since the forward pass over all target samples has to be computed at each training step.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.2" class="ltx_p">Finally, both loss terms can be combined as <math id="S5.p8.1.m1.1" class="ltx_Math" alttext="\mathcal{L}=\gamma\mathcal{L}_{SA}+(1-\gamma)\mathcal{L}_{CLA}" display="inline"><semantics id="S5.p8.1.m1.1a"><mrow id="S5.p8.1.m1.1.1" xref="S5.p8.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p8.1.m1.1.1.3" xref="S5.p8.1.m1.1.1.3.cmml">ℒ</mi><mo id="S5.p8.1.m1.1.1.2" xref="S5.p8.1.m1.1.1.2.cmml">=</mo><mrow id="S5.p8.1.m1.1.1.1" xref="S5.p8.1.m1.1.1.1.cmml"><mrow id="S5.p8.1.m1.1.1.1.3" xref="S5.p8.1.m1.1.1.1.3.cmml"><mi id="S5.p8.1.m1.1.1.1.3.2" xref="S5.p8.1.m1.1.1.1.3.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.1.1.1.3.1" xref="S5.p8.1.m1.1.1.1.3.1.cmml">​</mo><msub id="S5.p8.1.m1.1.1.1.3.3" xref="S5.p8.1.m1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p8.1.m1.1.1.1.3.3.2" xref="S5.p8.1.m1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S5.p8.1.m1.1.1.1.3.3.3" xref="S5.p8.1.m1.1.1.1.3.3.3.cmml"><mi id="S5.p8.1.m1.1.1.1.3.3.3.2" xref="S5.p8.1.m1.1.1.1.3.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.1.1.1.3.3.3.1" xref="S5.p8.1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S5.p8.1.m1.1.1.1.3.3.3.3" xref="S5.p8.1.m1.1.1.1.3.3.3.3.cmml">A</mi></mrow></msub></mrow><mo id="S5.p8.1.m1.1.1.1.2" xref="S5.p8.1.m1.1.1.1.2.cmml">+</mo><mrow id="S5.p8.1.m1.1.1.1.1" xref="S5.p8.1.m1.1.1.1.1.cmml"><mrow id="S5.p8.1.m1.1.1.1.1.1.1" xref="S5.p8.1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.p8.1.m1.1.1.1.1.1.1.2" xref="S5.p8.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p8.1.m1.1.1.1.1.1.1.1" xref="S5.p8.1.m1.1.1.1.1.1.1.1.cmml"><mn id="S5.p8.1.m1.1.1.1.1.1.1.1.2" xref="S5.p8.1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S5.p8.1.m1.1.1.1.1.1.1.1.1" xref="S5.p8.1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S5.p8.1.m1.1.1.1.1.1.1.1.3" xref="S5.p8.1.m1.1.1.1.1.1.1.1.3.cmml">γ</mi></mrow><mo stretchy="false" id="S5.p8.1.m1.1.1.1.1.1.1.3" xref="S5.p8.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.1.1.1.1.2" xref="S5.p8.1.m1.1.1.1.1.2.cmml">​</mo><msub id="S5.p8.1.m1.1.1.1.1.3" xref="S5.p8.1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p8.1.m1.1.1.1.1.3.2" xref="S5.p8.1.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="S5.p8.1.m1.1.1.1.1.3.3" xref="S5.p8.1.m1.1.1.1.1.3.3.cmml"><mi id="S5.p8.1.m1.1.1.1.1.3.3.2" xref="S5.p8.1.m1.1.1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.1.1.1.1.3.3.1" xref="S5.p8.1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S5.p8.1.m1.1.1.1.1.3.3.3" xref="S5.p8.1.m1.1.1.1.1.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.1.1.1.1.3.3.1a" xref="S5.p8.1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S5.p8.1.m1.1.1.1.1.3.3.4" xref="S5.p8.1.m1.1.1.1.1.3.3.4.cmml">A</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p8.1.m1.1b"><apply id="S5.p8.1.m1.1.1.cmml" xref="S5.p8.1.m1.1.1"><eq id="S5.p8.1.m1.1.1.2.cmml" xref="S5.p8.1.m1.1.1.2"></eq><ci id="S5.p8.1.m1.1.1.3.cmml" xref="S5.p8.1.m1.1.1.3">ℒ</ci><apply id="S5.p8.1.m1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1"><plus id="S5.p8.1.m1.1.1.1.2.cmml" xref="S5.p8.1.m1.1.1.1.2"></plus><apply id="S5.p8.1.m1.1.1.1.3.cmml" xref="S5.p8.1.m1.1.1.1.3"><times id="S5.p8.1.m1.1.1.1.3.1.cmml" xref="S5.p8.1.m1.1.1.1.3.1"></times><ci id="S5.p8.1.m1.1.1.1.3.2.cmml" xref="S5.p8.1.m1.1.1.1.3.2">𝛾</ci><apply id="S5.p8.1.m1.1.1.1.3.3.cmml" xref="S5.p8.1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S5.p8.1.m1.1.1.1.3.3.1.cmml" xref="S5.p8.1.m1.1.1.1.3.3">subscript</csymbol><ci id="S5.p8.1.m1.1.1.1.3.3.2.cmml" xref="S5.p8.1.m1.1.1.1.3.3.2">ℒ</ci><apply id="S5.p8.1.m1.1.1.1.3.3.3.cmml" xref="S5.p8.1.m1.1.1.1.3.3.3"><times id="S5.p8.1.m1.1.1.1.3.3.3.1.cmml" xref="S5.p8.1.m1.1.1.1.3.3.3.1"></times><ci id="S5.p8.1.m1.1.1.1.3.3.3.2.cmml" xref="S5.p8.1.m1.1.1.1.3.3.3.2">𝑆</ci><ci id="S5.p8.1.m1.1.1.1.3.3.3.3.cmml" xref="S5.p8.1.m1.1.1.1.3.3.3.3">𝐴</ci></apply></apply></apply><apply id="S5.p8.1.m1.1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1.1"><times id="S5.p8.1.m1.1.1.1.1.2.cmml" xref="S5.p8.1.m1.1.1.1.1.2"></times><apply id="S5.p8.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1.1.1.1"><minus id="S5.p8.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S5.p8.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.p8.1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S5.p8.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.p8.1.m1.1.1.1.1.1.1.1.3">𝛾</ci></apply><apply id="S5.p8.1.m1.1.1.1.1.3.cmml" xref="S5.p8.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.p8.1.m1.1.1.1.1.3.1.cmml" xref="S5.p8.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.p8.1.m1.1.1.1.1.3.2.cmml" xref="S5.p8.1.m1.1.1.1.1.3.2">ℒ</ci><apply id="S5.p8.1.m1.1.1.1.1.3.3.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3"><times id="S5.p8.1.m1.1.1.1.1.3.3.1.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3.1"></times><ci id="S5.p8.1.m1.1.1.1.1.3.3.2.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3.2">𝐶</ci><ci id="S5.p8.1.m1.1.1.1.1.3.3.3.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3.3">𝐿</ci><ci id="S5.p8.1.m1.1.1.1.1.3.3.4.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3.4">𝐴</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.1.m1.1c">\mathcal{L}=\gamma\mathcal{L}_{SA}+(1-\gamma)\mathcal{L}_{CLA}</annotation></semantics></math>, where <math id="S5.p8.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.p8.2.m2.1a"><mi id="S5.p8.2.m2.1.1" xref="S5.p8.2.m2.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.p8.2.m2.1b"><ci id="S5.p8.2.m2.1.1.cmml" xref="S5.p8.2.m2.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.2.m2.1c">\gamma</annotation></semantics></math> is a balancing factor which controls the relative weight of each term.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experimental evaluation</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.9" class="ltx_p">In order to assess the potential of using synthetically generated music for training tagging systems, we run a series of experiments. For all settings, we train with a batch size of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.p1.1.m1.1a"><mn id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><cn type="integer" id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">4</annotation></semantics></math> and use the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> optimizer with an initial learning rate of <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.p1.2.m2.1a"><mn id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><cn type="float" id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">0.001</annotation></semantics></math>, except for the fine-tuning setup where we decrease the initial learning rate to <math id="S6.p1.3.m3.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S6.p1.3.m3.1a"><mn id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><cn type="float" id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">0.0001</annotation></semantics></math>. For the domain adaptation experiment, we set the margin to <math id="S6.p1.4.m4.1" class="ltx_Math" alttext="m=2" display="inline"><semantics id="S6.p1.4.m4.1a"><mrow id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml"><mi id="S6.p1.4.m4.1.1.2" xref="S6.p1.4.m4.1.1.2.cmml">m</mi><mo id="S6.p1.4.m4.1.1.1" xref="S6.p1.4.m4.1.1.1.cmml">=</mo><mn id="S6.p1.4.m4.1.1.3" xref="S6.p1.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><apply id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1"><eq id="S6.p1.4.m4.1.1.1.cmml" xref="S6.p1.4.m4.1.1.1"></eq><ci id="S6.p1.4.m4.1.1.2.cmml" xref="S6.p1.4.m4.1.1.2">𝑚</ci><cn type="integer" id="S6.p1.4.m4.1.1.3.cmml" xref="S6.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">m=2</annotation></semantics></math> and the balance parameter to <math id="S6.p1.5.m5.1" class="ltx_Math" alttext="\gamma=0.7" display="inline"><semantics id="S6.p1.5.m5.1a"><mrow id="S6.p1.5.m5.1.1" xref="S6.p1.5.m5.1.1.cmml"><mi id="S6.p1.5.m5.1.1.2" xref="S6.p1.5.m5.1.1.2.cmml">γ</mi><mo id="S6.p1.5.m5.1.1.1" xref="S6.p1.5.m5.1.1.1.cmml">=</mo><mn id="S6.p1.5.m5.1.1.3" xref="S6.p1.5.m5.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.5.m5.1b"><apply id="S6.p1.5.m5.1.1.cmml" xref="S6.p1.5.m5.1.1"><eq id="S6.p1.5.m5.1.1.1.cmml" xref="S6.p1.5.m5.1.1.1"></eq><ci id="S6.p1.5.m5.1.1.2.cmml" xref="S6.p1.5.m5.1.1.2">𝛾</ci><cn type="float" id="S6.p1.5.m5.1.1.3.cmml" xref="S6.p1.5.m5.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.5.m5.1c">\gamma=0.7</annotation></semantics></math>. We furthermore use an early stopping criterion based on the classifier’s categorical cross-entropy loss with a patience period of <math id="S6.p1.6.m6.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S6.p1.6.m6.1a"><mn id="S6.p1.6.m6.1.1" xref="S6.p1.6.m6.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.p1.6.m6.1b"><cn type="integer" id="S6.p1.6.m6.1.1.cmml" xref="S6.p1.6.m6.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.6.m6.1c">5</annotation></semantics></math> epochs and select the best performing model for evaluation. For the GTZAN dataset, where excerpts are of length <math id="S6.p1.7.m7.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.p1.7.m7.1a"><mn id="S6.p1.7.m7.1.1" xref="S6.p1.7.m7.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.p1.7.m7.1b"><cn type="integer" id="S6.p1.7.m7.1.1.cmml" xref="S6.p1.7.m7.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.7.m7.1c">30</annotation></semantics></math>s, we use a random <math id="S6.p1.8.m8.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.p1.8.m8.1a"><mn id="S6.p1.8.m8.1.1" xref="S6.p1.8.m8.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.p1.8.m8.1b"><cn type="integer" id="S6.p1.8.m8.1.1.cmml" xref="S6.p1.8.m8.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.8.m8.1c">10</annotation></semantics></math>s excerpt during training and the central <math id="S6.p1.9.m9.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.p1.9.m9.1a"><mn id="S6.p1.9.m9.1.1" xref="S6.p1.9.m9.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.p1.9.m9.1b"><cn type="integer" id="S6.p1.9.m9.1.1.cmml" xref="S6.p1.9.m9.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.9.m9.1c">10</annotation></semantics></math>s segment during validation.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For all experiments evaluated on the GTZAN dataset, we report mean and standard deviation of the categorical accuracy and of the categorical cross-entropy loss over the three validation splits proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. When evaluating on synthetic data, we report mean and standard deviation over three random splits instead. All results are shown in Table <a href="#S6.T1" title="TABLE I ‣ VI Experimental evaluation ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a></p>
</div>
<figure id="S6.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results of the experimental evaluation. We report mean <math id="S6.T1.3.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S6.T1.3.m1.1b"><mi id="S6.T1.3.m1.1.1" xref="S6.T1.3.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S6.T1.3.m1.1c"><ci id="S6.T1.3.m1.1.1.cmml" xref="S6.T1.3.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.m1.1d">\mu</annotation></semantics></math> and standard deviation <math id="S6.T1.4.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S6.T1.4.m2.1b"><mi id="S6.T1.4.m2.1.1" xref="S6.T1.4.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S6.T1.4.m2.1c"><ci id="S6.T1.4.m2.1.1.cmml" xref="S6.T1.4.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.m2.1d">\sigma</annotation></semantics></math> of the categorical accuracy and of the categorical cross-entropy loss over the three artist-filtered validation splits proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</figcaption>
<table id="S6.T1.18.14" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T1.6.2.2" class="ltx_tr">
<th id="S6.T1.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S6.T1.6.2.2.3.1" class="ltx_text ltx_font_bold">System</span></th>
<th id="S6.T1.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S6.T1.5.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span> <math id="S6.T1.5.1.1.1.m1.1" class="ltx_Math" alttext="\mu(\sigma)" display="inline"><semantics id="S6.T1.5.1.1.1.m1.1a"><mrow id="S6.T1.5.1.1.1.m1.1.2" xref="S6.T1.5.1.1.1.m1.1.2.cmml"><mi id="S6.T1.5.1.1.1.m1.1.2.2" xref="S6.T1.5.1.1.1.m1.1.2.2.cmml">μ</mi><mo lspace="0em" rspace="0em" id="S6.T1.5.1.1.1.m1.1.2.1" xref="S6.T1.5.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.5.1.1.1.m1.1.2.3.2" xref="S6.T1.5.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.5.1.1.1.m1.1.2.3.2.1" xref="S6.T1.5.1.1.1.m1.1.2.cmml">(</mo><mi id="S6.T1.5.1.1.1.m1.1.1" xref="S6.T1.5.1.1.1.m1.1.1.cmml">σ</mi><mo stretchy="false" id="S6.T1.5.1.1.1.m1.1.2.3.2.2" xref="S6.T1.5.1.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.5.1.1.1.m1.1b"><apply id="S6.T1.5.1.1.1.m1.1.2.cmml" xref="S6.T1.5.1.1.1.m1.1.2"><times id="S6.T1.5.1.1.1.m1.1.2.1.cmml" xref="S6.T1.5.1.1.1.m1.1.2.1"></times><ci id="S6.T1.5.1.1.1.m1.1.2.2.cmml" xref="S6.T1.5.1.1.1.m1.1.2.2">𝜇</ci><ci id="S6.T1.5.1.1.1.m1.1.1.cmml" xref="S6.T1.5.1.1.1.m1.1.1">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.5.1.1.1.m1.1c">\mu(\sigma)</annotation></semantics></math>
</th>
<th id="S6.T1.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S6.T1.6.2.2.2.1" class="ltx_text ltx_font_bold">Loss</span> <math id="S6.T1.6.2.2.2.m1.1" class="ltx_Math" alttext="\mu(\sigma)" display="inline"><semantics id="S6.T1.6.2.2.2.m1.1a"><mrow id="S6.T1.6.2.2.2.m1.1.2" xref="S6.T1.6.2.2.2.m1.1.2.cmml"><mi id="S6.T1.6.2.2.2.m1.1.2.2" xref="S6.T1.6.2.2.2.m1.1.2.2.cmml">μ</mi><mo lspace="0em" rspace="0em" id="S6.T1.6.2.2.2.m1.1.2.1" xref="S6.T1.6.2.2.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.6.2.2.2.m1.1.2.3.2" xref="S6.T1.6.2.2.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.6.2.2.2.m1.1.2.3.2.1" xref="S6.T1.6.2.2.2.m1.1.2.cmml">(</mo><mi id="S6.T1.6.2.2.2.m1.1.1" xref="S6.T1.6.2.2.2.m1.1.1.cmml">σ</mi><mo stretchy="false" id="S6.T1.6.2.2.2.m1.1.2.3.2.2" xref="S6.T1.6.2.2.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.6.2.2.2.m1.1b"><apply id="S6.T1.6.2.2.2.m1.1.2.cmml" xref="S6.T1.6.2.2.2.m1.1.2"><times id="S6.T1.6.2.2.2.m1.1.2.1.cmml" xref="S6.T1.6.2.2.2.m1.1.2.1"></times><ci id="S6.T1.6.2.2.2.m1.1.2.2.cmml" xref="S6.T1.6.2.2.2.m1.1.2.2">𝜇</ci><ci id="S6.T1.6.2.2.2.m1.1.1.cmml" xref="S6.T1.6.2.2.2.m1.1.1">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.6.2.2.2.m1.1c">\mu(\sigma)</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T1.8.4.4" class="ltx_tr">
<th id="S6.T1.8.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">E2E-real</th>
<td id="S6.T1.7.3.3.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S6.T1.7.3.3.1.m1.1" class="ltx_Math" alttext="46.7\%(5.2\%)" display="inline"><semantics id="S6.T1.7.3.3.1.m1.1a"><mrow id="S6.T1.7.3.3.1.m1.1.1" xref="S6.T1.7.3.3.1.m1.1.1.cmml"><mrow id="S6.T1.7.3.3.1.m1.1.1.3" xref="S6.T1.7.3.3.1.m1.1.1.3.cmml"><mn id="S6.T1.7.3.3.1.m1.1.1.3.2" xref="S6.T1.7.3.3.1.m1.1.1.3.2.cmml">46.7</mn><mo id="S6.T1.7.3.3.1.m1.1.1.3.1" xref="S6.T1.7.3.3.1.m1.1.1.3.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S6.T1.7.3.3.1.m1.1.1.2" xref="S6.T1.7.3.3.1.m1.1.1.2.cmml">​</mo><mrow id="S6.T1.7.3.3.1.m1.1.1.1.1" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.T1.7.3.3.1.m1.1.1.1.1.2" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.T1.7.3.3.1.m1.1.1.1.1.1" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.cmml"><mn id="S6.T1.7.3.3.1.m1.1.1.1.1.1.2" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.2.cmml">5.2</mn><mo id="S6.T1.7.3.3.1.m1.1.1.1.1.1.1" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S6.T1.7.3.3.1.m1.1.1.1.1.3" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.7.3.3.1.m1.1b"><apply id="S6.T1.7.3.3.1.m1.1.1.cmml" xref="S6.T1.7.3.3.1.m1.1.1"><times id="S6.T1.7.3.3.1.m1.1.1.2.cmml" xref="S6.T1.7.3.3.1.m1.1.1.2"></times><apply id="S6.T1.7.3.3.1.m1.1.1.3.cmml" xref="S6.T1.7.3.3.1.m1.1.1.3"><csymbol cd="latexml" id="S6.T1.7.3.3.1.m1.1.1.3.1.cmml" xref="S6.T1.7.3.3.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S6.T1.7.3.3.1.m1.1.1.3.2.cmml" xref="S6.T1.7.3.3.1.m1.1.1.3.2">46.7</cn></apply><apply id="S6.T1.7.3.3.1.m1.1.1.1.1.1.cmml" xref="S6.T1.7.3.3.1.m1.1.1.1.1"><csymbol cd="latexml" id="S6.T1.7.3.3.1.m1.1.1.1.1.1.1.cmml" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.1">percent</csymbol><cn type="float" id="S6.T1.7.3.3.1.m1.1.1.1.1.1.2.cmml" xref="S6.T1.7.3.3.1.m1.1.1.1.1.1.2">5.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.7.3.3.1.m1.1c">46.7\%(5.2\%)</annotation></semantics></math></td>
<td id="S6.T1.8.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S6.T1.8.4.4.2.m1.1" class="ltx_Math" alttext="1.61(0.05)" display="inline"><semantics id="S6.T1.8.4.4.2.m1.1a"><mrow id="S6.T1.8.4.4.2.m1.1.2" xref="S6.T1.8.4.4.2.m1.1.2.cmml"><mn id="S6.T1.8.4.4.2.m1.1.2.2" xref="S6.T1.8.4.4.2.m1.1.2.2.cmml">1.61</mn><mo lspace="0em" rspace="0em" id="S6.T1.8.4.4.2.m1.1.2.1" xref="S6.T1.8.4.4.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.8.4.4.2.m1.1.2.3.2" xref="S6.T1.8.4.4.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.8.4.4.2.m1.1.2.3.2.1" xref="S6.T1.8.4.4.2.m1.1.2.cmml">(</mo><mn id="S6.T1.8.4.4.2.m1.1.1" xref="S6.T1.8.4.4.2.m1.1.1.cmml">0.05</mn><mo stretchy="false" id="S6.T1.8.4.4.2.m1.1.2.3.2.2" xref="S6.T1.8.4.4.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.8.4.4.2.m1.1b"><apply id="S6.T1.8.4.4.2.m1.1.2.cmml" xref="S6.T1.8.4.4.2.m1.1.2"><times id="S6.T1.8.4.4.2.m1.1.2.1.cmml" xref="S6.T1.8.4.4.2.m1.1.2.1"></times><cn type="float" id="S6.T1.8.4.4.2.m1.1.2.2.cmml" xref="S6.T1.8.4.4.2.m1.1.2.2">1.61</cn><cn type="float" id="S6.T1.8.4.4.2.m1.1.1.cmml" xref="S6.T1.8.4.4.2.m1.1.1">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.8.4.4.2.m1.1c">1.61(0.05)</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.10.6.6" class="ltx_tr">
<th id="S6.T1.10.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">E2E-synth</th>
<td id="S6.T1.9.5.5.1" class="ltx_td ltx_align_center"><math id="S6.T1.9.5.5.1.m1.1" class="ltx_math_unparsed" alttext="90.6.\%(1.6\%)" display="inline"><semantics id="S6.T1.9.5.5.1.m1.1a"><mrow id="S6.T1.9.5.5.1.m1.1b"><mn id="S6.T1.9.5.5.1.m1.1.1">90.6</mn><mo lspace="0em" rspace="0.167em" id="S6.T1.9.5.5.1.m1.1.2">.</mo><mo id="S6.T1.9.5.5.1.m1.1.3">%</mo><mrow id="S6.T1.9.5.5.1.m1.1.4"><mo stretchy="false" id="S6.T1.9.5.5.1.m1.1.4.1">(</mo><mn id="S6.T1.9.5.5.1.m1.1.4.2">1.6</mn><mo id="S6.T1.9.5.5.1.m1.1.4.3">%</mo><mo stretchy="false" id="S6.T1.9.5.5.1.m1.1.4.4">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S6.T1.9.5.5.1.m1.1c">90.6.\%(1.6\%)</annotation></semantics></math></td>
<td id="S6.T1.10.6.6.2" class="ltx_td ltx_align_center"><math id="S6.T1.10.6.6.2.m1.1" class="ltx_Math" alttext="0.34(0.02)" display="inline"><semantics id="S6.T1.10.6.6.2.m1.1a"><mrow id="S6.T1.10.6.6.2.m1.1.2" xref="S6.T1.10.6.6.2.m1.1.2.cmml"><mn id="S6.T1.10.6.6.2.m1.1.2.2" xref="S6.T1.10.6.6.2.m1.1.2.2.cmml">0.34</mn><mo lspace="0em" rspace="0em" id="S6.T1.10.6.6.2.m1.1.2.1" xref="S6.T1.10.6.6.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.10.6.6.2.m1.1.2.3.2" xref="S6.T1.10.6.6.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.10.6.6.2.m1.1.2.3.2.1" xref="S6.T1.10.6.6.2.m1.1.2.cmml">(</mo><mn id="S6.T1.10.6.6.2.m1.1.1" xref="S6.T1.10.6.6.2.m1.1.1.cmml">0.02</mn><mo stretchy="false" id="S6.T1.10.6.6.2.m1.1.2.3.2.2" xref="S6.T1.10.6.6.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.10.6.6.2.m1.1b"><apply id="S6.T1.10.6.6.2.m1.1.2.cmml" xref="S6.T1.10.6.6.2.m1.1.2"><times id="S6.T1.10.6.6.2.m1.1.2.1.cmml" xref="S6.T1.10.6.6.2.m1.1.2.1"></times><cn type="float" id="S6.T1.10.6.6.2.m1.1.2.2.cmml" xref="S6.T1.10.6.6.2.m1.1.2.2">0.34</cn><cn type="float" id="S6.T1.10.6.6.2.m1.1.1.cmml" xref="S6.T1.10.6.6.2.m1.1.1">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.10.6.6.2.m1.1c">0.34(0.02)</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.12.8.8" class="ltx_tr">
<th id="S6.T1.12.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">E2E-add</th>
<td id="S6.T1.11.7.7.1" class="ltx_td ltx_align_center"><math id="S6.T1.11.7.7.1.m1.1" class="ltx_Math" alttext="47.3\%(4.6\%)" display="inline"><semantics id="S6.T1.11.7.7.1.m1.1a"><mrow id="S6.T1.11.7.7.1.m1.1.1" xref="S6.T1.11.7.7.1.m1.1.1.cmml"><mrow id="S6.T1.11.7.7.1.m1.1.1.3" xref="S6.T1.11.7.7.1.m1.1.1.3.cmml"><mn id="S6.T1.11.7.7.1.m1.1.1.3.2" xref="S6.T1.11.7.7.1.m1.1.1.3.2.cmml">47.3</mn><mo id="S6.T1.11.7.7.1.m1.1.1.3.1" xref="S6.T1.11.7.7.1.m1.1.1.3.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S6.T1.11.7.7.1.m1.1.1.2" xref="S6.T1.11.7.7.1.m1.1.1.2.cmml">​</mo><mrow id="S6.T1.11.7.7.1.m1.1.1.1.1" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.T1.11.7.7.1.m1.1.1.1.1.2" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.T1.11.7.7.1.m1.1.1.1.1.1" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.cmml"><mn id="S6.T1.11.7.7.1.m1.1.1.1.1.1.2" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.2.cmml">4.6</mn><mo id="S6.T1.11.7.7.1.m1.1.1.1.1.1.1" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S6.T1.11.7.7.1.m1.1.1.1.1.3" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.11.7.7.1.m1.1b"><apply id="S6.T1.11.7.7.1.m1.1.1.cmml" xref="S6.T1.11.7.7.1.m1.1.1"><times id="S6.T1.11.7.7.1.m1.1.1.2.cmml" xref="S6.T1.11.7.7.1.m1.1.1.2"></times><apply id="S6.T1.11.7.7.1.m1.1.1.3.cmml" xref="S6.T1.11.7.7.1.m1.1.1.3"><csymbol cd="latexml" id="S6.T1.11.7.7.1.m1.1.1.3.1.cmml" xref="S6.T1.11.7.7.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S6.T1.11.7.7.1.m1.1.1.3.2.cmml" xref="S6.T1.11.7.7.1.m1.1.1.3.2">47.3</cn></apply><apply id="S6.T1.11.7.7.1.m1.1.1.1.1.1.cmml" xref="S6.T1.11.7.7.1.m1.1.1.1.1"><csymbol cd="latexml" id="S6.T1.11.7.7.1.m1.1.1.1.1.1.1.cmml" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.1">percent</csymbol><cn type="float" id="S6.T1.11.7.7.1.m1.1.1.1.1.1.2.cmml" xref="S6.T1.11.7.7.1.m1.1.1.1.1.1.2">4.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.11.7.7.1.m1.1c">47.3\%(4.6\%)</annotation></semantics></math></td>
<td id="S6.T1.12.8.8.2" class="ltx_td ltx_align_center"><math id="S6.T1.12.8.8.2.m1.1" class="ltx_Math" alttext="1.74(0.05)" display="inline"><semantics id="S6.T1.12.8.8.2.m1.1a"><mrow id="S6.T1.12.8.8.2.m1.1.2" xref="S6.T1.12.8.8.2.m1.1.2.cmml"><mn id="S6.T1.12.8.8.2.m1.1.2.2" xref="S6.T1.12.8.8.2.m1.1.2.2.cmml">1.74</mn><mo lspace="0em" rspace="0em" id="S6.T1.12.8.8.2.m1.1.2.1" xref="S6.T1.12.8.8.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.12.8.8.2.m1.1.2.3.2" xref="S6.T1.12.8.8.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.12.8.8.2.m1.1.2.3.2.1" xref="S6.T1.12.8.8.2.m1.1.2.cmml">(</mo><mn id="S6.T1.12.8.8.2.m1.1.1" xref="S6.T1.12.8.8.2.m1.1.1.cmml">0.05</mn><mo stretchy="false" id="S6.T1.12.8.8.2.m1.1.2.3.2.2" xref="S6.T1.12.8.8.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.12.8.8.2.m1.1b"><apply id="S6.T1.12.8.8.2.m1.1.2.cmml" xref="S6.T1.12.8.8.2.m1.1.2"><times id="S6.T1.12.8.8.2.m1.1.2.1.cmml" xref="S6.T1.12.8.8.2.m1.1.2.1"></times><cn type="float" id="S6.T1.12.8.8.2.m1.1.2.2.cmml" xref="S6.T1.12.8.8.2.m1.1.2.2">1.74</cn><cn type="float" id="S6.T1.12.8.8.2.m1.1.1.cmml" xref="S6.T1.12.8.8.2.m1.1.1">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.12.8.8.2.m1.1c">1.74(0.05)</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.14.10.10" class="ltx_tr">
<th id="S6.T1.14.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">E2E-DA</th>
<td id="S6.T1.13.9.9.1" class="ltx_td ltx_align_center"><math id="S6.T1.13.9.9.1.m1.1" class="ltx_Math" alttext="47.6\%(6.0\%)" display="inline"><semantics id="S6.T1.13.9.9.1.m1.1a"><mrow id="S6.T1.13.9.9.1.m1.1.1" xref="S6.T1.13.9.9.1.m1.1.1.cmml"><mrow id="S6.T1.13.9.9.1.m1.1.1.3" xref="S6.T1.13.9.9.1.m1.1.1.3.cmml"><mn id="S6.T1.13.9.9.1.m1.1.1.3.2" xref="S6.T1.13.9.9.1.m1.1.1.3.2.cmml">47.6</mn><mo id="S6.T1.13.9.9.1.m1.1.1.3.1" xref="S6.T1.13.9.9.1.m1.1.1.3.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S6.T1.13.9.9.1.m1.1.1.2" xref="S6.T1.13.9.9.1.m1.1.1.2.cmml">​</mo><mrow id="S6.T1.13.9.9.1.m1.1.1.1.1" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.T1.13.9.9.1.m1.1.1.1.1.2" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.T1.13.9.9.1.m1.1.1.1.1.1" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.cmml"><mn id="S6.T1.13.9.9.1.m1.1.1.1.1.1.2" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.2.cmml">6.0</mn><mo id="S6.T1.13.9.9.1.m1.1.1.1.1.1.1" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S6.T1.13.9.9.1.m1.1.1.1.1.3" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.13.9.9.1.m1.1b"><apply id="S6.T1.13.9.9.1.m1.1.1.cmml" xref="S6.T1.13.9.9.1.m1.1.1"><times id="S6.T1.13.9.9.1.m1.1.1.2.cmml" xref="S6.T1.13.9.9.1.m1.1.1.2"></times><apply id="S6.T1.13.9.9.1.m1.1.1.3.cmml" xref="S6.T1.13.9.9.1.m1.1.1.3"><csymbol cd="latexml" id="S6.T1.13.9.9.1.m1.1.1.3.1.cmml" xref="S6.T1.13.9.9.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S6.T1.13.9.9.1.m1.1.1.3.2.cmml" xref="S6.T1.13.9.9.1.m1.1.1.3.2">47.6</cn></apply><apply id="S6.T1.13.9.9.1.m1.1.1.1.1.1.cmml" xref="S6.T1.13.9.9.1.m1.1.1.1.1"><csymbol cd="latexml" id="S6.T1.13.9.9.1.m1.1.1.1.1.1.1.cmml" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.1">percent</csymbol><cn type="float" id="S6.T1.13.9.9.1.m1.1.1.1.1.1.2.cmml" xref="S6.T1.13.9.9.1.m1.1.1.1.1.1.2">6.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.13.9.9.1.m1.1c">47.6\%(6.0\%)</annotation></semantics></math></td>
<td id="S6.T1.14.10.10.2" class="ltx_td ltx_align_center"><math id="S6.T1.14.10.10.2.m1.1" class="ltx_Math" alttext="1.54(0.06)" display="inline"><semantics id="S6.T1.14.10.10.2.m1.1a"><mrow id="S6.T1.14.10.10.2.m1.1.2" xref="S6.T1.14.10.10.2.m1.1.2.cmml"><mn id="S6.T1.14.10.10.2.m1.1.2.2" xref="S6.T1.14.10.10.2.m1.1.2.2.cmml">1.54</mn><mo lspace="0em" rspace="0em" id="S6.T1.14.10.10.2.m1.1.2.1" xref="S6.T1.14.10.10.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.14.10.10.2.m1.1.2.3.2" xref="S6.T1.14.10.10.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.14.10.10.2.m1.1.2.3.2.1" xref="S6.T1.14.10.10.2.m1.1.2.cmml">(</mo><mn id="S6.T1.14.10.10.2.m1.1.1" xref="S6.T1.14.10.10.2.m1.1.1.cmml">0.06</mn><mo stretchy="false" id="S6.T1.14.10.10.2.m1.1.2.3.2.2" xref="S6.T1.14.10.10.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.14.10.10.2.m1.1b"><apply id="S6.T1.14.10.10.2.m1.1.2.cmml" xref="S6.T1.14.10.10.2.m1.1.2"><times id="S6.T1.14.10.10.2.m1.1.2.1.cmml" xref="S6.T1.14.10.10.2.m1.1.2.1"></times><cn type="float" id="S6.T1.14.10.10.2.m1.1.2.2.cmml" xref="S6.T1.14.10.10.2.m1.1.2.2">1.54</cn><cn type="float" id="S6.T1.14.10.10.2.m1.1.1.cmml" xref="S6.T1.14.10.10.2.m1.1.1">0.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.14.10.10.2.m1.1c">1.54(0.06)</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.16.12.12" class="ltx_tr">
<th id="S6.T1.16.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">TL</th>
<td id="S6.T1.15.11.11.1" class="ltx_td ltx_align_center"><math id="S6.T1.15.11.11.1.m1.1" class="ltx_Math" alttext="52.6\%(3.3\%)" display="inline"><semantics id="S6.T1.15.11.11.1.m1.1a"><mrow id="S6.T1.15.11.11.1.m1.1.1" xref="S6.T1.15.11.11.1.m1.1.1.cmml"><mrow id="S6.T1.15.11.11.1.m1.1.1.3" xref="S6.T1.15.11.11.1.m1.1.1.3.cmml"><mn id="S6.T1.15.11.11.1.m1.1.1.3.2" xref="S6.T1.15.11.11.1.m1.1.1.3.2.cmml">52.6</mn><mo id="S6.T1.15.11.11.1.m1.1.1.3.1" xref="S6.T1.15.11.11.1.m1.1.1.3.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S6.T1.15.11.11.1.m1.1.1.2" xref="S6.T1.15.11.11.1.m1.1.1.2.cmml">​</mo><mrow id="S6.T1.15.11.11.1.m1.1.1.1.1" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.T1.15.11.11.1.m1.1.1.1.1.2" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.T1.15.11.11.1.m1.1.1.1.1.1" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.cmml"><mn id="S6.T1.15.11.11.1.m1.1.1.1.1.1.2" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.2.cmml">3.3</mn><mo id="S6.T1.15.11.11.1.m1.1.1.1.1.1.1" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S6.T1.15.11.11.1.m1.1.1.1.1.3" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.15.11.11.1.m1.1b"><apply id="S6.T1.15.11.11.1.m1.1.1.cmml" xref="S6.T1.15.11.11.1.m1.1.1"><times id="S6.T1.15.11.11.1.m1.1.1.2.cmml" xref="S6.T1.15.11.11.1.m1.1.1.2"></times><apply id="S6.T1.15.11.11.1.m1.1.1.3.cmml" xref="S6.T1.15.11.11.1.m1.1.1.3"><csymbol cd="latexml" id="S6.T1.15.11.11.1.m1.1.1.3.1.cmml" xref="S6.T1.15.11.11.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S6.T1.15.11.11.1.m1.1.1.3.2.cmml" xref="S6.T1.15.11.11.1.m1.1.1.3.2">52.6</cn></apply><apply id="S6.T1.15.11.11.1.m1.1.1.1.1.1.cmml" xref="S6.T1.15.11.11.1.m1.1.1.1.1"><csymbol cd="latexml" id="S6.T1.15.11.11.1.m1.1.1.1.1.1.1.cmml" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.1">percent</csymbol><cn type="float" id="S6.T1.15.11.11.1.m1.1.1.1.1.1.2.cmml" xref="S6.T1.15.11.11.1.m1.1.1.1.1.1.2">3.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.15.11.11.1.m1.1c">52.6\%(3.3\%)</annotation></semantics></math></td>
<td id="S6.T1.16.12.12.2" class="ltx_td ltx_align_center"><math id="S6.T1.16.12.12.2.m1.1" class="ltx_Math" alttext="1.40(0.06)" display="inline"><semantics id="S6.T1.16.12.12.2.m1.1a"><mrow id="S6.T1.16.12.12.2.m1.1.2" xref="S6.T1.16.12.12.2.m1.1.2.cmml"><mn id="S6.T1.16.12.12.2.m1.1.2.2" xref="S6.T1.16.12.12.2.m1.1.2.2.cmml">1.40</mn><mo lspace="0em" rspace="0em" id="S6.T1.16.12.12.2.m1.1.2.1" xref="S6.T1.16.12.12.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.16.12.12.2.m1.1.2.3.2" xref="S6.T1.16.12.12.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.16.12.12.2.m1.1.2.3.2.1" xref="S6.T1.16.12.12.2.m1.1.2.cmml">(</mo><mn id="S6.T1.16.12.12.2.m1.1.1" xref="S6.T1.16.12.12.2.m1.1.1.cmml">0.06</mn><mo stretchy="false" id="S6.T1.16.12.12.2.m1.1.2.3.2.2" xref="S6.T1.16.12.12.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.16.12.12.2.m1.1b"><apply id="S6.T1.16.12.12.2.m1.1.2.cmml" xref="S6.T1.16.12.12.2.m1.1.2"><times id="S6.T1.16.12.12.2.m1.1.2.1.cmml" xref="S6.T1.16.12.12.2.m1.1.2.1"></times><cn type="float" id="S6.T1.16.12.12.2.m1.1.2.2.cmml" xref="S6.T1.16.12.12.2.m1.1.2.2">1.40</cn><cn type="float" id="S6.T1.16.12.12.2.m1.1.1.cmml" xref="S6.T1.16.12.12.2.m1.1.1">0.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.16.12.12.2.m1.1c">1.40(0.06)</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.18.14.14" class="ltx_tr">
<th id="S6.T1.18.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">FT</th>
<td id="S6.T1.17.13.13.1" class="ltx_td ltx_align_center ltx_border_b"><math id="S6.T1.17.13.13.1.m1.1" class="ltx_Math" alttext="54.8\%(6.2\%)" display="inline"><semantics id="S6.T1.17.13.13.1.m1.1a"><mrow id="S6.T1.17.13.13.1.m1.1.1" xref="S6.T1.17.13.13.1.m1.1.1.cmml"><mrow id="S6.T1.17.13.13.1.m1.1.1.3" xref="S6.T1.17.13.13.1.m1.1.1.3.cmml"><mn id="S6.T1.17.13.13.1.m1.1.1.3.2" xref="S6.T1.17.13.13.1.m1.1.1.3.2.cmml">54.8</mn><mo id="S6.T1.17.13.13.1.m1.1.1.3.1" xref="S6.T1.17.13.13.1.m1.1.1.3.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S6.T1.17.13.13.1.m1.1.1.2" xref="S6.T1.17.13.13.1.m1.1.1.2.cmml">​</mo><mrow id="S6.T1.17.13.13.1.m1.1.1.1.1" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.T1.17.13.13.1.m1.1.1.1.1.2" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.T1.17.13.13.1.m1.1.1.1.1.1" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.cmml"><mn id="S6.T1.17.13.13.1.m1.1.1.1.1.1.2" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.2.cmml">6.2</mn><mo id="S6.T1.17.13.13.1.m1.1.1.1.1.1.1" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S6.T1.17.13.13.1.m1.1.1.1.1.3" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.17.13.13.1.m1.1b"><apply id="S6.T1.17.13.13.1.m1.1.1.cmml" xref="S6.T1.17.13.13.1.m1.1.1"><times id="S6.T1.17.13.13.1.m1.1.1.2.cmml" xref="S6.T1.17.13.13.1.m1.1.1.2"></times><apply id="S6.T1.17.13.13.1.m1.1.1.3.cmml" xref="S6.T1.17.13.13.1.m1.1.1.3"><csymbol cd="latexml" id="S6.T1.17.13.13.1.m1.1.1.3.1.cmml" xref="S6.T1.17.13.13.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S6.T1.17.13.13.1.m1.1.1.3.2.cmml" xref="S6.T1.17.13.13.1.m1.1.1.3.2">54.8</cn></apply><apply id="S6.T1.17.13.13.1.m1.1.1.1.1.1.cmml" xref="S6.T1.17.13.13.1.m1.1.1.1.1"><csymbol cd="latexml" id="S6.T1.17.13.13.1.m1.1.1.1.1.1.1.cmml" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.1">percent</csymbol><cn type="float" id="S6.T1.17.13.13.1.m1.1.1.1.1.1.2.cmml" xref="S6.T1.17.13.13.1.m1.1.1.1.1.1.2">6.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.17.13.13.1.m1.1c">54.8\%(6.2\%)</annotation></semantics></math></td>
<td id="S6.T1.18.14.14.2" class="ltx_td ltx_align_center ltx_border_b"><math id="S6.T1.18.14.14.2.m1.1" class="ltx_Math" alttext="1.39(0.12)" display="inline"><semantics id="S6.T1.18.14.14.2.m1.1a"><mrow id="S6.T1.18.14.14.2.m1.1.2" xref="S6.T1.18.14.14.2.m1.1.2.cmml"><mn id="S6.T1.18.14.14.2.m1.1.2.2" xref="S6.T1.18.14.14.2.m1.1.2.2.cmml">1.39</mn><mo lspace="0em" rspace="0em" id="S6.T1.18.14.14.2.m1.1.2.1" xref="S6.T1.18.14.14.2.m1.1.2.1.cmml">​</mo><mrow id="S6.T1.18.14.14.2.m1.1.2.3.2" xref="S6.T1.18.14.14.2.m1.1.2.cmml"><mo stretchy="false" id="S6.T1.18.14.14.2.m1.1.2.3.2.1" xref="S6.T1.18.14.14.2.m1.1.2.cmml">(</mo><mn id="S6.T1.18.14.14.2.m1.1.1" xref="S6.T1.18.14.14.2.m1.1.1.cmml">0.12</mn><mo stretchy="false" id="S6.T1.18.14.14.2.m1.1.2.3.2.2" xref="S6.T1.18.14.14.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.18.14.14.2.m1.1b"><apply id="S6.T1.18.14.14.2.m1.1.2.cmml" xref="S6.T1.18.14.14.2.m1.1.2"><times id="S6.T1.18.14.14.2.m1.1.2.1.cmml" xref="S6.T1.18.14.14.2.m1.1.2.1"></times><cn type="float" id="S6.T1.18.14.14.2.m1.1.2.2.cmml" xref="S6.T1.18.14.14.2.m1.1.2.2">1.39</cn><cn type="float" id="S6.T1.18.14.14.2.m1.1.1.cmml" xref="S6.T1.18.14.14.2.m1.1.1">0.12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.18.14.14.2.m1.1c">1.39(0.12)</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.2" class="ltx_p">First, we train and evaluate the model solely on the <span id="S6.p3.2.1" class="ltx_text ltx_font_italic">GTZAN</span> dataset (<span id="S6.p3.2.2" class="ltx_text ltx_font_bold">E2E-real</span>). Given the relatively large number of network parameters and the rather small dataset volume, we observe that the model overfits early and yields a mean accuracy of <math id="S6.p3.1.m1.1" class="ltx_Math" alttext="46.7\%" display="inline"><semantics id="S6.p3.1.m1.1a"><mrow id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml"><mn id="S6.p3.1.m1.1.1.2" xref="S6.p3.1.m1.1.1.2.cmml">46.7</mn><mo id="S6.p3.1.m1.1.1.1" xref="S6.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><apply id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1"><csymbol cd="latexml" id="S6.p3.1.m1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p3.1.m1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.2">46.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">46.7\%</annotation></semantics></math>. We also train and evaluate the same model on the larger <span id="S6.p3.2.3" class="ltx_text ltx_font_italic">GTZAN-synth</span> dataset (<span id="S6.p3.2.4" class="ltx_text ltx_font_bold">E2E-synth</span>) and observe a much higher mean accuracy of <math id="S6.p3.2.m2.1" class="ltx_Math" alttext="90.6\%" display="inline"><semantics id="S6.p3.2.m2.1a"><mrow id="S6.p3.2.m2.1.1" xref="S6.p3.2.m2.1.1.cmml"><mn id="S6.p3.2.m2.1.1.2" xref="S6.p3.2.m2.1.1.2.cmml">90.6</mn><mo id="S6.p3.2.m2.1.1.1" xref="S6.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.2.m2.1b"><apply id="S6.p3.2.m2.1.1.cmml" xref="S6.p3.2.m2.1.1"><csymbol cd="latexml" id="S6.p3.2.m2.1.1.1.cmml" xref="S6.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.p3.2.m2.1.1.2.cmml" xref="S6.p3.2.m2.1.1.2">90.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.2.m2.1c">90.6\%</annotation></semantics></math>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.3" class="ltx_p">While the result on the synthetic data appears encouraging, it does not guarantee that this classifier will generalise well to real-world music excerpts. It is for example possible that the synthetic music excerpts only represent a narrow fraction of the distribution of real music examples of the same genre. Another possibility is that the generative model causes artefacts or characteristics that are not found in real-world examples, thus causing distributional discrepancies between real and synthetic data. The fact that such issues do indeed exist is confirmed when we repeat the first experiment and simply add <span id="S6.p4.3.1" class="ltx_text ltx_font_italic">GTZAN-synth</span> to the training splits of <span id="S6.p4.3.2" class="ltx_text ltx_font_italic">GTZAN</span> (<span id="S6.p4.3.3" class="ltx_text ltx_font_bold">E2E-add</span>). Here, we observe that the mean accuracy on the <span id="S6.p4.3.4" class="ltx_text ltx_font_italic">GTZAN</span> validation does not seem to improve (<math id="S6.p4.1.m1.1" class="ltx_Math" alttext="47.3\%" display="inline"><semantics id="S6.p4.1.m1.1a"><mrow id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml"><mn id="S6.p4.1.m1.1.1.2" xref="S6.p4.1.m1.1.1.2.cmml">47.3</mn><mo id="S6.p4.1.m1.1.1.1" xref="S6.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><apply id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1"><csymbol cd="latexml" id="S6.p4.1.m1.1.1.1.cmml" xref="S6.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p4.1.m1.1.1.2.cmml" xref="S6.p4.1.m1.1.1.2">47.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">47.3\%</annotation></semantics></math>) and the loss even slightly worsens (<math id="S6.p4.2.m2.1" class="ltx_Math" alttext="1.74" display="inline"><semantics id="S6.p4.2.m2.1a"><mn id="S6.p4.2.m2.1.1" xref="S6.p4.2.m2.1.1.cmml">1.74</mn><annotation-xml encoding="MathML-Content" id="S6.p4.2.m2.1b"><cn type="float" id="S6.p4.2.m2.1.1.cmml" xref="S6.p4.2.m2.1.1">1.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.2.m2.1c">1.74</annotation></semantics></math> vs. <math id="S6.p4.3.m3.1" class="ltx_Math" alttext="1.61" display="inline"><semantics id="S6.p4.3.m3.1a"><mn id="S6.p4.3.m3.1.1" xref="S6.p4.3.m3.1.1.cmml">1.61</mn><annotation-xml encoding="MathML-Content" id="S6.p4.3.m3.1b"><cn type="float" id="S6.p4.3.m3.1.1.cmml" xref="S6.p4.3.m3.1.1">1.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.3.m3.1c">1.61</annotation></semantics></math>).</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.2" class="ltx_p">In an attempt to mitigate any distributional discrepancies between real and synthetic data, we repeat the previous experiments but employ the DA method (<span id="S6.p5.2.1" class="ltx_text ltx_font_bold">E2E-DA</span>) described in Section <a href="#S5" title="V Supervised domain adaptation ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. We first visualise the intermediate representations after the penultimate layer with and without the additional DA loss term by approximating a two-dimensional representation using the t-sne <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> algorithm. The resulting plot in Figure <a href="#S6.F3" title="Figure 3 ‣ VI Experimental evaluation ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a)) reveals that there does indeed appear to be a distributional discrepancy between real and synthetic data which seems to largely disappear when DA is used (Figure<a href="#S6.F3" title="Figure 3 ‣ VI Experimental evaluation ‣ Towards training music taggers on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b)). However, the results do not indicate a significant improvement in classification accuracy (<math id="S6.p5.1.m1.1" class="ltx_Math" alttext="47.6\%" display="inline"><semantics id="S6.p5.1.m1.1a"><mrow id="S6.p5.1.m1.1.1" xref="S6.p5.1.m1.1.1.cmml"><mn id="S6.p5.1.m1.1.1.2" xref="S6.p5.1.m1.1.1.2.cmml">47.6</mn><mo id="S6.p5.1.m1.1.1.1" xref="S6.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p5.1.m1.1b"><apply id="S6.p5.1.m1.1.1.cmml" xref="S6.p5.1.m1.1.1"><csymbol cd="latexml" id="S6.p5.1.m1.1.1.1.cmml" xref="S6.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p5.1.m1.1.1.2.cmml" xref="S6.p5.1.m1.1.1.2">47.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p5.1.m1.1c">47.6\%</annotation></semantics></math>) and only the loss appears to have somewhat improved (<math id="S6.p5.2.m2.1" class="ltx_Math" alttext="1.54" display="inline"><semantics id="S6.p5.2.m2.1a"><mn id="S6.p5.2.m2.1.1" xref="S6.p5.2.m2.1.1.cmml">1.54</mn><annotation-xml encoding="MathML-Content" id="S6.p5.2.m2.1b"><cn type="float" id="S6.p5.2.m2.1.1.cmml" xref="S6.p5.2.m2.1.1">1.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p5.2.m2.1c">1.54</annotation></semantics></math>). One reason for this behaviour could be that by driving synthetic data towards the real-world examples, the lack of variety in the training dataset persists and thus generalisation does not improve.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.2" class="ltx_p">As a next step, we investigate if the model trained solely on synthetic data (<span id="S6.p6.2.1" class="ltx_text ltx_font_bold">E2E-synth</span>) can be adapted for to real world data via transfer learning or fine-tuning. In the transfer learning experiment (<span id="S6.p6.2.2" class="ltx_text ltx_font_bold">TL</span>), we freeze the convolutional layers from <span id="S6.p6.2.3" class="ltx_text ltx_font_bold">E2E-synth</span> and only train the last two layers on <span id="S6.p6.2.4" class="ltx_text ltx_font_italic">GTZAN</span>. For this setup, we observe an increase in mean accuracy to <math id="S6.p6.1.m1.1" class="ltx_Math" alttext="52.6\%" display="inline"><semantics id="S6.p6.1.m1.1a"><mrow id="S6.p6.1.m1.1.1" xref="S6.p6.1.m1.1.1.cmml"><mn id="S6.p6.1.m1.1.1.2" xref="S6.p6.1.m1.1.1.2.cmml">52.6</mn><mo id="S6.p6.1.m1.1.1.1" xref="S6.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.1.m1.1b"><apply id="S6.p6.1.m1.1.1.cmml" xref="S6.p6.1.m1.1.1"><csymbol cd="latexml" id="S6.p6.1.m1.1.1.1.cmml" xref="S6.p6.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p6.1.m1.1.1.2.cmml" xref="S6.p6.1.m1.1.1.2">52.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.1.m1.1c">52.6\%</annotation></semantics></math>. When we fine-tune on <span id="S6.p6.2.5" class="ltx_text ltx_font_italic">GTZAN</span> by initialising the network with the weights from <span id="S6.p6.2.6" class="ltx_text ltx_font_bold">E2E-synth</span> and by resuming training on all layers, we obtain an even slightly higher increase in mean accuracy to <math id="S6.p6.2.m2.1" class="ltx_Math" alttext="54.8\%" display="inline"><semantics id="S6.p6.2.m2.1a"><mrow id="S6.p6.2.m2.1.1" xref="S6.p6.2.m2.1.1.cmml"><mn id="S6.p6.2.m2.1.1.2" xref="S6.p6.2.m2.1.1.2.cmml">54.8</mn><mo id="S6.p6.2.m2.1.1.1" xref="S6.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.2.m2.1b"><apply id="S6.p6.2.m2.1.1.cmml" xref="S6.p6.2.m2.1.1"><csymbol cd="latexml" id="S6.p6.2.m2.1.1.1.cmml" xref="S6.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.p6.2.m2.1.1.2.cmml" xref="S6.p6.2.m2.1.1.2">54.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.2.m2.1c">54.8\%</annotation></semantics></math>. This indicates that training on synthetic data allows the network to learn useful features that aid in learning downstream tasks.</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.2" class="ltx_p">While these last two experiments demonstrate that synthetic data can help to improve performance of deep architectures on small datasets, it is worth noting that the results still do not reach those reported for systems that rely on transfer learning from large annotated datasets (i.e. <math id="S6.p7.1.m1.1" class="ltx_Math" alttext="82.1\%" display="inline"><semantics id="S6.p7.1.m1.1a"><mrow id="S6.p7.1.m1.1.1" xref="S6.p7.1.m1.1.1.cmml"><mn id="S6.p7.1.m1.1.1.2" xref="S6.p7.1.m1.1.1.2.cmml">82.1</mn><mo id="S6.p7.1.m1.1.1.1" xref="S6.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p7.1.m1.1b"><apply id="S6.p7.1.m1.1.1.cmml" xref="S6.p7.1.m1.1.1"><csymbol cd="latexml" id="S6.p7.1.m1.1.1.1.cmml" xref="S6.p7.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p7.1.m1.1.1.2.cmml" xref="S6.p7.1.m1.1.1.2">82.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.1.m1.1c">82.1\%</annotation></semantics></math> reported by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>) or more lightweight end-to-end systems (i.e. <math id="S6.p7.2.m2.1" class="ltx_Math" alttext="65.8\%" display="inline"><semantics id="S6.p7.2.m2.1a"><mrow id="S6.p7.2.m2.1.1" xref="S6.p7.2.m2.1.1.cmml"><mn id="S6.p7.2.m2.1.1.2" xref="S6.p7.2.m2.1.1.2.cmml">65.8</mn><mo id="S6.p7.2.m2.1.1.1" xref="S6.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p7.2.m2.1b"><apply id="S6.p7.2.m2.1.1.cmml" xref="S6.p7.2.m2.1.1"><csymbol cd="latexml" id="S6.p7.2.m2.1.1.1.cmml" xref="S6.p7.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.p7.2.m2.1.1.2.cmml" xref="S6.p7.2.m2.1.1.2">65.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.2.m2.1c">65.8\%</annotation></semantics></math> reported by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>).</p>
</div>
<figure id="S6.F3" class="ltx_figure">
<p id="S6.F3.1" class="ltx_p ltx_align_center"><span id="S6.F3.1.1" class="ltx_text"><img src="/html/2407.02156/assets/tsne.png" id="S6.F3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="240" height="420" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>t-sne visualisations of intermediate representations of a subset of real and synthetic data with (bottom) and without (top) DA for three genres.</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have investigated the use of artificially created music excerpts for training tagging systems on the example of a genre classification task. To this end, we release <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">GTZAN-synth</span>, a collection of synthetic music excerpts that follows the taxonomy of the well-known <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">GTZAN</span> dataset but is <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.p1.1.m1.1a"><mn id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><cn type="integer" id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">10</annotation></semantics></math> times larger. In a series of experiments, we first demonstrated that simply adding synthetic data during training does not yield a significant performance improvement. We furthermore investigated domain adaptation, transfer learning and fine-tuning and showed that the two last options are suitable methods for increasing the performance of deep architecture trained on small datasets. We believe that these findings together with the release of <span id="S7.p1.1.3" class="ltx_text ltx_font_italic">GTZAN-synth</span> can foster future research into this direction, for example on advanced prompt engineering for generative music systems, on the influence of scaling the amount of synthetic data or on extending this work to other custom tagging tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Pons, O. Nieto, M. Prockup, E. M. Schmidt, A. F. Ehmann, and X. Serra, “End-to-end learning for music audio tagging at scale,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">19th International Society for Music Information Retrieval Conference (ISMIR2018)</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Pons and X. Serra, “musicnn: Pre-trained convolutional neural networks for music audio tagging,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06654</em>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Efficient supervised training of audio transformers for music representation learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16418</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. McFee, E. J. Humphrey, and J. P. Bello, “A software framework for musical data augmentation.” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ISMIR</em>, vol. 2015.   Citeseer, 2015, pp. 248–254.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Man and J. Chahl, “A review of synthetic image data and its use in computer vision,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Journal of Imaging</em>, vol. 8, no. 11, p. 310, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Boikov, V. Payor, R. Savelev, and A. Kolesnikov, “Synthetic data generation for steel defect detection and classification using deep learning,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Symmetry</em>, vol. 13, no. 7, p. 1176, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. M. Ward, J. Harguess, and C. Hilton, “Ship classification from overhead imagery using synthetic data and domain adaptation,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">OCEANS 2018 MTS/IEEE Charleston</em>.   IEEE, 2018, pp. 1–5.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.-j. Kim, H. C. Cho, and H.-c. Cho, “Deep learning-based computer-aided diagnosis system for gastroscopy image classification using synthetic data,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 11, no. 2, p. 760, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F. Ronchini, L. Comanducci, and F. Antonacci, “Synthesizing soundscapes: Leveraging text-to-audio models for environmental sound classification,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.17864</em>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Kroher, H. Cuesta, and A. Pikrakis, “Can musicgen create training data for mir tasks?” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09094</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
G. Tzanetakis and P. Cook, “Musical genre classification of audio signals,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on speech and audio processing</em>, vol. 10, no. 5, pp. 293–302, 2002.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Farahani, S. Voghoei, K. Rasheed, and H. R. Arabnia, “A brief review of domain adaptation,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020</em>, pp. 877–894, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Evaluation of cnn-based automatic music tagging models,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.00751</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Won, K. Choi, and X. Serra, “Semi-supervised music tagging transformer,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.13457</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, “The million song dataset,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011)</em>, 2011.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, “Evaluation of algorithms using games: The case of music tagging.” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ISMIR</em>.   Citeseer, 2009, pp. 387–392.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Li, R. Yuan, G. Zhang, Y. Ma, C. Lin, X. Chen, A. Ragni, H. Yin, Z. Hu, H. He <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Map-music2vec: A simple and effective baseline for self-supervised music audio representation learning,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.02508</em>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Castellon, C. Donahue, and P. Liang, “Codified audio language modeling learns useful representations for music information retrieval,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.05677</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever, “Jukebox: A generative model for music,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.00341</em>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Musiclm: Generating music from text,” <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.11325</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
P. Li, B. Chen, Y. Yao, Y. Wang, A. Wang, and A. Wang, “Jen-1: Text-guided universal music generation with omnidirectional diffusion models,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.04729</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, “Simple and controllable music generation,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Smits and T. Borghuis, “Generative ai and intellectual property rights,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Law and Artificial Intelligence: Regulating AI and Applying AI in Legal Practice</em>.   Springer, 2022, pp. 323–344.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Barnett, “The ethical implications of generative audio models: A systematic literature review,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society</em>, 2023, pp. 146–161.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y.-W. Chen, Y.-H. Yang, and H. H. Chen, “Cross-cultural music emotion recognition by adversarial discriminative domain adaptation,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)</em>.   IEEE, 2018, pp. 467–472.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
F. Bittner, M. Gonzalez, M. L. Richter, H. M. Lukashevich, and J. Abeßer, “Multi-pitch estimation meets microphone mismatch: Applicability of domain adaptation.” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ISMIR</em>, 2022, pp. 477–484.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto, “Unified deep supervised domain adaptation and generalization,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 5715–5725.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. H. Foleis and T. F. Tavares, “Texture selection for automatic music genre classification,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Applied Soft Computing</em>, vol. 89, p. 106127, 2020. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1568494620300673

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and consequences,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Minds and Machines</em>, vol. 30, pp. 681–694, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High fidelity neural audio compression,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13438</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, vol. 9, no. 11, 2008.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Lee, J. Park, K. L. Kim, and J. Nam, “Samplecnn: End-to-end deep convolutional neural networks using very small filters for music classification,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 8, no. 1, p. 150, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
F. Medhat, D. Chesmore, and J. Robinson, “Masked conditional neural networks for audio classification,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Artificial Neural Networks and Machine Learning–ICANN 2017: 26th International Conference on Artificial Neural Networks, Alghero, Italy, September 11-14, 2017, Proceedings, Part II 26</em>.   Springer, 2017, pp. 349–358.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.02155" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.02156" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.02156">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.02156" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.02157" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:39:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
